describes a memory sparse version of the baum-welch algorithm with modifications to the original probabilistic table topologies to make memory use independent of sequence length .
in cases where sequence length is very large , or state number is very large , the linear memory methods outlined may offer some utility.
the baum-welch learning procedure for hidden markov models  provides a powerful tool for tailoring hmm topologies to data for use in knowledge discovery and clustering.
to accelerate estimation of the prior state probabilities, and decrease memory use, we reverse the originally proposed forward sweep.
comparing the various implementations of the baum-welch procedure we find that the checkpointing algorithm produces the best overall tradeoff between memory use and speed.
we provide a correct recurrence relation for the emission parameter estimate and extend it to parameter estimates of the normal distribution.
our performance-optimized java implementations of baum-welch algorithm are available at .
in this paper we also describe our approach to a linear memory implementation of the viterbi decoding algorithm .
we demonstrate the use of the linear memory implementation on an extended duration hidden markov model  and on an hmm with a spike detection topology.
