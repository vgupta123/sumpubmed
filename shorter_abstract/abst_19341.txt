we developed a novel method to perform optimal experiment design to predict which experiments would most effectively allow model selection.
because the design criterion is based on predictive distributions, which can be computed for a wide range of model quantities, the approach is very flexible.
in practice, the amount of experimental data is often insufficient to make a clear distinction between competing models.
the method is based on a k-nearest neighbor estimate of the jensen shannon divergence between the multivariate predictive densities of competing models.
often one would like to perform a new experiment which would discriminate between competing hypotheses.
a bayesian approach is applied to infer model parameter distributions.
the method reveals specific combinations of experiments which improve discriminability even in cases where data is scarce.
bayesian model selection offers a way to determine the amount of evidence that data provides to support one model over the other while favoring simple models.
we show that the method successfully uses predictive differences to enable model selection by applying it to several test cases.
