in computing the auc, the strategy of pooling the test samples from the various folds of cross-validation can lead to large biases; computing it as the average of per-fold estimates avoids this bias and is thus the recommended approach.
when analysing microarray and other small sample size biological datasets, care is needed to avoid various biases.
we analyse a form of bias, stratification bias, that can substantially affect analyses using sample-reuse validation techniques and lead to inaccurate results.
as a more general solution applicable to other performance measures, we show that stratified repeated holdout and a modified version of k-fold cross-validation, balanced, stratified cross-validation and balanced leave-one-out cross-validation, avoids the bias.
therefore for model selection and evaluation of microarray and other small biological datasets, these methods should be used and unstratified versions avoided.
substantial biases are shown in simulations and on the van 't veer breast cancer dataset.
the variations in class proportions in the training and test sets are negatively correlated.
we show that when estimating the performance of classifiers on low signal datasets , which are typical of many prognostic microarray studies, commonly used performance measures can suffer from a substantial negative bias.
stratification bias can substantially affect several performance measures.
in particular, the commonly used  leave-one-out cross-validation should not be used to estimate auc for small datasets.
