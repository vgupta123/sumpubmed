the normalization of deep coverage spikes, which would otherwise inhibit consensus resolution, enables high throughput sequencing  assembly projects to consistently run to completion with existing assembly software.
previous algorithms normalize read coverage based on rmkf, but do not include methods for the preferred selection of  extremely low coverage regions produced by extremely variable sequencing of random-primed products and  2-sided paired-end sequences.
the algorithm increases the incorporation of the most unique, lowest coverage, segments of a genome using an error-corrected data set.
the results obtained from conventional overlap-layout-consensus  were compared to simulated multi-de bruijn graph assembly alternatives trained for variable coverage input using sequence before and after normalization of coverage.
however, deep coverage variations in short-read data sets and high sequencing error rates of modern sequencers present new computational challenges in data interpretation, including mapping and de novo assembly.
bioinformaticsde novo assemblycoverage reductionnormalizationsingle cellsispatranscriptomicsmultiple displacement amplificationissue-copyright-statementÂ© the author 2014
coverage reduction was shown to increase processing speed and reduce memory requirements when using conventional bacterial assembly algorithms.
here we introduce neatfreq, a software tool that reduces a data set to more uniform coverage by clustering and selecting from reads binned by their median kmer frequency  and uniqueness.
new lab techniques such as multiple displacement amplification  of single cells and sequence independent single primer amplification  allow for sequencing of organisms that cannot be cultured, but generate highly variable coverage due to amplification biases.
