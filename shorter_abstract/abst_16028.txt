these data need to be pre-processed, analyzed and integrated with existing knowledge through the use of diverse sets of software tools, models and databases.
we have developed a generic pipeline system called cyrille <dig>  the system is modular in design and consists of three functionally distinct parts: 1) a web based, graphical user interface  that enables a pipeline operator to manage the system; 2) the scheduler, which forms the functional core of the system and which tracks what data enters the system and determines what jobs must be scheduled for execution, and; 3) the executor, which searches for scheduled jobs and executes these on a compute cluster.
given the volume of the data used and the multitude of computational resources available, specialized pipeline software is required to make high-throughput analysis of large-scale omics datasets feasible.
cyrille <dig> enables easy creation and execution of high throughput, flexible bioinformatics pipelines.
