in this paper, we ask three research questions:  how can redundancy be quantified in large-scale text corpora?
conventional wisdom is that larger corpora yield better results in text mining.
we analyze a large-scale ehr corpus and quantify redundancy both in terms of word and semantic concept repetition.
afor text mining, preprocessing the ehr corpus with fingerprinting yields significantly better results.
fingerprinting enables text-mining techniques to leverage available data in the ehr corpus, while avoiding the bias introduced by redundancy.
finally, we compare two mitigation strategies to avoid redundancy-induced bias:  a baseline strategy, keeping only the last note for each patient in the corpus;  removing redundant notes with an efficient fingerprinting-based algorithm.
we compare the results of these methods on synthetic data with controlled levels of redundancy and observe significant performance variation.
we focus on copy-and-paste redundancy: clinicians typically copy and paste information from previous notes when documenting a current patient encounter.
ehr corpora, however, exhibit specific statistical and linguistic characteristics when compared with corpora in the biomedical literature domain.
or does the redundancy introduce benefits by highlighting stable and important subsets of the corpus?
we measure the impact of redundancy on two standard text-mining applications: collocation identification and topic modeling.
thus, within a longitudinal patient record, one expects to observe heavy redundancy.
