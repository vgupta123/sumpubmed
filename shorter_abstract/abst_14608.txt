according to the shannon fundamental theorem, mutual information plays a crucial role in characterizing the efficiency of communication channels.
it is well known that this efficiency is determined by the channel capacity that is already the maximal mutual information between input and output signals.
a natural question arises about the relation between mutual information and correlation.
we present binary communication channels for which mutual information and correlation coefficients behave differently both quantitatively and qualitatively.
our research shows that the mutual information cannot be replaced by sheer correlations.
our results indicate that neuronal encoding has more complicated nature which cannot be captured by straightforward correlations between input and output signals once the mutual information takes into account the structure and patterns of the signals.
shannon informationcommunication channelentropymutual informationcorrelationneuronal encodingissue-copyright-statementÂ© the author 2015
on the other hand, intuitively speaking, when input and output signals are more correlated, the transmission should be more efficient.
