probabilistic fingerprinting may significantly reduce the use of computational resources when comparing very large files.
common usage patterns, such as comparing and transferring files, are proving computationally expensive and are tying down shared resources.
this primarily concerns logistics within and between data centers, but is also important for workstation users in the analysis phase.
consequently, it has a flat performance characteristic, correlated with data variation rather than file size.
biological data acquisition is raising new challenges, both in data analysis and handling.
not only is it proving hard to analyze the data at the rate it is generated today, but simply reading and transferring data files can be prohibitively slow due to their size.
we present an efficient method for calculating file uniqueness for large scientific data files, that takes less computational effort than existing techniques.
this method, called probabilistic fast file fingerprinting , exploits the variation present in biological data and computes file fingerprints by sampling randomly from the file instead of reading it in full.
the implementation of the algorithm is available as an open-source tool named pfff, as a command-line tool as well as a c library.
