we did this by careful assessment of interannotator agreement and the use of answer pooling of participant results to improve the quality of the final testing dataset.
for the training dataset, the gene list was pruned automatically to remove gene names not found in the abstract; for the testing dataset, it was further refined by manual annotation by annotators provided with guidelines.
these consisted of lists of normalized gene names found in the abstracts, generated by adapting the gene list for the full journal articles found in the model organism databases.
we found that there are intrinsic differences between the model organism databases related to the number of synonymous terms and also to curation criteria.
by comparing and pooling answers from the participant systems, we were able to add an additional check on the test data; this allowed us to find additional errors, especially in mouse.
the goal of the assessment was to evaluate the ability of automated systems to generate a list of unique gene identifiers from pubmed abstracts for the three model organisms fly, mouse, and yeast.
march 28â€“ <dig>  <dig> a critical assessment of text mining methods in molecular biology granada, spain
interannotator analysis on a small dataset showed that our gene lists for fly and yeast were good  but the mouse gene list had many conflicts , which resulted in errors .
we found that clear annotation guidelines are important, along with careful interannotator experiments, to validate the generated gene lists.
a critical step in interpreting the results of an assessment is to evaluate the quality of the data preparation.
finally, we found that answer pooling was much faster and allowed us to identify more conflicting genes than interannotator analysis.
also, abstracts alone are a poor resource for identifying genes in paper, containing only a fraction of genes mentioned in the full text .
we prepared and evaluated training and test materials for an assessment of text mining methods in molecular biology.
