the original concept of kl divergence was designed as a measure of distance between two distributions.
kl divergence is defined between two sample sets with and without the test sample.
we address the singularity problem due to small sample size during kl divergence calculation.
our idea was derived from markov blanket algorithm that is a feature selection method based on kl divergence.
data analysis without removing outliers could lead to wrong results and provide misleading information.
the performance of the proposed method is demonstrated on a synthetic data set, two public microarray data sets, and a mass spectrometry data set for liver cancer study.
compared to other algorithms, our proposed method shows better or comparable performance for small sample and high-dimensional biological data.
that is, while markov blanket algorithm removes redundant and irrelevant features, our proposed method detects outliers.
kernel functions are applied to avoid direct use of mapping functions.
we propose a new outlier detection method based on kullback-leibler  divergence.
to handle the non-linearity of sample distribution, original data is mapped into a higher feature space.
comparative studies with mahalanobis distance based method and one-class support vector machine  are reported showing that the proposed method performs better in finding outliers.
