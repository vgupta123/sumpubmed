using flux constraints, a large lp problem can be massively divided and parallelized into independent sub-jobs for deployment into computing clusters.
even for moderately-sized models , existing approaches based on the double description method must iterate through a large number of combinatorial candidates, thus imposing an immense processor and memory demand.
the speed of the algorithm was comparable to efmtool, a mainstream double description method, when enumerating all efms; the attrition power gained from performing flux feasibility tests offsets the increased computational demand of running an lp solver.
unlike the double description method, the algorithm enables accelerated enumeration of all efms satisfying a set of constraints.
based on an alternative elementarity test, we developed a depth-first search algorithm using linear programming  to enumerate efms in an exhaustive fashion.
the depth-first search algorithm has a constant memory overhead.
generating a complete set of efms for large-scale models, however, is near impossible.
the decomposition of complex metabolic networks into elementary flux modes  provides a useful framework for exploring reaction interactions systematically.
