in this study we use the datasets made available for training and testing automated icd-9-cm coding systems by the organisers of an international challenge on classifying clinical free text using natural language processing in spring  <dig>  the challenge itself was dominated by entirely or partly rule-based systems that solve the coding task using a set of hand crafted expert rules.
since this labeling task requires expert knowledge in the field of medicine, the process itself is costly and is prone to errors as human annotators have to consider thousands of possible codes when assigning the right icd-9-cm labels to a document.
icd-9-cm codes are used for billing purposes by health institutes and are assigned to clinical records manually following clinical treatment.
our results are very promising in the sense that we managed to achieve comparable results with purely hand-crafted icd-9-cm classifiers.
this result would have placed second in the challenge, with a hand-crafted system achieving slightly better results.
our results demonstrate that hand-crafted systems – which proved to be successful in icd-9-cm coding – can be reproduced by replacing several laborious steps in their construction with machine learning models.
our best model got a  <dig> % f measure on the training dataset and an  <dig> % f measure on the challenge test dataset, using the micro-averaged fβ= <dig> measure, the official evaluation metric of the international challenge on classifying clinical free text using natural language processing.
in this paper we focus on the problem of automatically constructing icd-9-cm coding systems for radiology reports.
since the feasibility of the construction of such systems for thousands of icd codes is indeed questionable, we decided to examine the problem of automatically constructing similar rule sets that turned out to achieve a remarkable accuracy in the shared task challenge.
