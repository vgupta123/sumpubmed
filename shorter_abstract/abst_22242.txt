the golden spike data set has been used to validate a number of methods for summarizing affymetrix data sets, sometimes with seemingly contradictory results.
we outline six stages in the analysis pipeline where decisions need to be made, and show how the results of these decisions can lead to the apparently contradictory results previously found.
it has been suggested that this data set should not be used for method comparison due to a number of inherent flaws.
we also show that, while flawed, this data set is still a useful tool for method comparison, particularly for identifying combinations of summarization and differential expression methods that are unlikely to perform well on real data sets.
we conclude with recommendations for preferred affymetrix analysis tools, and for the development of future spike-in data sets.
