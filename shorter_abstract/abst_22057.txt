reducing the effects of sequencing errors and pcr artifacts has emerged as an essential component in amplicon-based metagenomic studies.
because of its efficiency, flowclus can be used to re-analyze multiple large datasets together, thereby leading to more standardized conclusions.
denoising algorithms have been designed that can reduce error rates in mock community data, but they change the sequence data in a manner that can be inconsistent with the process of removing errors in studies of real communities.
among its other attributes, flowclus can analyze longer reads being generated from all stages of  <dig> sequencing technology, as well as from ion torrent.
it has processed a large dataset of  <dig>  million gs-flx titanium reads in twelve hours; using its more efficient  trie analysis option, this time was further reduced, to seven minutes.
flowclus uses a systematic approach to filter and denoise reads efficiently.
when used to analyze a mock community dataset, flowclus produced a lower error rate compared to other denoising algorithms, while retaining significantly more sequence information.
when denoising real datasets, flowclus provides feedback about the process that can be used as the basis to adjust the parameters of the algorithm to suit the particular dataset.
