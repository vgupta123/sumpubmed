this study examines the performance of the nearest centroid classifier coupled with the following feature selection algorithms.
comprehensive experiments, conducted on five popular cancer data sets, revealed that the less advocated sequential forward selection and boosted feature selection algorithms produce the most consistent results across all data sets.
this study tested a number of popular feature selection methods using the nearest centroid classifier and found that several reportedly state-of-the-art algorithms in fact perform rather poorly when tested via stratified cross-validation.
the use of mass spectrometry as a proteomics tool is poised to revolutionize early disease diagnosis and biomarker identification.
the revealed inconsistencies provide clear evidence that algorithm evaluation should be performed on several data sets using a consistent  cross-validation procedure in order for the conclusions to be statistically sound.
in addition, we tested several dimensionality reduction approaches, namely principal component analysis and principal component analysis coupled with linear discriminant analysis.
in contrast, the state-of-the-art performance reported on isolated data sets for several of the studied algorithms, does not hold across all data sets.
this paper examines feature selection techniques for proteomic mass spectrometry.
due to the sheer amount of information contained within the mass spectra, most standard machine learning techniques cannot be directly applied.
to fairly assess each algorithm, evaluation was done using stratified cross validation with an internal leave-one-out cross-validation loop for automated feature selection.
instead, feature selection techniques are used to first reduce the dimensionality of the input space and thus enable the subsequent use of classification algorithms.
student-t test, kolmogorov-smirnov test, and the p-test are univariate statistics used for filter-based feature ranking.
from the wrapper approaches we tested sequential forward selection and a modified version of sequential backward selection.
unfortunately, before standard supervised classification algorithms can be employed, the "curse of dimensionality" needs to be solved.
