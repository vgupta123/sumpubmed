two common approaches to determine prediction performance are cross-validation, in which all available data are iteratively split into training and testing data, and the use of blind sets generated separately from the data used to construct the predictive method.
in the present study, we have compared cross-validated prediction performances generated on our last benchmark dataset from  <dig> with prediction performances generated on data subsequently added to the immune epitope database  which served as a blind set.
we found that cross-validated performances systematically overestimated performance on the blind set.
it has long been known that cross-validated prediction performance estimates often overestimate performance on independently generated blind set data.
this was found not to be due to the presence of similar peptides in the cross-validation dataset.
rather, we found that small size and low sequence/affinity diversity of either training or blind datasets were associated with large differences in cross-validated vs. blind prediction performances.
an increasing number of peptides for which mhc binding affinities are measured experimentally have been selected based on binding predictions and thus are less diverse than historic datasets sampling the entire sequence and affinity space, making them more difficult benchmark data sets.
