BACKGROUND
explaining how the brain processing is so fast remains an open problem . thus, the analysis of neural transmission  processes basically focuses on searching for effective encoding and decoding schemes. according to the shannon fundamental theorem, mutual information plays a crucial role in characterizing the efficiency of communication channels. it is well known that this efficiency is determined by the channel capacity that is already the maximal mutual information between input and output signals. on the other hand, intuitively speaking, when input and output signals are more correlated, the transmission should be more efficient. a natural question arises about the relation between mutual information and correlation. we analyze the relation between these quantities using the binary representation of signals, which is the most common approach taken in studying neuronal processes of the brain.


RESULTS
we present binary communication channels for which mutual information and correlation coefficients behave differently both quantitatively and qualitatively. despite this difference in behavior, we show that the noncorrelation of binary signals implies their independence, in contrast to the case for general types of signals.


CONCLUSIONS
our research shows that the mutual information cannot be replaced by sheer correlations. our results indicate that neuronal encoding has more complicated nature which cannot be captured by straightforward correlations between input and output signals once the mutual information takes into account the structure and patterns of the signals.

keywords
shannon informationcommunication channelentropymutual informationcorrelationneuronal encodingissue-copyright-statementÂ© the author 2015

