BACKGROUND
oligonucleotide arrays have become one of the most widely used high-throughput tools in biology. due to their sensitivity to experimental conditions, normalization is a crucial step when comparing measurements from these arrays. normalization is, however, far from a solved problem. frequently, we encounter datasets with significant technical effects that currently available methods are not able to correct.


RESULTS
we show that by a careful decomposition of probe specific amplification, hybridization and array location effects, a normalization can be performed that allows for a much improved analysis of these data. identification of the technical sources of variation between arrays has allowed us to build statistical models that are used to estimate how the signal of individual probes is affected, based on their properties. this enables a model-based normalization that is probe-specific, in contrast with the signal intensity distribution normalization performed by many current methods. next to this, we propose a novel way of handling background correction, enabling the use of background information to weight probes during summarization. testing of the proposed method shows a much improved detection of differentially expressed genes over earlier proposed methods, even when tested on  spike-in datasets.


CONCLUSIONS
when a limited number of arrays are available, or when arrays are run in different batches, technical effects have a large influence on the measured expression of genes. we show that a detailed modelling and correction of these technical effects allows for an improved analysis in these situations.

