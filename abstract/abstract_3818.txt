BACKGROUND
this paper introduces the notion of optimizing different norms in the dual problem of support vector machines with multiple kernels. the selection of norms yields different extensions of multiple kernel learning  such as l∞, l <dig>  and l <dig> mkl. in particular, l <dig> mkl is a novel method that leads to non-sparse optimal kernel coefficients, which is different from the sparse kernel coefficients optimized by the existing l∞ mkl method. in real biomedical applications, l <dig> mkl may have more advantages over sparse integration method for thoroughly combining complementary information in heterogeneous data sources.


RESULTS
we provide a theoretical analysis of the relationship between the l <dig> optimization of kernels in the dual problem with the l <dig> coefficient regularization in the primal problem. understanding the dual l <dig> problem grants a unified view on mkl and enables us to extend the l <dig> method to a wide range of machine learning problems. we implement l <dig> mkl for ranking and classification problems and compare its performance with the sparse l∞ and the averaging l <dig> mkl methods. the experiments are carried out on six real biomedical data sets and two large scale uci data sets. l <dig> mkl yields better performance on most of the benchmark data sets. in particular, we propose a novel l <dig> mkl least squares support vector machine  algorithm, which is shown to be an efficient and promising classifier for large scale data sets processing.


CONCLUSIONS
this paper extends the statistical framework of genomic data fusion based on mkl. allowing non-sparse weights on the data sources is an attractive option in settings where we believe most data sources to be relevant to the problem at hand and want to avoid a "winner-takes-all" effect seen in l∞ mkl, which can be detrimental to the performance in prospective studies. the notion of optimizing l <dig> kernels can be straightforwardly extended to ranking, classification, regression, and clustering algorithms. to tackle the computational burden of mkl, this paper proposes several novel lssvm based mkl algorithms. systematic comparison on real data sets shows that lssvm mkl has comparable performance as the conventional svm mkl algorithms. moreover, large scale numerical experiments indicate that when cast as semi-infinite programming, lssvm mkl can be solved more efficiently than svm mkl.

availability
the matlab code of algorithms implemented in this paper is downloadable from http://homes.esat.kuleuven.be/~sistawww/bioi/syu/l2lssvm.html.

