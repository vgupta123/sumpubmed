BACKGROUND
genotypes generated in next generation sequencing studies contain errors which can significantly impact the power to detect signals in common and rare variant association tests. these genotyping errors are not explicitly filtered by the standard gatk variant quality score recalibration  tool and thus remain a source of errors in whole exome sequencing  projects that follow gatkâ€™s recommended best practices. therefore, additional data filtering methods are required to effectively remove these errors before performing association analyses with complex phenotypes. here we empirically derive thresholds for genotype and variant filters that, when used in conjunction with the vqsr tool, achieve higher data quality than when using vqsr alone.


RESULTS
the detailed filtering strategies improve the concordance of sequenced genotypes with array genotypes from  <dig> % to  <dig> %; improve the percent of discordant genotypes removed from  <dig> % to  <dig> %; and improve the ti/tv ratio from  <dig>  to  <dig> . we also demonstrate that managing batch effects by separating samples based on different target capture and sequencing chemistry protocols results in a final data set containing  <dig> % more high-quality variants. in addition, imputation is an important component of wes studies and is used to estimate common variant genotypes to generate additional markers for association analyses. as such, we demonstrate filtering methods for imputed data that improve genotype concordance from  <dig> % to  <dig> % while removing  <dig> % of discordant genotypes.


CONCLUSIONS
the described filtering methods are advantageous for large population-based wes studies designed to identify common and rare variation associated with complex diseases. compared to data processed through standard practices, these strategies result in substantially higher quality data for common and rare association analyses.

next generation sequencingsingle nucleotide variantsgenotypingimputationgenomics

