BACKGROUND
the information theoretic concept of mutual information provides a general framework to evaluate dependencies between variables. in the context of the clustering of genes with similar patterns of expression it has been suggested as a general quantity of similarity to extend commonly used linear measures. since mutual information is defined in terms of discrete variables, its application to continuous data requires the use of binning procedures, which can lead to significant numerical errors for datasets of small or moderate size.


RESULTS
in this work, we propose a method for the numerical estimation of mutual information from continuous data. we investigate the characteristic properties arising from the application of our algorithm and show that our approach outperforms commonly used algorithms: the significance, as a measure of the power of distinction from random correlation, is significantly increased. this concept is subsequently illustrated on two large-scale gene expression datasets and the results are compared to those obtained using other similarity measures.

a c++ source code of our algorithm is available for non-commercial use from kloska@scienion.de upon request.


CONCLUSIONS
the utilisation of mutual information as similarity measure enables the detection of non-linear correlations in gene expression datasets. frequently applied linear correlation measures, which are often used on an ad-hoc basis without further justification, are thereby extended.

