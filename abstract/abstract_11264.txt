BACKGROUND
graphical gaussian models are popular tools for the estimation of  gene association networks from microarray data. a key issue when the number of variables greatly exceeds the number of samples is the estimation of the matrix of partial correlations. since the  inverse of the sample covariance matrix leads to poor estimates in this scenario, standard methods are inappropriate and adequate regularization techniques are needed. popular approaches include biased estimates of the covariance matrix and high-dimensional regression schemes, such as the lasso and partial least squares.


RESULTS
in this article, we investigate a general framework for combining regularized regression methods with the estimation of graphical gaussian models. this framework includes various existing methods as well as two new approaches based on ridge regression and adaptive lasso, respectively. these methods are extensively compared both qualitatively and quantitatively within a simulation study and through an application to six diverse real data sets. in addition, all proposed algorithms are implemented in the r package "parcor", available from the r repository cran.


CONCLUSIONS
in our simulation studies, the investigated non-sparse regression methods, i.e. ridge regression and partial least squares, exhibit rather conservative behavior when combined with  false discovery rate multiple testing in order to decide whether or not an edge is present in the network. for networks with higher densities, the difference in performance of the methods decreases. for sparse networks, we confirm the lasso's well known tendency towards selecting too many edges, whereas the two-stage adaptive lasso is an interesting alternative that provides sparser solutions. in our simulations, both sparse and non-sparse methods are able to reconstruct networks with cluster structures. on six real data sets, we also clearly distinguish the results obtained using the non-sparse methods and those obtained using the sparse methods where specification of the regularization parameter automatically means model selection. in five out of six data sets, partial least squares selects very dense networks. furthermore, for data that violate the assumption of uncorrelated observations , the lasso and the adaptive lasso yield very complex structures, indicating that they might not be suited under these conditions. the shrinkage approach is more stable than the regression based approaches when using subsampling.

