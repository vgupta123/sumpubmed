BACKGROUND
vision-based surveillance and monitoring is a potential alternative for early detection of respiratory disease outbreaks in urban areas complementing molecular diagnostics and hospital and doctor visit-based alert systems. visible actions representing typical flu-like symptoms include sneeze and cough that are associated with changing patterns of hand to head distances, among others. the technical difficulties lie in the high complexity and large variation of those actions as well as numerous similar background actions such as scratching head, cell phone use, eating, drinking and so on.


RESULTS
in this paper, we make a first attempt at the challenging problem of recognizing flu-like symptoms from videos. since there was no related dataset available, we created a new public health dataset for action recognition that includes two major flu-like symptom related actions  and a number of background actions. we also developed a suitable novel algorithm by introducing two types of action matching kernels, where both types aim to integrate two aspects of local features, namely the space-time layout and the bag-of-words representations. in particular, we show that the pyramid match kernel and spatial pyramid matching are both special cases of our proposed kernels. besides experimenting on standard testbed, the proposed algorithm is evaluated also on the new sneeze and cough set. empirically, we observe that our approach achieves competitive performance compared to the state-of-the-arts, while recognition on the new public health dataset is shown to be a non-trivial task even with simple single person unobstructed view.


CONCLUSIONS
our sneeze and cough video dataset and newly developed action recognition algorithm is the first of its kind and aims to kick-start the field of action recognition of flu-like symptoms from videos. it will be challenging but necessary in future developments to consider more complex real-life scenario of detecting these actions simultaneously from multiple persons in possibly crowded environments.

issue-copyright-statementÂ© the author 2014

