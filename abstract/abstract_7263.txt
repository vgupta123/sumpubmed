BACKGROUND
protein fold recognition is a key step in protein three-dimensional  structure discovery. there are multiple fold discriminatory data sources which use physicochemical and structural properties as well as further data sources derived from local sequence alignments. this raises the issue of finding the most efficient method for combining these different informative data sources and exploring their relative significance for protein fold classification. kernel methods have been extensively used for biological data analysis. they can incorporate separate fold discriminatory features into kernel matrices which encode the similarity between samples in their respective data sources.


RESULTS
in this paper we consider the problem of integrating multiple data sources using a kernel-based approach. we propose a novel information-theoretic approach based on a kullback-leibler  divergence between the output kernel matrix and the input kernel matrix so as to integrate heterogeneous data sources. one of the most appealing properties of this approach is that it can easily cope with multi-class classification and multi-task learning by an appropriate choice of the output kernel matrix. based on the position of the output and input kernel matrices in the kl-divergence objective, there are two formulations which we respectively refer to as mkldiv-dc and mkldiv-conv. we propose to efficiently solve mkldiv-dc by a difference of convex  programming method and mkldiv-conv by a projected gradient descent algorithm. the effectiveness of the proposed approaches is evaluated on a benchmark dataset for protein fold recognition and a yeast protein function prediction problem.


CONCLUSIONS
our proposed methods mkldiv-dc and mkldiv-conv are able to achieve state-of-the-art performance on the scop pdb-40d benchmark dataset for protein fold prediction and provide useful insights into the relative significance of informative data sources. in particular, mkldiv-dc further improves the fold discrimination accuracy to  <dig> % which is a more than 5% improvement over competitive bayesian probabilistic and svm margin-based kernel learning methods. furthermore, we report a competitive performance on the yeast protein function prediction problem.

