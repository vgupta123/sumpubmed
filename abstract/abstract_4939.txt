BACKGROUND
classification using class-imbalanced data is biased in favor of the majority class. the bias is even larger for high-dimensional data, where the number of variables greatly exceeds the number of samples. the problem can be attenuated by undersampling or oversampling, which produce class-balanced data. generally undersampling is helpful, while random oversampling is not. synthetic minority oversampling technique  is a very popular oversampling method that was proposed to improve random oversampling but its behavior on high-dimensional data has not been thoroughly investigated. in this paper we investigate the properties of smote from a theoretical and empirical point of view, using simulated and real high-dimensional data.


RESULTS
while in most cases smote seems beneficial with low-dimensional data, it does not attenuate the bias towards the classification in the majority class for most classifiers when data are high-dimensional, and it is less effective than random undersampling. smote is beneficial for k-nn classifiers for high-dimensional data if the number of variables is reduced performing some type of variable selection; we explain why, otherwise, the k-nn classification is biased towards the minority class. furthermore, we show that on high-dimensional data smote does not change the class-specific mean values while it decreases the data variability and it introduces correlation between samples. we explain how our findings impact the class-prediction for high-dimensional data.


CONCLUSIONS
in practice, in the high-dimensional setting only k-nn classifiers based on the euclidean distance seem to benefit substantially from the use of smote, provided that variable selection is performed before using smote; the benefit is larger if more neighbors are used. smote for k-nn without variable selection should not be used, because it strongly biases the classification towards the minority class.

