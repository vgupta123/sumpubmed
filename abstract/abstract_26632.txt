BACKGROUND
reinforcement learning is a fundamental form of learning that may be formalized using the bellman equation. accordingly an agent determines the state value as the sum of immediate reward and of the discounted value of future states. thus the value of state is determined by agent related attributes  and the agent’s knowledge of the environment embodied by the reward function and hidden environmental factors given by the transition probability. the central objective of reinforcement learning is to solve these two functions outside the agent’s control either using, or not using a model.


RESULTS
in the present paper, using the proactive model of reinforcement learning we offer insight on how the brain creates simplified representations of the environment, and how these representations are organized to support the identification of relevant stimuli and action. furthermore, we identify neurobiological correlates of our model by suggesting that the reward and policy functions, attributes of the bellman equitation, are built by the orbitofrontal cortex  and the anterior cingulate cortex , respectively.


CONCLUSIONS
based on this we propose that the ofc assesses cue-context congruence to activate the most context frame. furthermore given the bidirectional neuroanatomical link between the ofc and model-free structures, we suggest that model-based input is incorporated into the reward prediction error  signal, and conversely rpe signal may be used to update the reward-related information of context frames and the policy underlying action selection in the ofc and acc, respectively. furthermore clinical implications for cognitive behavioral interventions are discussed.

keywords
model-based reinforcement learningproactive brainbellman equationreward functionpolicy functioncue-context congruencehungarian brain research programktia_13_nap-a-v/2zsuga judit issue-copyright-statement© the author 2016

