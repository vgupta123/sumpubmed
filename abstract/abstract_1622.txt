BACKGROUND
to train chunkers in recognizing noun phrases and verb phrases in biomedical text, an annotated corpus is required. the creation of gold standard corpora , however, is expensive and time-consuming. gscs therefore tend to be small and to focus on specific subdomains, which limits their usefulness. we investigated the use of a silver standard corpus  that is automatically generated by combining the outputs of multiple chunking systems. we explored two use scenarios: one in which chunkers are trained on an ssc in a new domain for which a gsc is not available, and one in which chunkers are trained on an available, although small gsc but supplemented with an ssc.


RESULTS
we have tested the two scenarios using three chunkers, lingpipe, opennlp, and yamcha, and two different corpora, genia and pennbioie. for the first scenario, we showed that the systems trained for noun-phrase recognition on the ssc in one domain performed  <dig> - <dig>  percentage points better in terms of f-score than the systems trained on the gsc in another domain, and only  <dig> - <dig>  percentage points less than when they were trained on a gsc in the same domain as the ssc. when the outputs of the chunkers were combined, the combined system showed little improvement when using the ssc. for the second scenario, the systems trained on a gsc supplemented with an ssc performed considerably better than systems that were trained on the gsc alone, especially when the gsc was small. for example, training the chunkers on a gsc consisting of only  <dig> abstracts but supplemented with an ssc yielded similar performance as training them on a gsc of 100- <dig> abstracts. the combined system even performed better than any of the individual chunkers trained on a gsc of  <dig> abstracts.


CONCLUSIONS
we conclude that an ssc can be a viable alternative for or a supplement to a gsc when training chunkers in a biomedical domain. a combined system only shows improvement if the ssc is used to supplement a gsc. whether the approach is applicable to other systems in a natural-language processing pipeline has to be further investigated.

