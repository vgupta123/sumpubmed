BACKGROUND
there has been recent concern regarding the inability of predictive modeling approaches to generalize to new data. some of the problems can be attributed to improper methods for model selection and assessment. here, we have addressed this issue by introducing a novel and general framework, the c1c <dig>  for simultaneous model selection and assessment. the framework relies on a partitioning of the data in order to separate model choice from model assessment in terms of used data. since the number of conceivable models in general is vast, it was also of interest to investigate the employment of two automatic search methods, a genetic algorithm and a brute-force method, for model choice. as a demonstration, the c1c <dig> was applied to simulated and real-world datasets. a penalized linear model was assumed to reasonably approximate the true relation between the dependent and independent variables, thus reducing the model choice problem to a matter of variable selection and choice of penalizing parameter. we also studied the impact of assuming prior knowledge about the number of relevant variables on model choice and generalization error estimates. the results obtained with the c1c <dig> were compared to those obtained by employing repeated k-fold cross-validation for choosing and assessing a model.


RESULTS
the c1c <dig> framework performed well at finding the true model in terms of choosing the correct variable subset and producing reasonable choices for the penalizing parameter, even in situations when the independent variables were highly correlated and when the number of observations was less than the number of variables. the c1c <dig> framework was also found to give accurate estimates of the generalization error. prior information about the number of important independent variables improved the variable subset choice but reduced the accuracy of generalization error estimates. using the genetic algorithm worsened the model choice but not the generalization error estimates, compared to using the brute-force method. the results obtained with repeated k-fold cross-validation were similar to those produced by the c1c <dig> in terms of model choice, however a lower accuracy of the generalization error estimates was observed.


CONCLUSIONS
the c1c <dig> framework was demonstrated to work well for finding the true model within a penalized linear model class and accurately assess its generalization error, even for datasets with many highly correlated independent variables, a low observation-to-variable ratio, and model assumption deviations. a complete separation of the model choice and the model assessment in terms of data used for each task improves the estimates of the generalization error.

