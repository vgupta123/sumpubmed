BACKGROUND
in many cases biomedical data sets contain outliers that make it difficult to achieve reliable knowledge discovery. data analysis without removing outliers could lead to wrong results and provide misleading information.


RESULTS
we propose a new outlier detection method based on kullback-leibler  divergence. the original concept of kl divergence was designed as a measure of distance between two distributions. stemming from that, we extend it to biological sample outlier detection by forming sample sets composed of nearest neighbors. kl divergence is defined between two sample sets with and without the test sample. to handle the non-linearity of sample distribution, original data is mapped into a higher feature space. we address the singularity problem due to small sample size during kl divergence calculation. kernel functions are applied to avoid direct use of mapping functions. the performance of the proposed method is demonstrated on a synthetic data set, two public microarray data sets, and a mass spectrometry data set for liver cancer study. comparative studies with mahalanobis distance based method and one-class support vector machine  are reported showing that the proposed method performs better in finding outliers.


CONCLUSIONS
our idea was derived from markov blanket algorithm that is a feature selection method based on kl divergence. that is, while markov blanket algorithm removes redundant and irrelevant features, our proposed method detects outliers. compared to other algorithms, our proposed method shows better or comparable performance for small sample and high-dimensional biological data. this indicates that the proposed method can be used to detect outliers in biological data sets.

3â€“ <dig> november  <dig> ieee international conference on bioinformatics and biomedicine   <dig> philadelphia, pa, usa

