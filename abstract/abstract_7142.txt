BACKGROUND
supervised learning for classification of cancer employs a set of design examples to learn how to discriminate between tumors. in practice it is crucial to confirm that the classifier is robust with good generalization performance to new examples, or at least that it performs better than random guessing. a suggested alternative is to obtain a confidence interval of the error rate using repeated design and test sets selected from available examples. however, it is known that even in the ideal situation of repeated designs and tests with completely novel samples in each cycle, a small test set size leads to a large bias in the estimate of the true variance between design sets. therefore different methods for small sample performance estimation such as a recently proposed procedure called repeated random sampling  is also expected to result in heavily biased estimates, which in turn translates into biased confidence intervals. here we explore such biases and develop a refined algorithm called repeated independent design and test .


RESULTS
our simulations reveal that repeated designs and tests based on resampling in a fixed bag of samples yield a biased variance estimate. we also demonstrate that it is possible to obtain an improved variance estimate by means of a procedure that explicitly models how this bias depends on the number of samples used for testing. for the special case of repeated designs and tests using new samples for each design and test, we present an exact analytical expression for how the expected value of the bias decreases with the size of the test set.


CONCLUSIONS
we show that via modeling and subsequent reduction of the small sample bias, it is possible to obtain an improved estimate of the variance of classifier performance between design sets. however, the uncertainty of the variance estimate is large in the simulations performed indicating that the method in its present form cannot be directly applied to small data sets.

