BACKGROUND
the speech signal contains both information about phonological features such as place of articulation and non-phonological features such as speaker identity. these are different aspects of the 'what'-processing stream , and here we show that they can be further segregated as they may occur in parallel but within different neural substrates. subjects listened to two different vowels, each spoken by two different speakers. during one block, they were asked to identify a given vowel irrespectively of the speaker , while during the other block the speaker had to be identified irrespectively of the vowel . auditory evoked fields were recorded using 148-channel magnetoencephalography , and magnetic source imaging was obtained for  <dig> subjects.


RESULTS
during phonological categorization, a vowel-dependent difference of n100m source location perpendicular to the main tonotopic gradient replicated previous findings. in speaker categorization, the relative mapping of vowels remained unchanged but sources were shifted towards more posterior and more superior locations.


CONCLUSIONS
these results imply that the n100m reflects the extraction of abstract invariants from the speech signal. this part of the processing is accomplished in auditory areas anterior to ai, which are part of the auditory 'what' system. this network seems to include spatially separable modules for identifying the phonological information and for associating it with a particular speaker that are activated in synchrony but within different regions, suggesting that the 'what' processing can be more adequately modeled by a stream of parallel stages. the relative activation of the parallel processing stages can be modulated by attentional or task demands.

