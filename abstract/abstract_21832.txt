BACKGROUND
cross-validation  is an effective method for estimating the prediction error of a classifier. some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the cv error estimate. we have evaluated the validity of using the cv error estimate of the optimized classifier as an estimate of the true error expected on independent data.


RESULTS
we used cv to optimize the classification parameters for two kinds of classifiers; shrunken centroids and support vector machines . random training datasets were created, with no difference in the distribution of the features between the two classes. using these "null" datasets, we selected classifier parameter values that minimized the cv error estimate. 10-fold cv was used for shrunken centroids while leave-one-out-cv  was used for the svm. independent test data was created to estimate the true error. with "null" and "non null"  data, we also tested a nested cv procedure, where an inner cv loop is used to perform the tuning of the parameters while an outer cv is used to compute an estimate of the error.

the cv error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. even though there is no real difference between the two classes for the "null" datasets, the cv error estimate for the shrunken centroid with the optimal parameters was less than 30% on  <dig> % of simulated training data-sets. for svm with optimal parameters the estimated error rate was less than 30% on 38% of "null" data-sets. performance of the optimized classifiers on the independent test set was no better than chance.

the nested cv procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both shrunken centroids and svm classifiers for "null" and "non-null" data distributions.


CONCLUSIONS
we show that using cv to compute an error estimate for a classifier that has itself been tuned using cv gives a significantly biased estimate of the true error. proper use of cv for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each cv loop. a nested cv procedure provides an almost unbiased estimate of the true error.

