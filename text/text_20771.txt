BACKGROUND
one of the main objectives of microbial ecology is to address how the variations in environmental conditions can shape the composition and structure of prokaryotic communities. for this purpose, it is critical to count on accurate estimates of the composition of the prokaryotic communities, and on a precise description of the environment in study. nowadays, the current knowledge about how the different environmental factors shape the distribution and diversity of prokaryotes is still scarce. although the influence of some of these factors, salinity for instance, has been widely studied and discussed  <cit> , the influence of many others, and especially the combination of different factors, is yet rather unknown.

a very complete ontology, envo, has been developed for the annotation of the environment of any organism or biological sample. there is also a smaller and easy-to-use set of terms preserving a mapping to envo  <cit>  and very suitable for environmental annotation, named habitat-lite  <cit> . these tools are very valuable resources for describing environment types, but an adequate and complete definition of individual environments must be compiled in terms of their physicochemical characteristics, thus allowing the performance of direct comparisons between them. in practice the environmental samplings are often poorly characterized, and there is not a common and established protocol stating the important variables to be measured. a very promising initiative in this direction is being carried out by the genome standards consortium , with the development of miens  <cit> , a specification of the needed information  to characterize environmental samplings. contextual information is defined, according to gsc, as "the set of data describing aspects of genomes and metagenomes such as geographic location and habitat type from which the sample was taken as well as the details of the processing of a sample". gsc also develops gcdml, a genomic contextual data markup language for annotating these data  <cit> .

determining the precise location of the sampling sites is also very important. it allows the linkage between environmental characteristics and geographical locations, useful to explore the existence of possible biogeographical patterns. these patterns are already well known for macro-organisms, but their existence and extent is controversial for micro-organisms  <cit> . also, it facilitates the identification of similar sites to study the spatial or temporal variation of their compositions. for that purpose, it is not enough to specify the name of the location, but also geographical coordinates  of all or most samples must be precisely annotated.

currently, many environmental samplings of prokaryotic composition are already available and deposited in the env section of the genbank database  <cit> . this is a very rich source of data for studying the structure of prokaryotic communities and their relationship with the environment. more than  <dig>  samples containing  <dig> s rdna sequences, the most widely used phylogenetic marker, can be found there. these samples have been recently organized in envdb database  <cit> . usually, the samples in the genbank database are very sparsely described from the environmental point of view. the only sources of environmental information in the entries are the title and a field named "isolation_source", sometimes citing the location and the prominent characteristics of the sample site. nevertheless, almost no environmental information can be found this way and therefore it must be looked for in the corresponding published article. if it exists, genbank lists the corresponding pmid  that can be used to trace the publication describing the experiment and the sampling procedure. then, contextual information can be extracted from it, but the manual inspection of the articles is a very tedious and error-prone process.

a large collection of text mining tools has been developed for many different purposes in the biological field. these tools have been focused mostly in applications for molecular biology and biomedicine, such as annotating genes and proteins, drugs and diseases, inferring the relationships between them and integrating that knowledge into genomic analyses  <cit> . as far as we know, no such tools have been developed for extracting information focused on ecology, although some of the techniques used in other areas can be applicable also in this context. this has been our motivation for developing envmine, a set of automatic text-mining tools devoted to retrieve contextual information from textual sources of any kind, envmine extracts physicochemical variables and geographical locations unambiguously, performing very well on both tasks. therefore it is a useful tool for annotating ecological data and to aid in the development of annotation schemas and specifications.

RESULTS
algorithm
identification of physicochemical variables
in order to avoid confusion, we think it is convenient to define the meaning of some terms used in this section, namely the 'variable', 'unit' and 'measure' terms. the process of measuring involves determining a value for a physicochemical characteristic, such as distance, temperature, or concentration. a variable is an individual physicochemical characteristic, such as distance, temperature or concentration. the result of measuring a variable is a measurement reported in some unit, which is a division of quantity  accepted as a standard. for instance, a meter is a unit for measuring distances. and finally, a measure is the discrete value of a given variable, such as  <dig> .

there are many physicochemical variables that can be used to characterize an environment. some of them are widely used, such as the depth of an oceanic sampling, or the temperature of a thermal spot. some others have been used much less frequently . a comprehensive list of all possible variables does not exist, and this prevents the usage of a dictionary-based approach for their identification. also, very often the variable itself is not cited, because it is implicit in their units. for instance, " <dig> gr of soil were collected" , "a small primary stream drains a  <dig> -ha" , or "the waters were at  <dig> degrees c" . therefore, the system must be capable of identifying both variables and units, and of linking units to the corresponding variables unambiguously.

the first task is to create a list of units as complete as possible. our approach to construct automatically such a list was to scan the articles for numeric values and to recover the words immediately following them, since the units are generally mentioned immediately after a measure . then we crosschecked these words with a comprehensive english dictionary . this allowed us to eliminate all instances referring to substantives . we assumed that a word following a number and not present in the dictionary must correspond to a unit . since units can be composed of more than one word , we collected the longest possible string whose words fulfill that criterion. the list of units collected in this way was manually curated, to obtain a final set of  <dig> different units .

the second step was to effectively identify the link between the units and their corresponding variables, taking also into account that a single unit can be cited in different ways . this can also be done automatically by finding joint citations of variables and units. for instance, in the sentence "samples were collected at  <dig> mts depth", the word "mts" is a unit present in the list, and the citation of the variable "depth" implies that "mts" is a valid unit for such variable. the variable must be in the same sentence and close to the unit  to produce the association. we followed an iterative procedure, starting with an initial list of ten variables  and adding new ones that were identified by the presence of unmatched units. the association list between variables and units obtained in this way was manually curated and completed, to obtain the correspondence of the units with  <dig> different variables.

in this way, it is possible to identify variables in the text even if they were not cited explicitly, simply by the presence of a numerical value  and a unit that was associated with it. some ambiguity can exist, because some units can correspond to more than one variable. for instance, the unit "meter" denotes a linear value, which can be used to measure length, distance or depth. the complete disambiguation of these instances is only possible if the correct variable is also cited.

some variables are expressed as a combination of units. for example, concentrations can be measured as a weight of a substance in a particular volume, which can be expressed by different combinations of units: grams/liter, gr liter- <dig>  nanogram per milliliter, etc. the association list also includes such combinations, specifying that the arrangement of weight and volume units corresponds to a concentration. the variables can be readily identified regardless of the specific units and the notation used.

physicochemical information can also appear in tables or, much more rarely, in figures. the information contained in tables can be extracted, because tables in html format can be parsed. individual rows and columns are clearly identified by html tags. extraction of physicochemical variables from the first row of the table, performed as explained above, indicate the meaning of each of the columns, and individual data points can be extracted from subsequent rows.

identification of geographic locations
the retrieval of geographic locations from texts and their normalization by their geographical coordinates allows the precise determination of the location of each sampling experiment, which is critical for performing biogeographical studies. the simple retrieval of the name of the site is not enough to identify the location exactly, because a very high degree of ambiguity exists . therefore, this task can be seen as two independent issues, first identification of the putative locations, and second matching these with their precise coordinates. some approaches for geo-disambiguation have been already proposed  <cit> 

syntactically, geographic locations are proper nouns, which are capitalized in many languages, including english. this provides a very useful morphological clue to distinguish them from common nouns. but other proper and common nouns are also capitalized . also, the usefulness of the syntactical structure of the sentence is limited, since the geographic location can play different roles in a sentence. for instance, in the sentence "lake cadagno is a meromictic lake in the piora valley of switzerland", "lake cadagno" is the subject of the sentence, whereas "piora valley" and "switzerland" compose an adverbial phrase.

the approach followed to identify possible geographical names is: first, identify all noun phrases  in the sentences by the usage of a part-of-speech tagger   <cit> . then, discard all nps not containing any capitalized words, and finally crosscheck the candidate names with a list of known geographical locations. for this purpose, we have used the geonames database  <cit> , an outstanding compendium of geographic names from all around the world, containing  <dig>  million features plus their geographical coordinates. geonames also offers web services, so that it can be queried by external resources. this is a very effective method for checking whether a particular candidate name corresponds to a real geographic location. the full procedure retrieves around 87% of the original locations. an example of the division of the text in nps and the results of submitting these to geonames is provided in additional file  <dig>  table s <dig> 

this method relies very much on the completeness of geonames database. in order to reduce this dependence on a single data source, we also set up a procedure for performing queries to google maps  <cit>  in a very similar way. google maps also provides web services allowing the retrieval of geographical coordinates for a given query. the usage of google maps slightly improves the results, since the combined procedure, geonames plus google maps, allows the retrieval of 90% of the original instances.

still, a real location cannot be identified if it is not present in any of the geographical databases used. nevertheless, these sites often contain a reference to a geographical feature . we have compiled a list of these features by manual curation of a list of the most common words in geonames database. the list contains  <dig> words referring to geographical features in several languages . then, we are able to identify the capitalized nominal phrases not present in geographical databases but containing any of these words . this procedure allows retrieval of an additional 3% of locations. in this case, since these locations are not present in geographic databases, no coordinates can be found for them. to overcome this probem, we are currently testing a procedure for querying google  <cit>  using the putative geographic name and the keywords "longitude" and "latitude". we estimate that up to 60% of the new locations could be matched with their coordinates in this way.

the second step is the disambiguation of the geographical names when more than one site is possible. this is done by inspecting the contiguous text for other locations that can help to resolve the ambiguity. often, the name of the region, the country, or related geographical features can be found in the vicinity of the ambiguous name. for instance, cities named "margate" exist in south africa, england, australia, the united states and canada, but its citation can be easily disambiguated if it appears as "margate, tasmania". the procedure consists in retrieving the country or countries for all the instances of both locations, and selecting the pair of instances whose countries match. in the example above, since tasmania belongs to australia, the mention of margate is also identified with the australian city. it is not necessary that both locations be in the immediate vicinity , but it is required that both be in the same sentence.

this procedure cannot directly disambiguate between two possible locations in the same country. in that case, the geographical coordinates are used to select the one with smaller distance to the location used to disambiguate. again using the example above, if a second city named margate exists in australia, it would not be possible to select the correct one in the first instance. then we would calculate the distance between the coordinates of the two cities named margate and the coordinates of tasmania, selecting the one with the shortest distance.

also a problem could arise if several countries are mentioned in the same sentence, usually corresponding to lists of locations . in practice this is a minor problem, because rarely a place with the same name is present in more than one country, and therefore there is no ambiguity . otherwise, the procedure assumes that the closest country in the text is the right one , which is sufficient to disambiguate most instances.

although this procedure of disambiguation performs very well, two problems remain: first, it cannot distinguish between locations with no country attached. these names correspond mostly to undersea features , and these features are usually not ambiguous . second, it cannot deal with instances in which no other location is cited in the same sentence. currently, the system works by assigning the location to the most relevant place, for instance the biggest city or the most commonly cited feature. this information is automatically provided by geonames database, where locations are sorted by relevance. an alternative strategy is to look for country names in adjacent sentences, or even in the whole article.

discriminating parts of the article
it is important to consider that most of the contextual information retrieved this way corresponds not to environmental information, but to experimental settings  or providers' addresses . therefore, it would be very valuable to have a way to discriminate the parts of the text that describe environmental characteristics from the rest. the distinction between parts of the article according to their rhetorical contexts has been addressed by previous works, for instance by the rethorical structure theory by mann and thompson  <cit> , or the zone analysis approaches by teufel and moens  <cit> , and mizuta and coworkers  <cit> .

here, a simple approach would be to identify the sections or subsections of the articles that are most likely to contain contextual information. examples are subsections such as "site and sampling characteristics", "sample collection", "site description", "sources of samples", etc. nevertheless, this procedure often fails since experimental data and settings can also be found in these sections, and environmental information can appear elsewhere. therefore, we have designed a simple classification approach for identifying individual sentences  into environmentally or experimentally-associated. we manually classified a set of  <dig>  sentences into environmental  or experimental , and used the frequencies of the words in them to train several types of classifiers available in the weka package  <cit> . we filtered the words using just those that were present in more than one sentence, and whose frequency in one of the sets at least doubled that in the other. in this way, we selected  <dig> useful words that were used to train the classifier, which was tested by means of a jack-knife cross-validation approach.

testing
obtaining full-text articles
we used the set of environmental samplings stored in the envdb database as a reference set from which to extract contextual information. since many of the sampling experiments remain unpublished and are just deposited in the database, only 36% of them can be linked to a pmid and therefore to their corresponding publication. for these, the abstract of the article is readily available, and in this way we obtained  <dig> abstracts. but since contextual data are usually cited in other sections of the article , the abstract is usually not enough and the retrieval of contextual information must be performed on the full-text article.

there are two main problems in accessing full-text articles. the first one is of non-technical nature: the need for a subscription to the corresponding journal. the second is that full-text articles are provided by editorials via their own web services, implying that the structure of the pages and archives can be very different between editorials and providers. it would be necesary to create a web parser capable of tracking the links to the full-texts, which could be technically complex. fortunately, we can take advantage of the fact that all articles are identified by a digital object identifier , provided by the international doi foundation . pubmed uses the field "aid" to store the doi for each article . this doi can be feed to the idf web server in http://dx.doi.org/, which automatically redirects to the full-text article. the queries can be constructed by merging the above address and the doi, such as http://dx.doi.org/ <dig> /j.femsle. <dig> . <dig>  following the example above. in this way, we were able to retrieve  <dig> full-text articles. the article can be downloaded in several formats, but it is advisable to use xml format if available, since all information on sections and subsections of the text is preserved. html format is also suitable. although pdf format is usable by converting it into html or plain text, and some tools are available to perform this step, it is not practical because the format of the text is lost  and therefore the headers identifying sections of the text are missed. also problems can arise derived from the format of the text itself .

sometimes, full details of the experimental procedures are provided in supplementary information. the retrieval and processing of supplementary materials is not easy: they are more difficult to retrieve since they are not identified with a doi; the format is very variable and often they are available only as pdf files. we have not attempted to deal with this issue in the scope of this article.

physicochemical variables
in order to evaluate the performance of the identification of physicochemical information,  <dig> instances of diverse variables were manually annotated from a set of the "materials and methods" sections of  <dig> different articles, which have not been used previously for the development of the extraction patterns. this corpus was generated by just one annotator, and therefore consistency in the annotation is ensured. this corpus is available in our web page http://brueghel.cnb.csic.es/envmine_corpuses/envmine_corpus_units.txt. then we used envmine to collect measures and variables from that set. the results are shown in table  <dig> 

the row "linked to variables" refers to these instances that were linked unambiguously to a given variable. recall is measured as the ratio between the number of correct instances and the total number of instances. precision if the ratio between correct instances and retrieved instances. f-value combines precision and recall as their weighted harmonic mean, calculated as f = /p + r.

the performance of envmine in the retrieval of physicochemical information is very good, achieving almost 97% recall. only a few instances were not retrieved, corresponding mainly to concatenated measures , or involving units that were not present in the original list . in most cases, the retrieved instances corresponded to a real physicochemical measure, achieving a precision of 97%. the most conspicuous error affected to rotational speed measures, because these are often expressed by the "g" unit, which is confounded with "grams", and consequently with a weight measure. we have also quantified how many of these instances could not be linked unambiguously to the real physicochemical variable. this occurs just for  <dig> instances . almost all of these correspond to size measures , which are ambiguous since they can match lengths, diameters, distances, depths, etc. envmine correctly identifies these as "size" measures, but cannot disambiguate further. the errors in the identification of variables are very rare , corresponding to instances such as " <dig> kg of manure ha-1", a measure of concentration that envmine wrongly identifies with a weight, confused by mention of "manure".

next, we applied envmine to the retrieval of physicochemical variables in the set of  <dig> full-text articles. the results indicate that physicochemical variables can be found in all sections of the text, although they most commonly appear in the methods section of the article, where sampling and experimental settings are described .

the most abundant variables found in that set of articles and their most common units are shown in table  <dig> 

the table shows only the variables that appear at least  <dig> times.

some of these variables occur very frequently since they indicate experimental settings. good examples are time, concentration and temperature measures. since we are mostly interested in retrieving information about environmental characteristics, we needed to find a way to discriminate parts of the text in which these could be cited. we have trained different types of classifiers to distinguish between environmental- or experimental-associated sentences, assuming that word usage would be different and some distinctive words would appear in each. the classifiers were generated using the frequencies of words in a set of  <dig>  sentences manually assigned into experimental or environmental categories, and their performance was assessed using a jackknife cross-validation test. the full results for all the tested classifiers are shown in table  <dig> 

the column "original" indicates the original distribution of sentences, "classified" shows the results of the classifier as the obtained number of sentences in each category, and "correct" specifies the number of correctly classified sentences.

the best among all the tested was a multinomial naïve bayes classifier, which has an excellent performance, especially for discriminating experimental-associated sentences. these results indicate that it is possible to distinguish accurately between parts of the article.

geographic locations
for testing the performance of the procedure, we manually annotated the locations found in two different sets, consisting of  <dig> full-text articles and  <dig> abstracts, respectively. as above, the corpus was generated by just one annotator, and it is available in the web address http://brueghel.cnb.csic.es/envmine_corpuses/envmine_corpus_locations.txt. we used envmine to retrieve geographical locations in these sets. the results are shown in table  <dig> 

precision is measured either considering the retrieval of just the name of the place , or also the correct coordinates .

the main difference between both datasets is that in the full-text of articles many of the locations correspond to manufacturers' addresses , which are easy to disambiguate since the state or country is commonly cited. since this could artificially increase the precision, we performed a second test using only abstracts of articles. in both corpuses, recall and precision are rather high. only a few locations were missed, and there are few errors, mostly consisting in the confusion of persons' names with locations. in the abstracts, the precision with coordinates is smaller because some locations were not present in the geographical databases, and were found according to the citation of a geographic feature . these instances were accurately retrieved , but it is not possible to obtain their coordinates.

the information on the geographic origin of the samples can be used for performing biogeographic studies such as the one shown in figure  <dig>  it shows the distribution of the  <dig> samples containing a single species belonging to the archaeal thermoprotei class, also from envdb database. a phylogenetic analysis of the sequences has been performed, and the phylogenetic distances between individual sequences can be correlated with the spatial distances between the samples containing these sequences. in this case, a moderate correlation is obtained , indicating that the barriers restricting the dispersion of this particular species are weak.

discussion and 
CONCLUSIONS
we have developed envmine with the main purpose of extracting contextual information from scientific articles, especially those linked to sampling experiments of different environments. we have shown that the retrieval of contextual information can be performed with high effectiveness and a low rate of errors, and therefore we conclude that text mining tools like this can be very valuable in the annotation of environmental characteristics and locations. nevertheless, we have found that such contextual information is not abundant. very often, the samples are very sparsely described from the physicochemical point of view. only very few variables are characterized, and there is no common criteria about which ones are to be measured. obviously, this makes it difficult to study the influence of physicochemical factors by the comparison of results from different samples. it is clear for us that a common protocol for the collection of contextual data is needed. in this context, we find that the migs/miens standard, the initiative of the genomic standards consortium to propose a minimum set of variables to be measured in each sampling, is extremely important  <cit> . envmine can help to annotate these variables from textual sources, reducing greatly the effort to produce complete descriptions of environmental samplings.

envmine is composed of two separated modules, devoted to the retrieval of physicochemical variables and geographical locations, respectively. both modules can be combined to create a catalog of environmental features in different locations of the world, which can be a valuable resource for the performance of a wide range of ecological studies and in some instances could be used to annotate poorly characterized samples.

envmine could also be easily combined with other text-mining tools. for instance, text detective  <cit>  can be used to precisely identify chemical compounds, which combined with concentration measures would provide a precise chemical profile of the environment. recently, spasic and colleagues described a method for constructing a controlled vocabulary in order to facilitate the retrieval of experimental techniques linked to metabolomic studies  <cit> . also, frantzi and colleagues developed termine, a text-mining system allowing the extraction of multi-word terms that can be used to build controlled terminology lists  <cit> . the combination of these tools can lead to a complete system for the extraction of experimental methodologies and protocols. this can allow the comparison of different results according to variation in the settings used in each experiment. for instance, the composition of the human intestinal microbiome is found to be rather diverse, and perhaps some apparently discordant results could be attributed to the usage of diverse experimental procedures that introduce particular biases, leading to different conclusions  <cit> . envmine can be used to complement the annotation of environments by envo or habitat-lite.

envmine is part of an effort to create a comprehensive database linking prokaryotic diversity and environmental features  <cit> . we hope that this resource can help to increase our knowledge of the assembly of natural microbial communities and their relationships with the environment.

envmine web server can be accessed at http://brueghel.cnb.csic.es/envmine.html

authors' contributions
jt and vdl conceived the study. jt designed and implemented the methods, and analyzed the results. both authors drafted the manuscript.

supplementary material
additional file 1
table s <dig>  curated list of measurement units, their frequency in the original set of articles, and their association with physicochemical variables.

click here for file

 additional file 2
table s <dig>  example of the nps retrieved from the text of articles, and the results obtained when sending them to geonames database. nps are shown between brackets, and these that retrieve results from geonames are coloured in red. the results are shown in tables below the text, including the position in the text, the type of feature and the location , with its precise geographical coordinates.

click here for file

 additional file 3
table s <dig>  number of instances in geonames database grouped by their feature type, indicating the number and percentage of ambiguous and unambiguous cases, and also the percentage of ambiguities with respect to the total. a feature is ambiguous when more than one location with identical name is present.

click here for file

 acknowledgements
this work was supported by project generous grants from the spanish ministry of science and innovation , the  <dig> th framework programme of the european union , and funds from comunidad de madrid, spain. the funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript
