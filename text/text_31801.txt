BACKGROUND
overview: compressive sensing
consider measurements y∈rm of a signal q∈rn: 
  y=Ωq 

where Ω∈rm×n is called the sensing matrix.

one key question  <cit>  is how many measurements m are needed to exactly recover the original signal q from Ω: 
if m>n and Ω is a full rank matrix, then the problem is overdetermined. if m=n and Ω is a full rank matrix, the problem is determined and may be solved uniquely for q.

if m<n, the problem is underdetermined even if Ω has full rank. we can restrict q∈rn to the subspace which satisfies y=Ωq. however, q cannot be determined uniquely.



for the underdetermined case, the least squares solution q∗=argminqq∥l2=Ω∗−1y is typically used as the “best guess” in many applications. however, if q is known to be sparse, meaning that many of its components are zero, one might expect that fewer than n measurements are needed to recover it. it is thus of interest to obtain a good estimator for underdetermined problems in which q is assumed to be s-sparse, meaning that s of the elements of q are nonzero. in principle, the theory of compressive sensing  asserts that the number of measurements needed to recover q∈rn is a number proportional to the compressed size of the signal , rather than the uncompressed size   <cit> . to be able to recover q, cs relies on two properties: sparsity, which pertains to the signals of interest, and incoherence, which pertains to the sensing matrix.

proposition <dig> 
 <cit> . suppose that any 2s columns of an m×n matrix Ω are linearly independent . then, any s-sparse signal q∈rn can be reconstructed uniquely from Ωq.

the proof  <cit>  of the above proposition also shows how to reconstruct an s-sparse signal q∈rn from the measurement y=Ωq where q is the unique sparsest solution to y=Ωq: 
  q∗=argminqql0subject toy=Ωq 

and ∥q∥l0:=∑i=1ni is the cardinality of q. however, the l0-minimization is computationally intractable . recent breakthroughs enable approximating l0-optimization by using l1-minimization which is a convex optimization problem and can be solved in a simple but effective way by linear programming: 
  q∗=argminqql1subject toy=Ωq 

the l1-minimization in equation  requires mild oversampling, more specifically, m≥c·μ2·s logn for some positive constant c where Ω can be decomposed as a product of a sparsity basis Ψ, and an orthogonal measurement system Φ  <cit> . we have m measurement in the Φ domain  under bases Ψ  and μ represents the coherence defined as follows:

definition <dig> 
 <cit> . the coherence of a matrix Ω is given by 
 μ=maxj<k|〈Ωj,Ωk〉|Ωj2Ωk <dig> 

where Ωj and Ωk denote columns of Ω. in plain english, the coherence measures that the largest correlation between any two columns in Ω.

several theoretical results  <cit>  ensure that the l1-minimization guarantees exact recovery whenever the sensing matrix Ω is sufficiently incoherent. for example, we can say that Ω is incoherent if μ is small. coherence is a key property in the compressed sensing framework because if two columns are closely correlated, it would be very difficult to distinguish whether the influence from the components in the sparse signal comes from one or the other  is a linear combination of each columns of the sensing matrix Ω with the components in q as coefficients). also, numerical experiments suggest that in practice, most s-sparse signals are in fact recovered exactly once m≥4s  <cit> . here, “exact recovery” means that we find the sparsest solution  such that y=Ωq.

therefore, if the sensing matrix Ω satisfies the incoherence condition 2·s logn, or in practice, m≥4s), a sufficiently sparse signal q can be exactly recovered from the limited dataset without any prior knowledge of the number of nonzero elements, their locations, and their values. on the other hand, if the condition is not satisfied, exact recovery cannot be guaranteed  <cit> . however, it is possible to use the property of coherence to guide biological experiment design, basically to collect a more informative dataset. as we will discuss in this paper, this can be done by inhibiting or stimulating certain genes to manipulate the gene expression.

cs can help reconstruct grns
in graph theory, a digraph can be represented by g= where v and e represent nodes and edges respectively. for grns, each node represents a gene and each edge represents an influence map which models how genes affect each other. for example, the interactions could be how genes inhibit or stimulate each other. since the connectivities of grns are typically unknown, often the best we can do is to select a set of possible candidate functions encoding possible unknown connectivities between genes.

in this section, we formulate a data-driven network identification problem into cs framework: first, we define a dynamical model of gene regulatory network. then, we encode system dynamics into the sensing matrix  and denote unknown connectivities between genes by q, a signal to be recovered.

a dynamical model of gene regulatory networks 
we consider a dynamical system described by: 
  x˙=f+u+g+w 

where x∈rn denotes the concentrations of the rate-limiting species which can be measured in experiments; x˙=x˙1x˙2…x˙n⊤∈rn is a vector whose elements are the change in concentrations of the n species over time; f:rn→rn represents biochemical reactions, such as those governed by mass action kinetics, michaelis-menten, or hill kinetics. thus, f can include functions of known form such as product of monomials, monotonically increasing or decreasing hill functions, simple linear terms, and constant terms  <cit> . u∈rn denotes the control input which could represent inhibitions and stimulations; for example, if we consider protein-protein interaction, u represents drug-induced perturbation such as inhibition and stimulation. for a grn, we consider the use of sirna as a control input to the system for gene knockdown; g:rnh→rn represents influence from hidden nodes xh∈rnh, which cannot be measured in experiments; nh is the number of hidden nodes and unknown; and w∈rn represents energy-bounded process noise or measurement noise. here x, x˙ and u is assumed to be known where x can be measured in experiments and x˙ may not be measured directly in experiments but we could calculate these quantities by interpolating x and using numerical derivatives. in  <cit> , the authors pointed out that although data on time derivative can be difficult to obtain especially in the presence of noise, it is possible to estimate the gene expressions relatively accurately by repeating measurement with careful instrumentation and statistics  <cit> .

since we do not know whether important nodes in the gene regulatory network are missing, how many missing nodes there are, and how they affect system dynamics ), we denote a vector of  influence from hidden nodes’ dynamics by v); also, without loss of generality, since we know u, we define y as follows: 
  y≜x˙−u=f+v+w 

formulating a dynamical system as a grn
the nonlinear function f can be decomposed into a linear sum of scalar basis functions, fb,i∈r, where we select the set of possible candidate basis functions: 
   

where n is the number of possible candidate basis functions and qij are unknown parameters which reflect underlying structure, i.e., influence of fb,i on the j-th state . typically, we may choose a larger set than necessary, and allow the cs method to indicate the importance of each function, as we shall describe. thus the equation  can be written as follows: 
  y=sqfb+v+w 

where sq=q11…qn1………q1n…qnn=q1⊤…qn⊤∈rn×n reflects the  underlying grn structure, qi=q1iq2i…qni⊤∈rn and fb=fb, <dig> …,fb,n⊤∈rn is the vector field which includes possible candidate basis functions. in this way, any biochemical reactions can be represented by a linear map sq and a function fb where sq encodes the underlying graph structure and fb includes all possible candidate functions that could be included in the biochemical reactions.

in practice, we can construct fb by selecting the most commonly used candidate basis functions to model grns or protein-protein interaction, for example, all monomials, binomials, other combinations and hill function.

example <dig> 
consider the simple nonlinear ordinary differential equations : 
  y1=x˙1=γ1x1+k1x2nact1+x2nacty2=x˙2=γ2x2+k211+x3nihby3=x˙3=γ3x3+k311+x1nihb 

where xi denotes the concentration of the i-th species, x˙i is the change in concentration of the i-th species, the γi denotes protein decay rate, the ki denotes the maximum promoter/inhibitor strength. here, there is no input , no hidden node  and no process noise . also, nact represents positively cooperative binding  and nihb represents negative cooperative binding . the set of odes corresponds to a topology where gene  <dig> is activated by gene  <dig>  gene  <dig> is inhibited by gene  <dig>  and gene  <dig> is inhibited by gene  <dig> as shown in figure  <dig>  we can write equation  as follows: 
 y=y1y2y3=x˙1x˙2x˙3=γ1000k100γ2000k200γ3k300⏟sqx1x2x311+x1nihbx2nact1+x2nact11+x3nihb=sqfb  where sq∈r3× <dig> represents the influence map.a graph representation of nonlinear odes.
 : among  <dig> components, only  <dig> components are non-zero   there exists hidden node x
h affecting x
 <dig> and process noise w.



we can also consider a version of equation  in which there exists a hidden node  affecting  as shown in figure  <dig>  as well as process noise. y=x˙1x˙2x˙3=sqfb+v+w 

wherev= ⊤andw= ⊤.

formulating grn into the cs framework
suppose the time series data are sampled from a real experimental system at discrete time points tk. by taking the transpose of both sides of equation , we obtain 
  y⊤=fb⊤)q+v⊤+w⊤ 

where q=sq⊤=q1q2…qn∈rn×n. assuming that m successive data points are sampled, then define: 
  y≜y1…yn=y1…yny1…yn………y1…yn∈rm×nΦ≜fb,1)…fb,n)fb,1)…fb,n)………fb,1)…fb,n)=fb⊤)fb⊤)…fb⊤)∈rm×nv1…vn≜v⊤v⊤…v⊤∈rm×nw1…wn≜w⊤w⊤…w⊤∈rm×n 

this leads to n independent equations: 
  yi=Φqi+vi+wi 

where yi=yi,yi,…,yi⊤∈rm represents the m successive data points, Φ∈rm×n consists of n possible candidate bases which are functions of given time series data x and qi∈rn represents the unknown influence map corresponding to the i-th species. since a biochemical reaction network is typically sparse, as a consequence, qi is sparse and we have n≫m for Φ because we assume the limited time series data and may choose a larger set of basis functions than necessary.

although we formulate n independent linear regression problems in equation , we consider n independent equations in equation  together by stacking yi as follows: 
  y=y1y2…yn=Φo…ooΦ…o…………oo…Φq1q2…qn+v1v2…vn+w1w2…wn≜Ωq1q2…qn+v+w 

now, equation  is in the form of the cs formulation in equation  where y∈rn·m is the measurement, Ω∈rn·m×n·n is the sensing matrix which consists of basis functions for the given time series data, v≜v1;v2;…;vn∈rn·m represents possibly large corruption from hidden nodes and w≜w1;w2;…;wn∈rn·m represents process or measurement noise.

in this paper, we make the following assumptions:

assumption <dig> 
we consider two cases: 
 we assume that we can measure all states x , thus v=0) and there is no measurement noise . also, there are enough columns in Φ in equation  to represent the underlying system in equation .

 we consider hidden node  where v) is assumed to be sparse, and process noise . here, we can also consider the case where the columns  of Φ may not be able to represent the underlying system. in this case, we consider the influence from the missing dictionaries as v.



we first consider the ideal case for simplicity in explaining the main results, and then extend the proposed method to the more general case.

RESULTS
formulating grn identification problem into cs
many existing algorithms  <cit>  consider the n independent linear regression problems in equation  separately. since the columns of matrix Φ are composed of time series data as in equation , it is difficult to a priori guarantee low correlation and sometimes Φ even suffers rank deficiency. also, pan et al.  <cit>  pointed out that correlation between the columns of Φ is usually high  is close to 1).

intuitively, if two columns of the sensing matrix are highly correlated, it is hard to distinguish the corresponding components in the sparse signal q . in order to deal with high coherence in equation , many method combines cs with different techniques such as bayesian formulation  <cit> , kalman filter  <cit> , and granger causality  <cit> . also, it is a well known problem in the lasso formulation of network inference: if there is high coherence in the sensing matrix, one can use an elastic net which combines the l <dig> and l <dig> norms. although each reconstruction result  might be the optimal solution in the sense of its formulation , the identified graph may not represent the underlying grn. in other words, if the data set is not informative enough to fully explore the underlying system, while the identified graph structure based on the given data set may be an optimal solution of the particular optimization problem, it may not represent the true system.

in this paper, our goal is to get the smallest data informative enough to recover the underlying graph structure exactly. since the proposed method maintains the cs framework by reducing coherence of the sensing matrix, the method is fundamentally different than any other methods which make use of different techniques in conjunction with the l <dig> optimization  which leads to a sparse representation of the network. hence, we use all the properties of cs in order to access the ability to exactly reconstruct the underlying graph structure, reveal deficiencies in the data and model, and design new experiments to remedy the deficiencies if necessary.

while maintaining the cs framework, in order to deal with high coherence, we formulate equation  instead of equation  since we have strongly uncorrelated columns in Ω. in other words, since Ω has many independent columns, we have more degrees of freedom to reduce coherence of Ω. we will show that by using a transformation, the components of the sensing matrix can be made more uniformly distributed so that we could reduce coherence.

moreover, since each qi has different degrees of sparsity in general, if we consider n independent equations in equation , Φ should satisfy the incoherence condition m≥ <dig> maxi stated in proposition  <dig>  on the other hand, Ω only needs to satisfy the condition m·n≥2·∑i=1nqil <dig>  since the averaged sparsity =∑i=1nqil0n≤maxiqil <dig> is smaller than maxiqil <dig>  we can reduce the required number of samples . also, in case of rank deficiency, we can simply remove the corresponding rows .

optimal design of sensing matrix
reducing coherence by transformation
in order to reduce coherence, first we rearrange Ω with respect to a spatial information, and then we consider the transformation in order to reduce coherence : 
  z≜z¯z¯…z¯=Ψ1sΨ2s…Ψmsq1q2…qnv¯v¯…v¯+w¯w¯…w¯≜Ω¯q1q2…qn+v¯+w¯ 

where ⊗ represents the kronecker producta, Ω¯∈rm·n×n·n represents the rearranged sensing matrix multiplied by the transformation Ψjs, and z¯, v¯ and w¯ are defined in equation . we want to find Ψjs to minimize 
  minμwhereΩ¯=ψ11sΦ <dig> :ψ12sΦ <dig> :…ψ1nsΦ <dig> :ψ21sΦ <dig> :ψ22sΦ <dig> :…ψ2nsΦ <dig> :…………ψm1sΦm,:ψm2sΦm,:…ψmnsΦm,:has full row rank 

where ψijs∈rn represents the j-th column of Ψis. in this paper, we propose a heuristic approach and a novel way to find Ψis by solving the optimization iteratively to reduce coherence .

example <dig> 
 consider a simple linear system  where n= <dig> n= <dig> m= <dig> s= <dig> , and the elements of a are randomly chosen such that there is no isolated node. figure  <dig> shows the sensing matrix for both Ω  and the transformed sensing matrix . by reducing coherence, the components of the transformed sensing matrix  are more uniformly distributed and the coherence is reduced by up to  <dig>  although μ is close to  <dig>  in figure  <dig>  also, figure  <dig> shows the result of the inferred graph structure based on given time series data without any a priori information where the x-axis represents indices of the influence map . here, there are  <dig> states  in a linear system so the influence map a has  <dig> elements. although l <dig> and l2b norm minimizations fail to recover the exact signal, cs in equation   recovers the exact signal  by reducing coherence of the sensing matrix. note that  solves the n independent equations in equation  without reducing coherence and  solves equation  with l2-regularization.reducing coherence for a linear system.   the sensing matrix Ω∈r20× <dig>  without  and with transformation Ψ  and the corresponding coherence distribution   reconstruction result  l <dig>  l <dig> with Ω  l
 <dig> optimization with Ω
Ψ  where x-axis represents indices of q∈r <dig> and y-axis represents coefficient of q . by reducing coherence of the sensing matrix with transformation, we can recover the exact structure in  . however, both l <dig> and l <dig> fail to recover the exact structure.



example <dig> 
 consider simple nonlinear odes as follows: 
  x˙1=γ1x1+k+12x2nact1+x2nactx˙2=γ2x2+k−2311+x3nihbx˙3=γ3x3+k+13x1nact1+x1nact 

where =, k+12= <dig> , k+13= <dig> , k−23= <dig> , nact= <dig>  and nihb= <dig> . the set of odes corresponds to a topology  as shown in figure  <dig>  figure  <dig> shows that we can reduce the coherence by up to  <dig>  and  shows that only cs recovers the exact graph structure, and l <dig> regularization does not encourage sparsity but distributes the coefficients to be more similar to each other.reducing coherence for a nonlinear system.   time series of x
 <dig>  x
 <dig>  x
 <dig> for model in equation   the sensing matrix without  and with transformation Ψ matrix  and the corresponding coherence distribution   reconstruction results. by constructing the sensing matrix Ω∈rn·m×n·n where n= <dig> n= <dig> m= <dig> and s= <dig>  we recover the s-sparse signal in  . however, both l <dig> and l <dig> fail to recover the exact structure q.



designing effective experiments
consider the case where the sensing matrix is not incoherent. if the coherence condition 2·slog) is not satisfied, exact recovery cannot be guaranteed  <cit> . we use the transformation ; see method section ‘reducing coherence by transformation’) in order to reduce the coherence but obviously, sometimes we might have inherent limits to how much the coherence can be reduced. there are possible reasons: 
since we solve the relaxed problem in equation  iteratively, p might be sub-optimal.

if the time series data of two different gene expressions, xi and xj are highly correlated, it might be difficult to reduce coherence. in this case, we need to design a new experiment to remedy deficiencies in the data.



as we mentioned, the incoherence of the sensing matrix can be used not only as a good metric to guarantee exact recovery but also as a guideline for designing new experiments. for example, from the coherence distribution, we can identify which columns of the sensing matrix have high coherence, i.e., fb,i and fb,j in equation . intuitively, in order to reduce ambiguities from the highly correlated columns of the sensing matrix, we should perturb either fb,i or fb,j. thus, it is possible to use this property of coherence to guide biological experiment design, to collect a more informative dataset. by doing this, we can minimize the number of experiments and reduce the cost of experiments.

example <dig> 
 in figure  <dig>  exp# <dig> represents the original experimental data set which has the limitation of reducing coherence by Ψ. since we consider linear dynamics, i.e., fb,i=xi and fb,j=xj, we found that x <dig> and x <dig> cause high coherence as shown in figure  <dig> . thus, in order to reduce this high coherence, we should perturb either x <dig> or x <dig> guideline for designing new experiment based on coherence distribution.  coherence distribution of the original Ω based on exp# <dig> show that x
 <dig> and x
 <dig> cause high coherence. thus, in order to reduce this high coherence, we should perturb either x
 <dig> or x
 <dig>  we design two different experiment sets; for exp# <dig>  we perturb x
 <dig> and for exp# <dig>  we perturb x
 <dig>  as we expected, exp# <dig> is a more effective experiment to reduce the coherence between x
 <dig> and x
 <dig> and we recover the exact graph structure as shown in figure  <dig>  on the other hand, the coherence of exp# <dig> remains almost the same as that of exp# <dig> and we fail to recover the exact graph structure in figure  <dig> 
design effective experiment for a linear system.  reconstruction results based on different experiments: exp# <dig> represents the original experimental dataset which has limitation to reduce coherence; exp# <dig> represents non-effective experimental dataset; and exp# <dig> represents the effective experimental dataset  the sensing matrix and reconstruction result  coherence distribution comparison. this example demonstrates that if there is a limitation to reduce coherence in the original dataset, we can also design a more effective experiments based on the coherence distribution. by revealing deficiency in the data, the proposed method can help reduce the cost of experiments.



to show the effectiveness of the new experiment, we design two different experiment sets and compare the reconstruction results with each other; for exp# <dig>  we perturb x <dig> and for exp# <dig>  we perturb x <dig>  as we mentioned earlier, intuitively, we expect that exp# <dig> to be a more informative experiment to identify the graph structure since we would like to reduce the coherence between x <dig> and x <dig>  as we expected, in exp# <dig>  we can reduce the coherence more than that of exp# <dig> as shown in figure  <dig>  and recover the exact graph structure as shown in figure  <dig>  on the other hand, the coherence of exp# <dig> remains almost the same as that of exp# <dig> and we fail to recover the exact graph structure. this numerical example illustrates that by using the property of coherence, we can guide biological experiment design more effectively.

example <dig> 
 consider the following set of odes: 
 x˙1=γ1x1+k+12x2nact1+x2nact+k+13x3nact1+x3nactx˙2=γ2x2+k−2111+x1nihb+k−2311+x3nihbx˙3=γ3x3+k−3111+x1nihb+k−3211+x2nihb 

where =, k+12= <dig> , k+13= <dig> , k−21= <dig> , k−23= <dig> , k−31= <dig> , k−32= <dig> , nact= <dig>  and nihb= <dig> . the corresponding topology is shown in figure  <dig> . the reconstruction error using exp# <dig> data is shown in figure  <dig>  and the reconstruction error illustrates difficulties of resolving ambiguities from x <dig> and x <dig>  this can be captured by the coherence distribution of the sensing matrix based on the exp# <dig> dataset; the correlation between the columns corresponding to x <dig> and x <dig> is close to the correlation between the columns corresponding to x <dig> and x <dig>  based on the coherence distribution, we design two trials; for exp# <dig>  we perturb x <dig> and for exp# <dig>  we perturb x <dig>  as we expected, exp# <dig> is not an effective experiment in terms of information. on the other hand, by using exp# <dig>  we can reduce both the maximum coherence and the averaged coherence, and reconstruct the exact graph structure as shown in figure  <dig> design effective experiment for a nonlinear system.  reconstruction result based on different experiment : exp# <dig> represents the original experimental dataset which has limitation to reduce coherence; exp# <dig> represents non-effective experimental dataset ; and exp# <dig> represents the effective experimental dataset   the time series of x
 <dig>  x
 <dig>  x
 <dig> for each experiment , the reconstruction error  and coherence comparison for each experiment   the corresponding topology  coherence distribution of the sensing matrix. in exp# <dig>  we reduce the coherence below  <dig> .



both examples  <dig> and  <dig> illustrate that if the transformed sensing matrix is not incoherent enough to guarantee exact recovery, we can design a new experiment based on the distribution of coherence. also, we show that the proposed experiment can help to reduce coherence more and thus reconstruct the exact graph structure. for a fair comparison, we use the same number of time points as m here. however, in practice, we can also stack all the experimental data sets together if we assume that the linear map sq and the set of basis functions fb does not change for different experiment: 
  z1…zk=Ω¯1…Ω¯kq1q2…qn+v¯1…v¯k+w¯1…w¯k 

where the subscript zi,Ω¯i,v¯i,w¯i represents the i-th experiment. as the number of measurements increase , one may be able to reduce the coherence. however, one can reduce the coherence only if the additional measurements provide us more useful information. as a trivial example, one could stack exactly the same data on top of the first, and increase m to 2m, however the coherence is exactly the same as that of the original dataset.

recovery of gene regulatory networks
in this section, we present reconstruction of the exact graph structure and show how the condition for exact recovery will be used. first, we consider the ideal case where there are no hidden nodes and no measurement noise. second, we extend the ideal case to the more general case.

reconstructing gene regulatory networks 
in equation , q represents the s-sparse network structure which we want to reconstruct from the time series gene expression by solving the l1-norm optimization: 
  minq∥l1s.t.z~=Ω~q 

where z~≜pz, Ω~≜pΩ¯ and  is the optimal transformation in equation .

proposition <dig> 
if the sensing matrix Ω~ constructed from time series data, multiplied by the optimal transformation, , has 2s linearly independent columns, then any s-sparse network structure q can be reconstructed uniquely from z~=Ω~q.

proof.
, then there are two s-sparse graph structures q <dig> q <dig> with Ω~q1=Ω~q <dig> =0). however, q1−q <dig> is 2s-sparse, so there is a linear dependence between 2s columns of Ω~ .

the requirement of 2s linearly independent columns in proposition <dig> may be translated to an incoherence condition on the sensing matrix. that is, if the unknown s-sparse signal q is reasonably sparse, it is possible to recover q under the incoherence condition on the sensing matrix. although the sensing matrix consists of redundant dictionaries, the coherence of the sensing matrix can be reduced. in a heuristic way, we multiply the redundant dictionaries Φk,: by a randomly chosen matrix Ψks at each time step k and iterate this step until the coherence is decreased. or, we can find the optimal transformation  in equation  to reduce the coherence. in the previous numerical examples, we illustrated that the coherence of the sensing matrix is decreased by transformation and showed the exact reconstruction of graph structure.

example <dig> 
 here, we compare the success rate of the proposed method with other methods such as l <dig> and l <dig>  figure  <dig> shows statistics of  <dig> trials for a simple linear case . here, we count the number of successes of each method when any of the methods recover the exact structure. by reducing coherence, we can improve the success rate as shown in figure  <dig>  also, l2-regularization does not encourage sparsity but distributes the coefficients to be more similar to each other as shown in figure  <dig> comparison
l
 <dig> 
l
 <dig>  and
cs
.  statistics of success rate and error among  <dig> trials:  success rate  and reconstruction error   component-wise reconstruction error. by reducing coherence, we can improve the success rate as shown in figure 7
. also, since l2-regularization does not encourage sparsity but distributes the coefficients to be more similar to each other, component-wise errors are distributed as shown in .



graph reconstruction with hidden nodes
the main contributions of the proposed method in the previous section is the conversion of the problem of inferring graph structure into the cs framework. then, we demonstrate that one could recover sparse graph structures from only a few measurements. however, for practical use, the proposed method needs to be able to deal with both sparsely corrupted signals ) and measurement noise  in equation .

in general, the assumption of accessibility or observability of all nodes  <cit>  is not satisfied. thus, we focus on the case in which the hidden node affects observable nodes directly as shown in figure  <dig>  also, without loss of generality, the hidden node dynamics could be any arbitrary dynamic model. or, even if there is no hidden node, a small portion of the biological experiment dataset could be in practice contaminated by large error resulting from, for example, mislabeling, or improper use of markers or antibodies. moreover, all biological datasets are contaminated by at least a small amount of noise from measurement devices. therefore, the proposed method should be robust. we note this goes beyond the results in  <cit>  due to the consideration of hidden node dynamics with measurement noise.

here, the question is whether it is still possible to reconstruct the graph structure reliably when measurements are corrupted. since hidden nodes and measurement noise are considered, the number of time points is assumed to be greater than that of the previous case  <cit>  . thus, the number of rows of the sensing matrix is assumed to be greater than the number of columns. if the number of the time points m is limited, then we can stack z¯ with different Ψks or including different dataset as described in equation .

in cs literature  <cit> , two decoding strategies for recovering the signal from a corrupted measurement are introduced, where the corruption includes both a possible sparse vector of large errors and a vector of small error affecting all the entries. it is shown that two decoding schemes allow the recovery of the signal with nearly the same accuracy as if no sparse large errors occurred. our contribution is converting the problem of inferring the graph structure with hidden nodes into the highly robust error correction method framework  <cit>   and showing how this can improve the reliability of reconstruction.

example <dig> 
 consider nonlinear odes as follows: 
  x˙1=−γ1x1+α13hact+v1x˙2=−γ2x2+α21hact+v2x˙3=−γ3x3+α34hact+β32hihb+v3x˙4=−γ4x4+β41hihb+v <dig> 

where hact,hihb represents hill functions for activation and inhibition respectively, and vi represents arbitrary corruption shown in figure  <dig> and assumed to be sparse ) = 1). the magnitude of vi are about 50% of the magnitude of x˙. since we consider arbitrary corruption, we need more time points . by using two-step refinements, first we estimate sparse large corruption v as shown in figure  <dig>  and then, we reconstruct q .arbitrary corruption with no measurement noise.  reconstruction with corrupted signal.  time series of x,y, v
 reconstruction results of v and q where each circle represents sampled time points . by using two-step refinements, first we estimate sparse large corruption v and then, we reconstruct q. note that since we consider arbitrary corruption, we have more time points .



in practice, a specific node is corrupted by a hidden node and a small portion of the dataset can be largely corrupted by human error. also, since we choose the set of possible candidate basis functions of the sensing matrix in equation , the columns of the sensing matrix may not be able to represent the underlying system . then, we can consider the influence from these missing dictionaries as v.

example <dig> 
 recall a model in equation  with different parameters and consider sparse large corruption v and small magnitude noise w . figure  <dig> shows the time series data and reconstruction result.arbitrary corruption with measurement noise.  reconstruction with corrupted signal.  time series of x,y, v
 reconstruction results of v and q where each circle represents sample time points . here we consider arbitrary large corruption with measurement noise. by choosing the parameters ε
 <dig> ε
 <dig> properly in equation , we can recover the graph structure q within the small error bound.



geometric view
in equation , since we assume all nodes are accessible and perfect measurement , we can solve equation  directly without filtering out the unmodelled dynamics in equation  ). if there exist hidden nodes or measurement noise, we can still provide an unambiguous indication of the existence of these corruption .

the intuition is that z can be decomposed as the superposition of an arbitrary element in v and of an element in v⊥ as shown in figure  <dig>  in other words, z can be decomposed as the superposition of modelled dynamics and anomalies caused by hidden node or unmodelled dynamics. this geometric view enables us to understand how we could reveal deficiencies in our model: 
z¯=q∗z=0: there is no hidden node ⇒z∈rfigure  <dig> 
geometric view of two-step refinement. a geometric view of two consecutive l
1-norm optimizations. by using two-step refinements, first we estimate sparse large corruption v and then, we reconstruct q. for the ideal situation with no unobservable species  or noise, we directly reconstruct q without estimating v.



z¯≠0: z cannot be represented by Ω¯q so there might be hidden nodes or our dictionaries in the sensing matrix Ω¯ are not sufficient to represent z .



her <dig> overexpressed breast cancer
we apply the proposed algorithm to study a breast cancer signaling pathway by reconstructing the graph structure using an rppa dataset  <cit>  as shown in figure  <dig> . here, we choose small networks which are composed of  <dig> nodes and known to be sparsely connected, i.e., pi3k→pdk→akt and pdk→akt→mtor in order to satisfy our assumption such that the influence on observable nodes from a hidden node should be sparse . the graph structures identified by the proposed method are consistent with the current understanding of the networks, whereas those found using l1- and l2-optimizations fail to reconstruct the known structure as shown in figure  <dig> her2+ overexpressed breast cancer.
cs reconstruction result using reverse phase protein array data . see additional file 1: figure s <dig>  s <dig> and s <dig> for further details . here, we choose small networks which are composed of  <dig> nodes and known to be sparsely connected in order to satisfy our assumption such that the influence on observable nodes from a hidden node should be sparse. the identified graph structures are consistent with the current understanding of the networks.
sub networks inferred for the her2/ <dig> signaling network from reverse phase protein array data. the columns show the networks inferred by l1-optimization, l2-optimization, and cs. the network structures identified by cs agree with the current understanding of the network, whereas those found using l <dig> and l <dig> optimization do not. see additional file 1: figure s <dig>  s <dig> and s <dig> for further details .



also, an abstract model of the breast cancer signal pathway proposed by m. moasser  <cit>  is considered, as shown in additional file 1: figure s <dig> where phlpp isoforms are a pair of protein phosphatases, phlpp <dig> and phlpp <dig>  which are important regulators of akt serine-threonine kinases  and conventional protein kinase c  isoforms. phlpp may act as a tumor suppressor in several types of cancer due to its ability to block growth factor-induced signaling in cancer cells  <cit> . phlpp dephosphorylates ser <dig>  in akt, thus partially inactivating the kinase  <cit> . unfortunately, in our rppa dataset, we do not have phlpp so we simply consider three nodes . figure  <dig> shows the result of the proposed method using the rppa dataset. the reconstructed graph structure matches up to the known structure ). specifically, our result can capture the partial inactivating characteristics of phlpp ⊣aktps473).

discussion
many network inference algorithms use l1-norm optimization to find the actual network structure, but they cannot guarantee exact recovery. in this paper, we propose a novel approach to reconstruct grns based on compressive sensing with a dynamic system model. we demonstrate that the incoherence condition is essential for exact recovery. this is not properly considered in inference methods based on l1-norm optimization. also, we illustrate how the incoherence condition can be used to design new experiments effectively. finally, we consider a more general setting, in which hidden genes exist and all measurements are contaminated by noise, and we show that the proposed method leads to reliable reconstruction.

we compare all the results with l1- and l2-norm optimization, since l1-norm optimization is widely used in network inference and many inference methods combine the l1- and l2-norms . we highlight the improvement by the proposed method compared with these commonly used methods. our use of the method to propose new experiments, and its extension to a more general setting goes beyond the results in any network inference methods.

from the lessons learned from the numerical study using both synthetic datasets and real rppa datasets of her <dig> overexpressed breast cancer signaling pathways, the proposed method can be applied to sub-networks in order to satisfy our assumption about the number of hidden nodes relative to the network size, and has great potential for reconstructing grn by designing effective experiments. note that the proposed method is not limited to the network size itself but rather by this assumption and the incoherence condition. in the ideal case with no unobservable species  and no noise, the method can be applied to entire networks if the incoherence condition is satisfied  where n is the number of nodes, m is the number of time points and s represents the sparsity of the network structure. for example, suppose we only have one time point measurement  and then, the proposed method cannot be applicable for any network  since it is impossible to satisfy the incoherence condition. as an example, if we consider n nodes which are simply connected , then n·m<2s. with a reasonable number of time points , the incoherence condition could be satisfied. similarly, in the practical case with unobservable species  and measurement noise, in order to guarantee the exact recovery of the graph structure, we should satisfy assumption  <dig> for a given network  and the incoherence condition. suppose we choose a small sub-network where a hidden node or unobservable node affects all nodes in the chosen sub-network: then, it is impossible to infer the exact structure of the sub-network. however, if a hidden node only affects relatively few nodes in the given network, we can still reconstruct the exact structure of the sub-network by inferring the hidden node influence first. therefore, the proposed method is not limited to the network size itself but rather by assumption  <dig> and the incoherence condition. in practice, we can work on a sub-network at a time and integrate the identified sub-networks.

finally, we evaluate performance with respect to the size of the problem under typical parameters  in  which provides us a brief guideline of application, e.g., for an n-genes network with certain kinetic features, how many time points/how much resolution are appropriate for the reconstruction.

in this paper, we do not consider any a priori knowledge of connectivity. however, since we may have partial information of connectivity, we can also use prior knowledge of connectivity in the form of known nonzero elements in q allowing fewer experiments to be performed. also, here we only consider protein expression or gene expression levels but we can also consider many other types of useful information that can be incorporated into the network reconstruction process, such as sequence motifs and direct binding measurements . since it has been shown that chip-seq signals of histone modification are more correlated with transcription factor motifs at promoter sites in comparison to rna level, time series histone modification chip-seq could provide a more reliable inference of grns in comparison to method based on expression level. we are interested in continuing this research direction for large-scale networks.

CONCLUSIONS
we proposed a method for reconstructing sparse graph structures based on time series gene expression data without any a priori information. we demonstrated that the proposed method can reconstruct graph structure reliably. also, we illustrated that coherence in the sensing matrix can be used as a guideline for designing effective experiments.

second, the proposed method is extended to the cases in which dynamics is corrupted by hidden nodes and the measurement is corrupted by human error in addition to the measurement noise. using a two-step refinement procedure, we demonstrate good performance for the reconstruction of graph structure. a set of numerical examples is implemented to illustrate the method and its performance. also, a biological example of her <dig> overexpressed breast cancer using an rppa dataset is studied. we are currently applying our method to recover the her <dig> signaling pathway, where a significant part of the network is currently unknown.

