BACKGROUND
in gene expression studies, the number of samples in most data sets is limited, while the total number of genes assayed is easily ten or twenty thousand. such high dimension and low sample size data arise not only commonly in genomics but also frequently emerge in various other areas of science. in radiology and biomedical imaging, for example, one is typically able to collect far fewer measurements about an image of interest than the number of pixels.

these hdlss data present a substantial challenge to many methods of classical analysis, including cluster analysis. in high dimensional data, it is not uncommon for many attributes to be irrelevant. in fact, the extraneous data can make identifying clusters very difficult  <cit> . robust clustering methods are needed that are resistant to small perturbations of the data and the inclusion of unrelated variables  <cit> .

the bisecting k-means algorithm is a hybrid of hierarchical clustering and the k-means algorithm. it proceeds top-down, splitting a cluster into two in each step, after which it will select one cluster based on a selection rule  to further split. in each splitting step, it randomly picks a pair of data points that are symmetric about the "center" of the data and assigns all other data points to one cluster or the other based on distance to the two selected points, thus the algorithm is similar to the k-means algorithm. the center is usually the mean. this whole process continues until each point is a cluster or a predefined number of clusters is reached.

similar to other commonly used methods that are based on mean, e.g. k-means, bisecting k-means is not robust because the mean is susceptible to outliers and noise  <cit> . as a common remedy, the bisecting k-median algorithm, which replaces the mean by the componentwise median, is less sensitive to outliers. however, the componentwise median may be a very poor center representative of data, because it disregards the interdependence information among the components and is calculated separately on each component . for example, the componentwise median of the points ,  and  for arbitrary reals a, b, c is  which even does not lie on the plane passing through the three points.

a new center representative for multivariate data that is robust and takes into account the interdependence among the dimensions is clearly needed.

of the various multivariate medians, however, those defined via statistical depth functions are advantageous because the theory of statistical depth has been quite nicely established, though it is still relatively young and still under development. analogous to linear order in one dimension, statistical depth functions provide an ordering of all points from the center outward in a multivariate data set. linear order induces an ordering and ranking for 1-dimensional observations. median is the "deepest" point in the data set. in contrast, for dimension d ≥  <dig>  there is no natural order. as compensation, it is convenient and natural to orient to a "center", the deepest point, that is, the multivariate median. this leads to center-outward ordering of points and to a description in terms of nested contours. tukey  <cit>  first introduced halfspace depth. oja  <cit>  defined oja depth. liu  <cit>  proposed simplicial depth. zuo and serfling  <cit>  considered projection depth. other notions include zonoid depth  <cit> , generalized tukey depth  <cit> , and spatial depth  <cit>  among others. see  <cit>  for a systematic exhibition.

of the various depth functions, the spatial depth is especially appealing because of its computational ease and mathematical tractability, see vardi  <cit> , serfling  <cit> , chaudhuri  <cit>  and koltchinskii  <cit>  among others. the spatial depth  of a point x w.r.t. a distribution f is defined as

 spd =  <dig> - ||e
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaatuudjxwak1uy0hmmaehbfv3yslgzg0uy0hgiud3bagabaiab=ri8fbaa@388c@fs||, x ∈ ℝd, 

where s = x/||x|| is the spatial sign function  = 0) with euclidean norm ||·||. the sample spatial depth is

 spd=1−‖1n∑i=1ns‖,x∈ℝd,
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqabeqacaaabaacbagae83uamlae8huaalae8hraqkae8hkagccbigae4heagnae4hlawiae4nray0aasbaasqaaiab+5gaubqabagccqggpaqkcqgh9aqpcqaixaqmcqghsisldaqbdaqaamaalaaabagaegymaedabagaemoba4gaamaaqahabagaem4uamlaeiikagiaemieagnaeyoei0iaemiwag1aasbaasqaaiabdmgapbqabagccqggpaqkasqaaiabdmgapjabg2da9iabigdaxaqaaiabd6gaubqdcqghris5aagccagljwuaaypcsdgaeiilawcabagaemieagnaeyici48efv3yslgznfgdojdaryqr1ngbprginfgdobcv39gaiqaacqqfdeiudaahaawcbeqaaiabdsgakbaakiabcycasaaaaaa@5f4b@ 

where fn is the empirical distribution function of the data x <dig> ...,xn. points deep inside the data cloud have high depth values, while the points on the outskirts have lower depth values.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuaiwaamgawcaaaa@2dac@, since unit vectors have different directions and they cancel each other out. the depth of y is approaching  <dig>  see the diagram on the left in figure  <dig>  when y is outside the data cloud , the sum of ei has a large norm, thus the depth is approaching  <dig>  the point where the spatial depth attains its maximum value  <dig> is called the spatial median. the spatial median represents the geometric center of the data, in particular, for a symmetrical distribution, the spatial median is the symmetric center. the spatial depth and the spatial median possess many nice properties. robustness is one of them.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuaiwaamgawcaaaa@2dac@, since unit vectors have different directions and they cancel each other out. the depth of y <dig> is approaching  <dig>  y <dig> is outside the data cloud, the sum of e has a large norm, thus the depth is approaching  <dig> 

from the definition of the sample spatial depth, it is not difficult to see that the depth value of a point x does not change if any observations are moved to ∞ along the rays connecting them to the point x. thus the spatial depth and the spatial median are highly robust in the presence of outliers. in fact, the breakdown point of the spatial median is 1/ <dig>  depending on neither the data nor the dimension and reaching the highest possible value for the translation equivalent location estimator. here the "breakdown point" is the prevailing quantitative measure of robustness proposed by donoho and huber  <cit> . roughly speaking, the breakdown point is the minimum fraction of the "bad" data points that can render the estimator beyond any boundary. it is clear to see that one bad point of a data set is enough to ruin the sample mean. thus, the breakdown point of mean is 1/n →  <dig>  the lowest possible value. that is, the sample mean vector is not robust, hence neither is the clustering method k-means which is based on nonrobust sample means.

unlike the componentwise median, the spatial median is equivariant under orthogonal transformations  of the data though it is not equivariant under general affine transformation. the spatial median may not be a reasonable estimate when the scale of different coordinates of the data are widely different. it is, however, very desirable for preprocessed gene data, where variables are isometric.

the complexity of the spatial median is o for sample size n regardless of the dimension. this independence of dimension is particularly important for hdlss data because high dimension usually causes problems for classical methods.

in our bisecting k-spatialmedian algorithm, we propose the use of a robust spatial median to replace the non-robust mean or the less-robust componentwise median to determine the center of the data. the bisecting k-spatialmedian algorithm is shown to be more robust than the bisecting k-median algorithm in high dimension.

for the selection criterion, we replace the largest variance criterion, which is sensitive to outliers, and propose a depth-based notion, relative average depth , which characterizes the separatedness of a data set. with its range in  <cit> , a smaller value of the relative average depth indicates less separatedness and a larger value is an indication of higher separatedness. indeed, in conjunction with the robust spatial median, we can use any existing selection criterion, including largest variance.

RESULTS
simulation study
to demonstrate the difference in performance between algorithms based on the spatial median and the componentwise median, we conduct a simulation of four clusters in ℝ <dig>  see figure  <dig>  clusters i and ii are comprised of data points  with x generated from the uniform distributions u and u; and clusters iii and iv comprised of data points  and  with y from u and z from u, where iii and iv have the same sample size equaling the sum of the sample sizes of i and ii. we observe that the bisecting k-median completely fails to separate the four clusters, while the proposed bisecting k-spatialmedian successfully finds the four clusters. as shown in figure  <dig>  the four clusters were perfectly identified by the bisecting k-spatialmedian algorithm. since the output of the bisecting k-median is the whole data set, its graph is in one color, without identification of clusters.

the phenomenon observed in the above simulated data seems unrepresentative because the data structure appears so contrived. but actually this is a quite general structure for hdlss data. in fact, hall et al.  <cit>  show that there is a tendency for hdlss data to lie deterministically at the vertices of a regular simplex and all the randomness in the data appears as a random rotation of this simplex. based on this geometric representation, we have shown that the angle between any two distinct data points centered at their common mean is approximately perpendicular, and all these centered data points will lie on the coordinate axes. see the methods section for more details.

the bisecting k-spatialmedian algorithm
based on the spatial median, we propose the bisecting k-spatialmedian algorithm. specifically, the bisecting k-spatialmedian algorithm recursively splits a cluster by randomly choosing one point cl as the center of one subcluster. let c be the spatial median of the whole data set. then the center cr of the other subcluster is determined as the symmetric point of cl about c, namely, cr = c - . every point x in the cluster is assigned to the subcluster containing cl or cr according to the smaller euclidean distance ||x - cl|| or ||x - cr||. this process is repeated until the convergence criterion is met, namely, the centers of the subclusters no longer change. after the cluster is split into two subclusters, a selection rule is needed to determine which subcluster is to be further split.

the basic bisecting k-spatialmedian algorithm follows:

initialize:

   k: number of clusters

   c: center  of the data cluster

   cl: center of left subcluster

   cr: center of right subcluster

for i =  <dig> to k -  <dig> do

   choose a cluster to split by the selection rule

   randomly select a point cl as center of left subcluster

   compute cr = c - 

   for j =  <dig> to maxiter do

      for each data point xi

         if ||xi - cl|| ≥ ||xi - cr||

            assign xi to the right subcluster

         else

            assign xi to the left subcluster

      end

      let ml be the spatial median of the left subcluster

      let mr be the spatial median of the right subcluster

      if ml == cl and mr == cr

         break

      cl = ml

      cr = mr

   end

end

subcluster selection rule
in the bisecting k-spatialmedian algorithm, we need to decide which cluster is to be further split in each step. selecting the one with the largest variance is a very common approach. here we propose a new rule based on the statistical spatial depth.

suppose that a data set is naturally composed of two clusters j <dig> and j <dig>  let d1w
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegymaedabagaem4dachaaaaa@3051@ be the sum of spatial depth values of all data points in j <dig> with respect to j <dig>  let d2w
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegomaidabagaem4dachaaaaa@3053@ be the sum of spatial depth values of all data points in j <dig> with respect to j <dig>  note that d1w
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegymaedabagaem4dachaaaaa@3051@ or d2w
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegomaidabagaem4dachaaaaa@3053@ represents "within-depth", because it is calculated with respect to the cluster to which the data points belong. let d1b
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegymaedabagaemoyaigaaaaa@3027@ be the sum of spatial depth values of all data points in j <dig> with respect to j <dig>  similarly, let d2b
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegomaidabagaemoyaigaaaaa@3029@ be the sum of spatial depth values of all data points in j <dig> with respect to j <dig>  d1b
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegymaedabagaemoyaigaaaaa@3027@ or d2b
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegomaidabagaemoyaigaaaaa@3029@ represents ''between-depth", because it is calculated with respect to the other cluster. see figure  <dig> for a graphic display. the within-depth is larger when a cluster is more condensed whereas the between-depth is smaller when two clusters are further away from each other.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegymaedabagaem4dachaaaaa@3051@ represents the sum of spatial depth of all data points in j <dig> with respect to j <dig>  d2w
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegomaidabagaem4dachaaaaa@3053@ represents the sum of spatial depth of all data points in j <dig> with respect to j <dig>  note that dw represents "within-depth", because it is the depth of data points with respect to the cluster to which they belong. let d1b
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegymaedabagaemoyaigaaaaa@3027@ be the sum of spatial depth of all data points in j <dig> with respect to j <dig>  similarly, let d2b
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaegomaidabagaemoyaigaaaaa@3029@ be the sum of spatial depth of all data points in j <dig> with respect to j <dig>  db represents "between-depth", because they are depth of data points with respect to the other cluster.

let |j1| and |j2| represent the number of data points in j <dig> and j <dig> respectively. the relative average depth is defined as

 rad=d1w|j1|+d2w|j2|−d1b|j1|−d2b|j2|.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaieaacqwfsbgucqwfbbqqcqwfebarcqgh9aqpdawcaaqaaiabdseaenaadaaaleaacqaixaqmaeaacqwg3bwdaaaakeaacqgg8bafcqwgkbgsdawgaawcbagaegymaedabeaakiabcyha8baacqghrawkdawcaaqaaiabdseaenaadaaaleaacqaiyagmaeaacqwg3bwdaaaakeaacqgg8bafcqwgkbgsdawgaawcbagaegomaidabeaakiabcyha8baacqghsisldawcaaqaaiabdseaenaadaaaleaacqaixaqmaeaacqwgibgyaaaakeaacqgg8bafcqwgkbgsdawgaawcbagaegymaedabeaakiabcyha8baacqghsisldawcaaqaaiabdseaenaadaaaleaacqaiyagmaeaacqwgibgyaaaakeaacqgg8bafcqwgkbgsdawgaawcbagaegomaidabeaakiabcyha8baacqgguaglaaa@5854@ 

as shown from figure  <dig>  if a data set is naturally composed of two clusters and thus should be split into two, the within-depth should be relatively large and the between-depth relatively small, therefore the relative average depth  which is essentially the averaged difference between the within-depth and the between-depth will be relatively large compared to the rad of a data set that is more condensed and cannot be split into two clusters obviously. in fact we have shown that a larger value of rad indicates less condenseness of a data set. see section methods for technical details. hence we obtain a new selection rule: a cluster with the largest value of rad should be selected to split.

the following simulation demonstrates the relationship between the value of rad and the condenseness of a data set. as shown in figure 4a, two clusters were generated from normal distributions with means μ <dig> =  and μ <dig> = , covariances Σ <dig> =  and Σ <dig> =  for the same sample size  <dig>  obviously the data comprises of two clusters and should be split as such. the relative average depth rad =  <dig> . if the second cluster is moved from μ <dig> =  to μ <dig> = , the two clusters are further away from each other, as shown in figure 4b. compared with the previous situation, this new data should have higher priority to be selected for further splitting. the relative average depth increases to rad =  <dig> . table  <dig> lists the values of rad with one cluster being moved further away from another one with μ <dig> = . we can see that the rad value increases slowly when the two clusters are more separated.

applications
data sets
we use the proposed bisecting k-spatialmedian algorithm to analyze two well known data sets. the first is the colon cancer data   <cit> , which is comprised of expression levels of  <dig> genes describing  <dig> samples . the second is a pediatric acute lymphoblastic leukemia  data from st. jude children's research hospital   <cit> , which includes  <dig>  gene expression measurements  per patient from  <dig> patients with six different subtypes of all.

in the investigation at sjcrh,  <dig> cases of pediatric all were analyzed on the u <dig> a and b chips, involving six primary subtypes of all: bcr-abl, e2a-pbx <dig>  hyperdiploid >  <dig>  mll, t-all and tel. the original data has patient information with two additional subtypes, which did not fit into one of the above primary diagnostic groups or were added for the analysis of relapse and secondary aml. our study did not include these two subtypes.

design of the experiment
since the mean is known to lack robustness, we will focus on the comparison of the bisecting algorithms based on componentwise median and spatial median in this paper.

the two data sets were used to compare the performance of the proposed bisecting k-spatialmedian with the bisecting k-median. since the class labels of the two data sets are known, the number k of clusters is also known. the alon data set has two classes, so k =  <dig>  for the all data from sjcrh, k =  <dig>  the algorithms are applied on the two datasets and terminated when k clusters have been reached.

in order to investigate the performance of the proposed clustering algorithm for hdlss data, we test them on the two data sets for various dimensions, i.e., different number of genes selected. for the all data which has  <dig> genes, we test the dimensions d
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaat0uy0hwztfgdpnwy1egaryqthrhal1wy0l2yhvdaiqaacqwfdepraaa@3826@ = {100; 200; 500; 1000; 1500; 2000; 3000; 4000; 5000}; for the alon data which has  <dig> genes we test the dimensions d
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaat0uy0hwztfgdpnwy1egaryqthrhal1wy0l2yhvdaiqaacqwfdepraaa@3826@ = {50; 100; 200; 500; 1000; 2000}.

for each d
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaat0uy0hwztfgdpnwy1egaryqthrhal1wy0l2yhvdaiqaacqwfdepraaa@3826@, we trim the data with only d
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaat0uy0hwztfgdpnwy1egaryqthrhal1wy0l2yhvdaiqaacqwfdepraaa@3826@ "most important" genes. we use the svm-rfe-annealing algorithm  <cit>  to select the d
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaat0uy0hwztfgdpnwy1egaryqthrhal1wy0l2yhvdaiqaacqwfdepraaa@3826@ most important genes. all clustering algorithms are then applied to the trimmed data.

validation of the clustering results is usually not easy. however, in situations where data are already categorized, as with these data, we can compare the predicted clusters from our algorithms with the true class labels. to display the results, we build a confusion matrix in which rows represent the predicted clusters while columns represent the true clusters. the number in the cell  is the number of observations that are from cluster j but are predicted to be from cluster i. the rows and columns are "matched" by a brute force algorithm, and this is optimistic. two evaluation measures, entropy and misclustering rate, are used. see the methods section for more details.

because the bisecting divisive clustering algorithm randomly selects a point as the center of the subcluster cl, it is non-deterministic and therefore yields stochastic clustering results. to evaluate the stochastic clustering result, we ran each algorithm  <dig> times and calculated the average entropy and misclustering rates as the clustering measures. these algorithms select the next subcluster to split based on the criterion of the largest variance. we compare the performance of our proposed bisecting k-spatialmedian with bisecting k-median based on the same selection rule, the largest variance, on the two data sets. the performance of bisecting k-spatialmedian with the selection criterion of the relative average depth is also presented.

to investigate the robustness of our proposed procedure, we compare the sensitivity of the proposed algorithm to noise with the bisecting k-median algorithm. we add noise to the alon data and then apply the three algorithms  on it to investigate their performance.

we generated a percentage of random noise and added to the alon data by changing the expression value of a point to either the maximum or minimum value of all data points. in this way, some data points are changed to have extreme values and more likely to become outliers. experiments show that our proposed algorithms based on spatial median perform better than the bisecting k-median algorithm in this noisy environment.

the result on the alon data
from figure 5a and 5b, we can see that both of the algorithms using spatial median have lower entropy and misclustering rates than the one using componentwise median in most of the cases. when we use more than  <dig> genes in clustering, the algorithms using spatial median are better than the one using componentwise median, which demonstrates that spatial median is more robust in higher dimensional data. also the performance of the algorithm using median is decreasing dramatically with dimensions increasing from  <dig> to  <dig>  while the performance of the algorithms using spatial median does not degrade as much.

additional file  <dig> gives an example of the relationship of the number of runs and average entropy of the alon data. in additional file  <dig>  the entropy values get more stable with the number of runs increasing, which justifies the need of running the clustering algorithms multiple times. the average misclustering rate and the number of runs have the similar relationship.

the result on the sjcrh data
similarly, figure  <dig> reports the entropy and misclustering rates of the algorithms on the trimmed sjcrh data. we can see that in most of the cases after  <dig> genes are used, both of the algorithms using spatial median are better than the bisecting k-median. the largest difference between bisecting k-spatialmedian and median is more than 10%. the results are consistent with the results on the alon data.

similarly, figure  <dig> shows the entropy values with standard deviation of the three algorithms. we can see that the three algorithms display similar variation, less than  <dig>  in most cases, although the algorithm using median achieves the lowest standard deviation. standard deviation appears to be more consistent with median than with spatialmedian on the sjcrh data. the very similar results are obtained by using misclustering rate.

additional file  <dig> gives an example of the relationship of the number of runs and average entropy of the sjcrh data. in additional file  <dig>  the entropy values get more stable with the number of runs increasing. the average misclustering rate and the number of runs have the similar relationship.

the result on the noisy alon data
we randomly add noise to the alon data to see how well the algorithms based on the componentwise median and the spatial median perform in a noisy environment.

to this end, we randomly pick 10% of data from the alon data, and reset their values to be either the maximum or minimum value in the data matrix.

we applied the three algorithms to this noisy data and observed that all the algorithms have been influenced by the noise. however, the bisecting k-median is more susceptible to the noise, which can be demonstrated by the fact that it cannot separate the two clusters at all.

this process is repeated several times and the results are very consistent. we further increase the amount of noise from 10% to 20% and get a similar result.

CONCLUSIONS
the spatial depth function provides a robust location estimator whereas componentwise median may not work well in high dimension and low sample size data, which is illustrated by easily designed simulation. the experimental results on real data sets further verify that the spatial median based bisecting clustering algorithm is more robust to outliers and noise in high dimensional data, such as gene expression data, than the bisecting k-median algorithm.

