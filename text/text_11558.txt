BACKGROUND
classification of cancer tissue samples based on microarray expression data is of great interest in recent years. this was driven by biomedical applications to differentiate cancerous tissue samples from normal samples as well as different tumor subtypes. though many methods have been recently developed, further improvement in classification accuracy is needed before molecular methods can be used to replace laborious histological approaches. an obvious challenge for effective classification is that the number of samples is much smaller than the number of available features. there are two directions to tackle the challenge. some studies focus on how to design or create better classifiers with a given sets of features. examples are svm
 <cit> , top scoring pair 
 <cit> , k-top scoring pair 
 <cit> , and hyper-box enclosure 
 <cit> . a common feature of these methods is that they depend critically on prescreening of genes such as through comparing the absolute value of a t-statistic. the other direction is to seek ways to reduce the dimensionality of the feature space and select the informative genes for effective classification with new or existing classifiers. efforts in this direction include, for example, prediction analysis of microarrays , individual-gene-ranking methods by evaluating the discriminating power of classes , redundant gene filtering through correlation analyses
 <cit>  or based bayes error filter   <cit> .

individual gene-ranking methods perform gene preselection through a univariate criterion function to provide a list of top ranked genes. however, the combination of top ranked genes through individual gene-ranking may not produce a top ranked combination of genes because individual ranking tends to ignore the redundancy and interaction among genes. as a result, such methods tend to have low power when the co-regulation of multiple genes or pathways during tumor progression is not fully utilized. an example is shown in table
 <dig> of this paper, in which the combination of six genes selected yields high accuracy for classification of the colon cancer data
 <cit> . however, two out of the six genes have p-values greater than  <dig> . these two genes have extremely low chance to be selected by individual gene-ranking methods. recently, chopra et al.
 <cit>  proposed to use doublets made from gene pair combinations as inputs to cancer classification algorithms. the rationale behind the doublets is that biomolecular pathways may be stronger biomarkers for cancer as compared to individual genes
 <cit> . it is shown by chopra et al.
 <cit>  that upon using doublets, classification accuracy of several classification algorithms were consistently improved across different datasets compared to the same algorithms with the same number of single genes.

the paired t-test was conducted according to the method described in section "importance ordering and significance of the selected genes".

it is more realistic and promising to extend the doublets method to include multiple genes instead of only gene pairs since pathways often involve several crucial genes. for example, it has been reported that colorectal carcinoma is developed from the accumulation of genetic alterations, including chromosomal instability, gene mutations, and epigenetic abnormality after initiated by inactivation of the adenomatous polyposis coli tumour-suppressor pathway in a cell within the colon
 <cit> . the doublet method can describe the co-regulation patterns of two genes  with simple operations such as summation, difference, and multiplication. however, for a pathway involving three or more genes, such simple operations are not sufficient to describe the exponentially increasing number of patterns. in such cases, unknown and possibly complex interactions among genes will add additional features to the already n-p hard problem due to the small sample size and large number of features. if all genes and their interactions at all levels  were to be considered in the search space, the dimension of the search space is ultra-high. consider the colon cancer microarray data
 <cit>  for an example, the feature subset search space contains  <dig> features when all interactions are included while the sample size is only  <dig>  the colon cancer data contains the least number of genes among those we considered. the other cancer microarray datasets contain at least  <dig> genes and the sample size is mostly much less than  <dig>  including interactions among genes essentially makes the feature subset search space have infinite dimension.

available variable selection methods often fall into one of the three categories: filtering approaches, wrapper approaches, and embedded methods, where the latter class combines advantages of filters and wrappers. a filtering approach assesses the relevance of a feature subset without consulting with a classification algorithm, meanwhile a wrapper approach searches the optimal feature set that maximized the classification performance defined in terms of an evaluation function . it has been well accepted that wrapper approaches tend to provide better classification accuracy than the same algorithms with variable selection through filtering approaches . one limitation with the wrapper methods is that the search of optimal feature subsets for different classification algorithms needs to be conducted separately. consequently the feature subset selected by one algorithm does not generalize well to other algorithms
 <cit> . another limitation of the traditional wrapper methods is that the estimation of the evaluation function in feature subset selection may cause an overfitting problem when the sample size is small. note that a sample size of  <dig> with less than  <dig> features is deemed small in
 <cit> . the sample sizes in the data setting of this article are extremely small in presence of the near infinite dimensional feature search space. therefore, a simple application of traditional wrapper methods will incur a serious overfitting problem in current settings that leads to overly optimistic evaluation of the function and poor classification accuracy of the test data. a third limitation with the wrapper methods is that they need almost prohibitive computation by exhaustively searching through all possible combinations of gene sets. hill-climbing  and best-first search are such examples. they are only applicable when the number of features is small. partial search schemes, such as sequential forward selection, backward elimination, sequential floating backward elimination and random search, were often employed. a drawback with the partial search is that the algorithm may converge to a locally optimal solution instead of the global optimum. how to perform the search in the space of feature subsets has been studied for many years. seeking effective searching schemes in wrapper methods for data with a large number of features remains to be an active topic. consistent with our goal of considering gene interactions in the search space is a claim by cover and campenhout
 <cit>  that even for multivariate normally distributed features, no greedy search procedure that selects one feature at a time can find the best feature subset of a desired size; even an algorithm that adds the best pair and removes the worst single feature can fail.

embedded methods use internal information of the classification model to perform feature selection. support vector machine recursive feature elimination , known as an excellent feature ranking algorithm, is one of the embedded methods. in svm-rfe algorithm, the objective function j is half the l <dig> norm of the weight vector. in linear kernel svm, the weight vector can be calculated explicitly. in each iteration, the elimination of the feature with the least squared weight will cause the least effect on j
 <cit> . therefore, the weight vector is adopted as ranking criterion. to improve the efficiency of the algorithm, more features can be eliminated at each step
 <cit> . recently, liu et al.
 <cit>  extended svm-rfe to rbf kernel based on recursive feature elimination  by expanding nonlinear rbf kernel into its maclaurin series to calculate the weight vector.

ensemble methods are a class of popular methods that combine the effort of both classifier building and feature selection. an ensemble method uses multiple classifiers to produce a learner system. many mechanisms have been proposed for the creation of ensemble of classifiers in the literature. these include using different subsets of training data with a single learning technique, using different learning methods, or using different training parameters with a single learning method. one popular ensemble algorithm for datasets with a large number of features is the random subspace method
 <cit> , which represents a class of learning ensembles of weak classifiers to achieve good prediction accuracy. a random subspace method generates a large number of weak classifiers each trained on randomly chosen low dimensional subspace of the original input space. the ensemble output is obtained through majority voting or aggregation techniques . random space search methods effectively reduce the curse of dimensionality problem and are known to improve weak classifiers. random forest with the tree classifier, one of the random subspace search methods, has been demonstrated to have comparable performance to other classification methods including diagonal linear discriminant analysis , k nearest neighbor , and svm. open issues related to random subspace search methods include the conditions under which ensemble outperforms an individual classifier and how to determine a suitable ensemble size for a task with given computational requirements in terms of memory size and cpu time
 <cit> . other ensemble methods including bagging
 <cit>  which employs majority voting over results from a large number of bootstrap samples, and boosting
 <cit>  which construct classifiers on weighted versions of the training set which depends on previous classification results. bagging changes the distribution of the data stochastically and boosting changes the distribution of the data set adaptively based on the performance of previously created learners. skurichina and duin
 <cit>  demonstrated usefulness of bagging, boosting and the random subspace method in linear discriminant analysis. see
 <cit>  for a more detailed review of different ensemble methods. an ensemble combing bagging, boosting, rotation forest and random subspace version of the same learning algorithm using a voting methodology was also proposed in
 <cit> .

there are some unsettled challenges associated with svm-rfe and the random space search methods:  in svm-rfe and some of the random space search methods, the number of variables to be selected or the subspace dimension size is often set to be a fixed known parameter that requires the user to supply a value. the svm-rfe methods only provide a ranking of the features and rely on the user to specify the number of features to be selected. the weighted random subspace method by li and zhao
 <cit>  only considered the number of variables to be  <dig> in all their experiments. in practice, the number of informative features is unknown and need to be well estimated in order for the ensemble classifiers to have good performance. when the number of features used in the subspace ensemble is less than the true number that generated the response variable, the resulting classifiers may not capture the pattern. when the number of features used is more than the true one, the noisy features included would degrade the performance of the classifiers. how to reliably estimate the number of informative features remain to be a challenging problem.  the svm-rfe top ranked genes do not generalize well to other algorithms. the top ranked genes from svm-rfe may have poor classification performance when they are used with other classifiers. for instance, we obtained the list of ranked genes using the svm-rfe algorithm  on several cancer datasets. then we obtained the loocv accuracy with the top k genes in each data set using the linear discriminant classifier  and the naïve bayes classifier  for, k= <dig>  …,  <dig> . the loocv accuracy of the lda classifier using the top ranked genes provided by svm-rfe accuracy increases initially as the number of genes increases but then decreases and fluctuates a lot when the number of genes is between  <dig> and  <dig>  in many cases, the accuracy falls below 70% for different cancer microarray data sets. the nb classifier applied to the colon cancer data even has an overall decreasing pattern for loocv accuracy as the number of top ranked genes from svm-rfe increases.

in this article, we present a hybrid method to overcome some of the challenges mentioned above for ensemble and wrapper methods in the high dimensional feature selection with small sample sizes. that is, we consider how to avoid overfitting and provide effective search of informative genes in the infinite dimensional search space with limited resource , how to incorporate gene interactions in the search scheme to provide high classification accuracy for multiple algorithms, and how to determine the number of informative genes. we named our method binary matrix shuffling filter . bmsf is a data-driven guided random search algorithm implemented through binary matrix shuffling coupled with svm to perform filtering. the method introduces intermediate data matrices with binary values to convert the high-dimensional feature space search into the optimization problem of support vector machine regression  with the response being the matthew’s correlation coefficient and predictors being multiple factors each with two levels . extensive search of the large dimensional feature space including gene interactions is made feasible by reducing the number of models trained and exploiting predications more often to take full advantage of svr’s property of being time-consuming in training yet fast in prediction. as a result of less model training, the method smartly avoids the overfitting problem in small sample size and poor generalization drawback for the feature subset selection to other algorithms. the significance of our method includes:  the gene selection process considers the differentiating power of a gene conditional on the possibly nonlinear effects of other gene combinations. this allows complicated interactions among genes to be fully incorporated to reflect pathway changes during tumor progression.  bmsf often selects a relatively small number of genes that can accurately classify samples. this helps to achieve the goal of finding a minimal gene list to facilitate the search for new diagnostic tools and follow up study.  bmsf not only leads to improved loocv classification accuracy of svm that was wrapped with the variable selection process, but also results in better performance for multiple classifiers including naive bayes , linear discriminant analysis , quadratic discriminant analysis .  using the genes selected by the bmsf, svm, lda, and qda reached or exceeded the highest loocv accuracy reported in literature. this confirms that allowing interactions among features in the search space coupled with a manageable search scheme as in bmsf not only respect natural underlying bio-molecular reaction mechanisms but also provide better accuracy for biomarker selection.

RESULTS
here we present the performance of the proposed algorithm on  <dig> benchmark binary class expression datasets all related to human cancers, including central nervous system, colorectal, diffuse large b-cell lymphoma, leukemia, lung, and prostate tumors. the sample size and number of genes in each dataset are summarized in table
 <dig> 

final list of informative genes
as mentioned in the introduction, many classifiers and feature selection methods preprocess the data by applying a univariate test  to reduce the number of genes before conducting further refined procedures. the preprocessing briefly assesses the main effect of one gene without considering the effects of other genes. to be consistent with the literature and make it easy for comparison, we only report the selected genes after a preprocessing with the t-test and then applying our algorithm on the genes with p-value less than α= <dig> . table
 <dig> summarizes the selected genes, some of which are new while others can also be found in the literature. for example, consider the three informative genes found for the leukemia data. x <dig>  was found to play an important role in differentiating acute myelogenous leukemia  and acute lymphoblastic leukemia  by multiple authors
 <cit> . zyxin is a zinc-binding phosphoprotein that concentrates along the actin cytoskeleton and at focal adhesions that enable cells to adhere to the extracellular matrix and at which protein complexes involved in signal transduction assemble
 <cit> . y <dig> was reported as one of the top five genes with the highest selection frequency for classification of the leukemia data in yang et al.
 <cit> . high expression of the protein encoded by y <dig> is associated with poor prognosis and advanced stages in myelodysplastic syndrome which frequently transforms into aml
 <cit> . d <dig> was reported to be a discriminatory gene between aml and all in golub et al.
 <cit>  and was also among the top ranking genes reported in broberg
 <cit>  based on multiple gene ranking methods. real time pcr measurement of d <dig> was highly expressed from patients with aml
 <cit> . the protein encoded by d <dig> is a member of the large atp-dependent chromatin remodeling complex swi/snf family of proteins, which have helicase and atpase activities and are thought to regulate transcription of certain genes by altering the chromatin structure around those genes
 <cit> . in addition, this protein can bind brca <dig>  as well as regulate the expression of the tumorigenic protein cd <dig>  it is reported in medina et al.
 <cit>  that the protein encoded by d <dig> is a bona fide tumor suppressor and a major factor in lung tumorigenesis. the accession number of the genes for other datasets and their putative functions are listed in the additional file
1: table s <dig> 

leave-one-out cross-validation accuracy
the leave-one-out cross-validation  method was used to estimate the accuracy of classifiers. for each sample in the dataset, we used the rest of the samples in the dataset to serve as training data for model building and prediction for the class of this sample. for classifiers that have tuning parameters , the optimal parameters were first estimated with 5-fold cv using the training data and then used in the modeling. the classification accuracy of each dataset is the ratio of the number of the correctly classified samples to the total number of samples in that dataset, i.e., /total number of samples. to assess the generality of the selected informative genes, we also evaluate the performance of lda, qda, and nb using the selected genes in addition to svm in loocv accuracy. the results along with some methods published in the recent literature are summarized in figure
 <dig> and the numerical values  are given in additional file
1: table s <dig> 

note that all methods in additional file
1: table s <dig> use all the samples in each data set to perform feature selection. as pointed out by a reviewer, the feature selection needs to use the training data if the main purpose is to estimate the generalization error for future samples. in such case, which genes are selected and the number of genes selected are not the focus of the study since each training data in the loocv procedure leads to a separate list of genes. in the end, there are as many lists of genes as the sample size. these different lists of genes are not helpful when the purpose is to find informative genes indicative of the cancer status for a general population. as far as we know, all of the loocv accuracy reported in the literature for these  <dig> data sets used all the samples to select informative genes. we followed the same routine so that our results can be compared to those reported in the literature.

first, we consider the average accuracy for each algorithm across all cancers. using the informative genes selected with our method, the performance of svm has significantly improved from average accuracy  <dig> % to  <dig> % across nine datasets. lda and qda cannot be applied directly to high dimensional dataset since they require estimation of the high dimensional covariance matrices whose estimate is not accurate with limited sample sizes. with the genes selected by our method, bmsf-lda and bmsf-qda performed very well with average accuracy  <dig> % and  <dig> % across nine datasets. the top three methods with highest average accuracy among all those in additional file
1: table s <dig> are bmsf-svm, bmsf-lda and bmsf-qda. the k-tsp method used to give the best performance among those reported in literature for these nine datasets. it has  <dig> %,  <dig> %, and  <dig> % more classification errors on average than bmsf-lda, bmsf-qda, and bmsf-svm, respectively.

for individual cancer dataset, the bmsf either outperforms or has comparable performance to the best result reported in the literature. for the cns data, it can be seen from figure
 <dig> that bmsf-lda and bmsf-qda have equivalent accuracy as k-tsp, which are slightly higher than bmsf-nb and bmsf-svm. the rest of the classifiers have much lower accuracy. for the colon data, bmsf-svm has much better performance than the other algorithms. for the dlbcl data, bmsf-svm have comparable top performance to k-tsp and tsp, followed by bmsf-lda,svm, bmsf-qda, and sign-pam that have slightly lower accuracy. for the gcm data, all the classifiers with the bmsf selected variables have better accuracy than those using pam and tsp. in this case, svm also gives good result. for the leukemia dataset, bmsf -lda, bmsf-svm and svm have the best accuracy followed by bmsf-qda. for the lung data, bmsf-svm gives the best accuracy and bmsf-nb, svm, sign-pam, mul-pam and k-tsp fall slightly below. for the prostate <dig> data, bmsf-svm is ahead of the other algorithms; bmsf-lda and tsp have comparable performance that is slightly higher than bmsf-qda; bmsf-nb is not as good as the rest of the algorithms. for the prostate <dig> data, the four algorithms using bmsf selected variables obviously outperform the rest of the algorithms with a big margin. bmsf-svm, bmsf-lda, bmsf-qda and bmsf-nb had accuracies  <dig> %,  <dig> %,  <dig> % and  <dig> %, respectively, while direct application of svm only gave an accuracy of  <dig> % and the highest accuracy reported in the literature is  <dig> %. for the prostate <dig> data, bmsf-nb/qda/svm and svm all achieved 100% accuracy; bmsf-lda, mul-pam, k-tsp, and tsp have equivalent performance.

beyond accuracy comparison, we also report the number of genes used in each classifier in table
 <dig>  excluding the tsp method that can only consider pairs, the number of genes selected by our method is among the top two smallest sets over all data.

*results obtained in tan et al.
 <cit> †results obtained in chopra et al.
 <cit> .

Þresults obtained in zhang and deng
 <cit> Ѕresults obtained in dagliyan et al.
 <cit> ‡ results obtained using all genes.

ϒrefers to each of the classifiers using our selected genes.

average of absolute correlation among genes
in general the average of absolute correlation  among all genes in each dataset is not very high due to the fact that the majority of genes contain a lot of random noises. the genes retained after prescreening with the t-test typically have higher aac than that for the entire set of genes. this is because many co-regulated genes displaying changes during the cancer progression were retained after the prescreening. our algorithm tends to select gene sets with low redundancy. this can be seen in the top panel of figure
 <dig>  the aac after the filtering steps  and fine evaluation steps  is much smaller than the aac after the t-test for all but the leukemia, lung, and prostate <dig> datasets. the aac for leukemia increased at the end of our algorithm compared to that after the t-test. for the lung and prostate <dig> datasets, the aac did not change much. incidentally, these three datasets are the only ones that the best loocv accuracy is 100% and there are other algorithms that could have comparable performance as our algorithm does.

the bottom  <dig> panels of figure
 <dig> shows the relationship between the aac in the original datasets and nb, svm classifiers. in can be seen that the loocv accuracy of nb tends to be lower for datasets with higher original aac. bmsf-nb is not as influenced by the original aac on accuracy as the nb. for the svm classifier, original aac is related to its performance and there may be other factors affecting its performance. regardless of the magnitude of the original aac, bmsf-svm produced consistently much better results.

variability of results and joint effects of genes selected from different runs of bmsf
even though we did gene prescreening with t-test for consistency with most methods in the literature, our result indicated that the prescreening was not necessary. figure
 <dig> depicts the changing pattern in the number of genes and the corresponding best mcc values at each round of filtering steps in sections  <dig>  -  <dig> . the second and third columns correspond to the direct application of our method on the colon cancer data, while the last two columns correspond to the case with prescreening via t-test. even though the prescreening with the t-test can save some time by drastically reducing the number of genes at the beginning, it did not result in better mcc values. further fine evaluation with the procedure in section  <dig>  led to a final list of  <dig> genes with loocv accuracy  <dig> % for the case with prescreening  and  <dig> genes with loocv accuracy  <dig> % for the direct application . genes t <dig>  m <dig>  x <dig> and t <dig> deemed unimportant by the univariate t-test were among the final list of informative genes that gave excellent loocv classification accuracy. this suggests that the synergistic effects of these genes gave more differentiating power than their individual effect.

the paired t-test was conducted according to the method described in section "importance ordering and significance of the selected genes".

the list of informative genes may be different in separate runs of our algorithm. this is because the random matrix x generated in section  <dig>  may contain different combinations of genes to be included for filtering. this, however, should not significantly affect the accuracy of the algorithm as there are several rounds of random matrix generation and filtering to include as many combinations as possible. to support this argument, we obtained  <dig> lists of informative genes for the leukemia dataset by running our algorithm multiple times. the average accuracies are reported in table
 <dig>  the three lists of informative genes are totally different but the loocv accuracies are very close suggesting that this dataset has high redundancy. biological interpretation of such phenomenon would be that the selected genes are not the only genes that have gone through changes during cancer progression. two lists of the genes could each be a partial list of all genes showing differences between the cancer statuses. on the other hand, our algorithm is meant to select a parsimonious set of genes to achieve high loocv classification accuracy. the selection process is not designed to exhaustively find all genes that exhibit difference for the two categories.

from a biological point of view, it might be interesting to know whether the combination of the gene lists from different runs can produce similar classification accuracy. studying this can also infer the stability of bmsf algorithms in terms of commonly selected genes in different runs, and the joint effects of the genes on classification accuracy. as an example, we considered multiple runs of bmsf-svm on the leukemia data. we performed  <dig> runs  and randomly choose the results from k runs, where k =  <dig>   <dig> …,  <dig>   <dig>   <dig>   <dig> is the union size. the informative genes from the k runs are combined to make a new list of informative genes and loocv classification with this combined list of genes is conducted. this procedure of random selection of k runs and classification was repeated  <dig> times to assess the results of the joint effect. as the union size increases, the size of the combined list of genes increases. the average loocv accuracy from the  <dig> random selections of single run is  <dig> % with standard deviation  <dig> %. thirty random selections of two runs yield an average accuracy  <dig> % with standard deviation  <dig> %. for  <dig> random selections of three runs, the average accuracy using the combined genes from three runs is  <dig> % with standard deviation  <dig> %. for combined genes from more than four runs, the average accuracy from  <dig> random selections is at least  <dig> % with standard deviation less than  <dig> %. the results are summarized in figure
 <dig>  the left plot in figure
 <dig> gives the average loocv accuracy from  <dig> classifications using the  <dig> combined lists from random selections of k runs. the standard errors are indicated with the error bars. the right plot in figure
 <dig> gives the number of genes in the combined list as a function of the union size k. the error bars in the right plot gives the standard deviation from the  <dig> random selections. in summary, as the union size k increases, the average accuracy increases and the standard deviation of the accuracy decreases.

comparison with  <dig> other variable selection methods from rankgene and mrmr
to compare with other variable selection methods, we choose variables with bmsf and  <dig> other variable selection criteria available through rankgene at http://genomics <dig> bu.edu/yangsu/rankgene/ and mrmr at http://penglab.janelia.org/proj/mrmr/. rankgene provides eight ranking criteria including t-statistic, twoing rule, information gain, gini index, max minority, sum minority, sum of variances, and one-dimensional support vector machine. these criteria rank genes based on their capability to distinguish between the classes. the user is required to specify the number of genes to be selected. mrmr conducts minimum redundancy maximum relevance feature selection
 <cit> . mid and miq are two versions of mrmr highly recommended by peng et al.
 <cit>  and ding and peng
 <cit> . the mrmr can be used alone with a pre-specified number of variables to select or used with a classification algorithm to choose the set of variables that minimize cross-validation error.

mid and miq discretize the expression data into intervals for both noise reduction and ease of estimation of the mutual information. it has been reported in ding and peng
 <cit>  that discretization leads to better classification accuracy in mrmr than directly classifying the continuous expression data.

different discretization schemes were reported to give consistent results  when the expression values are transformed into  <dig> or  <dig> states by comparing to μ±kσ for k ranges from  <dig>  to  <dig>  where μ and σ are gene specific mean and standard deviation respectively. we take k= <dig> in our experiments such that the expression values greater than μ+σ were discretized into state 1; values between μ-σ and μ+σ are transformed to state 0; and values less than μ-σ are transformed to state − <dig> 

as rankgene does not provide a list of the informative genes as bmsf and mrmr do, we consider to base our comparisons on the loocv classification accuracy of four algorithms  using a pre-specified number of genes selected from each criterion. the error rate of mrmr tends to decrease as the number of genes increases
 <cit> . though bmsf and mrmr with a classification algorithm both report a list of informative genes, the numbers of genes are different. mrmr tends to need more genes to have low error rates. we set the number of genes for rankgene and mrmr to be the number selected according to bmsf on each of the nine cancer datasets. this allows us to examine if the space of informative genes generated by bmsf can be more easily classified than those generated by mrmr or rankgene.

the loocv accuracy for each of the four classification algorithms  is presented in the dotplot in figure
 <dig> . in the plot, the coordinate of a point in the horizontal axis indicates the accuracy. a point located to the right represents higher accuracy than a point located to the left. in most of the cases, the algorithms with variables selected by bmsf reach the highest loocv accuracy. an explanation that bmsf outperforms mrmr is that mrmr criteria maximize the average of all mutual information values between individual variable and class and minimize the average mutual information between two feature variables. if one variable is selected at a time in incremental search, it is shown in peng et al.
 <cit>  that mrmr is equivalent to maximizing the dependency between the features and the target class . in reality, however, multiple variables may be selected at the same time. in such case, joint effects among multiple variables are not taken into account in mrmr. as a result, the list of selected variables for any pre-specified number is not the most efficient feature set to distinguish the classes among methods that allow multiple variables to be selected at a time. this is the case in our comparison.

it has been a common belief that a wrapper type feature selector can yield high classification accuracy for a particular classifier and less generalization of the selected variables on other classifiers. bmsf does not have such limitation as the superiority of bmsf is not restricted to the svm that was wrapped with the algorithm for variable selection. for all four classification algorithms we considered , bmsf consistently outperforms the other variable selection criteria on each cancer data. the variables selected by bmsf yield comparable relative performance for different classifiers. this suggests that bmsf has the combined high accuracy property of wrapper feature selectors and generality property of filter type variable selectors.

comparison with random forest and svm-rfe
in this subsection, we report comparison with the random forest and svm-rfe algorithms. we apply the svm-rfe with linear kernel. the algorithm starts with all the features and eliminates one feature with the least squared weight at each step until all the features are ranked. in a consistent way to the treatment by
 <cit>  for application of svm-rfe, the original datasets are normalized by subtracting the mean of the corresponding gene vector from each gene’s expression data and then dividing by the corresponding standard deviation.

svm-rfe does not perform feature selection. instead, it only provides a list of ranked genes and the user need to decide the number of genes to be selected. for this reason, we cannot do a general fair comparison with it because our method decides the number of genes to be selected. so we only report a brief comparison, in which the svm-rfe was applied to the entire data and a list of ranked genes are obtained. we then plotted the loocv accuracy of lda, nb, and svm classifiers using k top ranked genes from svm-rfe for k =  <dig>  …,  <dig>  the accuracy of svm in general increases as more genes are included in the model. the accuracies of lda and nb may fluctuate drastically or decrease as the number of genes increases. this suggests that the gene ranking by svm-rfe is svm specific and may not generalize well to nb or lda. the plotted accuracy curve is an oracle situation which assumes the number of genes is a known priori. without knowing the number of genes to be used, additional variability will add to the loocv accuracy. for convenience of comparison, the loocv accuracy of the lda, nb, and svm classifiers using the genes selected by bmsf are shown in the plot with diamond-shaped points. the horizontal coordinates of the points are set to be the number of genes selected by bmsf. at each point location, the value on the curve of the same color as the diamond point is the svm-rfe accuracy if svm-rfe uses the same number of genes as bmsf. these values are given in additional file
1: table s <dig>  with the same number of genes as bmsf, the qda classifier yielded comparable average accuracy for svm-rfe and bmsf; the svm and nb classifiers had less average accuracy with the svm-rfe genes than with the bmsf genes; the lda classifier had slightly better accuracy with the svm-rfe genes than with the bmsf genes. we emphasize that the users are not recommended to interpret the results discussed here strictly since in practice it is necessary to estimate the number of genes in order to use the svm-rfe which will lead to additional variability on the results.

we also compare with the random forest gene selection algorithm genesrf provided by diaz-uriarte
 <cit> . genesrf was first applied to each dataset to select genes followed by an application of lda, qda, nb, and svm classifiers using the selected genes with loocv. since the genesrf algorithm also automatically determines the number of genes to be selected, we can compare bsmf and genesrf on a fair basis. the results are also reported in additional file
1: table s <dig>  the lda classifier with the genesrf selected genes has lower accuracy than the same classifier used with the bmsf selected genes for  <dig> out of the  <dig> datasets. the exception is the lung cancer data, for which the bmsf and genesrf genes have comparable performance. the average accuracy of the lda classifier over the  <dig> datasets with bmsf genes is  <dig> , while that for the genesrf genes is  <dig> . similarly, the qda and svm classifiers show better accuracy with the bmsf selected genes than with the genesrf selected genes for most of the datasets. the nb classifier showed comparable performance for the bmsf and genesrf genes. these results are expected since it is reported in díaz-uriarte and alvarez de andre’s  <dig> that the random forests feature selection has comparable performance to knn and svm. from our earlier comparison in section  <dig>  and additional file
1: table s <dig>  we know that bmsf outperforms svm in most of the datasets. in terms of the number of genes selected from an entire dataset, genesrf reported less numbers for  <dig> of the datasets than bmsf and both genesrf and bmsf selected no more than  <dig> genes in these  <dig> datasets. in the remaining  <dig> datasets, genesrf selected more genes than bmsf. in particular, genesrf selected  <dig> genes while bmsf only selected  <dig> genes for the gcm data.

upon the request of a reviewer, we also conducted comparison with repeated 10-fold cross validation. svm-rfe is excluded from the comparison since it does not determine the number of genes. we compare the gene selection using the bmsf and random forest by examining at the average accuracy from multiple runs. for each random partition of the data into 10-folds,  <dig> of the subsets are used as trainning and the rest is used as the test data. bmsf and random forest  are used for gene selection and svm, nb, lda, qda are used to build model with the training data and predict the class of the test data. the accuracy from each run of 10-fold cv is the proportion of correctly classified samples. we conducted  <dig> random partitions and reported the mean and standard deviation of the 10-fold cv accuracy from the  <dig> runs in table
 <dig>  the results show similar pattern to the loocv accuracy discussed earlier. that is, the nb classifier using the bmsf has comparable performance to that using genesrf selected genes. the svm and lda classifiers with the bmsf selected genes show much better performance than the same classifiers using the genesrf selected genes. the qda classifier with the genes selected by bmsf has only slightly better results than those selected by the genesrf.

bmsf and random forest  are used for gene selection and svm, nb, lda, qda are used to build model with the training data and predict the class of test data.

discussion
in this article, we proposed a new variable selection algorithm bmsf that can select a small set of feature variables taking into account variable interactions to provide highly accurate classification of the samples. the bmsf method automatically conducts multiple rounds of filtering and guided random search in the large feature subset space and reports the final list of informative genes. though the variable selection process is wrapped with svm, the variables selected have general applicability to multiple classification algorithms. svm, lda, qda and nb achieved superior loocv classification accuracy with the selected variables from bmsf compared to  <dig> other variable selection criteria.

different runs of bmsf may produce different lists of informative genes. this phenomenon corresponds to the fact that there are many possible prognostic genes. our goal is to find a minimal set of such genes that the combination of them can well differentiate the cancer status of the patients. additional genes that are important for sample classification can be obtained by carrying out multiple runs of bmsf. for example, the u82759_at selected from the second run of bmsf on the leukemia data encode hoxa <dig>  that has a higher level of expression in the samples from aml patients than in the all patients
 <cit> ; the m23197_at gene selected on the third run encodes cd <dig> antigen  that is a membrane protein. cd <dig> has been reported to be a target for antibody-based anti-leukaemic therapies as about 85%-90% of acute aml cases are considered to be cd <dig> positive
 <cit> . consequently, the resulting genes found from different runs can be combined to provide a larger gene set signature. our experiments showed that the joint effects of the genes from different runs typically increase the classification accuracy.

it may be interesting to combine multiple variable selection methods for better results. we comment that bmsf can be used along with other methods as long as the operation does not violate the principle of each method. consider prediction analysis of microarrays  by tibshirani et al.
 <cit>  for an example. pam is not a classification algorithm alone. instead, it performs both variable selection and classification. a thresholding parameter Δ needs to be estimated with cross-validation. each parameter value Δ corresponds to a set of genes selected for classification. if we apply pam to each cancer dataset with the variables already selected by bmsf, pam would conduct variable selection again using all samples within the subset of genes selected. this violates the principle of pam when only a small number of variables remain in the model because pam employs the thresholding idea to identify sparse signals out of a large amount of noise . however, pam may be applied first to the original dataset since it tends to select many more variables than bmsf. for example, the pam application in leukemia dataset with the best Δ value yields a model with more than  <dig> variables. in such a case, bmsf can still be applied to further reduce the number of variables.

as with all feature selection results in microarray data, the variables selected may or may not be a subset of cancer progression signature. future validation of clinical relevance of the selected genes through multiple external cancer cohorts composes another line of work . they mainly address how to assess a gene set signature’s prognostic value of a predefined size relative to random gene sets. boutros et al.
 <cit>  also presented a nonlinear steepest gradient descent  algorithm to identify prognostic gene signatures from their 158-gene rt-pcr training dataset of  <dig> patients. as is discussed in the introduction the greedy search strategy with wrapper method is not tractable in current setting with high dimensional feature space. our work in this article provides a useful tool in this regard.

currently, this software was only implemented to perform two-class classification. in recent years, svm has been extended to perform multi-class classification and regression. further directions along this line can consider extending the method to perform variables selection when there are multiple classes or when the response variable is continuous. we anticipate such extension to be very helpful to improve the accuracy of multi-class cancer classification and genome wide association study.

CONCLUSIONS
in summary, considering the discriminating power of a gene after adjusting for the joint effect of many other genes and their possible interactions can improve the classification accuracy. we provided an effective algorithm bmsf to carry out the search of important variables in the high dimensional feature space allowing interactions of a large number of genes. the algorithm automatically reports a list of very small number of selected genes that renders high classification accuracy for multiple classifiers. our method not only overcomes the difficulty associated with the search schemes of traditional wrapper methods in large dimensional search space but also has good generality for the genes selected. this is confirmed by testing our method on the  <dig> benchmark binary class gene expression datasets all related to human cancers.

