BACKGROUND
two types of reconstruction method for directed networks have been developed and applied to a variety of experimental datasets. these methods are either based on bayesian scores  <cit>  or rely on the identification of structural independencies, which correspond to missing edges in the underlying network  <cit> .

bayesian inference approaches have the advantage of allowing for quantitative comparisons between alternative networks through their bayesian scores but they are limited to rather small causal graphs due to the super-exponential space of possible directed graphs to sample  <cit> . hence, bayesian inference methods typically require either suitable prior restrictions on the structures  <cit>  or heuristic search strategies such as hill-climbing algorithms .

by contrast, structure learning algorithms based on the identification of structural constraints typically run in polynomial time on sparse underlying graphs. these so-called constraint-based approaches, such as the pc  <cit>  and ic  <cit>  algorithms, do not score and compare alternative networks. instead they aim at ascertaining conditional independencies between variables to directly infer the markov equivalent class of all causal graphs compatible with the available observational data. yet, these methods are not robust to sampling noise in finite datasets as early errors in removing edges from the complete graph typically trigger the accumulation of compensatory errors later on in the pruning process. this cascading effect makes the constraint-based approaches sensitive to the adjustable significance level α, required for the conditional independence tests. in addition, traditional constraint-based methods are not robust to the order in which the conditional independence tests are processed, which prompted recent algorithmic improvements intending to achieve order-independence  <cit> .

in this paper, we report a novel network reconstruction method, which exploits the best of these two types of structure learning approaches. it combines constraint-based and bayesian frameworks to reliably reconstruct graphical models despite inherent sampling noise in finite observational datasets. to this end, we have developed a robust information-theoretic method to confidently ascertain structural independencies in causal graphs based on the ranking of their most likely contributing nodes. conditional independencies are derived using an iterative search approach that identifies the most significant indirect contributions to all pairwise mutual information between variables. this local optimization algorithm, outlined below, amounts to iteratively subtracting the most likely conditional 3-point information from 2-point information between each pair of nodes. the resulting network skeleton is then partially directed by orienting and propagating edge directions, based on the sign and magnitude of the conditional 3-point information of unshielded triples. identifying structural independencies within such a maximum likelihood framework circumvents the need for adjustable significance levels and is found to be more robust to sampling noise from finite observational data, even when compared to constraint-based methods intending to resolve the order-dependence on the variables  <cit> .



constraint-based methods
constraint-based approaches, such as the pc  <cit>  and ic  <cit>  algorithms, infer causal graphs from observational data, by searching for conditional independencies among variables. under the markov and faithfulness assumptions, these algorithms return a complete partially directed acyclic graph  that represents the markov equivalent class of the underlying causal structure  <cit> . they proceed in three steps detailed in algorithm 1: 
1) inferring unnecessary edges and associated separation sets to obtain an undirected skeleton.

2) orienting unshielded triples as v-structures if their middle node is not in the separation set .

3) propagating as many orientations as possible following propagation rules , which prevents the orientation of additional v-structures  and directed cycles   <cit> .



however, as previously stated, the sensitivity of the constraint-based methods to the adjustable significance level α used for the conditional independence tests and to the order in which the variables are processed  favors the accumulation of errors when the search procedure relies on finite observational data.

in this paper, we aim at improving constraint-based methods, algorithm  <dig>  by uncovering the most reliable conditional independencies supported by the  available data, based on a quantitative information theoretic framework.

maximum likelihood methods
the maximum likelihood ℒg is related to the cross entropy h=−∑{xi}plog) between the “true” probability distribution p from the data d and the approximate probability distribution q=∏ip generated by the bayesian network g with specific parent nodes {paxi} for each node xi, leading to  <cit> , 
  ℒg=e−nh=e−n∑ihxi|paxi 

where ∑ih is the  entropy of the underlying causal graph. this enables to score and compare alternative models through their maximum likelihood ratio as, 
  ℒg′ℒg=e−n∑ihxi|paxi′−hxi|paxi 

note, in particular, that the significance level of the maximum likelihood approach is set by the number n of independent observational data points, as detailed in the methods section below.

methods
information theoretic framework
inferring isolated v-structures vs non-v-structures from 3-point and 2-point information
applying the previous likelihood definition, eq.  <dig>  to isolated v-structures  and markov equivalent non-v-structures , one obtains, 
  ℒv=e−nh+h+h=e−nh+i fig.  <dig> inference of v-structures versus non-v-structures by 3-point information from observational data. a isolated v-structures are predicted for i <  <dig>  and  isolated non-v-structures for i >  <dig>  e generalized v-structures are predicted for i <  <dig> and  generalized non-v-structures for i >  <dig>  in addition, as i are invariant upon xyz permutations, the global orientation of v-structures and non-v-structures also requires to find the most likely base of the xyz triple. choosing the base xy with the lowest conditional mutual information, i.e., i= minxyz), is found to be consistent with the data processing inequality expected for  non-v-structures in the limit of infinite dataset, see main text. in practice, given a finite dataset, the inference of  v-structures versus non-v-structures can be obtained by replacing 3-point and 2-point information terms i and i by shifted equivalents, i
′ and i
′, including finite size corrections, see text 



where i=h+h−h is the 2-point mutual information between x and y, and, 
  ℒnv=e−nh+h+h=e−nh+i 

where i=h+h−h is the conditional mutual information between x and y given z. hence, one obtains the likelihood ratio, 
  ℒvℒnv=e−ni−i=e−ni 

where we introduced the 3-point information function, i=i−i, which is in fact invariant upon permutations between x,y and z, as seen in terms of entropy functions, 
  i=h+h+h−h−h−h+h 

as long recognized in the field  <cit> , 3-point information, i, can be positive or negative <i), unlike 2-point mutual information, which are always positive, i≥ <dig> 

more precisely, eq.  <dig> demonstrates that the sign and magnitude of 3-point information provide a quantitative estimate of the relative likelihoods of isolated v-structures versus non-v-structures, which are in fact independent of their actual non-connected bases xy, xz or yz, 
  ℒvℒnv=ℒvℒnv=ℒvℒnv=e−ni 

hence, a significantly negative 3-point information, i< <dig>  implies that a v-structure is more likely than a non-v-structure given the observed correlation data. conversely, a significantly positive 3-point information, i> <dig>  implies that a non-v-structure model is more likely than a v-structure model.

yet, as noted above, 3-point information, i, being symmetric by construction, it cannot indicate how to orient v-structures or non-v-structures over the xyz triple. to this end, it is however straightforward to show that the most likely base  of the local v-structure or non-v-structure corresponds to the pair with lowest mutual information, e.g., i=minxyz), as shown by the likelihood ratios, 
  ℒvℒv=ℒnvℒnv=e−nie−ni 

note, in particular, that choosing the base with the lowest mutual information is consistent with the data processing inequality expected for non-v-structures, fig. 1b–d.

hence, combining 3-point and 2-point information allows to determine the likelihood and the base of isolated v-structures versus non-v-structures. but how to extend such simple results to identify local v-structures and non-v-structures embedded within an entire graph g?

inferring embedded v-structures vs non-v-structures from conditional 3-point and 2-point information
to go from isolated to embedded v-structures and non-v-structures within a dag g, we will consider the markov equivalent cpdag of g and introduce generalized v-structures and non-v-structures, fig. 1e–h. we will demonstrate that their relative likelihood, given the available observational data, can be estimated from the sign and magnitude of a conditional 3-point information, i, eq.  <dig>  this will extend our initial result valid for isolated v-structures and non-v-structures, eq.  <dig> 

let’s consider a pair of non-neighbor nodes x,y with a set of upstream nodes {ui}n, where each node ui has at least one direct connection to x  or y  or to another upstream node uj∈{ui}n  or only undirected links to these nodes . thus, given x,y and a set of upstream nodes {ui}n, any additional node z can either be: 
i) at the apex of a generalized v-structure, if all existing connections between x, y, {ui}n and z are directed and point towards z, fig. 1e, or else,

ii)z has at least one undirected link with x, y or one of the upstream nodes ui  or at least one directed link pointing towards these nodes , fig. 1f–h. in such a case, z might contribute to the mutual information i and should be included in the set of upstream nodes {ui}n, thereby defining a generalized non-v-structure, figs. 1f–h.



then, similarly to the case of an isolated v-structure , the maximum likelihood ℒv of a generalized v-structure pointing towards z from a base xy with upstream nodes {ui}n can be expressed as, 
  ℒv=e−nh+h+h+h=e−nh+i 

where i is the conditional mutual information between x and y given {ui}, i=h+h−h−h.

likewise, the maximum likelihood ℒnv of a generalized non-v-structure of base xy with upstream nodes {ui}n and z can be expressed as, 
  ℒnv=e−nh+h+h=e−nh+i 

where i=h+h−h−h is the conditional mutual information between x and y given z and {ui}. hence, 
  ℒvℒnv=e−ni 

where we introduced the conditional 3-point information, i=i−i.

hence, a significantly negative conditional 3-point information, i< <dig>  implies that a generalized v-structure is more likely than a generalized non-v-structure given the available observational data. conversely, a significantly positive conditional 3-point information, i> <dig>  implies that a generalized non-v-structure model is more likely than a generalized v-structure model.

yet, as the conditional 3-point information, i, is in fact invariant upon permutations between x,y and z, it cannot indicate how to orient embedded v-structures or non-v-structures over the xyz triple, as already noted in the case of isolated v-structures and non-v-structures, above.

however, the most likely base  of the embedded v-structure or non-v-structure corresponds to the least correlated pair conditioned on {ui}, e.g., i=minxyz), as shown with the following likelihood ratios, 
  ℒvℒv=ℒnvℒnv=e−nie−ni 

note, in particular, that choosing the base with the lowest conditional mutual information, e.g., i=minxyz), is consistent with the data processing inequality expected for the generalized non-v-structure of fig. 1f–h, i≤min,i), as shown below for i and i, by subtracting i on each side of the inequality i≤i, leading to, 
  i≤i≤i+i≤ii≤i 

where we have used the chain rule, i=i+i, before adding i on each side of the inequality. the corresponding inequality holds between i and i, implying the data processing inequality.

finite size corrections of maximum likelihood
maximum likelihood ratios, such as eq.  <dig>  suggest that 1/n sets the significance level of the maximum likelihood approach, as h−h≫1/n should imply a significant improvement of the underlying model g′ over g. in practice, however, there are o/n) corrections coming from the proper normalization of maximum likelihoods , 
  ℒg=e−n∑ihxi|paxiz 

the model g can then be compared to the alternative model g∖x→y with one missing edge x→y using the maximum likelihood ratio, 
  ℒg∖x→yℒg=e−nizz 

where i=h−h.

then, following the rationale of constraint-based approaches, eq.  <dig> can be reformulated by replacing the parent nodes {pay}∖x with an unknown separation set {ui} to be learnt simultaneously with the missing edge candidate xy, 
  ℒg∖xy|{ui}ℒg=e−ni+kx;y|{ui} 

  kx;y|{ui}=logz/z 

where the factor kx;y|{ui}> <dig> tends to limit the complexity of the models by favoring fewer edges. namely, the condition, i<kx;y|{ui}/n, implies that simpler models compatible with the structural independency, x ⊥ ⊥ y|{ui}, are more likely than model g, given the finite available dataset. this replaces the ‘perfect’ conditional independency condition, i= <dig>  valid in the limit of an infinite dataset, n→∞. a common complexity criteria in model selection is the bayesian information criteria  or minimal description length  criteria  <cit> , 
  kx;y|{ui}mdl=12∏iruilogn 

where rx,ry and rui are the number of levels of the corresponding variables. the mdl complexity, eq.  <dig>  is simply related to the normalisation constant of the distribution reached in the asymptotic limit of a large dataset n→∞ . however, this limit distribution is only reached for very large datasets in practice.

alternatively, the normalisation of the maximum likelihood can also be done over all possible datasets including the same number of data points to yield a  normalized maximum likelihood  criteria  <cit>  and its decomposable  <cit>  and xy-symmetric version, kx;y|{ui}nml, defined in the appendix.

then, incrementing the separation set of xy from {ui} to {ui}+z leads to the following likelihood ratio, 
  ℒg∖xy|{ui},zℒg∖xy|{ui}=eni+kx;y;z|{ui} 

with i=i−i and where we introduced a 3-point conditional complexity, kx;y;z|{ui}, defined similarly as the difference between the 2-point conditional complexities, 
  kx;y;z|{ui}=kx;y|{ui},z−kx;y|{ui} 

however, unlike 3-point information, i, 3-point complexities are always positive, kx;y;z|{ui}> <dig>  provided that there are at least two levels for each implicated node ℓ∈x,y,z,{ui}, i.e.rℓ≥ <dig> 

hence, we can define the shifted 2-point and 3-point information in eqs.  <dig> &  <dig> for finite datasets as, 
  i′=i−kx;y|{ui}n 

  i′=i+kx;y;z|{ui}n 

this leads to the following maximum likelihood ratios equivalent to eqs.  <dig> &  <dig> for v-structure over non-v-structure and between alternative bases, 
  ℒvℒnv=e−ni′ 

  ℒvℒv=ℒnvℒnv=e−ni′e−ni′ 

hence, given a finite dataset, a significantly negative conditional 3-point information, corresponding to i′< <dig>  implies that a v-structure x→z←y is more likely than a non-v-structure provided that the structural independency, x ⊥ ⊥ y|{ui}, is also confidently established as, i′< <dig>  by contrast, a significantly positive conditional 3-point information corresponds to i′> <dig> and implies that a non-v-structure model is more likely than a v-structure model, given the available observational data.

probability estimate of indirect contributions to mutual information
the previous results enable us to estimate the probability of a node z to contribute to the conditional mutual information i, by combining the probability, pnv, that the triple xyz is a generalized non-v-structure conditioned on {ui} and the probability, pb, that its base is xy, where, 
  pnv=ℒnvℒnv+ℒv 

  pb=ℒnvℒnv+ℒnv+ℒnv 

that is, using eqs.  <dig> &  <dig> including finite size corrections of the maximum likelihoods, 
  pnv=11+e−ni′ 

  pb=11+e−ni′e−ni′+e−ni′e−ni′ 

then, various alternatives to combine pnv and pb exist to estimate the overall probability that the additional node z indirectly contributes to i. one possibility is to choose the lower bound slb of pnv and pb, since both conditions need to be fulfilled to warrant that z indeed contributes to i, 
  slb=minpnv,pb 

the pair of nodes xy with the most likely contribution from a third node z can then be ordered according to their rank r defined as, 
  r=maxzslb 

and z can be iteratively added to the set of contributing nodes  of the top link xy=argmaxxyr to progressively recover the most significant indirect contributions to all pairwise mutual information in a causal graph, as outlined below.

robust inference of conditional independencies using the 3off <dig> scheme
the previous results can be used to provide a robust inference method to identify conditional independencies and, hence, reconstruct the skeleton of underlying causal graphs from finite available observational data. the approach follows the spirit of constraint-based methods, such as the pc or ic algorithms, but recovers conditional independencies following an evolving ranking of the network edges, r, defined in eq.  <dig> 

all in all, this amounts to perform a generic decomposition for each mutual information term, i, by introducing a succession of node candidates, u <dig> u <dig> …, un, that are likely to contribute to the overall mutual information between the pair x and y, as, 
  i=i+i=i+i+……+i+i 

or equivalently between the shifted 2-point and 3-point information terms including finite size corrections , 
  i′=i′+i′+…+i′+i′ 

hence, given a significant mutual information between x and y, i′> <dig>  we will search for possible structural independencies, i.e.i′< <dig>  by iteratively “taking off” conditional 3-point information terms from the initial 2-point  information, i′, as 
  i′=i′−i′−i′−…−i′ 

and similarly with non-shifted 2-point and 3-point information, 
  i=i−i−i−…−i 

3off <dig> algorithm
the 3off <dig> scheme can be used to devise a two-step algorithm , inspired by constraint-based approaches, to first reconstruct network skeleton  before combining orientation and propagation of edges in a single step based on likelihood ratios .

reconstruction of network skeleton
the 3off <dig> scheme will first be applied to iteratively remove edges with maximum positive contributions, i′> <dig>  corresponding to the most likely generalized non-v-structures , while minimizing simultaneously the remaining 2-point information, i′ , consistently with the data processing inequality. such 3off <dig> scheme  will therefore progressively lower the conditional 2-point information terms, i′>⋯>i′>i′ and might ultimately result in the removal of the corresponding edge, xy, but only when a structural independency is actually found, i.e.i′< <dig>  as in constraint-based algorithms for a given significance level α. yet, the skeleton obtained with the 3off <dig> scoring approach is expected to be more robust to finite observational data than the skeleton obtained with pc or ic algorithms, as the former results only from statistically significant 3-point contributions, i′> <dig>  based on their quantitative 3off <dig> ranks, r.

the best results on benchmark networks using these quantitative 3off <dig> ranks are obtained with the nml score . the mdl score leads to equivalent results, as expected, in the limit of very large datasets . however, with smaller datasets, the most reliable results with the mdl score are obtained using non-shifted instead of shifted 2-point and 3-point information terms in the 3off <dig> rank of individual edges, eq.  <dig>  this is because the mdl complexity tends to underestimate the importance of edges between nodes with many levels . for finite datasets, it easily leads to spurious conditional independencies, i′< <dig>  when using shifted 2-point and 3-point information, eq.  <dig>  whereas using non-shifted information in the 3off <dig> ranks  tends to limit the number of false negatives as early errors in {ui} can only increase i≥ <dig>  in the end, in eq.  <dig> 

orientation of network skeleton
the skeleton and the separation sets resulting from the 3off <dig> iteration step  can then be used to orient the edges and to propagate orientations to the unshielded triples. however, while the constraint-based methods distinguish the v-structures orientation step  from the propagation procedure , the 3off <dig> algorithm intertwines these two steps based on the respective likelihood scores of individual v-structures and non-v-structures .

as stated earlier, the magnitude and sign of the conditional 3-point information, i , indicate if a non v-structure is more likely than a v-structure. hence, all the unshielded triples can be ranked by the absolute value of their conditional 3-point information, that is, in decreasing order of their likelihood of being either a v-structure or a non-v-structure. as detailed in the step  <dig> of algorithm  <dig>  the most likely v-structure is used to set the first orientations, following r <dig> orientation rule. the possible propagations are then performed, following r <dig> propagation rule, starting from the unshielded triple having the most positive conditional 3-point information. the following most likely v-structure is considered when no further propagation is possible on unshielded triples with greater absolute 3-point information. if conflicting orientations arise , the less likely v-structure and its possible propagations are ignored.

note that we only implement the r <dig> and r <dig> propagation rules, which are applied in decreasing order of likelihood. in particular, we do not consider propagation rules r <dig> and r <dig> which are not associated to likelihood scores but enforce the hypothesis of acyclic constraint.

as for the 3off <dig> skeleton reconstruction, the orientation/propagation step of 3off <dig> allows for a robust discovery of orientations from finite observational data as it relies on a quantitative framework of likelihood ratios taken in decreasing order of their statistical significance. during this step, 3off <dig> recovers and propagates as many orientations as possible in an iterative procedure following the decreasing ranks of the unshielded triples based on the absolute value of their conditional 3-point information, |i′|.

RESULTS
tests on benchmark graphs
we have tested the 3off <dig> network reconstruction approach to learn benchmark causal graphs containing  <dig> to  <dig> nodes, figs.  <dig>   <dig>   <dig>   <dig> and  <dig>  the results are evaluated against other methods in terms of precision , prec=tp/, recall or sensitivity , rec=tp/, as well as f-score =2×prec×rec/ for increasing sample size n= <dig> to  <dig>  data points.
fig.  <dig> child network. . precision, recall and f-score for skeletons  and cpdags . the results are given for aracne , pc , bayesian hill-climbing  and 3off <dig> 

fig.  <dig> alarm network. . precision, recall and f-score for skeletons  and cpdags . the results are given for aracne , pc , bayesian hill-climbing  and 3off <dig> 

fig.  <dig> insurance network. . precision, recall and f-score for skeletons  and cpdags . the results are given for aracne , pc , bayesian hill-climbing  and 3off <dig> 

fig.  <dig> barley network. . precision, recall and f-score for skeletons  and cpdags . the results are given for aracne , pc , bayesian hill-climbing  and 3off <dig> 

fig.  <dig> hepar ii network. . precision, recall and f-score for skeletons  and cpdags . the results are given for aracne , pc , bayesian hill-climbing  and 3off <dig> 





we also define additional precision, recall and f-scores taking into account the edge orientations of the predicted networks against the corresponding cpdag of the benchmark networks. this amounts to label as false positives, all true positive edges of the skeleton with different orientation/non-orientation status as the cpdag reference, tpmisorient, leading to the orientation-dependent definitions tp′=tp−tpmisorient and fp′=fp+tpmisorient with the corresponding cpdag precision, recall and f-scores taking into account edge orientations.

the alternative inference methods used for comparison with 3off <dig> are the pc algorithm  <cit>  implemented in the pcalg package  <cit>  and bayesian inference using the hill-climbing heuristics implemented in the bnlearn package  <cit> . in addition, we also compare the skeleton of 3off <dig> to the unoriented output of aracne  <cit> , an information-based inference approach, which iteratively prunes links with the weakest mutual information based on the data processing inequality. we have used the aracne implementation of the minet package  <cit> . for each sample size, 3off <dig>  aracne, pc and the bayesian inference methods have been tested on  <dig> replicates. figures  <dig>   <dig>   <dig>   <dig> and  <dig> give the average results over these multiple replicates when comparing the cpdag  of the reconstructed network  to the cpdag  of the benchmark network.

for each method, the plots presented in figs.  <dig>   <dig>   <dig>   <dig> and  <dig> are those obtained for the parameters that give overall the best results over the five reconstructed benchmark networks . in particular, we used the stable implementation of the pc algorithm, as well as the majority rule for the orientation and propagation steps  <cit> . pc’s results are shown on figs.  <dig>   <dig>   <dig>   <dig> and  <dig> for α= <dig> . decreasing α tends to improve the skeleton precision at the expense of the skeleton recall, leading in fact to worse skeleton f-scores for finite datasets, e.g.n≤ <dig> . the same trend is observed for cpdag f-scores taking into account edge orientations, with best cpdag scores at small sample sizes, obtained for larger α, e.g.n≤ <dig>  aracne threshold parameters for minimum difference in mutual information is set to ε= <dig>  as small positive values typically worsen f-scores . bayesian inference are obtained using bic/mdl scores and hill-climbing heuristics with  <dig> random restarts  <cit>  . finally, the best 3off <dig> network reconstructions are obtained using nml scores with shifted 2-point and 3-point information terms in the rank of individual edges, see methods. using mdl scores, instead, leads to equivalent results, as expected, in the limit of very large datasets . however, with smaller datasets, the most reliable results with mdl scores are obtained using non-shifted instead of shifted 2-point and 3-point information terms in the 3off <dig> rank of individual edges, as discussed in methods .

all in all, we found that the 3off <dig> inference approach typically reaches better or equivalent f-scores for all dataset sizes as compared to all other tested methods, i.e. aracne, pc and bayesian inference, as well as the max-min hill-climbing  hybrid method  <cit>  . this is clearly observed both on the skeletons  and even more clearly when taking the predictions of orientations into account .

applications to the hematopoiesis regulation network
the reconstruction or reverse-engineering of real regulatory networks from actual expression data has already been performed on a number of biological systems . here, we apply the 3off <dig> approach on a real biological dataset related to hematopoiesis. transcription factors play a central role in hematopoiesis, from which derive the blood cell lineages. as suggested in previous studies, changes in the regulatory interactions among transcription factors  <cit>  or their overexpression  <cit>  might be involved in the development of t-acute lymphoblastic leukaemia . the key role of the hematopoiesis and the potentially serious consequences of its disregulations emphasize the need to accurately establish the complex interactions between the transcription factors involved in this critical biological process.

the dataset we have used for this analysis  <cit>  consists of the single cell expressions of  <dig> transcription factors, known for their role in hematopoiesis. five hundred ninety seven single cells representing  <dig> different types of hematopoietic progenitors have been included in the analysis . we reconstructed the corresponding network with the 3off <dig> inference method, fig.  <dig>  and four other available approaches, namely, pc  <cit>  implemented in the pcalg package  <cit> , bayesian inference using hill-climbing heuristics as well as the max-min hill-climbing  hybrid method  <cit> , both implemented in the bnlearn package  <cit> , and, finally, aracne  <cit>  implemented in the minet package  <cit>  .
fig.  <dig> hematopoietic subnetwork reconstructed by 3off <dig>  the dataset  <cit>  concerns  <dig> transcription factors,  <dig> single cells,  <dig> different hematopoietic progenitor types. red and blue edges correspond to experimentally proven activations and repressions, respectively as reported in the literature , while grey links indicate regulatory interactions for which no clear evidence has been established so far. thinner arrows underline 3off <dig> misorientations

nml
α
 = 10
−1
α
 = 10
−2
bde
bic
bde
bic
ε=0
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
⇏
11
9
7
6
6
10
8
8
0
2
4
5
5
1
3
3


3off <dig> uncovers all  <dig> interactions for which specific experimental evidence has been reported in the literature  as well as  <dig> additional links . by contrast, randomization of the actual data across samples for each tf leads to only  <dig>  spurious interactions on average between the  <dig> tfs, instead of the  <dig> inferred edges from the actual data, and  <dig>  spurious interactions on average, instead of the  <dig> interactions predicted among the  <dig> tfs involved in known regulatory interactions, fig.  <dig>  this suggests that around 10– <dig> % of the predicted edges might be spurious, due to inevitable sampling noise in the finite dataset. in particular, the 3off <dig> inference approach successfully recovers the relationships of the regulatory triad between gata <dig>  gfi1b and gfi <dig> as described in  <cit>  and reports correct orientations for the edges involving gata <dig> . the network reconstructed by 3off <dig> also correctly infers the regulations of pu. <dig> by gfi <dig>  <cit> , gfi <dig> by lyl <dig>  <cit> , meis <dig> by ldb <dig>  <cit> , and the regulations of lyl <dig> by ldb <dig>  <cit>  and erg  <cit> . finally, the interactions   <cit> ,   <cit>  and   <cit>  are correctly inferred, however, with opposite directions as reported in the literature. yet, overall 3off <dig> outperforms most of the other methods tested for the reconstruction of the hematopoietic regulatory subnetwork . only the bayesian hill-climbing method using a bde score leads to comparable results by retrieving  <dig> out of  <dig> interactions and correctly orienting  <dig> of them. these encouraging results from the 3off <dig> reconstruction method on experimentally proven regulatory interactions  could motivate further investigations on novel regulatory interactions awaiting to be tested for their possible role in hematopoiesis .

CONCLUSIONS
in this paper, we propose to improve constraint-based network reconstruction methods by identifying structural independencies through a robust quantitative score-based scheme limiting the accumulation of early fn errors and subsequent fp compensatory errors. in brief, 3off <dig> relies on information theoretic scores to progressively uncover the best supported conditional independencies, by iteratively “taking off” the most likely indirect contributions of conditional 3-point information from every 2-point  information of the causal graph.

earlier hybrid methods have also attempted to improve network reconstruction by combining the concepts of constraint-based approaches with the robustness of bayesian scores . in particular  <cit> , have proposed to exploit an intrinsic weakness of the pc algorithm, its sensitivity to the order in which conditional independencies are tested on finite data, to rank these different order-dependent pc predictions with bayesian scores. more recently  <cit> , have also combined constraint-based and bayesian approaches by first identifying both parents and children of each node of the underlying graphical model and then performing a greedy bayesian hill-climbing search restricted to the identified parents and children of each node. this max-min hill-climbing  approach tends to have a high precision in terms of skeleton but a more limited sensibility, leading overall to lower skeleton and cpdag f-scores than 3off <dig> and bayesian hill climbing methods on the same benchmark networks, figures s21-s <dig>  interestingly, however, the mmhc approach is among the fastest network reconstruction approaches, figure s <dig>  allowing for scalability to large network sizes  <cit> .

the 3off <dig> algorithm is expected to run in polynomial time on typical sparse causal networks with low in-degree, just like constraint-based algorithms.however, in practice and despite the additional computation of conditional 2-point and 3-point information terms, we found that the 3off <dig> algorithm runs typically faster than constraint-based algorithms for large enough samples, by avoiding the cascading accumulation of errors that inflate the combinatorial search of conditional independencies in traditional constraint-based approaches. instead, we found that 3off <dig> running time displays a similar trend as bayesian hill-climbing heuristic methods, figs.  <dig>   <dig>   <dig>   <dig> and  <dig> 

all in all, the main computational bottleneck of the present 3off <dig> scheme pertains to the identification of the best contributing nodes at each iteration. in the future, it could be interesting to investigate whether a more stochastic version of this 3off <dig> method, based on choosing one significant conditional 3-point information instead of the best one, might simultaneously accelerate the network reconstruction and circumvent possible locally trapped suboptimal predictions through stochastic resampling.

finally, another perspective for practical applications will be to include the possibility of latent variables and bidirected edges in reconstructed networks.

appendix
complexity of graphical models
the complexity kg,d of a graphical model is related to the normalization constant z of its maximum likelihood as kg,d=logz, 
  ℒg=e−nhz=e−nh−kg,d 

for bayesian networks with decomposable entropy, i.e.h=∑ih, it is convenient to use decomposable complexities, kg,d=∑ikxi|paxi, 
  ℒg=e−n∑ihxi|paxi−∑ikxi|paxi 

such that the comparison between alternative models g and g∖x→y  leads to a simple local increment of the score, 
  ℒg∖x→yℒg=e−ni+Δky|pay∖x 

  i=h−h≥ <dig> 

  Δky|{pay}∖x=ky|{pay}−ky|{pay}∖x≥ <dig> 

a common complexity criteria in model selection is the bayesian information criteria  or minimal description length  criteria  <cit> , 
  ky|{pay}mdl=12∏jpayrjlogn 

  Δky|{pay}∖xmdl=12∏jpay∖xrjlogn 

where rx,ry and rj are the number of levels of each variable, x, y and j. the mdl complexity, eq.  <dig>  is simply related to the normalisation constant reached in the asymptotic limit of a large dataset n→∞ . the mdl complexity can also be derived from the stirling approximation on the bayesian measure  <cit> . yet, in practice, this limit distribution is only reached for very large datasets, as some of the least-likely ∏jrj combinations of states of variables are in fact rarely  sampled in typical finite datasets. as a result, the mdl complexity criteria tends to underestimate the relevance of edges connecting variables with many levels, ri, leading to the removal of false negative edges.

to avoid such biases with finite datasets, the normalisation of the maximum likelihood can be done over all possible datasets with the same number n of data points. this corresponds to the  normalized maximum likelihood  criteria , 
  ℒg=e−nh∑|d′|=ne−nh=e−nh−kg,dnml 

we introduce here the factorized version of the nml criteria  <cit>  which corresponds to a decomposable nml score, kg,dnml=∑xikxi|{paxi}nml, defined as, 
  ky|{pay}nml=∑jqylogcnyjry 

  Δky|{pay}∖xnml=∑jqylogcnyjry−∑j′qy/rxlogcnyj′ry 

where nyj is the number of data points corresponding to the jth state of the parents of y, {pay}, and nyj′ the number of data points corresponding to the j′th state of the parents of y, excluding x, {pay}∖x. hence, the factorized nml score for each node xi corresponds to a separate normalisation for each state j= <dig> …,qi of its parents and involving exactly nij data points of the finite dataset, 
  ℒg=e−n∑ih−∑i∑jqilogcnijri 

  =en∑i∑jqi∑krinijknlognijknij−∑i∑jqilogcnijri 

  =∏i∏jqi∏krinijknijnijkcnijri 

where nijk corresponds to the number of data points for which the ith node is in its kth state and its parents in their jth state, with nij=∑krinijk. the universal normalization constant cnr is then obtained by averaging over all possible partitions of the n data points into a maximum of r subsets, ℓ1+ℓ2+⋯+ℓr=n with ℓk≥ <dig>  
  cnr=∑ℓ1+ℓ2+⋯+ℓr=nn!ℓ1!ℓ2!⋯ℓr!∏k=1rℓknℓk 

which can in fact be computed in linear-time using the following recursion  <cit> , 
  cnr=cnr−1+nr−2cnr− <dig> 

with c0r= <dig> for all r, cn1= <dig> for all n and applying the general formula eq.  <dig> for r= <dig>  
  cn2=∑h=0nnhhnhn−hnn−h 

or its szpankowski approximation for large n  , 
  cn2=nπ21+232nπ+112n+o1n3/ <dig> 

  ≃nπ2exp89nπ+3π−1636nπ 

then, following the rationale of constraint-based approaches, we can reformulate the likelihood ratio of eq.  <dig> by replacing the parent nodes {pay}∖x in the conditional mutual information, i, with an unknown separation set {ui} to be learnt simultaneously with the missing edge candidate xy, 
  ℒg∖xy|{ui}ℒg=e−ni+kx;y|{ui} 

where we have also transformed the asymmetric parent-dependent complexity difference, Δky|{pay}∖x, into a {ui}-dependent complexity term, kx;y|{ui}, with the same xy-symmetry as i, 
  kx;y|{ui}mdl=12∏iruilogn 

  kx;y|{ui}nml=12∑j′{ui}∑kxrxlogcnkxj′ry−logcnj′ry+∑kyrylogcnkyj′rx−logcnj′rx 

note, in particular, that the mdl complexity term in eq.  <dig> is readily obtained from eq.  <dig> due to the markov equivalence of the mdl score, corresponding to its xy-symmetry whenever {pay}∖x={pax}∖y. by contrast, the factorized nml score, eq.  <dig>  is not a markov-equivalent score . to circumvent this non-equivalence of factorized nml score, we propose to recover the expected xy-symmetry of kx;y|{ui}nml through the simple xy-symmetrization of eq.  <dig>  leading to eq.  <dig> 

additional file
additional file  <dig> 
complementary evaluations for the 
3off2
 inference approach and comparisons with alternative reconstruction methods and parameters values. in this additional file, the results of the 3off <dig> inference approach are evaluated against other methods in terms of precision , p
r
e
c=t
p/, recall or sensitivity , r
e
c=t
p/, as well as f-score =2×p
r
e
c×r
e
c/ and execution time when comparing the cpdag of the reconstructed network  to the cpdag  of the benchmark network. the alternative methods are the pc algorithm, the bayesian inference method using the hill-climbing heuristics, the max-min hill-climbing  hybrid method and the aracne inference approach. 



from the fourteenth asia pacific bioinformatics conference san francisco, ca, usa.  <dig> -  <dig> january 2016

competing interests

the authors declare that they have no competing interests.

authors’ contributions

sa, lv and hi conceived and performed the research. sa, lv and hi wrote the manuscript. all authors read and approved the final manuscript.

publication costs

publication costs for this article were funded by the région ile-de-france.

declarations

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2016: bringing maths to life . the full contents of the supplement are available online at http://www.biomedcentral.com/bmcbioinformatics/supplements.

