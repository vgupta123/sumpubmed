BACKGROUND
predictive modeling is frequently used to find molecular signatures of disease phenotypes from genomic data, which helps us better understand underlying biological processes behind phenotypes and reduce data acquisition cost for future clinical samples by doing targeted profiling instead of genome-wide screens. to this aim, supervised machine learning methods such as linear classification and regression algorithms are trained to predict disease phenotypes, and features with relatively higher importance values  in these parametric models are included into the signature. however, as illustrated by existing studies  <cit> , molecular signatures identified by such algorithms may not be robust due to small sample size or highly correlated nature of genomic data.

gene set analysis methods try to identify gene sets on which disease phenotypes are dependent by calculating an enrichment score for each gene and transforming these scores into gene set scores using a summarization procedure  <cit> . the main advantage of these approaches is the ability to bring prior biological knowledge into the analysis in the form of biological pathways or sets of genes with similar biological functions  <cit> , leading to more robust and clinically interpretable results than predictive modeling approaches. however, they usually assume linear dependencies between genomic data and phenotype, which may not reflect the underlying biology of disease, and have difficulties in using very small or large gene sets in the analysis.

to benefit from the best of both worlds, integrating gene set analysis and predictive modeling is already considered in many existing studies , which modify linear classification and regression algorithms to include gene set information while doing feature selection for molecular signature extraction. even though this family of methods capture dependencies between genes, they still fail to capture nonlinear dependencies between genomic data and phenotype.

we suggest to integrate these two components using a nonlinear framework by extending our earlier bayesian formulation  <cit> . here, we develop a novel bayesian multiple kernel learning algorithm, which trains a binary classifier with a sparse set of active gene sets using a sparsity-inducing prior, i.e. the spike and slab prior  <cit> . using gene sets within a probabilistic formulation helps us identify more robust signatures even with small sample sizes. using a kernel-based formulation enables us to capture nonlinear dependencies between genomic data and phenotype, and to use overlapping gene sets and gene sets with different sizes without any major concern. we also generalize our proposed formulation to multitask learning setting to model multiple related datasets  conjointly, leading to better predictive performance and more robust molecular signatures. to the best of our knowledge,  <cit>  provides the first joint formulation of gene set analysis and nonlinear predictive modeling, which performs a survival analysis on breast cancer patients using both clinical and genomic data, using an existing discriminative multiple kernel learning algorithm. however, our approach has important advantages over their method:  more robustness on clinical datasets with small sample size due to its probabilistic nature,  its ability to perform automatic model selection  due to its fully bayesian inference mechanism and  its ability to model multiple related datasets conjointly due to its multitask learning variant.

we perform repeated random subsampling validation experiments on two cancer and two tuberculosis datasets to demonstrate the better predictive performance of our two algorithms over a baseline bayesian nonlinear algorithm and to show the biological relevance of the genes and gene sets selected to disease phenotypes modeled.

materials
in this study, we use two cancer and two tuberculosis datasets, where we solve binary classification problems to predict phenotype values from genomic data and to extract molecular signatures of disease phenotypes.

diagnosis of micro-satellite instability in colorectal and endometrial carcinomas
micro-satellite instability is a hypermutable phenotype caused by the loss of dna mismatch repair activity. it is frequently observed in several tumor types such as colorectal, endometrial, gastric, ovarian and sebaceous carcinomas  <cit> . tumors with micro-satellite instability do not respond to chemotherapeutic strategies developed for micro-satellite stable tumors, leading to its clinical importance. that is why we address the problem of predicting micro-satellite instability status of cancer patients from their gene expression data. we use two publicly available datasets provided by ‘the cancer genome atlas’  consortium:  ‘colon and rectum adenocarcinoma’  patients  <cit>  and  ‘uterine corpus endometrial carcinoma’  patients  <cit> .

the phenotype values of cancer patients for both datasets are downloaded from the tcga website , which groups the patients into three categories:  ‘micro-satellite instability high’ ,  ‘micro-satellite instability low’  and  ‘micro-satellite stable’ . the preprocessed genomic characterizations of primary tumors from the patients  are downloaded from https://www.synapse.org/%23%21synapse:syn <dig>  where  <dig>  normalized gene expression intensities are provided for each profiled primary tumor. we remove the patients with missing phenotype value or genomic data from further analysis. at the end, there are  <dig> and  <dig> patients with available phenotype value and genomic data for coadread and ucec datasets, respectively. table  <dig> summarizes the final datasets by listing the numbers of patients in each category together with the total number of patients.

msi-h micro-satellite instability high, msi-l micro-satellite instability low, mss micro-satellite stable




diagnosis of tuberculosis in adult and pediatric individuals
tuberculosis is responsible for  <dig>  million deaths in  <dig> according to the world health organization, which makes it the second greatest killer due to a single infectious agent after hiv. it is also the leading cause of death for hiv-infected people. its diagnosis is currently based on clinical and radiological features, sputum microscopy and tuberculin skin testing, which usually give false results in hiv-infected individuals  <cit> . new clinical diagnostic tests, especially for resource poor settings such as low-income countries with high rates of hiv, are needed to identify tuberculosis cases correctly for proper treatment. that is why we address the problem of predicting tuberculosis status of individuals from genome-wide rna expression in host blood. we use two publicly available datasets of hiv-infected and -uninfected individuals from south africa and malawi:  adult individuals   <cit>  and  pediatric individuals   <cit> .

the phenotype values and the genomic data for adult and pediatric datasets are downloaded from ncbi’s gene expression omnibus using geo series accession numbers gse <dig> and gse <dig>  respectively, where the individuals are grouped into three categories:  ‘active tuberculosis’ ,  ‘latent tuberculosis infection’  and  ‘other disease’ . these repositories contain background subtracted and quantile normalized intensities of  <dig>  <dig> probes for each individual. there are  <dig> and  <dig> individuals with available phenotype and genomic data for adult and pediatric datasets, respectively. table  <dig> summarizes the datasets by listing the numbers of individuals in each category together with the total number of individuals.

atb active tuberculosis, ltbi latent tuberculosis infection, od other disease




methods
we consider the problem of predicting phenotype values from genomic data using classification algorithms. instead of training classifiers that use all available features, we want to develop classifiers that use very few but biologically relevant input features to identify a molecular signature of the phenotype and to reduce the data acquisition cost for test samples. however, the molecular signatures identified from, for example, gene expression data are not robust when we have limited training data  <cit> . in such cases, we obtain different molecular signatures from different subsets of the same training set due to highly correlated nature of data, which makes knowledge extraction quite difficult. instead, we can use our prior biological knowledge to group the input features and pick the relevant groups that are predictive of the phenotype while training the classification algorithm. we first discuss our proposed method that can learn a classifier and identify predictive gene sets simultaneously on a single dataset. we then explain how we extend our method to model multiple related datasets by identifying a common set of predictive gene sets across them.

sparse bayesian multiple kernel learning
we formulate the prediction task as a binary classification problem defined on the genomic data, denoted as domain x, and the phenotype, denoted as domain y. we are given an independent and identically distributed sample {xi∈x}i=1n and a class label vector y={yi∈y}i=1n, where n is the number of data points, and y={− <dig> +1}. we are also given a list of gene sets {im}m=1p, which encode our prior biological knowledge in terms of gene names, where im list the names of genes in the gene set m, which may be a set of genes from a biological pathway or a set of genes with similar biological functions, and p is the number of gene sets.

we choose to develop a nonlinear classifier to predict phenotype from genomic data using a kernel-based formulation due to its three main advantages  <cit> :  we can learn robust classifiers for tasks with very high dimensional representations such as genomic data and small sample size .  we can learn better classifiers using nonlinear kernels such as the gaussian kernel .  we can use domain-specific kernels  to better capture the underlying biological processes  <cit> . to calculate similarities between the data points, we have multiple kernel functions defined over gene sets, namely, {km:x×x→ℝ}m=1p, which are used to calculate the kernel matrices {km}m=1p. for each gene set, the corresponding kernel km considers only the features extracted from or related to the genes in im. we choose to learn a weighted combination of the input kernels {km}m=1p while training a binary classifier, which is known as multiple kernel learning  <cit> , by extending our earlier bayesian formulation  <cit>  with a sparsity-inducing prior on the kernel weights. figure  <dig> gives a schematic description of the proposed model.
fig.  <dig> schematic description of sparse bayesian multiple kernel learning. for each gene set, the corresponding kernel considers only the features extracted from or related to the genes in this gene set. we then learn a weighted sparse combination of these kernels while training a binary classifier to predict the phenotype values




probabilistic model
our proposed probabilistic model, called ‘sparse bayesian multiple kernel learning’ , has three main parts:  finding kernel-specific latent variables using the same set of sample weights over the input kernels,  assigning sparse weights to these latent variables using the spike and slab prior  <cit>  and  generating predicted outputs using the latent variables and these sparse weights together with a bias parameter.

the first part has the following distributional assumptions: 
 λi∼gamma∀iai|λi∼normalai; <dig> λi−1∀igim|a,km,i∼normalgim;a⊤km,i,σg2∀,  where the superscript indexes the rows, the subscript indexes the columns, normal represents the normal distribution with the mean vector μ and the covariance matrix Σ, and gamma denotes the gamma distribution with the shape parameter α and the scale parameter β. we generate the latent variables g
m for each input kernel k
m using the same set of sample weights a. note that we need to use a small noise parameter σ
g while generating the latent variables to better generalize to test data points.

the second part has the following distributional assumptions: 
 κ∼betasm|κ∼bernoulli∀mω∼gammaem|ω∼normalem; <dig> ω−1∀m,  where beta denotes the beta distribution with the shape parameters ζ and η, and bernoulli represents the bernoulli distribution with the success probability parameter π. we generate a binary indicator variable s
m and a normally distributed weight e
m for each input kernel. the product of these two variables s
m
e
m is a simple parameterization of the spike and slab prior, which is more amenable to approximate inference.

the third part has the following distributional assumptions: 
 γ∼gammab|γ∼normalfi|b,e,s,gi∼normalfi;⊤gi+b,1∀iyi|fi∼kronecker∀i,  where ∘ represents the hadamard product, and kronecker denotes the kronecker delta function that returns  <dig> if its argument is true and  <dig> otherwise. the predicted outputs f, similar to the discriminant outputs in support vector machines, are introduced to make the inference procedures efficient  <cit> . the nonnegative margin parameter ν is introduced to resolve the scaling ambiguity and to place a low-density region between two classes, similar to the margin idea in support vector machines, which is generally used for semi-supervised learning  <cit> .
fig.  <dig> graphical model of sparse bayesian multiple kernel learning. random variables are shown as empty circles, whereas observed variables are shown as filled circles. hyper-parameters are ignored for simplicity




inference using variational bayes
we need to infer the posterior distribution over the model parameters and the latent variables, which we denote as Θ={λ,a,g,κ,s,ω,e,γ,b,f}, given the input kernel matrices {km}m=1p and the class labels y to find the predictive distribution for test data points. unfortunately, exact inference for our proposed probabilistic model is intractable. instead of using a computationally expensive gibbs sampling approach  <cit> , we choose to perform variational inference, which maximizes a lower bound on the marginal likelihood using an ensemble of factored posteriors to infer the joint parameter distribution  <cit> .

we approximate the posterior distribution over the model parameters and the latent variables by a variational distribution: 
 p≈q, 


where we assume that the variational distribution has a simpler form than the posterior distribution to make inference tractable. the inference problem can be defined as finding the nearest variational distribution to the posterior distribution with respect to a distance function. we perform mean-field variational bayes, which measures the distance between distributions q and p using ‘the kullback–leibler divergence’ denoted as kℒ. we can decompose the log evidence as 
 logp=∫qlogpΘ,y|{km}m=1pqdΘ⏟ℒ+∫−qlogpΘ|{km}m=1p,yqdΘ⏟kℒ,  where we assume without loss of generality that all model parameters and latent variables are continuous variables, and see that minimizing kℒ amounts to maximizing the lower bound ℒ.

we start by writing q as a factorized approximation: 
 q=qqqqqqqqqq,  where we couple the weights e with the binary indicator variables s due to their strong correlation. note that we choose not to have the factorization as qq because it gives a unimodal distribution, but the true posterior distribution may have exponentially many modes. to capture this multimodal structure, we choose to formulate the factorization as qq, which can be approximated efficiently  <cit> . we then write ℒ in the form of expectations: 
 ℒ=eq−eq,  where we iteratively maximize ℒ with respect to each factor until convergence. the approximate posterior distribution of a specific factor τ can be found as 
 q∝expeqlogpΘ,y|{km}m=1p. 


inference details
we define the factors for the first part of our probabilistic model as 
 q=∏i=1ngamma,β)q=normal,Σ)q=∏i=1nnormal,Σ),  where α,β,μ, and Σ denote the shape parameter, the scale parameter, the mean vector and the covariance matrix of their arguments, respectively. the approximate posterior distributions can be updated as 
 α=αλ+1/2β=1/βλ+〈ai2〉/2−1Σ=diag+σg−2∑m=1pkmkm⊤−1μ=Σσg−2∑m=1pkm〈⊤〉Σ=σg−2i+〈⊤〉−1μ=Σσg−2⊤〈a〉+〈fi〉〈s∘e〉−〈b〉〈s∘e〉,  where 〈h〉 denotes the posterior expectation as usual, i.e. eq.

the factors for the second part of our probabilistic model are defined as 
 q=beta,η)q=∏m=1pbernoulli)q=gamma,β)q=∏m=1pnormal,Σ),  where ζ,η and π denote the shape parameters and the success probability parameter of their arguments. we can update the approximate posterior distributions as 
 ζ=ζκ+∑m=1p〈sm〉η=ηκ+p−∑m=1p〈sm〉π=1/)α=αω+p/2β=1/βω+∑m=1p/2−1Σ=1/〈ω〉μ=0Σ=1/⊤〉)μ=Σ∑i=1n〈gim〉−∑l≠m〈sl〉〈el|1〉〈gilgim〉,  where the auxiliary variable r
m is defined as 
 rm=logκ1−κ−12〈em2|1〉〈gm⊤〉+〈em|1〉∑i=1n〈fi〉−〈b〉〈gim〉−∑l≠m〈sl〉〈el|1〉〈gilgim〉.  we define the factors for the third part of our probabilistic model as 
 q=gamma,β)q=normal,Σ)q=∏i=1ntruncatednormal,Σ,ρ),  where truncatednormal) denotes the truncated normal distribution with the mean vector μ, the covariance matrix Σ and the truncation rule ρ such that truncatednormal)∝normal if ρ is true, and truncatednormal)= <dig> otherwise. the approximate posterior distributions can be updated as 
 α=αγ+1/2β=−1Σ=−1μ=Σ∑i=1n〈fi〉−〈⊤〉〈gi〉Σ=1μ=〈⊤〉〈gi〉+〈b〉ρ≜fiyi>ν,  where we can fortunately calculate the expectation of the truncated normal distribution in closed-form.

prediction scenario
we can replace pa|{km}m=1p,y with its approximate posterior distribution q and obtain the posterior predictive mean of the latent variables g
⋆ for a new data point x
⋆ as 
 〈g⋆〉=⊤〈a〉. 


the posterior predictive mean of the predicted output f
⋆ can also be found by replacing p with its approximate posterior distribution qqq: 
 〈f⋆〉=〈⊤〉〈g⋆〉+〈b〉,  where we use 〈f
⋆〉 to predict the class label by looking at its sign.

sparse bayesian multitask multiple kernel learning
we formulate the joint modeling of prediction tasks on multiple datasets using a multitask learning approach, which models distinct but related tasks conjointly to improve overall generalization performance. we are given t datasets, and, for each dataset, we have an independent and identically distributed sample {xt,i∈x}i=1nt and a class label vector yt={yt,i∈y}i=1nt, where n
t is the number of data points in the dataset t. we also have a list of gene sets {im}m=1p, which are shared across the tasks, and the corresponding kernel functions {kt,m}m=1p for each task.

probabilistic model
our single-task learning model sbmkl is extended towards multitask learning to obtain ‘sparse bayesian multitask multiple kernel learning’ .

the distributional assumptions of the first part can be modified as 
 λt,i∼gamma∀at,i|λi∼normalat,i; <dig> λt,i−1∀gt,im|at,kt,m,i∼normalgt,im;at⊤kt,m,i,σg2∀,  where we have task-specific model variables and latent variables.

the distributional assumptions of the second part are written as 
 κ∼betasm|κ∼bernoulli∀mωt∼gamma∀tet,m|ωt∼normal∀,  where the binary indicator variables are shared across the tasks, which helps us transfer information between them.

the distributional assumptions of the third part can be modified as 
 γt∼gammaγt;αγ,βγ∀tbt|γt∼normalbt; <dig> γt−1∀tft,i|bt,et,s,gt,i∼normalft,i;⊤gt,i+bt,1∀yt,i|ft,i∼kroneckerft,iyt,i>ν∀,  where we have task-specific bias parameters and predicted outputs.

inference using variational bayes
we approximate the posterior distribution over the model parameters and the latent variables by a variational distribution: 
 pΘ|{{kt,m}m=1p,yt}t=1t≈q,  where we start inference by writing q as a factorized approximation: 
 q=∏t=1tqq∏t=1t×∏t=1t. 


inference details
the update equations of the approximate posterior distributions for all model parameters and latent variables are very similar to those of sbmkl except for the binary indicator variables. we can update the approximate posterior distribution of them as 
 π=1/)  where the auxiliary variable r
m is defined as 
 rm=logκ1−κ−12∑t=1t〈et,m2|1〉〈gtm⊤〉+∑t=1t〈et,m|1〉∑i=1nt〈ft,i〉−〈bt〉〈gt,im〉−∑l≠m〈sl〉〈et,l|1〉〈gt,ilgt,im〉. 


prediction scenario
we can use the approximate posterior distribution q instead of p and obtain the posterior predictive mean of the latent variables g
t,⋆ for a new data point x
t,⋆ in the task t as 
 〈gt,⋆〉=⊤〈at〉. 


the posterior predictive mean of the predicted output f
t,⋆ can also be found by replacing pbt,et,s|{kt,m}m=1p,ytt=1t with its approximate posterior distribution qqq: 
 〈ft,⋆〉=〈⊤〉〈gt,⋆〉+〈bt〉,  where we use 〈f
t,⋆〉 to predict the class label by looking at its sign.

baseline algorithm
we use a kernelized bayesian classification algorithm, which is known as relevance vector machine  <cit> , as the baseline algorithm. its distributional assumptions are defined as 
 λi∼gammaλi;αλ,βλ∀iai|λi∼normalai; <dig> λi−1∀iγ∼gammaγ;αγ,βγb|γ∼normalb; <dig> γ−1fi|a,b,ki∼normalfi;a⊤ki+b,1∀iyi|fi∼kroneckerfiyi>ν∀i,  where the predicted outputs of data points are modeled as a linear function of their kernel representations . we again learn the posterior distribution over the model parameters and the latent variables using a deterministic variational approximation as we do for our methods. we call this algorithm ‘bayesian relevance vector machine’ . we have three main reasons for choosing this particular baseline algorithm:  brvm can make use of kernel functions to obtain nonlinear models like our methods.  we can see the effect of using gene set information by comparing our methods to brvm.  brvm uses the same type of inference mechanism with our methods.

RESULTS
to illustrate the effectiveness of our proposed methods sbmkl and sbmtmkl, we report their results on four datasets  and compare them to the baseline algorithm brvm, which does not make use of gene set information, using repeated random subsampling validation experiments.

experimental settings
for each dataset, we create  <dig> random train/test splits to obtain robust results. for each replication, the training set is defined by randomly selecting  <dig> % of the data points with stratification on the phenotype, and the remaining  <dig> % of the samples are used as the test set. the training set is normalized to have zero mean and unit standard deviation, and the test set is then normalized using the mean and the standard deviation of the original training set.

we extract gene sets from ‘the molecular signatures database’   <cit> , which contains curated pathway gene sets from online databases such as ‘the kyoto encyclopedia of genes and genomes’   <cit>  and ‘the pathway interaction database’   <cit> . in our experiments, we use  <dig> pid pathways reported in msigdb as our gene set collection.

to calculate similarity between data points for all methods, we use the gaussian kernel: 
 kgaussian=exp−∥xi−xj∥22/,  where ∥·∥ <dig> denotes the ℓ
2-norm, and we set the kernel width s to the mean of pairwise euclidean distances between the data points: 
 s=1n2∑i=1n∑j=1n∥xi−xj∥ <dig>  


for brvm, we calculate a single kernel over all input features. for sbmkl and sbmtmkl, we calculate a separate kernel function for each gene set over the corresponding features. note that the gaussian kernels calculated on the gene sets take values between  <dig> and  <dig> by definition, and there is no need for eliminating small/large gene sets or performing additional normalization steps to remove the effect of gene set size.

the hyper-parameter values of brvm are selected as =, = and ν= <dig>  the hyper-parameter values of sbmkl and sbmtmkl are selected as =,σ
g= <dig> ,=,=,= and ν= <dig>  note that  are set to these particular values to produce very sparse binary indicator variables, leading to classifiers with very few gene sets used for prediction. for brvm, we perform  <dig> iterations during variational inference, whereas we perform  <dig> iterations for sbmkl and sbmtmkl.

we use ‘area under the receiver operating characteristic curve’  to compare classification results. auroc is used to summarize the receiver operating characteristic curve, which is a curve of true positives as a function of false positives while the threshold to predict labels changes. larger auroc values correspond to better performance.

classification results on the cancer datasets
on the cancer datasets, we run binary classification experiments to separate msi-h patients from others , which is in agreement with the earlier studies that combine msi-l and mss tumors into the same group  <cit> . for brvm and sbmkl methods, we train a separate classification model on each dataset, whereas, for sbmtmkl, we train a joint model on both datasets. figure  <dig> compares the performance of brvm, sbmkl and sbmtmkl on both datasets in terms of auroc over  <dig> replications using box-and-whisker plots, and also reports the average auroc value for each experiment. we clearly see that our methods with sparse gene set weights, leading to classifiers with very few active features, obtain results comparable to or even better than brvm. note that brvm uses all available input features of the genomic data for classification. for example, sbmkl falls behind brvm just by  <dig>  % on coadread dataset, but obtains  <dig>  % higher average auroc on ucec dataset. the average auroc values become even higher if we model both datasets together using our multitask learning method sbmtmkl, which outperforms brvm by  <dig>  and  <dig>  % on coadread and ucec, respectively. our sparse classifiers obtain these results using very few active features ; sbmkl uses  <dig>   and  <dig>   out of  <dig>  <dig>  input features  on the average, whereas sbmtmkl uses  <dig>   features  on the average  and obtains better classification results than brvm and sbmkl on both datasets.
fig.  <dig> auroc values on the cancer datasets for msi-h versus others classification. the box-and-whisker plot shows the results over  <dig> replications in repeated random subsampling validation experiments of brvm, sbmkl and sbmtmkl on both datasets. the numbers just below the dataset names give the average auroc value for each experiment




classification results on the tuberculosis datasets
on the tuberculosis datasets, we perform binary classification experiments to separate individuals with atb from others , which is critical in clinical settings because we should correctly identify individuals who need tuberculosis treatment  <cit> . figure  <dig> compares the performance of brvm, sbmkl and sbmtmkl on both datasets. we see that our methods obtain results better than brvm. on adult and pediatric datasets, sbmkl outperforms brvm by  <dig>  and  <dig>  % using  <dig>   and  <dig>   out of  <dig>   <dig>  input features  on the average, respectively. our multitask learning method sbmtmkl again has the highest auroc values on both datasets and outperforms brvm by  <dig>  % on adult and  <dig>  % on pediatric using  <dig>  <dig>   features  on the average.
fig.  <dig> auroc values on the tuberculosis datasets for atb versus others classification. the box-and-whisker plots show the results over  <dig> replications in repeated random subsampling validation experiments of brvm, sbmkl and sbmtmkl on both datasets. the numbers just below the dataset names give the average auroc value for each experiment




biological results on the cancer datasets
to illustrate the biological relevance of our methods, we analyze their abilities to identify relevant gene sets based on the binary indicator variables inferred during training. for each gene set, we count the number of replications in which the corresponding binary indicator variable is nonzero. table  <dig> lists the top  <dig> most frequently selected gene sets together with their selection frequencies for three scenarios:  sbmkl on coadread,  sbmkl on ucec and  sbmtmkl on coadread and ucec. we see that sbmkl is able to identify wnt_noncanonical_pathway and tgfbrpathway as the top two gene sets in the first scenario, which are reported to be involved in the initiation and progression of colorectal cancer  <cit> . however, their selection frequencies are quite low . similarly, for ucec, it is able to identify two apoptosis-related gene sets, namely, p53downstreampathway and notch_pathway, as the top gene sets with more than  <dig>  frequencies, which are known to be associated with endometrial cancer  <cit> . when we jointly model both datasets using our multitask learning method sbmtmkl, we are able to identify p53downstreampathway, notch_pathway and wnt_noncanonical_pathway as the top gene sets with increased frequencies compared to those of sbmkl. we see that multitask learning decreases the effect of random subsampling by picking relevant gene sets more frequently, leading to more robust knowledge extraction for both datasets.

wnt_noncanonical_pathway
p53downstreampathway
p53downstreampathway

tgfbrpathway
notch_pathway
notch_pathway

deltanp63pathway
nfat_tfpathway
wnt_noncanonical_pathway

tap63pathway
il5_pathway
nfat_tfpathway

rb_1pathway
p53regulationpathway
ar_pathway

nfat_3pathway
cdc42_reg_pathway
rhoa_pathway

atf2_pathway
avb3_opn_pathway
reg_gr_pathway

smad2_3nuclearpathway
wnt_noncanonical_pathway
upa_upar_pathway

p73pathway
reg_gr_pathway
bmppathway

myc_activpathway
upa_upar_pathway
rac1_pathway
the table displays the top  <dig> most frequently selected gene sets together with their selection frequencies for three scenarios




we also count the number of replications for each gene in which it is included in the final classifier. figure  <dig> displays the top  <dig> most frequently selected genes together with their selection frequencies for three scenarios. crebbp, ep <dig>  jun and mdm <dig> are among the top  <dig> genes for all scenarios, which is reasonable considering their functions in cell cycle. we see that the selection frequencies of the first two scenarios are lower than those of the third scenario, which is consistent with our gene set selection results. our multitask learning method sbmtmkl includes several genes in the top  <dig> that are not selected by sbmkl in two other scenarios, which may lead to interesting findings. for example, e2f <dig>  e2f <dig> and e2f <dig> are used in the final classifier in all replications, which are reported to be related to cellular proliferation  <cit> .
fig.  <dig> gene selection results on the cancer datasets for msi-h versus others classification. the bar plots display the top  <dig> most frequently selected genes together with their selection frequencies for three scenarios. blue bars show the genes that are in the top  <dig> for all scenarios, and orange bars show the genes that are in the top  <dig> only for multitask learning scenario




biological results on the tuberculosis datasets
we also evaluate the gene set selection results of our methods on the tuberculosis datasets. table  <dig> lists the top  <dig> most frequently selected gene sets together with their selection frequencies for three scenarios:  sbmkl on adult,  sbmkl on pediatric and  sbmtmkl on adult and pediatric. we see that the gene set selection frequencies of sbmkl on pediatric dataset are quite low  compared to those on adult dataset. however, when we model both datasets using our multitask learning method sbmtmkl, the selection frequencies of the top  <dig> gene sets become significantly higher , leading to more robust gene set signatures.

erbb_network_pathway
a6b1_a6b4_integrin_pathway
rhodopsin_pathway

ap1_pathway
ras_pathway
erbb_network_pathway

cone_pathway
integrin1_pathway
ap1_pathway

ar_tf_pathway
rac1_pathway
syndecan_1_pathway

ceramide_pathway
rhodopsin_pathway
plk1_pathway

rhodopsin_pathway
syndecan_1_pathway
ceramide_pathway

syndecan_1_pathway
atm_pathway
atm_pathway

fanconi_pathway
atf2_pathway
ar_tf_pathway

rxr_vdr_pathway
thrombin_par1_pathway
atf2_pathway

hnf3bpathway
il12_2pathway
hnf3apathway
the table displays the top  <dig> most frequently selected gene sets together with their selection frequencies for three scenarios



fig.  <dig> gene selection results on the tuberculosis datasets for atb versus others classification. the bar plots display the top  <dig> most frequently selected genes together with their selection frequencies for three scenarios. blue bars show the genes that are in the top  <dig> for all scenarios, and orange bars show the genes that are in the top  <dig> only for multitask learning scenario




CONCLUSIONS
integrating gene set analysis and predictive modeling is already considered by many existing studies, which fail either to capture nonlinear dependencies between genomic data and phenotype or to model multiple related datasets conjointly.

in this study, we integrate gene set analysis and nonlinear predictive modeling of disease phenotypes by casting this problem into a binary classification framework defined on the gene sets with a sparsity-inducing prior on their weights. to this aim, we propose a bayesian multiple kernel learning algorithm, which produces a classifier with sparse gene set weights, by extending our earlier bayesian formulation  <cit> . we then generalize this new algorithm to multitask learning to be able to model multiple related datasets conjointly, leading to better generalization performance and to more robust molecular signatures. the main novelty of our methods is the integration of gene set analysis and nonlinear predictive modeling using a probabilistic formulation, which enables us to robustly capture nonlinear dependencies between genomic data and phenotype even with small sample sizes, and to use overlapping gene sets and gene sets with different sizes without any major concern. our approach brings us two side advantages:  we can identify very few gene sets predictive of the phenotype, which may shed light on underlying biological processes.  we can reduce the data acquisition cost for test samples in clinical settings by collecting only the features used in our classifier.

to demonstrate the performance of our algorithms sbmkl and sbmtmkl, we perform repeated random subsampling validation experiments on four datasets of two major human diseases, namely, cancer and tuberculosis. on the two cancer datasets  <cit> , we decide whether a colorectal or endometrial tumor displays micro-satellite instability using its mrna gene expression data. on the two tuberculosis datasets  <cit> , we diagnose whether an adult or pediatric individual has an active tuberculosis infection using his/her whole blood rna expression data. we compare our two methods to a baseline bayesian nonlinear algorithm that is trained on all available genomic data without using gene set information. our methods obtain comparable or even better predictive performance using very few features  on all datasets. we also show that we are able to identify biologically relevant genes and gene sets for cancer and tuberculosis phenotypes, which are validated by the existing studies from the literature. the results of our multitask learning algorithm show that modeling multiple related datasets conjointly enables us to further improve the generalization performance and to better understand biological processes behind disease phenotypes.

in the experiments reported, we use real-valued gene expression measurements as genomic data. our methods can also be applied to discrete data such as mutation profiles of tumors, which are hard to use in classical gene set analysis methods due to their very sparse nature. as a possible extension, we plan to use our kernel-based formulations on cancer datasets to identify driver mutations using kernels for discrete data such as the jaccard similarity coefficient.

from the 10th international workshop on machine learing in systems biologyden haag, the netherlands. 3- <dig> september 2016

declarations
this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2016: proceedings of the tenth international workshop on machine learning in systems biology . the full contents of the supplement are available online at http://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-17-supplement- <dig> 


funding
publication of this article was funded by koç university.

availability of data and materials
all datasets used in this study were made publicly available previously by the corresponding data providers as mentioned in the manuscript. matlab and r implementations of our two methods are available at https://github.com/mehmetgonen/sbmkl.

authors’ contributions
mg designed the study, implemented the algorithms, carried out the computational experiments, analyzed the results, and wrote the manuscript.

competing interests
the author declares that he has no competing interests.

consent for publication
not applicable.

ethics approval and consent to participate
not applicable.
