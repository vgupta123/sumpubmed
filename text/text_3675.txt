BACKGROUND
the exponential growth of peer-reviewed literature and the breakdown of disciplinary boundaries heralded by genome-scale instruments have made it harder than ever for scientists to find and assimilate all the publications important to their research  <cit> . thus, tools such as search engines are becoming indispensable. motivated by the desire to develop more effective text retrieval algorithms for consumers of the life sciences literature, this study poses a straightforward question: is searching full text more effective than searching abstracts? that is, given a particular information need expressed as a query, is the user more likely to find relevant articles searching the full text or searching only the abstracts?

this question is highly relevant today due to the growing availability of full-text articles. in the united states, the nih public access policy now requires that all research published with nih funding be made publicly accessible. more broadly, the open access movement for the dissemination of scientific knowledge has gained significant traction worldwide. these trends have contributed to the accumulation of full-text articles in public archives such as pubmed central and on the websites of open access journal publishers. the growth of freely-available content represents a significant opportunity for scientists, clinicians, and other users of online retrieval systems â€“ it is now possible to go beyond searching bibliographic data  in sources such as medline to directly search contents of the full text. indeed, zweigenbaum et al.  <cit>  identified analysis of full-text articles as one of the frontiers in biomedical text mining.

to be clear, i refer to full-text search as the ability to perform retrieval on the entire textual content of an article , whereas abstract search refers to retrieval based only on abstract and title . this work focuses on content-based searching only; experiments do not take advantage of metadata , since they would presumably be available for both full-text and abstract searches.

not surprisingly, there isn't a straightforward answer to the question posed in this study. i describe experiments with medline abstracts, full-text articles, and spans  within full-text articles using data from the trec  <dig> genomics track evaluation. the experiments examined two retrieval models: bm <dig> and the ranking algorithm implemented in the open-source lucene search engine. results show that treating an entire article as an indexing unit does not consistently yield higher effectiveness compared to abstract search. however, retrieval based on spans, or paragraphs-sized segments of full-text articles, consistently outperforms abstract search. results suggest that highest overall effectiveness may be achieved by combining evidence from spans and full articles. these findings affirm the value of full text collections for text retrieval and provide a starting point for future work in exploring algorithms that take advantage of full text.

one important implication of this work is the need to develop scalable text retrieval algorithms, since full-text articles are significantly longer than abstracts. in the near future, the amount of available content will likely outgrow the capabilities of individual machines, thus requiring the use of clusters for basic processing tasks. i explore this issue with ivory, a toolkit for distributed text retrieval built on top of hadoop, an open-source implementation of the mapreduce programming model  <cit> .

related work
despite much optimism among researchers about the potential of full-text articles, it is not completely clear how present techniques for mining the biomedical literature, primarily developed for title, abstract, and metadata , can be adapted to full text. for some tasks, full text does not appear to offer much beyond title and abstract. for example, consider the medical text indexer , a production tool at the national library of medicine  that assists human indexers in assigning mesh descriptors to medline citations based on title and abstract. gay et al.  <cit>  reported their experiences on extending mti to base mesh recommendations on full text. after much tuning and careful selection of article sections, a modest gain in effectiveness was achieved. however, the improvements were most likely "not worth the extra effort"  and there are currently no plans to integrate full-text capabilities into the nlm article processing pipeline. furthermore, it is unclear whether the particular treatment of article sections is generalizable beyond the relatively small set of articles examined in the study.

other researchers have also tried to exploit information in full-text articles for various text mining tasks. for example, yu et al.  <cit>  used surface patterns to extract synonyms for gene and protein names, and reported higher precision on full text than on abstracts. more recently, seki and mostafa  <cit>  explored an "inference network" approach to mining gene-disease associations. taking advantage of full text yields a small gain in effectiveness . however, the results were based on a small collection of articles and can only be characterized as preliminary. as with the mti experiments, there remain questions about the generalizability of the results beyond those articles examined.

there is no doubt that a significant amount of information is contained in well-written abstracts. journal requirements for structured abstracts assist authors in crafting highly-informative prose that covers the key aspects of their work. in the clinical domain, for example, demner-fushman et al.  <cit>  showed that information in abstracts is sufficient to identify articles that are potentially useful for clinical decision support. in the biomedical domain, yu  <cit>  examined text associated with images in articles and concluded that sentences in the abstract suffice to summarize image content. in fact, she suggested that abstract sentences may actually be superior, since associated text in the article typically describes only experimental procedures and often does not include the findings or conclusions of an experiment. in a related study, yu and lee  <cit>  discovered that 88% of figures and 85% of tables can be summarized by sentences in the abstract, and that 67% of abstract sentences correspond to images in the full-text articles. these results demonstrate that, at least for some tasks, it is unclear what additional value full-text content adds beyond title, abstract, and metadata.

additionally, researchers have compared the term content of abstracts with that of full text. shah et al.  <cit>  examined a collection of articles and found that abstracts were the richest source of relevant keywords. this finding was echoed by schuemie et al.  <cit> , who concluded that the density of useful information is highest in the abstract, but information coverage in full text is greater than that of abstracts. however, the practicality of mining full-text articles is unclear due to increased computational requirements. both these articles focused on characterizing texts and did not examine the impact of abstract vs. full text on a text-mining task.

there has been much work on processing full-text content, for example, the automatic extraction of gene names and protein interactions in the biocreative evaluations  <cit> . however, that body of research differs significantly from the goals of this study in that i am primarily interested in differences between full text and abstracts, and the impact of these differences on effectiveness in text retrieval. in gene identification and related information extraction tasks, the text collection is fixed, while researchers attempt to develop effective algorithms . specifically, the tasks are designed in a manner such that the source of the text is not a relevant factor. experiments in this article, on the other hand, focus both on algorithms and data.

the problem of retrieving sub-document segments of text has previously been explored in the information retrieval literature  <cit> ; the task is often called passage retrieval. recently, there has been substantial interest in xml retrieval, which shares many of the same issues as passage retrieval since xml documents naturally support retrieval at different granularities . the locus of research activity resides in the initiative for evaluation of xml retrieval , an annual evaluation forum that began in  <dig>  <cit> . for several years, the evaluation used a collection of xml-encoded journal articles from the ieee computer society. most relevant to this work, the genomics "track" at the  <dig> text retrieval conference  has explored passage retrieval in the life sciences domain. the overview paper  <cit>  provides an entry point into that literature . experiments reported in this article used the test collection from the trec  <dig> evaluation.

there is one important difference between this work and the research discussed above. whereas i ask the question "is searching full text more effective than searching abstracts?", previous work simply assumes that the answer is yes. research papers on passage retrieval often take for granted that retrieving passages provides value to the user. the inex evaluations implicitly assume that retrieving xml fragments is desirable . since widespread availability of full text is a relatively recent phenomenon in the life sciences, and searching abstracts has been available for decades , it is worth questioning the premise that searching full text is inherently superior. focusing on this question, however, certainly does not diminish the contributions of previous work: to the extent that full text is shown to be valuable for text retrieval in this specific domain, findings from related work can be leveraged to inform the development of full-text retrieval algorithms for the life sciences literature.

comparison of full text and abstracts
in comparing search effectiveness on full-text articles and abstracts, it makes sense to begin by discussing their characteristics and enumerating potential advantages and disadvantages. such an analysis could guide the interpretation of experimental results.

length is the most obvious difference between full-text articles and abstracts â€“ the former provides systems with significantly more text to process. most information retrieval algorithms today build on "bag of words" representations, where text is captured as a weighted feature vector with each term as a feature  <cit> , or as a probability distribution over terms in the case of language modeling approaches  <cit> . these models derive from statistical properties such as term frequency and document frequency. more text could yield a more robust characterization of these statistics . however, the potential downside is that full text may introduce noise. for example, an article may contain an elaborate discussion about related work that does not directly pertain to the article's main focus. articles often contain conjectures  or future work . in these cases, term statistics may be misleading due to the presence of "distractors" that dilute the impact of important terms. in contrast, abstracts focus on the key ideas presented in the articles and little else. similar observations have been made with respect to collections of newswire documents . many news stories are written about one central topic, but contain many aspects that are only loosely connected â€“ in these cases, a global characterization of the entire document may not accurately capture what the article is "about".

another challenge with full-text articles is the variety of formats in which they are distributed. whereas there is a small number of formats for encoding medline abstracts, full-text collections can be found in xml, sgml, pdf, and even raw html . each format is associated with idiosyncrasies that make pre-processing a challenge. just to give one example, there are numerous ways to encode greek symbols  â€“ sometimes, they are directly encoded in extended character sets ; often, they are "written-out" ; in other cases, they are encoded as html entities . the prevalence of special characters in the literature complicates seemingly simple tasks such as tokenization. the problem is further exacerbated when one tries to combine full-text collections from different sources .

access to full text is necessary for certain fine-grained retrieval tasks, e.g., image search  <cit> . recently, there has been substantial interest in this problem  <cit> , based on both image features and features derived from text associated with images, e.g., captions and sentences in which the figures are referenced. full text is obviously essential for extracting these features. for other tasks, access to full text may be desirable, e.g., question answering. unlike a search engine, a question answering system attempts to return a response that directly answers the user's question. due to their specificity, users' questions sometimes may not be sufficiently addressed by abstracts â€“ often, useful nuggets of information may be found in parts of articles that are not well reflected in the abstract .

however, it is also important to take into account other considerations in information seeking. since many users are uncomfortable reading large amounts of text on screen, they often print out journal articles first before reading them in detail. in these cases, finer-grained passage and image access capabilities are most useful in helping users decide what articles they want to read, rather than directly answering users' questions. often, complex information needs such as those faced by biologists cannot be answered with a paragraph or an image â€“ instead, information returned by the system must be considered in the context of the entire article from which the segment was extracted . for this reason, i focus on a system's ability to identify relevant articles. this corresponds to ad hoc retrieval, a task that has been well studied by information retrieval researchers in large-scale evaluations such as trec.

based on this discussion, it is clear that there are advantages and disadvantages to full-text retrieval. however, the extent to which these various factors balance out and affect search effectiveness can only be determined empirically. the next section presents a series of experiments that explore these issues.

RESULTS
test collection
retrieval experiments in this article were conducted with data from the trec  <dig> genomics track evaluation  <cit> , which used a collection of  <dig>  full-text articles from highwire press. medline records that correspond to the full-text articles were also provided as supplementary data. to standardize system output, the organizers of the evaluation divided up the entire collection into  <dig>  million "legal spans"  that represent the basic units of retrieval. the test collection contains  <dig> information needs, called "topics", and relevance judgments, which are lists of legal spans that were assessed by humans to be relevant for each topic. despite the availability of relevance judgments at the level of spans, i focused on article-level relevance in order to support a meaningful comparison of abstract and full-text search. section  <dig>  describes this test collection in more detail.

retrieval conditions
a matrix experiment was devised with two different retrieval models  and three different data conditions. the goal was to compare the effectiveness of different experimental settings, as quantified by standard retrieval metrics . the retrieval models examined were:

â€¢ the okapi bm <dig> ranking algorithm  <cit>  , as implemented in ivory, a toolkit for distributed text retrieval. ivory was developed with hadoop, an open-source implementation of the mapreduce programming model .

â€¢ the ranking algorithm implemented in the open-source search engine lucene, which represents a modified tf.idf retrieval model . due to its popularity and ease of use, lucene provides a good baseline for comparison. this ranking algorithm was also implemented in ivory, primarily for evaluation of efficiency and scalability .

the matrix design consisted of three different data conditions. the following indexes were built :

â€¢ abstract index, built on the abstracts and titles of articles in the highwire collection, taken from the medline records. each abstract was considered a "document" for retrieval purposes.

â€¢ article index, built on the full-text articles in the highwire collection . the entire text of each article was considered a "document" for retrieval purposes.

â€¢ span index, built on the legal spans in the highwire collection. in this experimental condition, each of the  <dig>  million spans in the collection was treated as if it were a "document".

for each cell in the matrix experiment, i performed a retrieval run with  <dig> queries, taken verbatim from the trec  <dig> genomics track test data . with the abstract and article indexes, retrieval results consisted of  <dig> article ids in rank order. for the span index, the ranked results consisted of span ids. as a post-processing step, i applied a script to create a ranking of articles from a ranking of spans, using two different methods described by hearst and plaunt  <cit> :

â€¢ maximum of supporting spans : the score for an article is computed as the maximum of scores for all spans contained in that article. article ids were sorted by this score in descending order. this method favors articles that have a single high-scoring span.

â€¢ sum of supporting spans : the score for an article is computed as the sum of scores for all spans contained in that article. article ids were sorted by this score in descending order. this method favors articles that have many potentially-relevant spans.

in order to ensure that the post-processing script generated a ranked list of  <dig> articles ,  <dig> spans were retrieved initially.

evidence combination
an obvious extension to the matrix experiment discussed in the previous section is to integrate evidence from multiple sources. as an initial exploration, i conducted a series of experiments that combined the results of span retrieval with either article or abstract retrieval. in both cases, runs were combined by first normalizing each set of scores and then averaging scores from the different runs. the goal of this experiment was to examine the effect of combining content representations at different granularities. note that in principle a simple average of the scores could be replaced with weighted linear interpolation , but in absence of a principled approach to determining parameters, i opted not to explore this option for fear of overtraining on limited data.

evaluation metrics
for all experimental conditions, i arrived at a ranking of  <dig> articles, which supported a meaningful comparison across all data conditions. to evaluate effectiveness, three different metrics were collected:

â€¢ mean average precision , the single-point metric of effectiveness most widely accepted by the information retrieval community. the standard cutoff of  <dig> hits was used.

â€¢ precision at  <dig> , the fraction of articles in the top twenty results that are relevant. the cutoff of twenty equals the number of hits on a result page in the present pubmed interface. in a web environment, many searchers only focus on the first page of results.

â€¢ interpolated precision at recall of 50% , which attempts to capture the experience of a dedicated, recall-oriented searcher . this metric quantifies the amount of irrelevant material a user must tolerate in order to find half of the relevant articles â€“ the higher the ip@r <dig>  the less "junk" a user must sort through. in this study, i characterize ip@r <dig> as a "recall-oriented" metric. the recall level of 50% was arbitrarily selected.

section  <dig>  focuses on retrieval effectiveness with ivory, comparing bm <dig> and the implementation of lucene's ranking algorithm. section  <dig>  focuses on efficiency, comparing lucene with the implementation of its ranking algorithm in ivory.

retrieval effectiveness
results of the matrix experiment described in section  <dig>  with ivory are presented in table  <dig>  the three parts of the table show effectiveness in terms of map, p <dig>  and ip@r <dig>  the columns report figures for the two retrieval models: bm <dig> and the ivory implementation of lucene's ranking algorithm. abstract retrieval  is taken as the baseline, and the table provides relative differences with respect to this condition. the wilcoxon signed-rank test was applied in all cases to assess the statistical significance of the results.

for all metrics, relative improvements over baseline are shown; ** = statistically significant ; * = statistically significant ; Â° = not significant.

results show that for abstract retrieval, bm <dig> is significantly more effective in terms of map than the lucene ranking algorithm . comparing abstract retrieval with article retrieval , map is significantly higher  for lucene but differences are not statistically significant for bm <dig>  for the lucene ranking algorithm, article retrieval significantly outperforms abstract retrieval for the other two metrics as well. on the other hand, for bm <dig>  article retrieval either hurts , or doesn't have a significant impact . article retrieval, which simply treats full-text articles as longer "documents", does not appear to yield consistent gains in effectiveness.

for retrieval using the span index, the "max" strategy for generating article rankings appears to be more effective than the "sum" strategy. in terms of map and ip@r <dig>  both span retrieval strategies significantly outperform retrieval with the abstract index, for both bm <dig> and the lucene ranking algorithm. for ip@r <dig>  in particular, the gains are quite substantial. however, span retrieval does not appear to have a significant impact on p <dig> compared to the abstract retrieval baseline.

results for the evidence combination experiments are shown in table  <dig>  where span retrieval is combined with abstract and article retrieval. in the interest of space, effectiveness metrics are only shown for the "max" strategy. relative differences are shown with respect to the span retrieval baseline. as with before, the wilcoxon signed-rank test was applied in all cases to assess the statistical significance of the results. for the lucene ranking algorithm, combining span retrieval with article retrieval yields significant gains for all three metrics. other differences are not statistically significant.

for all metrics, relative improvements over baseline are shown; ** = statistically significant ; * = statistically significant ; Â° = not significant.

a >> b indicates that a is significantly better than b ; a > b indicates that a is significantly better than b ;

retrieval efficiency
given that there is value in searching full text, the question of efficiency must be addressed. the potential effectiveness gains of full text come at a significant cost in terms of dataset size. in its compressed form as distributed, the collection of  <dig>  full-text articles from highwire press occupies  <dig>  gb . the corresponding medline abstracts, also in compressed form, take up only  <dig> mb . the difference in size is more than an order of magnitude â€“ which means that algorithms must process significantly more data to realize the potential gains of full-text content. articles in the highwire collection contain on average  <dig> terms , while abstracts contain on average only  <dig> terms. for reference, spans average  <dig> terms, or a bit less than half the length of abstracts.

open-source text retrieval systems available today, designed to run on individual machines, would have no difficulty handling the highwire collection and even collections that are much larger. however, these articles represent only a small fraction of the material already available or soon to be available. based on recent estimates, records are added to medline at a rate of approximately  <dig> k per month. a lower bound on the growth of available full-text content can be estimated by examining the growth of pubmed central. over the past two years, the digital archive has grown by approximately  <dig> k articles per month. however, the growth rate is uneven due to retrospective conversion; more recently, this figure is closer to  <dig> k articles per month. nevertheless, full-text collections will inevitably outgrow the capabilities of individual machines â€“ the only practical recourse is to distribute computations across multiple machines in a cluster.

i have been exploring the requirements of scaling to larger datasets with ivory, a toolkit for distributed text retrieval implemented in java using hadoop, which is an open-source implementation of the mapreduce framework  <cit> . section  <dig>  describes ivory in more detail, but an evaluation of its efficiency is presented here. specifically, i compare the original implementation of lucene with the ivory implementation of the lucene ranking algorithm .

all experiments were conducted with amazon's elastic compute cloud  service, which allows one to dynamically provision clusters of different sizes. ec <dig> is an example of a "utility computing" service, where anyone can "rent" computing cycles at a reasonable cost. for this work, ec <dig> provided a homogeneous computing environment that supports easy comparison of different cluster configurations. the basic unit of computing resource in ec <dig> is the small instance-hour, the virtualized equivalent of a processor core with  <dig>  gb of memory, running for an hour. i experimented with the following configurations:

â€¢ lucene , running on a single ec <dig> instance. default settings "out of the box" were used for all experiments.

â€¢ ivory , running on an ec <dig> cluster with  <dig> slave instances . this is comparable to a cluster with  <dig> cores.

â€¢ same as above, except with  <dig> slave instances, comparable to a cluster with  <dig> cores.

as an aside, note that physical equivalents of these clusters are quite modest by today's standards. quad-core processors are widely available in server-class machines, and with dual processor packages, a 20-core cluster is within the means of most research groups.

running times for index construction for the three different configurations are shown in table  <dig>  lucene, which was not designed to run on a cluster, takes over a day to build the span index . with either cluster configurations, indexing takes less than an hour. these figures should be interpreted with the caveat that lucene builds a richer index that supports complex query operators and that i am comparing a single core to clusters. however, the point is that lucene cannot be easily adapted to run on multiple machines and thus indexing speed is fundamentally bound by the disk bandwidth of one machine. a cluster can take advantage of the aggregate disk bandwidth of many machines, and mapreduce provides a convenient model for organizing these disk operations .

the speedup demonstrated by ivory is important because time for inverted index construction places an upper bound on how fast a researcher can explore the solution space for algorithms that require manipulating the index. since research in information retrieval is fundamentally empirical in nature, progress is driven by iterative experimentation. thus, exceedingly long experimental cycles represent a potential impediment to advances in the state of the art.

in terms of retrieval, running times on the entire set of  <dig> topics from the trec  <dig> genomics track are shown in table  <dig>  the gains in efficiency are not quite as dramatic, but still substantial. in its present implementation, ivory was designed for batch-style experiments, not real-time retrieval . these numbers are therefore only presented for reference, and should not be taken as indicative of efficiency in operational settings . my experiments primarily focus on indexing efficiency, which is more important for the issues explored in this study.

taking advantage of full-text content requires more computational resources to cope with the increased quantities of data. inevitably, full-text collections will outgrow retrieval systems designed to run on single machines â€“ necessitating the development of distributed algorithms. the mapreduce framework provides a practical solution for distributed text retrieval.

discussion
is searching full text more effective than searching abstracts? the answer appears to be yes. furthermore, experimental results suggest that span-level analysis provides a promising strategy for taking advantage of full-text content. whereas simply treating entire articles as indexing units yields mixed results, span retrieval consistently outperforms abstract retrieval. combining span- and article-level evidence yields the highest effectiveness across a range of experimental conditions.

why does span retrieval work? further analysis of results in section  <dig>  reveals some interesting observations. focusing on the "max" strategy, table  <dig> shows that, overall, span retrieval has a relatively small effect on precision , but a large impact on recall . this makes sense: key ideas in an article are likely reinforced multiple times, often in slightly different ways. this potentially alleviates mismatches between query terms and terms used by authors â€“ in essence, span indexing gives a retrieval algorithm multiple opportunities to identify a relevant article. this enhanced recall leads to higher overall effectiveness in terms of map.

in general, the "max" strategy for generating article rankings from span rankings appears to be more effective than the "sum" strategy. why is this so? one possibility is the issue of length normalization. in the current implementation, longer articles tend to have higher scores simply because they contain more spans; thus, there is an inherent bias in the "sum" strategy. length normalization plays an important role in text retrieval  <cit> , but i leave a thorough exploration of this issue for future work.

the findings in this article pave the way for future advances in full-text retrieval algorithms for the life sciences, which can draw from a wealth of previous work in the information retrieval literature on passage retrieval, xml retrieval, etc. in fact, the effectiveness of span retrieval confirms a well-known finding: ranking algorithms benefit from techniques that exploit document structure, particularly for longer documents.

remaining focused on the problem of using full-text content to improve article ranking, how in general can article structure be exploited? within the space of "bag of words" models, strategies can be organized in terms of two questions:

â€¢ at what levels of granularity should retrieval algorithms build representations of full-text content?

â€¢ how should evidence from multiple representations be combined to rank articles?

these two questions provide context for future work. as a start, i have experimented with two different indexing granularities , but alternative approaches include sliding windows  <cit> , multi-paragraph segments  <cit> , hierarchically-overlapping segments  <cit> , and segments based on topic shifts  <cit> . there are many strategies for integrating evidence from multiple content representations and representations at different granularities . i have begun to examine some of these strategies, but there are many more possibilities yet to be explored. for example, differential treatment of article sections may improve effectiveness since some sections are more important than others, i.e., more likely to contain relevant information. earlier work on a smaller collection of documents from the federal register illustrated the potential of assigning weights to different section types  <cit> . more recently, tbahriti et al.  <cit>  found section-specific weights to be helpful for retrieval in the context of structured abstracts in the life sciences. however, one challenge that must be overcome for this strategy to work on a large scale is the lack of standardized section headings â€“ both across journals and different types of articles .

in this work i have focused on exploiting full-text content to better rank articles. alternatively, one could leverage full text to directly return relevant information, i.e., with passage retrieval techniques. this was, in fact, the original design of the trec  <dig> genomics track evaluation. of course, this begs the question: how are they related? in the information retrieval literature, a distinction is made between passage retrieval and document retrieval that exploits passage-level evidence. this exactly parallels the present discussion about retrieving segments of full-text content versus leveraging full-text content to enhance article retrieval. however, i argue that the two are complementary from a user interface point of view.

leaving aside non-traditional search interfaces, a retrieval system must ultimately present users with lists of results. consider the two approaches to exploiting full text in this context:

even if the primary goal of a system is to leverage full-text content to enhance article retrieval, results have to be presented in a manner that suggests the relevance of an article. this necessarily involves creating some type of surrogate for the article, which can either be indicative or informative. common techniques for generating such surrogates include displaying titles and metadata  and short keyword-in-context extracts . the first is primarily indicative, while the second aims to be informative. extraction of informative text segments from articles is essentially a passage retrieval task â€“ and in some cases, this information may already be available as a natural byproduct of the article ranking process. for example, in algorithms that integrate evidence from multiple spans within an article, those salient spans might form the basis of generating article surrogates.

even if the primary goal of a system is to directly retrieve relevant passages, the passages must still be couched within the context of the article containing the passages . in addition, there will be cases where a passage retrieval algorithm suggests multiple passages extracted from the same article . to facilitate result presentation, it would be desirable to group passages by the articles that contain them â€“ which essentially involves article ranking.

in other words, the distinction between retrieving passages and retrieving articles becomes blurred when one considers elements of the user interface. both approaches must grapple with the same issues, thus creating synergies where algorithms specifically developed for one purpose may be useful for the other.

CONCLUSIONS
experiments in this article with the trec  <dig> genomics track test collection illustrate that there is significant value in searching full-text articles. given the rapidly growing availability of full text in online digital archives, this is a positive development for scientists who depend on access to the literature for their research. results show that retrieval at the level of paragraphs within full text is significantly more effective than searching abstracts only. combining span- and article-level evidence appears to yield the best results. however, much work remains in developing effective full-text retrieval algorithms for the life sciences literature: toward that end, this work presents a first step.

one important issue in moving from searching abstracts to searching full text is that of scalability. gains in effectiveness come at a cost â€“ algorithms must process significantly more text. although currently-available tools designed to run on single machines suffice to handle present test collections, it is clear that future systems must distribute computations across multiple machines to cope with ever-growing quantities of text. as illustrated by ivory, mapreduce provides a convenient framework for distributed text retrieval. the combination of greater effectiveness enabled by full text and greater efficiency enabled by cluster computing paves the way for exciting future developments in information access tools for the life sciences.

