BACKGROUND
the revolutionary next generation sequencing  technology has provided a cost effective, fast, and efficient means for large scale detection of variants  in the genome. in medicine, the value of ngs for variant detection is continuously increasing, not only in the research domains but also in the daily clinical practice, largely for diagnosis and prognostics and growingly for treatment as well. the computational workflow for variant analysis is a multistep process that includes spotting the variants with high accuracy, evaluating their effect, and reporting all pieces of knowledge related to them. for human samples, the requirements of quality, accuracy, speed, and integrated knowledge sources are higher, compared to samples from other organisms. such sophistications render the pipeline more complex and more data intensive, and hence necessitate the availability of high performance computing resources and storage that can be allocated in a reliable and quick manner.

cloud computing is another revolutionary technology that has changed the way computational resources are made available. concepts like virtualization, on-demand allocation of machines, and key-value storage became familiar to all scientists from different domains. commercial cloud computing service providers took the lead in introducing cloud computing services on a pay-as-you-go basis and with well-defined pricing schemes. academic clouds use the same concepts, but they usually offer limited resources for free, usually based on the first-in-first-service principle. amazon web services   <cit>  has pioneered the provisioning of cloud computing services. it then followed by other providers like microsoft azure  <cit>  and rackspace  <cit> . very recently, google   <cit>  has arrived at the scene and offered very competitive products compared to amazon.

the use of cloud computing as a cost effective and scalable infrastructure for running the variant analysis workflow has been evaluated in different studies  and a number of ready-to-use systems have been developed. in academia, there are games  <cit> , simplex  <cit> , atlas <dig>  <cit> , and stormseq  <cit> . in industry, the cloud based versions are accessed through web-interface, usually on a subscription basis. important examples coming from major ngs manufacturers include basespace from illumina  <cit>  and ionreporter from lifetech  <cit> .

the above mentioned variant analysis systems run only in amazon’s cloud. the use case scenario of using that cloud platform is composed of the following steps: the user instantiates virtual machines at their own cost using pre-configured machine images deposited in amazon. the instantiated virtual machines include middleware packages that are then invoked to establish a computer cluster. example middleware packages include starcluster  <cit> , vappio  <cit> , and elastichpc  <cit> . once the cluster is configured, the input data are either uploaded to the shared cluster storage from the local computer or read from the amazon storage s <dig>  once the data transfer is complete, the execution of the analysis task starts. when the computation is over, the results are made available for download or deposited in s <dig> amazon storage. finally, the virtual machines are terminated.

the latest iaas  products of google and azure in combination with large price reduction have changed the landscape of the cloud computing market. they attracted many users and resulted in a segmented market, which accordingly would profit the clinical grade processing of ngs data. mid  <dig>  google announced new pricing schemes that have driven cloud prices further down. google introduced about 30% reduction in prices compared to amazon for most of the on-demand virtual machine types. furthermore, google charges per minute , while amazon still charges per hour. up to this date , the amazon prices did not change, but one would expect so soon due to the severe competition.

to cope with the current market status and to satisfy clinical practice needs, it is important to provide new cloud-based bioinformatics packages supporting multiple clouds: first to serve different users registered in different cloud systems, and second to seize best sales offers. furthermore, the new cloud systems should also be able to run in a hybrid mode using cloud resources from different providers  for the same workflow to 1) optimize the performance, 2) reduce the cost, and 3) provide a sort of fault tolerance with smooth non-interruptible execution, by migrating failed tasks in one cloud to another. in other words, the end user would gain the following advantages:run the workflow on the cloud platform of choice, without adhering to one provider.

make use of new virtual machine offers  and seize best discounts.

have a kind of redundancy in case of failure of one cloud site. the response to failure can even take place in execution time and the workflow can migrate to another cloud site.

mix cloud resources and best offers from different cloud service providers, which can indeed help reducing the overall cost. .




contribution
in this paper, we present mc-genomekey, a multi-cloud based package for variant detection and analysis. our package enables the users to run the workflow either in amazon, google, microsoft azure, or on any platform supporting openstack interface or amazon-like interface. we support two main multi-cloud use scenarios: first, the whole workflow can run entirely in one cloud environment. second, parts of the entire workflow can run in one cloud site and other parts can run in another cloud. in the second scenario, it is possible to distribute whole tasks or even individual jobs of the same task to different cloud sites. the use of multiple cloud resources in mc-genomekey can be decided in the design time or in the run time in response to some events requiring re-distribution of tasks or jobs on different clouds. as we show by experiments, the availability of these scenarios will help the user reach the best performance at lowest cost.

in addition to its novel  cloud features we introduce in this paper, mc-genomekey package has superior features compared to the currently existing solutions. in table  <dig>  we compare the features of mc-genomekey to that of stormseq  <cit> , atlas <dig>  <cit> , simplex  <cit> , and wep  <cit> , and the previous single-cloud version of genomekey  <cit> . notably, mc-genomekey has advanced and robust parallelization technique that runs in a computer cluster. furthermore, it has the feature of analyzing multiple samples in a batch mode, saving the total execution time and cost. it also includes a comprehensive annotation of variants based on the annovar package  <cit> . finally, the proposed solution is modular, where each tool can be easily replaced by a newer version or another one.table  <dig> comparison of different systems for variant analysis


aheterogeneous cluster means nodes of different virtual machine types and also from different clouds


+failure handling means response to failure of compute nodes in cloud, as in the case of spot instances




providing a package implementing these scenarios in a multi-cloud setting is not a straightforward task, because all of the aforementioned clouds are built with different architecture, usage scenarios, apis, and business models. furthermore, providing such scenario requires changes in the workflow execution model and in the cluster middleware to cope with this heterogeneity. all these challenges will be addressed in the implementation section of the paper.

implementation
mc-genomekey basic features
mc-genomekey is a package implementing the variant detection and analysis workflow, based on the python-based cosmos workflow engine  <cit> . its basic features can be categorized into three groups: 1) variant analysis workflow specifications, 2) workflow engine and parallelization schemes and 3) cloud support. the details of these features are as follows:

variant analysis workflow specifications
figure  <dig> shows the basic phases of the workflow for variant detection and annotation. here is a description of these steps.fig.  <dig> variant analysis workflow. variant analysis workflow. the figure shows the major phases and other major steps in each stage



quality check: this is to verify the quality of the input read and trimming out the low quality terminal parts of the reads. the default program for this steps is the fastx  <cit>  toolkit.


read alignment: this is to map the reads to the reference human genome . the default alignment program is bwa  <cit> .


variant calling: this is to analyse the read alignment file to determine the positions where the variants exist. the default program for this is the gatk variant caller  <cit> . the variant calling step itself is a pipeline including a number of operations: it includes realignment of reads around the variant, quality score recalibration, and generation of the variant file in a vcf format.


variant annotation: this is to annotate all the variants with all possible knowledge from different structural, functional, and population databases. the default system used for annotation is the annovar package  <cit> .




usability
mc-genomekey is a user friendly package. the workflow can be invoked using a desktop client or from the package web-site. the parameters for each step can be set through a configuration file in case of the command line interface or through a web-form when using the web-interface.

setting up the cloud cluster is achieved via a simple interface, where the user defines the number and type of virtual machines. the use of different clouds requires the user’s credentials registered in each cloud site to run the system in the respective cloud. therefore, the user provides own credentials for each cloud beforehand. for amazon and azure clouds, the user enters credentials in the form of certificates and private keys. for google, the user provides an oauth <dig> token. to facilitate the generation of the token for google, we provide a web-based form in the mc-genomekey website that forwards the user to google to manage own cloud resources and authenticate the application.  in case of using the mc-genomekey web-site, the user is recommended to disable all certificates and tokens after completion of computation. apart from the security issues, all other technical details related to the setup of the cloud resources and parallel execution of the workflow is kept hidden from the user. figure  <dig> shows some screen shots of the web-interface.fig.  <dig> screenshots of the mc-genomekey website. screen shots of the web interface: a the user enters own credentials for amazon and google cloud. b the user sets workflow parameters, e.g., alignment and variant calling parameters. c the user defines the size of cluster, type of nodes, and use of spot instances or not. d the user sets the job configuration parameters, where the user can select the “recovery method” to respond to termination of spot instances. in this screen the input and output folders are defined




when the computer cluster starts in the cloud, an additional website is automatically generated in the master node of the cluster in order to monitor the cluster and to manage the nodes in the run time. there are also pages to monitor the execution of the analysis workflow. the access to these pages is explained in the package manual.

workflow engine and parallelization scheme
mc-genomekey is based on the python-based workflow engine cosmos  <cit> , which is optimized to run data analysis jobs in parallel over a computer cluster. once the input ngs data is defined, the different steps of mc-genomekey are executed using the cosmos engine as follows: mc-genomekey first creates a directed acyclic graph  of job dependencies, where the jobs are defined based on partitioning the input data . the dag assures the correct execution of the workflow as it assures that a job is executed only when its input is made available from previous jobs. figure  <dig> shows a simplified example dag and the associated data flow for the variant detection and analysis workflow.fig.  <dig> data flow associated with the parallel execution of the mc-genomekey jobs. data flow associated with the jobs of mc-genomekey in the form of dag . a job is executed only if all the input became available




the parallelization scheme works as follows: the set of input reads is partitioned into blocks based on the read group ids. once the alignment is produced, the aligned reads are then partitioned by chromosomes. this way of partitioning was already implemented in the first version of genomekey  <cit> , but the associated scalability is limited, because 1) the read group field is a characteristic of the illumina sequencing platforms and might not be properly filled with other platforms, and 2) the maximum number of jobs for the read mapping step that can run in parallel cannot exceed the number of read groups, and the maximum number of jobs for the variant caller cannot exceed the number of chromosomes. to improve this situation, we have implemented a more flexible and advanced parallelization scheme in this version of mc-genomekey that works as follows: the input bam is processed to increase the number of read groups, by creating sub-read groups associated with a parent read group, which increases the number of jobs that can run in parallel for the read mapping step. in other words, the read groups are re-written or properly added to the bam file. the same idea is also adopted for processing the reads from the same chromosome in parallel, where we define sub-chromosomes corresponding to large segments of the chromosome. the splitting by sub-chromosomes is optional in the package and is allowed only for large segments  so that the statistics for the variant calling is not affected.

the data flow among the tasks/jobs of the workflow is achieved by using files and each task/job recognizes its input by certain file extensions associated with the respective job id. these intermediate files are automatically managed by the engine. the parallelization of job execution over the cluster nodes is managed by the drmaa package  <cit> , which encapsulates different job schedulers  and efficiently deals with job submissions, monitoring, and errors using single common interface. to make the data available for all cluster nodes, a shared storage in the form of ebs volume is created and mounted to the master node. the input is moved to this shared storage from the user’s computer or from the user’s s <dig> account before computation. the intermediate data is kept in the shared storage. this shared storage remains alive even if all spot instances get terminated; this is because it is attached to the on-demand master node.

cloud features
mc-genomekey supports different scenarios for using the cloud computing resources. these include the following:individual cloud: in this scenario, the user selects the cloud platform to be used. unlike other cloud-based variant analysis systems, mc-genomekey can support amazon, google, and microsoft azure. the user is prompt to enter the credentials of the cloud of choice, the path to the input data, set the parameters in the configuration file, and starts the workflow execution. mc-genomekey then launches the process of creating the computer cluster in the cloud using the deposited machine images and returns process id to the user. the user does not need to be persistently connected to the internet. one can disconnect and use the process id anytime to monitor the status of execution using another command line or using the web-interface. once the process is over, the results are deposited to persistent cloud storage or are downloaded to the user local machine. the cloud computer cluster is finally terminated and a report is generated.

multi-cloud: there are different forms to use multi-cloud either topological or temporal. topological forms specify where the tasks and jobs are executed. considering the variant analysis workflow, we identify three forms:○replication: the same workflow runs simultaneously on different clouds on different datasets to increase the throughput, or to overcome some limitations at one site.

○task distribution: different tasks are executed in different clouds. for example, the task of read mapping can run in amazon and the remaining workflow can run in google. in this case, different job queues can be used, but there should be a master task queue to manage the tasks themselves. note that the task queue is handled by the workflow engine itself, while the jobs are managed by the job scheduler engine wrapped by the rdmaa in each cluster.

○job distribution: certain jobs within the same task run in one cloud and other jobs run in another cloud. for example, in case of ngs reads of different lengths, one can run the alignment of longer reads in one cloud and shorter reads in another cloud. the distribution of jobs on different clouds requires that one master job queue manages the jobs over the different cloud sites.







the creation of a computer cluster with separate job queues or shared queue is supported by the multicloud version of the elastihpc package  <cit> .

temporal forms address the time when multiple cloud resources are allocated. for the variant analysis workflow, assume, for example, that the user decides that task a runs in cluster c <dig> in one cloud and the subsequent task b runs in cluster c <dig> in another cloud. to save cost, c <dig> is created only when task a is over or when it starts to produce output. also some tasks can start in cloud c <dig> in case of failure of some jobs in cloud c <dig>  in other words, when jobs have to be migrated to finish the computation. in mc-genomekey, we support that each phase can run in different cloud and we support the migration of tasks in case of failure of some nodes to other nodes in another cloud.

of course, the use of multicloud scenarios will involve the transfer of intermediate results using the internet, which is a bottleneck for adopting these scenarios. nevertheless, in the following paragraphs, we will present one use case, where the use of multi-cloud can effectively contribute to a more cost effective and reliable usage of the cloud.

multicloud and spot instances
the use of multicloud scenario can be of great benefit when using spot instances of amazon. the spot instance model of amazon is about the use of computing resources at lower prices, when amazon environment is not fully loaded. how far prices get reduced depends on the load and this continuously changes over time. the user who wishes to use spot instances has to bid for a price. if the instance price becomes lower than the bid price, then instances are initiated. if the instance price becomes higher than the initial bid price, then the running instances are terminated without notice. in general, the price of a spot instance is much less than that of the equivalent on-demand instance, which is a very attractive feature. table  <dig> shows different machine types with their on-demand and spot prices in amazon. it also includes the prices of equivalent on-demand machines in google. it can be directly observed that the spot instances are the best to use to save cost. however, the major risk with the spot instance model is that the machines can terminate before the computation is over, and this risk is not rare to happen. table  <dig> shows different statistics about the lifetime of different spot instances against different bid prices. this table was computed based on history information of the amazon sport instances; the aws command describe-spot-price-history  was used to retrieve the price information for a period of three months. it can be observed that the average lifetime increases with the larger bid price. it can also be observed that the higher the machine specification the shorter the lifetime. this means that the average lifetime can be enough to finish short-time workflows, but there is a high probability to lose spot instances for long-time workflows.table  <dig> on demand prices for amazon and google instances

spot instance prices for amazon are the minimum prices observed such that the instance was available for at least one hour 


prices are computed for three months period from october until december  <dig>  instances of type m3-2xlarge were available all the period with a bid price of  <dig> $




to overcome the dilemma associated with the spot instances, one can use a multicloud computation environment which is a mix between amazon and google instances to assure termination of computation with reduced cost and minimal overhead. the cost reduction stems from the spot instance model and the fact that google is cheaper than amazon and it charges the user per minute and not per hour.

mc-genomekey offers three alternative solutions to the problem of sudden termination of spot instances: 1) wait-and-rebid, 2) continue with on-demand instances in the same cloud, and 3) migrate execution of failed jobs to another cloud. each of these solutions will be discussed in the following paragraphs. the last two scenarios are based on the multi-cloud to achieve the best performance and cost.

for ease of presentation, we assume that the master node of the main computer cluster in aws is an on-demand node and other worker nodes are spot instances. the on-demand node keeps the workflow running, even if all spot instances are terminated and it supports the scenarios mentioned above to restore the computation power. this cluster of heterogeneous compute nodes can be readily created with the elastichpc package  <cit>  without extra effort. in the following we summarize how the four scenarios will work:
scenario  <dig>  wait and rebid: as depicted in the sequence diagram of fig. 4a, if the price of any or all of the spot instances exceeds the bid price , mc-genomekey will remove the failed instances from the cluster configuration. the user then uses the cloud-client software to make a new bid for the spot instances. when the new spot instance starts before the completion of the workflow, they are added to the currently running nodes and join the execution plan. for this scenario, mc-genomekey is modified to keep the intermediate data in the ebs volume  of the master node.fig.  <dig> scenarios for handling sudden termination of spot instances. sequence diagrams showing scenario  <dig>  where the computation continues in the same amazon  cloud and scenario  <dig>  where computation filed jobs in terminated spot instances are migrated to google  cloud





scenario  <dig>  continue in the same cloud with on-demand nodes: in this scenario, the failed spot instances are replaced with on-demand instances in the same cloud. the job scheduler is set-up to remove the failed ones and add the new ones.


scenario  <dig>  migrating failed jobs to google cloud: as depicted in fig. 4b, mc-genomekey will replace terminated spot worker nodes in amazon with other ones in google. the failed jobs on the terminated spot nodes will be moved to the new nodes and re-executed there. this scenario includes the case where all the cluster nodes are spot instances that get terminated and the execution of the variant analysis pipeline switches completely to google.




to optimize the performance and reduce the effect of data transfer over the internet, we do not transfer the whole data from the beginning, but only those needed to complete the tasks to run on google. for example, if the failure occurs in the annotation step, then only the respective vcf files are sent to google. furthermore, we dispatch also the subsequent jobs following the failed ones in the dag. this will prevent moving the data back and forth between the amazon and google instances; i.e., the data needed for computation is transferred only once and the workflow continues to the end.  when the workflow finishes successfully, the output of the google instances will be transferred to the output directory in the aws cluster on the shared storage and the whole directory will be compressed and uploaded to aws s <dig>  the default setting is that the intermediate data needed for computation in google is moved from amazon and it is deleted automatically when the google machines are terminated. in addition, we give the user an option to keep a copy of the data transferred to google in amazon and send the intermediate data generated by google back to amazon with the output.

extra implementation details
mc-genomekey is based on a client-server architecture. the user installs a client module at own desktop to control the analysis process. the client module uses the cloud provider apis to manage the cloud resources. the client also controls the created instances, configures them, and starts the analysis workflow and follows its progress.

specifying the job parameters, such as the workflow name, input and output path on s <dig> and output directory to upload to s <dig>  and the response to spot instance failure , are achieved through a workflow configuration file.

the multi-cloud version of elastichpc is used to create and manage instances of the computer cluster in the selected cloud. another configuration file is required for elastichpc to specify the cloud resources, this file includes a number of nodes, machine types, storage, and security. the elastichpc website includes detailed explanation of the configuration file. for each cloud environment, we have created a virtual machine image including all necessary settings and tools to the run the mc-genomekey on each cloud. the master and worker nodes of the cluster are created from the respective image. the master node also works as a server that responses to requests from the client and responses to any failure in the cluster nodes.

implementing the spot instance scenarios
once the cluster nodes are setup in amazon, a control program in the master node automatically starts to monitor the state of the running spot instances. this program reports node status to the user and initiates the recovery procedure as specified by the user in the job configuration file.

for the 3rd scenario, we updated cosmos task database with a field to associate the task with the worker node  and with the chromosome being processed on the gce worker node . also a specific queue is created to dispatch tasks to gce instances in case of spot instance failure, this procedure is depicted in fig.  <dig> 

RESULTS
experiment1: performance in different clouds
we measured the performance of mc-genomekey on aws, google, and azure using the following two datasets:a whole genome dataset of  <dig> gb from  <cit> . the ngs reads come from illumina ngs machines and cover all genomic regions with an average depth of about 30x.

an exome sequence dataset , also from  <cit> . the ngs reads come also from an illumina ngs machine and cover only the exons of the genome. exome sequencing is important in many clinical applications, where clinicians search for disease-causing variants in the coding regions.




in aws cloud, we used a master node of type m <dig> medium  and four r <dig> xlarge instances; each has  <dig> cores and  <dig> gb ram and costs  <dig> $/h. in google, we used master node of type n1-standard- <dig>  and four n1-standard- <dig> instances; each has  <dig> cores and  <dig> gb ram and costs  <dig> $/h. it is worth noting that gce instances are charged per minute . in azure, we used master node of a <dig> instance family  and four d <dig> v <dig> instances; each has  <dig> cores and  <dig> gb ram and costs  <dig> $/h.

in the established computer clusters, the master node was not an execution node, but it is a control node responsible for 1) the initiation and orchestration of the workflow and related data flow, 2) monitoring its status, and 3) carrying out the migration process from aws to google if needed. this configuration makes it possible to select a machine of lower specifications for the master node, which dramatically reduces the overall cost of the cluster in case of a migration scenario as will be explained below.

table  <dig> shows the results of running the whole variant analysis workflow for the exome and whole genome datasets using a cluster of  <dig> nodes in different cloud environments. from the table we can observe that the performance on amazon and google is almost the same, and better than that of azure. we also observe that the read mapping and variant calling are the most time consuming steps. the reason of cost reduction by google compared to amazon is the reduced machine price. although azure instances are cheaper than the equivalent ones in amazon, the low performance raised the cost of using azure to be higher than that of amazon.table  <dig> running times of the variant analysis workflow in different clouds

running times  of the variant analysis workflow in different clouds using a cluster of  <dig> nodes.  we give the time of different steps. the total time and cost  are in the rows titled “total”. the best running times and options are underlined




scalability using more nodes for the whole genome dataset
table  <dig> shows the running times of the workflow using different cluster sizes running in google cloud. . we assured the correctness of the results, by comparing the resulting vcf files and verifying identical output. from the table, we observe good scalability with the increasing number of nodes.table  <dig> total running times for running mc-genomekey on the whole genome dataset using different clusters of increasing node number




experiment 2: the spot model and live job migration
if the above workflow runs on amazon using spot instances, it would cost much less than that of google. however, the problem is that there is no guarantee that the machines are not terminated before the computation is completed. in this section, we provide experiments to measure scenario  <dig> in case all and some worker spot nodes fail.

in case all spot instances fail, all tasks/jobs assigned to spot instances are migrated to google cloud. table  <dig> shows the times and costs when the machines are terminated at different time points while the workflow is executed, using different bid prices. we simulated the termination of spot instances at different time points of the workflow, by sending termination signals to the worker nodes. the created cluster automatically detects any termination of the nodes and executes the migration process. in case, some spot instances fail, same number of machines are created in google and the tasks/jobs assigned to those nodes run there. table  <dig> shows the time and costs when machines are terminated at different time points while the workflow executed, using different bid prices. for this table, we assume that half the spot instances fail.table  <dig> the cost of using spot instances for case  <dig>  where all spot instanced get terminated

$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
1:51:33
 <dig> 
 <dig> 
31:39:43
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
19:18:06
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
6:11:30
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
the cost of using spot instances with different bid prices and failure time points given for case  <dig> where all spot instanced get terminated for the exome and the whole genome datasets. gce cluster setup time is nearly  <dig> min. for every bid we provided the average lifetime of cluster in brackets. costs in bold are the most likely ones with the respective bid price and its most likely time of failure. the best expected costs for a given experiment are underlined. the best expected cost for exome is  <dig>  using bid of  <dig>   and for whole genomes comes is  <dig>   with  <dig> nodes and bid price of  <dig> 


$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
1:51:33
$ <dig> 
$ <dig> 
$ <dig> 
31:39:43
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
19:18:06
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
6:11:30
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
$ <dig> 
the cost of using spot instances with different bid prices and failure time points given for case  <dig>  where some spot instances get terminated  for exome and whole genome datasets. gce cluster setup time is nearly  <dig> min. for every bid we provided the average lifetime of cluster in brackets. costs in bold are the most likely ones with the respective bid price and its most likely time of failure. the best expected costs for a given experiment are underlined. the best expected cost for exome is  <dig>  using bid of  <dig>  and for whole genomes is  <dig>  with a bid of  <dig>  finishing in  <dig> h. if one has to finish in less than  <dig> h, the best price is  <dig>  with  <dig> nodes and bid price of $ <dig> 




the results in these table can be read as follows: if we set the bid of $ <dig>  for instances of type r <dig> xlarge, then the expected life time is about  <dig> h, which is enough for running the workflow for the exome dataset without interruption. . in this case the workflow would cost about $ <dig>  which is much less than that of google .

this appears fine; but in fact one can do better when using the multicloud feature of mc-genomekey. for example, if we reduce the bid to $ <dig> , then the whole workflow would cost about $ <dig>  assuming no failure. but there is a probability that some spot instances terminate before workflow completion, because the expected lifetime using this bid is about  <dig> min. in this case, the solution is to migrate the failed jobs to google cloud. the expected cost in case of all nodes failure would be $ <dig>   and it would be $ <dig>  in case of some node failure . both costs are less than the cost when running on google, and in case of partial termination the cost is even less than running in a single cloud using higher bid price. scanning all values in table  <dig> in combination with the running time of each phase in the workflow given in tables  <dig> and  <dig>  one can identify that the best bid price leading to the best expected cost is $ <dig>  with total cost of $ <dig>  for all node failure and for some node failure.

such cost advantage is more apparent when dealing with whole genome data. for example, if we take a bid price of $ <dig> , the workflow would cost $ <dig>  - $ <dig>  in case no failure of spot instances, which is much less than the $ <dig> of google. even in case of failure and migration, the best prices at that bid would range between $ <dig>  and $ <dig> . it is worth mentioning that with more nodes there is a better chance that no job fails at the first two critical phases . i.e., the failure would occur at the easier quicker steps, which will lead to reduced time. our estimation when using  <dig> nodes is that the total time would drop to about  <dig> h with estimated cost ranging between $ <dig> and $ <dig> 

the data transfer times and costs are given in tables  <dig> and  <dig>  the amount of data transferred depends on the time point where the spot instances get terminated. the worst case scenario is when the sport instances get terminated at the very beginning of the workflow and in this case it is important to move most of the data from one cloud to another. for the exome data set, the worst case time is about  <dig> min at a cost of about $ <dig> . for the whole genome, the worst case data transfer time was about  <dig> min at a cost of about $ <dig> . this lead to an increase in the computation time, but the cost remained still less than using google only. when the spot instances fail in other time points, the increase in cost and time is neglected.

to sum up, merging the spot instance model with low bid and use of google as a host migration cloud would lead to cost reduction in many cases with minor increase in the running time.

CONCLUSIONS
in this paper we have introduced mc-genomekey, a package for variant analysis and annotation using computing resources from different cloud providers.

mc-genomekey can run either on google, amazon, azure clouds, or any combination of the three. it can also run in any cloud based on openstack. the package supports that the jobs of the same workflow be distributed over nodes coming from different clouds. in addition, it offers the option of handling the spot instance model by migrating jobs among the same or different clouds. the new features of the package allows easier usage through a command-line and web interface, allows faster execution through improved parallelization, and is more cost-effective via exploiting best business offers in different clouds. the associated cost reduction is a step towards the elimination of the barriers limiting the use of ngs in clinical settings.

mc-genomekey can be tuned to work with organisms other than human, provided that the parameters of the tools are set properly and the annotation database are put in required formats. fortunately, the annovar annotation system supports other organisms, such as mouse, worm, yeast, among others. in a future version of our system, we would support other organisms of interest to the community.

it is worth mentioning that mc-genomekey supports that each phase of the variant analysis workflow runs in a different cloud, as specified by the configuration file . however, we did not expose this feature to the mc-genomekey web-interface, because it is not yet of practical relevance. the reason is that the current pricing scheme makes it always cheaper to run the whole workflow in google if one searches for best price. also the current machine configurations and performance make it faster to run the whole workflow in amazon if one searches for fastest time, as shown in table  <dig> 

in spite of its cost advantage, the spot instance model was not considered as a useful model because of the sudden termination of nodes when the spot price exceeds the bid price. mc-genomekey has provided an efficient solution based on migrating the jobs associated with the terminated nodes to other on-demand nodes in the same cloud or in google cloud. it is also worth mentioning that the robust use of spot instances can lead also to a faster execution given a certain budget ceil. that is, the reduced price allows the user to allocate more machines to finish as fast as possible within the budget limit.

transferring the data among cloud sites is a limiting factor for some use case scenarios of multicloud. the use of high-speed data transfer solutions  would provide an efficient solution to this problem. in the next versions of mc-genomekey, we will integrate high speed data transfer solutions to further improve the performance. in addition, we will also work on integrating the streaming techniques presented in  <cit> , where the computation can take place while the data is being transferred to save more time.

while editing this manuscript google has provided the custom instances, where the user can shape the machines specifications. although there is no change in the price per compute unit, this will provide more flexibility in selecting the best infrastructure for certain applications. also while working on this manuscript, amazon introduced the concept of reserved spot instances, where the user can reserve the spot node without termination for up to  <dig> h with a price ranging between  <dig> and 100% of the equivalent on-demand price. these two examples show the severe competition between google and amazon and the importance of mc-genomekey enabling the user select best prices and options.

mc-genomekey is available at http://nubios.nileu.edu.eg/mcgk and https://bitbucket.org/shazly/mcgk. we are also working in integrating it in the tavaxy workflow management system  <cit> .

availability and requirements 

project name: mc-genomekey.


project home page:
http://nubios.nileu.edu.eg/mcgk, https://bitbucket.org/shazly/mcgk



operating system: linux.


programming language: python, c, c++, java.


other requirements: na


license: gpl.


any restrictions to use by non-academics: no restrictions.

abbreviations
awsamazon web services

bambinary alignment/mapping

bqsrbase quality score recalibration

bwaburrows-wheeler aligner

cpucentral processing unit

gatkgenome analysis tool kit

gcegoogle compute engine

grch38genome reference consortium  build 38

hg19human genome build 19

indelinsertion/deletion

maqmapping and assembly with qualities

ngsnext-generation sequencing

ramrandom access memory

s3simple storage service

snpsingle nucleotide polymorphism

vcfvariant call format

vqsrvariant quality score recalibration

