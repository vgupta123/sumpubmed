BACKGROUND
the objective of proteomics is to investigate proteins on a global scale  <cit> . the high-throughput and sensitive tandem mass spectrometry  platform is now a supporting technology for protein identification in proteomic research  <cit> . using the shotgun strategy, a large number of ms/ms spectra can be gathered in a few hours  <cit> . the ms/ms data is generally processed by the so-called database searching method  <cit> . automated software such as sequest  <cit>  and mascot  <cit>  can rapidly assign tryptic peptides to ms/ms spectra by searching a protein sequence database and then identify proteins by utilizing the identified peptides. a notable problem in the ms/ms data processing is the high false positive rate  of the database search results  <cit> . thus, validation of database search results is unavoidable and necessary work, particularly when processing the large amount low accuracy ms/ms spectra with sequest  <cit> .

there are many proposed parameters and algorithms for evaluating sequest database search results  <cit> . such approaches must confront two main problems: first, the complex physical and chemical mechanisms of the shotgun experiment make it difficult to model the matches between ms/ms spectra and peptides with a one-size-fits-all algorithm  <cit> . thus, database search software provides multiple scores, and many empirical and intuitive parameters are used in the validation of database search results. these parameters describe different aspects of the quality of the match and provide complementary information to the validation of the database search results. combining these parameters while considering their relationships is difficult. second, many factors can affect the distributions of quality control parameters, including the sample, the database, the experimental conditions, and other random factors  <cit> . avoiding the effects of such factors during the validation of database search results is difficult. in addition, large-scale proteomics always uses multiple, complementary ms/ms platforms and multiple database search software tools to acquire more results with a high confidence level. thus, a universal framework for quality control of results is needed  <cit> .

recently, the randomized database method has become an attractive framework for quality control of ms/ms database search results. by constructing a negative control dataset for each experiment ms/ms dataset and the given database, the randomized database method can provide a universal foundation for the result quality control for different types of database search software and minimize the effects of differences in samples, experiment conditions, and databases  <cit> . in the randomized database method, the negative control dataset is generated by searching the constructed randomized database and used to simulate random matches from the normal database. the false positive rate can be estimated using the numbers of matches from the normal and randomized database given a set of filter criteria.

moore et al. used the reverse database  for their qscore model in  <dig>  <cit> . subsequently, qian et al.  <cit>  and peng et al.  <cit>  used the reverse database method to investigate the problem of optimizing the cutoff value of xcorr and Δcn in yeast and human proteome research, respectively. recently, higdon et al.  <cit>  investigated some problems encountered in the application of the reshuffled database. as they noted, searching a combined database can yield more accurate fpr estimation than individually searching normal and reshuffled databases. based on the binomial distribution, huttlin et al. investigated the minimum error associated with the estimated fpr  <cit> . they pointed out that the estimated fpr for a large dataset could be quite accurate. randomized database methods have been widely used in many research projects  <cit> . however, different groups use different criteria; there is no standard statistical framework that can easily integrate commonly used parameters for the quality control of database search results.

there are two primary problems with the randomized database method: how to determine the filter criteria and how to estimate the fpr in succession. based on the hypothesis that random matches are randomly drawn from normal and randomized databases, formula  <dig> can be used to estimate the actual fpr  <cit> ; elias et al  <cit>  recommended formula  <dig> for reliable data quality control:

  fpr=nrnn 

  fpr=2nrnn+nr 

where nr and nn are the preserved number of peptide matches that pass certain filter criteria and derive from the randomized and normal databases, respectively. huttlin et al  <cit>  have given a statistical interpretation of formula  <dig> by using the binomial distribution. so, in this paper, we used formula  <dig> to estimate fpr. generally, the filter criteria are discriminant functions  of database search scores. determining the acceptance boundaries for database search scores  is a simple and commonly used method  <cit> . lopez-ferrer et al sought to introduce a statistical model that would provide a more complex df and thus improve the sensitivity of filter criteria  <cit> . in their model, xcc) and dcc of random matches were considered to follow normal distributions, and the distributions of xcc and dcc were assumed to be independent. the contour line of the estimated joint distribution of xcc and dcc was used as the filter boundary. however, we found that normal distributions do not fit well the distributions of xcc and dcc of the random matches in the lcq control dataset used in this paper; the χ <dig> goodness of fit test shows that we can reject the null hypothesis h <dig>  at a significance level of  <dig> . furthermore, the correlation between xcc and dcc is significant  which is inconsistent with the independence assumption made by lopez-ferrer et al. another problem with their model is that it cannot be generalized to the situations involving more parameters.

multivariate nonparametric models can describe data with complex and variable statistical structures. the term nonparametric is not meant to imply that such models do not use any parameters but rather denotes that the number and nature of the parameters are not fixed in advance but flexible. this advantage makes nonparametric models a powerful tool for addressing the problem of multiple parameters with variable distributions in the validation of database search results. using a set of kernel functions ; the nonparametric model can fit the distribution of multiple parameters directly with considerable accuracy  <cit> . generally, parameter estimation for a nonparametric model is an iterative optimization procedure. the fully nonparametric probability density function estimate  procedure proposed by archambeau et al.  <cit>  and david et al.  <cit> , which is based on a maximum likelihood estimate  and expectation-maximization  algorithm, is easily implemented with computer programs. in this paper, based on the randomized database searching, fnpdfe was used to estimate the multivariate pdf of the commonly used database scores, the contour lines of the estimated pdf were taken as the candidate dfs. we demonstrated that the fpr estimation errors of the newly introduced method were acceptable on the control datasets from different instruments , its sensitivity was also proved to be improved on the control datasets and the real sample datasets.

RESULTS
in this section, the dfs of the nonparametric model were discussed at first, and then we show that the sensitivity of the model could be improved by incorporating more features. the accuracy of the fpr estimation of the nonparametric model was investigated and the performance of the nonparametric model was proved superior by comparing with other commonly used methods in proteomics.

nonparametric model and the df
in order to illustrate the shape of the dfs derived from the nonparametric model, a two dimension model which used xcorr and Δcn was investigated at first. because xcorr significantly correlate with the charge state   <cit> , the matches with different charge states were processed individually. since a large percentage of correct matches have a double charge, the matches in the control dataset with a double charge are discussed here. using a trial and error approach, a model with  <dig> gaussian functions  fit the distribution well . figure 1a and figure 1b show the histogram and density function, respectively. the estimated error for each bin is shown in figure 1c. the small error  also demonstrates that the fit is accurate.

dfs that can simultaneously reject as many false positives as possible and accept as many true positives as possible are preferred. thus, the region in the feature space with fewer random matches is more preferred, and the contour lines of the pdf of the random matches are good candidate dfs . generally, random matches have a small Δcn and xcorr, while correct matches have a large Δcn and xcorr. correct matches with the peptide isoform  <cit>  have a small Δcn and a large xcorr. matches with a small xcorr and a large Δcn may be due to the limited search space of the database searching. these matches are rare and more likely to be random matches; they may be localized to the accepted region of the contour line dfs because these results are also rare random events. a new df of xcorr was added to exclude such matches: xcorr > mxcorr, where mxcorr is the mean of xcorr of randomized database matches . given an expected fprα, a target value fα can be searched to ensure the calculated fpr  is less than or equal to α. when searching for fα, nn and nr were counted according to the rules:

  ∑i=1npfg≤fα 

and

  xcorr > mxcorr  

where x =  is the observation, and n =  <dig> is the number of gaussian functions. many fα satisfied formula  <dig> and formula  <dig>  the one with the largest nn was used in the final df. figure  <dig> shows the dfs for different expected fprs and different charge states. the shapes of the boundaries were significantly different, which indicates that it is difficult to fit all the distributions of different charge states with a simple distribution. the nonparametric model can provide feasible solutions to this complex problem. since the resulting dfs are smooth, this method is more robust than the k nearest neighbor method  <cit> .

incorporating more features
one obvious advantage of the nonparametric model is that it can easily integrate more scores for validating peptide identifications. by taking into account more features and performing the classification in a high-dimension feature space, a more reasonable df can be found, and thus, higher sensitivity can be achieved. here, another powerful parameter called sim introduced by zhang  <cit>  in  <dig> and discussed by sun et al.  <cit>  recently was added to the nonparametric model. sim measures the similarity between the experiment and the predicted ms/ms spectrum which was generated by the kinetic model introduced by zhang  <cit>  and the mass error tolerance for aligning the ions was specified as  <dig> .

for the lcq control dataset, by trial and error, we found a nonparametric model with  <dig> component gdfs can work well . we also tried a model with  <dig> component gdfs, but its performance was not improved and two of the component gdfs had a coefficient pi near  <dig> . thus, we selected  <dig> component gdfs to build the model. when the expected fpr was  <dig>  and  <dig> , the actual fpr was  <dig>  and  <dig> , respectively. the number of peptide matches after filtering was  <dig> and  <dig>  which were  <dig>  and  <dig>  respectively higher than the results of the nonparametric model using xcorr and Δcn, respectively. the sensitivity increased to  <dig>  and  <dig>  respectively, and the specificity did not change. thus, by incorporating more features, the nonparametric model can provide greater discriminating power. in the following part of this paper, we discussed the nonparametric model with three features: xcorr, Δcn and sim only. all the model parameters used in this paper were provided in additional file  <dig> 

the accuracy of the fpr estimation
the control datasets were generated by analyzing a set of known proteins and peptides with ms/ms platforms, which were commonly used to validate the performance of mathematical models for peptide identification  <cit> . table  <dig> reports the actual fpr and the number of validated matches at two commonly expected fprs of  <dig>  and  <dig> . from table  <dig>  the following propositions can be made:

 in most cases, the fprs estimated by formula  <dig> were close to but larger than the actual fprs. thus, the quality of the resulting datasets was better than claimed. it facilitates the strict result quality control but some sensitivity is lost.

 for little datasets, such as + <dig> charge state matches of different instruments, the actual fpr was larger than the corresponding estimated fpr. the error of the fpr estimation was also a bit larger. this result agrees with the conclusions of huttlin et al  <cit> .

 the estimated fprs were not equal but close to the expected fpr. the smaller the resulting datasets, the larger the difference between estimated fpr and expected fpr. this arises from the rounding error in formula  <dig>  for example, with an expected fpr of  <dig> , the allowable number of random matches was less than  <dig> for the + <dig> charge dataset of lcq, because only  <dig> matches were left after filtering. thus, it is impossible to have an estimated fpr exactly equal to  <dig> . a preferred alternative is rounding the estimated fpr to  <dig> .

 the error of the fpr estimation at the expected fpr of  <dig>  is larger that of  <dig> . this result means that some unexpected contaminants exist. for example, in the lcq control dataset, peptide "hvgdlgnvtadk" was identified with high database scores xcorr =  <dig> , Δcn =  <dig> ) and the matched percentage of predicted ions reached 91% . this peptide comes from protein sp|p00441| sodc_human, which is not a protein in the control sample. but this peptide also belongs to protein sp|p00442|sodc_bovin, which may be contaminants in the sample because  <dig> proteins  of bovine were added to the control sample.

 manually checking the confirmed matches by the nonparametric model, we found that some results with large xcorr but very small Δcn were confirmed. in some cases, the peptide in the second rank was correct. for example, in the ltq dataset , a peptide "leaelek" was identified with xcorr =  <dig>  and Δcn =  <dig>  . the peptide at the second rank was "lealeek", a peptide from control protein p62937|ppia_human, because of the theoretic mass spectrum similarity between these peptides, which will result in some fpr estimation error.

compare the performance of nonparametric model with other methods
two other methods were also be widely used in the proteomic research. the first one  searches for the optimized cut-off values of xcorr and Δcn simultaneously while making the number of confirmed matches reached its maximum given an expected fpr. the resulting accepted region on the xcorr-Δcn plane is a rectangle. the second one  is peptideprophet , which is an empirical statistic model, introduced by keller et al  <cit> . peptideprophet provided the estimated error rates  at different probability score cut-offs. eer has similar meaning with fpr, so we used it as the measure of the quality of the resulting dataset and only the probability score cut-offs without additional criterion were used to filter the matches. in order to name it easily, we denote the nonparametric model as m <dig> in the following part of this paper. for the control datasets, the confirmed matches, the actual fpr and the sensitivity were listed in table  <dig> . some conclusions can be drawn:

 in each case, the sensitivity of m <dig> is the highest. the difference in sensitivity of different methods ranges from  <dig> % to  <dig> %.

 for the lcq and ltq dataset, the performance of m <dig> and m <dig> differs little and peptideprophet  which was trained by a lcq control dataset  <cit>  does not seem to work well on the ltq/ft dataset.

 the performance of the nonparametric model differs little on the dataset of different instruments. when the expected fpr is  <dig> , the sensitivity is above  <dig>  and it is above  <dig>  when the expected fpr is  <dig> .

 fpr estimation errors exist for different methods. in some cases, the error is large. this may be caused by the calculation errors because of unexpected contaminants and random errors.

application to large datasets
shotgun experiments always generate large datasets  <cit> . thus, the nonparametric model demonstrated to be effective with the control dataset should be validated using large datasets. at first, we investigated the quality of the confirmed matches by the nonparametric model . another  <dig> parameters which were commonly used to validate the peptide identifications of sequest database search results were calculated for each match. they are maximal continuous b or y ion series length   <cit> , the matched percentage of the predicted ions by sequest   <cit> , ranked preliminary score   <cit> , the continuity of b or y ion series   <cit> , the matched percentage of ion intensities in the experiment mass spectrum   <cit>  and the matched percentage of the peak number in the experiment mass spectrum   <cit> . the percentages of the confirmed results which passed the empirical rules  convinced us that most of these matches had a high confidence level. it must be noted that rsp =  <dig> is a strict rule  <cit>  and some correct matches may be lost if we require rsp =  <dig>  for instance, only 76% correct matches are with rsp =  <dig> in the ltq control dataset.

as a case study, we investigated the overlaps of the three methods on the ltq dataset. more than 90% of the matches confirmed by m <dig> or m <dig> were covered by m <dig> , and  <dig>   and  <dig>   of the matches confirmed by the nonparametric model were covered by m <dig> ∪ m <dig>  each method of the three can all provide some matches that are not covered by the other two because they utilize different filter boundaries and different parameters.

in table  <dig>  we gave the numbers of confirmed matches, non-redundant peptides, identified proteins  and the percentage of proteins with at least  <dig> or  <dig> peptide hits . the nonparametric model can confirm up to  <dig> % more proteins than the other two kinds of methods, which indicated that our model has a higher sensitivity. for the same kind of instrument, three methods gave about the same percentage of proteins with at least  <dig> or  <dig> peptide hits at different confidence levels. the percentage of proteins with at least  <dig> peptide hits reaches above 50% for the lcq or ltq dataset, but it is about 40% for the ltq/ft dataset. it is interesting that the percentage of proteins with at least  <dig> or  <dig> peptide hits can not be improved by improving the confidence level of the peptide identifications when one method is used.

note * it was the count of minimal protein list assembled by dbparser algorithm  <cit> .

discussion
due to the complexity of the peptide identification problem, many parameters have been proposed for use in modeling the quality of matches between ms/ms spectra and peptides. for example, xcorr and sim assess the similarity between theoretical and experimental spectra, and Δcn assesses the effect of database size. there are two main reasons for the simultaneous existence of multiple parameters. first, the complex physical and chemical process of the ms/ms platform makes it difficult to model the peptide identification problem universally  <cit> ). second, the huge computational burden of the database search makes it difficult to implement complex models. thus, most ms/ms data processing approaches currently used include two steps: 1) find candidate peptides quickly and thus reduce the search space; 2) validate the results carefully by taking into account more information. as in this paper, a popular way for quality control of data in shotgun proteomics is to generate a set of easily calculated scores measuring the quality of the matches in different ways and then to combine these parameters to validate the results  <cit> . the randomized database method provides a feasible framework for constructing a negative control dataset and controlling the fpr of the acquired dataset. the nonparametric model introduced in this paper provides a framework for feature integration and determination of nonlinear dfs. however, if too many parameters are used, the nonparametric model will encounter a computational problem. with too many variable parameters in the model, there may be many solutions to the mle equations. thus, the iterative process of the em algorithm may reach a local minimum, and good performance of the model cannot be guaranteed. thus, when many features are used, it is recommended that the features be partitioned into different groups by hierarchical clustering  <cit>  and the nonparametric model be applied to each cluster. other feature-space reduction methods such as principal component analysis  and partial least squares  can also be used  <cit> .

the em algorithm is guaranteed to converge  <cit> . however, if there are too many variables, it may reach a local minimum. for double-charged matches in the lcq control dataset , we also tried a gaussian mixed model with  <dig> components . the values of the ml function calculated in the iterative process of the em algorithm increased monotonically for the gaussian mixed model with  <dig> components, whereas for the gaussian mixed model with  <dig> components they initially increased and then decreased along the iterative step . the performance  of the 15-mixed models demonstrated the same pattern. it was confirmed that too many variables  do not lead to better performance. it is fortunate that the gaussian model with  <dig> mixed functions fit the data satisfactorily. for the large dataset and the model with more features, the number of component functions did not exceed  <dig>  if a more complex mixed model is needed, we recommend the following strategies: 1) optimize the ml function directly using more robust nonlinear optimization techniques such as the conjugate gradient and quasi-newton methods  <cit> ; 2) directly fit the histogram with an optimized binned method  using a rbf neural network; or 3) use another nonparametric model such as the adaptive kernel density estimation proposed by silverman  <cit> .

the computational burden of the nonparametric model may be doubted, especially for the huge ltq dataset. it is lucky that it does not need so many observations to build the nonparametric model. if the dataset is too large, we can resample the observations and use fewer observations to build the model. we tried this approach on the ltq complex dataset. the results achieved by the model built with randomly selected  <dig>  observations differed little from that of the model built with all the  <dig>  observations. thus, in the model building procedure, if the number of the observations exceeds  <dig> , we resample the dataset and randomly select  <dig>  observations to build the model and if the number of the observations is less than  <dig> , all the observations are used. therefore, the consumed time of the model building was less than  <dig> min on a pc with intel pentium  <dig>  <dig> g cpu and  <dig> mb memory.

the nonparametric model proposed in this paper is easy to use. first, a combined database is prepared containing the normal and randomized protein sequence. then database search is performed on the combined database and the results are collected; the normal and randomized database matches are labeled with the assistance of references provided by the database search software. the randomized database matches are then used to build the nonparametric model. in this step, a parameter set different from that described here can be used. to obtain the final results, a search for the df described in the "nonparametric model and filter boundary" section given an expected fpr is performed. the workflow shown in figure  <dig>  has been implemented by several matlab  scripts and in-house c++ programs. the database search results were collected using an in-house program called outsum.exe, which were stored in the *.out files given by sequest. the resulting data, stored in a plain-text file, were loaded into a matlab workspace. a script called noparq.m was used to build the nonparametric model. the programs used in this paper were provided in a compressed archive .

CONCLUSIONS
in this paper, we provide a framework for validation of peptide identification in shotgun proteomics that is based on the randomized database method and a nonparametric model. the practical problems in implementing the nonparametric model were investigated, and its performance was found to be better than that of traditional methods. the nonparametric model can provide a more flexible and accurate solution for df determination for quality control of large datasets in shotgun proteomics research. all the programs used in this work are available by request from the authors.

