BACKGROUND
comprehensive elucidation of the functional and structural units present in the proteome is the ultimate goal in proteomics research, and it is expected to provide basic data for a rational understanding of complex biological systems. as proteomics studies are being pursued  <cit> , the development of efficient methodologies for dissecting long protein sequences into their domains is becoming critical. this is because biologically important proteins are often large and are thus difficult to express, purify and characterize in a high throughput manner  <cit> .

experimental approaches for dissecting proteins are usually based on limited proteolysis, which has been used to explore protein domain boundaries  <cit> . although experimental protein dissection methods have been extended to high throughput protocols  <cit> , they remain essentially expensive and time-consuming.

computer-aided protein dissection approaches are relatively inexpensive, and thus represent promising methodologies that have practical values in high throughput proteomics research. the strategies for predicting novel domain regions, without sequence similarity to domain databases, can be categorized into two classes. the first strategy aims at directly predicting domain regions by analyzing various sequence properties of the foldable region . the second strategy is to first predict the location of the domain boundaries and then use this information to infer the domain's position . both strategies are essential to efficiently identify novel protein domains.

proteomics projects require the identification of soluble and well behaved proteins enabling a fast structural/functional analysis  <cit> . solubility is an important criterion strongly reflecting a protein's suitability for biophysical characterization. it can be readily monitored, and solubility assays are thus applied to large-scale studies  <cit> . furthermore, when solubility is used to assess domain dissection experiments, it appears that a large fraction of soluble fragments are indeed well folded as assessed by nmr  <cit> .

so far, reports of high throughput protein domain production protocols have mainly focused on their development and on optimizing individual experimental steps of the protocols. no study has mathematically modelled a protein domain production protocol in terms of a production system, and thus substantial room for cost-optimization through mathematical modeling remains available. in this report, we present a mathematical model for empirically optimizing large-scale protein domain production. our model conceptually divides domain predictions into the prediction of the domain region and its boundary, and it computes the expected number of successfully produced soluble domains, using a conditional probability between these two events. we estimated the model parameters using the experimental results from a computer-aided identification of novel soluble protein domains from kazusa huge protein sequences, in which  <dig> fragments, encoding  <dig> novel putative domains with slightly different domain boundaries, were expressed by using an e-coli-based cell-free system, and their solubilities were assessed with sds-page gels.

RESULTS
mathematical model of protein dissection
in our mathematical model, the prediction of a protein domain is conceptually divided into two steps: a first step that predicts the existence of a domain, and a second step that predicts its boundaries . "fragments" are domain fragments with specific termini residues, and each fragment is either soluble or insoluble. soluble fragments and insoluble fragments belong to the soluble  and insoluble set , respectively . s and sc are mutually exclusive sets, and sc is the complement of s. we define a "soluble domain" as a domain that encodes at least one soluble fragment. a fragment that is associated with a "soluble domain" is an element of the set d . according to this definition, some fragments encoding a soluble domain may be insoluble. the fragments encoding non-soluble domains, i.e., predicted domains for which all fragments are insoluble, are elements of the set dc . practically, a domain is defined as non-soluble if all of the tested fragments associated with a given domain  are insoluble. d and dc are also mutually exclusive sets, and their elements are fragments . the above classification yields four fragment categories : s ∩ d , sc ∩ d , s ∩ dc , and sc ∩ dc .

the probability of successfully predicting a soluble fragment, p, is expressed as   

since s ∩ dc is empty and all sets are exclusive. p is thus related to p as   

where p is the conditional probability of obtaining a soluble fragment of a soluble domain. when p and p are given, the probability of successfully predicting the existence of a soluble domain is calculated as   

note that eqs. 1- <dig> are direct consequences from probability rules for independent sets, without any approximations or assumptions.

in a large-scale experiment aimed at obtaining as many soluble domains as possible, cost-optimization is achieved by maximizing the number of soluble domains for a fixed number of tests. as an approximation, our model computes the expected number of producing soluble domains, edomain, by assuming average probability values over all of the protein domains examined in the experiment. our model examines m protein domains and generates n fragments per domain. according to this model, the expected number of soluble domains, edomain, is given by:   

where m and n are, respectively, the number of domains and the number of fragments per domain that are assessed. when the total number of fragments  is held constant, the probability of obtaining one or more soluble fragments, pn, is a function of n and p. the explicit form of pn depends on the experimental setting, and is derived in the next section for two specific cases.

we can further modify eq.  <dig> to add a set-up cost related to the analysis of a new domain. the set-up cost is taken into account by expressing the total cost as mn + mr, where r is the ratio between the supplemental cost of analyzing a fragment from a new domain and that of analyzing a new fragment from the current domain. keeping the total cost constant  yields:   

derivation of pn for two basic experimental settings
let us derive pn for two basic types of experimental settings. in the first one, the generation of n fragments occurs by independent events . this situation occurs in genetic screening experiments, where n fragments per domain are randomly selected and tested, allowing multiple copies of the same fragment to be tested. in this case, the mathematical expression for pn is simply calculated as:   

where pn is the probability of obtaining one or more soluble fragments of a soluble domain when n fragments are simultaneously tested. f is the  number of all of the testable  fragments, and f is the  number of soluble fragments associated to a domain.

the second situation occurs when n fragments are generated, but each fragment is selected only once . this situation occurs when the fragments are identifiable, such as in our pilot experiment with the kazusa sequences. pn is derived by using a hypergeometric distribution , which describes the probability of obtaining no soluble elements when n elements are drawn without replacement from a finite population of fcn elements . pn is given by   

where pnc is the probability of obtaining no soluble fragments when n fragments are tested  a javascript program, implementing eqs. 5- <dig>  is available in additional file  <dig> 

parameter estimation from a pilot experiment with kazusa huge domains
the application range of our mathematical model is not restricted to describe or analyze a specific domain prediction method , or experimental procedure . the specific settings/protocols are taken into account by adjusting or optimizing the values of the model's parameters. here, we will estimate typical values for the parameters using the solubility of protein domains predicted in the kazusa huge protein sequences  <cit> .

we first identified  <dig> putative domains using proteomix  <cit>  , and for each domain, we expressed several fragments with different n and/or c-terminal residues distributed over the predicted termini window . this yielded a total of  <dig> fragments; namely,  <dig> fragments per domain were generated, on average, to probe the domain termini. among the  <dig> fragments,  <dig>  were expressed correctly and were eventually assessed ;  <dig> fragments  were soluble, and  <dig> fragments  were insoluble .

for the purpose of discussion, let us estimate the parameters using the  <dig> fragments corresponding to the  <dig> domains that expressed correctly. among these domains, 75% , and 52%  and p =  <dig> ; table 1) of the fragments were soluble when  <dig> fragments were tested on average . p, which is the conditional probability of correctly predicting a soluble fragment of a soluble domain, is  <dig>  . the calculation of p from eq.  <dig>  using the obtained values of p and p, generates  <dig> . substituting  <dig>  and  <dig> for p and n, respectively, in eq.  <dig>  yields edomain/m as  <dig> , which is equal to p, within rounding error, since pn is almost  <dig> when n =  <dig>  the discrepancy from the above experimental edomain/m value of  <dig>  , is due to the use of average values. indeed, for  <dig> domains, all of the fragments were soluble, and  <dig> domains yielded a mixture of soluble and insoluble fragments . however, the corrections appear to be modest  and will not significantly affect the following discussion .

a: p; b: p; c: p; d: p; s, sc, d, dc: see the main text; all: all the considered set; p : the probability of predicting a soluble fragment of a soluble domain, similarly for the sc ∩ d , s ∩ dc , and sc ∩ dc ; standard deviations were computed by bootstrapping, in which  <dig>  sets of  <dig> randomly selected fragments were sampled.

let us perform a simple mathematical simulation to assess the discrepancy between the above experimental and calculated values of  <dig>  and  <dig> . the discrepancy can be resolved by setting the number of tested fragments per domain to, for example,  <dig>  and by uniformly using for all domains the above experimentally determined average values of  <dig>  and  <dig>  for p and p, respectively. as a result,  <dig> fragments are produced, corresponding to  <dig> domains, of which  <dig> do not yield any soluble fragment, since p =  <dig> . since p =  <dig> ,  <dig>  fragments are soluble. these soluble fragments are uniformly distributed among the  <dig> domains, and thus  <dig>   fragments per domain out of  <dig> fragments are soluble, which yields  <dig>  for p. using eq.  <dig>  p =  <dig> / <dig>  =  <dig> , and thus p pn =  <dig>  * { <dig> -  <dig> =  <dig> , which is equal, within rounding error, to the experimentally determined value of  <dig>  .

in the present experiment,  <dig> domains  could not be analyzed because of failure to express a fragment with a correct molecular weight, or to assess the solubility, which obviously indicates that the efficiency of our automated protein expression system has room for improvement. these  <dig> domains can be included in the analysis and will give lower  limits by considering them as insoluble  domains. the inclusion yields 50%  and 25%  for, respectively, the lower probability limit of predicting the domain existence and that of predicting a soluble fragment for  <dig> tested fragments, on average. similarly, some or all of the  <dig> domains that were considered insoluble in the above discussion might turn out to be soluble if more fragments were tested.

effect of the cleavage site on the solubility of the dissected domain
the effect of the cleavage site on the solubility of the dissected domain was also examined . all of the fragments corresponding to  <dig> domains  were soluble ; while kiaa <dig> - <dig>  kiaa <dig> - <dig>  kiaa <dig> - <dig>  kiaa <dig> - <dig>  kiaa <dig> - <dig>  and kiaa <dig> - <dig> were insoluble , indicating a failed domain prediction. on the other hand, the solubility of the remaining  <dig> domains was dependent on the position of the cleavage site, and both soluble and insoluble fragments were produced, depending on slight shifts of the cleavage residue at each of the n- and c- domain terminal ends . the effects of the cleavage site within the window were also examined. except for site  <dig>  which corresponded to either the n or c protein terminus, no differences in the yields of soluble fragments were observed . note that the small number of cleavage at site  <dig> simply results from the fact that not all domains were located at the n or c protein terminus.

optimum fragment number per domain
let us use our model to analyze protein dissection experiments with different settings. since p is independent of n, we can simplify eq.  <dig>  and examine the normalized expected number of soluble domains, edomain, instead of edomain:   

where pn can be computed with either eq.  <dig> or  <dig>  since no noticeable difference will occur for practical purposes . when no set-up cost is present , edomain is a monotone decreasing function of n . thus, one fragment per predicted domain will optimize the number of soluble domains . this is analytically demonstrated by showing that noptimum is the solution of a transcendental equation with a unique solution .

set-up costs are typically generated by the purchase of new chemicals associated with the examination of a new domain, and may, for example include the clone's cost, from which the domain fragments are prepared by pcr. large r values may occur in a genetic screening-type experiment, where the cost of assessing a new fragment is small as compared to that of starting a new experiment with a new domain. for r >  <dig>  edomain is not a monotone decreasing function of n, but reaches a maximum at n = noptimum . the value of noptimum increases for increasing values of r .

as an example, using the values derived from our pilot experiment  and assuming that r =  <dig>  we find that testing three fragments per predicted domain would yield more soluble domains than just one, for the same total cost . note that, as r increases, the peak broadens and the maximum edomain value, edomain , becomes smaller . thus, in practical terms, the edomain dependence on n is small for large r and it becomes less important to accurately determine noptimum. finally, we note that in eq.  <dig> , edomain depends on the ratio of f and f, which is the probability of finding the correct cleavage sites, but it does not depend directly on f alone, which is the total number of possible cleavage sites. the direct dependency on f alone is also minimal when the single copy model  is used .

insight into a wide range of experimental settings can be obtained by analyzing the behavior of noptimum for several values of p and r . for example, we find that noptimum increases with decreasing values of p, which is intuitively sensible, since a smaller p requires a larger number of trials for finding the termini residues that yield a soluble domain fragment. noptimum is between  <dig> and  <dig> for r =  <dig> and a broad range of p >  <dig> , which covers most experimental settings including our pilot experiment using kazusa sequences. a p >  <dig>  would also cover typical domain prediction tools such as armadillo prediction, which has a p value estimated by cross validation method between  <dig>  and  <dig>   <cit> . finally, for a typical value of r =  <dig> and p >  <dig> , noptimum is between  <dig> and 3; and for p =  <dig> , noptimum is  <dig> fragments, even for r =  <dig>  .

CONCLUSIONS
we have presented a novel mathematical formulation for optimizing large-scale protein domain production experiments. our model demonstrated that the testing of one to seven fragments per domain will fit most high throughput protein domain production experiments. the probabilistic approach presented here is not limited to protein domain production, and it can be readily modified and applied for designing various types of large-scale mutational analyses or screening libraries.

