BACKGROUND
the high-density oligonucleotide microarray, also known as genechip®, made by affymetrix inc , has been widely used in both academic institutions and industrial companies, and is considered as the "standard" of gene expression microarrays among several platforms. a single genechip® can hold more than  <dig>  probe sets for every gene in human genome. a probe set is a collection of probe pairs that interrogates the same sequence, or set of sequences, and typically contains  <dig> probe pairs of 25-mer oligonucleotides  <cit> . each pair contains the complementary sequence to the gene of interest, the so-called perfect match , and a specificity control, called the mismatch   <cit> . gene expression level is obtained from the calculation of hybridization intensity to the probe pairs and is referred to as the "signal"  <cit> . the normalization method used in genechip software is called scaling and is defined as an adjustment of the average signal value of all arrays to a common value, the target signal value in order to make the data from multiple arrays comparable  <cit> .

the purpose of data normalization is to minimize the effects of experimental and/or technical variations so that meaningful biological comparisons can be made and true biological changes can be found among multiple experiments. several approaches have been proposed and shown to be effective and beneficial. they were mostly from studies on two-color spotted microarrays  <cit> . some authors proposed normalization of the hybridization intensities, while others preferred to normalize the intensity ratios. some used global, linear methods, while others used local, non-linear methods. some suggested using the spike-in controls, or house-keeping genes, or invariant genes, while others preferred all the genes on the array. for genechip data, some have proposed different models to normalize signal values or normalize probe pair values  <cit> . despite the presence of other alternatives, many biologists still use the default scaling method and consider that such method is satisfactory and is useful to identify biological alterations  <cit> . with the increasing awareness and usage of genechip technology and willingness to continue to use genechip software among many biologists, it is worth improving the performance or correcting the problems of the software. in this report, the author has demonstrated that in the scaling algorithm excluding 2% of the probe sets with the highest and the lowest values did not have much benefit. however, the logarithmic transformation of signal values prior to scaling proved to be the optimum normalization strategy and is strongly recommended.

RESULTS
the statistical algorithm in current genechip software  for gene expression microarray data has eliminated the negative gene expression values, a problem present in earlier versions of the software  <cit> . it uses a robust averaging method based on the tukey biweight function to calculate the gene expression level from the logarithm transformed hybridization data  <cit> . the reported data of a probe set is the antilog of the tukey biweight mean multiplied by a sf and/or a normalization factor . when both the sf and nfaffy are equal to  <dig>  there is no normalization or manipulation of original data. both nfaffy and sf are computed in virtually the same way. nfaffy is calculated in comparison analysis to compare the array average of one experiment with that of a baseline experiment, while sf is obtained from the signal average of one experiment comparing with a common value, the target signal in absolute analysis  <cit> . the average value used in genechip is a trimmed average. it is not calculated from all probe sets, but from 96% of the probe sets after the 2% of the probe sets with the highest and the 2% of the lowest signals were removed.

in this report, a total of  <dig> experiments with rat u34a genechip were analyzed. as shown in table  <dig>  the total hybridization signals varied although all arrays were hybridized with the same amount of biotin-labeled crna and scanned with the same scanner of identical settings. the array of the highest hybridization intensities had  <dig>  times more signals than that of the lowest. the average array signals had  <dig> % variation in terms of coefficient of variation. the mean signals were significantly greater than the median signals on each array, indicating a non-normal distribution. the density plot showed a long-tailed and skewed distribution  and the average of such data is known to be sensitive to the larger values in the data set.

the rat u34a genechip contained  <dig> probe sets; hence 2% was about  <dig> probe sets. the sum of the 2% of the probe sets with the lowest signals accounts for less than  <dig> % of the total signals  and its impact on sf calculation can be ignored. however, the sum of the 2% of the probe sets with the highest signals, the trimtotal as used in this report, was responsible for about 40% of the total signals . the remaining 96% of the probe sets used for sf calculation, produced only about 60% of the signals. excluding 4% of the probe sets did not reduce the variation, but rather slightly increased the variation, which in turn resulted in a wider range of sfs . it was also found that the trimtotal was highly correlated with total signal , but less with medians  and the mean of log signals . the trimmed percentage  was found to be negatively associated with the median  and the mean of log signals , but not with the total signal of all probe sets.

among other approaches to global linear normalization, one can also use the median signal or the mean of logarithm transformed signals to calculate the nf. nflogmean showed a higher correlation with nfmedian than with sf. there were larger differences between nflogmean and sf than those between nflogmean and nfmedian . to test if the larger difference was a result of removing 4% of the probe sets from the calculation, another nf, the nftrimlogmean was obtained using the same data as for sf, but with a log transformation. there is a very significant correlation between nftrimlogmean and nflogmean . the 4% of the probe sets that was removed from nftrimlogmean calculation reduced the total data by only 4% after log transformation.

since it is impossible to obtain the true normalization factor, an average of the four global linear nfs mentioned above was used instead to estimate the 'true' nf. to compare them with the true nf, a score  is introduced. each nf is calculated against the respective 'true' nf to obtain its nfscore. the average nfscore  is  <dig> % ,  <dig> % ,  <dig> % and  <dig> % , and the sum of nfscore is  <dig> ,  <dig> ,  <dig>  and  <dig>  for sf, nfmedian, nftrimlogmean and nflogmean, respectively . the sum of nfscore indicated an accumulated variation from the true nf, and the larger the number, the larger the accumulated variation. an attempt to add a 5th nf obtained from the arithmetic mean of all probe sets of the array was also made to calculate and compare nfscore with each nfs, and the results showed the same conclusion . it is fair to conclude that nflogmean produced the least variation.

discussion
logarithmic transformation is a well-accepted approach for stabilizing variance and has become a common choice for data transformation and normalization for spotted microarrays  <cit> . much improvement has been made in genechip microarray technology and accompanying software during the past few years. the current version of genechip software has improved its performance and is better than the earlier versions that used the average difference to express levels of gene expression  <cit> . however, the normalization algorithm was inherited and remains the only and default option for gene expression data processing in both mas  <dig> and the newly released genechip operating software  software. they continue to use the arithmetic mean of signals to obtain the sf in absolute analysis  and the nf in comparison analysis   <cit> . it is clearly shown here that the trimmed average and the resulting sf had a larger variance than the median-based nf, or the nf based on the mean of log transformed signals. similar results were observed in other genechip expression arrays, such as mouse u74a and human u133a . elimination of the highest and the lowest 2% of the probe set signals did not stabilize the trimmed means. when intra-array variance was reduced by 40%, this approach cannot be considered to be optimal. the logarithmic transformation of signals stabilized the variation well and made the normalization process much less dependent upon the mean and less affected by the outliers.

although simple and popular, the global linear normalization has its drawbacks, especially when the relationship among multiple experiments or genes is not linear. to address such problems, several methods have been proposed to conduct local and non-linear normalization,  <cit> . data normalization is a very critical and important step for microarray data mining process. the use of different approaches to normalization may have a profound impact on the selection of differentially expressed genes and conclusions about the underlying biological processes especially when subtle biological changes are investigated  <cit> .

CONCLUSIONS
normalization of microarray data allows direct comparison of gene expression levels among experiments. a global linear normalization, called scaling has been widely used in genechip microarray technology for gene expression analysis. the scaling factor  is calculated from a trimmed average of gene expression level after excluding the 2% of the data points of the highest values and the lowest values. it is shown here that the 2% of the probe sets of the highest signals contained from 34% to 54% of the total signals. elimination of the outliers did not reduce, but increased the variation among multiple arrays. instead, normalization factors obtained from the mean of the log transformed signals had the best performance. thus, the current scaling method, although widely used, is not optimal and needs further improvement. the mean of logarithm transformed signals is highly recommended to use for normalization factor calculation.

