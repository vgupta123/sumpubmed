BACKGROUND
as compared to whole genome sequencing , at the present time, genotype sampling is a more cost effective strategy to identify variants associated with traits of interest. genome-wide association studies  using fixed content marker arrays represent a genotype sampling strategy that has been successfully used in a number of studies to identify snps significantly associated with complex traits  <cit> . additionally, by increasing the number of markers genotyped and using linkage disequilibrium to guide marker selection an increasing proportion of genomic variance can be interrogated during gwas  <cit> . the principal caveat of using fixed content arrays is that there is a diminishing return on investment for adding additional markers and a portion of the sequence variation is expected to be poorly interrogated. further, variants that are specific to any population that are not used to guide snp selection, including variants that are specific to a particular trait, are likely to be poorly interrogated. finally, trait-specific and rare variants may be poorly imputed from the typed markers. identifying population-specific sequence variants and adding them to the fixed content is a potential solution to this problem in order to increase the effectiveness of gwas. however, if sequencing costs continue to decline, it will eventually become more effective to employ sequencing techniques using the population of interest to find trait-associated variants.

even with exponential declines in the cost of next-generation genomic sequencing, there are still difficulties associated with using wgs to conduct association studies of complex traits because the moderate to small effect sizes of variants typically involved in the etiology of such traits requires large sample sizes. one approach to increase the power of such studies without increasing sequencing cost is to use whole-exome sequencing , in which only a small fraction of the genome is sequenced, but at high coverage   <cit> . a second approach is to perform wgs, but reduce the overall coverage. the success of this low-coverage strategy is contingent upon the ability to locate variant sites and accurately call genotypes when each site may only be covered by a small number of reads . however, if variant calling in low-coverage wgs is acceptable, the increased genomic landscape sequenced relative to gwas and wes allows for a greater chance of discovering novel variants and associations.

the variant calling algorithms that are currently used to measure genotypes from sequence data can be divided into three groups, each using successively more information. the simplest variant callers make use of the reads collected at a single genomic position from a single sample, and apply a statistical model to determine the most likely genotype at that position for that sample. the second type of variant caller includes information from multiple samples at a single site. in this case the multiple samples provide a prior likelihood that the site is a variant, and the caller determines the most likely distribution of alleles given the data. both of these methods are implemented in the program gatk unified genotyper  <cit> . the third type of caller uses multiple samples, but also incorporates the observed correlation between nearby variants, i.e. linkage-disequilibrium , to improve calls. the program thunder, which is based on the mach imputation algorithm  <cit> , uses a hidden markov model to identify genomic regions shared by different samples, and uses this information in calling variants  <cit> . in  <cit> , li et al. show that ld-based variant calling using thunder increases calling accuracy in low-coverage samples to the point at which low-coverage approaches become viable.

low-coverage wgs produces high quality genotype calls for common polymorphisms  where several of the sequenced samples can be expected to have the minor allele. the confidence with which rarer alleles are detected depends on the quality of the sequence, the number of individuals in the sample, and the prior probability that the allele exists. low-coverage wgs detects more polymorphisms than can possibly be detected with existing fixed content arrays but is not as good as deep wgs for detecting with confidence rare alleles or alleles that are specifically typed with fixed content arrays. the utility of low-coverage wgs will be greatest for studying populations that have not been subjected to prior systematic variant detection. low-coverage wgs with ~4x coverage currently costs between  <dig> and  <dig> fold more per sample than fixed content genotyping arrays, depending on coverage. if the cost of wgs falls, the current relative economic advantage of fixed content genotyping arrays is expected to decline in favor of the more comprehensive wgs.

the current report is part of an ongoing family study to identify sequence variants that predispose to substance dependence in a community sample of american indians  <cit> .

the specific aims of this report were to  validate the low-coverage wgs approach by comparing different variant calling approaches and the resulting variant calls from low-coverage wgs  to genotypes for approximately  <dig>  polymorphic variants measured with a first generation axiom affymetrix exome chip array and  provide a preliminary demonstration of the utility of the called variant data obtained from low-coverage wgs by investigating the kinship structure and calculating founder allele frequencies for this sample.

RESULTS
sample concordance
to evaluate the efficacy of variant calling methods the results of each caller were compared to genotypes from the fixed content exome snp chip. because the exome chip is specifically constructed to contain low-frequency markers, there is a subset of markers that have not been validated by affymetrix in known heterozygotes. an initial comparison between exome chip genotypes and thunder’s variant calls shows that for roughly  <dig> % of the non-monomorphic exome chip sites, concordance is essentially zero, indicating a systematic failure of the marker on the exome chip, with a and b alleles becoming reversed. while this data could potentially be recovered by reversing the a and b alleles in analysis, the chip manufacturer recommends dropping this relatively small number of markers. we therefore remove from consideration any marker with a concordance less than 2% across all samples .

we define the concordance of sample s as c
s
 = n
s,m
/n
s,v
, where n
s,v
 is the number of genotypes for sample s that contain at least one non-reference allele on the exome chip, and n
s,m
 is the number of those genotypes where the variant call matches the exome chip genotype. for the purposes of this calculation, an explicit or implicit missing genotype call from a given caller is interpreted as a homozygous reference call.

these calculations revealed a number of notable trends . first, increased information included in the calling algorithm leads to an increased fidelity of calling, with multi-sample calling outperforming single sample calling, and ld-aware methods outperforming both. the median concordance rates are  <dig> % ,  <dig> % , and  <dig> % . second, the marked dependence on sample depth observed with single sample and multi-sample calling is reduced for ld-aware calling. for samples with average coverage less than two, single sample and multi-sample callers fare poorly, with concordances dropping below  <dig> percent, whereas with thunder calling, no sample has less than  <dig> % concordance. the median improvement of ld-aware calling over the other methods is dependent on depth; for samples with less than 5x coverage, the median improvement is approximately 30%, decreasing to only 2% for samples with greater than 10x coverage. of note, concordance rates for ld-aware calling were similar in magnitude for variants inside and outside of coding regions .

in addition to overall concordance, we examined the false positive rate for each sample and genotype caller, defined as the fraction of variant sites for that sample called with at least one non-reference allele by the genotype caller that are homozygous reference on the exome chip. as seen in figure 2b, the overall false positive rate for all callers is generally less than  <dig> , with only a slight dependence on depth. the median values across sample are  <dig>  for multi-sample calling,  <dig>  for single-sample calling, and  <dig>  for ld-aware calling. surprisingly, in this low-coverage data, multi-sample calling produces a higher false positive rate than the single sample approach, but both are outperformed by ld-aware calling. note that real false positive rates in a sequencing experiment will likely be higher than those reported here, since we are restricting the current data set to the well-behaved variants that are part of a genotyping chip.

site calling
imputation uses correlations between nearby variants to help call genotypes. decreased variant frequency causes uncertainty in these correlations and therefore lower fidelity of calling. to investigate the role of variant frequency, we examine the fraction of variant sites discovered by thunder as a function of frequency and compare with the variant sites discovered by multi-sample unified genotyper. we define a variant site as a site at which at least one alternate allele is detected from among all samples in the exome chip data. we consider that site to be found by a variant caller if at least one alternate allele is called at that position, regardless of whether or not any genotypes match.

figure  <dig> displays the fraction of sites discovered within a set of frequency ranges. for variant frequencies greater than  <dig> , both multi-sample unified genotyper and thunder perform well, finding greater than 97% of the variant sites, with slightly more variants being discovered by unified genotyper. as the frequency decreases, both methods degrade in performance, however, thunder degrades more quickly. at the lowest frequencies, where the minor allele count in the exome chip data is  <dig>  unified genotyper identifies approximately 57% of the variant sites, while thunder identifies only 41%. note that there is no contradiction between higher concordance rates for thunder and better variant finding with unified genotyper, because ld-based methods are less adept at finding variants with a lower minor allele frequency. when a site is not noted as variant by thunder, this is equivalent to a reference call for all samples. because these variants have preferentially low frequencies, most of these reference calls will be correct, and the concordance is largely unaffected.

when unified genotyper is run in single-sample mode, the probability of finding each variant in a given sample is independent of the frequency of that variant in other samples. the overall probability of finding a site, then, will be increased for higher frequency sites since each sample with the variant provides an independent chance of finding the site. assuming that the probability of finding any given site in a single sample is p, and that the frequency of the variant site is f, the probability of finding the site in the cohort is given by 1-
f
. a curve of this form is shown in figure  <dig>  and the single-sample results follow its general shape.

confounding effect of kinship
to provide preliminary evaluations of the utility of the called variant data obtained from low-coverage wgs, we attempted to estimate kinship coefficients between all pairs of individuals in the present sample using the called variant data and to generate allele frequencies for the identified loci.

the ability of ld aware genotype calling methods to reliably impute genotypes will be affected by the presence of closely related relatives and the effective population size. this study has both closely related and cryptically related individuals from a native american tribe as well as population admixture with individuals of known european ancestry. inclusion of close relatives should improve the ability of ld aware imputation algorithms to call genotypes on specific haplotypes. in contrast, we would expect that the level of admixture present in the study population would effectively reduce the ability of ld aware algorithms to call genotypes. thus, there is the potential that the results from the present study may not be generalizable to other populations. because correlations between array generated genotypes and imputed genotypes from sequence data appear to be stable as a function of coverage, the confounding inclusion of relatives and admixture do not appear to be a significant factor. an analysis of a nearly-unrelated subset of the data would not be directly comparable to our original results, because of the large number of samples that must be removed even to limit relations to cousin-level.

during the process of confirming the relationships between subjects and their self-reported familial relationships, we observed a trend that suggests a possible impact of including subjects with different degrees of relatedness. figure  <dig> shows the distribution of kinship coefficients calculated from imputed genotypes using prest  <cit>  for groups that are expected to have the same kinship coefficient. there is a trend that the measured kinship coefficient is less than expected for closely related individual and greater than expected for distantly related and unrelated individuals. this suggests an uncorrected bias in how genotypes are imputed and/or that individuals have cryptic relationships. a comparison with kinship coefficients calculated from genotypes for a set of common variants contained on the exome chip exhibited greater than expected ibd sharing for all relative pairs suggesting an influence of cryptic relationships . the greater than expected ibd sharing for close relative pairs seen in the exome chip genotypes also suggests the possibility of bias in how genotypes are imputed, but this could also be the result of differences in content between the selected set of variants from the wgs dataset and the variants contained on the exome chip that were used to estimate kinship. in either circumstance, the high concordance rates between the wgs and exome chip data indicates this bias has a minimal impact on individual genotype calls.

corrected allele frequencies
to estimate allele frequencies for this population, it is necessary to correct for family structure. simply counting prevalence of alleles will result in an upward bias due to inclusion of alleles that are identical by descent. one solution is measuring the prevalence in a subset of the samples chosen with all pairs having a kinship coefficient calculated from the pedigree structure below a specified threshold. using a subset of less related individuals, however, will increase error due to sampling from a smaller population. a more powerful solution involves the use of the best linear unbiased estimate  of the allele frequencies based on the observed alleles, and kinship coefficients based on the pedigree structure  <cit> . allele frequencies were estimated using this method as implemented in the package mqls  <cit> . in the current data set, mqls-calculated corrections to the simple prevalence frequencies are small, with a root-mean-squared correction across all variants of less than 1%. these corrections are of similar direction and magnitude to those calculated in a simple allele prevalence on a less related subset of the cohort . the deviation between these frequencies and the whole sample uncorrected frequencies correlates with the mqls corrections at r = <dig>  indicating that mqls approach is performing as expected. this also provides further evidence that any bias in genotype as suggested by the underestimation of ibd sharing among closely related individuals using wgs data is likely to have a minimal impact on genotype calls.

figure 5a displays a two-dimensional histogram comparing mqls-calculated allele frequencies versus the frequencies from  <dig> genomes for samples with european ancestry  <cit> . figure 5b shows a histogram of the frequency differences between the two populations. in each case, the frequencies displayed are those of the non-reference allele rather than the minor allele. in each figure, similar features can be observed. first, the distribution is strongly peaked, mostly at very lopsided allele frequencies, where the european and native american samples agree closely. second, the distribution has wide tails of highly-variant allele frequencies. third, there is asymmetry between the two populations with a greater number of variants having a higher frequency in the european-ancestry cohort than in the native american group, reflecting the admixture of the current native american data set.

CONCLUSIONS
low-coverage wgs represents a genotype sampling strategy between fixed content snp arrays and deep wgs, but there are economic and coverage trade-offs between each of these approaches. because fixed content snp arrays became economically viable before either of the sequencing approaches, much has been written about the proportion of common sequence variants that can be imputed with commonly used, fixed content snp arrays . currently, however, there is substantial interest in studying increasingly rare variants in association studies of complex traits, which has led to increased focus on wgs approaches. assuming that this is likely to continue and the costs associated with wgs will continue to drop, wgs will eventually prevail as the analysis method of choice leading to the question of whether to pursue low-coverage or deep wgs strategies.

depth of coverage and calling technology are not chosen independently when designing a sequencing experiment. rather, an experimenter with a fixed budget must choose between fewer samples with higher coverage and more samples with lower coverage. the ability of ld-based calling makes the latter option viable, but previous studies do not explicitly show it to be preferable.

to make clear the viability of low-coverage sequencing, the present study evaluated variant calls from low-coverage sequence data using the ld-aware calling software thunder and the single- and multi-sample calling options in unified genotyper. the results demonstrated increased fidelity of variants calls made using the ld-aware thunder relative to calls made using multi-sample gatk to genotypes generated from a fixed content snp microarray. nonetheless, this increased fidelity came at the cost of failing to identify a number of very low frequency variants . despite this trade-off, we conclude that low-coverage sequencing still presents specific advantages over deep sequencing when economic conditions are fixed. these advantages are best illustrated using a hypothetical example in which we assume that equivalent genotype concordance could be achieved with ld-aware calling at 5x coverage and multi-sample calling at 15x coverage as suggested by figure  <dig>  in such a case, as seen in figure  <dig>  the call rate of the lowest frequency variants will be approximately 40%. however, for a fixed cost, three times more samples can be sequenced at low coverage than at high coverage. even assuming, in a worst case, that the ld-aware call rate will still be 40%, and further assuming that multi-sample variant calling at 15× coverage results in 100% call rate for the lowest-frequency variants, the total number of low-frequency variants discovered will still be greater in the lower coverage sample. this advantage becomes even more apparent if the higher-coverage sample is sequenced at a depth of 30× or greater.

furthermore, the net effect of the ability to find rare variants is unlikely to be felt in single-variant association tests, where it is uncommon for sample sizes to be sufficient to detect a significant association. rather, burden tests, in which attempt to identify genes or pathways where more deleterious mutations are observed in cases than controls  are expected to benefit from the greater discover of rare variants.

in the present study, we have not attempted to compare these variant callers across a full range of cohort sizes and depths, and cannot therefore generalize that any set of parameters will produce similar results using ld-aware calling. however, the performance of ld-aware calling as a function of these parameters has been addressed via extensive simulations in  <cit> , revealing several trends. first, genotype concordance is roughly independent of sample size  for coverage above 6x. second, as coverage decreases, the concordance decreases for smaller cohorts; at 2x coverage,  <dig> samples are required to produce concordance equivalent to the 6x case. finally, the snp discovery rate is far more sensitive to cohort size than concordance.

to provide a preliminary demonstration of the utility of the called variant data obtained from low-coverage wgs, we used these data to confirm the kinship structure of the study pedigrees, and compared calculated founder allele frequencies for this sample to those reported in the  <dig> genomes project for european ancestry participants. though this would represent only a very preliminary use of low-coverage wgs data in a gene-finding expedition, this illustration suggests that these data are appropriate for further analysis in linkage and associations studies.

though secondary to scientific concerns, the costs of storage and computation must also be considered when choosing sequencing and variant calling strategies. storage costs do not actively influence the choice of high or low coverage wgs because the storage scales with total number of reads, while the choice between high and low coverage is whether those reads will be distributed across few or many samples. the aligned sequence read data for this project with  <dig> samples with an average read depth of  <dig>  was  <dig> terabytes. in a high-coverage sequencing project with the same sequencing budget, the storage would be approximately the same, but fewer samples would be covered. the storage of variant data that is used by each approach for association analysis is roughly proportional to the product of the sample size and number of variants typed or imputed, and it is typically negligible compared to the read data. in this data set the compressed vcf files with  <dig> samples and  <dig>  m variants require  <dig> gigabytes or  <dig>  bytes per genotype, while the exome chip data in binary plink format for  <dig> samples for  <dig> snps require  <dig> megabytes or  <dig>  bytes per genotype. the main difference between the per-genotype size of the data files is the amount of stored supporting information and compression.

there is also a substantial greater cost required for the infrastructure required by both low-coverage and deep wgs analysis than fixed content genotype array analysis. the infrastructure for imputation with low-coverage wgs adds substantially to the cost. in this experiment the average cpu time for producing all-site single sample gatk unified genotyper calls was  <dig> hours using  <dig> cores of a single node in a linux cluster with  <dig> blade servers, each with 8-core  <dig>  ghz intel processors, 2× <dig> m l <dig> cache  and  <dig> gb of shared memory. the same cluster used approximately  <dig> cpu hours to run thunder on all  <dig> samples. more than  <dig> terabytes of disk space would have been used had metafiles generated during processing not been regularly deleted.

two aspects of thunder may limit its use in some situations. first, thunder only operates on snps, and does not call indels. second, thunder only works on autosomal chromosomes. neither of these limitations is fundamental in nature, but overcoming each will require further development. because of the advantages of low-coverage wgs described above it is likely that it will increasingly be used because of its economic advantages over deep wgs.

the present report demonstrates the viability of low-coverage wgs for identifying variants associated with complex traits and describes specific advantages this approach might possess over deep wgs. notably, additional improvements in variant calling routines are likely to occur, which could further improve data generated from low-coverage wgs. for example, while ld-based calling improves the overall call quality of low-coverage sequencing, low-coverage does not preclude the use of multi-sample techniques to discover low-frequency variant sites. though we have not explored this possibility, an approach merging multi-sample results at low frequency with ld-based results at a higher frequency may be better than either method alone. because ld-aware calling relies on correlations of nearby variants, its performance may be enhanced in a data set with many related samples, as in the current study. however, the performance of ld-based calling in settings with unrelated data  <cit>  suggests that this effect is not vital to the conclusions presented here.

