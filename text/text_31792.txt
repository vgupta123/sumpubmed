BACKGROUND
next-generation sequencing  has become the mainstream method for obtaining high quantities of genomic data during the past decade, and the increased accessibility of massive datasets has driven up the need for compatible analytic algorithms and software  <cit> . there are several key components for an assembly algorithm, including the capacity to handle massive data sets, the accuracy and efficiency of the assembly, the nature of the data set itself, and the intended use of the assembly results. the former two are dependent of the hardware and algorithms implemented, whereas the latter two influences the optimization strategy and the type of information to be extracted during assembly. for example, metagenomic studies commonly aim to understand the composition and relative abundances of the data set as well as the intra-species or inter-population heterogeneity, therefore the assembly depth and length as well as accuracy are prioritized for such data sets  <cit> .

a viral quasispecies is a group of highly genetically related viruses found in a single carrier and can be both abundant  and greatly diversified  within patient carriers . two main ngs platforms, 454/roche pyrosequencing  <cit>  and illumina genome analyzer  <cit> , have been commonly used for recent quasispecies-related studies. pyrosequencing has longer sequence reads and typically does not require data set assembly , although some studies still performed de novo assembly  <cit>  or reference sequence assembly  <cit> . illumina sequencing generates much larger data sets compared to pyrosequencing, but its shorter read length limits the efficiency for de novo assembly  <cit> . therefore, illumina sequenced viral quasispecies data sets are usually assembled using reference sequences as templates  while de novo assembly is applicable but not commonly used  <cit> .

the high throughput illumina platform, compared to the pyrosequencing platform, is capable of detecting greater amounts of genetic variation within viral quasispecies  <cit> . however, a major challenge for illumina quasispecies ngs studies is the sequence assembly of the data sets. sequence assembly using a reference approach is not only subject to bias of the chosen reference sequence, but also assembles less reads and thus less genetic variation information in the assembly  <cit> . de novo assembly should be able to provide the most complete and accurate genetic information of ngs data, but can be hindered by regions with high levels of diversity. the commonly used de novo assembly algorithms, such as velvet  <cit> , soapdenovo  <cit> , clc genomics workbench , and euler-sr  <cit> , were not originally intended for the assembly of metagenomics data with high diversity and coverage depth. recent progress have been made in the development of de novo assembly algorithms for metagenomes, such as metavelvet  <cit>  and genovo  <cit> .

in this study, we propose a partial de novo-reference assembly strategy, pdr, which is a de novo-reference hybrid assembly strategy that utilizes the completeness of de novo assembly while complementing its low-efficiency with reference assembly. pdr generates an in situ reference sequence by de novo assembly of a smaller yet less diverse partial data set followed by the reference assembly of the full data set. results show that the pdr assembly results are more complete and accurate than direct de novo or reference assembly of highly polymorphic metagenomic data sets. we also present a novel blast-based assembly pipeline, bbap, capable of both de novo and reference assembly specifically designed for assembly of metagenomic data sets. the assembly efficiency and accuracy of both pdr and bbap were examined using actual ngs data sets as well as in silico generated simulated ngs data sets and compared with the assembly results of other assembly methods.

RESULTS
to examine the performance of bbap and the proposed hybrid assembly strategy, we acquired  <dig> ngs data sets of hbv viral quasispecies from  <dig> hbv patient samples  <cit> . the  <dig> data sets used for assembly consisted of an average of  <dig> , <dig> 101-bp raw reads ,  <dig> , <dig> high quality reads , and  <dig>  hrurs . the optimized parameters for bbap assembly are listed in additional file 1: table s <dig>  the same parameters were used for all bbap assemblies in this study unless mentioned otherwise.table  <dig> average assembly statistics of all  <dig> data sets using bbap with multiple approaches

contigs assemblede
the full data sets were used in the bbap assembly with fd, sr, and pdr approaches, whereas partial data sets consisting of 1% of randomly selected rrs were used in the bbap pd assembly approach


apartial data set de novo assembly


bfull data set de novo assembly


csanger reference assembly


dpartial data set reference assembly of the full data set


eonly minimum assembled contig length > 150 bp was shown


rrs raw reads, hqrs high quality reads , urs unique representative reads, hrurs high redundancy unique representative reads , rihrurs reads included in high redundancy unique representative reads




bbap de novo assembly of full and partial data sets
the de novo assembly of the full data sets  resulted in an average of  <dig>  contigs  for each library with an average contig length of 321 bp, suggesting that the assembly results were fragmentized . for de novo assembly of partial data sets  of each data set, five partial data sets were initially randomly generated and assembled independently. because the pd assembly results of the partial data sets from each library were highly similar , a single partial data set and its assembly results were used for representation of the sample in further analyses. the pd assembly yielded fewer number of contigs and longer average maximum contig lengths, indicating the pd assembly results were not as fragmentized as fd assembly. furthermore, pd assembly required fewer contigs than the fd assembly to span the full genome to recover the full length hbv genome . pd assembly also yielded a higher proportion of mapped hrurs  and rihrurs  than fd, further demonstrating its better assembly efficiency.fig.  <dig> assembly results of full and partial d2_ <dig> data set by a bbap, b velvet, c soapdenovo, and d genovo. the contigs were aligned to the sanger reference sequence. metavelvet assembly results for both full and partial d2_ <dig> data set were identical to those of velvet and thus not shown




fragmentation is possibly due to high polymorphic reads from the same genomic regions recognized by bbap as different haplotypes and subsequently assembled into separate clusters. the proportion of polymorphic sites in overlapping contig regions of d2_ <dig> fd assembly was  <dig> times higher than that in non-overlapping regions . a similar trend was also found in d2_ <dig> pd assembly . the shorter fd assembled contigs  had a significantly higher proportion of polymorphic sites than the longer fd assembled contigs . hrurs that were included or excluded in the partial data sets  had average redundancies of  <dig> x  and 38x , respectively, within the full data set. additionally, the redundancies of the included hrurs in the full and partial data sets were highly correlated . this suggests the random selection partial data sets was unbiased and effectively excluded hrurs of low redundancies, resulting in lower polymorphism levels and, in turn, less fragmented assembly results.

bbap reference assembly with different reference sequences
to fully represent the full data set, the pd assembled contigs were used as references for the reference assembly of the full data set . for comparison purposes, a sanger sequence from each patient sample was chosen as the reference sequence for the reference assembly of the full data set . sr assembly resulted in single contigs with average lengths of 3207 bp, whereas pdr assembly produced an average of  <dig>  contigs with maximum and average lengths of 3148 bp and 1268 bp, respectively . both pdr and sr recovered full hbv genomes and similar levels of polymorphism in the consensus sequences , but the pdr assembly additionally identified hbv structural variants .

pdr alignment accuracy was also higher than sr. sr assembly of d2_ <dig> resulted in a single contig with  <dig>  hrurs, but only  <dig>  of the sr assembled hrurs were mapped to the two main pdr assembled contigs  covering the full hbv genome and have identical sequences as the sr contig. not only did the remaining  <dig> hrurs all mapped to one of the nine pdr assembled variant contigs, but the sr alignment qualities of those  <dig> hrurs was less optimal than the  <dig>  hrurs, shown by the significantly greater blast e-value and lower blast alignment score , both supporting the higher alignment accuracy of pdr assembly. overall, results of sr assembly and pdr assembly were similar in recovering sequence variation, but the latter included more hrurs and rihrurs with increased accuracy due to the additional mapping options of the shorter hbv variant contigs provided by the de novo assembly of the partial data set, whereas the lower assembly accuracy of the former resulted in low quality alignments and slightly more polymorphic sites.

we were able to measure the polymorphism level of bbap assembly results  by calculating the nucleotide frequencies for each position . furthermore, the nucleotide frequencies derived from bbap pdr assembly were validated by pyrosequencing , demonstrating the assembly results of bbap are reliable.

bbap assembly results compared with other assembly methods
we next compared the efficiency and accuracy of bbap to different assembly methods using both full and partial d2_ <dig> data set. similar to bbap fd, the full data set assemblies by velvet, metavelvet, soapdenovo, and genovo resulted in fragmented contigs. de novo assembly of full data set with velvet resulted in  <dig> contigs with maximum and average lengths of 1102 bp and 303 bp, respectively , and recovered only 19% of the hbv genome . metavelvet assembly results, which are based on initial velvet assembly results, did not show any improvement and were completely identical to velvet assembly results for both full and partial data set. soapdenovo generated  <dig> assembled contigs with maximum and average lengths of 934 bp and 340 bp, respectively, and covered 14% of the hbv genome . genovo assembly for the d2_ <dig> data set resulted in a total of  <dig> contigs with maximum and average contig lengths of 1352 bp and 395 bp, respectively, but only 44% of the hbv genome were recovered .table  <dig> comparison of d2_ <dig> assembly results with different methods and different data set sizes




we proposed that the high polymorphic nature of virus quasispecies may have hindered the efficiency of sequence assembly, and a randomly extracted yet less polymorphic partial data set may provide a better start for initial assembly as shown in fd vs. pd assemblies. assembly results of different methods all show that the assembly of the partial data set not only generated longer contigs, but also recovered more than 90% of the full hbv genome, demonstrating that exclusion of low redundant hrurs by random selection of partial data effectively reduced level of polymorphism which, in turn, improved the assembly results as judged by contig length and coverage .

we also noticed that bbap had better performance in recovering structural variants than the other methods tested. while some of bbap assembled hbv variants were validated by pcr sequencing , both velvet/metavelvet and soapdenovo did not identify any contigs with hbv structural variation. although genovo assembled  <dig> structural containing contigs, their accuracies were questionable as most of them with non-retraceable junction regions .fig.  <dig> schematic summary of corresponding hbv genome  regions for assembled contigs identified as hbv variants. arrows indicate 5’ to 3’ direction. only reads containing the sequences spanning the junction regions were assembled separately into variant contigs; reads spanning non-junction regions of the variant contigs  were assembled into the main hbv contig. the l <dig> sequence, which is similar to t <dig>  resulted from hbv variant validation with pcr using specialized primers followed by sanger sequencing. positions are in correspondence with nc_ <dig>  with dotted lines representing the remaining portion of the circular hbv genome, and the boxed section indicating the encapsidation signal 




results of in silico data set assembly
for a more general assessment and comparison of bbap performance, in silico ngs data sets were generated from the ncbi hbv complete genome and assembled separately using bbap fd, velvet, metavelvet, soapdenovo, and genovo. data set sizes were set to  <dig> , <dig> ,  <dig>  ,  <dig>  , and  <dig>  hqrs in combination with error rates of 10− <dig>  10− <dig>  and 10−4/site. due to computing time considerations, the maximum simulated data set size of  <dig> x was approximately 10% of the d2_ <dig> data set size. five independent data sets were generated for each parameter combination. bbap assembly results were highly consistent regardless of the data set parameter values. all but one of the  <dig> assembly results had both perfect coverage and accuracy; the lone standout assembly result had perfect coverage but a  <dig>   accuracy . the single “inaccurate” nucleotide was not an assembly error, but rather a degenerate nucleotide  representing the reference nucleotide  and the in silico generated erroneous nucleotide . the corresponding in silico data set was generated with the highest error rate  and smallest data set size , which is the most likely parameter value combination for erroneous nucleotides to exceed the minimum nucleotide frequency threshold .table  <dig> assembled results of in silico generated data sets from the reference hbv genome by different methodsa


0
0
0
0
0
0
0
 <dig> 
0
 <dig> 
0
5
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
1
0
11
0
 <dig> 
 <dig> 
0
 <dig> 
0
 <dig> 
 <dig> 
0
 <dig> 
0
2
0
0
1

aresults represent averages of the assembly results of  <dig> replicate data sets. bold areas indicate average assembly results with <80% coverage




velvet assembly of the in silico data sets produced mixed results . data sets with low error rates and/or small data set sizes were assembled with near perfect coverage and accuracy, whereas both large data sets and high error rates were poorly assembled. as the degree and amount of polymorphism are proportional to the error rate and data set size, respectively, results suggest velvet is inefficient in assembling highly polymorphic data sets. unlike the assembly results for d2_ <dig> data sets, metavelvet in silico data set assembly results, compared to velvet results, were improved with higher coverage and less fragmentation . metavelvet has wider parameter handling range than velvet, but was still unable to assemble highly polymorphic data sets with high error rates and large data set sizes. similar to that of velvet and metavelvet, soapdenovo could not efficiently assemble data sets of high polymorphism . in addition, soapdenovo also performed poorly when assembling data sets of low polymorphism . only data sets of medium sizes and error rates were efficiently assembled by soapdenovo . genovo assembly of smaller data set sizes , regardless of the error rate, were highly consistent, with only a single nucleotide assembly error among all  <dig> assembly results . the assembly result for the largest data sets  were slightly fragmentized across all error rates and on average  <dig> assembly errors were identified among high error rate  data sets.

discussion
we developed bbap, an assembly pipeline designed for the accurate and efficient assembly of highly polymorphic metagenomic ngs data sets. bbap implements a unique blast-based greedy algorithm to assemble data set reads and provides multiple intuitive parameters, depending on the nature of the data set, the sequencing platform, and information demands, to adjust the threshold for read alignment, variant retention, and error removal during assembly. bbap assembly results of both real and simulated ngs data sets were of higher quality than assembly results of other methods compared.

we also introduce a new partial de novo-reference  assembly strategy, which in situ generates reference sequences by de novo assembly of a randomly extracted partial data set to be subsequently used for the reference assembly of the full data set. current assembly approaches typically assemble the full data set straightforward with either de novo or reference assembly methods, each with their respective advantages and disadvantages. reference assembly is a much more direct process than de novo assembly which reduces alignment ambiguities and low coverage issues. however, the quality of reference assembly is reliant on the representation level of the reference sequence, as the assembly result will be biased towards the reference sequence and sequence variations not represented by the reference sequence will not be captured. de novo assembly, which is independent of reference sequences, possesses the potential to generate a more complete assembly result including majority consensus sequences and minor variant sequences, but can be hindered by coverage gaps that lack sequencing information and polymorphic regions with high levels of diversity as shown in tables  <dig> and  <dig> 

the partial de novo-reference assembly strategy utilizes the advantages of both traditional approaches to contemplate each other. de novo assembly of a randomly extracted yet less polymorphic partial data set provides assembly results that are more complete and highly representative of both majority sequence as well as minor variant sequences in the full data set. in turn, the following reference assembly not only assembles more reads due to the accurate representation of the reference sequences, but also has increased assembly accuracy than both straight-up de novo and reference assemblies . more importantly, the improved quality of assembly resulting from this hybrid pdr approach was not limited to bbap, as better assembly results using partial data sets were also demonstrated by velvet, metavelvet, soapdenovo, and genovo .

the assembly efficiency of metagenomics data sets is also dependent on the algorithms each assembly method employs. velvet, metavelvet, and soapdenovo all assemble ngs data sets through the construction of de bruijn graphs and eulerian paths. de bruijn graphs contain overlapping sequence information represented by branching nodes and stemming vertices, and is extremely sensitive and results quickly deteriorate even with the slightest amount of polymorphism  <cit> . the assembly algorithm of velvet and soapdenovo both manipulate the constructed de bruijn graph with error removal and simplification to generate optimal assembly results, which effectively excludes the essential polymorphism information vital to metagenomics data sets during assembly. in contrast, metavelvet decomposes the de bruijn graphs into individual subgraphs and assembles each subgraphs into separate contigs. on the other hand, bbap adopts a greedy assembly approach by incorporating and clustering sequence reads through blast results, and genovo implements a bayesian-based probabilistic model and takes into account the potential presence of multiple genomes in the data set. therefore, it was reasonably expected for bbap, metavelvet, and genovo to have better assembly results than velvet and soapdenovo when assembling metagenomics data sets, and this was consistent with our results that support bbap, metavelvet, and genovo are better equipped to assemble metagenomics data sets than velvet or soapdenovo.

we compared the average assembly times for in silico and ngs data sets on our server  between all methods to further assess the performance of both bbap and pdr. for smaller in silico data sets  bbap assembly time was slightly longer than velvet, metavelvet, and soapdenovo, but still within a couple minutes . bbap assembly time for the largest in silico data sets tested  were similar to the assembly time by the other methods except genovo, which required considerably much more assembly time than bbap or the other methods for all in silico data sets. the average bbap pdr assembly time  for the  <dig> ngs data sets was drastically faster than the average bbap fd assembly time . overall, results suggest not only do both bbap and pdr individually increase assembly efficiency and accuracy compared to their respective counterparts, but the combination of bbap and pdr together further improves the overall assembly quality of metagenomic data sets.

viral pathogens are responsible for the majority of pandemic and epidemic diseases listed by the world health organization. recent studies have utilized the advantages of ngs data sets of the viral quasispecies genome to construct genome-wide diversity profiles for studying the virus-host interactions during infection and, treatment and vaccination  <cit> . resistance associated variants and novel variants of the viral quasispecies usually are rare and not detectable by conventional or low depth sequencing, therefore detection of minor variants is clinically important for customizing patient management and treatment strategies  <cit> . our results show that bbap and pdr not only provided an accurate assembly sequence but also generates a high resolution diversity profile of the data set. additionally, we were able to detect and recover novel variants that were otherwise undetectable to alternative assembly methods.

CONCLUSIONS
assembly of a highly polymorphic ngs data set is a complicated process as it involves multiple steps  and is dependent of several prerequisite factors . in addition, a functional understanding of the algorithms and sufficient parameters are important for the optimization of assembly results. we believe both bbap and the partial de novo-reference assembly strategy will provide a powerful tool for future metagenomic and viral quasispecies studies.

