BACKGROUND
structural variation  is one of the key features of genetic variations among individuals. sv includes several types of sequence-level polymorphisms such as insertions, deletions, inversions, translocations, and duplications or copy number variations. a number of studies have implicated relationships between such svs and human phenotypes including diseases such as cancer susceptibility  <cit> , mental disorders  <cit> , metabolic disorders  <cit> , and some types of intractable diseases  <cit> .

while most single nucleotide polymorphisms  are di-allelic and easier to detect, many svs are multi-allelic in general and their patterns vary significantly among different sv types  <cit> . consequently, the detection of svs is much more difficult than that of snps. large-scale genomic svs have conventionally been investigated by southern blot analysis. in later years, fluorescence in situ hybridization to dna fibers , which is based on the hybridization of fluorescent probes onto chromosomes, has been widely used for the detection of svs  <cit> . such large genomic deletions in specific chromosomal regions have been reported to be associated with severe neuropathy and neurocognitive deficits  <cit> . the development of microarray technologies, such as array comparative genomic hybridization  and whole-genome snp genotyping technologies, has enhanced the study of human svs at the genome-wide level by detecting gains and losses of dna regions compared to the reference genome  <cit> . a high-resoultion statistical method to detect svs with a hidden markov model from illumina high-density snp genotyping data has been proposed  <cit> . however, there are limitations to array-based methods for sv detection. first, because the snp probes of these arrays do not uniformly represent svs distributed across the whole genome, some svs outside the targeted region might not be detected at all. second, the arrays can only detect svs of relatively large sizes covering more than several kilobases. third, they cannot detect the precise breakpoints of the svs. finally, novel insertions cannot be detected since they are not pre-included in array probes.

recent progress in ngs technologies have enabled us to detect svs more directly. more recently, several types of computational methods based on ngs data have been proposed for finding svs with higher resolution than snp array-based methods. in these analyses, typically 35- <dig> bp paired-end reads are mapped to the reference genome, and svs are inferred from the status of the mapped reads. the first approach, called read depth , utilizes the depth of coverage of mapped reads  <cit> . essentially, lower and higher depth values imply deletions and duplications of the region, respectively. the second approach, read pair , uses anomalous paired-end mappings of reads  <cit> . according to the separation distance and read orientation, svs can be inferred. the third strategy, split read , evaluates partial mapping of reads; this is employed by pindel  <cit>  and clipcrop  <cit> . pindel uses a portion of paired end reads in which one of the pair is unmapped. on the other hand, clipcrop uses 'soft-clipped' reads, in which a part of the read maps to the reference genome and the other does not. the soft-clip information can be obtained from the mapped result encoded in the sequence alignment/map  format  <cit> . the fourth approach, sequence assembly , assembles novel sequences from short reads locally. however, currently, there appear to be only a few integrative tools to detect all kinds of svs for different types and size, and the characteristics and performance of these various tools have not yet been extensively studied. recently, whole-genome sequencing data of many individuals have been produced very rapidly, as in the  <dig> genomes project  <cit> . hence, the development of a reliable and robust sv detection method from whole-genome sequencing data is urgently needed.

first, we evaluated the performance of several sv detection tools with simulated paired-end sequencing data for various deletion sizes. based on the evaluation, it would be possible to gain >90% precision and recall for a broad range of deletion sizes by combining different types of algorithms in a straightforward way. however, deletions detected by different callers often contain multiple entries for the same deletions, and these entries may have differences in their sizes, positions, and their reliablities. thus, a naïve combination of multiple callers' results may fail to produce accurate detection calls. we propose the integrated structural variant calling pipeline , which combines existing sv detection methods and resolves this problem by selecting a reliable subset of deletion calls and unifying duplicated entries. a tool based on a similar concept has been proposed, named svmerge  <cit> . the tool also combines sv detected results from multiple callers and generates non-redundant calls like isvp. the tool handles svs other than deletions, but the size of the results is restricted to > <dig> bp. isvp handles smaller deletions consistently and our procedure in the merging step does not depend on deletion size . in addition, the parameters employed in filtering and merging steps of isvp are determined by evaluating simulation data for a wide range of sizes.

we also investigate the relationship between depth of coverage and sv detection performance with our pipeline, and show that high coverage sequencing  is necessary to obtain good performance in sv detection in the simulation experiment. finally, we apply our proposed pipeline to whole genome sequencing data obtained from an na <dig> sample with an average depth of 45× and present a comprehensive picture of deletion events in which the resolution ranges from  <dig> bp to more than  <dig>  bp. we also confirm that some of the predicted deletions with our pipeline have been validated in several independent experiments  <cit>  and that its performance is equivalent to or better than that of tools used independently for all the datasets.

methods
evaluation of sv detection algorithms
we first compare and evaluate the performance of existing deletion callers from the synthetically generated ngs read data with various ranges of deletion size and depth of coverage. typically, sv detection algorithms are classified into the following types: read depth , read pair , split read , assembly , and combinations of those algorithms. table  <dig> summarizes the characteristics of deletion callers used in our comparison. breakdancer   <cit>  is classified as an rp-type tool that uses discordant read pairs  to detect svs. this method uses a distribution of fragment lengths of paired-end reads to find anomalous read pairs. its computational cost is much lower than that in other algorithms, and hence, easily applicable to find large deletion sizes. pindel  <cit>  is an sr-type tool that uses part of paired end reads in which one of the pair is unmapped. it splits each unmapped read and determines the break points of svs by an algorithm called pattern growth approach. delly  <cit>  uses a combination of rp, rd, and sr approaches. gatk haplotype caller   <cit>  is an as-type method that performs local de novo assembly of haplotypes via de bruijn graphs to detect snps and indels at base-pair resolution. however, the method needs a large amount of computational resources in terms of both memory space and cpu time. in our computational analysis, we used bd max version  <dig> , delly version  <dig> . <dig>  pindel version  <dig> . <dig>  and hc of gatk version  <dig> - <dig> 

sv detection tools are summarized according to algorithm types, detectable sv types, and computational resources required in our analyses. for each tool, cpu time and maximum memory size were measured for the 30× simulation data and the 45× whole genome sequence data of na <dig>  rp, rd, sr, and rp stand for read pair, read depth, split read, and assembly approaches, respectively. 'bd' is breakdancer. hc, haplotype caller; 'del', deletion; 'ins', insertion; 'inv', inversion; 'dup', duplication; 'tra', translocation.

* hc was performed with explicitly specified maximum memory size.

simulation data preparation
we prepared an artificial human genome sequence by adding svs, insertions, and deletions to the reference genome hg <dig> at randomly selected regions. the size of the svs follows the size distribution shown in the histogram at the bottom-right corner of figure  <dig>  which was constructed based on sv calling results from sequencing data in the  <dig> genomes project  <cit> . we then synthetically generated 100-bp paired-end reads from the genome sequence and prepared a set of simulated sequence data with average depths of 5×, 10×, 20×, and 30×. the insert size of paired-end reads was set to follow a normal distribution with a mean of  <dig> and a standard deviation of  <dig>  and a  <dig> % substitution error was considered at each nucleotide position. for paired-end mapping of the simulated data, we used burrows-wheeler aligner   <cit>  with the default options. the resultant sam file was then used in subsequent sv callers.

evaluation metrics for deletion calls
we defined the precision and recall of deletion calls for given size s as follows:

 precision= ∑i∈{all called svs with size=s}maxj∈{ all prepared svs}qji/n, 

 recall= ∑i∈{all prepared svs with size=s}maxj∈{ all called svs}qij/n, 

where n is the number of called or prepared svs and q is a quality value that is defined for each overlap between prepared svs and called svs, and takes a value between  <dig> and  <dig>  the quality q is defined as:

 qij=size/size, 

where a and b are the effective regions of called and prepared svs, respectively. the effective region is extended from the actual region by a fixed length margin as shown in figure  <dig>  the margin is introduced in order to retain sv calls that were correct but slightly deviated from the actual sv region due to ambiguity of mapping to the reference genome, often observed at interspersed repeats and low-complexity regions. we used  <dig> bp for the margin in our analysis; this resulted in 1-bp deletion call quality, with the position deviating  <dig> bp from the prepared deletion being  <dig> . the difference in quality score arising due to the introduction of the margin converged to  <dig> as the sv size became larger.

the proposed pipeline for calling deletions
as shown in figure  <dig>  isvp consists of three steps: 1) sv calling, 2) filtering, and 3) merging. in the sv calling step, we employ selected tools with different algorithms in parallel to detect a whole range of deletion sizes. next, in the filtering step, we extract information such as the sv type, called position, and size from each caller's output. we only utilize deletion calls whose size is within a predefined range that is determined to keep precision better than 90% from simulation data analysis . in the merging step, we first convert results from each caller to an extended bed format, which is convenient to compare overlaps of calls. in order to remove duplications, we remove one of the sv candidates whose precision is lower than the other if they overlapped each other by more than two thirds of their called regions. the precision for each call is determined by called size based on simulation results, and the detection of overlaps between calls is performed using bedtools  <cit> . finally, we merge the results into a unified sv call list.

RESULTS
simulation data analysis
we evaluated the performance of each tool in detecting deletions in terms of precision, recall, and their harmonic mean  with simulation data of varying deletion sizes and read coverages. the evaluation results with each tool for read coverage 30× are summarized in figure  <dig>  notably, hc predicted deletions highly accurately with precision >90% for sizes < <dig> bp. since the method is an as approach, this result suggests that the local de novo assembly algorithm around deleted regions was successful for relatively short deletion sizes. pindel performed better than other methods in terms of precision for deletion sizes between  <dig> bp and  <dig>  bp, retaining >90% precision and >90% recall. this result suggests that an sr approach, in which split reads were used for identifying breakpoints of deletions, was effective for identifying medium-size deletions. for deletion sizes > <dig>  bp, bd and delly performed comparably well, with precision >90% and recall >90%. these similar performances were possibly explained by the fact that they employ similar computational algorithms . the recall of delly was better than that of bd in our analysis.

based on the evaluation of deletion calls with each tool for simulation data, we determined the ranges of deletion size used in the filtering step of isvp . we used bd, pindel, and hc in the sv calling step of isvp. although the recall of delly was better than that of bd for simulation results , we used bd because the method showed slightly better precision in longer sv regions. as we will discuss in the section on computational resources, hc needs more central processing unit  time and memory space than pindel, and pindel can also detect deletion sizes < <dig> bp with high precision and recall . however, hc has even more precise calls in the region, and also determines the ploidy of each call, which is not estimated by pindel.

we confirmed that isvp succeeded in achieving >90% precision and recall for almost all sizes of deletions when the average coverage of depth was 20× and 30×, as shown in figure  <dig>  we also found that it was hard to achieve precision and recall >90% at the same time for sequence data with average coverages lower than 10×. the result showed that the depth of coverage was consistently effective for almost all deletion sizes. therefore, sequencing data of high coverage was essential for detecting deletions accurately and comprehensively.

real data analysis
we obtained the whole genome sequence of hapmap sample na <dig> from illumina hiseq  <dig>  the 100-bp paired-end data with an average depth of 45× was kindly provided by illumina inc. we applied isvp to the na <dig> data and predicted a total of  <dig>  deletions whose size ranged from  <dig> bp to  <dig> , <dig> bp. the histogram of predicted deletion calls with isvp is shown in figure  <dig>  it should be noted that the number of deletions exponentially declined with increasing deletion size. in addition, notable peaks around  <dig> bp and  <dig>  bp were found, which correspond to alus and long interspersed nuclear elements , respectively. these peaks were also found and reported in the  <dig> genomes project  <cit> .

in order to evaluate the prediction results of isvp in comparison to those of other methods, we compared these results with those of the experimentally validated deletion sets from studies by mills  <cit> , conrad  <cit> , and kidd  <cit> . the number of predictions with each method that was also validated by these studies is shown in table  <dig>  here, we defined true positive calls if their quality scores  were more than  <dig> . the typical deletion sizes in the mills, conrad, and kidd data were around  <dig> bp,  <dig>  bp, and  <dig>  bp, respectively. hc could not find most of the validated deletions because the as algorithm by nature has difficulty finding relatively long deletions. isvp and pindel performed well with the mills dataset, compared to bd and delly. on the other hand, isvp, bd, and delly performed better than pindel with the conrad and kidd datasets, as expected.

the sensitivity of deletion calling for each tool was confirmed by comparing to multiple datasets from the literature . the n in parentheses shows the number of total validated deletions for each experiment. bd and hc stand for breakdancer and haplotype caller, respectively.

although the numbers of true positives obtained using delly for the validated sets were close to those obtained using bd, the number of deletion calls with sizes > <dig> bp was significantly larger than that seen with bd, as shown in table  <dig>  this indicates that excessive numbers of false positives might have been called with delly. we examined the deletions only called by delly and found that, they consist of calls supported by a few  reads, or reads of low mapping quality. as expected from the simulation data analysis, isvp outperformed bd, pindel, and hc for all the datasets, verifying that our approach is effective and robust for deletion calling from real data analysis.

computational resources
in our analysis, we used red hat enterprise linux server release  <dig>  operating system with intel xeon cpu e5- <dig> processors running at  <dig>  ghz. for each sv calling tool, the required computational resources for sv detection from the simulation data with an average depth of 30× and real data  are summarized in table  <dig>  as mentioned in the background section, the largest amount of cpu time was required for as, followed by sr and rp, using simulation data. by comparing the results of simulation and real data, we see that bd and pindel required predictable amounts of cpu time and memory space based on the simulation data . for hc, we found that the cpu time was several times larger than expected. delly required relatively larger resources in terms of cpu time and maximum memory size for the real data . for isvp, most of the computational resources that isvp use are in the sv calling step. the cpu time and memory space consumed for the successive filtering and merging steps are less than  <dig> minutes and  <dig> gb, respectively.

discussion and 
CONCLUSIONS
we investigated several types of sv calling tools and evaluated their performance with a detailed simulation analysis. we found that there were significant differences in performance according to the employed algorithms and deletion size. each tool had its strength and weakness, and there was no algorithm that consistently outperformed others. hc, an as approach, performed especially well for deletions in the size range 1- <dig> bp. pindel, an sr approach, performed relatively better than other methods for deletions of 100- <dig>  bp. bd and delly, both rp aproaches, were able to detect large deletions. importantly, regardless of the algorithm used, high-coverage reads were consistently informative for detecting deletions. based on the simulation results, we developed isvp, a new pipeline to unify these methods with filtering and merging processes to comprehensively and reliably detect genomic svs. our approach succeeded in achieving more than 90% precision and 90% recall for a broad range of deletion sizes. we showed that a relatively higher depth of coverage  was required to gain good performance in sv detection from simulation experiments. this high-coverage requirement may be one of the reasons why comprehensive catalogs of svs are still limited at the moment.

by applying isvp to human whole genome sequence data from a hapmap na <dig> sample, we detected numerous svs that were biologically explainable, and some of them have been validated by other independent experiments. isvp is broadly applicable to high-coverage whole genome sequencing data with reasonable computational resources, which will enhance the genome-wide detection of svs for the identification of disease-causing variants. however, the number of recalls from real data was smaller than that expected from the computational simulation. this problem may be related to the complexity of sequences around the svs, which has not been sufficiently investigated yet.

our future work will include a study of the performance of isvp for other various types of svs other than deletions, such as insertions, duplications, and translocations, which are more difficult to detect and validate. furthermore, developing a pipeline to genotype multiple samples simultaneously is also a challenging and promising task.

competing interests
the authors declare that they have no competing interests.

authors' contributions
tm, nn, and mn conceived the study, tm, nn and kk designed the computational experiment, tm performed the analysis, and tm, nn, kk, and mn interpreted the results. mt, ao, ys, and yyk collaborated on data collection and interpretation of the results. tm, nn, kk, ys, yyk, and mn wrote the manuscript. all the authors read and approved the final manuscript.

