BACKGROUND
biological systems are extremely complex and as such the information content within them is huge. thus biological research is facing ‘big data’ problem  <cit> . next-generation sequencing  has made this problem vastly more challenging. today’s sequencing-based experiments provides a better understanding of complex biological systems allowing for various novel functional assays, including quantification of protein–dna binding or histone modifications   <cit> , transcript levels   <cit> , spatial interactions   <cit> , dna methylation modifications   <cit>  and others  <cit> . hence, proper interpretation of sequencing data has become particularly important. yet such interpretation relies heavily on complex computational analysis — a new and unfamiliar domain to many of the biologists — which, unlike data generation, is not universally accessible to everyone.

ngs data analysis comprises of several steps  <cit> . of these, ‘spliced alignment’ which involves alignment of the fragment  sequences to the reference genome, is one of the most crucial steps. during this step, the unknown reads as obtained from the sequencer are aligned with the reference genome, extended across the neighbouring aligned reads and annotated as genes, transcripts, or transcript isoform variants. this step thus reveals the identity and significance of the reads with respect to the reference genome. moreover this is the most time-consuming and computationally resource intensive step for such analysis  <cit> . in this work, we have tried to put forward a solution so as to overcome the problem of longer execution time and computational resource intensiveness involved in ‘spliced alignment’ step of ngs data analysis.

spliced alignment can be achieved using various software packages  <cit> . these packages can be broadly classified into two classes: unspliced aligners like bwa  <cit> , soap  <cit> , bowtie  <cit>  and spliced aligners like gsnap  <cit> , qpalma  <cit> , abyss  <cit> , trans-abyss  <cit> , tophat  <cit> . the spliced aligners are capable of aligning reads to the reference genome as well as allows for the identification of novel splice junctions. among the several available spliced aligners, tophat is the most widely used alignment tool. tophat is built on an ultrafast short read mapping program bowtie. at first it maps the reads to the genome to obtain the potential exons. this initial mapping information provides tophat with all the plausible splice junctions. the unmapped reads are then mapped against these plausible junctions to obtain the transcripts with annotated and novel splice junctions.

though tophat is one of the extensively used spliced aligners and its execution supports multithreading, it does not utilize cpu efficiently, leaves a large memory footprint during its execution and hence increases the time of execution. moreover the alignment protocol executes certain steps repeatedly while running on individual data sets belonging to the same experiment. such execution of redundant steps increases the execution overhead. thus there is a scope for considerable improvement of tophat, with regard to execution time, cpu and memory utilization without affecting the output. this can be achieved by modifying the execution workflow of tophat. thereby, the execution of the steps that underutilizes cpu can be run in parallel, the garbage memory can be cleared after completion of each execution step and the repetitive execution of the redundant steps can be skipped.

in this work, we have developed pipelined version of tophat , wherein we take up a modular approach by breaking tophat’s serial execution into a pipeline of multiple stages. we have implemented this modified spliced alignment execution pipeline for single end reads in a standalone system. the execution workflow for pvt utilizes cpu and memory comparatively more efficiently and reduces execution time without sacrificing the transcript annotation output of a typical tophat run. moreover we have customized pvt pipeline for paired end reads that ensures better performance over tophat.

the volume of ngs dataset has reached an order of terabase and can reach upto zetabase in the near future. these massive sequencing datasets demand high-performance computational resources, rapid data transfer, large-scale data storage, and competent data analysts. this increasing volume appears to impede data mining and analysis by researchers. hence standalone workstations will not be sufficient to handle such huge dataset. this necessitates the use of scalable, fast and computation intensive resources. a computational system known as ‘cloud’  <cit> , consisting of computation and data service provided via the internet, has recently been developed. cloud computing allows users to avail services provided by data centres without building their own infrastructure. the infrastructure of the data centre is shared by a large number of users, reducing the cost to each user. to manage the flood of ngs data, several large-scale computing framework have been implemented in cloud by crossbow  <cit> , galaxy  <cit>  and stormseq  <cit> . implementation of tophat in cloud cannot utilize the cloud resources efficiently due to lack of pipelined execution workflow.

here we propose an efficient execution workflow termed as pvt-cloud to facilitate execution of spliced alignment step in ngs analysis  in a cloud computing system. this will allow low maintenance, cost effective, scalable and dynamic control of the extensive computational resource.

RESULTS
we designed a new execution workflow termed as pvt to expedite the ‘spliced alignment’ step in ngs data analysis. we ran pvt on the dataset containing single end reads  and paired end reads . we compared its performance  with that of tophat. results show that pvt outperforms tophat.

single end reads
tophat and its pitfalls
the different steps along with their corresponding functions involved in the ‘spliced alignment’ is discussed in details in methods section . the order of execution of these steps during ‘spliced alignment’ using tophat is shown in additional file 4: figure s <dig>  the execution time for the single end read input dataset using tophat is shown in additional file 5: figure s <dig>  computational resource  utilization at different steps of ‘spliced alignment’ was noted for the entire single end read dataset. we have shown the result for srr <dig> data only in figure  <dig>  since this data consists of maximum number of filtered reads.

for the find_juncs step, we found that the cpu remains significantly underutilized  for a considerable span of time as compared to other steps. moreover, the average cpu utilization throughout the entire execution period for a single dataset is ~39%. further cache memory utilized by tophat during find_juncs step is also more compared to other steps these observations from the utilization graphs  gave us the clue that find_juncs is that step which can be modified to bring significant improvement in the efficiency of tophat. hence, parallelizing find_juncs would lead to an increase in cpu and memory utilization and decrease execution time. however, care should be taken towards the extent of parallelization so as to ensure no buffer overflow or cpu overutilization.

while analyzing the steps and workflow of ‘spliced alignment’, in details we found a redundant step in tophat where the alignment tool builds the bowtie indices for aligning the reads with the genome repeatedly for processing each data file of the same experiment. this adds on to the resource consumption. these indices can be built once and used for different data files unless the reference genome is updated or a new reference organism is being worked upon. we could increase the efficiency of tophat and reduce its execution time by building the bowtie reference indices only once for the analysis of the entire data set of an experiment.

pvt
the performance analysis reports obtained on executing tophat for single end read dataset  motivated us to develop pvt  wherein we modified the execution workflow of tophat for efficient utilization of cpu, memory and time. in pvt, the steps filter_reads, gene_align and genome_align are executed serially but we parallelized the find_juncs step so as to ensure proper utilization of computational resources, thereby reducing the execution time. figure  <dig> shows the pvt alignment pipeline and its order of execution for single end reads. rather than searching for the plausible junctions in the whole reference genome at a go, we looked for the junctions simultaneously across all reference chromosomes of the genome. thus we were able to parallelize find_juncs by first splitting the reads that mapped to the genome , based on chromosomal reference and then concatenating the split files into comparable chromosomal reference . the find_juncs is then run on separate chromosomal file references in parallel and the output of possible junctions are concatenated . this is followed by serial execution of the steps- junc_align, span_reads and report. parallel execution was achieved by executing the steps in both foreground and background. the blue and green bordered boxes in figure  <dig> represent the processes run in the foreground and background respectively. the parallelization of this step and the execution of the subsequent steps is scheduled in such a way that no dependent step begins its execution before the completion of the previous step.

prior to executing an independent step of the pipeline, clearing cache facilitates further parallelization. in pvt, the cache memory is cleared at every step to save the memory and meet the demand of other steps with high memory requirement. this helps avoid thrashing due to memory shortage and hence brings down the memory requirement for find_juncs .

in pvt, we have also addressed the problem of additional resource consumption for building bowtie indices. here, the bowtie indices were built just once for analyzing the entire dataset, thereby removing the redundancy involved in the process followed by tophat. this indexing took ~10 minutes in our standalone configuration for the single end read analysis, which reduces  <dig> % of the total execution time taken by tophat.

improvement in find_juncs: the cpu utilization during the execution of pvt pipeline for the entire single end read dataset  was monitored. results for srr <dig> are given in figure  <dig> since this data consists of highest number of filtered reads. results for other datasets have been provided in additional file 6: figure s <dig>  the cpu utilization in the find_juncs increases from a meagre ~12% to approximately 50% with our modified pipeline i.e. by parallelizing the find_juncs . we also observed that parallelization of find_juncs is limited to a cpu utilization of ~50% in an  <dig> core cpu standalone system. increasing the extent of parallelization to more than  <dig> processes with such dataset in an  <dig> core machine, increases the time of execution thereby putting the other processes in background to halt. however, the degree of parallelization can be extended provided find_juncs is executed in a standalone system having greater number of cores. overall, the performance gain using pvt will vary depending on the volume of the input data, availability of number of cpu cores and memory of the standalone system.

pvt outperforms tophat
 execution time:

the execution time for each step involved in tophat and pvt pipelines, run on the entire dataset was noted. as srr <dig> and srr <dig>  has larger number of filtered reads, the results for this data has been shown only . overall there was a significant reduction of ~23% in execution time using pvt, when compared to tophat execution. the improvement of pvt over tophat in find_juncs step for the entire single end read dataset is shown in additional file 6: figure s <dig> 

we compared the performance  of pvt with that of tophat throughout the entire ‘spliced alignment’ step  for the  <dig> input single end read datasets . figure  <dig> shows that there is a strong correlation between the number of filtered reads and the total execution time in case of both tophat and pvt. thus we were able to predict the execution time based on the volume of the input data . we also assessed the prediction accuracy  of the two pipelines by fitting a linear regression model to the curve, plotting time vs. number of filtered reads as shown in figure  <dig>  we obtained an r-squared value of  <dig>  for tophat whereas for pvt pipeline, r-squared value improved to  <dig> . in case of tophat there is a steep increase in the execution time when the input data size exceeds the available ram size. this may be due to the fact that tophat does not clear garbage memory after every step of execution thereby increasing memory thrashing. however, the steepness of the slope decreases for pvt  when we modified the execution by clearing the garbage memory at every step of the pipeline. we also observed that for smaller datasets  the performance gain of pvt over tophat was low but as the number of filtered reads increases, the performance gain increases.

 cpu utilization:

using pvt for ‘spliced alignment’, the average cpu utilization increased from ~39%  to as high as ~66% for the entire single end read dataset  and additional file 9: figure s <dig>  respectively).

paired end reads
tophat and its pitfalls
paired end reads  consist of sequence reads that occur in pairs  and are obtained on sequencing both ends of the rna simultaneously. this technique is used mainly to resolve large structural arrangements like insertions, deletions and inversions or repetitive regions of the transcriptome and is also useful for de novo transcriptome assembly. here we modified pvt for efficient ‘spliced alignment’ of paired end reads. the dataset srr <dig>  for paired end reads  comprises of 45 m reads .

the workflow of tophat for paired end read analysis is given in additional file 4: figure s <dig>  additional file 10: figure s <dig> gives the utilization graphs  and cache memory usage) during tophat execution of the paired end read dataset, where the steps denoted by ‘l’ are the ones for the left kept reads and ones denoted by ‘r’ are the right kept reads. the execution time for the different steps is shown in figure  <dig> for srr <dig>  the total execution time for ‘spliced alignment’ of paired end reads using tophat takes 4 hrs  <dig> mins for this dataset in the specified system configurations .

on analysing the execution log file for tophat, we observed that during the execution of the steps gene_align, genome_align , junc_align and span_reads, the left kept reads and right kept reads are processed independently of each other. these independent steps can be executed simultaneously which can reduce the entire job execution time. but tophat performs sequential execution of the independent steps along with the dependent steps, thereby increasing the waiting time for the dependent step to begin execution. this increases the overall execution time for analysing paired end read data set using tophat.

moreover, tophat breaks the read into segments. since these segments are mapped against the reference genome  and against the spliced junctions  independently, these steps can be parallelized for both the left and right kept reads. execution time of tophat can be further reduced by building the bowtie reference indices only once for the analysis of the entire data set of an experiment. these motivated us to design pvt for paired-end reads where we have addressed these disadvantages.

pvt
in pvt for paired end reads, we scheduled the pipeline so as to run the independent jobs simultaneously on the ‘host machine’ as well as on the ‘remote machine’ so as to reduce the entire job execution time. the workflow is shown in additional file 11: figure s <dig> 

the execution of the filter_reads step generates two output files corresponding to the left and right kept reads respectively. while running tophat, we observed that the output containing lesser number of filtered reads  after executing filter_reads takes lesser execution time for subsequent steps as that compared to an output having higher number of reads. again, there was a waiting time for the dependent processes to begin their execution while running tophat. hence, for an efficient analysis it is evident to transfer and further process the output of the step filter_reads containing lower number of reads to the remote machine since its transfer time  and execution time would be less than the transfer and execution of its counterpart with higher number of reads. thereby, the execution of the steps  and required transfers for the low read files, would complete simultaneously with the execution of aforementioned steps for higher read files. this will effectively reduce the waiting time for executing the dependent steps.

we denoted the output containing lower number of filtered reads by rlow  and the other having higher number of filtered reads by rhigh. hence for efficient execution of subsequent steps, rlow is transferred to the remote machine while rhigh is executed on the host machine. after filter_reads, the execution for rlow and rhigh can be forked into two simultaneous alignment processes executing the consecutive steps, gene_align and genome_align.

during find_juncs, the output of genome_align from both rlow and rhigh are required as input to find the spliced junctions. this requires transfer of the output generated in the remote machine  to the host machine. thus find_juncs is a dependent step, which waits for the two simultaneous processing of the outputs  of genome_align to complete. after the spliced junctions are found, the output for r
low
 is again transferred to the remote machine for consecutive execution of the steps junc_align and span_reads. simultaneously these steps for rhigh is executed on the host machine. in the final step report, the aligned output obtained from rlow and rhigh after execution of the previous steps  can be concatenated, thereby generating the final alignment output and possible spliced junctions.

theoretically the total time required for the execution of pvt and tophat  and ) for paired end reads can be calculated as follows:

  ti_pvt=maxtt→rlow+terlow+tt←rlow,terhigh 

  ti_tophat=terlow+terhigh 

here i denotes the sets of independent steps executed simultaneously in the host machine and remote machine. for i= <dig>  the set of independent consecutive steps to be executed are gene_align and genome_align and for i= <dig>  the set of independent consecutive steps to be executed are junc_align and span_reads.

 ti_pvt=theeffectivetimerequiredforcompletionofeachindependentstepsofpvt 

 ti_tophat=theeffectivetimerequiredforcompletionofeachindependentstepsoftophat 

 tt→rlow=transfertimeofrlowtotheremotemachineforexecutingeachsetofindependentsteps 

 terlow=executiontimerlowintheremotemachineforeachsetofindependentsteps 

 tt←rlow=transfertimeofrlowfromremotemachineafterexecutionofeachsetofindependentsteps 

 terhigh=executiontimerhighinthehomemachineforeachsetofindependentsteps 

thereby, the total execution time ttotal_pvt  and ttotal_tophat  is given as follows:

  ttotal_pvt=tfr+∑i=12ti_pvt+tfj+tr 

  ttotal_tophat=tfr+∑i=12ti_tophat+tfj+tr 

here

 tfr=timecontributionfromthestepfilter_reads 

 ∑i=12ti_pvt=timecontributionfromthetwosetsofindependentstepsdefinedaboveforpvt. 

 ∑i=12ti_tophat=timecontributionfromthetwosetsofindependentstepsdefinedabovefortophat. 

 tfj=timecontributionfromthestepfind_juncs 

 tr=timecontributionfromthestepreport 

the improvement  in the total execution time of pvt over that of tophat can be given by

  ttotal_tophat−ttotal_pvtttotal_tophat× <dig> 

using the above equations  and , we theoretically estimated the total pvt execution time for srr <dig> based on tophat execution time needed for each step. we obtained all the execution times terlow,terhigh,tfr,tfjandtr from tophat execution times noted for paired end read analysis. the transfer times in pvt tt→rlowandtt←rlow for the required inputs and outputs  using scp were noted for the corresponding inputs and outputs in tophat.

substituting the execution time required for each step using tophat and the transfer times for the independent steps  in equations ,  and ,we obtained a significant reduction of ~34% in execution time using pvt for srr <dig>  when compared to that using tophat.

 and  denotes the execution time for left and right kept reads respectively.

*time calculated using equation .

in the experiment, we implemented the pvt pipeline for srr <dig> in two similar standalone configurations and obtained the time duration for each of the steps as given in table  <dig>  the improvement of pvt over tophat as observed for the same data  experimentally was ~41% which is more than that as obtained by theoretical calculation i.e. ~34%. such added improvement might be due to additional modification of pvt over tophat which has been done by parallelizing sub-steps of genome_align and junc_align. this time reduction couldn’t be taken into account while calculating performance improvement of pvt over tophat theoretically.

pvt- pipeline setup for processing multiple data files
processing of multiple data files  in an experiment consists of repeated computation for spliced alignment. hence, pipelined execution of spliced alignment increases the speedup. pvt enables implementation of the execution workflow as a pipeline, consisting of multiple stages  <cit> . each stage can work on different execution steps at the same time thus requiring the pipeline to be run on separate instance/standalone configurations.

in pvt we have defined five stages  based on the steps executed for spliced alignment . we have selected the steps in each stage so that there is balanced length of pipeline stages which will increase the speedup. based on our analysis of execution time for both single end read and paired end reads , we merged the steps to build each stage of the pipeline as shown in additional file 11: figure s <dig> 

as stage ii,  is most time consuming, we can choose a larger instance/higher machine configuration or a cluster of instances to bring down the execution time of this stage to a comparable length as that of the other stages in the pipeline. thus, we are able to overlap the execution of multiple data files i.e. when a job is executed in a particular instance/standalone configuration , the next submitted job is executed in another instance/standalone configuration .

the stages of the pvt pipeline have different delays. hence we need to put appropriate buffers  in between the stages to synchronize their executions. since a buffer can be mounted to a single instance at a time, it will create problem for parallel execution of the consecutive stages in the pvt pipeline. to solve this, we can use two buffers and mount them to the consecutive stages in an alternate fashion as shown in figure  <dig>  considering a particular instance executing stage i of the pipeline, instance  executes its next stage. while executing multiple srr files submitted in the job queue in descending order, the head of the queue has the higher job number, thus the job with higher number i.e. srrj+ <dig> will be called in for execution before the job having lower job number srrj- <dig>  the buffers are mounted to the instances as shown by the solid lines at every even time slots and by the dotted lines at every odd time slots of the pipeline.

the pvt pipeline, described above, does not suffer from any kind of pipeline hazards  <cit>  since pvt stages do not have any dependency on each other. overall, there will be a significant reduction in execution time in pvt as compared to that in tophat.

the dynamic reservation of the instances and set up of the pipeline buffers as discussed above, can be faithfully implemented in the cloud computing system which provides an efficient and manageable architecture for pvt.

pvt-cloud: pragmatic cloud architecture
the presence of huge sequence data for alignment analysis, requires an extensive computational resource. although tophat in multithreaded mode can be implemented in cloud, it would reserve the instances for a longer duration of time, increasing the reservation cost and hence is incapable of taking the advantage of such an extensive computational resource to process extensive dataset. direct implementation of tophat in cloud fails to utilize the elastic feature of cloud resources. as pvt is able to overlap the execution of multiple data files and each pipeline stage  works on a different execution step at the same time, it brings upon a huge improvement compared to tophat. here we propose an architecture termed as pvt-cloud which can be implemented in a middleware based cloud architecture ) based on that as proposed by khatua et al.  <cit> .

we have defined an application as a set of ngs data, each of which is specified by an url to corresponding databases. the end user submits such an application to application repository for analysing each ngs data using pvt. the end user may set their policy, qos  etc. to control analysis of their ngs data, if required. the deployer module sets up the pipeline for pvt and finds the initial optimal resources for the pvt and the qos provided. once the pvt is setup, either in a standalone or cluster system, the required monitoring agents are automatically installed within the deployed resources. the monitoring agents send the status of the resources as well as pvt execution dynamically to the monitor module. the monitor module correlates the information sent by the agents to generate events for the controller module. each event designates a significant change  required in the pvt execution. once an event is received, the controller module takes the necessary action to optimize the current stage of pvt execution. for example, at the completion of a pvt stage, the controller module will schedule the current sequence data to the next stage while allocating the current stage to the next submitted sequence data. the monitor module sends notification to the user on successful completion of analysis of the submitted dataset. in this way, analysis of multiple ngs data will be carried out concurrently within pvt using a limited amount of resources.

presently, due to lack of resources, we are unable to show the performance of pipelined execution of pvt-cloud.

CONCLUSIONS
ngs data helps to understand the biomolecular interactions in depth. in order to analyze such large volume of data with high degree of accuracy, an efficient protocol is necessary that improves computational resource utilization. these days biologists are facing problems to manage ‘big data’. this demands better and enhanced insight into various ngs data. in any kind of sequence data analysis, alignment to the reference genome is the most important step to annotate and extract the significance of the read. tophat is the most widely used spliced alignment tool that determines transcript variants for both novel and annotated ones based on the alignment of the read sequences with the reference genome.

in this work we modified the tophat workflow for both single end and paired end reads in order to increase its efficiency with respect to its computation time and computational resource utilization . in pvt for single end reads, we parallelized the steps where the computational resource is underutilized and removed the redundant steps during the execution of each dataset which improved its efficiency and enforced utilization of computational resource along with the reduction of the execution time. for paired end reads we rescheduled the execution of each steps and distributed the job in separate machines, in addition to removing the redundant steps during the execution of each dataset. for single end read analysis, pvt resulted in reduction of the execution time to ~23% as compared to tophat, whereas for paired end read analysis the execution time reduced to ~41%. further, we proposed a cloud architecture pvt-cloud for running single end and paired end reads in cloud for a time effective method of processing ngs data. our modified protocol thus increases the degree of parallelization, computational resource utilization and thereby reduces the execution time in both standalone and distributed system architecture.

overall, our approach suggests betterment towards executing the spliced alignment step efficiently with a significant reduction in execution time and proper utilization of computational resources. pvt in cloud system ensures better performance than that in a standalone system. implementing pvt will speed up the execution process and will provide a cost-effective solution for ngs data analysis.

