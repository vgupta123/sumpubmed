BACKGROUND
the use of large-scale experimental and information technologies has dramatically increased the pace of production of biomedical findings, and the number of scientific articles has grown rapidly as well, which makes it impossible for human to retrieve or keep up to date with all the related information from the literature. during the last few years, there has been a surge of interest in information extraction and text mining of the biomedical literature  <cit> . when mining the biomedical literature, a big challenge is the problem of ambiguity inherent in natural language because one textual term may have several different meanings or senses . a number of natural language processing systems in the biomedical domain reported decreased precision due to the ambiguity problem  <cit> . weeber  <cit>  found that in order to replicate swanson's literature-based discovery of the involvement of magnesium deficiency in migraine, it was important to resolve the ambiguity of an abbreviation mg, which can denote either magnesium or milligram.

wsd is very critical for the biomedical text processing community but also very difficult because of the rapid growth of new words and new senses due to a large increase in discovery of biomedical entities. in  <dig>  the umls metathesaurus  <cit> , a comprehensive resource that specifies and categorizes biomedical concepts, contained  <dig>  ambiguous terms, and in  <dig>  the number increased to  <dig> , an increase of 126% within  <dig> years  <cit> . more importantly, this figure does not include the many terms associated with gene or gene products, and therefore the amount of ambiguity is likely to be much larger. studies associated with gene names have shown that the ambiguity problem is complicated because a gene term: 1) may refer to a gene or another type of biomedical term  <cit> , or to a general english word  <cit> ; 2) may be used to denote an rna, a protein, or a gene  <cit> ; or 3) may be highly ambiguous across multiple species  <cit> . if each ambiguous gene symbol in an article were accompanied by its corresponding long form, the disambiguation task would be much easier. however, schuemie  <cit>  analyzed  <dig>  biomedical full-text articles and found that only 30% of the gene symbols in the abstracts were accompanied by their corresponding full names, and only 18% of the gene symbols in the full text were accompanied by their gene names. schijvenaars  <cit>  showed that 33% of the human genes in their thesaurus were affected by homonymy. chen  <cit>  found that  <dig> % of mouse genes were ambiguous with other gene names and 233% additional 'gene' instances were retrieved when gene names that were also english words were included when processing the literature.

to demonstrate the extent of the ambiguity problem in medline we searched medline abstracts to determine how many abstracts contained gene symbols that were ambiguous with general english words or biomedical terms. using data from entrez gene <cit> , the gene-specific database at the national center for biotechnology information , we formed two ambiguous word lists for the mouse organism: a gene-english list  and a gene-umls list . then we searched  <dig>  abstracts that are known to be related to mouse genes  to determine the number of abstracts that contained at least one ambiguous word in each of the above two lists respectively, so that we could determine the percent of abstracts that contained a word that was ambiguous with an english word or with a umls term respectively. we repeated the same procedure for the fly and yeast organisms as well. results showed that for the mouse organism alone,  <dig> %  of the abstracts were affected by an ambiguity between a gene symbol and a general english word, and  <dig> %  were affected by an ambiguity between a gene symbol and a umls term. for the fly organism, both numbers were also over 99%, while the number was much less for the yeast organism:  <dig> % and  <dig> % respectively. to demonstrate that the ambiguity problem is not limited to a small set of words, we systematically removed ambiguous words with a frequency  higher than a threshold and re-calculated the percentage of abstracts that contained the remaining ambiguous words. in order to reduce the percent of abstracts with ambiguity from gene-english and gene-umls to a relative low level , ambiguous words with frequencies higher than  <dig> % would have to be removed, which covered  <dig> %  and  <dig> %  of all the ambiguous words in the two lists respectively. the same study, which was also performed for the fly organism, showed similar results, but with slightly higher ambiguity rates. this study shows that the ambiguity among gene symbols, english words and other biomedical terms is extensive and the distribution of ambiguity is very sparse. this study therefore demonstrates that word sense disambiguation is critical for biomedical text mining and retrieval tasks because ambiguous words have a substantial affect on performance. for the details of the ambiguity study, please refer to the sub-section "gene ambiguity for mining medline" in the methods section.

research in automated wsd can be traced back to the 1950s  <cit> . a number of wsd methods have been addressed for the general english domain. more recently, supervised machine learning  technologies have received considerable attention and have shown promising results  <cit> . bruce  <cit>  applied a bayesian algorithm and chose features based on their "informative" nature. they tested their methods on the interest corpus, which is a corpus consisting of  <dig> different senses for the word interest, and achieved a precision of 79%. lee  <cit>  evaluated a variety of knowledge sources  and supervised learning algorithms , naive bayes, adaboost, and decision tree algorithms) for wsd on the senseval- <dig> and senseval- <dig>  <cit>  data. using all of the knowledge sources, the svm method achieved the highest accuracy rate of  <dig> %. mohammad  <cit>  studied the contribution of lexical features and syntactic features to wsd, and results showed that simple lexical features  used in conjunction with part of speech information achieved better results  than other feature combinations.

another type of wsd approach uses established knowledge from curated terminology systems  <cit> . in the biomedical domain, schijvenaars  <cit>  developed a simple thesaurus-based algorithm to disambiguate human gene symbols using training data from pubmed abstracts and annotations from the online mendelian inheritance in man <cit> . the system achieved an accuracy rate of  <dig> % on an automatically generated testing set. schijvenaars's study described an effective method for gene disambiguation, but the evaluation results were limited to certain conditions. the automatically generated testing set contained human genes symbols that appeared as long-form and short-form pairs ) in articles, where at least  <dig> articles were determined to be associated with each gene sense. however, in situations where the gene symbol in the paper is ambiguous with a common english word or other type of biomedical word, which is not an abbreviation , the performance of the method is not known: a complete non-abbreviated word may have different characteristics in the text than an abbreviation. for example, this method may not be appropriate for testing a word such as "blind", which is not an abbreviation, but refers to both a gene and a general english word. an additional issue is that this study limited the disambiguation of gene symbols to gene senses and one other category called "non-gene sense", but the actual sense in this category was not resolved. this could be critical for nlp systems accessing phenotypic or disease-related information. an additional limitation of a knowledge-based method is that terms associated with phenotypic senses or general english senses may have little reliable background knowledge available. therefore, this type of method may not be applicable and ml approaches may be useful. recently, humphrey <cit>  proposed another type of statistical-based method to resolve the ambiguity problem within the umls metathesaurus. they used a journal descriptor indexing  method, which is ultimately based on statistical associations between words in a training set of medlne citations and a small set of journal descriptors assumed to be inherited by the citations. on a testing set with  <dig> ambiguous strings from nlm's wsd test collection, the overall average precision for the highest-scoring jdi version was  <dig>  compared to  <dig>  for the baseline method.

supervised ml methods have also been applied to wsd in the biomedical domain. hatzivassiloglou <cit>  developed a disambiguation system to determine the class of a known biomedical named entity by choosing one of three pre-defined senses: gene, rna, protein. he investigated the contribution of different features: positional information of surrounding words, capitalization information, stop-words and similarly distributed word removal, and stemming, and obtained accuracy rates up to 85% with optimised feature combination. ginter  <cit>  introduced a new family of classifiers, which were based on an ordering and weighing of the feature vectors obtained from word counts and word co-occurrence in the text. this method was used to determine whether a term was a gene versus a protein and achieved 86% accuracy. podowski  <cit>  built a two-step classification system to disambiguate gene symbols: the first classifier determined whether the word was a gene versus a non-gene, and the other determined the appropriate gene for a symbol classified as a gene by the first classifier. they reported an f-measure of over  <dig>  for genes with sufficient number of known document references. liu  <cit>  investigated the effect of window size and claimed that biomedical ambiguous words needed a larger window size than general english ambiguous words. in liu's  <cit>  paper, the gold standard data set was automatically constructed utilizing the fact that authors sometimes define abbreviations when they are first introduced in documents using parenthesized expressions  and that the same abbreviation had the same sense within a document. the training data set was automatically annotated using unambiguous synonyms, and for some senses, there were limited samples  for certain datasets. in this study, we used  <dig> abbreviations from liu's abbreviation list. however, we used a different method to collect the datasets because we wanted to control the sample sizes of the senses for our experiments. leroy  <cit>  tried to reduce the training sample size by supplying external knowledge from the umls for supervised machine learning algorithms, but the results were not promising. gaudan  <cit>  developed an algorithm based on use of svms to resolve abbreviations in medline and claimed a precision of  <dig> % and a recall of  <dig> % on their testing set. in their study, rare senses  were excluded from the testing set. this makes the disambiguation task easier because it reduces the problem of sparse senses. in addition, the training set was created based on long-form and short-form pairs, where ambiguous words not having long-forms were not tested. there is a good review of current research of wsd in biomedical domain by schuemie  <cit> .

most of the above papers reporting on the use of ml for wsd follow a similar pattern. a set of ambiguous words is selected, a corpus for each word is collected, and the different senses within the corpus are annotated . a feature vector is then formed based on the context of the ambiguous word, a supervised machine-learning algorithm is used on a portion of the corpus to train a classifier for the word, and the classifier is tested on the remaining corpus. the main variations are usually in the selection of features and choice of machine-learning algorithms. experiments are usually performed on a fixed amount of documents  per an ambiguous word, where the entire set consists of all the senses, and the sense distribution is generally uneven. results  are reported and an analysis of a few issues is often described, but the results of different experiments are usually not comparable because multiple confounding issues affect them. these papers are important in that they report on useful methods and provide insights and overall results. however, a deeper and more systematic analysis is needed in order to obtain a better understanding of the different factors affecting the performance of ml methods for wsd. in this paper, we discuss a number of issues explicitly and describe some experiments that simulate a variety of situations where different sense distributions, different sample sizes, different levels of difficulties, and different cross validation methods are studied and the effects are quantified. we subsequently based our assessment of performance on error rates and associated standard errors. although some issues we have addressed in this paper have been mentioned by other papers, our work differs from related work because we focus on a systematic study of issues affecting performance and quantify their effects in order to further understanding of the components of the error rate, which should lead to an improved and more generalizable methodology. our method also differs from related work because the sample size for each sense is always fixed, whereas in related work the sample size for the entire corpus is generally fixed but not the sample sizes of the senses.

RESULTS
four ambiguous abbreviations: bpd, bsa, pca, and rsv, were used in this study. they were chosen because they were associated with varying degrees of differences between their respective senses. for example, two of the senses of pca studied are very similar whereas two senses of bsa are very different. table  <dig> lists the detailed information about the abbreviations and their senses, and the methods section explains the differences in more detail. for each abbreviation, we measured error rates of the svm classifier under different combinations of sample size, sense distribution, cross validation scheme , and multi-class svm algorithms . for details of the testing data set and experimental design, please refer to the methods section.

tables  <dig>   <dig> and  <dig> display the results for bsa, pca and rsv, each of which has two senses. the distribution shown with bold font in column  <dig> is the estimated distribution of the senses, which is calculated based on the number of retrieved articles for each sense and the number of retrieved articles for all the senses. column  <dig> is the number of total samples from all senses. the range of sample size per sense ranges from 10– <dig>  with increments of  <dig> per sense. average error rates  and average standard errors  were reported for each combination of distribution and sample size .

with a distribution of  and 5-fold cross-validation, the error rate for bsa dropped from  <dig> % at sample size  <dig> to  <dig> % at sample size  <dig>  with the same sample size change, the error rate for pca dropped from  <dig> % to only  <dig> %. results for bpd are shown in table  <dig>  which contains the results from three different multi-class svm algorithms. we used friedman's test  <cit>  to compare the different multi-class algorithms, and stratified the analysis by probability distribution using sample size  and multi-class algorithm  as the two factors in the anova table. the analysis, adjusted appropriately for multiple testing, revealed that only the pair  differed and there was no statistically significant difference  between "mc-svm" and "one-vs-rest" svm algorithms. this agrees with work by rifkin and klatau  <cit> . a description of the different multi-class algorithms is provided in the methods section

figures  <dig>   <dig> and  <dig> show the error rate versus the sample size for each distribution of the bsa, pca and rsv data sets with 5-fold cross-validation. as the figures indicate, the reduction of the error rate as a function of the sample size is more dramatic for bsa than for pca. for bsa there is about a four-fold reduction in the error rate when the sample size increases from  <dig> to  <dig> for sense distributions ,  and , while there is a two-fold reduction for sense distribution  and no reduction for . in contrast, for rsv, a two-fold reduction of the error rate was observed for distributions , ,  and  for an increase in the sample size from  <dig> to  <dig>  the distribution  behaved the same as bsa.

for bsa and rsv there was no significant effect of the sense distributions on the error rates for all different sample sizes, but for pca the effect of the sense distribution on the error rate was significant. multiple comparisons, adjusted for multiple testing, indicated that when the overall significance level is  <dig> , the sense distributions  and  impact the error rate. these results show that almost balanced sense distributions and rather large training sample sizes reduce the error rate to approximately half of our best guess, which is using the majority sense.

to address the issue of whether a meaningful reduction in the error rate was achieved by increasing the sample size, we performed further statistical analysis on the results of the bsa and pca data set. to test the null hypothesis of no differences in the error rates among the different sample sizes  for the bsa and pca abbreviations, we used friedman's test. then we performed sub-analysis using the sign-test . the results are summarized as follows and they apply to both 5-fold and 10-fold cross-validation schemes. when the senses are well separated, any increase in the sample size results in a statistically significant decrease of the error rate. this holds for all sense distributions and it is in agreement with the finding that for bsa there was no significant effect of the sense distributions on the error rates for the different sample sizes used. there are, however, differences when the meanings of the senses are not well separated . as the friedman's test indicated, the effect of the sense distribution on the error rate is significant. when the sense distribution is  there are statistically significant differences between the pairs of error rates produced under sample size , the sample sizes  and the sample sizes . the differences in the error rates produced under sample sizes  and  are borderline significant . when the sense distribution is , an increase in the sample size from  <dig> to  <dig> and from  <dig> to  <dig> does not produce statistically significant differences in the corresponding error rates. for all other sense distributions, an increase in the sample size did not produce a significant reduction in the error rate – that is, there are no statistically significant differences between the error rates. we would like to stress here a limitation of the current study. this is the fact that the experiments were carried out only  <dig> times: this rather small number of replication of the experiments may have contributed to observing borderline significance.

discussion
issues and our experiments
"sample size', "sense distribution" and "degree of difficulty" were three of multiple confounding issues that affect the performance of a wsd classifier. results from our experiments demonstrated that these three factors were intrinsically connected. notice that as expected, with any distribution, the error rate generally decreased as the sample size increased. however the observed decrease in error rate was more dramatic in the cases where the different senses were well separated. for example, in bsa, the error rate dropped to approximately 5% when the sample size was  <dig> and the sense distributions were almost balanced, and it was approximately 8% for other distributions with the same size. notice also the relatively small standard deviations that are associated with those error rates. moreover, when two senses of a word are very different, then the reduction that is observed in the error rate is meaningful in the sense that it is generally outside the limits of  ±  for increases in the sample size from  <dig> to  <dig> to  <dig>  in contrast, when the separation between the two senses is poor , increasing the sample size does not help much, and a very large increase in size is needed for a small reduction in the error rate. in particular, we notice that when the sense distribution  was very unbalanced , then the error rate was almost equal to the minority sense proportion. all these findings indicate that the effectiveness of an increase in the sample size is very dependent on the degree of difficulty. when the degree of difficulty is very high, increasing the sample size will not help much unless an extraordinarily large size is used, which would be very costly.

there are different types of wsd and some are more difficult than others. for example, if two senses are syntactically different, a reliable part of speech tagging method could be effective in resolving the ambiguity. for senses that correspond to the same syntactic category, the similarity of their semantic categories will affect the difficulty of the task . even for senses within the same semantic class, two close senses will be much more difficult to classify than two unrelated meanings. for example, in rsv, both senses  are associated with a "virus" concept, but the two concepts are very different types of viruses, and therefore the contexts in which they occur are likely to be different as well. as shown in figure  <dig>  pca, which has two very close senses, had much higher error rates than bsa, which has two unrelated senses. therefore, when comparing the performance of different wsd systems, data sets with the same degree of difficulty should be used. resnik  <cit>  stated the importance of the semantic similarity of senses and proposed a method to compute performance, which takes similarity of senses into account. our study is different because it quantified the effect of similarity of senses, and studied the relation between "similarity of senses" and other issues such as "sample size" and "sense distribution". when considering gene symbol disambiguation, we could categorize the tasks as involving four different types of disambiguation: 1) classifying whether a term is a noun or another syntactic part of speech, such as a verb, in which case the term cannot be a gene; 2) classifying whether a term refers to a gene or a non-gene sense ; 3) classifying which gene a term refers to if it is ambiguous with multiple genes or which non-gene sense a term refers if it is ambiguous with multiple non-gene senses; 4) classifying which product  a term refers to if it is known to be a particular gene. podowski's  <cit>  work covered task types  <dig> and  <dig>  while hatzivassiloglou's  <cit>  work addressed task type  <dig>  many evaluations report their results for a set of words, but the difficulty levels and types of disambiguation task types are not stratified.

to be able to identify whether there are significant differences in the error rates due to different sample sizes and sense distributions while controlling for the abbreviation used, we used friedman's procedure. notice that if we stratify by the abbreviation, the mean error rates form a two-way table where the columns correspond to different sample sizes and the rows correspond to different sense distributions. the significance of this methodology is that it provides a comprehensive way to quantify the effects of sample size and sense distribution on the error rate. for bsa, rsv and bpd, we found that the effect of the sense distribution on the error rate was insignificant. for pca this effect was significant. the effect of different sample sizes on the error rate was significant for bsa, rsv, and bpd. for pca, although the effect of sample size on the error rate was significant, this effect was observed only when the sample size was increased from  <dig> to  <dig>  and for fairly balanced sense distributions such as  and . for those two distributions, an increase from  <dig> to  <dig> was also significant. smaller increases in the sample size had an insignificant effect.

we performed further sub-analysis using non-parametric multiple comparisons to identify the pairs of sample sizes that differ when the abbreviations bsa and rsv were analyzed. this analysis revealed that in the case of bsa the improvements in terms of error rate were statistically significant across distributions as the sample size increased from  <dig> to  <dig>  for the case of rsv, a much more substantial four-fold increase in the sample size was needed in order to observe an appreciable decrease of the error rate. effects of "sense distribution" have been addressed in other papers  <cit>  because it is believed that the performance of a wsd classifier may change if the distribution of the different senses is unbalanced. for example, when there is a majority sense for an ambiguous word, the improvement of a wsd classifier is believed to be very small. results from our study showed there was a difference only when the distribution was very uneven and the task was difficult. for example, for pca, when the majority sense was over  <dig> , the error rate started to decrease and when it was over  <dig> , the error rate dramatically decreased so that use of the majority sense was as effective as the ml methods, but with much less cost.

other confounding issues of wsd
other issues in addition to sample size, distribution of senses, and difficulty of the task also affect the performance and subsequent assessment of wsd classifiers, as noted below:

• features used
as often discussed in various papers, different features were evaluated to see their contribution to classifier performance  <cit> . from these papers, there was no single combination of features that seemed to be associated with the best results for any type of wsd task. this could also be due to the existence of other confounding factors in the datasets that were used. in our study, we controlled for this factor by using "bag-of-word" features in all experiments, but it would be interesting to see if the performance improves when different feature vectors are used

• ml algorithm
most papers reported that different ml algorithms did not show much difference on performance  <cit> . but some reported that certain classification algorithms were better than others. for example, mooney  <cit>  did a comparison study among a naïve bayes classifier, perceptron, decision-tree learner, k-nearest-neighbor classifier, logic-based disjunctive normal form, conjunctive normal form and a decision-list learner, and the results showed that the naïve bayes and perceptron classifiers performed significantly better than all others. it is still an unclear issue, probably due to the interaction of different combinations of issues. the comparison between different classifiers should be a carefully controlled experiment. the notion that a lower absolute error rate is indicative of the superiority of a classifier is generally flawed because it ignores the possibility that the differences in the different experiments performed are not statistically significant  <cit> . statistical tests  <cit>  can be used to compare different classifiers.

• baseline reported
it is very important that the baseline of a classification task is reported because it shows how much of an improvement there is using a classifier as compared to the baseline. as shown in our experiments, when there is a majority sense of  <dig>  or more, the performance of a wsd classifier may seem high, but that is not due to the classifier. several papers  <cit>  realized this issue and reported results for the baseline. more specifically, they excluded samples with a majority sense larger than a threshold because they realized the contribution of the classifier would not be much for those cases.

• results with confidence intervals
when reporting the results , not all papers reported confidence intervals . when comparing the performance of wsd classifiers, those metrics are critical because they indicate whether or not an improvement is statistically significant; if there is a large deviation, there may not actually be an improvement even though one error rate is smaller than the other.

• feasibility
one of the problems of supervised machine learning for wsd is the need for an annotated training  data set for each ambiguous word, which may require a huge effort. there are two approaches that address this problem: 1) designing an efficient sampling method to lower the cost of manual sense tagging  <cit> , or 2) use of an automated method to generate sense-tagged data  <cit> , but this may not always be possible or may inadvertently introduce bias. in our study, we proposed a simple "full-term substitution" method, which is described in more detail in the methods section, to automatically generate training data, but this is only applicable for abbreviations.

in this study, we used a "full-form substitution" method to automatically generate the data set for the experiments, which is an artificial training set. we compared the estimated sense distribution from our method with that of liu's method  <cit>  and found they were similar for most of the abbreviations , and that the majority senses based on use of each method were the same. we did not compare the substitution method with other methods for wsd. in addition, we used an svm classifier for all the experiments. since the goals of our study did not include the comparison of different algorithms, we do not present related results here. other studies showed that different ml algorithms had similar performance for wsd tasks  <cit> . thus, it is likely that our findings are applicable to other ml methods because similar issues have been discussed in the general ml literature  <cit> .

earlier studies have investigated a number of the issues discussed here in the context of constructing better classifiers. a discussion of some of the issues involved can be found in  <cit> . here, we examined these issues in the context of word sense disambiguation. the methodology we used to quantify the impact of various factors on the error rate, and hence on the performance of the wsd classifier, is a well-known, theory-based, statistical methodology. the methodology is easy to apply, it provides a principled way of studying the effects of the different factors on the error rate, and since it is based on a strong theoretical foundation, it guarantees that the results to apply to all abbreviations with similar characteristics. therefore, although we studied only four abbreviations, the results concerning sample size, sense similarity, and distributions are likely to be generalizable for abbreviations with similar characteristics. the results presented here agree with general results presented in the literature on the performance of classifiers  <cit> .

future work
to further analyze the effects of "sample size", "sense distribution" and "degree of difficulty" on the error rate, an error decomposition model will be explored. methods to measure the degree of distances among different senses are also being studied.

CONCLUSIONS
in this paper, we aimed to further an understanding of the different factors affecting the performance of ml techniques for wsd by systematically simulating a variety of situations where different sample size, sense distribution, degree of difficulty, and cross validation methods were used. we evaluated the performance of svm classifiers for those situations. results from our experiments showed that: 1) increasing the sample size generally reduced the classifier error rate, but this was limited mainly to well-separated senses ; in difficult cases an unusually large increase in sample size was needed to increase performance slightly, which was costly and impractical, 2) the sense distribution did not have much effect on classifier performance for cases where the senses were separable, 3) when there was a majority sense of over 90%, choosing the majority sense seemed to be the most effective strategy because the cost was low as was the error rate, 4) the error rate was proportional to the similarity of senses, and 5) there was no statistical difference between results using 5-fold or 10-fold cross-validation. in this paper, we also demonstrated that ambiguity of biomedical entities is a significant problem, which has a substantial impact on text mining and retrieval tasks in the biomedical domain.

ml methods are still needed for wsd, which is critical for increasing the accuracy of biomedical natural language, text mining, and information retrieval systems. ml methods are especially important for those cases that cannot readily be addressed using knowledge-based methods. therefore it is important that we understand the different elements affecting their performance. in order to improve our understanding of the ml methods, it is critical that in addition to reporting on overall results, papers also report on the baseline performance, the distribution of senses in the datasets, the standard deviation or confidence intervals, the types of ambiguity addressed, and the difficulty of the task as well as the methods and features used.

