BACKGROUND
linkage disequilibrium  is the non-random association of alleles at different loci and decays with increasing distance between loci  <cit> . high ld regions reflect the presence of chromosomal segments  that are transmitted from parents to offsprings more often than expected by chance. ld is traditionally assessed by the normalized d′ coefficient  <cit> . the r <dig> coefficient  <cit>  is currently more commonly used than d′ to identify independent signals in genome-wide association studies . in this regard, the r <dig> was also considered in haplotype block recognition algorithms  <cit> . nevertheless, the d′ should remain the statistics of choice for ld modeling because of its more direct biological interpretation. it reflects the history of recombination, mutation, and selection events that cause some chromosomal segments to be less diverse than others and, therefore, influence the haplotype distribution. moreover, it has been shown that r <dig> is not significantly more precise, accurate or efficient than d′ <cit> . the d′ and r <dig> coefficients capture similar information but their range of variation can be very different  <cit> . the d′ goes from - <dig> to + <dig> and is independent of the allelic frequencies of the two markers involved. when d′ =  <dig>  the two markers are independent , while |d′| =  <dig> indicates that no more than three of the four possible haplotypes are being observed in the sample . in contrast, the range of r greatly depends on the allele frequencies and equals - <dig> or + <dig> only when the two markers have the same allele frequency. in such cases, |r| =  <dig> indicates that knowing the allele at one marker allows to determine the allele at the other marker . but when the two markers have very different allele frequencies, the interpretation of r <dig> becomes difficult. this is especially relevant with the data generated by the new sequencing technologies, that allow genotyping markers over a very wide spectrum of allele frequencies. in such situations, the r <dig> may fail to identify the correct relationship between nearby variants. in gwas, this may lead to a wrong definition of the identified loci.

although, in the past, haplotype blocks have been mainly used to identify tag snps  <cit> , a variety of other applications is possible with currently available data. recently, analysis of exome-chip data has shown that within-gene ld-block distribution can be informative of the gene function and of the possible relationship between genes and specific groups of phenotypes  <cit> . another application is the genome-wide haplotype association scan, which was successful in uncovering risk loci for coronary artery disease  <cit> , alzheimer’s disease  <cit>  and breast cancer  <cit> . so far, genome-wide haplotype association scans have been mostly performed based on fixed- or variable-width sliding window methods, which systematically miss haplotypes that are longer than some specified maximal window widths. an efficient genome-wide haplotype block recognition could help overcome such limitations, thus enhancing the biological interpretation of the results. in the study of rare variants, where collapsing methods  are becoming increasingly popular  <cit> , the availability of haplotype blocks at genome-wide level would allow to collapse variants based on block boundaries, capturing inter-genic variants, and avoiding the problem to define the gene boundaries. additional applications include downstream analyses of gwas, such as pathway-based approaches, where statistics for multiple snps are summarized into gene-specific p-values, which are then employed for gene ranking  <cit> . in pathway-based analyses, snp-to-gene mapping is typically based on snp proximity to the gene boundaries. with this method, when a region is gene-dense, it may be problematic to assign snps to a single, specific gene. an ld-based assignment would overcome this limitation and increase the power of downstream analyses  <cit> . in general, ignoring the ld structure in downstream analyses of gwas can result in the misinterpretation of the findings  <cit> . popular genome browsers, such as the ensembl  <cit>  or ucsc  <cit> , are suitable for visualizing the ld distribution over regions of interest. however, they only allow pairwise ld calculation between markers at < <dig> kb distance from each other and do not provide any ld-block partition. with no predefined block partition, the visual assessment of such ld patterns might be influenced by investigator’s subjectivity. on the other hand, the  <dig> kb distance constraint may limit the investigation of larger strong-ld regions. with the availability of pre-calculated, threshold-free ld blocks, we would overcome both these limitations.

there is extensive literature on haplotype block inference  <cit> , including methods based on probabilistic graphical models  <cit> . the latter allow an accurate identification of snp clusters, even in situations when snps are not necessarily contiguous. however, due to its simplicity, the most commonly used ld-based algorithm remains the one proposed by gabriel et al. <cit> , which is implemented in haploview  <cit> . the haploview algorithm is widely used in genetic association studies and it is included in popular software, such as plink  <cit> . however, with a Θ time and memory complexity, where n is the number of snps, the algorithm is applicable only to short genomic segments containing no more than a few thousand snps. unless runtime and memory usage are artificially reduced by splitting large segments into smaller chunks, the algorithm cannot be applied to densely genotyped segments or genome-wide analyses.

in this paper, we describe how we improved efficiency and scalability of the haploview algorithm  by adopting an incremental computation of the haplotype blocks based on iterative chromosome scans and  by estimating d′ confidence intervals  using the approximate variance estimator proposed by zapata et al. <cit> . the incremental computation strategy led to an algorithm, termed mig ++, that has Θ memory complexity and omits more than 80% of the pairwise ld computations, while obtaining exactly the same final haplotype block partition as haploview. in contrast to haploview, the new algorithm can consider pairwise ld between snps at any distance. with mig ++, we performed the haplotype block recognition of the entire hapmap phase ii dataset of ceph haplotypes. by introducing the approximate variance estimator, the performance of the mig ++ was further improved and allowed us to perform the block partition of the entire  <dig> genomes project dataset of ceph haplotypes. to show a practical application of the obtained genome-wide block partition, we finally compared snp-based and haplotype block-based association tests in a gwas context.

methods
haplotype block definition
the haplotype block recognition algorithm proposed by gabriel et al. <cit>  is based on |d′| and its 90% ci, with cl and cu being the lower and upper bounds of the ci, respectively. snp pairs are classified as follows:  in strong ld if cl ≥  <dig>  and cu ≥  <dig> ;  showing strong evidence of historical recombination  if cu< <dig> ;  non-informative, otherwise. informative pairs are those satisfying conditions  or . a haplotype block was then defined as follows:


definition  <dig>  . let c = 〈g <dig>  …,g
n
〉 be a chromosome of n snps, g = 〈g
i
, …, g
j
〉 a region of adjacent snps in c, l the number of strong ld snp pairs in g, and r the number of strong ehr snp pairs in g. then, g is a haplotype block if

 the two outermost snps, g
i 
and g
j
, are in strong ld, and

 there is at least a proportion d of informative pairs that are in strong ld, i.e.: l /  ≥ d.

in their original work, gabriel et al. <cit>  set d =  <dig>  after investigating the fractions of strong ld snp pairs in genomic regions of different length and in different populations.

the haploview algorithm  <cit>  performs a haplotype block partition in two steps:  all regions satisfying definition  <dig>  are collected in a set of candidate haplotype blocks;  from this set of candidates, a subset of non-overlapping regions that satisfy definition  <dig>  is selected. in the first step, the entire chromosome is scanned and, for every snp pair, the |d′| ci is computed and stored in an n × n matrix. the matrix is then traversed to identify the pairs that satisfy definition  <dig> . these pairs mark regions of different length that are candidates to become haplotype blocks. in the second step, the candidate regions are sorted by decreasing length and processed starting with the largest one. if a region satisfies definition  <dig> , it is classified as a haplotype block, and all other overlapping candidate regions are discarded. regions not satisfying definition  <dig>  are skipped. this process continues with the next largest candidate region, until the candidate set is completely processed and the list of haplotype blocks is complete.

the overall complexity of the algorithm is mainly determined by the first step. more specifically, the Θ time and memory complexity is due to the computation and maintenance of the n × n ci matrix. for this reason, we concentrated our improvements on the first step.

incremental computation of haplotype blocks
the core ideas of our optimizations are to compute haplotype blocks incrementally and to omit, as soon as possible, regions that cannot be extended to larger blocks due to an insufficient proportion of strong ld snp pairs. in this way, we avoid both unnecessary computations and the storage of an n × n ci matrix. the incremental haplotype block computation is based on the concepts of a snp-pair weight and a region weight described below.


definition  <dig>  . let c and d be as defined in definition  <dig>  for a given pair of snps g
i 
and g
j
, the snp-pair weight, w, is defined as follows:

 w=1-difgiandgjare in strong ld,-difgiandgjshow strong ehr,0otherwise. 


definition  <dig>  . let g be as defined in definition  <dig>  the region weight of g, w¯, is defined as the sum of all snp-pair weights in g: 

 w¯=∑v=i+1j∑u=ivw. 

the following theorem defines a haplotype block based on the region weight.


theorem  <dig>  let g be as defined in definition  <dig>  g is a haplotype block if w  = 1 - d andw¯≥ <dig> 

proof
from definition  <dig>  if snps g
i
 and g
j
 are in strong ld, then w  = 1 - d. therefore, definition  <dig>  is satisfied. g contains s=∑v=i+1j∑u=iv <dig> possible snp pairs, of which l are in strong ld, r show strong ehr, and the remaining ones are non-informative. from definitions  <dig> and  <dig>  it follows that w¯=∑v=i+1j∑u=ivw=l+r+·0=l-d. if w¯≥ <dig>  then l - d ≥ 0 ⇒ l /  ≥ d. therefore, definition  <dig>  is also satisfied.

theorem  <dig> is the basis for the incremental haplotype block reconstruction, which is the core of our optimizations. in the following, we present three gradual improvements of the haploview algorithm: a memory-efficient implementation based on the gabriel et al. <cit>  definition ; mig with additional search space pruning ; and mig + with iterative chromosomal processing . theorem  <dig> ensures that all three algorithms produce block partitions that are identical to the original haploview results.

the mig algorithm
for a given chromosomal segment c containing n snps, the maintenance of an n × n matrix containing all the |d′| cis can be avoided by storing n region weights in a unidimensional vector w
n ×  <dig>  in each element of w, w, we store the weight of a chromosomal region that starts at snp g
i
. when the region is enlarged by including additional snps to the right of g
i
, the weight w is updated accordingly. this procedure, illustrated in figure  <dig>  begins with setting all the weights to  <dig>  at the initial stage, the vector w represents all one-snp regions. then, the region starting at snp g <dig> is enlarged by including the next snp, g <dig>  therefore, starting from g <dig>  chromosome c is processed one snp after the other, from left to right. for a snp g
j
, with j ≥  <dig>  all snp pair weights w, i = j -  <dig>  …,  <dig>  are computed and added up as s = w + ⋯ + w .

s and w are updated for every computed weight w. before the update, s = w + ⋯ + w and w contains the region weight w¯, which was already computed for the previous snp g
j- <dig>  then, s is incremented by w and w is incremented by the new value of s. w now represents the region weight w¯, i.e., w¯=w¯+w+⋯+w. whenever w = 1 - d and w¯≥ <dig>  theorem  <dig> is satisfied and the region 〈g
i
, …, g
j
〉 is added to the set of candidate haplotype blocks. this procedure is repeated with the next snp, g
j+ <dig>  an example of the first three computational steps is given in figure  <dig>  the pseudocode is provided in algorithm a. <dig> .

mig reduces the memory complexity from Θ to Θ. moreover, instead of identifying candidate regions that satisfy only definition  <dig>  , mig checks immediately both conditions  and . this yields a smaller set of candidate blocks, and therefore indirectly speeds up also the second step of the haploview algorithm.

the mig + algorithm
while mig drastically reduces the memory requirements by avoiding the maintenance of the ci matrix, it still computes weights for all snp pairs, totaling n /  <dig> computations as in haploview. to omit unnecessary computations, we apply a search space pruning to the mig algorithm to identify regions that cannot be further extended to form a haplotype block. the pseudocode is shown in algorithm a. <dig> .

instead of computing weights for all pairs of snps, only weights w, …, w are computed, where b=min≥0}) and w¯max=max{w¯∣j<k≤n}. the function w¯max is an upper bound for the weight of all regions 〈g
i
, …, g
j
, …, g
k
〉 that start at g
i
 and end after g
j
, i.e., those extending beyond the region 〈g
i
, …, g
j
〉. if w¯max< <dig> for some i, none of the regions 〈g
i
, …, g
k
〉 can satisfy definition  <dig> . the smallest i, that can be a potential starting point of a region with a positive weight, can therefore be set as breakpoint b. regions starting left of b and stopping right of j receive negative weights and are discarded .

the upper bound, w¯max, is estimated assuming that all unprocessed snps to the right of g
j
 are in strong ld with each other and with all snps in the region 〈g
i
, …, g
j
〉. then, w¯≤w¯+·s, where w¯ is already computed and s =  + ) /  <dig> is the number of unprocessed snp pairs. since s is largest for the longest region 〈g
i
, …, g
n
〉, we have max{w¯∣k>j}≤w¯+·+)/ <dig>  and the estimated upper bound w¯max is defined as follows: 

 w¯max=w¯+·+)/ <dig>  

the mig + algorithm performs at most λn /  <dig> computations, where λ, 1 - d ≤ λ ≤  <dig>  depends on the data. the worst case of λ =  <dig> occurs only in the unlikely situation when a few very large blocks span an entire chromosome.

the mig ++ algorithm
a limitation of the mig + algorithm is its blindness about the unprocessed area to the right of the current snp g
j
. assuming strong ld for all snp pairs in this area results in a conservative upper bound, w¯max, for the region weights. an additional optimization step allows to obtain a more precise estimate of w¯max and further prunes unnecessary computations. the pseudocode of the modified algorithm, mig ++, is given in algorithm a. <dig> .

the improved algorithm is an iterative procedure that, at each iteration, scans the chromosome from left to right and computes the weights only for a limited number of snp pairs. for a snp g
j
, the snp pairs considered in an iteration are restricted to a window of size win: only the weights w, …, w are computed, where t= max and 1 ≤ win ≤ n . at each new iteration, the window size is increased by a number of snps equal to win. therefore, the number of computed snp-pair weights increases proportionally. this allows a more precise estimation of the upper bounds for the region weights with every new iteration.

by considering all snp-pair weights computed in all previous iterations for the estimation of the upper bound, w¯max, the algorithm requires linear time for each individual snp pair to sum up all weights inside the corresponding region. we use a computationally cheaper constant-time solution, though it may lead to a less accurate estimation. since w¯≤w¯+w¯-w¯, we have max{w¯∣k>j}≤w¯+max{w¯∣k>j}-w¯. an upper bound w¯max can then be computed as follows: 

 w¯max=w¯+max{w¯∣k>j}-w¯. 

max{w¯∣k>j} is computed in linear time after every scan of the chromosome, whereas w¯ is computed in constant time. thus, the computation of the upper bound w¯max for each individual snp pair requires only constant time.

when win = n, mig ++ is identical to mig +. when win =  <dig>  the number of iterations becomes too large, introducing a significant computational burden. we propose to set win = ⌈ / 2⌉, that corresponds to 1 - d percent of all snp pairs, which is the minimal fraction of snp pairs that must be considered before one can be sure that an n-snp segment is not a haplotype block.

the mig ++ performs at most λn /  <dig> computations, where λ, 1 - d ≤ λ ≤  <dig>  depends on the data. however, the value of λ obtained with the mig ++ algorithm is expected to be always smaller than that from the mig + algorithm, because of the more precise estimation of w¯max.

alternative methods to estimate the d′ ci
a critical step of the gabriel et al. <cit>  approach is the estimation of the d′ ci. in a genome-wide context, this calculation can be repeated hundreds of millions of times. in haploview, the cis are obtained by means of the likelihood-based procedure proposed by wall and pritchard  <cit> , which requires from  <dig> to  <dig>  iterations. this method can be replaced with a computationally cheaper solution, based on an approximated estimator of the d′ variance, as proposed by zapata et al.  <cit> . this solution would make the whole block recognition algorithm significantly faster.

the wall and pritchard  method
the true allele frequencies of each snp are assumed to be equal to the observed allele frequencies. the likelihood of the data in the four-fold table obtained by crossing any snp pair, conditional to the |d′| value, can be expressed as l = p. l is evaluated at each value of |d′| =  <dig>  × p, with p =  <dig>   <dig>  …,  <dig>  cl is defined as the largest value of |d′| such that ∑i=0p-1l/∑i=01000l≤α, where α is the significance level. similarly, cu is defined as the smallest value of |d′| such that ∑i=p+11000l/∑i=01000l≤α.

the approximate variance  method
consider two snps, u and v, with alleles {u <dig>  u2} and {v <dig>  v2}, respectively. let nuivj and fuivj denote, respectively, the absolute and relative frequencies of the four possible haplotypes, u
i
v
j
, with fui and fvj being the marginal frequencies of the two snps. in total, n=∑nuivj haplotypes are observed. zapata et al.  <cit>  showed that the variance of d′ can be approximated as follows: 

 v≈1-|d′|×n·v-|d′|dmax×+|d′|f3/n·dmax <dig>  

where d′=d/dmax;d=fu1v1-fu1fv1; d
max
 is min{fu <dig> fv1} when d >  <dig> or min{fu1fv <dig> } when d < 0; f <dig> is fv <dig> when d′ >  <dig> or fv <dig> when d′ < 0; f <dig> is fv <dig> when d′ >  <dig> or fv <dig> when d′ < 0; f <dig> is fu1v <dig>  fu1v <dig>  fu2v <dig>  and fu2v <dig> when d
max
 is fu1fv <dig>  fu1fv <dig>  fu2fv <dig>  and fu2fv <dig>  respectively; and 

 v≈fu1fu2fv1fv2+d-d2/n. 

 when d′ = ± <dig>  then v =  <dig>  the 1 - α ci of d′ is equal to d′±zα/2v, where z
α/ <dig> is the 1 - α /  <dig> percentile of the standard normal distribution.

experimental evaluation
the experimental evaluation was based on the phased ceph genotypes included in the hapmap phase ii   <cit>  and the  <dig> genomes project phase  <dig> release  <dig>   <cit>  databases. the hapmapii dataset included  <dig> , <dig> snps from  <dig> haplotypes  and the 1000g dataset included  <dig> , <dig> snps from  <dig> haplotypes .

to compare the new algorithms to the standard haploview, in terms of runtime and memory usage, the ideal solution would have been that of randomly sampling regions with different characteristics from the hapmapii or 1000g datasets. however, the haploview algorithm was so computationally expensive that it prohibited to consider a sufficiently large number of random regions and, therefore, to obtain a representative sample of all possible scenarios over the whole genome. for this reason, we selected the regions such that the most extreme scenarios, in terms of median snp minor allele frequency  and median inter-snp distance, were covered. to identify such representative regions, we performed the systematic scan of all snps in the genome using a sliding window of  <dig>  snps, after removing chromosomal centromeres and the hla region. for each sliding region, the median maf and inter-snp distance were recorded . all regions were then represented in a two-dimensional euclidean space, where the normalized inter-snp distance was plotted against the normalized median maf . a total of nine regions were chosen for the experiments: the eight regions located on the outermost boundaries of the euclidean space and the region closest to the center of the space. these regions represent scenarios with extreme and moderate median maf and median inter-snp distance. the procedure was repeated using larger sliding windows of  <dig>  to  <dig>  snps. if not stated otherwise, in the experimental results we report median values over the nine regions for every different window size.

the block partitions obtained with the wp and av methods for d′ ci estimation were compared in terms of total number of blocks, median number of snps per block, proportion of snps clustered into blocks, and median within block haplotype diversity. haplotype diversity  <cit>  is defined as the ratio between the number of common haplotypes and the total number of haplotypes within a block. common haplotypes are those occurring more than once. the haplotype diversity index ranges from  <dig>  to  <dig> .

the three mig algorithms were implemented in c++. to guarantee a fair comparison, the original java implementation of the haploview algorithm was rewritten in c++, too. by default, haploview considers only snp pairs within a maximal distance of 500kbp. we removed this constraint because it could affect the block partitions of very wide regions. for the wp method, we set the number of likelihood estimation points to  <dig>  . we didn’t consider the population specific two-, three-, and four-marker rules, proposed by gabriel et al. <cit>  when very short regions are processed, because they have no impact on the computational efficiency of the algorithms. all experiments were run on a machine with an opteron  <dig> quad core  cpu.

genome-wide association study of rheumatoid arthritis
we applied our haplotype block partitioning algorithm to the genome-wide association study of the north american rheumatoid arthritis consortium  dataset. data consisted of  <dig> cases and  <dig>  controls. the samples were genotyped at  <dig>  autosomal and sex chromosome snps. quality check was performed with plink  <dig>   <cit> : we excluded  <dig>  snps with a call rate of <90%,  <dig>  snps with a minor allele frequency of < <dig> , and  <dig> snps because of significant deviation from hardy-weinberg equilibrium in controls . no samples were excluded because of low call rate ;  <dig> cases and  <dig> controls were removed because of sex mismatch;  <dig> case and  <dig> controls were additionally excluded after population stratification test based on principal component analysis performed with eigensoft  <dig> . <dig>  <cit> . after the quality control,  <dig>  autosomal snps and  <dig>  samples were available for analyses.

haplotypes were phased using shapeit version  <dig>  <cit> . to achieve good accuracy, we set  <dig> conditioning states per snp. recombination rates were taken from hapmap phase ii build  <dig> and effective population size was set to  <dig>  . the estimated haplotypes were submitted to mig ++ and processed with the wp and av methods. we obtained  <dig>  wp blocks, covering  <dig>  snps, with  <dig>  singleton snps outside of any block. the av method identified  <dig>  blocks, covering  <dig>  snps, and  <dig>  singleton snps.

the genome-wide association scan was based on a logistic regression model adjusted for sex and the top  <dig> eigenvectors obtained from eigensoft  <dig> . <dig>  <cit> . the association between disease status and individual snps or haplotype blocks was tested with a likelihood ratio test using plink  <dig> . <dig>  <cit>  with the logistic-genotypic and omnibus options, respectively. within each block, haplotypes with frequency of < <dig>  were collapsed together to preserve power. singleton snps outside blocks were treated as in the snp-based analysis, therefore producing analogous results. genomic control  correction was applied to both snp- and block-based gwas results. bonferroni-corrected significance thresholds were set to  <dig>  × 10- <dig> for analysis based on the wp block partition ,  <dig>  × 10- <dig> for the analysis based on the av method , and  <dig>  × 10- <dig>  for the individual snp analysis.

RESULTS
runtime and memory with the wp method
the mig ++ omitted more unnecessary computations than mig +, which is reflected by the smaller λ coefficient in both hapmapii and 1000g datasets . the λ values decreased with increasing number of snps in the region. when increasing the region size, after a rapid decline for small regions, λ reached stable values with both mig + and mig ++ algorithms and in both datasets. this behavior relates to the ld decay with distance. in regions of  <dig>  snps in the 1000g dataset, the mig ++ algorithm was able to omit ∼80% of the calculations , while mig + could omit ∼60% of the calculations . an example of the reduction of the number of calculations is given in figure  <dig>  where mig + and mig ++ are compared to haploview, which is represented by the entire triangle.

runtime and memory with the av method
when we introduced the av method to estimate the d′ ci, we observed a drastic reduction of the computational time of the mig algorithm. with the av approach, the median runtime needed to analyze sequences of  <dig>  snps in the 1000g dataset was of  <dig> minutes. the same analysis took a median of  <dig>  hours with the wp method . proportional time reduction was observed for mig + and mig ++. similar results were obtained in the hapmapii dataset .

we observed that the introduction of the av method caused a slight increase of the λ coefficient . this is because, with the av method, more snp pairs are classified to be in strong ld. this causes an increase of the number of possible configurations to be checked and results in a larger set of candidate haplotype blocks. with the av method, the mig algorithms identified tens of millions of candidate haplotype blocks . the number of candidate blocks was even larger when the av method was applied directly to haploview, where candidate blocks need to satisfy only definition  <dig> . this significantly larger number of candidate blocks explains the increase in runtime of haploview when using the av method: for regions of  <dig>  snps in the 1000g dataset, the median runtime was of  <dig> hours with the av against the  <dig> hours with the wp method .

block partitions with the wp and av methods
the characteristics of the different block partitions obtained with the wp and av methods are summarized in figure  <dig>  the av method produced a smaller number of blocks than the wp method . the median number of blocks per region increased along with the number of snps, and it increased faster for the wp compared to the av method. considering the median number of snps per block, the av method produced larger blocks than the wp method . for very short regions  both methods generally induced larger blocks. this is because such small regions might be completely covered by a single or very few haplotype blocks. the median number of snps per block decreased along with the increase of the length of the region considered. overall, the av method assigned a higher percentage of snps to blocks compared to the wp method, which left more singleton snps outside of any block . in the analysis of the hapmapii dataset,  <dig> % of the snps were clustered within blocks with the av method and  <dig> % with the wp method. in the analysis of the 1000g dataset, the percentages were of  <dig>  and  <dig> , respectively.

we observed that 100% of the blocks identified by the wp method were overlapping blocks identified by the av method. more specifically, 80% to 90% of the blocks based on the wp method were completely included within blocks based on the av method . the remaining 10% to 20% of wp blocks whose borders were crossing borders of av blocks, could be entirely attributed to the selection mechanism in the step  of the algorithm, when larger candidate blocks are prioritized over the shorter ones. in fact, when, instead of looking at the final block partition, we focused on the intermediate set of candidate blocks before the final pruning, we observed that 100% of the candidates from the wp method were entirely included within the candidate blocks from the av method.

consistently with the findings of larger av blocks, we observed a generally higher haplotype diversity in the partitions obtained with the av method compared to the wp method . for instance, when considering regions of  <dig>  snps in the 1000g dataset, we observed median within-block haplotype diversity indices of  <dig>  and  <dig>  with the av and wp methods, respectively. slightly higher diversity indices were observed in the hapmapii dataset:  <dig>  and  <dig>  for the av and wp methods, respectively. the within-block diversity was more variable in short than in long regions because, as observed above, when regions are too small, then it might be difficult to identify more than one block.

whole genome haplotype block recognition
the linear memory complexity and the significant reduction of the number of computations allowed us to run mig ++ on a genome-wide scale. we could run mig ++ on the full hapmapii dataset using both the wp and av methods for d′ ci derivation. using the more efficient av method, we were also able to run a genome-wide haplotype block partition of the complete 1000g dataset. the runtime for the two datasets is shown in figure  <dig>  for hapmapii, the maximal runtime was of  <dig> hour when using the av method and of  <dig> hours when using the wp method. in both cases, the maximal runtime was observed for chromosome  <dig>  which contained  <dig>  snps. the median λ value across all chromosomes was  <dig>   for the av method and  <dig>   for the wp method. for the 1000g dataset, the maximal runtime using the av method was of  <dig> hours on chromosome  <dig>  which contained  <dig>  snps. the median λ value across all chromosomes was  <dig>  . the maximal memory usage was very low and didn’t exceed  <dig> mb and  <dig>  gb for the hapmapii and 1000g datasets, respectively.
++ 
algorithm on whole-genome data.
d


′ 

cis are estimated with the wp and av methods.

the characteristics of the whole-genome block partitions obtained with the av and wp methods are summarized in table  <dig>  the results were similar to the experiments on smaller regions. in the hapmapii dataset, fewer and larger blocks were detected with the av method than with the wp method. with the av method, a higher percentage of snps was assigned to blocks and the within-block haplotype diversity index was slightly smaller. however, the haplotype diversity was close to one for both methods, indicating that in both cases the number of possible haplotypes should be very limited. when applying the av-based mig ++ algorithm to the 1000g dataset, we observed a higher percentage of snps in blocks and a slightly smaller diversity index, which is explained by the higher number of snps per block.

for both hapmapii and 1000g datasets, the largest blocks were located over the chromosomal centromeres and spanned tens of millions of base-pairs  . some of these very large blocks were characterized by very low and irregular snp density. after filtering out these exceptionally large blocks, the largest block identified by the wp-based mig ++ algorithm in the hapmapii dataset was located in chromosome  <dig>  it was  <dig> , <dig> bp long and included  <dig> snps. when using the av method, the largest block was located in chromosome  <dig>  it was  <dig> , <dig> bp long and included  <dig> snps. in the 1000g dataset, the largest block detected by the av-based mig ++ was located in chromosome  <dig>  it was  <dig> , <dig> bp long and included  <dig>  snps.

genome-wide association study of rheumatoid arthritis
after the gwas, we observed a genomic inflation factor λ of  <dig>  for the snp-based analysis,  <dig>  for the av block-based analysis, and  <dig>  for the wp block-based analysis. after gc correction, in the snp-based analysis,  <dig> snps were genome-wide significant. of them,  <dig> were located inside  <dig> av blocks and  <dig> inside  <dig> wp blocks. from the av and wp block-based analyses, we observed  <dig> and  <dig> genome-wide significant blocks, respectively. twenty-three of such blocks were the same between the two methods. the results from the snp- and block-based analyses are compared in table  <dig>  the first part of the table shows the  <dig> genome-wide significant loci detected by both snp- and block-based analyses. in most cases, the av and wp methods brought to identical results. one exception was the 4
th
 locus, where two adjacent av blocks including  <dig> and  <dig> snps, respectively, corresponded to two adjacent wp blocks of  <dig> and  <dig> snps, respectively. that is, one snp shifted from one block to another. in terms of significance, results were practically unchanged. a second exception was locus  <dig>  were a block was detected only with the wp but not with the av method. the last two exceptions were loci number  <dig> and  <dig>  in both cases, an av block was split into two wp blocks. the second part of table  <dig> shows a number of loci that wouldn’t have been detected with a snp-based gwas, but were uncovered by at least one of the two block partition methods. the av and wp methods produced similar results. we didn’t observe any clear advantage of one method compared to the other. the last section of the table shows that there was a small number of loci uncovered only by the snp-based analysis. for these loci, the p-values from the block-based analyses were often close to the significance level, with the exception of the last two loci.

discussion
we propose an algorithm for haplotype block partitioning, termed mig ++, which represents a scalable implementation of the haploview algorithm and produces the same results in a much shorter time and using a substantially smaller amount of main memory. mig ++ can process large dna regions using only a handful of megabytes of main memory. in such situations, haploview would require gigabytes. in terms of runtime, the mig ++ is several times faster than haploview. we also demonstrated that more than 80% of calculations were not necessary for the purpose of block recognition and could be omitted, thus achieving a higher efficiency. the improved performance of the algorithm makes it possible to process very large chromosomal segments. when the approximated variance estimator, proposed by zapata et al. <cit> , is used to estimate the d′ ci, the mig ++ can be applied genome-wide and process high density datasets, such as the 1000g, in a very short time.

with its very small memory requirements, the mig ++ can process any number of snps. this allowed us to avoid haploview’s restrictions on the maximal haplotype block length  and to consider the ld between snps at any distance. our whole-genome experiments showed that the haplotype blocks, based on the gabriel et al. <cit>  definition, can span more than 500kbp and can extend over several millions of base pairs. this empirical result suggests that limiting the maximal block length may alter the block partition. the alteration can be substantial because the algorithm prioritizes the largest blocks. the smallest blocks are retained only when they do not overlap with the largest ones. for this reason, to constrain the block length within pre-specified limits may induce a cascade of effects and may affect the final partition of very large segments. this is relevant, for example, when assessing the ld pattern of loci selected from gwas, with the aim of identifying genes related to the lead snp. in such cases, different partitions could imply different genes to be selected for follow-up.

with the mig ++ algorithm, we were able to run a haplotype block recognition of the entire hapmapii dataset. however, it still required an unacceptably long time to apply the algorithm to larger and denser genomes, such as the 1000g dataset. this limitation is due to the use of the wall and pritchard  <cit>  method, which models the |d′| likelihood and derives the |d′| cis using an iterative procedure. in contrast, if the d′ variance is estimated with the approximated formula suggested by zapata et al. <cit> , it is possible to derive the d′ ci with a single mathematical calculation. thanks to this computationally less demanding solution, we could perform a complete block recognition of the hapmapii dataset in  <dig> hour and to process the entire 1000g dataset in  <dig> hours. to the best of our knowledge, this is the first time that such a marker-dense genome has been partitioned with a threshold-free approach. previously, block partition of the whole genome could only be achieved by dividing chromosomes into small chunks or by restricting computations using sliding window approaches. such choices may introduce artificial breaks to the real haplotype structure.

it is important to note that the block partition obtained with an algorithm based on the av method is not fully equivalent to a partition obtained based on the wp method. for large sample sizes and for common variants, the estimated variance of the d′ statistic is going to be similar, whichever method is used. however, when crossing a common with a rare snp, it often happens that one of the four possible haplotypes is not present in the sample. in such situations, it is very likely that the d′ ci shrinks to  <dig> because the approximated variance is zero. in this way, the snp pair is systematically classified as a strong ld pair. as a result, snps with rare alleles are easily grouped together into very large blocks, boosting the region coverage and the median number of snps per block. the wp method is less sensitive to extreme d′ values, and the resulting blocks are generally shorter. however, we observed that most  of the haplotype blocks obtained with the wp method were contained within the larger blocks obtained with the av method. that is, the use of the av method produced a coarser partition, where av blocks entirely contained one or more wp blocks. for this reason, the av blocks showed a higher haplotype diversity, in the terms described by patil et al. <cit>  and zhang et al. <cit> , than the wp blocks.

to provide an application of a whole-genome haplotype block partition, we analyzed the data from the north american rheumatoid arthritis consortium  dataset using both block partitions: the one obtained with the standard wp method and the one obtained with the av approach. as observed in previous studies  <cit> , the gwas results were dominated by the hla locus on chromosome  <dig>  however, other loci were identified in other chromosomes. for what concerns the two block partition methods, the results were very similar, suggesting that the av approach might be a convenient way to run a fast recognition of the haplotype blocks. however, we recognize that ours was an empirical application based on half a million genotyped snps. results might be different in a larger context, such as that of a gwas based on the  <dig> genomes dataset, where the number of av blocks is expected to be much smaller than the number of wp blocks, and the av blocks are expected to be much larger than the wp ones. our empirical analysis of the narac data also confirmed previous observations that snp- and block-based analyses are complementary to each other  <cit> . in fact, in our analysis some loci were identified only by the single-snp analysis, other loci were identified only by the haplotype-block analysis, and others by both methods. thus, genome-wide haplotype association scans are not in competition with standard gwas. genome-wide haplotype association scans should be considered as complementary tools that may help to identify loci that could be overlooked by methods based on single-snp analysis. we also observed that haplotype blocks may simplify gene annotation. while only one gene, the hla-dra  <cit> , which was reported by previous gwass, was directly implied by a genome-wide significant snp, four additional previously reported genes were implied by genome-wide significant blocks: the apom, hla-dqa <dig>  hla-drb <dig>  and hla-dqa <dig> genes  <cit> .

CONCLUSIONS
we have provided an efficient and scalable haplotype block recognition algorithm, termed mig ++, which improves the well-known haploview algorithm by reducing memory complexity from quadratic to linear and by omitting approximately 80% of unnecessary computations. the improved algorithm was able to efficiently process dense genomic segments of any size. when applied to individual-level data, where genotypes are available, the mig ++ efficiency can be exploited to set up haplotype-based  association scans that could account for the correct underlying haplotype distribution. this seems to be especially relevant when rarer variants are involved. if ran on summary results from gwas, the mig ++ could help identify biologically plausible scenarios for snp-set analysis and it could support a more correct annotation of genes surrounding variants of interest. from a population-genetic point of view, the method could facilitate the comparison of human genomes across different ethnicities, helping to highlight structural differences. finally, the algorithm opens up the possibility to integrate genome-wide ld-based haplotype block structure into visual assessment tools, thus improving the interpretation of already available, but incomplete, ld heatmaps .

the mig algorithms are available in the ldexplorer r package at http://www.eurac.edu/ldexplorer together with usage instructions and examples. further improvements will include application of parallel computation techniques to mig ++ in order to further speed up the processing while keeping memory requirements low.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
dt developed the mig algorithms, implemented all algorithms in c++, designed and performed all experiments, and drafted the manuscript. jg supervised the project and revised the manuscript. cp supervised the project, planned the experiments, and revised the manuscript. the authors read and approved the manuscript.

supplementary material
additional file 1
appendix. includes pseudocode for the mig algorithms, illustrations of chromosomal regions sampling procedure, and figures illustrating various haplotype block properties.

click here for file

 acknowledgements
the genetic analysis workshop was funded by nih grant r <dig> gm <dig>  our work was based on data that was gathered with the support of grants from the national institutes of health , and the national arthritis foundation.

we are grateful to christian fuchsberger , james f. gusella , richard h. myers , francisco domingues , christian x. weichenberger , and cosetta minelli  for their valuable comments and helpful discussion.
