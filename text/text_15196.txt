BACKGROUND
microarray technology has enabled the acquisition of comprehensive quantitative information about mrna, the transcriptome, in a tissue sample. because the functions of a cell are primarily determined by expression of the genome, we can assess the state of a cell by examining its transcriptome. however, microarray data may contain irrelevant individual differences as well as noise arising from artifacts of measurement. indeed, the quality of data generated by microarray assays has been questioned  <cit> . in our efforts to identify essential transcriptomic differences, the significance of observed changes should be evaluated objectively by statistical tests. by the tests, uncertain information can be omitted from further investigations, such as clustering, principal component analysis or pathway analyses.

the test methodologies should be consistent with the data characteristics and the purpose of the test. as with other statistical methods, the principle of a test methodology is based on some assumptions; for accurate analyses, the assumptions should be consistent with the characteristics of the data and the consistency should be checked. additionally, application of the methodology should be adequate for the purpose of the test  <cit> . since a statistical test is in balance between false positive and negative errors, those with overly stringent conditions will produce unnecessary false negatives. therefore, such strictness is irrelevant when one considers the intrinsic advantage of having complete transcriptome-wide coverage for the discovery of novel findings.

for the tests of gene expression levels, parametric methods such as student’s t-test or analysis of variance  are frequently used. generally, these methodologies estimate a p-value, which is the probability that a difference larger than that observed would occur by chance, when actually no difference among populations exists. if the p-value is less than a predetermined threshold, then the observed difference is considered to be significant. both in t-test and anova, the p-value is calculated by assuming that within group differences are normally distributed; if this assumption does not hold, we cannot accurately evaluate the observed differences among the groups.

microarray methodology simultaneously measures the expression levels of a large number of genes, and the expression levels of several genes are frequently analyzed collectively. accordingly, some concerns related to multiple tests  <cit>  have been expressed, such as an increase in the family-wise error rate   <cit>  or a false discovery rate   <cit> . efforts to control the multiplicity effect are becoming common in microarray studies; according to the assessment of statistical methodologies for microarray analyses conducted by jafari and azuaje  <cit> ,  <dig>  and  <dig> % have been applied in research and methodology studies, respectively. since many tutorial reviews have strongly recommended control of the multiplicity effect  <cit> , the proportion may be even higher. related to this concern, reducing the size of data by focusing to particular genes were also attempted  <cit> .

multiplicity of tests can increase fwer when we group a set of tests together as a family  <cit> ; in the presented cases of microarray, the whole set of data from a sample is recognized to form a family. inevitably, fwer, the expectation of having one or more false positives among the whole family, will become much greater than the expectation of the occurrence of a false positive in an individual test. therefore, if we wish to control fwer, a compensation of each estimated p-value or threshold is required. a simple solution for the compensation is to use the bonferroni correction, which compensates for the threshold by dividing it by the multiplicity; i.e., the number of gene contents of a chip. however, since the number of genes in a typical dataset is large, a correction involving division by such a large value will make the test extremely strict. holm’s procedure  <cit>  obviates this strictness to some extent by assigning different thresholds according to a ranking of p-values. nonetheless, such methods are considered to be strict since the families of microarray data contain very large numbers of genes.

as the number of tested subjects increases, fdr, the number of false positives among the declared positives, may also increase when large numbers of true null hypotheses are expected  <cit> . on the assumption that all null hypotheses are true, methodology that deals with fdr employs the likely calculation of holm’s procedure with more relaxed conditions for the compensating threshold; however, the fdr methodology is still stricter than the original tests without compensations.

despite these efforts to find a practical solution, the methodologies would inevitably make the tests very conservative, increase the false negatives, and reduce the overall information obtained. to deal with the strictness and to regain some of information that may be lost, extremely relaxed thresholds of the tests  were recommended  <cit> . actually, such relaxed conditions have been used in many studies, and it is not difficult to imagine that the thresholds were invoked ad hoc after the calculations had been performed. indeed, posterior tuning of the threshold to obtain better achievement was even attempted  <cit> . additionally, several offshoots have been produced for fdr methodologies, providing new options to analysts  <cit> . such alterations to the application would inevitably change the meaning of the methodology and thus, it seems that fdr has been used as an indicator in an arbitrary fashion.

both fwer and fdr assume high prior probabilities to the null hypotheses; i.e., the population means are identical. in addition, in a recently published book that featured microarray data  <cit> , efron insisted that pr is high in large-scale inferences, because most of the cases have small, uninteresting, but non-zero differences. this argument may sound useful for gene selection; indeed, his purpose was to "reduce a vast collection of possibilities to a much smaller set of scientifically interesting prospects". however, this is not necessarily consistent with the current demands of microarray data analyses; since many genes have functional relationships, significance can be tested on such cell functions as well. interesting functions can be easily found and tested by pathway analysis using databases  <cit>  and/or annotation key words  <cit> . rather, if the high pr scenario unnecessarily increases false negatives, it could limit important information that could be used at higher levels of analyses. moreover, to negate these small differences, renovation of the null hypothesis and test statistics are required. nevertheless, efron did not give any alternative methods, and the complex concept of "interesting" therefore introduced ambiguity in the application of the test. regardless, in both principle and application, evidence for estimation of pr is critically important.

we note a trend in the transition of proposed methodologies and the applications described above in that the tightened conditions to deal with the proposed multiplicity have been relaxed enough to employ the unusual handling of the threshold. while it is true that such relaxed application of the test can reduce the number of false negatives, the arbitrariness in choosing both the methodologies and the threshold can damage the objectivity of a test. indeed, as the transition proceeded, the appropriateness of any of the premises in the methodologies was not confirmed. additionally, the suitability of the methodologies to the purpose of the test has been left unexamined. for example, no concrete reason has been proposed to explain why the multiplicity should be considered. as will be discussed below, handling of plural test results simultaneously is not a sufficient reason for compensations of the multiplicity  <cit> . accordingly, the theoretical bases of present methodologies are rather fragile. in this article, we verify some of the premises against real microarray data from two popular platforms, and we will discuss the appropriateness for the awareness concerning multiplicity.

methods
data sources
several sets of agilent 44k chip data  <cit>  and affymetrix genechip data  <cit>  were obtained from the gene expression omnibus  repository  <cit> ; the series id of the data were gse <dig> and gse <dig>  respectively . mouse liver transcriptome data was obtained from mice administered different diets and the number of measurements in each group was five. data were normalized by sample according to the three-parameter lognormal distribution model  <cit>  by using supernorm data processing service ; the normalized data are available in the geo repository under the series id of gse <dig>  only those data in which signal intensity coincided with the theoretical data distribution were subjected to further analysis.

data analysis
statistical significances in gene expression levels between groups were estimated by using the t-test with welch’s approximation on normalized gene data. those were also estimated by two-way anova on normalized perfect match  data of affymetrix genechips, under the assumption that differences in pm data were the sum of group effects and probe sensitivity  <cit> . the compensations were performed by using p.adjust function of the r. the threshold used was  <dig>  in a two sided manner.

the integrated distribution of gene-wise data variations were compared against the normal distribution using quantile-quantile  plots. for each gene of the high calorie-fed group, normalized data - normalized within each chip - was collected . agilent platform data were selected because an artifact could produce a normal distribution if the average of many pm cell data produced on the affymetrix genechip platform were used, according to the central limit theorem. the collected data were further z-normalized using their mean and standard deviation  to cancel the differences in expression levels and sds among genes. the renormalized data were then ranked from  <dig> to  <dig> according to the signal intensity among the repeats in each gene. in each of the ranks, distribution of the renormalized data was presented at the corresponding theoretical quantiles by using boxplots. the boxes and bars represent the quartiles, and whiskers represent extreme data points that are no more than  <dig>  times the interquartile range from the box.

within-group sd values among the agilent chip data were estimated by using normalized z-scores. within-group sd values among affymetrix genechips, which measure a transcript using multiple pm probes, was estimated as the root mean square of the sds for the probes. the degrees of correlations between the sds were estimated in spearman’s ρ by using cor function of the r.

data simulation
a virtual dataset was produced for simulating a scenario in which genes share a common level of noise. the virtual dataset was used to estimate within-group standard deviations and p-values. each imaginary level was generated by summing the group effect, probe sensitivity, and noise component; these components were produced by generating normally distributed random numbers, of which sds were set to be identical to the root mean square of the sds observed in each of the genes of real data. scripts for the r is available as the additional file  <dig> 

RESULTS
variation in biological replications obeys normal distribution
the inconvenience of using parametric methods is that their premise assumes a certain distribution of the population, i.e., in cases of t-tests and anovas, data variation should be normally distributed. however, it is possible to confirm the actual distribution of data when considering the potential suitability of methodologies. a gene-wise distribution of variation can be verified by comparing the quantiles of real data with their corresponding theoretical values on a quantile-quantile  plot . unfortunately, because the number of experimental replicates is limited, assessment of the validity of this relationship for each gene is not very precise. additionally, this attempt will produce a number of qq plots equal to the gene contents, and thus the problem of making assessments using numerous vague results becomes apparent.

the general trend of these distributions will be revealed by integrating the gene-wise qq plots. the integration was performed using expression data further normalized among individual genes, and then determining the distributions of the renormalized expression data for each rank among individual genes . the data distribution for each of the ranks was presented using a box and whisker plot and compared with the theoretical value of normal distribution . the median of each rank distributed along the y=x line, and the height of each box and the length of each whisker showed similar levels of data fluctuations among ranks. both the coincidence with the normal distribution and the similarity in data fluctuations suggest that the variation of gene expression levels tended toward a normal distribution .

the compensating method and the number of declared positive genes
to determine the effects of the fwer and fdr compensating methodologies, the test results were compensated accordingly, and the numbers of significant genes were compared . the first category of groups compared high calorie and normal diets, with and without resveratrol administration. the agilent chips, which measure a gene by using a probe of a single spot, were used in this category. repeats of five or four measurements were normalized and processed by t-tests. the second category compared the effects of very low fat and normal diets in the scd-/- mouse and the +/+ mouse. the affymetrix genechips, which measure a gene by using several probes separately placed in the chip, were used in this category. the differences between the experimental groups were tested by using two-way anova on the normalized pm data . additionally, as the third category of groups, t-tests were performed by using gene expression data, which are estimated by summarizing the corresponding pm data of a gene. since anova on the pm data can handle a ten-fold larger number of data points, the estimated p-values could become quite low. therefore, almost half of the genes remained positive in fwer compensations . however, in those values estimated from gene data by t-tests, the compensations severely reduced the number of positive genes, even by fdr compensation, and no differences could be found in some combinations.

each gene exhibits a unique tendency in stability of expression levels
to select the proper methodology of testing, the noise level of the microarray technique must be known. if data variations are primarily attributed to technical noise, a constant level of noise can be expected among the genes, although the variations observed for each gene will either be over- or underestimated simply by chance. consequently, a test can be recognized as a part of the repetitions performed under the same conditions, coinciding with neyman's perspective  <cit> , and therefore the observed p-values would fluctuate mainly due to the noise; in such a case, pr must be high. this could be a valid reason to group a family from the whole set of a sample. conversely, if the microarray assay is sufficiently accurate and shows individual differences between samples, then each gene will exhibit unique tendencies with respect to the stability of expression levels. if this scenario is true, a correlation in the gene-wise variation of different groups will be apparent. in this case, p-values will show some evidence of variation, and grouping of the family would be unnecessary, negating the fwer scenario.

such a correlation can be evaluated using the standard deviation  within experimental groups; because the data variation is normally distributed , the magnitude of data variation could be evaluated using the sd. thus, a correlation was observed in scatter plots comparing gene-wise sds obtained from experimental groups of mice  fed different diets . for comparative purposes, an artificial dataset  was generated to demonstrate the case in which technical noise was the primary cause of the observed data variation. clearly, the real and the virtual datasets are different. in addition, the sds observed in the real data did not exhibit any relationship with the signal intensity . this independence between sds and signal intensities implies that the observed correlation between sds is not restricted to any particular range of signals, precluding the possibility that the effect of noise on weaker signals was responsible for the observed correlation.

distribution of p-values is complex
distribution of estimated p-values will give important information for selecting suitable methodologies for the test, since the origin of data variation can also be estimated from the distribution. if variations in the data can primarily be attributed to technical noise, which is a suitable case for high pr scenario, then the distribution of p-values can be simulated by using random numbers . in the simulation, within-group variance, the sensitivity of each pm probe for their target transcripts, and between-group variance were set to be identical to those observed in the real data . conversely, if the variation in expression data originates from biological differences and therefore is unique to the genes, then prediction of the p-value frequency distribution will be difficult since it will be affected by the stability of individual genes, which cannot be inferred at present. figure  <dig> represents p-value frequencies of real data , which varied among the combination of groups and are inconsistent with the high-noise scenario described above. the departure of the simulation from the real data suggests that the effect of technical noise on the test results would be limited. additionally, the rate of true null hypotheses also can be estimated by the distribution of p-values. the case for all true null hypotheses, for example, can be simulated by removing the between-group variance, which will result in a uniform distribution . in figure  <dig>  the distribution of the real data is not uniform ; particularly, the smallest p-value class contained considerably more genes than expected from the all null scenario. this outcome shows that the number of true null hypotheses would not be very large.

discussion
variations in the expression levels of each gene within a group were normally distributed , supporting the proposal that parametric tests are appropriate for the analysis of microarray data. actually, such falsifiability in the principles of a method is necessary to ensure analytical objectivity and it is one of benefits of parametric methods. even so, the distribution observed in figure  <dig> does not necessarily negate the possible occurrence of outliers, such as those attributable to dust during hybridization, and it is possible that such outliers could alter the test results. rather, the application of robust alternative functions, such as trimmed-mean and median absolute deviation to assess the data distribution parameters of the tests, may be applied to resolve such problems.

the gene-wise tendency observed for within-group sds  as well as the complex distribution of p-values  revealed that the primary origin of data variability was not due to technical noise, the level of which would be common to all genes. the primary origin of data variation therefore appears to be related to biological differences between individual samples, which could be taken to reflect the variability in the expression of individual genes. while the quality of microarray assays has been questioned  <cit> , the actual level of noise is therefore low enough to reflect biological differences among samples; thus, considerable improvements have been made to chips, reagents, and experimental protocols  <cit> , and advances in data analysis have resolved many of the problems previously associated with the normalization of data  <cit> , thereby improving the robustness of the assays and reproducibility of observations.

as has been described before, the main purpose of testing significance of a gene is to reduce uncertain signals in higher level of analyses. even if the technical noise is low, individual organisms have biological differences, and some genes may frequently and drastically change their expression levels according to biological requirements. to observe between-group differences for such genes, the tested data may lack a sufficient number of biological repeats. such volatility or stability of a gene can be estimated from within-group differences found in the forms of sds , and the significance of the between-group differences can be tested by using parametric methods. it should be noted that the test is performed for each gene independently, since both the observed sd and the between-group differences are unique to the gene. in this sense, there is no reason to combine some test results in order to evaluate them.

therefore, the suitability of the definition of a family by the gene contents of the microarray data should be reconsidered. actually, although it is a very crucial decision, there are no fixed rules for how we determine a family  <cit> ; rather, a family should be decided according to the purpose of the test  <cit> . in cases in which we wish to select only a few genes among the whole set of data and just concentrate on those genes, fwer could be important because the genes definitely should not be false positives. in the early years of microarray technology, such an application could be possible; however, in practice, the expressional changes are often confirmed by other methods or by a different level of observations such as enzymatic activities, even in such experiments. additionally, we rather tend to analyze the transcriptome as a whole, identifying trends in global changes. it is true that as the number of items to be analyzed increases, so too does the fwer. however, a few false positives may not be problematic, since transcriptome-wide observations such as primary component analysis or pathway analysis will not be much affected by a single false positive, since we would be handling hundreds of true changes. consequently, we do not need to control fwer for microarray data analysis, unless the purpose of the tests is very sensitive to an error.

the appropriateness for the concerns of increasing fdr should also be reconsidered. originally, the concern over fdr was based on the high probability of a true null hypothesis  <cit> . in a test for a true null, the p-value will be given by random effects and hence would not support the evidence; consequently, the expectation of a false positive should be estimated by using the threshold and not the p-value in the premise of fdr methods. however, each subject of transcriptome analysis is a so called 'point null' or 'sharp’ hypothesis, i.e., a double-sided test for coincidence of continuous variates, so rarely could this outcome be true in principle. in particular, we define a population within each gene under a specified set of experimental conditions. the expression level of the gene under those conditions can be represented by the center of the population's distribution, which would be normally distributed . the null hypothesis of each test is that the centers of the compared populations are identical. since expression levels are continuous values, the probability of the center having any particular value is null, and the probability of coincidence in some populations's centers is also null. actually, the distribution of the p-values supports the rare occurrence of a true null . therefore, as the premise of high probability of true nulls contradicts reality, the concern of increasing fdr is not applicable for transcriptome analyses.

the idea that compensation is unnecessary would also be true with respect to data obtained in sequencing-based methodologies, such as rna-seq  <cit> , when a transcript is measured with a sufficient number of reads. although those data are intrinsically discrete, they can be viewed as continuous data in a practical sense with a large number of reads. however, the precision of the data will become worse with fewer reads. the expected precision can be estimated according to the binominal distribution model; for example, reads of  <dig> and  <dig> out of one million reads would have a 95% interval estimate of 81- <dig> and  <dig> - <dig> , respectively. such technical noise will be added to the individual differences; in extreme conditions, the random effects will practically determine the test results. under such conditions, we should address the multiplicity problem. since pr would not be uniformly high but a function of the numbers of reads, the fdr  <cit>  would be too conservative; further investigations will be required for more suitable compensation.

we should not compensate for multiplicity of tests unless there is a good reason for doing so. it is now obvious that the high pr scenario is against the evidence presented here. this means that the currently proposed problems for multiplicity in microarray data, fwer  <cit>  and fdr  <cit> , have been negated in their principles. additionally, the excessively strict conditions will increase false negatives  and thereby disturb the higher levels of analyses. indeed, judgment of whether a finding is interesting or not is not necessarily performed for each gene; rather, it is important to remove "uncertainties about the direction" cases  <cit> , in which we cannot distinguish "up" or "down" expressional changes from the following analyses.

a far more important problem should concern the design and management of experiments. as was discussed, the principal source of noise is in individual differences among samples, but not in the measuring technique. since experiments are performed by using a limited number of replicated experiments, any small differences arising in experimental conditions among groups can introduce significant biases that may manifest as a global level of false positives. unfortunately, such experiment-based false positives cannot be controlled by any of statistical methods in principle, since what was observed actually occurred in that experiment. to control for such biases, experimental groups should be randomized  beyond groups, to avoid being treated in any specific order.

CONCLUSIONS
microarray analysis is accurate enough to observe individual differences among samples, and performing parametric tests for the results is recommended to confirm the significance of transcriptomic differences among groups. it should be noted that, in most of the cases, fwer or fdr should not be considered with respect to the tests; these procedures are inappropriate for global transcriptome analyses and will increase false negative errors, eliminating information that would otherwise be obtained. rather, strict control for false positive errors should be considered in higher levels of analyses, but not in the gene-wise case. a more important source of problems would be in the design and management of the experiment, since any biological differences of conditions among groups will produce false biases in the data.

competing interests
the author declares that he has no competing interests.

supplementary material
additional file 1
list of data id used in the figures. the list of geo id of the data used in the calculations.

click here for file

 additional file 2
scripts for the r. scripts used to perform  <dig> way anova and the simulations.

click here for file

 acknowledgements
i would like to thank dr. n. mitsuda in aist for bringing up discussions about this issue and dr. s. youssefian for his precious comments on the manuscript.

this article has been published as part of bmc systems biology volume  <dig> supplement  <dig>  2011: 22nd international conference on genome informatics: systems biology. the full contents of the supplement are available online at http://www.biomedcentral.com/1752-0509/5?issue=s <dig> 
