BACKGROUND
widespread availability of high-throughput dna sequencing instruments, and the development of novel techniques based on sequencing, provides a potentially very valuable resource for researchers in genomics. however, transforming sequence data into biologically meaningful information requires sophisticated computational infrastructure and support. the size of the required computational infrastructure is outpacing what many labs and even universities are able to support. in addition, the setup and maintenance associated with a computational infrastructure presents significant problems for individual investigators and small labs that may not have the necessary informatics support.

fortunately, a computational model – cloud computing  <cit>  – has recently emerged and is ideally suited for the analysis of large-scale sequence data. in this model, computation and storage exist as virtual resources in remote datacenters, and can be dynamically allocated and released as needed. however, cloud computing resources are not yet suitable for immediate “as is” use by experimental biologists. in the current model, cloud resources are acquired as independent, stripped-down units that must first be customized for the intended use. they then must be configured to work in unison and a mechanism must be provided for the data uploaded and analyzed on those resources to persist beyond the life of a  cloud compute resource.

to date, there are several projects and solutions in the context of high throughput sequencing and bioinformatics in general that utilize cloud computing to deliver the computational capacity and on-demand scalability . however, these projects primarily target specific problems and provide custom solutions for a given tool or methodology. although very valuable, such tools provide minimal help for researchers wanting to compose a variety of tools into an analysis pipeline or for researchers that want to use their own tools when utilizing cloud computing resources. thus, there is a need for a flexible solution that can be utilized in a variety of scenarios and that provides support for customization.

this paper introduces cloudman from the galaxy project  <cit> , an integrated solution that leverages existing tools and packages by providing a generic method for utilizing those tools on cloud resources and abstracting out low-level informatics details. this solution handles all of the intricacies of cloud computing resource acquisition, configuration, and scaling to deliver a personal compute cluster in a matter of minutes. all interaction with galaxy cloudman and the associated cloud cluster management is performed through a web based user interface and requires no computational expertise. the deployed cluster comes preconfigured with all of the bioinformatics packages available in the nerc bio-linux workstation   <cit>  as well as a range of ngs tools available within the galaxy framework  <cit> . moreover, the process of tool deployment is fully automated and decoupled from the base machine image, making it possible to very simply add additional tools to an individual cloud cluster instantiation.

RESULTS
instantiating and controlling a cloud cluster
the galaxy cloudman application currently supports creation of a compute cluster on amazon’s ec <dig>  <cit>  cloud computing infrastructure. the process of instantiating a cluster does not require any computational experience, and requires no compute infrastructure or software beyond the web browser used to control the cluster. galaxy cloudman is thus ideal for independent researchers and small labs that have a specific or periodic need for computational resources but lack informatics expertise and commitment to manage and maintain a computational cluster. the process of instantiating a cloudman compute cluster consists of three steps:  create an amazon web services  account and sign up for the ec <dig> and s <dig> services,  use the aws management console to start a master ec <dig> instance, and  use the cloudman web console on the master instance to manage the cluster size. step one needs to be performed only once, usually by a person controlling the cloud cluster. steps two and three need to be performed each time running jobs on a compute cluster is desired, but, again, only by the person controlling the cluster. once set up, additional users may use the cluster simply through the galaxy web interface without requiring any system accounts or privileges. a single instance of cloudman controls a single cluster – of potentially variable size – but a single user may create as many cloudman cluster instances as desired.

once cloudman starts, it automatically configures the master instance as a head node of a sun grid engine   <cit>  compute cluster but it does not start any additional worker instances or assign persistent storage to the cluster. in the context of cloud computing, compute instances are usually transient, meaning that any changes made to an instance while the instance is alive are lost at instance termination. in order to persist any data uploaded to the cloud or any analysis results, the data needs to be stored on an external data volume. in the case of cloudman on ec <dig>  amazon’s elastic block storage   <cit>  volumes are used for data persistence.

once available, the cloudman web interface  allows a user to configure additional features of the cluster. currently, the following features are supported: association of a persistent data volume with the cluster, addition of a range of ngs tools , and addition of the galaxy analysis interface. without a persistent data volume, a user may use the cluster for a proof-of-concept computation or a one-time analysis. for clusters that are maintained over time, adding persistent storage is initiated with a click of a mouse, with all infrastructure intricacies handled automatically by cloudman. similarly, if a completely configured instance of galaxy is desired for use of a range of ngs tools, it is trivial to do so through the cloudman ui.

in addition to the user-level cluster functionality, cloudman makes it easy to exploit what is arguably the most unique and powerful feature of cloud computing - elasticity. through the cloudman web interface, one can scale the size of the cloud cluster at runtime by adding or removing worker instances comprising the cluster . similarly, the size of the persistent data volume  associated with a cluster can easily be expanded. within ec <dig>  individual ebs volumes used as persistent data storage medium within cloudman, have a predefined size. as the use of a given cluster expands, users may consume the space associated with the given cluster. the cloudman web interface allows ‘growing’ the size of the persistent data volume associated with a cluster. in the background cloudman orchestrates the following steps to accomplish the task at hand:  stop any services using the user data volume,  detach the current user data volume from the master instance,  snapshot the detached volume,  create a new volume of user-specified size based on the snapshot from step  <dig> and attach it to the master instance,  grow the file system on the new data volume, and  resume any services.

because galaxy cloudman is built on top of a bio-linux machine image, all of the tools available within bio-linux can be used on the instantiated cloud cluster. accessing the bio-linux tools is realized through a command line interface - just like on any other compute cluster. as indicated earlier, the sge job manager is configured and used on the cluster, making it possible for users to simply copy their job scripts to the cloud cluster and run them there - but with the scalability offered through cloud computing.

when a given cluster is no longer needed, the cloudman web interface is used to terminate all of the services and worker instances. if persistent data storage was associated with the cluster, the data is preserved while the cluster is offline, and made available in the same state once the cluster is instantiated again. it takes only a few minutes to scale up or down a cluster and consume the required amount of resources.

tool availability
by default, the galaxy cloudman is built on top of a bio-linux machine image available from cloudbiolinux  <cit>  and thus makes all of the tools packaged by nerc bio-linux  <cit>  immediately available. nerc bio-linux represents a set of packaged and fully featured bioinformatics tools that enable users to focus on tool usage rather than tool installation and configuration. by building on top of such varied set of bioinformatics tools, one can combine the cluster controlling functionality of cloudman with the variety of tools. in addition to the tools available through bio-linux, a set of ngs tools available through galaxy are also available for use, including: bowtie  <cit> , bwa  <cit> , and samtools  <cit> . if a user desires additional tools, we have provided a mechanism for streamlining the tool installation process . a script used to automatically install all the tools available to a default instance of cloudman cluster is available at https://bitbucket.org/afgane/mi-deployment/; using this script and customizing it to include the desired tools provides a simple method for modifying the capabilities of a cluster instance. the script supports the ability to install additional tools at cluster runtime only or to persist the changes for future cluster invocations.

CONCLUSIONS
to keep up with the growth of data being produced in life sciences and the accompanying computational demand, there is a need for increased access to computational resources. cloud computing offers access to such resources but still makes it difficult to create complex deployments of useful standalone infrastructures. this is especially cumbersome for individuals and small labs that lack informatics support to fully harness this general-purpose infrastructure.

the default version of galaxy cloudman can be used “as-is” to support creation and control of fully functional compute clusters on cloud resources; it supports a broad range of bioinformatics tools and it makes it possible to add additional tools with little effort. the process of tool deployment is completely automated and well documented, making it reproducible in other environments. overall, the system is simple to use and it targets individual researchers so they can gain access to the computational resources they need without requiring support from skilled bioinformatics personnel.

the source code for the entire project is available under the mit licence and is available from http://bitbucket.org/galaxy/cloudman/. documentation and detailed instructions on how to use galaxy cloudman are also available from http://usegalaxy.org/cloud.

