BACKGROUND
high-throughput technologies such as microarray or next-generation sequencing enable researchers to routinely measure the expressions of tens of thousands of genes in many patients. typically, long lists of interesting candidate genes are generated by subsequent computational analyses. however, interpreting these gene lists is challenging. recognizing that genes act in concert to drive various biological processes, gene set enrichment analysis  was introduced  <cit>  to summarize genomics data using a predefined gene set. nowadays, gsea is a heavily used tool in bioinformatics  <cit>  and has been successfully applied to gain insights into the biological function of diseases such as cancer and diabetes.

however, the gsea procedure can be highly time-consuming since significance of a calculated enrichment score is typically tested using a resampling strategy drawing large numbers of permutations. when a whole database of gene sets is used, the amount of required permutations is even higher in order to account for multiple hypothesis testing. furthermore, size and availability of input data sets continue to increase driven by advances in high-throughput technologies  <cit> . thus, developing fast software solutions is of high importance to research. previous work on accelerating gene set analysis has been limited to cloud computing  <cit> . we present the rapidgsea suite – an efficient parallelization of the gsea method for commonly available multi-core cpus and cuda-enabled gpus. by using a combination of parallelization techniques we can achieve speedups of one order-of-magnitude on xeon cpus and around two orders-of-magnitude on a single gpu compared to broadgsea.

implementation
this section is divided into three parts. first, we give a brief explanation of the sequential gsea algorithm and its four major processing steps for estimating the nominal p-value of a determined enrichment score using a single gene set. second, we introduce novel parallelization schemes for single and multiple gene set probing and their explicit implementation optimized for multi-core cpus and cuda-enabled gpus. finally, we describe the usage of our standalone application and the bundled package for the r framework.

the sequential algorithm
the traditional gsea algorithm operates on a real-valued gene expression matrix d of shape |g|×|p| where gi∈g denotes |g| unique gene identifiers and pj∈p enumerates |p| patient identifiers each labelled by a binary phenotype l∈{ <dig> } encoding cases and controls. the computation of the enrichment score statistics can be split into four major stages:

computation of local deviation measures
for each gene symbol gi  a local deviation score Δ is computed that encodes the inter-class deviation between cases and controls. as an example, the difference of means between both classes can be employed to express their variability per gene: 
 Δ=μi−μiμi=∑j=0|p|−1lmdμi=∑j=0|p|−11−lmd 

where m=∑j=0|p|−1l and m=|p|−m denote the number of patients in each class from the set { <dig> }. variations that combine intra-class means and standard deviations e.g. 
  <dig> fold change:Δ=μi−μiσi+σi,t-test:Δ=μi−μiσi2+σi <dig> 

are common choices for Δ in gsea implementations. please note that extensions from binary to real-valued phenotype profiles l∈ℝ using euclidean distance, pearson’s product-moment or spearman’s rank-order correlation coefficient are straightforward  <cit>  and thus will not be discussed further in this paper.

gene ranking
after computation of the local deviations, the indices i∈{ <dig> …,|g|−1} enumerating the gene symbols gi are reordered such that 
 Δgσ,…,Δgσ,…Δgσ 

is a sorted  sequence of local deviation scores. the sequence of reordered gene symbols gσ is called gene ranking according to Δ and will later be used to determine the enrichment score statistic. figure  <dig> illustrates the first and second stage of the gsea algorithm.
fig.  <dig> computation scheme of stages  <dig> and  <dig>  schematic overview depicting the computation of gene transcription differences  and the gene ranking procedure  of the sequential gsea algorithm operation on a single set of phenotype labels )j=



enrichment score computation
to elucidate significant differences in gene regulation across different phenotypes, it is generally insufficient to consider transcription differences Δ) individually. each gene can be significantly up- or down regulated by chance alone, or through correlation with processes such as the cell cycle. in principle, information can be gained from clustering genes according to their regulation  <cit> . interpretation of the resulting clusters, however, is often unclear. instead, prior information about gene classes that are assumed to behave correlatedly , is used in the analysis. today, this is typically achieved through the framework of gsea, which considers the significance of the transcription profile of a set of gene symbols s⊂g as a whole as opposed to individual enrichment values.

let s be a gene set supposedly correlated to the observed phenotypes and σ the aforementioned reordering of gene symbols. the enrichment scorees is then determined as the maximal amplitude of a weighted running sum statistic ρ∈: 
 es=ρargmaxk|ρ|whereρ=∑i=0k1α·|Δ)|qifgσ∈s−1βifgσ∉s 

with precomputed constants α=∑g∈s|Δ|q and β=|g|−|s|. the exponent q≥ <dig> is usually chosen from the set { <dig> , <dig> } and controls the leverage of the weights |Δ)|. please note that the special case q= <dig> is the well-known kolmogorov-smirnov statistic  <cit> . figure  <dig> illustrates an example for the linear-weighted  computation of es using a toy data set.
fig.  <dig> computation scheme of stage  <dig>  schematic overview of the incremental computation of a linearly-weighted  kolmogorov-smirnov statistic operating on our toy data set. the enrichment score is determined by the maximum amplitude of the running sum ρ



significance estimation
similar to pearson’s correlation coefficient the enrichment score takes values in the interval  with |es|= <dig> indicating perfect correlation and |es|≈ <dig> implying no dependency between s and the observed phenotypes in terms of the used deviation measure. when es=± <dig> all gene symbols g∈s are situated at the top/bottom of the ranked gene list. in contrast, small values are observed if the gene symbols g∈s are scattered over the index domain and thus are unlikely to explain the phenotype distribution.

es values have no intrinsic significance, though. a value of es= <dig> , as computed in our toy model in fig.  <dig>  might correspond to a high or low significance, depending on the probability to arrive at such a value by chance alone. unfortunately, closed forms for the statistical distribution of enrichment score are inaccessible. therefore, p-values are typically estimated by sampling the null distribution using a permutation of phenotype labels. please note that while some gsea implementations allow to permute gene identifiers instead of phenotype labels  <cit>  to estimate the null distribution, phenotype permutation is often considered the more appropriate choice – genes are expected to feature statistical dependencies within a single patient, while probes gained from distinct patients are less likely to do so. hence, in the following we only consider phenotype permutation.
fig.  <dig> computation of enrichment scores on permuted phenotype labels. schematic overview of the incremental computation of the enrichment score statistics operating on our toy data set where the phenotypes of patient  <dig> and  <dig> have been swapped: ))j=




when probing more than one gene set at once, p-value estimates have to be adjusted for multiple hypothesis testing. as an example, bonferroni-corrected acceptance levels and family-wise error rates  are frequently used criteria to evaluate the significance of enrichment scores. the need for a large number of samples in the space of permutation is even more pronounced during multiple hypothesis testing: let e∈Π be the identity permutation in the set of n tested permutations Π. then the p-value estimate for a fixed gene set s is strictly positive  <cit>  and lower-bounded by inverse sample size: 
 p^s=1n∑π∈Π|es|≥|es|≥1n 

the molecular signature database v <dig>   <cit>  contains more than  <dig>  gene sets divided into eight major collections. thus, when testing all gene sets at a bonferroni-adjusted significance level of α= <dig> , <dig> we have to probe more than  <dig> , <dig> permutations in order to allow the result p^s<α. for the rest of the paper, we focus on the efficient computation of the enrichment score table es since p-value estimates and other statistics such as fwer can be determined using its entries in a post-processing phase.

the parallel algorithm
gsea can be parallelized using coarse-grained computation schemes such as assigning threads to each permutation π or gene set s since all entries in es can be processed independently. this approach will be used in our multi-threaded shared memory implementation of gsea : the set of n probed permutations is split into m partitions each of approximate size nm and afterwards m threads independently operate on the individual chunks. this can easily be achieved in shared memory architectures using openmp pragmas. moreover, extensions to distributed memory architectures using the message passing interface  are conceivable.

however, cuda-enabled accelerators can maintain up to several thousands of threads  but only exhibit a limited amount of ram . as a result, fine-grained computation schemes that parallelize the aforementioned building blocks of the gsea algorithm have to be employed to exploit the full compute capabilities of cuda-enabled accelerators. in the following, we will present the fine-grained parallelization scheme for each processing stage separately.

computation of local deviation measures
many local deviation measures used in traditional gsea e.g. difference of means or fold change can be expressed in terms of intra-class means and standard deviations. therefore, we have to separately accumulate sums of expression values and their squares for each of the two phenotypes. although efficient implementations for parallel reduction on cuda-enabled accelerators are known  <cit>  we instead parallelize the loop over the gene symbols since each row of the data matrix d can be processed independently without the need for expensive synchronization as used in reduction algorithms. moreover, the number of gene symbols will most likely exceed the number of probed patients and thus the loop over gi is better suited for massively parallel computation. during the calculation of statistical moments we encounter two challenges:

first, the numerically stable computation of standard deviations is known to be a stubborn task. on the one hand, when accumulating a large number of entries  one has to account for numeric stability using cancellation-compensation  <cit>  or two-pass algorithms for the standard deviations. on the other hand, when dealing with only a few patients one-pass or cancellation-compensated online algorithms for the standard deviation might be a proper trade-off between accuracy and speed  <cit> . rapidgsea exploits the c++ template engine to provide specialized and user-customizable accumulator functors adaptable to the task’s requirements.

second, the gene-wise computation of transcription differences Δ accumulates statistical moments along the rows of the matrix d. using a cuda thread block of up to  <dig>  cuda threads for a fixed permutation of the phenotype array l) it is advisable to transpose d to guarantee coalesced access to global memory. more specifically, since a warp of  <dig> threads is executed simultaneously on the gpu their concurrent reads from the same column of d would result in excessive cache misses. in contrast, when transposing d the same access pattern causes consecutive threads to simultaneously access consecutive memory. this change from column-major-order to row-major-order traversal decreases the runtime of this processing step by one order-of-magnitude in our experiments. since d usually tends to be smaller than  <dig> mb, we can use a standard bank conflict-free out-of-place algorithm for matrix transposition  <cit> . figure  <dig> depicts the described computation scheme for two cuda thread blocks each consisting of ten cuda threads. please note that the genes are distributed using a block-cyclic distribution if the number of genes exceed the number of threads.
fig.  <dig> fine-grained parallelization of stages  <dig> and  <dig>  parallelization of the deviation score computation operating on the transposed data matrix d
t. each thread block draws a permutation by shuffling the original phenotype label list in shared memory. the threads within a thread block independently accumulate gene transcription differences for each gene symbol identifier  ensuring coalesced reads from global memory. finally, the local deviation scores are sorted using the segmented radix sort primitive of cub



the sampling of permutations can be accomplished using the pseudo random number generators  from the curand library  <cit>  bundled with the cuda sdk. unfortunately, curand does not provide host-sided calls for the random number generators defined in the device api. thus, we implemented the keep it simple stupid  prng  <cit>  for the cpu and gpu in order to provide consistent results across architectures. both curand’s xorwow prng and our kiss implementation pass all tests of the dieharder suite  <cit> . the permutation of the phenotype labels l) is generated by reordering the original label list l in shared memory using a fisher-yates shuffle.

gene ranking
up to this point, the transcription differences Δ have been computed for a batch of permutations that fit into the ram of the gpu. unfortunately, we cannot directly apply a key-value sort to Δ within the same kernel due to the  <dig> kb limitation of shared memory. thus, after termination of the previous kernel, we call a device-wide key-value radix sort primitive cub::devicesegmentedradixsort from the cub  <cit>  library specifically optimized for the efficient sorting of segmented arrays. this approach is up to one order-of-magnitude faster than stacking single device-wide cub::deviceradixsort calls for each permutation or aliasing global memory to the block-wide cub::blockradixsort primitives. the number of concurrently sorted arrays has been set to  <dig> as a proper trade-off between runtime and memory consumption. at the end of this stage, we have stored the sorted deviation scores Δ) and corresponding indices σ for each of the probed permutations in global memory. figure  <dig> illustrates the described workflow.

enrichment score computation
the computation scheme for the running sum statistic is similar to the processing of local deviation scores. for each permutation a cuda thread block operates on a pair ,Δ)) of reordered gene symbols and gene transcription differences. the test whether a gene identifier is part of a gene set gσ∈s is usually implemented with hash sets on cpus. efficient hashing algorithms on cuda-enabled devices are stated in the literature  <cit>  which typically involve linked lists or binary search in sorted arrays in order to resolve collisions. however, we decided to encode the affiliation of a gene g with a binary bit mask b. the computation of the bit mask can be delegated to the cpu using stl hashes. further, the corresponding execution time can be overlapped with the deviation score and gene ranking kernels. as a result, we can determine a gene’s affiliation on the gpu in constant time by reading the corresponding entry of the bit mask from global memory.

each thread k within a thread block processes one gene set sk. shared memory can be utilized to avoid slow accesses to global memory since all threads in a warp have to access the same entry from the bit mask b,sk) in random order. to achieve this, batches of  <dig> entries of reordered gene transcription differences Δ) and bit mask entries b,sk) are consecutively loaded into shared memory  and afterwards processed in order. due to the large number of genes we again use numerically stable kahan summation  <cit>  in order to suppress cancellation in floating point arithmetic. finally, the maximum amplitude of the weighted kolmogorov-smirnov statistic is written to the enrichment score table es and consecutively transferred to the host. figure  <dig> illustrates the described procedure.
fig.  <dig> fine-grained parallelization of stage  <dig>  parallelization of the enrichment score computation operating on the ranked genes and precomputed bit masks. again, each thread block processes a permutation. the threads within a thread block independently accumulate the running sum statistic for each of the probed gene sets. shared memory is utilized to suppress redundant reads from global memory



significance estimation
when only computing p-value estimates the counting of values in the tails of the null distribution could be accomplished on the gpu using the device-wide reduction primitive cub::devicesegmentedreduce from the cub library. a similar approach for the computation of the fwer is conceivable. however, we decided to copy es to the host in order to provide the full information for consecutive analysis and visualization of sampled distributions.

bindings for the r language
the core algorithm written in cuda and c++ <dig> is provided as standalone application and additionally as rcpp-based  <cit>  package for r. the latter includes functions for the reading of gene expression tables , class assignment labels  and gene sets files  as well as methods for the querying and selection of the used gpu .

RESULTS
the performance of rapidgsea is compared to the broadgsea java application in version  <dig> . <dig>  <cit>  on the following platform: 
 intel xeon e5- <dig> v <dig> @  <dig>  ghz ghz  with  <dig> gb ddr <dig> ram

 nvidia geforce gtx titan x with  <dig> gb gddr <dig> ram, nvidia tesla k40c with  <dig> gb gddr <dig> ram disabled ecc, nvcc ver.  <dig> 

 ubuntu  <dig>  lts, gcc ver.  <dig> . <dig>  icedtea ver.  <dig> . <dig> openjdk 64-bit server vm



in our experiments, we use gene expression data  consisting of  <dig> mds patients and  <dig> healthy controls where the array spots have been collapsed to |g|= <dig>  unique gene symbols by max pooling ambiguous mappings in the affymetrix human genome u <dig> plus  <dig>  array   <cit> . we further choose the smallest  and the biggest  collection from the molecular signatures database  <dig>   <cit> . the number of tested permutations ranges from  <dig>  up to  <dig>  =  <dig> , <dig> samples. single-precision runs are executed on the geforce gtx titan x and double-precision experiments on the tesla k40c gpu. if not stated otherwise, rapidgsea and broadgsea have been configured to read the input data from disk and afterwards to write the full enrichment score table es to the file system in order to ensure fair competition.

accuracy and compliance of enrichment scores
we have evaluated the compliance of computed enrichment scores between broadgsea and rapidgsea using the identity permutation on the  <dig> gene sets of the hallmark collection under the difference of classes measure. the deviation of computed enrichment scores between rapidgsea and broadgsea comply within six digits for both single and double-precision arithmetic . using identical floating point data types the computed scores of both rapidgsea components, cudagsea and ompgsea, are indistinguishable.
fig.  <dig> compliance of computed enrichment scores. histograms of the difference of computed enrichment scores between rapidgsea and broadgsea over the  <dig> gensets from the hallmark collection. both, single and double-precision residues comply within six digits



however, a comparison of computed histograms es is more complex due to different implementations of random number generators. thus, we have approximated the probability density functions  of the enrichment score distribution using n= <dig>  permutations and n= <dig>  bins uniformly sampling the interval . afterwards, the approximate cumulative distribution functions  are computed by prefix summation. the maximum absolute difference of approximated cdfs, also know as kolmogorov distance, 
 dist=maxk|cdfk−cdfk| 

is then determined for each of the  <dig> gene sets. note, the kolmogorov distance is a reasonable choice since it determines the measurement error of the area under the pdf of the enrichment score distribution and thus relates to the error of the estimated p-value. figure  <dig> visualizes the described procedure for one gene set. the minimum/median/maximum absolute deviation between the approximated cdfs produced by rapidgsea and broadgsea over the  <dig> gene sets is given by  <dig> / <dig> / <dig> . when comparing two histograms both computed by broadgsea with different seeds the same metrics yield  <dig> / <dig> / <dig> . moreover, in  <dig> out of  <dig> cases rapidgsea produces histograms with a smaller kolmogorov distances to broadgsea in contrast to  <dig> cases where both histograms produced by broadgsea are more similar. concluding, the deviations in estimated areas are reasonably small and mainly caused by different samples in permutation space.2fig.  <dig> compliance of computed histograms of enrichment scores. histograms of the computed enrichment scores of rapidgsea and broadgsea using n= <dig>  permutations over one of the  <dig> gensets from the hallmark collection. both, the computed pdfs  and cdfs  are visually almost indistinguishable. the absolute difference of cdfs  and thus the absolute error of p-values is bounded by less then  <dig>  in this example



scaling over multiple cores
we perform a strong scalability test of our ompgsea implementation over multiple cores of the xeon cpu. note, ompgsea is part of the cudagsea binary and can be selected using the -cpu flag. the time needed to process the  <dig> gene sets defined in the h collection is measured for a fixed input size of n= <dig>  permutations and a variable number of threads. the experiments cover performance measurements for up to ten physical cores each executing a single thread and a hyper-threaded scenario where up to twenty threads are assigned to ten physical cores. when taking measurements on less than ten physical cores we enforce a thread’s affinity using the taskset command in order to avoid rescheduling by the operating system. the obtained runtimes are listed in table  <dig> and illustrated in fig.  <dig>  the first experiment utilizing only physical cores reveals almost linear speedup for ompgsea with an efficiency of roughly  <dig> % for ten cores. however, the hyper-threaded variant exhibits slightly super-linear behaviour for up to nine physical cores and an efficiency of  <dig> % for all cores. throughout the rest of this paper all reported runtimes of ompgsea refer to the hyper-threaded ten core scenario running approximately ten times faster than the corresponding single-core application. please note that the time for writing the enrichment score table es to disk has been neglected during this benchmark.
fig.  <dig> scaling ompgsea over multiple cores. runtime in seconds  and speedup in comparison to single-threaded performance  using up to ten physical cores with and without hyperthreading


noht
1
2
3
4
5
6
7
8
9
10

ht
1
2
3
4
5
6
7
8
9
10
runtime in seconds, speedup and parallelization efficiency using up to ten physical cores with disabled hyperthreading  and enabled hyperthreading  for a fixed number of n= <dig>  permutations on the hallmark gene set collection



comparison between rapidgsea and broadgsea
the execution time of rapidgsea and broadgsea is measured on the aforementioned data set over a wide range of permutations  using the hallmark  and curated  collections. the experiments include parsing of input files, memory transfers over pcie when using cuda and writing the enrichment score table es to spinning disk. the obtained runtimes and speedups are listed in table  <dig> and illustrated in figs.  <dig> and  <dig>  numbers in square brackets or dashed lines indicate linearly extrapolated runtimes for broadgsea in log-log space for large amounts of permutations.
fig.  <dig> performance comparison between rapidgsea and broadgsea on hallmark gene set collection . runtime in seconds of rapidgsea and broadgsea  and speedups of rapidgsea in comparison to broadgsea  for up to  <dig>  permutations on the hallmark  collection consisting of  <dig> gene sets

fig.  <dig> performance comparison between rapidgsea and broadgsea on curated gene set collection . runtime in seconds of rapidgsea and broadgsea  and speedups of rapidgsea in comparison to broadgsea  for up to  <dig>  permutations on the curated  collection consisting of  <dig>  gene sets. please note that dashed lines indicate linearly extrapolated results in log-log space


h 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> ,576

c <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> ,576
runtime in seconds and speedups of rapidgsea compared to broadgsea  using up to n= <dig>  permutations on the hallmark  and curated  collection. please note that runtimes in square brackets indicate linearly extrapolated runtimes of broadgsea in log-log space



our multi-threaded implementation ompgsea outperforms broadgsea on both gene set collections  by at least one order-of-magnitude. note, although broadgsea spawns more than twenty threads the majority remains idle during processing. therefore, broadgsea cannot benefit from the additional physical cores of the xeon processor. the same behaviour can be observed on an intel i <dig> i3970x cpu with six physical cores.

moreover, cudagsea outperforms broadgsea by around two orders-of-magnitude with growing speedups for an increasing number of permutations. this can be explained by the thread occupancy of the used gpus. both, the geforce titan x and the tesla k40c can store at once tens of thousands of permutations  within their  <dig> gb of ram. thus, when probing a small number of permutations the majority of streaming multi-processors remain idle. furthermore, the parsing of input files and dumping of results takes several seconds and cannot be parallelized on the gpu.

CONCLUSIONS
in this paper, we have introduced rapidgsea – a software suite consisting of two tools for facilitating permutation-based gsea: cudagsea and ompgsea. cudagsea is a cuda-accelerated tool using fine-grained parallelization schemes on massively parallel architectures while ompgsea is a coarse-grained multi-threaded tool for multi-core cpus. ompgsea outperforms the state-of-the-art implementation of gsea  by at least one order-of-magnitude in terms of execution times while providing compliant results. furthermore, cudagsea outperforms broadgsea by around two orders-of-magnitude. the time for probing  <dig> , <dig> permutations on a gene expression data set consisting of  <dig>  unique gene symbols and  <dig> patients can drastically be reduced from roughly  <dig> days for broadgsea to less than two hours using rapidgsea on a commonly available tesla k40c gpu in double-precision or less than one hour on a geforce titan x in single-precision.

a possible direction of future research in order to further reduce runtimes is the parallelization of gsea on a compute cluster with multiple gpus attached to each node. furthermore, extensions of gsea to consider graph-based  or network-based  correlations between gene symbols and observed phenotypes have gained increasing attention in recent years. it will be interesting to investigate how the parallelization techniques discussed in this paper can be applied to accelerate these extended enrichment methods.

availability and requirements
project name: cudagsea project home page: https://github.com/gravitino/cudagseaoperating system: linux programming language: c++, cuda, r other requirements: cuda-capable gpulicense: gnu lgpl any restrictions to use by non-academics: none

endnotes
 <dig> please note that throughout this manuscript, we use zero-based indexing.

 <dig> individual results for each gene set can be found at the github repository of rapidgsea.

abbreviations
apiapplication programming interface

cudacompute unified device architecture

fwerfamily-wise error rate

gseagene set enrichment analysis

mpimessage passing interface

pcieperipheral component interconnect express

prngpseudo random number generator

