BACKGROUND
in recent years, the biology community has been increasingly focused on locating genes and identifying their functions. advances in technology have allowed a vast amount of biological data to be collected, including the sequencing and annotation of rapidly growing numbers of genomes, the generation of genetic and physical maps, and the description of both mutant and wild type phenotypes/traits. each data collection, however, has its own scope, terminology, and descriptions, which vary not only across domains but also within domains by research group and individual. to unify the vocabulary and descriptions within a domain, and also to serve as a bridge among various sub-domains, many bio-ontologies have been developed. in fact, there has been enormous effort put forth by the biology community towards the development of various ontologies  <cit>  with a significant contribution being made thereby to plant science research. for example, the gene ontology   <cit>  contains terms and definitions used to describe biological processes, cellular components, and molecular functions. this ontology has been developed to allow ready access to gene function knowledge across different species. other relevant ontology examples include the plant ontology   <cit> , an umbrella ontology covering various sub-domains in the plant realm including growth/developmental stages, plant structure  <cit> , and the trait ontology , a sub-ontology that describes characteristics of a plant such as height, leaf color, and disease resistance  <cit> ; and the phenotype and trait ontology , which is an ontology of phenotypic qualities defined "to be used in conjunction with ontologies of 'quality-bearing entities'"  <cit> . pato contains descriptions of many general phenotypic qualities, including both qualitative and quantitative characteristics, and some groups have expressed interest in using pato for phenotype descriptions  <cit> .

phenotype annotations are often recorded based on the entity-quality  model, in which the entity  is defined in terms of concepts from one or more ontologies, e.g. po or go, and the quality  is assigned to a concept from an ontology of qualities, e.g. pato. as an example, consider the representation of a "green maize leaf" using the eq model. one would have to specify the taxon , the po plant structure , the to trait , and the pato identifier . annotations recorded in this way are said to facilitate comparisons of phenotype descriptions within and across species. by manually translating free-text phenotype descriptions to the eq model, washington  <cit>  demonstrated the ability to compare human and animal disease phenotypes utilizing the hierarchical structure of the ontology as well as the annotation frequency. a similar approach for comparing phenotype annotations within and across plant structure and species could be applied.

because phenotype annotation and curation remain very time-consuming tasks, various tools have been created to help with annotation. phenote  <cit>  is one such tool that describes phenotypes using the eq model, and this tool was utilized by washington in  <cit> . in addition, the solanaceae community has developed and implemented a user-friendly annotation utility that exploits bio-ontologies  <cit> . even with the help of annotating software, it is still overwhelming for curators to annotate all the available phenotype images. automatic annotation methods are needed, and there has been some work on this topic already. beck  <cit>  successfully mapped the output from two phenotyping pipelines in mice to both the mammalian phenotype ontology as well as the eq model, which facilitated automatic annotation of mouse phenotype data generated using these pipelines. the approach mainly relies on standardized and measurable phenotypes of mice, such as body weight and average pulse rate, which is different from the subjective descriptions often used to annotate plant phenotypes, such as "small irregular chlorotic lesions on a maize leaf."

although textual descriptions of phenotypes seem to be clear to human understanding, they are subject to various semantic issues when handled in a computational manner. despite the tremendous amount of time and effort put forth in their construction and maintenance, current ontologies remain plagued by issues related to semantic ambiguity, granularity, and heterogeneity. ambiguity occurs when a single term has different meanings, though sometimes the differences are very subtle. consider the various yellow plant structures in figure  <dig>  one would like the term "yellow" to represent the same quantitative hue across all phenotypes, body parts, and taxa. however, the "yellow" describing a maize leaf ) is obviously different from that which describes a maize ear, kernel, or tassel ), and the differences are even more striking when one ventures into other taxa ). though detailed color names do exist for these yellow variations, phenotype annotators typically use very coarse descriptors for colors, like "yellow", as a soft qualitative description, even though these terms lack the specificity of more specific color names or the rigor of quantitative color measures.

granularity refers to the level of detail used in the definition of an ontological term. with regard to visually observed phenotypes, the terms present in existing ontologies have an insufficient amount of detail to provide precise phenotypic descriptions. for example, the description of a necrotic lesion can be coarsely described in terms of color as "brown". also, leaf color, as used in pato, is considered coarse because no guidelines exist for differentiating subtle differences in color, which may be biologically significant, e.g. "green", "light green", and "lemon green".

heterogeneity refers to the situation where the same visually observed phenotype is described with different terms. for example, "yellowish" and "yellow" may be used to describe the same color. a less obvious form of heterogeneity arises when individual image curators have different representations of the same semantics based on their level of expertise and individual specialties  <cit> ; a "green" leaf for one expert may be "light green" for another, or one curator might describe viviparous kernels as "lemon yellow" while another might use "light yellow."

it is not difficult to see the complexity of describing visually observed phenotypes when trying to minimize these semantic issues. therefore, an ontological framework has been developed for standardizing visual phenotypic information and making that information readily exchangeable within the plant science community. our proposed framework, the computable visually observed phenotype ontological framework  for plants, not only attempts to solve the abovementioned problems, but also aims to facilitate automatic phenotype image annotation and to improve the precision and accuracy of visually observed phenotype searches. the purpose of this framework is not to develop a complete ontology for any species; instead, the goal is to provide a skeleton for the plant community to follow when defining their traits of interest. two real-world case studies are included to demonstrate the use of the framework.

methods
quantitative view of descriptions of visually observed phenotypes
in this section, we introduce textual annotations of visually observed phenotypes using examples from zea mays  for two purposes. first, we would like to identify the prevailing characteristics described in these annotations to ensure that these are modeled by our framework. second, and more importantly, we would like to demonstrate the capability and necessity of using computational algorithms for accurate, objective, and comprehensive measurement of these characteristics.

the sample annotations for maize are taken from two sources:  neuffer's mutants of maize  <cit> , which provides descriptions of several lesion mimic mutants, and  descriptions of southern leaf blight  infections, based on the scoring rubric mentioned in  <cit> . though these sample annotations clearly do not cover every aspect of every known plant phenotype, they do illustrate how the framework can be applied and imply how it can be extended to cover other visually observed phenotypes in these and other plant taxa.

to reduce semantic ambiguity and heterogeneity and achieve finer granularity, very specific definitions for ontology terms need to be provided. though in some cases, devices or gauges are available that can make these specific definitions, most semantic concepts related to visually observed phenotypes are either difficult or tedious to manually quantify  or easy to measure but difficult to partition measurements into semantic terms. for example, plant height is an easy trait to measure; however, partitioning the continuum of heights for a specific kind of plant into terms like "dwarf", "short", "average", "tall" can be more difficult, especially in cases where there are no clear boundaries. how does one decide the quantitative boundary between these semantic terms?

our approach, which utilizes a training set of phenotype imagery annotated with semantic labels in conjunction with data mining techniques, facilitates both the determination of very specific definitions of these vague semantic concepts and, in doing so, allows the computer to learn the quantitative boundary between concepts within the same semantic class. imagery was chosen because many of the relevant semantic terms, which may or may not be included in existing bio-ontologies, can be accurately quantified using computer vision and image processing  algorithms. in order to show the utility of computational algorithms for measuring a variety of semantic concepts, we provide below sample measurements for each characteristic discussed to illustrate the capability and necessity of cv/ip algorithms as part of the ontological framework. this work is not intended to introduce novel or improved algorithms for phenotype quantification, but rather to illustrate the power of computational methods for quantifying phenotypic appearances and to use the measurements from these methods to better define semantic concepts.

maize is used as the primary prototype for the development and demonstration of this framework. after studying and categorizing descriptions of maize lesion mimic mutants and corn plants afflicted with slb, the prevailing characteristics in these descriptions were determined. these characteristics include color, size, shape, frequency/distribution, and spatial/anatomic relationships. we discuss the importance of each of these characteristics with respect to our example maize phenotypes below. figures  <dig> shows a summary of the various maize phenotypes and some example measurements that can be captured by image processing algorithms.

color
color is very important in the characterization of many plant traits , the analysis of several diseases, as well as the description of mutants. for maize lesion mimic mutants , color can be used to determine whether a lesion is necrotic  or chlorotic  and, more importantly, can be used to differentiate mutants, since different mutants produce different colored lesions. furthermore, color can play an important role in separating normal tissue from abnormal tissue, as evidenced in the slb scoring annotations. as an illustration, consider the first row of figure  <dig>  which shows a leaf with a light slb infection, a leaf from one of the les mutants, and a leaf with a heavy slb infection consisting of predominantly green, yellow, and brown tissues, respectively.

color specification and quantification are both plagued by semantic ambiguity and heterogeneity. our framework minimizes these semantic issues by precisely describing color using quantitative color profiles rather than semantic terms. for example, consider the different leaves  in the top row of figure  <dig>  the color variations have been captured computationally using a normalized hue histogram  <cit>  ) consisting of  <dig> equi-width bins. we decided to use the hue saturation value  color space as it more closely represents the way humans perceive color than the traditional red green blue  color space. the open-source cv/ip library opencv  <cit>  was utilized to determine hue values from the images. in this chart, the horizontal axis corresponds to  <dig> discrete ranges of the hue component, and the vertical axis shows the normalized frequency of each hue range. h <dig> represents the lowest values of hue, while h <dig> represents the highest values. the hue histogram shows that the green leaf  has significantly higher frequencies in low hue ranges , with a peak in h <dig> resulting from the brown lesion expression on the leaf. the yellow leaf  has values in the middle hue ranges , and the brown leaf  in the high hue values . this type of cv/ip algorithm can be used to quantify color in any phenotype.

shape
shape is an important characteristic that is used to categorize various plant-related structures. this may include the shape of more obvious structures like leaves and fruit, but can also refer to the shape of substructures, e.g. any lesions on the maize leaf surface. the semantics used to describe shape also suffer from semantic vagueness. consider again the set of maize les mutants. the shapes of the lesions in the mutant descriptions in  are described as round, elliptical, and irregular, respectively, in  <cit> . to precisely describe these shapes, one of many shape algorithms can be applied. as a simple example, one could use roundness to help define the shape of a lesion , which is defined as the ratio of the area of a lesion to the area of the circle with diameter d. the closer the ratio is to one, the closer the shape of the lesion approaches a perfect circle.  shows the roundness histogram, also partitioned into  <dig> equi-width bins, for the les mutants in the second row. roundness bins r0-r <dig> represent different ranges of roundness from low to high. the leaf with mostly small, round shapes  has higher values in the high-roundness bins  than the other two leaves. however, this leaf  also has some less round lesions, which produce signals in the low-roundness ranges . the leaf with fewer round lesions  has significantly lower values in the high-roundness partitions  and higher values in the low-roundness partitions  than leaf . the leaf with many irregular shapes  has the highest values in r0-r <dig>  these profiles can be used as an aggregate measure of lesion shape on a leaf.

size
the concept of size is well recognized and established in the existing ontologies and is used in various phenotype descriptions. for maize les mutants and slb infections, the sizes of lesions are typically described as being small, medium, or large. for example, a les <dig> mutant is more likely to have large necrotic spots on its leaves, while les <dig> mutants typically have small necrotic spots. again, computational methods can be used to minimize semantic vagueness by accurately capturing and representing size terms. to quantify a les mutant with respect to size, one would like to measure and record the sizes of all the lesions on a leaf and describe the phenotype as a distribution or aggregation of individual lesion sizes.  shows, from left to right, maize leaves with small, medium, and large lesions as well as a chart  showing the differences in the lesion size for each of these leaves, with z <dig> representing the smallest-sized lesions and z <dig> the largest-sized lesions. each leaf shows higher signals in the area of the histogram corresponding to the size of lesions it contains. it should be noted that the visualized differences in the profile of lesion sizes imply boundaries between the concepts in this semantic class using this computational measure.

frequency and distribution
current ontologies like to and pato contain some terms related to the frequencies and distributions of various characteristics of plant structures. in addition to these, terms related to the frequency of lesion expression  and the distribution of objects  are necessary to fully describe complex visual traits. in  <cit> , mutants are often defined by the frequency or distribution of lesions: "les <dig> plants are like les <dig> but lesions are more frequent", or "les <dig> plants have evenly distributed spots on the leaf blade"  <cit> .

because of the subjectivity in human perception, these characteristics are particularly difficult to manually measure and are especially suited for quantification via computational approaches. a simple approach to represent frequency and distribution information is a histogram of distances from a lesion to its nearest neighbor .  show three les mutants with very different frequencies and distributions of lesions, along with the corresponding nn distance histograms . each partition of the histogram represents a range of nn distances with d <dig> containing the shortest distances and d <dig> the largest distances. histograms with high values in the shorter distance bins  are said to have dense expression, whereas sparse expression is represented by higher values in the larger distance bins . if a leaf has many lesions, the area under the corresponding plot is larger than the plot of a leaf with fewer lesions. using measurements such as these, distribution characteristics can be more precisely quantified.

spatial relationship
spatial relationships can also be important when describing visual phenotypes and can be expressed in two ways:  relative to other objects  or  relative to a particular plant structure or landmark . pato contains descriptions of spatial terms from the first category, under the accession pato:0001631; terms from the second category could be obtained by pre-composing  <cit>  terms from the plant structure ontology  and the spatial  ontology from the obo foundry  <cit> .

to illustrate how algorithms can be used with these types of terms, consider , which contains three leaves whose phenotypes are described with spatial relationships:  a leaf with moderate expression on the third of the leaf closest to the stem, no expression on the middle third of the leaf, and some expression on the third of the leaf closest to the tip,  a leaf that has small lesions throughout the leaf with particularly high expression on the fourth of the leaf closest to the tip, and  a leaf with quite uniform expression on the entire length of the leaf. by dividing the leaf into different partitions, we can more precisely quantify some of the different spatial characteristics found in these leaves. for example,  contains a histogram of the number of lesions in each partition. note that the image characteristics described above can be clearly seen in this plot.

the computable visually observed phenotype ontological framework
in order to formalize the modeling, representation, and quantification of high-level semantic concepts used in phenotype descriptions, the computable visually observed phenotype ontology framework  for plants has been designed. this framework maintains interoperability with existing bio-ontologies by interfacing species-specific characteristics, anatomical structures, phenotypes, and trait concepts with the bio-ontologies po, to, and pato. because the qualitative terms of the current bio-ontologies suffer from ambiguity, heterogeneity, and granularity problems, cvopof links them to low-level quantitative measurements in a computational way, by evaluating imagery content of visually observed phenotypes. cvopof increases the utility of bio-ontologies, especially in terms of information retrieval . as indicated in figure  <dig>  the framework is divided into four components: a visual phenotype ontology , an ontology for imagery and computer algorithms, an annotated computational pipeline, and a semantic mapping interface to link the ontologies together.

visual phenotype ontology
in this first portion of the cvopof framework, we introduce the structure for a new ontology skeleton called vphenoo. this aspect of the framework is constructed to relate and organize the terms used in phenotype descriptions and annotations. this is accomplished by leveraging well-established bio-ontologies and including additional terms where necessary. a vphenoo can be divided into the following five layers, which define the domain for the ontology, the first four of which are formed by interfacing to other established ontologies/terminologies:

• the plant taxon layer is linked to the plant taxonomy classification http://www.ncbi.nlm.nih.gov/taxonomy/taxonomyhome.html/.

• the plant structure layer is linked to plant structure identifiers in the plant ontology http://www.plantontology.org. this connects to the plant taxon layer using "part of" relationships.

• the plant abnormality layer is linked to corresponding species-specific information such as disease, mutation, etc. maize lesion mimic mutants , for example, represent a class of mutations. the ontological relationship used to connect a plant abnormality to the plant structure layer is an "expressed on" relationship.

• the phenotype expression layer contains semantic concepts found in phenotype descriptions, many of which can be linked to to and pato, which identify quality bearing traits and qualitative values for those traits http://www.gramene.org/plant_ontology/; http://bioontology.org/wiki/index.php/pato:main_page. an "expressed by" relationship is used to join this layer to the plant abnormality layer.

• the phenotype quantification layer contains more precise quantitative values for the semantics in the layer above. this layer, which uses a "describes" relationship to connect to the phenotype expression layer, will be discussed in more detail after the semantic mapping interface.

in figure  <dig>  the conceptual schema for the first four layers of a sample vphenoo for lesion mimic mutants in maize are shown. the included semantics in this figure are not meant to represent the complete set of relevant terms, but rather just a sample of a vphenoo's structure.

ontology of imagery and computer algorithms
since imagery is used in cvopof to computationally determine the correspondence between semantic concepts and quantification of these concepts, an ontology to organize the information related to imagery and computer algorithms used to process imagery was included in the framework. the rationale is twofold. first, it provides a listing of the types of measurements that can be made from image content using computer algorithms. second, by providing this listing, the ontology helps a user group construct an imaging pipeline  for standardized imaging and processing of phenotype images. this also ensures that measurements taken from these images are comparable, which is a prerequisite for utilization of our semantic mapping module.

the semantics in an ontology of imagery and algorithms can be divided into three layers, as depicted in figure  <dig>  the imagery layer is the top layer  and contains semantics related to imagery. images may be categorized by their adherence to certain imaging protocols. these protocols facilitate the use of certain normalization algorithms to transform them to baselines where the images become comparable. an imaging protocol may correspond to the inclusion of a color or size standard placed in the field of view or the use of a certain background type or color when imaging a phenotype. images may also be classified by whether they exist on their own or as part of a series of images to capture a temporal phenotype. semantics also exist to describe image transformations that occur during image preprocessing, which may include segmentation or isolation of important aspects of the image. the middle layer in this ontology, the algorithms layer, contains algorithms that produce measurements  from imagery. as discussed previously, these features are the low-level quantitative components that are crucial for resolving semantic heterogeneity, facilitating more advanced phenotype searches, and future development of automatic annotation utilities. the algorithms in this layer are classified by the type of information they extract and also by the number of images that are used to produce the information. finally, the lowest layer is the machine representation layer, which describes any post-processing transformations done on the output of the algorithms from the layer above, specifically normalization. an example ontology corresponding to the images and algorithms used during the processing of the maize leaf images is shown in figure  <dig> 

it should be noted that the values of any input parameters for these algorithms are not included in the ontology, as that would add unnecessary complications. since the values of these parameters determine the quality of the output features, the appropriate parameter values will need to be determined and annotated to a computational pipeline.

computational pipeline
the consistent and accurate quantification of classes of phenotypes from imagery necessitates the construction of a standard computational pipeline that determines an imaging standard for capture of these phenotypes as well as the set of algorithms used to produce the required features for representing specific phenotypic traits. the benefit of such a pipeline is that all phenotype images adhering to the standard and processed using the pipeline will be comparable in terms of high-level semantic descriptions as well as low-level computational features. this is an essential aspect of the semantic mapping procedure discussed in the next section, as images are not computationally comparable if  they cannot be transformed to a common baseline, i.e. if they do not adhere to some imaging standard, or  there is a not a common set of features collected for the entire set. the pipeline will guide image processing from an un-standardized, raw image through pre-processing, feature extraction, and post-processing , all of which are covered components of an ontology of imagery and algorithms. the pipeline can be annotated with the described ontology and extended so that the appropriate input parameters for each algorithm are stored.

as a concrete example, consider the annotated computational pipeline in figure  <dig> constructed for capturing and processing maize leaf phenotypes. the input and output to each step in the processing pipeline correspond to a value in the imagery and algorithms ontology, and these terms have been supplemented with parameter information to provide additional algorithmic detail.

semantic mapping
the most critical part of the framework is the semantic mapping interface, which is the computational module that utilizes a data mining approach to automatically learn the quantitative boundaries between semantic concepts. input to this module consists of a set of phenotype images that have been  processed using the computational pipeline to measure the desired phenotypic characteristics  and  manually annotated  with the semantic concepts of interest. for example, table  <dig> shows selected features extracted for the anna russian tomato variety  using tomato analyzer. once all this information for all the tomato fruits for which we have measurements and semantic labels has been amassed, it is passed to this module for processing.

this tomato variety is labeled by sgn as being "medium" in size with an "oxheart" shape. the features were extracted using the tomato analyzer software.

our semantic mapping module is a knowledge discovery process that determines complex and flexible association rules among a sufficiently large training set of visual semantics and machine-readable features from plant images. our approach emphasizes the use of flexible semantics to address the heterogeneous semantic assignments that are inherent in the descriptions of plant phenotypes. a key to the success of the predictive power of our algorithm is to identify a training dataset that would return accurate models while being minimal in size to reduce the burden on those imaging and manually annotating the phenotypic appearances. while some approaches suggest large amounts of training data  <cit>  to compensate for poor training data choice, other research  <cit>  shows that prediction gain decreases with adding more data to the training and eventually reaches a plateau where adding more training data does not bring any improvements in performance. in our approach we capitalize on the tradeoff between size and quality of training data. we choose the training data to contain representative examples from all semantic assignments.

we model the mapping between semantic and machine-readable features by creating a set of association rules  <cit>  via data mining. the association rules in our model are mined using the total-from-partial tree structure  <cit>  over the entire image database with semantic assignment for phenotypic expressions. each association rule r  has a set of feature value ranges as its antecedent  and a semantic as its consequent . an example rule is {f <dig> ∈  ʌ f <dig> ∈ }→ ellipticallesion. this rule maps the semantic "elliptical lesions" into a two-dimensional feature subspace of low-level features f <dig> and f <dig> . a new image is considered relevant to the semantic "elliptical lesion" if its feature measurements fall in the specified range. to make the rules more flexible, after the association rules are discovered, the antecedents are refined through fuzzification; the crisp feature intervals in the antecedents are replaced with possibility distributions that model the relevance of the individual feature subspace. the following equation is the asymmetric possibility function used to model the semantic assignment for a feature subspace ϑ of a feature f. the shape of this function is controlled by three parameters: center - λ <dig>  width - λ <dig>  and slope - λ <dig>   

all color features  were generated using equi-width binning, and all lesion features  distance) were generated using equi-area binning. the numbers in parenthesis in the number of features column show the total number of features as well as the number per channel for the rgb and hsv histograms.

the rules from the semantic mapping module become the concepts in the fifth and final layer of a vphenoo, the plant quantification layer. figure  <dig> shows three rules that were derived from the semantic mapping module for our maize leaf image collection  in addition to the encoding of these rules into the sample vphenoo. the linkage of the semantic categories to these rules provides the concrete tie between the measurements extracted from the phenotype image content and the meaning of the semantic concepts. representing this layer in this way has the added benefit of providing implicit linkages between the plant quantification layer and the computational pipeline, as each feature represented in a rule's antecedent is linked to the algorithm that generated that feature.

RESULTS
maize lesion mimic mutant phenotypes
the framework was first applied to leaf phenotypes in zea mays. the dataset consists of  <dig> leaf images covering  <dig> different lesion mimic mutants taken from a genetics maize field at the university of missouri. three different curators assigned semantic labels to each image, and consensus was used to determine the final labels. semantic categories assigned to the images included terms related to lesion coverage, lesion size , lesion shape , and lesion color . each semantic label assigned to each image was coded by the curator in terms of degree of appearance, as either "none", "few", "moderate", or "extensive." as an example, figure  <dig> shows the consensus labeling for one of the  <dig> images.

in addition to semantic labeling, each of the maize leaf images was also processed using the computational pipeline in figure  <dig>  this pipeline consists of a series of c++ computer programs developed in-house for measuring various characteristics of lesion mimic mutant phenotypes. these programs utilize function calls from the open source cv/ip library opencv. as a result of this pipeline, a total of  <dig> features was obtained for each image. table  <dig> describes the entire set of features.

the semantic labels and image features were passed to the semantic mapping module. to avoid problems stemming from the curse of dimensionality, this module automatically performs feature selection  to statistically determine which measurements are best able to distinguish individual semantics; those features that are not helpful or that are highly correlated with other useful features are excluded before mapping begins. semantic mapping was performed individually on each class of semantics . multiple rules were generated for each semantic term, and a summary of the outputted rules are shown in table  <dig>  the full set of generated rules from this dataset can be found in additional file  <dig> 

we also conducted experiments on the generated rules to evaluate how well the rules were able to predict the chosen semantic labels. to determine the quality of the generated rules, a resubstitution experiment was carried out. the mean average precision  for each of the semantic categories as well as the average map for each semantic class are shown in table  <dig>  precision-recall curves were also generated . inspection of these results shows highly accurate results for lesion coverage and lesion color with decreasing quality of results for lesion shape and lesion size. the reason for the decline of results in these latter two semantic classes becomes obvious when one looks more closely at lesion segmentation in many of these images. though human perception is able to project boundaries between individual lesions in images like figure  <dig>  upon closer inspection of the image  many of these so-called individual lesions are in fact touching and should be treated as a "single" larger lesion. while this visual illusion will not have an effect on the lesion coverage or color semantics, it will definitely have an effect on lesion sizes and shapes.

tomato fruit phenotypes
in addition to zea mays, the framework was applied to some fruit phenotypes in a second species, solanum lycoperiscum. this dataset consisted of  <dig> tomato fruit images, each of which contained  <dig> to  <dig> individual fruits, from  <dig> different tomato varieties. table  <dig> provides the number of fruits from each tomato variety. these images in addition to the semantic labels associated with the different tomato varieties were obtained from sgn. the semantic labels indicated the shape and the size of the tomato fruit.

in this case study, instead of developing a brand new set of algorithms, as was required for the maize case study, an available tool from the plant community, the tomato analyzer  <cit> , was used for processing the images. a total of  <dig> features was extracted from each tomato fruit using this software, and table  <dig> provides a listing of those features. the semantic module was again applied to these semantic labels and image features. a summary of the generated rules is provided for this dataset in table  <dig>  and the complete details about all the mined rules can be found in additional file  <dig> 

the same resubstitution experiment was conducted on this dataset to once again verify the quality of the image features and generated rules. the map values  and precision-recall plots  again show high quality results for all semantic categories. it is noteworthy to compare the map results between maize and tomato. the rules for the tomato fruit have higher precision results than those rules for maize leaves. this is because, in the current collection, the maize leaf phenotype expression is more complicated, in terms of appearance, measurement, and semantic labeling, than the tomato fruit phenotype expression. this is evidenced by the increased number of rules linked in the ontology for maize leaves.

merged vpheno ontology for multiple species
a vpheno ontology containing semantics from maize leaf lesion phenotypes and tomato fruit phenotypes was constructed from the two case studies presented above. this ontology contains semantics in all five of the described layers in a vphenoo, from taxonomic classification to semantics describing phenotypic appearance to automatically determined association rules linking semantic concepts to measurements of these phenotypes from imagery. the maize portion of the ontology contains  <dig> nodes -  <dig> of which are high-level semantics with  <dig> corresponding to the generated semantic rules for the maize leaf. the remaining part of the ontology is related to the tomato fruit phenotypes and consists of  <dig> nodes -  <dig> high-level semantics and  <dig> generated rules. this ontology is available, in web ontology language  format, in additional file  <dig> 

these case studies explicitly demonstrate how cvopof utilizes a variety of established ontologies including po, to, and pato, and also the framework's flexibility in being able to accommodate both customized algorithms as well as publically available computational tools .

discussion
applications
the semantic mapping module provides the groundwork for two advanced phenotype applications: retrieval and annotation. combining a vphenoo with content-based image retrieval  makes efficient querying of visually similar phenotypes from semantics tractable  <cit> . semantic searches use canonical semantics and/or phenotype annotations as queries. an example system, illustrated in figure  <dig>  shows a semantic search for maize lesion mimics. in this example, a user selects a combination of two semantic terms  to search for relevant phenotype images from the database. for each semantic term, a list of the most relevant images is formed. the image lists are fused together to create a ranked list of images that best match all semantics and are returned for the user to consult.

in addition to semantic search, the semantic mapping module also facilitates an annotation utility. with this application, a user could submit an image to the system, which would generate a feature vector using the devised computational pipeline. feature values would be matched to antecedents to find applicable association rules, and then through probabilistic means, those rules would be used determine the relevance of each semantic to the image. this is also part of the application shown in figure  <dig>  for the result image shown in the middle pane, the list of semantics with non-zero relevance to this image is displayed in the bar chart to the left of the image. the height of each bar corresponds to the relevance of the corresponding semantic.

potential applications
use of the cvopof framework makes possible the development of a number of more advanced applications and utilities for analysis and annotation. first, when images have corresponding descriptions annotated using a vphenoo, cbir searches could be used to facilitate studies of the similarities and differences in semantic terms used to describe visually similar phenotypes, perhaps across anatomical structure and even taxa. in addition, one could consider using the semantic mapping module to generate association rules for each of several individual curators. with the generated rules, one could quantify and perform statistical analyses on the similarities and differences between individual curators' perceptions. once the system learned the annotation style for an individual, the system could then allow database curators to detect inconsistencies or possible errors in annotation so as to maintain high levels of repository integrity. alternatively, the rules from individual curators could be combined to form "consensus" rules, which could represent a standard for annotation of particular phenotypes.

the framework could also be used for training purposes. a trainee could use the system to become familiar with the semantics of phenotype descriptions by examining annotated descriptions in conjunction with phenotype imagery. the training should improve the consistency of newly assigned phenotype descriptions as well as human perception regarding plant phenotypes.

utilization of the framework
in order to fully benefit from this framework, a great deal of collaboration, commitment, and careful planning by the plant community are required. this includes everything from construction of the ontologies and labeling of image examples by a central administrative group to utilization of the unified information system by more peripheral end users in that plant phenotype's domain.

to start the process, an administrative group must be formed. this group should contain experts from the particular taxon/phenotype community as well as cv/ip experts. this collaboration will be vital to the success of the framework, as both groups bring expertise that is essential for correctly identifying and measuring phenotype expression. the administrative group will first identify those phenotypes of interest that can be captured and represented through imagery. the distinguishing characteristics of these phenotypes must then be determined, and, with the aid of computer vision experts, the algorithms and parameters necessary to quantify those traits need to be selected.

once these items are determined, attention can be turned to ontology creation. the top layers in the vphenoo can be constructed with either semantic terms present in existing ontologies or by new terms. though ontology construction will be tedious, the realized benefit of minimizing semantic vagueness in the descriptions should be worth the effort.

in parallel with ontology construction, a pipeline for processing the phenotype images can be built. this can be constructed and annotated using terms from an ontology of imagery and algorithms. the pipeline will ensure that appropriate phenotype measurements will be made for each image submitted to the system. it may also ensure that an image adheres to the defined imaging protocol.

following these steps, a set of training image examples will need to be constructed and annotated. multiple example images representing each of the various concepts in the vpheno ontology should be collected and annotated. these training images should also be processed using the pipeline to obtain phenotype measurements from the image content itself.

after these training data are compiled, the semantic mapping module can be executed to determine the correspondences between high-level semantics  and low-level features . the last step in the initial construction phase of the system is to add the generated rules to the vpheno ontology.

after the administrative group has created a vphenoo and computational pipeline and after the semantic mapping module has been trained with the training data, users can begin to take full advantage of the ontological framework. they may submit un-annotated images of their phenotypes to the system. phenotype measurements will be obtained from the pipeline appropriate to the type of image submitted, and these will be used to automatically generate a list of semantic terms relevant to the image. the list of terms can be reviewed by curators and edited appropriately.

the initial construction phase is only the beginning, as the system is intended to evolve. a number of events will likely occur that the system must be able to accommodate. for example, what will be the process when users request more semantic terms to be added to the ontology? how does the system handle additional training data provided by users who collect, manually annotate, and wish to submit additional phenotype data to the system? if there is additional data for training the semantic mapping module, when is retraining performed? computer scientists will undoubtedly continue to develop algorithms for measuring various phenotypic characteristics or they may make improvements to existing algorithms. how will the system accommodate inclusion of these algorithms into the processing pipelines?

the most complicated task is the addition of a new semantic concept. after a request for a new semantic concept has been approved by the central administrative group, the first step will be to include that semantic concept into the vphenoo. this may simply be adding a related semantic in an existing part of the ontology or it may require an extensive addition to the ontology . after this step, the administrative group will need to evaluate whether the semantic can be measured from images in the training data and using algorithms in the pipeline. if the answer is no to either question, the group will need to define the imaging protocol, select cv/ip algorithms, create  the existing computational pipeline, and make any necessary modifications to the ontology of imagery and algorithms. once features are decided upon, a sufficient training set will need to be constructed. it is quite difficult to provide specifics on the exact size of a sufficient training set, as it is dependent on several factors including the quality of the image labels and the ease with which the semantic labels in the class can be separated quantitatively. with a good training set, semantic mapping can be performed. the map scores and precision-recall curves can be examined to determine the quality of the outputted rules. if the rules show unsatisfactory performance, adjustments to the training set  can be made. if, on the other hand, the rules are of adequate quality, they can be added to the vpheno ontology in the plant quantification layer.

users may also submit new training data to the system. approval should be received on the quality of the submitted images and the manual annotations. afterwards, the image can be added to the training set database and processed by the appropriate pipeline. using the features extracted from the image, the current set of semantic rules can be applied to the image, which will generate a list of relevant semantic terms. these terms can be compared to the manual annotations, and statistics maintained on the quality of the current rule set can be updated based on this assessment.

retraining of the semantic mapping module can be initiated in one of two ways. first, if the updated statistics from the added data cause the rule quality to fall below a specified threshold, this could be used to initiate a retraining of the system. alternatively, an assessment of rule quality could be performed on a regular basis. if the size of the new data is large enough, retraining of the associated rules could be performed. after retraining is accomplished, the updated rules could be included in the vpheno ontology. the rules that are being replaced could be deleted, deprecated, or made obsolete. deprecating the old rules would have the benefit of allowing an analysis of the changes in the rules.

while the most extensive and comprehensive benefits from the framework are achievable through widespread collaboration by plant communities, adoption of the framework by smaller cohorts of researchers or even individual research groups can still provide the noted advantages, though on a smaller scale. in this case, the interoperability of phenotypic information modeled by the framework would be limited to only those groups utilizing the framework, which would notably include utilization of the same imaging protocol for phenotype capture. it is noteworthy to mention, however, that any automatic annotations generated through the use of the framework, though not directly comparable, could still be of use to the rest of the plant community.

framework limitations
though there are many benefits for using the cvopof for organization, quantification, and annotation of plant phenotypes, the framework is not without limitations. first, the framework requires a means to make direct or indirect measurements of relevant phenotype characteristics, specifically through phenotype images . the phenotype images must follow a defined imaging protocol so as to facilitate processing by computer algorithms, and computer algorithms need to exist that can measure the semantics of interest. second, the semantic mapping module requires sufficient training examples for each semantic concept it is to be trained on. though it is difficult to specify precisely how many training images are needed per semantic concept, the general rule of thumb is the more training examples the better. too few training examples can result in no outputted rules from the semantic mapping module, or at the very least rules of poor quality. the map scores and precision-recall curves reported above provide evidence of the quality of the generated rules. implicit in a sufficiently large training set of images is the requirement for semantic labels for each of the submitted phenotype images. this could be problematic as semantic labeling can be a very laborious task. it can also be difficult for humans to label images in an objective and consistent manner. finally, a potentially major limitation of the framework is that optimal use of the framework will require extensive time, effort, and resources from the plant community to decide upon the species and phenotypes to image, to obtain high-quality phenotype images, to find or develop robust algorithms for measuring phenotypic appearances, to provide consistent and objective labels, and to train and retrain the system as more training data, computer algorithms, and semantic concepts are included. there will also need to be an understanding that this system's accuracy will evolve over time. despite these limitations, high throughput phenotyping, analysis, and annotation are expected to be critical to ensure rigorous scientific discovery in plant genomic research.

CONCLUSIONS
we have proposed a new ontological framework for visual phenotype semantics that leverages several existing ontologies  and expands them, through phenotype imagery and computational processing, to include a robust low-level computational level that facilitates the linkages of high-level semantic concepts found in qualitative observations to quantitative measurements of these concepts from phenotype images. the computational processing aspect of the framework is highly flexible in that it can utilize measurements from existing publically available computational tools  or from customized algorithms . the linkages are computed by a semantic mapping module that utilizes data mining techniques. the by-product of this framework is the ability to more precisely define phenotypic conceptual semantics in individual plant domains, which leads to a reduction in semantic ambiguity and heterogeneity and improvement in semantic granularity.

this framework facilitates the development of a number of phenotype-related applications. next generation information retrieval tools like semantic search in addition to automatic semantic extraction from phenotype images have already been demonstrated. with integration of multiple plant structures and species, the framework also has the potential to facilitate phenotype retrievals across plant structure and species, though more investigation is required. in addition, advanced and comprehensive phenotype annotation analysis could be performed by applying various data mining and knowledge discovery tools to a repository of images annotated using the framework.

competing interests
the authors declare that they have no competing interests.

authors' contributions
the ontological framework was conceived and developed by jh, who also prepared the manuscript. jg contributed to the development of framework and also worked extensively on manuscript preparation. ab contributed the implementation and writing for the semantic mapping module. ms provided expertise from the plant science perspective and provided guidance during manuscript preparation. lv was our resident ontology expert whose insight was critical during framework development and manuscript preparation. crs directed the project and provided essential guidance during development of the framework as well as during preparation of the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
this file contains two excel sheets. the first contains a listing of generated rules for the maize leaf and tomato fruit phenotypes. the second sheet has detailed information about the antecedents of each rule. each antecedent is connected to its rule by the value in the "ruleid" column.

click here for file

 additional file 2
this file contains the complete sample ontologies in the common format  for our constructed vpheno ontology for maize lesion mimic mutants and our ontology of imagery and computer algorithms. it can be loaded into protégé for viewing.

click here for file

 acknowledgements
the authors would like to thank peter balint-kurti for slb images and scoring descriptions, toni kazic for les mimic mutant images, sgn for making tomato fruit images and semantics available, van der knaap's lab at ohio state for making tomato analyzer publically available, and jing han for feature extraction implementation. the project was supported by nsf grant #dbi- <dig>  and jg is supported by nlm grant #2t15lm007089- <dig> 
