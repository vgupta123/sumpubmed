BACKGROUND
in recent years progress has been made in protein structure prediction by incorporating information on local protein structure. david baker's group has successfully used local fragment predictions  <cit>  in conjunction with a fragment assembly procedure to substantially improve new fold predictions  <cit> . also for fold recognition and remote homology detection methods the integration of local fragment predictions led to improved results  <cit> .

methods for analyzing fragments focus on sequence or structure or both. we are looking for fragments that occur in several proteins, that are sufficiently similar in structure, and that exhibit enough sequence similarity to be detectable by discriminative methods.

specifically, we address the question: given a local sequence fragment, how much can we learn about the local structure it adopts? it is expected that, in many cases, knowledge of the sequence of the fragment is not enough to determine its structure. often long-range interactions with parts of the amino acid chain that are far away in sequence but close in space can be determinants for local structure. however, in other cases the local sequence properties give rise to a single or just a few local structures. knowledge of these conserved cases enables the prediction of local structure in the respective regions of the protein.

in the following we briefly review two approaches to analyzing protein fragments  and compare them with our own method.

baker's approach  <cit>  starts by clustering fragments based on sequence similarity. in the second step for each of the clusters the structural variation of the fragments is examined. clusters that vary too much are discarded. the remaining clusters represent sequence neighborhoods that adopt only one or few local structures .

hunter's approach  <cit>  clusters fragments based on structural similarity in order to define  <dig> canonical local structures. these are supposed to roughly model local structural variation. after tabulating the corresponding sequences of each of the classes and calculating position-specific amino-acid probabilities, a simple prediction algorithm  is used to predict the canonical fragment structure based on sequence information .

our approach  combines the advantages of both ideas. baker's algorithm partitions sequence space regardless of the corresponding structures. as sequences vary to a greater extent than structures, incorporating structural information while partitioning sequence space is crucial. this idea can be realized using a classifier, which partitions sequence space into decision regions, such that inference of local structure is optimized. in place of hunter's simple naive bayes approach, we suggest a more flexible classifier like a support vector machine or a random forests to map from local sequence to structure. here, the question of the partitioning granularity of sequence space amounts to the classifier regularization problem, which can be addressed by commonplace validation methods. moreover, our method of defining recurrent local structures is different from hunter's approach and exhibits improved performance regarding hunter's quality criteria.

early work on local structural fragments was done by rooman et al.  <cit> , who use a hierarchical procedure to cluster 7-residue fragments into a coarse grouping , which was shown to be similar to dssp classes. fetrow  <cit>  uses a neural net to produce a low-dimensional feature representation for a distance- and angle-based description of local structure. using k-means, this reduced feature representation is clustered into  <dig> groups, which show significant amino acid sequence patterns. camproux et al.  <cit>  use the hidden states in an hmm to derive a 12-letter alphabet of fragments which considers the distances within and the volume spanned by the fragment as well as consecutiveness of these structural building blocks. de brevern et al.  <cit>  propose a 16-letter alphabet generated by a self-organzing map based on a dihedral angle similarity measure. similarly to camproux's work, the chaining of consecutive fragments is considered. using a bayesian approach local structure is predicted based on local sequence . recently the predictive performance on this alphabet was improved by etchebest et al.  <cit> . a comprehensive evaluation of these and other structural alphabets was done by karchin et al.  <cit> .

RESULTS
to validate our methods we examine the quality of the structural clustering procedure, the accuracy of the classification into the resulting structural classes and the accuracy of the probability estimates in the classification step.

local structure clustering
we chose cα distance matrix comparison for clustering structure fragments, as it provides a vector space representation for structural similarity and is thus applicable to large data sets . in order to estimate the suitability of this representation, we examined its correlation with the widely used root mean square deviation . scatter plots depicting strongly positive correlation and a pearson correlation coefficient of  <dig>  indicate that distance matrix comparison is a reasonable representation of structural similarity. although there are refined versions of distance matrix comparison, for instance by downweighting large distances, here we used the basic version.

a tradeoff exists between accentuating a natural clustering tendency, the number of clusters , and the complexity of the subsequent classification task . finally we decided on a threshold based on the following considerations:  there is no clear natural clustering as shown in the next section.  posing a reasonable task for the subsequent classification the number of clusters should not be too large.  a comparison with the results of hunter et al.  <cit>   requires a similar setting regarding modeling quality.

clustering with a threshold of  <dig> for the cα distance matrix comparison we obtain  <dig> clusters on the whole dataset . as can be seen in figure  <dig>  the distribution over the  <dig> classes is highly skewed. one class containing about 29% of the fragments dominates the other classes which contain between 1% and 5% of the fragments. as expected, the dominant class contains alpha helical fragments. beta strand fragments are distributed among several structural classes. their geometrical variation is almost as high as in loop regions, as is to be expected  <cit> .

the structural clustering in  <cit>  results in two dominant classes, one for alpha, and one for beta fragments. in terms of quantization error our clustering is superior, indicating that a subpartitioning of beta fragments is advisable. however, because of this different class balancing the classification results are not comparable. with two dominant clusters for alpha and beta fragments, secondary structure prediction would suffice to produce high classification accuracy.

quality of the structural clustering
the validation of clustering results aims at quantifying to which extent the clustering resembles the natural grouping of data points. many clustering algorithms require the specification of parameters, e.g. the number of clusters for k-means or a linkage threshold for hierarchical agglomerative clustering algorithms. quality measures are used to adjust these parameters in order to cluster the data points appropriately.

in our clustering procedure we specify a threshold which controls the binning of fragments in the leader clustering algorithm. this parameter indirectly controls the number of clusters for the subsequent k-means refinement.

as a first quality criterion of the clustering we used the mean of the averaged pairwise within-cluster distances. the result of the leader algorithm depends on the input order of the data, thus we repeated the clustering procedure ten times with shuffled input. the resulting curves are smooth, exhibiting no natural cutoff . this resembles the observations of hunter and subramaniam  <cit> .

as a second quality criterion of the clustering process we used the quantization error in terms of rmsd. clustering can be regarded as a data grouping process by which we represent whole groups by single representatives . as representative of a structural cluster, we choose the fragment with the lowest sum of distances to all other fragments in the respective cluster.

the quantization error is defined as the average rmsd distance of all fragments in the data set to their respective representatives. for two reasons we use the root mean square deviation to compute the quantization error. first, the rmsd is easier to interpret as it is commonly used to compare protein structures. second, this validation shows that the cα distance matrix measure, which we used for efficient clustering, is able to find structure representatives that are good in terms of rmsd.

for  <dig> clusters, our clustering results in a quantization error of  <dig>  Å. hunter and subramaniam specify the quantization error of their method for  <dig> clusters as  <dig>  Å  <cit> . the difference between these two figures is an indication for the superiority of the structural representatives produced by our clustering method.

in the supplementary material  we provide the position-specific propensities for the types of secondary structure . alpha helices are clearly represented by cluster  <dig>  this includes a few fragments with transitions to coils at the ends. fragments in cluster  <dig> also have a clear helical center whereas coil structures dominate towards the ends. overall, extended β-elements and coils are rather mixed in the clustering. clusters  <dig> and  <dig> show stretches of β-strands with coils at the beginnings and ends, respectively. clusters  <dig> and  <dig> both exhibit a β tendency in the middle and more coils towards both ends.

the supplement also contains information on the structural variability in each cluster, visualized as pairwise rmsd and pairwise secondary structure dissimilarity plots. these plots show that most of the clusters could be further divided into smaller subgroups. this is consistent with the smoothness of the kneeplot in figure  <dig> 

examples of structural clusters
to give an impression of the structural clusters, we show some examples in figure  <dig>  random samples of ten fragments from clusters were taken, superpositioned by kearsley's quaternion method  <cit>  and visualized in pymol  <cit> . note that the sample is not necessarily representative. from the resulting visualizations, we chose a few that demonstrate interesting local structures like helices, beta strands, beta strands attached to turns or loop regions. whereas some of these clusters represent well defined local patterns, others are only a coarse grouping of similar structures.

classification
in this section we describe experimental results of the sequence to structure mapping by a discriminative classifier. knowing that local sequence plays only a partial role in the formation of local structure, it is obvious that the mapping is a difficult task, when totally neglecting tertiary relationships.

examining the correlation between sequence similarity  and structural similarity  for local windows of  <dig> residues in length shows negligible correlation of - <dig> ,  <dig> , and  <dig>  respectively.

all of the following observations refer to fragments with a length of  <dig> residues. the structure labels obtained from the previous step are for a fixed clustering threshold of  <dig>  which leads to  <dig> class labels.

first we show results for classification without confidence estimates, with focus on support vector machines as the predictive model. then, results for the probability estimates are shown. the respective problematic svm performance suggests a shift of focus towards random forests as classification model.

prediction accuracy
we used decision trees , support vector machines  and random forests  to predict local structure classes given sequence information. the peak accuracies for c <dig> , svms, and rfs are  <dig> ,  <dig> , and  <dig>  respectively .

as a rather simple model, a decision tree was used to estimate the difficulty of the classification problem. using c <dig>  with standard parameters and input based on physico-chemical properties yields classification accuracy of  <dig> . this stresses the inherent complexity of the mapping task.

for svms the training complexity is quite high, thus exhaustive testing of all interesting parameter combinations is not possible. we used a simple grid-search procedure on promising parameter regions. by manual intervention these regions where extended into directions, in which further improvement was expected. for the relevant svm parameters c  and γ  we decided to use the parameter ranges proposed by hsu et al.  <cit> . c ranges over { <dig>   <dig>   <dig>   <dig>   <dig>  64}, γ ranges over { <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig>  2}.

experimenting with different profile representations , the encoding of amino acid properties exhibits clear advantages. for c =  <dig> and γ =  <dig>  the prediction accuracy rises from  <dig>   to  <dig>  . the performance using only representations of single sequences is below that of using amino-acid profiles.

changing the size of the training set from  <dig>  to  <dig> , the classification accuracy rises from  <dig>  to  <dig> . however, the run time is increased significantly to approximately one day for five-fold cross validation on  <dig>  fragments .

for random forests the overall accuracy is  <dig>  using the standard setting of  <dig> trees .

accuracy of probability estimates
to achieve better confidence in the prediction step, for each classified sample a vector is returned that contains a probability for each of the classes. this probability is supposed to represent the confidence with which the sample can be assigned to the class. high probabilities represent high confidence.

to retrieve probability estimates from support vector machines predictions need to be postprocessed . predicting just the class with the highest probability, classification accuracy drops to  <dig>  . thus the postprocessing step significantly decreases predictive performance. during corresponding tests in  <cit>  no performance loss was observed. in our case, due to the imbalance in the class sizes, probability estimates are biased towards the dominant class . this results in an over-representation of the dominant class at the top-rank.

similarly to the boxplots for the class-wise probability estimates in figure  <dig>  for svms it can be observed as well that the ranges of the given estimates reflect class size . this also explains the poor performance of the svm probability estimates. if the sensitivity towards the smaller classes were higher , the specificity would decrease dramatically, as many of the samples in the dominant class would be erroneously classified into one of the smaller classes. thus by predicting conservative estimates for the small classes svms lose overall accuracy.

for random forests probabilistic outputs are a natural extension of the standard algorithm, yielding the same classification accuracy. due to the low overall accuracies, probability estimates are mandatory, thus we decided to focus on using random forests.

limiting predictions to high confidence decisions can increase classification accuracy. in figure  <dig> we plot confidence thresholds against accuracy. however, interpreting the resulting plot can be misleading. keeping the class specific ranges of confidence predictions from figure  <dig> in mind, it becomes obvious, that large parts of the plot describe predictions for the dominant class.

to level out the effect of dominant classes, we evaluate prediction accuracy conditioned on the predicted class . thus given the classifier decision for a specific class, the real class is more likely to be the predicted one than another.

the confusion matrix shows some dominant mispredictions. looking at these cases shows that the classes which are confused with each other, often exhibit similar secondary structures . for example class  <dig> which is confused with class  <dig>  both are coiled with a tendency to helix at the end. likewise, cluster  <dig> which is often predicted instead of clusters  <dig> and  <dig> show a similar combination of coil and helix. clusters  <dig>   <dig>  and  <dig> have a conserved extended center with coils towards the ends. clusters  <dig> and  <dig> are coiled with a helical tendency at the beginning and a bias towards extended structures at the end.

despite the difficulties resulting from the imbalance of class sizes, the probability estimates entail signals also for the occurrence of underrepresented classes. using them to enrich local structure candidates for fragment assembly or incorporating them into an alignment score for fold recognition can improve prediction quality. in an enrichment prediction, we allow the classifier to return several class suggestions per input sample. in figure  <dig> the classification accuracy is shown, if several class suggestions are allowed. with up to three class suggestions the prediction contains the correct class in more than 54% of the cases. in protein structure prediction based on fragment analysis this can reduce the search space significantly. if additional constraints are taken into account, e.g. smoothing over the predictions of overlapping fragments, the results can be improved further.

to evaluate the predicted probabilities of the single classes, without interfering effects of the class distribution, we generated receiver operating characteristics  plots of the results . for an introduction to roc plots see  <cit> . for classifiers with continuous output , roc graphs plot the false positive rate against the true positive rate. the false positive rate of a fixed class i is defined by fpr=false positivesnegatives
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqqggbgrcqqgqbaucqqgsbgucqgh9aqpdawcaaqaaiabbagamjabbggahjabbygasjabbohazjabbwgaljabbccagiabbchawjabb+gavjabbohazjabbmgapjabbsha0jabbmgapjabbaha2jabbwgaljabbohazbqaaiabb6gaujabbwgaljabbeganjabbggahjabbsha0jabbmgapjabbaha2jabbwgaljabbohazbaaaaa@517d@. this is the number of samples erroneously classified in class i divided by the total number of samples not belonging to class i. the true positive rate is defined by tpr=true positivespositives
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqqgubavcqqgqbaucqqgsbgucqgh9aqpdawcaaqaaiabbsha0jabbkhayjabbwha1jabbwgaljabbccagiabbchawjabb+gavjabbohazjabbmgapjabbsha0jabbmgapjabbaha2jabbwgaljabbohazbqaaiabbchawjabb+gavjabbohazjabbmgapjabbsha0jabbmgapjabbaha2jabbwgaljabbohazbaaaaa@50bc@. this is the number of samples correctly classified in class i divided by the total number of samples in class i. the most important property of these rates is their independence from the class distribution. random guessing would generate identical false positive and true positive rates, on average. therefore, the diagonal  in the roc plot is the performance of random guessing .

in figure  <dig> we compare the impact of different sequence representations on classification performance. for a single sequence representation the classification performance for each of the classes is computed as the area under the roc curve . thus, the  <dig> auc measures can be illustrated as a density of aucs. comparing the density curves shows that input representation by frequency profiles performs better than input based on sequence alone. furthermore, using amino acid property profiles slightly increases the performance compared to standard amino acid profiles.

comparability of classifier performance
comparing the performance of the classification procedure to other methods  is difficult. obstacles for an objective comparison are  different prediction protocols  and  different representation of local structure space leading to a different prior distribution of class labels.

the final prediction rate equals to 36% that is less than the 44% of hunter's method. one of the reviewers pointed out, that this seemingly lower rate is in fact better because the latter method predicts most local protein structures to be in only  <dig> of the  <dig> clusters and also tends to over-predict β-structures.

in order to compare our results to hunter we adjusted our clustering manually, grouping together previously separated extended beta structures. aside from the large α-helical class this leads to a second large class. performing the classifier analysis with this modified fragment labeling leads to an increased classification accuracy of 44%. this shows that simple performance measures like accuracy are not able to capture the classification result. ultimately the performance of local structure suggestions should be valued by their usefulness in later stages, e.g. their contribution to fragment assembly approaches.

a note on fragments of various lengths
in our experiments above we limited the fragment size to seven residues. hunter and subramaniam use fragments of length seven, as structural variation of 7-residue fragments can be modeled at accuracies below  <dig> Å using fewer than  <dig>  canonical local shapes. moreover, current structure databases do not provide enough data for accurate modeling of longer fragments  <cit> . bystroff and baker note that the correlation between sequence and structure increases as the fragment length increases from three to eight, but slowly decreases for longer fragments  <cit> .

applying our method to fragments longer than  <dig> residues fails due to the strong increase in structural variation. the clustering procedure that we used requires a structure similarity threshold, specifying a limit of tolerated dissimilarity within a cluster. after setting these thresholds for longer fragments based on visual inspection of fragment pairs, the number of obtained clusters grew dramatically. many of the clusters contain just one or a few fragments, which have no close neighbors in structure space. the complexity of the leader clustering algorithm is linear in the number of clusters and in the number of data points. however, if the number of clusters grows on the order of the number of samples, this amounts to quadratic complexity, rendering large-scale experiments infeasible. the approach by hunter and subramaniam is hampered by the same problem. baker ignores this problem, as the number of clusters for sequence space partitioning is pre-specified, not considering the increasing structural variation with growing fragment length. therefore, longer recurring fragments are implicitly discarded, unless they are represented super-proportionally to form a well conserved cluster.

in order to analyze the variation in structure space, we used a kd-tree to efficiently retrieve nearest neighbors of all data points. for length  <dig>  only  <dig> data points had no neighbors closer than a given cα distance matrix score threshold . for length  <dig> the number of "lonely data points" increased to  <dig>  roughly  <dig>  for length  <dig> and roughly  <dig>  for length  <dig>  however, as for longer fragments the usefulness of rigid comparison becomes questionable, flexible distance measures should be taken into account.

as the length of fragment pairs increases, the use of rigid structure comparison becomes questionable and techniques should be employed that take structural flexibility into account.

several recent approaches use smaller fragments of  <dig>   <dig>  or  <dig> residues  <cit> . while this reduces sequence-structure correlations, the advantage is that with a smaller set of representatives accurate modeling is possible. on the other hand chaining smaller fragments leaves more degrees of freedom in a fragment assembly approach. ideally a fragment assembly procedure either uses long fragments for modeling conserved parts and small fragments to fill the gaps, or the context of small fragments is taken into account, e.g. by studying chains of consecutive fragments .

CONCLUSIONS
we introduced a new approach to local protein structure prediction. in contrast to baker's approach  <cit> , we take into account structural information while partitioning sequence space. as sequence diversity is much higher than structural variation, it is expected that unsupervised learning in sequence space is harder than unsupervised learning in structure space. in this case the problem of choosing the correct granularity of the partitioning of sequence space takes the shape of the classifier regularization problem. therefore, standard methods like cross-validation can be used to determine a partitioning with high predictive power. in contrast to baker we can provide estimates of conservation even for less conserved fragments.

in contrast to hunter's approach  <cit> , we incorporate protein family information by using profiles instead of sequences. for secondary structure prediction this provision yielded a significant increase in performance  <cit> . the same could be observed in our case. the accuracy was further improved by using profiles based on amino-acid properties.

we see the main contributions of this work in  proposing a model for partitioning sequence space in dependence on the corresponding structures,  suggesting a representation of sequences through features based on amino-acid properties which can easily be used in classifiers and  introducing a distance metric for structural fragments which can be used in vector-space based methods.

there is room for further development of our method. the clustering procedure used for identifying recurrent local structure patterns is simple and can be enhanced in many ways. an important step is the removal of structural outliers. for increasing fragment sizes the number of expected outliers grows significantly. procedures for iterative pattern expansion are an interesting approach to reducing the size of the search space for longer fragments. also approximate nearest neighbor algorithms can be used to prefilter fragments. another way of getting around the algorithmic restriction to smaller fragments is provided by methods working on sub-samples of the data set. we suggest that the quantization error quality criterion is a suitable measure for assessing the quality of the representative fragment set and thus can be used to evaluate further methodical improvements.

on the long run, there are numerous ways to push forward the understanding of local sequence structure relationships. studying the evolutionary and physical role of protein fragments can lead to deep insights into the development of protein structures and function, as well as details of the folding process. interesting questions include the automatic detection of candidates for evolutionarily conserved fragments  <cit> , detection of patterns in the topology connecting smaller conserved fragments .

some of these questions are so fundamental that finding a comprehensive answer and understanding their relationships might be of similar difficulty as the protein structure prediction problem itself.

