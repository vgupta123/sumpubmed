BACKGROUND
in dna microarray experiments the expression pattern of many thousands of genes is discovered, which gives the possibility to reflect biological states of cells. the primary use of dna microarrays is the identification of genes expressed differentially between various conditions. differentially expressed genes  can be identified on the basis of different criteria; most often they are identified on the basis of p-values of statistical tests. degs are important characteristics of experimental results; they are summed up in the so called gene signatures and are further used in many contexts. the efficiency of identification of degs can be further verified e.g., by conducting sample classification experiments based on gene expression signatures selected from the top differentially expressed genes. the problem of construction of gene expression signatures for their use in molecular classifiers was studied in numerous papers; the discussion of many useful ideas can be found in  <cit> .

a challenge in identification of degs based on dna microarray data is a dimensionality problem; a small number of samples versus tens of thousands of genes’ expressions measured in each sample. a large number of statistical tests for finding degs result in the occurrence of many false discoveries among genes called differentially expressed. this problem can further manifest itself in the irreproducibility of results of different studies, e.g., a deg discovered in one study is not found in another one, or a molecular classifier designed in one study does not properly predict sample status for analogous data collected in another study. the proportion of false discoveries among genes called differentially expressed  can be controlled by using corrections for multiple testing  <cit> . however, introducing limits on fdr leads to the decrease of sensitivity of the procedure for discovering degs.

concerning the above described problem, methods for increasing the detection power for dna microarray data, i.e., for improving sensitivity of the process of the discovery of degs while keeping the fdr under control, have been proposed in earlier papers  <cit> . these methods are based on two-step procedures, where the first step is pre-selection  aimed at removing some pool of non-informative genes, and the second step is the discovery of degs in the pool of retained genes. if the pool of genes removed in the first step includes no or only few degs, the detection power of the process of discovery of degs becomes improved.

methods for increasing the detection power for degs, proposed in earlier papers, can be grouped according to criteria used for filtering out non-informative genes. the first group includes methods based on introducing thresholds for means or variances of gene expression signals. this approach was studied in several papers  <cit> , and it has been shown that for certain ranges of threshold values for means or variances of gene expressions  filtering increases sensitivity of discovery of degs. the second group includes methods based on detection calls  assigned to probe sets by the affymetrix mas  <dig>  signal pre-processing procedure. these labels are aimed at indicating whether the specific mrna is detectable  based on rejecting the null hypothesis in the wilcoxon signed rank test. a method based on probe detection calls, proposed in  <cit> , removes all genes except the fraction called ‘present’ in at least one group of samples. hackstadt and hess  <cit>  compared detection call methods and methods based on overall mean and variance filtering  on a probe set level in different combinations with the two fdr control methods and three pre-processing methods. they have discovered that both filtering methods, by detection call and variance  paired with either of the false discovery rate multiple testing correction methods considered led to an increase in the number of differentially expressed genes identified. the third group includes methods based on fitting statistical models to probe sets expression data  <cit> . these models can be factor analysis models  <cit>  or principal component analysis  models  <cit> , which explore sources  of variation in the data and allow the researchers to retain only the genes corresponding to components with large enough variation. a method named “i/ni - calls”, proposed by talloen et al.  <cit> , is based on approximating the probe intensity values by products of unknown loadings and factors. the authors of  <cit>  assume normal priors for loadings and estimate probe set signal variation by the variance of the hidden factor . they call a probe set informative if the variance of the hidden factor exceeds the assumed threshold. lu et al.  <cit>  propose another, simpler strategy to filter out non-informative genes , which uses pca analysis on the probe-level data. they call their method pvac  and use variability captured by the first principal component as a measure of consistency among probes in a probe set and consequently as a threshold for filtering out genes. the pvac method shows sensitivity comparable to the method reported by talloen et al.  <cit>  but its use offers several advantages. it does not rely on any distribution assumptions, no selection of informative prior is required. the approach is also computationally simpler and therefore more practical.

two-step procedures for degs discovery should be constructed in such a way that the first step of gene filtering is nonspecific , i.e., information on the samples’ class labels is ignored. otherwise the control of fdr becomes lost. in a recent paper by bourgon et al.  <cit> , they derive a more restrictive and precise “marginal independence” condition, which states that the criterion for gene filtering in the first step and test statistics for degs discovery in the second step should be independent under the null hypothesis. a violation of this condition can again lead to the loss of fdr control. a group of two-step procedures for degs discovery constructed in such a way that the first step is based on setting thresholds on sample means or variances and the test statistics in the second step is given by the t distribution was proven in  <cit>  to satisfy the marginal independence condition. therefore these methods are of special interest, due to the reliability of the estimated values of sensitivities and fdrs.

a basic parameter in these methods is the size of the pool of genes to be filtered out. the choice of this parameter is of crucial importance since filtering out too few genes does not improve the sensitivity enough, while filtering out too many genes can lead to the removal of degs together with non-informative genes. papers  <cit>  and  <cit>  address the problem of the choice of the size of the pool of genes to be filtered out. in  <cit>  two methods for specifying this number are considered. in the first method, the number of genes to be filtered out is estimated on the basis of the number of probe sets labeled “absent” by the affymetrix mas  <dig>  signal pre-processing procedure. authors of  <cit>  also recommend another, simpler method of filtering out 50% of probe sets. lu et al.  <cit>  also use this recommendation for filtering by overall mean or variance. however, different datasets may contain different numbers of non-informative genes, so using a fixed proportion  of filtered out genes may lead to the loss of efficiency of the filtration method. therefore, in the paper we address the problem of adaptive choice of the size of the pool of genes to be filtered out. by adaptive choice we mean the approach with the threshold level for filtering depending on the probability distribution of the analyzed signal . we propose a method based on the decomposition of the probability density function into a mixture of gaussian components and on the hypothesis that the gene content of the gaussian components is meaningful with respect to informative versus non-informative status of genes. we use the maximum likelihood method with the em algorithm to obtain decompositions computationally. we also compare results of our adaptive filtering method to results obtained in references  <cit>  and  <cit> .

methods
analogously to  <cit>  and  <cit>  we consider gene filters based on sample mean and sample variance in either log <dig> or original scales. we use the following abbreviations for naming different filtering methods: nf – no filter, s – signal mean-intensity-based filter, v – variance-based filter, lv – variance-based filter calculated on the log <dig> scale data. the letter “a” as a prefix corresponds to adaptive version of the filter, underscore with a given number p as a suffix corresponds to the fixed percentage p of genes filtered out. for example, alv represents  adaptive method used for variance calculated on the log <dig> scale expressions, while v_ <dig> represents the method of filtering out 50% genes based on setting a threshold for variance calculated on the original scale.

data
for testing performance of different filtering methods, we use four datasets previously analyzed in the referenced papers: an artificially created dataset, a spike-in dataset, a rat diabetes dataset and a leukemia dataset.

the artificially created dataset is produced by using the same method as that described in  <cit> . all distribution parameters are set to the same values. two scenarios for simulations are  expression signals independent between genes and  the signal values between genes follow a “clumpy dependence”  <cit> . the simulated data include two groups of five samples with signal values generated for  <dig>  genes for each sample. the number of true equally expressed genes  varies from 70% to 95%. in both scenarios, the simulation is repeated  <dig> times.

the spike-in dataset  database accession number gse21344) consists of  <dig> affymetrix drosophila genome  <dig>  microarrays  representing two different conditions, each of which contains  <dig>  identical crnas at different relative concentrations. for each condition, the total amount of crna is the same, and there are similar numbers of up- and down-regulated crnas:  <dig>  individual rnas are up- and  <dig> are down-regulated, with known fold changes varying between  <dig>  and  <dig>  and  <dig>  rnas are identical in abundance between the two conditions. the amount of rna hybridized to the arrays in the current experiment is calibrated such that the gene intensities fell within the range commonly seen in experiments stored in geo  <cit> .

the rat diabetes dataset  was obtained in an experiment conducted to detail global changes that occur in gene expression in the left ventricular of rat hearts related to streptozotocin-induced diabetes  <cit> . expression profiles were recorded sixteen weeks after induction. samples obtained from seven animals from each of the groups  were hybridized to an affymetrix rat genome  <dig>  <dig>  genechip .

the leukemia dataset comes from a microarray experiment on the affymetrix hg-u95av <dig> platform  done on the pretreatment leukemia samples from bone marrow and/or peripheral blood. molecular diagnostic studies confirmed the presence of bcr/abl gene rearrangements in  <dig> patients. forty six cases did not harbor any major molecular abnormality  <cit> .

all datasets used in this study were previously published and are publicly available either in the geo database or on the author’s web site. researches involving human participants  <cit>  and animals  <cit>  fulfilled requirements concerning informed consents of participants and ethical approval by appropriate institutions.

microarray normalization procedure
microarray normalization is done by using the robust multichip average algorithm rma  <cit>  that includes background correction, quantile normalization and summarization by the median polish approach. the rma procedure includes log <dig> transformation. if necessary, in order to obtain the original scale we perform the inverse transformation – the base  <dig> power function.

gaussian mixture decomposition
the analyzed signal, denoted by x, assigned to each probe set of the microarray chip corresponds to the mean or variance of gene expressions computed over the samples. in the case when x corresponds to the sample mean  the expression signal is log <dig> transformed. in the case when x corresponds to the sample variance, two further possibilities are considered,  the expression signal is log <dig> transformed and then x is computed as the logarithm of the sample variance ,  the original expression signal is used and then x is computed as the logarithm of the sample variance . the logarithm transformation is aimed at reducing skewness of distributions of sample variances. genes/probe sets on the microarray chip are numbered  <dig>   <dig> … n. n is the total number of genes/probe set on the microarray chip. the value of the signal x corresponding to gene/probe set no. n is denoted by xn.

let f denote the probability density function corresponding to the analyzed signal x. the gaussian mixture decomposition model  of f is:

 fx=∑k=1kαkfkx,μk,σk,∑k=1kαk= <dig>  

in the above expression k stands for the number of gaussian components, αk are non-negative component weights, fk is the probability density function of the k-th gaussian component:

 fkx,μk,σk=1σk2πexp−x−μk22σk <dig> 

and μk, σk are k-th gaussian component parameters – mean and standard deviation.

the gaussian mixture model is fitted to the experimental expression mean intensity or variance by using the method of maximization of the log-likelihood function:

 logl=∑n=1nln∑k=1kαkfkxn,μk,σk 

the expectation maximization  algorithm  <cit>  for recursive maximization of the likelihood function is applied. the initial values for decomposition parameters are randomly generated.

for the cases of analyses of the spike-in dataset, the rat diabetes dataset and the leukemia dataset we estimate the number of components k in the mixture distribution by launching em recursions many times with different k, and using the bayesian information criterion   <cit>  evaluated using values of parameters found in em procedure:

 bic=−2*logl+3*k−1*logn 

the estimated value of the number of components k corresponds to the smallest value of bic, searched over the range from  <dig> to  <dig>  for large n, the bic criterion leads to quite reliable estimates of the numbers of components  <cit> . after the decomposition of the probability density function, each gene is assigned to one of the gaussian components by using the maximum a posteriori  rule  <cit> . in other words, if xn is the signal value corresponding to the gene n, then this gene is assigned to component number k if αχfχ takes the maximal value for χ = k.

in the case of simulated data we take two approaches. in the first approach we assume that estimating the number of gaussian components by using the bic criterion is not necessary since the scenario of the simulation experiment imposes existence of two groups of genes. we therefore decompose the distribution of the signal x into a fixed number of  <dig> gaussian components. in the second approach we use the same method of estimating number of components k as the one described in the previous paragraph. it should be noted that, in the second approach, in each of multiple repetitions of simulation experiment, the estimated number of components k can be different.

gene filtering
our method for gene filtering involves removing genes belonging to components, which we expect to contain mostly non-informative genes. it is known that genes corresponding to either low values of mean expression or to low values of variance of expression are more likely to be non-informative  <cit> . the same property should pertain to gaussian components. when we decompose the sample means or sample variances into gaussian components, we can order the components with respect to their location parameter . then we remove genes which belong to components located at the left hand side of the signal scale, i.e., with the lowest values of this parameter. we assume that their inclusion into the further analysis would lead rather to false discoveries than to detection of true degs.

the problem is how many components corresponding to low values of x should be removed. we propose and analyze two methods for choosing the number of components to remove. the first one is based on the “top three” rule . more specifically, we assume that three components with highest values for parameter of location, called high-level expressed genes, medium-level expressed genes and low-level expressed genes, are informative and we retain genes corresponding to these components. other genes are removed. the second method is to use a clustering procedure, which classifies estimated gaussian components into two groups. we have chosen k-means clustering in three dimensional space with coordinates given by means, standard deviations and weights of gaussian components. the k-means algorithm minimizes the within-cluster sums of squared euclidean distances from each point to the center of the cluster. the number of clusters is assumed equal to  <dig>  two three dimensional clusters are ordered with respect to their location along the “mean of gaussian component” coordinate. then the cluster which location along this coordinate corresponds to a smaller value is considered non-informative. consequently, genes that belong to the gaussian components within this cluster are removed.

in certain situations we have to use adjustments of the filtering methods described above depending on k. namely, if the number of components is k =  <dig> or k =  <dig>  which can result from estimation, then the top <dig> method is considered as equivalent to nf. the clustering method works properly in all situations where k ≥  <dig>  in all computations we have never encountered the situation where k =  <dig>  however, if encountered, both methods top <dig> and k-means would be equivalent to nf.

discovery of degs, correction for multiple testing
for discovery of degs we use the two-sample t-test with equal variances, as in other studies  <cit> . for multiple testing correction we use the procedure for controlling fdr introduced by storey and tibshirani, called further q-value fdr correction  <cit> . the fdr constraint equal to  <dig>  is used.

assessment of the detection power of methods for discovery of degs
for the artificially created dataset and the spike-in dataset true differential expression status of each probe set is known, so in experiments with a various t-statistics threshold for these data sets we can always figure out which of the detected degs are true and which are false. we illustrate and compare detection powers achieved by investigated methods by using the following indexes: receiver operating characteristic curve   <cit>  plotted in the coordinate system sensitivity versus fdr, area under the roc curve  and the f <dig> measure defined as the harmonic mean of 1-fdr and sensitivity.

 f1=2*1−fdr*sensitivity1−fdr+sensitivity 

larger values of f <dig> measure suggest better performance of the method. it takes the maximum value  <dig> for sensitivity equal to  <dig> and fdr equal to  <dig> 

for the artificially created dataset, we additionally change the structure of the simulated data by assuming different proportions between eegs and degs, and we study their influence on the detection power of different methods. for experimental datasets, where the true differential expression status of probe sets is not known, for comparing different filtering methods we use the index proposed in  <cit> , defined by the number of null hypotheses which can be rejected in the set of genes remaining after filtering under a given constraint on fdr.

computational environment, developed scripts
all calculations and analyses were done in matlab  <dig>  environment by mathworks. all script files are available on request from the authors .

RESULTS
in this section, we present the results of using decompositions of distributions of sample means and variances into gaussian components for gene filtering. we also compare these results to results obtained with gene filtering methods reported in  <cit>  and  <cit> .

gaussian mixture decompositions of sample means and variances
in figure  <dig>  we present histograms of sample means and variances of the expression data and their gaussian mixture decompositions obtained by using em iterations and the bic criterion. the numbers of gaussian components obtained in each of these computational experiments are reported as entries of table  <dig>  where in parentheses, we also give the numbers of gaussian components corresponding to high levels of signal x, retained as informative using the k-means method. the rationale for declaring gaussian components informative or non-informative is described in the methods section. for simulated data we did not apply the lv method because the scenario of their generation already implies the expression values range corresponding to the logarithmic scale. computing the logarithm  creates data unsuitable for rational interpretation. in  <cit>  the same approach was used.

entries in the table present the number of gaussian components in the estimated mixture distributions. additionally, in parentheses, the number of gaussian components retained as informative based on the k-means method, are given.

comparisons of detection powers of algorithms with different filtration methods
the artificially created dataset contains  <dig> artificial samples  created by using the simulation algorithm described in  <cit> . for two scenarios considered, the scenario of expressions simulated independently for each gene and the scenario of the “clumpy dependence” between expressions of different genes, the results of analyses and comparisons are rather similar. one difference is that dispersions of both sensitivities and fdrs obtained across  <dig> simulations are higher in the dataset with “clumpy dependence”. therefore, we report only results concerning the “clumpy dependence” simulation model. in figure  <dig> we present comparisons of efficiencies of detection of degs in the simulated dataset for two approaches, fixed 50% filtering threshold on sample means and variances proposed in  <cit>  and our adaptive approach based on gaussian mixture decompositions into a fixed number of  <dig> gaussian components. in the upper panel of figure  <dig>  we show roc curves  corresponding to different filtration methods , obtained for the case where the proportion between informative and non-informative genes was set to 85% eegs versus 15% degs. we also plot a roc curve corresponding to the case where no filtering step is applied . we have also studied the influence of the proportion between degs and eegs in the dataset on the median sensitivity  achieved by different filtering methods at 5% fdr. we present median sensitivities versus the percent of eegs in the lower panel  of figure  <dig>  results of applying adaptive filtration based on the gaussian decomposition method with the estimated number of components are reported in table  <dig> 

results illustrated by the area under the roc curve  from fdr equal  <dig> to 20% and median sensitivity at 5% fdr calculated across  <dig> simulations for the case when the proportion between informative and non-informative genes was set to 85% eegs versus 15% degs.

in the first step of the analysis of the spike-in dataset, we compare the results of gene filtering methods with the percentage of genes to filter out recommended in  <cit>   to our adaptive methods based on gmm . as a reference, we also show the results of degs discovery for the case where no filtering step is applied . comparisons between different methods concern roc curves , which are shown in the upper plot  of figure  <dig> and f <dig> measures shown in the lower plot  of figure  <dig>  the adaptive filtration methods as, av, alv used for producing the roc curves in the upper plot  of figure  <dig> are all based on the k-means method. in the lower plot , circles and x-signs mark points defined by values of degs resulting from applying thresholds following from our adaptive gaussian mixture algorithms .

in table  <dig>  we additionally present comparisons of different gene filtration methods, on the basis of two indexes describing the power of degs detection, the area under the roc curve  and the sensitivity of degs search at the 10% fdr level.

data illustrated by two indexes: the area under the roc curve in figure  <dig>  from fdr equal  <dig> to 20% and sensitivity at 10% fdr.

in the second step of the analysis of the spike-in dataset we compare the results of applying alv and av filtering methods to the pvac method proposed by lu et al.  <cit> . in figure  <dig> we present  a roc curve corresponding to pvac filtering method and, for comparison, by using different colors, roc curves corresponding to alv and av filtering methods, and lv filtering methods with two different thresholds. here alv and av filtering methods are based on k-means method. the application of the pvac method results in the removal of genes except those which belong to the first principal component. in different datasets, the proportions between filtered out  and retained genes are therefore different. in figure  <dig>  we also plot roc curves corresponding to lv methods with percentages of removed genes equal to those obtained when using the pvac method .

the assessment of the power of different methods to discover degs in the rat diabetes dataset and the leukemia dataset is based on the method proposed in  <cit>  of counting numbers of null hypotheses, which can be rejected under a given constraint on fdr, assumed equal to 5%. in figure  <dig> in the upper panel  we present the plots of the numbers of genes called degs versus percentages of genes filtered out for the diabetes dataset and in the lower panel  analogous plots for the leukemia dataset. genes are called degs based on the t-test with q-value correction method for fdr. three different curves correspond to filtrations based on sample means, variances and variances calculated on the expression values in log <dig> scale. circles and x-signs mark points defined by the values of degs resulting from applying thresholds following from our adaptive gaussian mixture algorithms . the black vertical line represents the threshold proposed in  <cit>  on the basis of registration of “absent” calls returned by the affymetrix mas  <dig>  signal pre-processing procedure. if the pvac filtering method is used in the first step of degs discovery then, in the second step, for diabetes data  <dig>  null hypotheses can be rejected under a 5% constraint on fdr and for leukemia data the analogous number is  <dig>  these numbers are marked by horizontal dashed lines.

discussion
comments to comparisons
for the artificially created dataset, we see from the plots in figure  <dig> that in the analyzed range of fdr, all of the filtering methods increase median the sensitivity of degs search compared to the situation with no filtering. the increase of the proportion of eegs ) leads to the decrease of sensitivity of all methods. based on the comparisons of plots in figure  <dig> we conclude that as is the best filter for the simulated dataset. one can notice that adaptive filtering by signal performs best in terms of sensitivity versus fdr because the method simulation of the expression signal values meets the assumptions of the s method more closely than filtering by variance, which is a limitation of the simulation methodology used here. apart from using adaptive approach based of gaussian mixture decompositions into fixed number of  <dig> gaussian components  we have also tested the second approach, where the number of gaussian components was assumed unknown and was estimated by using bic criterion and pools of removed and retained genes were determined by using either top <dig> or k-means methods. for each of  <dig> values of proportions of eegs, which are distributed linearly in the range 70%-95%, we have performed  <dig> simulations . the simulation scenario was the same as that reported in figure 2; the only difference was that instead of using a fixed number of components we assumed unknown number of components and estimate it using bic criterion. the number of gaussian components, different for each of  <dig> simulations and different for different proportions of eegs and degs, varied from  <dig> to  <dig>  in the as method, and from  <dig> to  <dig>  in the av method. consistently to the simulation scenario, in the decompositions of sample means there were always two dominating components . results of applying different filtration methods to simulated data for the case when the proportion between informative and non-informative genes was set to 85% eegs versus 15% degs are presented in table  <dig>  again, the as filtration method was outperforming other filtration methods. when we used k-means method for as filter roc curves and plots of sensitivity were very similar to those presented in figure  <dig>  however, due to the small number of mixture components the use of top <dig> method for simulated data rarely increased sensitivity of finding degs. in the majority of cases as method was used on  <dig> components model, so introducing top <dig> method gave the same results as nf. the mixture decompositions of the distributions of sample variances were most often built of  <dig> components and retaining  <dig> components gave results similar to v_ <dig>  av filter gave similar results to v_ <dig> filter for the range of values of proportion of eegs 70% - 80%. when we further increased the number of eegs we filter out too many genes with av, which resulted in decreasing sensitivity of finding degs. at eegs = 90% the median sensitivity resulting from using v_ <dig>  was equal to  <dig> % compared to median sensitivity  <dig> % resulting from using av.

for the spike-in dataset, where we use the f <dig> measure and roc curves to compare filtering methods and show results in figure  <dig>  we observe that at low values of fdr the highest sensitivity is achieved by our alv method ). however, at higher values of fdr we see a flattening of the roc curve for the alv method. this shape of the fdr curve is related to the fact that the application of the alv method leads to filtering out quite a high percentage  of genes in this dataset. from figure  <dig> we can notice that the use of the alv method gives the worst sensitivity of finding degs. from plot  we notice that the methods k-means and top <dig> lead to the same result in as and alv filtration. from the plots of f <dig> indexes versus percentages of genes filtered out  of figure 3) one can see that the threshold values obtained by using av, as and alv methods are close to optimal i.e. close to values of filtering thresholds corresponding to maxima of the f <dig> measure. in comparisons of adaptive to fixed threshold methods  we conclude that as outperforms s_ <dig>  av outperforms v_ <dig> but alv led to worse result than lv_ <dig>  also both av and as outperform the no filtering method. as in  <cit>  we also check influence of filtering, with a smaller number of replicates . in all cases adaptive filtering increases the sensitivity of finding degs. the general conclusion is that when the number of replicates is smaller, the increase is higher. in the spike-in dataset analysis filters based on variances give better results than those based on means. comparisons of the pvac method to variance based filters, shown in figure  <dig> leads to the conclusion that pvac is indeed a highly effective method, but still similar to av. in the range of low values of fdr, pvac is outperformed by our alv method.

contemplation of roc curves in figures  <dig>  and  <dig> leads to an observation that when fdr changes , relations between sensitivities of different methods can become inverted. if the increase of fdr was continued to very high values , then the highest sensitivity would be achieved by no filtering  method. this shows that all filtering methods  are under risk of committing type ii statistical errors of removing some proportion of true degs and that different methods can offer different compromises between sensitivities and fdr. when indexes like f <dig> or auc are used, some methods can fully outperform others, as discussed above.

the plot in the upper panel  of figure  <dig> demonstrates that for the rat diabetes dataset, filtering thresholds found by using our adaptive methods are  close to optimal with respect to the measure given by the number of genes that can be called degs. s and v gene filtering methods based on adaptive thresholds are superior to the method of using 34% threshold level resulting from “absent” calls of probe sets, analyzed in  <cit> . the use of the adaptive version of the lv method leads to poor results. we can explain this by contemplating the histogram shown in figure 1h, which does not exhibit distinctive gaussian components. in this situation, there is a high overlap between two components detected, which leads to the removal of excessive number of genes called uninformative. concerning the comparison of our adaptive methods to the pvac method, the level of  <dig>  genes obtained by using the pvac method was outperformed by our av method.

the comparison between upper and lower plot in figure  <dig> shows that the choice of the type of filter can be crucial for the obtained results. for the leukemia dataset the best result is obtained after using the alv method, which is the worst choice for the rat diabetes dataset. this stems from the fact that in the diabetes dataset the degs belong to the group of highly expressed genes , which is not true for the leukemia dataset. strictly, for no filtering case in diabetes dataset, median of mean expression of estimated degs across all treatments is equal to  <dig>  and of estimated eegs is equal to  <dig>  . median of variance of expression on the log <dig> scale of estimated degs across all treatments is equal to  <dig>  and of estimated eegs is equal to  <dig>  . for no filtering case in leukemia dataset, median of mean expression of estimated degs across all treatments is equal to  <dig>  and of estimated eegs is equal to  <dig>  . median of variance of expression on the log <dig> scale of estimated degs across all treatments is equal to  <dig>  and of estimated eegs is equal to  <dig>  .

assessment of the proposed methodology
our adaptive filtering methods based on gaussian mixture decompositions do not use sample class labels. combined with the t-test they satisfy the “marginal independence” condition  <cit>  mentioned in the introduction section, since they only use sample means or variances corresponding to gene expressions signals. therefore we consider the proposed methodology as a reliable approach for gene filtering.

the representation of the probability distribution function as a mixture of components can be related to certain hypotheses concerning measured signals. components of signals defined by means or variances can be interpreted as corresponding to technical  noise, biological variation or to biological or cellular processes. decomposition of the distribution of the signal x into a mixture of  components is based on well-developed methods of statistical modeling  <cit> . different variants of methods of decompositions of signal distribution into mixtures of components were already successfully applied to several problems of analyses of dna microarray data, examples of related papers are  <cit> . in  <cit>  mixture decompositions are used for unsupervised clustering of microarray data. authors of paper  <cit>  propose mixture models for assessing differential expression between samples in microarray data. lee et al.  <cit>  use a mixture model for analysis of replicated microarray experiments. in this paper, we extend the range of applications of the mixture decomposition methodology to the problem of filtering genes in dna microarrays.

the mechanism of adaptation related to the mixture decomposition approach can be intuitively explained as follows. if in the analyzed data there are many genes or probe sets, highly corrupted by noise, with low levels of signal to noise ratios, then there would most probably exist corresponding gaussian components with low values of x signal and quite high component weights. these components will be removed by our filtration procedure. both of the two proposed selection methods, top <dig> and k-means, have adaptation potential. it seems, however, that the k-means method can lead to better results as seen in figures  <dig> and  <dig> 

CONCLUSIONS
the power of our adaptive method for improving detection of degs is compared to the results reported in earlier papers  <cit> . efficiencies of methods for improving degs detection power in microarray data are compared by using two datasets, in which the status of each gene is known. adaptive filtering repeatedly takes the highest places in comparisons of detection powers by different indexes . the efficiencies of different two-step methods for improving degs detection power are also estimated and compared for the rat diabetes and leukemia datasets, where the status of genes is not known, by comparing the numbers of the discovered degs for the same limits on fdr. the numbers of degs found by using adaptive filtering  belong to the highest among the compared methods. in conclusion, the number of genes to filter out by overall mean and variance should not be fixed, but rather found based on probe set signal properties , and the methodology for setting adaptive thresholds based on mixture decompositions is competitive compared to other gene filtering approaches.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
mm - performed all computational work related to samples preprocessing, filtering and classification and contributed with several hypotheses concerning possible methodologies for formulating the conditions for removing and retaining components. he also helped in writing the manuscript. rj – elaborated and implemented procedures for data normalization and storage. ap – helped in writing the manuscript and formulating conclusions. jp – outlined the research and helped in writing and organizing the manuscript. all authors read and approved the final manuscript.

