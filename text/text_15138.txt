BACKGROUND
distinct cell phenotypes are largely modulated by unique gene expression patterns, stemming from the interaction of the genome with its environment. such crosstalk is mediated by specialized cis-regulatory modules , including enhancers  <cit> , silencers, promoters, and insulators . among these, enhancers constitute the most prominent class of gene expression regulators. early experiments indicated that sequences located far from the gene promoters are often responsible for mediating gene transcription  <cit> . such genetic elements are called enhancers, defined as short dna sequences regulating temporal and cell-type specific basal gene-transcription levels, from transcription start sites , at distances ranging from hundreds of bases to, in rare cases, even megabases . knowing their properties, regulatory activity, and genomic targets is crucial to the functional understanding of cellular events, ranging from cellular homeostasis to differentiation. recent genome-wide investigation of epigenomic marks has indicated that enhancer elements could be enriched for certain epigenomic marks, such as complex, albeit predictive, combinatorial signatures of histone modifications. our efforts in this paper are motivated by these recent advances in epigenomic profiling methods, which have uncovered enhancer-associated chromatin features in different cell types and organisms . specifically, in this paper, we use recent state-of-the-art deep learning methods and develop a deep neural network -based architecture  to predict the presence and types of enhancers in the human genome. we call our system “ep-dnn”, an acronym for “enhancer prediction deep neural network”.

historically, computational identification of enhancers has proven to be challenging for several reasons  <cit> . first, the search space for enhancers is large—billions of dna base pairs—scattered across 98 % of the non-coding genome. second, while enhancers regulate genes in cis, they do not display distinct locational or orientation-centric signals relative to the genes that they regulate—potentially located upstream, downstream, or even in introns of the genes that they modulate, often regulating multiple genes  <cit> . enhancers function at a distance from their target genes via chromatin loops that bring the enhancers and target genes into proximity  <cit> , or via direct erna transcription from the enhancer dna sequences  <cit> . third, although a few computational attempts have been made to elucidate sequence-based signatures of enhancers , they are very recent, and yet to be widely adopted possibly because of the challenge of building models sophisticated enough to perform the classification task. we empirically validate this by considering some intuitive approaches for discriminating different forms of enhancers and non-enhancers  and observe that these will be highly inaccurate.

several high-throughput experimental approaches exist to identify enhancers  <cit> . the first is mapping specific transcription factor binding sites  through chip-seq  <cit> . this stems from the fact that short enhancer dna sequences serve as binding sites for tfs, and the combined regulatory cues of all bound tfs determine ultimate enhancer activity  <cit> . however, this approach requires the knowledge of the tf subset that is not only expressed but also occupies all active enhancer regions in the spatiotemporal setting, such as in a specific cell type at a point of time  <cit> . therefore, predicting enhancer activity from sequence-based information, such as from the tf motif content, remains challenging  <cit> . in addition, this approach is limited by the lack of available chip-grade antibodies that specifically recognize these subsets. the second approach is based on mapping transcriptional co-activator binding sites   <cit> . however, not all enhancers are marked by a set of co-activators and also often lack available chip-grade antibodies. the third approach relies on identifying open chromatin regions by dnase-i hypersensitivity  mapping  <cit> , which lacks specificity due to the fact that the identified regions can correspond to other crms. finally, the fourth approach involves histone modification signatures produced by chip-seq that consistently mark enhancer regions . due to their consistency in marking enhancers, we use histone modification features for our computational prediction of enhancer signatures.

through analysis of the prior computational approaches to enhancer prediction, we conclude that the difficulty of computationally predicting enhancer sites is because of two primary factors. first, they did not use the full spectrum of available features, i.e., all the histone modifications and their enrichment values in a wide region around the hypothesized enhancer site, denoted by the enhancer peak. second, they did not use a highly expressive classifier, one that can extract the distinguishing features from a complex landscape. this complex epigenomic landscape is captured by our empirically-derived distribution of normalized read counts of the four most distinguishing histone modifications for different types of functional sites from embryonic stem cells  . the most distinguishing histone modifications are chosen by analyzing the internal weights of trained dnn models. our empirical plots show that even for the top histone modifications, the overlap between any positive type and unknown is significant, which illustrates that simple rule-based classifiers would not perform well .fig.  <dig> 
a a notional schematic showing the enhancer and the tss  relative to some of the true positive markers —dnase-i hypersensitivity site , p <dig> binding site, and transcription factor binding site  . b various forms of these tpms overlap with the enhancer and the promoter sites. an overlap of the dhs with the tfbs can indicate an enhancer, while an enhancer is typically distal to the tss. tpms refer to dhs, p <dig>  cbp, and tfbs

fig.  <dig> distributions of chip-seq values for each functional type. unk refers to unknowns. p <dig>  cbp, tf, and dhs are considered positive, while tss and unk are considered negative



we address both of these problems, the first by starting with an  exhaustive set of features and then doing feature selection through an innovative mechanism, to identify a top-k most relevant features. empirically, the full set has  <dig> features ,  <dig> features for hepg <dig>  and  <dig> features for hela, and the reduced feature set for each cell line. for the second shortcoming, we use recent state-of-the-art deep learning methods and develop a dnn-based architecture  to predict the presence and types of enhancers in the human genome, “learning” from the combinatorial histone modification codes. we get the histone modification data from nih epigenome roadmap for h <dig> and imr <dig> and from the nhgri encode database for hepg <dig> and hela s <dig>  the enhancement level is available from chip-seq experiments and for our prediction, we bin them into 100 bp windows around the peaks of the regulatory elements, the total extent around the peak being  <dig> kbp.

next, we adapt a previously proposed theoretical mechanism  <cit>  for interpreting the results of the dnn to rank order the features according to how important they are for performing the classification task. we find that there are certain overlaps between which histone modifications are important for which cells, and even at a finer granularity, which spatial feature is important within each histone modification. for example, if one considers the top- <dig> most important histone modifications for each cell type, there is none that is common among the  <dig> cell types. however, between pairs of cell lines there are some commonalities—h3k18ac is important for both h <dig> and imr <dig>  while h3k9ac and h3k79me <dig> are found to be important for both hepg <dig> and hela s <dig> . we use this weight analysis to address the problem that training a dnn is computationally expensive. we rank order the features by importance and train dnns using only the top k features, for varying values of k. we compare the result of the prediction by the reduced dnns to that of the prediction by the full dnn . we find that the accuracy is within 90 % using the reduced k-value for each of the  <dig> cell types.fig.  <dig> venn diagram showing overlaps of top- <dig> histone modification among the four cell types



summarizing, we make the following contributions in this paper:we bring to bear a powerful data-analytic technique to enhancer prediction, namely, a deep neural network -based technique. we train dnn models that outperform prior computational approaches in terms of prediction accuracy for  <dig> different cell types, h <dig>  an embryonic cell type, and imr <dig>  a mature cell type.

we then make our dnn models interpretable, and by interpreting them, we find cell type-specific differences among our  <dig> distinct cell types. these differences highlight that different histone modifications are important for the prediction problem in these different cell types and even within a histone modification, which spatial features are important tend to differ.

we come up with a method to reduce the computational cost of training our dnn by reducing the input feature space in a data-driven manner.



biological use case of our solution, ep-dnn
we hypothesize some possible use cases for ep-dnn. first, an experimentalist by knowing the features and the histone modifications that are important to the task of classification can choose to collect data only for the most important ones among them. second, our data analytic technique reduces the noise inherent in the biological data by focusing on the important features. with respect to the spatial features, ep-dnn provides an insight into how far away from the peak locations, one needs to consider the effect of a functional element. in a prior approach, a  <dig> kbp region was used but this was ad hoc. armed with our analysis, any computational procedure, not just ours, can make an informed decision, e.g., only a narrow region  may be enough for a certain histone modification and a certain type of enhancer, while a wider region may be needed for a different type. third, we have started the process of identifying overlaps among the different cell types with respect to the important histone modifications. taking this process further, we can cluster different cell types with respect to their enhancer characteristics by using ep-dnn and then perform extrapolations for data from newer methods, such as, global run-on and sequencing , chromosome conformation capture  technologies, and their genome-wide derivatives that are used to analyze the activity levels and the 3d structures of the genome, respectively.

related work: previous computational methods based on histone modifications
won et al. proposed the usage of hidden markov models  to predict enhancers using three primary histone modifications  <cit> . firpi et al. focused on the importance of recognizing the histone modification signals through data transformation and employed time-delayed neural networks  using a set of histone marks selected through simulated annealing  <cit> . fernández et al. used support vector machines  on a subset of histone modifications. the subset had been determined through using genetic algorithms  <cit> . rfecs  improved upon the limited number of training samples in previous approaches using random forests , in order to determine the optimal subset of histone modification signatures in order to predict enhancers  <cit> . deep uses features derived from histone modification marks or attributes coming from sequence characteristics and inputs them in an ensemble prediction framework, which comprises multiple svms and an artificial neural network to vote on the results from the svms  <cit> . they show impressive results on three separate databases, encode , fantom <dig>  and vista.

methods
we present a high level overview of our approach in fig.  <dig>  in the figure, we show, separately, the training phase and the prediction phase. in the training phase, we create an optimal dnn using a set of histone modifications and the associated spatial features, and further, we do the weight analysis to rank order the features by their importance in discriminating the positive and the negative samples. in the prediction phase, we use the same set of features to predict if a regulatory region is an enhancer or not, followed by validation of our results. we predict enhancers in four distinct human cell types—embryonic stem cells , imr <dig>  hepg <dig>  and hela s <dig>  which were generated as a part of the nih epigenome roadmap project  <cit>  and the nhgri encode project  <cit>  .fig.  <dig> overview of our solution approach in which we train dnns using the histone modifications and their associated features. we perform weight analysis and feature selection to identify the optimal dnn, which is then used for predicting if a regulatory region is an enhancer or not



datasets
the development of ep-dnn is motivated by the availability of data from large scale projects, such as the encode project  <cit> , which has annotated  <dig>  putative human enhancers, with the current estimate of enhancer numbers being over a million  <cit> . another extensive database is the nih roadmap epigenomics project  <cit>  that also provides publicly-available epigenomics maps, complementary to encode. in addition, the ncbi’s gene expression omnibus  repository  <cit>  also hosts much previous work and data on enhancer prediction. we have used data from all  <dig> of these large-scale data repositories for arriving at our training and validation data for ep-dnn. in particular, we used nih roadmap epigenomics for chip-seq histone modification data for h <dig> and imr <dig>  which includes following  <dig> modifications in bed format: h3k36me <dig>  h3k27me <dig>  h3k4me <dig>  h3k4me <dig>  h3k9ac, h3k9me <dig>  h3k27ac, h2ak5ac, h2az, h2bk120ac, h2bk12ac, h2bk15ac, h2bk20ac, h3k18ac, h3k23ac, h3k4ac, h3k4me <dig>  h3k56ac, h3k79me <dig>  h3k79me <dig>  h4k20me <dig>  h4k5ac, h4k8ac, h4k91ac, and dnase i hypersensitivity sites  and transcription factor binding sites  data. the encode chip-seq experiment includes following  <dig> modifications for hepg2: h2az, h3k27me <dig>  h3k4me <dig>  h3k4me <dig>  h3k9me <dig>  h4k20me <dig>  h3k27ac, h3k36me <dig> h3k4me <dig>  h3k9ac, and h3k79me <dig> and following  <dig> for hela s3: h3k36me <dig>  h3k4me <dig>  h4k20me <dig>  h3k27ac, h3k4me <dig>  h3k79me <dig>  h3k27me <dig>  h3k4me <dig>  and h3k9ac in bam or bed format as well as dhs and tfbs data. bam files were converted to bed files using bedtools  <cit> . the p <dig> binding data for h <dig> and imr <dig> was downloaded from geo repository gse <dig>  generated by bing ren’s laboratory. for the tss locations for hela s <dig> and hepg <dig>  the fantom  <dig> consortium  <cit>  was used, which hosts cell-specific cap analysis of gene expression  data; cage measures tss expression levels by sequencing large amounts of transcript 5′ ends, termed cage tags. macs <dig> was used to call cage peaks, from which we selected the ones overlapping with true tss cage markers, available in the fantom database. for p <dig> data for hela s <dig> and hepg <dig>  we used encode chip-seq data, available from gse <dig> 

transcriptional co-activators—p <dig> and related acetyltransferases—bind to transcription factor  activation domains and have been found to localize to many active enhancers, but not all  <cit> . further, p <dig> co-activators are ubiquitous, present in all cell types, and control the expression of numerous genes. therefore, by using p <dig> enhancer signatures for training, we can also find other types of enhancers , generalizing well toward prediction of multiple classes of enhancers. the number of peak calls of functional elements in the dataset used for cell types, h <dig>  imr <dig>  hela s <dig>  and hepg <dig>  is presented in table  <dig> table  <dig> the number of peak calls of functional elements in the dataset used for training and prediction for the cell types: h <dig>  imr <dig>  hela s <dig>  and hep g <dig>  obtained through chip-seq and dnase-seq



preprocessing of histone modification inputs
previous studies indicate h3k4me <dig>  h3k4me <dig>  h3k4me <dig>  and h3k27ac as the top histone modifications  <cit> , indicative as markers of active enhancers. therefore, we selected them for our ep-dnn model. however, distinct from prior work, we wanted to see if there are other histone modifications that may also be important in the discrimination among enhancers and non-enhancers. thus, we selected all the remaining  <dig> histone modifications that are available in the nih epigenome roadmap for h <dig> and imr <dig> 

these other histone modifications are: h2ak5ac, h2bk120ac, h2bk12ac, h2bk15ac, h2bk20ac, h2bk5ac, h3k14ac, h3k18ac, h3k23ac, h3k27me <dig>  h3k36me <dig>  h3k4ac, h3k56ac, h3k79me <dig>  h3k79me <dig>  h3k9ac, h3k9me <dig>  h4k20me <dig>  h4k5ac, h4k91ac for h <dig>  and h2ak5ac, h2bk120ac, h2bk12ac, h2bk15ac, h2bk20ac, h3k14ac, h3k18ac, h3k23ac, h3k27me <dig>  h3k36me <dig>  h3k4ac, h3k56ac, h3k79me <dig>  h3k79me <dig>  h3k9ac, h3k9me <dig>  h4k20me <dig>  h4k5ac, h4k8ac, h4k91ac for imr <dig>  for the hepg <dig> cell type, we only had access to a smaller number of histone modifications,  <dig> in all. these are: h2az, h3k27ac, h3k27me <dig>  h3k36me <dig>  h3k4me <dig>  h3k4me <dig>  h3k4me <dig>  h3k79me <dig>  h3k9ac, h3k9me <dig>  and h4k20me <dig>  also, for the hela cell type, we only had access to  <dig> histone modifications. these are: h3k27ac, h3k27me <dig>  h3k36me <dig>  h3k4me <dig>  h3k4me <dig>  h3k4me <dig>  h3k79me <dig>  h3k9ac, and h4k20me <dig> 

the chip-seq reads of these histone modifications were binned into 100 bp intervals and normalized against its corresponding inputs by using an rpkm  measure. multiple replicates of histone modifications were used to minimize batch-related differences, and the rpkm-levels of the replicates were averaged to produce a single rpkm measurement per histone modification. we will refer to this enrichment level of a histone modification as its signature. the histone modification signatures of each bin location are then used as input to the dnn.

a notional schematic of the enhancer and the tss  relative to the various relevant sites—dhs, tfbs, and p <dig> is given in fig.  <dig>  the bounding box is the dhs and we are only considering sites that are overlapping with the dhs. the peak location is shown for each element and the activity level curve is shown on both sides of the peak region.

deep neural network  model
dnns have the traditional advantage that they provide feature extraction capabilities and do not require manual feature engineering or transformation of the data, which in turn would have required domain knowledge. ep-dnn was found to be less computationally expensive than the larger ensemble methods that combine multiple algorithms  or multiple models of the same kind . we experimentally show this higher computational cost of deep-encode  and rfecs in our results section, when performing detailed evaluation of our method.

to train our dnn, we first select distal p <dig> co-activator binding sites through chip-seq, then further select though overlapping dhss that are distal to tss, as regions representing enhancers . of these,  <dig>  p <dig> peak calls were selected for h <dig> and  <dig>  peaks for the imr <dig> cell line to represent enhancers for the training set. however, p <dig> co-activators also bind to distal tsss, which are not enhancers. therefore, we also select tss that overlap with dhs, as well as random 100 bp bins that are distal to known dhs or tss to represent non-enhancers. we include  <dig>  tss peaks from h <dig> and  <dig>  peaks from imr <dig> in our training set to distinguish between p <dig> binding sites that are enhancers and tss that are not, and  <dig>  random distal background sites were selected for h <dig> and  <dig>  for imr <dig> to represent non-enhancers for training.

for testing the dnn, we used all known distal p <dig> and cbp co-activator and tfbs that overlap with dhs as positive enhancer sites, and tss as non-enhancer sites.

a fully connected dnn with  <dig> inputs,  <dig> output, and softplus activation functions for each neuron was used to make enhancer predictions. each input sample consists of k number of 20-dimensional vectors of 100 bp bin rpkm-levels, windowed from − <dig> to +1 kb at each bin location. each 20-dimensional vector corresponds to a histone modification. this gives a total of  <dig> features in all for the full dnn for h <dig> and imr <dig>  with  <dig> histone modifications used for those. training was done in mini batches of  <dig> samples via stochastic gradient descent. to prevent overfitting, dropout training  <cit>  was applied, with a dropout rate of  <dig> . an optimal architecture of  <dig> hidden layers, comprising of  <dig> neurons in the first layer,  <dig> in the second, and  <dig> in the third, was found through cross-validation on half the training data, selected randomly. the full training set was used to train the model before prediction. a convergence on the mean squared error could be achieved with only  <dig> epochs of training. this extensive training mechanism was found to be suitable to optimize the dnn with its fairly large parameter space.

training and prediction
for training, we found that best results were obtained when the ratio of the number of positive and negative training examples was 1: <dig>  the class distribution was not modified during testing. prediction accuracy is evaluated using 5-fold cross-validation. we compare results for within-cell type prediction, i.e., we train on cell type c <dig> and predict on the same cell type c <dig> 

evaluation metrics: the standard precision and recall metrics misrepresent actual prediction performance on real data, since there are many more unknown functional sites than just the p <dig>  cbp, nanog, sox <dig>  oct <dig> binding enhancers or tss. ideally, we would have to evaluate performance on all these sites that are unaccounted for. however, most are not experimentally verified and are unknown. thus, there is not enough data to make an accurate evaluation of the precision and recall of any computational model. this observation has been made by prior computational approaches for enhancer prediction, such as rfecs. consequently, they have also not used the standard precision and recall metrics in their evaluation. furthermore, functional enhancers are experimentally verified by single peak locations. however, in reality, enhancers exist in various levels  and sizes  that more or less gradually decrease around the peaks. these peaks are not available during prediction on real data because we are trying to predict for locations that have not yet been experimentally verified. therefore, any computational model must be able to predict for the peak as well as the surrounding non-peak regions. further, the evaluation method must synthesize some criterion to determine what is the ground truth  for any genic region away from the peak location. therefore, the traditional evaluation using precision and recall metrics cannot be used in this case.

however, once a positive enhancer prediction has been made, it can be validated, and the metric that we use to compare the performance of ep-dnn is the validation rate. this metric has been used previously for evaluation of enhancer prediction, in rfecs  <cit> , and we modify it slightly here. in our definition, we refer to true positive markers  as distal dhs sites, p <dig>  cbp, and tfbs that are greater than 1 kb away from tss.if a predicted enhancer lies within  <dig>  kb of a tpm, then ep-dnn’s prediction is “validated”. in this case, we know that this site is either a known or an unknown enhancer, safely assumed to be an enhancer since it overlaps with a dhs site.

otherwise, ep-dnn’s prediction is “invalidated”. this means that it is either a tss or an unknown, but we know for a fact it is not an enhancer.



the modification from rfecs’ validation is that they had a separate class called unknown and we do not. we categorize it as a mistake if an unknown category is predicted by us as an enhancer. this is because while its exact functional characteristic is unknown, what is known for certainty is that it is not an enhancer. hence, predicting this as an enhancer is an error.

as dnn’s output is a number , we need to convert it to a class label by comparing it with a threshold. by varying the threshold, we can control the tradeoff between the number of enhancers being predicted and the validation rate. in general, as the threshold is increased, the number of enhancers being predicted goes down and the validation rate goes up. to compare against previous algorithms, in the first experiment, we used the same training and testing datasets for h <dig> and imr <dig> with rfecs and deep-en, the competitive approaches, as for ep-dnn. however, for hela s <dig> and hepg <dig>  we used a smaller dataset in the interest of computational time .

reduced dnn
since the cost of training an entire dnn with the total set of inputs— <dig> features for each of h <dig> and imr <dig>   <dig> for hepg <dig> and  <dig> for hela—can be significant, we take a principled approach to reducing the number of input features without significantly affecting the accuracy of our prediction. we refer to the dnn with the complete set of  <dig> features as the “full dnn”. we approach this in a two-pronged way. we summarize these two steps first and then give the details for each step.

step 1: we analyze the weights of the edges in the full dnn and come up with the importance score of each input feature, which approximates how much the feature influences the final output of the dnn model. we then rank the features by their importance scores. we will refer to this rank-ordered list as “ol” .

step 2: from the ol, we take the top-k features and create a dnn with k input features and keeping the rest of the dnn architecture the same as the full dnn. we reduce the value k in steps of  <dig> and observe the drop-off in the validation rate. we find that the curve has a knee at a value of k, which implies that the accuracy degrades significantly if we use less than k features. we then use the top-k features for our experiments for the classifier which we call “reduced dnn”.

details of step 1
the weights in a trained dnn contain information regarding the histone modification feature inputs and enhancers. in order to extract this information, we took a previous feature selection method  <cit>  that determines feature importance from shallow neural network architecture connection weights, then expanded it for deep architectures and applied it to our initial- <dig> feature dnn. the equation used is given below, where i is a neuron whose importance score we are calculating, and ni is the set of neurons in the next layer  that i feeds into. the importance score of neuron i, denoted si, is computed as si=∑j∈niwijsj 

in this formulation, the neuron j is in the next layer to neuron i  and the weight of the edge that connects neuron i to neuron j is wij. we start with the output neuron  and set the importance of that neuron to  <dig>  we then propagate the importance back to the previous layer’s neurons by using the weights of the edges connecting to the output neuron. the reader may notice that this is similar to the back propagation method, which is used to train dnn in the first place. finally, we reach the input layer where each feature feeds into one neuron and the importance of the input neuron gives the importance of that feature.

details of step 2
through step  <dig>  we get an importance score for each input feature. we rank the features according to their importance scores in an ordered list ol . then we start reducing the total number of features used by dnn by removing features at the end of ol. we train and evaluate dnn using the reduced feature set and compare its accuracy to that of the full dnn. in order to save time, we do this in steps of  <dig> features. we find that the accuracy is high and comparable to that of the full dnn till we get down to a certain number of features depending on the cell type.

note that we make a simplification here with respect to training the reduced dnns. we reuse the architecture of the full dnn in our reduced dnns for all the internal layers  and do not optimize the architecture for each reduced set of features. the exact approach would have taken far too long since dnn training to come up with an optimized architecture is by far the most time consuming step. empirically, we find that this simplification does not significantly affect the accuracy of the model.

RESULTS
a. can a simple deterministic rule-based classifier work?

we asked ourselves the question: can a simpler deterministic, rule-based classifier tell apart the different forms of enhancers and between the enhancers and the non-enhancers? the classifier will use the same set of features that our ep-dnn uses, namely, the normalized expression values  of the histone modifications. the intuitive way to structure such a classifier would be to consider the mean and the standard deviations of these features. these could be considered at the peak locations of these functional elements or also consider the bins around these adjoining regions. we experimented with both these approaches.

we show the mean and variance values for the  <dig> most discriminative histone modifications on h <dig> in fig.  <dig> . the main insight is that there is significant overlap in the expression values at the different functional sites. thus, a simple classifier based on these features will be highly inaccurate in discriminating among the different classes. even for the top histone modifications, the overlap between any positive type and unknown is significant, at  <dig>  % or more. taking the average of values of nearby locations does not help much, with the minimum overlap reducing only slightly to  <dig>  %. while tss has low overlap with other types for some histone modifications, tss sites make up less than  <dig>  % of all negative samples and are therefore not significant in the performance of the classifier.

these observations motivated us to look for more expressive classifiers and we settled on dnns.

b. validation rate of ep-dnn for  <dig> different cell types

figure  <dig> shows the variation of validation rates for the three protocols—our protocol ep-dnn and the two recent protocols, deep-en and rfecs—for the two cell types, h <dig> and imr <dig>  these are all same-cell predictions, i.e., we train on the same cell type as the one that we are trying to make predictions for. the dnn emits a numeric output, whose range may vary depending on what activation function is used, e.g., a sigmoid activation function results in the range  <cit> . by varying the threshold parameter for the output from the dnn, we are able to get a varying number of enhancer predictions. table  <dig> summarizes the result, for a fixed number of enhancer predictions, at approximately  <dig>  enhancer predictions. the first and most important observation is that ep-dnn performs better for validation rates across the entire range of number of enhancers being predicted.  also note that the slope of the curve for ep-dnn is lower than for deep-en and rfecs, implying that even when the protocol makes a large number of enhancer predictions, ep-dnn is more accurate. the only exception to the better performance of ep-dnn happens for imr <dig> same-cell prediction, for high threshold values  where deep-en and rfecs outperform ep-dnn. this likely happens because deep-en and rfecs do a certain amount of overfitting to training data  and such overfitting shows a  better prediction at high threshold values. this use case with high threshold values is arguably of use to experimentalists who are particular about high confidence predictions of enhancers for imr <dig> fig  <dig> comparison of ep-dnn against two state-of-the-art computational approaches for prediction of enhancer locations with respect to validation rate


ep-dnn

deep

rfecs

ep-dnn

deep

rfecs


thus, this indicates that our ep-dnn model is more powerful as a classifier for datasets where the positive and negative examples may be more “inter-mixed”, and thus, harder to classify. this underlines a fundamental motivation for our use of dnn—the increased power of the model, at the expense of a greater effort in tuning the algorithm. further, given that the h <dig> cell type is an embryonic cell type that is formative in character, it stands to reason that the differences between the signatures of enhancers and non-enhancers may be harder to resolve in it. we can contrast this to the adult cell type  used in our study, imr <dig>  where these differences while easier to resolve by a classifier, presents a learning field that is devoid of the richness and subtleties of enhancer signatures presented by the embryonic cell type. the conclusion from the above scenario can be summed up as follows: first, ep-dnn is a better learning model; second, the h <dig> cell type  presents a harder learning task.

figure 6a and b shows the evaluation of ep-dnn with h <dig> and imr <dig> for the full model and for a range of top-k values. figure 6c and d shows the evaluation of ep-dnn with two other cell types, hela and hepg <dig>  here, we do not have results from rfecs and deep for a comparative comparison. in terms of relative performance of ep-dnn in these cell lines compared to h <dig>  we see that the performance is better. this again speaks to the characteristics of the embryonic cell h <dig> where there is less differentiation among the different functional elements, making it harder for a model to differentiate between them.fig.  <dig> 
a and b show the evaluation of ep-dnn with h <dig> and imr <dig> for the full model and for a range of top-k values. c and d show the evaluation of ep-dnn with two other cell types, hela and hepg2



validation rate summary table
we benchmark the validation rates for our technique along with two other state-of-the-art computational techniques  for predicting enhancers, in table  <dig>  we evaluate the prediction of enhancers within a given cell line, individually for human embryonic stem cell line  and a human lung cell line . we keep the number of predictions by each technique to be close , for purposes of comparison. we find that dnn performs better in terms of validation rate. the advantage is more pronounced for prediction for the h <dig> cell type, where it is observed that enhancer prediction is a more difficult task than for imr <dig>  the improvement with dnn can be attributed to the use of the powerful dnn modeling technique, including multiple hidden layers and a large number of neurons at each layer, extensive feature selection, and optimization of the architecture and the parameters of the dnn.

validation rate detailed investigation
upon detailed investigation into the factors that contribute to the validation rates, we find that the dhs that are distal from the tss and the ones that are not p <dig>  cbp, or tfbs , are the most numerous enhancers and provides the single largest contribution toward the validation rate. for imr <dig>  tfbs and cbp regions are not present in its dataset. the p300s and cbps are more numerous in the data than the proportion in which they appear in our predictions. this can be explained by two factors. first, ep-dnn creates a model that generalizes well and does not overfit to the training data, which is all p <dig> for positive training examples, and consequently has a lower performance in predicting p <dig> sites. second, the enrichment curves for p300s and cbps are narrower, and thus, the signature may be weak toward the edge of the  <dig>  kbp boundary from the enhancer peak location. the greatest contribution to the validation rate comes, as before, from the dhs regions that are not p <dig> binding sites but are enhancers. note that we find that dnn is more prone to error in classifying some tss sites as enhancers, more so than deep-en and rfecs. however, the difference in tss mis-prediction is not too significant between dnn and the others and tsss are only a small fraction of the negative samples. thus, in aggregate, the validation rates of ep-dnn are higher for the entire set of  <dig> cell lines.

c. determining the most discriminative features

we trained an initial dnn using all  <dig> features from the  <dig> histone modifications, then calculated the importance score for each feature from the learned weights of the dnn according to equation  <dig>  the features were then sorted by their respective importance scores and different subsets of features were used to train dnns, starting with the top  <dig> features, top  <dig> features, and so on, ending at the full set of features. the architecture is fixed, with  <dig> neurons in the first layer,  <dig> in the second, and  <dig> in the third. each dnn was tested against  <dig> random subsamples of 20 k samples each, chosen from among all the chromosomes. figure  <dig> shows average subsample validation rate of each dnn, for h <dig>  imr <dig>  and hepg <dig> cell line, as we vary the number of features that the dnn takes as input.fig.  <dig> feature importance score and validation rate when only a subset of features is used, for cell types h <dig>  imr <dig>  and hepg2



we can see that validation rate increases sharply as important features are added. however, the rate increases more gradually after the top- <dig> features are added, for h <dig> and imr <dig>  and after the top- <dig> features are added for hepg <dig>  we omitted the hela plot because it looks similar to the hepg <dig> plot with the knee at the same point as well. this is interesting considering that the total number of features in hepg <dig> and hela s <dig> are different , though they are close compared to the feature set for h <dig> and imr <dig> . these results for the  <dig> cell lines confirm the validity of our weight analysis method and that it does indeed find the most important features for dnn and allows us to use a reduced subset of features for the final system. however, how big the reduced set should be is cell-type dependent.

figure  <dig> shows the comparison between the full 480-feature dnn and the dnn with the selected top-k features for each of the  <dig> cell lines. for a given cell line, we plot the curves for different values of k. to generate this figure, we vary the threshold that is used as a comparison point for dnn’s raw output. thus, as the threshold is raised, fewer number of enhancer predictions are made. an approximate determination of the realistic range for predicting prominent enhancer activity is when the validation rate is above  <dig> ; beyond that the predictions are too uncertain due to marginal enhancers or sites exhibiting weak enhancer signatures being predicted. within this operational range, the validation performance with the reduced  <dig> features is no more than 5 % worse than with the full feature set. for much of the operational region, the difference is 2 % or less. thus, we see that the reduction in the feature space, which reduces the cost of biological experiments to collect the data and the size of input data that a dnn has to be trained and tested with, does not hurt the enhancer prediction performance significantly. the interpretability of the dnn comes as another benefit of our process of reducing the feature set based on the importance scores of the features as calculated by our method.

we also see that if we reduce the number of features considered to be below that of the knee of the curve in fig.  <dig> for any cell line, then the validation rate performance does fall significantly. for example, for h <dig>  if we use only the top- <dig> features, out of the total of  <dig>  then the validation rate drops by almost 20 % compared to the full model . similarly, for hepg <dig>  when using only the top- <dig> features out of the total of  <dig> features, the validation rate suffers by  <dig>  %. we can conclude that our sweep over the value k for the top-k features gives us a principled way of choosing how to subset the total feature set for training the dnn. this top-k applies to more data from the same cell line as well as in some cases across cell lines. for example, the knee of the curve is seen at approximately  <dig> for both h <dig> and imr <dig>  it remains a subject of future investigation to find out when the generalization of the top-k can be done across cell lines and for which specific cell lines.

d. importance of the individual histone modifications for classification

next, we find the importance of each histone modification by summing up the importance scores of its  <dig> features. the results are shown in fig.  <dig>  from this result, we take the top four histone modifications for each cell type and create a venn diagram showing which histone modifications overlap with which cell types in fig.  <dig>  due to the highly non-linear nature of the dnn, the absolute values of the calculated importance scores do not show the absolute magnitude of importance. however, the values do indicate the importance of a single histone modification with respect to the task of predicting enhancer. by comparing the importance values, we can rank the histone modifications.fig.  <dig> importance scores of each histone modification, calculated as the sum of its  <dig> features. the important histone modifications are different for the four cell types, albeit with some overlap

fig.  <dig> importance scores for each feature of different histone modifications. the four specific histone modifications shown here represent the different patterns found through our analysis



the most important histone modifications according to our analysis confirm previous reports of h3k4me <dig>  <cit> , h3k27ac  <cit> , and h3k4me <dig>  <cit> , being the most important ones, in various combinations, overall in global enhancer prediction. however, comparing the histone modification importance within each cell type reveals cell-type specific differences. while h3k4me <dig> and h3k27ac are the most important histone modifications for h <dig>  for imr <dig>  h3k4me <dig> and h3k27ac are the most important. for hela s <dig>  we see h3k4me <dig>  also a known good predictor of enhancers, is the most important while h3k79me <dig> is the most important for hepg <dig>  although the well-known histone modifications  place in the top important ranks, we can also see histone modifications are different for the cell types  in finer granularity. this information can help computational scientists when building models to make predictions on specific cell types. further, it can also help life-science researchers optimize their experiments and collect the features for the most important histone modifications, for the cell type that they are focusing on.

e. importance of the histone modification features for classification

figure  <dig> shows importance scores of features within each histone modification. we selected four histone modifications to show the four distinct feature-importance patterns that we observe in the data. we omit the results from the other cell types since they have the same patterns as the h <dig> results presented here. this reveals that the most important features within a histone modification are not always centered at the enhancer site location, and consequently, it is detrimental to use fixed window sizes around the enhancer location, as all prior computational approaches have done. window sizes that are too small can lead to important features being excluded, while large window sizes will include noise in the data that can be detrimental to prediction accuracy. furthermore, certain “unimportant” histone modifications do contain relatively important features. this is why omitting histone modifications, altogether, even though they were reported to be unimportant can hurt the classifier’s performance. thus, analysis at this finer granularity of features, rather than the coarser granularity of histone modifications used in prior approaches, is needed.

sorting by the feature importance allows us to select only the most important and necessary features for prediction, instead of a fixed window size that has been used with previous methods. this allows us to reduce the number of input features necessary without a significant loss in its actual performance.

f. visualization of the feature space

to visualize the characteristics of feature space, we compute t-sne transformation  <cit>  with pca initialization. first we compute pca to create the  <dig> most important features and then we map them to a 2-dimensional space using the visualization method called t-sne, a variation of stochastic neighbor embedding  <cit> . this method tries to maintain the same distance between two points in the 2-d space as in the original  space. this has been found to be an effective way to visualize high-dimensional data since it represents each data object by a two-dimensional point in such a way that similar objects are represented by nearby points, and that dissimilar objects are represented by distant points. the resulting two-dimensional map of the data that reveals the underlying structure of the objects, such as the presence of clusters. figure  <dig> show the result of mapping feature spaces for all  <dig> features  and selected top  <dig> features  into 2-dimensional t-sne data, with red circles indicating the negative samples and blue circles the positive samples. note that in the earlier experiment  we were considering at the granularity of histone modifications; here we are considering the finer granularity of features, bins of histone modifications .fig.  <dig> t-sne visualization of the h <dig> cell line for all  <dig> features  and for the top  <dig> features, ranked according to their importance, i.e., discriminative ability . in both cases, the close clustering of the positive  and the negative  samples show that it is a difficult task to separate them. our dnn-based architecture, through its multi-layered structure and complex tuning procedure, is able to distinguish among the different kinds of positive elements  and the different kinds of negative elements 



from the t-sne plots, we see that there is not a distinct separation between the positive and the negative samples. this further emphasizes that it is not easy for a simple rule-based classifier to separate the positive and the negative examples and motivates our use of a relatively sophisticated classifier like dnn. our result, even after selecting the most important  <dig> features, does not show a clean separation between the positive and the negative examples .

g. execution time of the different models

in table  <dig>  we show the time to train and to predict using ep-dnn and ep-dnn-reduced, for our current prototype implementation in python, which underneath uses the toolkits keras and theano. these are done for the h <dig> cell line and the full model uses  <dig> features, while the reduced model uses  <dig> features. the training dataset has 40 k samples  and the prediction is also done for 40 k samples. we also compare the runtime of our approach with deep and rfecs, both of which are implemented in matlab. this comparison is not perfect because the implementations of these approaches take a smaller number of histone modifications than we do; deep-en uses only  <dig> modifications and rfecs only  <dig> modifications. since actual run times are highly dependent on several factors, such as the level of parallelization, hardware, platform, or implementation language, each method’s runtime was measured as the cpu clock time, under the same environment implemented in matlab2014rb  and in python , with no parallelization. we acknowledge that some algorithms are more easily parallelizable than others and our method of using serial execution alone does not bring that aspect out. however, we followed this approach to take out the variability of different parallelization methods in order to compare the runtime results of the different protocols.

we see that a reduction from the full model to the reduced model of 1/ <dig> of the features gives a slightly higher than proportional improvement to the training and the prediction times. rfecs has a much faster training time because it makes use of the highly efficient vectorized matrix computation of matlab. in terms of the prediction time, which should be sped up as far as possible, ep-dnn-reduced falls in between deep-en  and rfecs, with deep using  <dig> features  and rfecs using  <dig> features. however, deep performs poorly in terms of its training time and the time becomes infeasible for larger datasets.

CONCLUSIONS
enhancers are short dna sequences that modulate gene expression patterns. recent studies have shown that enhancer elements could be enriched for certain histone modification combinatorial codes, leading to interest in developing computational models to predict enhancer locations. however, prior attempts had suffered from either low accuracy of prediction or lack of interpretability of the results about which histone modifications are biologically significant. in this paper, we developed a dnn-based method, called ep-dnn, which addressed both of these issues. we find validation rates of above 90 % for the operational region of enhancer prediction for all four cell lines that we studied. the hardest to predict cell line is a human embryonic cell line called h <dig>  possibly because the different functional elements are not fully differentiated in it yet, but ep-dnn outperforms deep-en and rfecs, the two most recent computational approaches. then, we developed a method to analyze a trained dnn and determine which histone modifications are important, and within that, which features proximal or distal to the enhancer site, are important. we uncovered that the important histone modifications vary among cell types, with some commonalities among them. we can then reduce the heavy computational cost of training a dnn by selecting the top-k features to use. we find that for h <dig>  selecting a subset of 1/ <dig> of the total set of features gives approximately the same validation rate as the full model, while reducing the computational time for training by a little more than 3x. our results have implications for computational scientists who can now do feature selection for their classification task and for biologists who can now experimentally collect data only for the relevant histone modifications. in ongoing work, we are experimenting with parallelizing our computational approach and investigating further cell types to uncover possible groupings among cell types with respect to their enhancer characteristics.

declarations
this article has been published as part of bmc systems biology vol  <dig> suppl  <dig> 2016: selected articles from the ieee international conference on bioinformatics and biomedicine 2015: systems biology. the full contents of the supplement are available online at http://bmcsystbiol.biomedcentral.com/articles/supplements/volume-10-supplement- <dig> 

funding
this work was supported by nsf center for science of information  grant ccf- <dig> and nsf grant cns- <dig> 

availability of data and materials
all code and some samples of data will be made available at: https://bitbucket.org/cellsnmachines/enhancer-prediction-sysbio <dig>  the complete data files can be downloaded from publicly available sources, as described in the paper.

author’s contributions
nta and sk did the detailed design, implementation, and experimentation on the protocol. sc came up with the conceptual idea of using dnn for enhancer prediction, drove the design, and the experimental strategy. cf did some parameter configuration experiments for the dnn. mh did the dataset processing. ayg provided high-level guidance relative to the pitch of the paper. sc prepared the bulk of the manuscript, while nta, sk, and cf contributed write up to the paper. all authors read, edited, and approved the final manuscript.

competing interests
the authors declare that they have no competing interests.

ethics approval and consent to participate
not applicable.
