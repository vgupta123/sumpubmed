BACKGROUND
dna microarrays, with their ability to capture a substantial fraction of a cell state, are one of the most powerful tools in the molecular biology. from a machine learning point of view, standard microarray experiments generate an information system in which each object  is described by a vector of features corresponding to expression levels of a large number of genes . additionally, microarray experiments generate a decision corresponding to the investigated state, such as the presence of a disease, the application of a certain stimulation, the state of the organism, the tissue, etc.

because the number of investigated genes is always much larger than the number of measurements in a dna microarray experiment, gene selection with these data belongs to the p≫n-class of problems, which is known to promote a number of issues related to the stability, statistical power and feasibility of certain methods. moreover, because a measured set of genes is almost always not specifically targeted for a certain decision , these data will contain a large number of redundant features.

for these reasons, it is usally desired to reduce the dimensionality of a microarray dataset. dimension reduction is often achieved by feature selection  because it is the only method that maintains a direct relationship between a feature and a gene  <cit> ; this is why this process is often called gene selection in the context of microarray data.

it is often assumed that gene selection both provides meaningful insight into the data  and serves as a pre-processing step that optimises next methods in the analysis pipeline.

however, this assumption is wrong  <cit>  and fature selection may only have one of two aims that require different approaches and tools: finding the minimal optimal subset of features that is the smallest that will allow a given classifier to achieve maximal accuracy, or fiding the all relevant subset, that is of all features relevant to the analysed phenomenon.

this is because the goal of the minimal optimal selection is to optimise certain classifier, thus it will be affected by inherent biases of that method. for example, it may favour genes with expression levels that have certain characteristics, like follow a specific distribution. also, in p≫n datasets, false associations that are equal to or stronger than the true association are very likely to arise at random. while minimal optimal selection will greedily reduce blocks of redundant features, such artefacts can displace relevant genes from the final selection and lower the stability and recall of the method.

unfortunately, only the minimal optimal problem is traditionally tackled because both its application and assessment  are straightforward. yet only the solution to the all relevant problem can enable deeper insight in mechanics of an analysed phenomenon that go beyond just identifying the brightest signs of its occurrence.

the random forest algorithm is popular in the life sciences because it supports p≫n datasets, is robust to large amounts of noise, requires little parameter tuning and requires no predictor transformation  <cit> . random forest also natively produces a feature-importance measure that directly expresses the role of a feature in all interactions utilised in the model, including weak and multivariate ones. these characteristics make random forest a promising classification algorithm for gene selection tasks  <cit> .

to this end, a number of random forest-based feature selection methods have been proposed for gene selection. in this work, four state-of-art methods of this class are analysed: the artificial contrasts with ensembles   <cit>  and boruta  <cit>  methods, which are all relevant approaches, and the recursive feature elimination  and regularised random forest   <cit>  methods, which are minimal optimal approaches.

whenever possible, methods were re-evaluated with all three feature importance measures provided by the random forest algorithm as well as the importance scores provided by the random ferns  <cit>  algorithm, which is similar to a random forest but relies on a simpler and more stochastic base classifier.

because all machine learning algorithms are heuristic methods, the correctness and optimality of their solutions cannot be guaranteed. consequently, any methodology implementing these approaches must properly validate the results. in particular, if only a single application of a machine learning algorithm is applied to an entire dataset, subtle errors with very serious consequences may be introduced  <cit> . to avoid this limitation, the work presented here employed bootstrap  <cit> , method where each selection procedure was re-applied  <dig> times on resamples of the original dataset. moreover, apart from performing the usual analysis of post-selection classification accuracy, a novel self-consistency-based approach for assessing the stability and robustness of a gene selection method was developed and applied.

because the sole aim of this work was to investigate the characteristics of various gene selection methods, all tests were performed on four standard pre-processed microarray datasets: colon, leukemia, srbct and prostate. moreover, for clarity, no additional sources of information about the datasets, such as temporal context, gene ontology or microarray calibration techniques   <cit>  were considered.

RESULTS
post-selection classification accuracy
the most common method for the assessment and tuning of feature selection methods is to perform an error analysis on a classifier trained on a set containing only the selected features. this method is motivated by the seemingly obvious assumption that because the presence of noise and redundant features decrease classification accuracy, minimal error will be achieved with a set lacking these artefacts.

following this approach, each set of gene selections over bootstrap iterations was used to build a corresponding set of random forest validation models that were tested on objects not present in the corresponding resamples, and thus not used in feature selection or in the model training step. these results are presented in figure  <dig> 

it is clear that, with the exception of the rrf method, all investigated methods produced nearly indistinguishable post-selection errors. due to high variability in the results, however, even the result of the rrf method for the srbct and prostate sets were not significantly different from the results of the best performing method in each respective set.

this results also suggests the selection of the random ferns’ depth and the random forest importance source did not influence the post-selection error.

consequently, although an analysis based on post-selection error will obviously detect the removal of a significant amount of non-redundant information that is usable for the classifier, it is clear that its resolution is too low to serve as a reliable assessment of gene selection quality. because the post-selection error is also a highly variable statistic, one should never rely on a single estimate of its value. in the most striking example from this analysis, the application of the rfe method to the colon dataset produced a range of error values over all iterations sampled that varied by almost 50% .

on the other hand, no significant improvement over the models built from an entire dataset was observed. this result demonstrates the established fact that, due to its ensemble construction, the random forest method can handle a large number of noisy features without a significant increase in error.

self-consistency
gene selection quality was assessed by comparing the sets of genes selected by a given method over the  <dig> bootstrap iterations. from these data, genes that were selected in more iterations of the bootstrap than would be expected to occur at random were identified as significant selections; these genes are referred to as significantlyself-consistent selections  in this paper.

c
f
c/f
c
f
c/f
c
f
c/f
c
f
c/f
the average number of significantly self-consistent and all selected genes by a given method in one bootstrap iteration. c – the average number of significantly self-consistent genes, f – the average number of selected genes.

overall, the highest number of scss were produced by the boruta method; in the best cases, the scss covered 56–64% of all selections and approximately 55% on average. while more scss were found in all sets using the random ferns importance measure than with any of the random forest-based measures, the difference was noticeable only in the case of the prostate set. moreover, the use of both algorithms led to very similar scs ratios. out of the random forest-based importance measures, there was no measure that was clearly the best, but the raw importance measure seemed to be the most reliable choice. the increase in the random ferns depth parameter consistently contributed to an increase in the number of genes found by the boruta method. for the colon, leukemia and prostate datasets this effect was accompanied by a proportional increase in the number of scss, which caused the scs ratio to be approximately constant. this was not the case for the srbct set, however. in the srbct set, the number of scss did not increase and, therefore, its ratio dropped with the fern depth. still, the overall performance of the boruta method was surprisingly stable across the investigated importance sources, and it is unlikely that an incorrect set-up will substantially diminish its performance.

as expected for a minimal-optimal method, the rfe algorithm selected a much smaller number of genes than the rf-ace or boruta methods . except in case of prostate set, the number of scss was fairly stable and was approximately an order of magnitude smaller than when the boruta method was used. however, the number of found genes varied in an inconsistent manner across different importance sources. while the scs ratios in the leukemia and srbct sets were reasonably stable and reached 58% and 68%, respectively, the scs ratios were less than 40% in the colon set and ranged from 6% to 58% in the prostate set . therefore, it is likely that the minimal optimal sets still contained a significant fraction of irrelevant genes . moreover, rfe’s results can be significantly altered by the importance source.

the rrf algorithm selected the least number of genes from all sets, ranging from  <dig> to  <dig> . moreover, the results from the rrf algorithm were very inconsistent. the largest scs ratio achieved by the rrf algorithm was 22% in the srbct set, while the number of scss found by the rrf algorithm never exceeded  <dig> 

execution time
the average execution time of the selected algorithms is provided in the table  <dig>  the slowest method was the boruta algorithm using the random forest importance measure, with computational training time ranged from hours to days, especially for larger sets. the rf-ace and rrf algorithms required far less execution time, which never exceeded  <dig> hour for the colon, leukemia and srbct sets or  <dig>  hours for the much larger prostate set.

the execution time of selected algorithms, represented as the mean over  <dig> bootstrap iterations. all algorithms investigated in this study were run single-threaded.

however, while the difference in the computational time of the boruta algorithm was minor for random forest importance sources, the employment of random ferns resulted in significant increases in speed that ranged from  <dig> to  <dig> times faster. consequently, the execution time of the boruta algorithm was comparable to or even shorter than that of rf-ace and rrf. in the case of rfe, the gain from using random ferns was much smaller because this algorithm also relies on random forest for assessing the classifier accuracy from the current subset of genes.

CONCLUSIONS
as far as post-selection classification accuracy is concerned, all investigated methods were effectively equivalent. this proves that assessing gene selection algorithms in this way may be deceiving or inconclusive and, therefore, calls for deep and careful investigation of the significance of the observed accuracy differences.

out of all the analysed methods, the boruta algorithm found the most genes predicted to be important and, at the same time, achieved the highest ratio of self-consistent selections in its results. although it remains unknown how many of these novel genes are biologically relevant, these results provide strong justification that the selections generated by this method are promising candidates that should be explored further to identify more subtle aspects of the phenomena investigated via microarray experiments.

despite the fact that boruta requires an impractical amount of computation time in its default set-up, using the importance source produced by the random ferns algorithm decreased its running time to levels comparable with other investigated methods without sacrificing or improving the selection quality.

as expected, the minimal optimal rfe and rrf methods selected a much smaller subset of genes than the all relevant methods. however, the rfe and rrf methods achieved a similar level of selection stability and, thus, also generated a substantial amount of false positives. this result suggests that, even when focused on the most pronounced associations, it is important to be aware of the effects of the p≫n issues that are inherent to microarray data.

