BACKGROUND
the accuracy of phylogenetic inference often relies on the use of an appropriate model of molecular evolution  <cit> . inaccurate tree reconstructions can be the result of both stochastic and systematic error. stochastic error is the inevitable consequence of using finite datasets, and decreases as datasets grow in size. systematic error results from biases such as the failure to adequately model the patterns of molecular evolution that generated the data   <cit> , and can be amplified in large datasets, often resulting in strong support for the incorrect tree topologies  <cit> . improving approaches to model selection, even within existing phylogenetic frameworks, can help to reduce systematic error and improve the reliability of phylogenetic inference.

accounting for the heterogeneity in the rates and patterns of evolution among sites in a dna sequence alignment is an important part of selecting a model of molecular evolution  <cit> . among the methods proposed to account for this are mixture models  <cit>  and partitioning  <cit> . mixture models account for among-site heterogeneity by combining estimates of the likelihood of each site in the alignment under more than one model of molecular evolution. partitioning accounts for among-site heterogeneity by splitting an alignment into several groups of sites  and estimating model parameters independently for each subset. although mixture models are an elegant way to account for among-site heterogeneity, partitioning remains more popular, more widely implemented, and is currently the only approach that is computationally efficient enough to work on very large datasets  <cit> . thus, our focus in this manuscript is on developing methods to improve the selection of partitioning schemes for phylogenetic analyses, with a view to improving the inference of phylogenetic trees from large datasets.

an inherent obstacle in partitioned phylogenetic analyses is the choice of an appropriate partitioning scheme. one approach would be to evaluate every possible partitioning scheme for a given dataset and choose the best scheme, perhaps according to one of the commonly used information theoretic metrics such as the aicc  <cit>  or bic  <cit>  or by some measure of biological features in the data. however, comparing all possible partitioning schemes is practically impossible because the number of partitioning schemes is astronomical even for very small alignments  <cit> . for example, some of the smallest alignments used today, associated with dna barcoding studies, contain ~ <dig> base pairs  <cit> , which can be grouped into more than  <dig>  ×  <dig> possible partitioning schemes: well beyond anything that can be feasibly analyzed by brute force. a related approach is to allow the data inform the assignment of sites to subsets, and to integrate out the uncertainty in these assignments in a bayesian framework  <cit> . although this method is elegant, it has a high computational burden that renders it impractical for all but modestly sized datasets.

the most commonly used method for partitioning alignments, and the only one currently suited to very large datasets, is to define subsets according to structural features of the sequences in the alignment, such as gene boundaries, codon positions, structural components of rrnas , or some combination of these. we call this ‘traditional’ partitioning throughout this manuscript. this approach is also known as mechanistic modeling because it describes known biological or mechanistic processes and is motivated by the observation that different molecular features can have different patterns of molecular evolution  <cit> . recently, various methods have been proposed to algorithmically refine traditional partitioning schemes by grouping together similar subsets of sites  <cit> . one example of this method is the partitionfinder greedy algorithm  <cit> , which works by joining a pre-defined subset with every other pre-defined subset and then selecting the grouping that most improves the aicc or bic score. this is repeated until no more groupings improve the score. using this method can result in large improvements in model fit. however, despite their popularity, all traditional partitioning approaches make an important assumption that is rarely questioned: that all of the sites in each of the pre-defined subsets  have evolved under a single evolutionary model.

a number of recent studies have suggested that traditional approaches to partitioning can be inadequate. evidence suggests that there can be substantial heterogeneity of the evolutionary process within a single codon position of a single gene  <cit>  and within a single stem or loop of rrna  <cit> . if this is true, then traditional approaches to partitioning may fail to adequately account for the variation in patterns of molecular evolution within each traditionally defined subset of sites. for smaller datasets, these limitations can be overcome by applying newer methods  <cit> , but for larger datasets the limitations of traditional partitioning remain a problem.

another limitation of traditional partitioning involves its application to new types of molecular markers. many of the latest methods for assembling phylogenomic datasets result in large alignments that consist either entirely or largely of non-protein coding dna )  <cit> . it can be difficult to determine a good partitioning scheme for these datasets with traditional approaches because we understand little about the molecular evolution of the sequenced regions, and the datasets lack convenient features such as codon positions on which subsets can be defined a priori. thus, we face the problem that we lack adequate ways to model molecular evolution for some of the largest and most promising empirical datasets in our field.

one approach to choosing a partitioning scheme for large datasets is to group sites into subsets using estimates of site rates  <cit> . kjer and honeycutt  <cit>  showed that partitioning an alignment in this way resulted in a mammal mitochondrial genome phylogeny that was better supported and more congruent with phylogenies based on nuclear data. ellingson et al.  <cit>  showed that this approach improved both topologies and node support for a phylogeny of fish. however despite their promise, these methods have not been widely adopted. this is perhaps because they are difficult to use and require various decisions  to be made before the analysis is conducted.

in this study, we develop a new algorithm that automatically defines partitioning schemes by clustering similar sites together into subsets. our approach improves on previous work in three important ways. first, while previous approaches  <cit>  have required the user to choose the number of subsets before the analysis is carried out, our method estimates the optimal number of subsets directly from the data. this is important, because the optimal number of subsets may be difficult to predict in advance, and is influenced by several variables: e.g. the variation in substitution patterns among sites, the range of gtr submodels that can be selected for each subset, and the method used to evaluate the fit of the model to the subset . second, our method scales to work with the large datasets being produced today. third, we explicitly test for, and address, the presence of a suspected bias in previous implementations of this approach: that the partitioning scheme selected by the method may be biased towards the phylogenetic tree from which the site rates were calculated  <cit> .

we demonstrate our approach on a wide range of datasets. our results show that our method can be used to select partitioning schemes for the full range of datasets used in phylogenetics: from small barcoding datasets to large phylogenomic datasets consisting of ultra-conserved elements. in all cases, our method finds partitioning schemes that outperform those selected with traditional approaches to partitioning, when measured by metrics such as aicc and bic.

methods
terminology
we follow the terminology established in other studies on partitioning  <cit>  where a ‘subset’ refers to a set of sites for which the parameters of a nucleotide substitution model are estimated independently from other subsets. each site can be assigned to only one subset. in the phylogenetics community, a subset is often referred to as a ‘partition’; we avoid using the word ‘partition’ because it has conflicting definitions in other fields  <cit> . a ‘partitioning scheme’ constitutes a collection of subsets that include every site in the alignment once and only once.

iterative k-means partitioning algorithm
we present an algorithm  that automatically selects a partitioning scheme for a given alignment without the need for pre-defined subsets. we first give an overview, then expand on each step below:estimate a starting tree topology from the multiple sequence alignment;

start with a partitioning scheme that has all sites assigned to a single subset, and choose the best-fit substitution model for that subset;

calculate the information theoretic score of the current partitioning scheme;

for each subset in the current partitioning scheme, test whether that subset should be further divided:generate site rates for the focal subset;

divide the focal subset into two subsets using k-means clustering;

choose the best-fit substitution model for each of the two new subsets;

calculate the information-theoretic score of the partitioning scheme in which the two new subsets from 4c replace the focal subset;

if the information theoretic score improves, label the focal subset for division.



if no subsets have been labeled for division, terminate the algorithm. otherwise, define a new partitioning scheme in which each labeled subset in the list is replaced by the two correspondingly smaller subsets defined in step 4b and return to step  <dig> 
this figure illustrates the progress of a hypothetical run of the iterative
k
-means algorithm. the algorithm commences with an alignment that is treated as a single subset, and for which the aicc score has been calculated . during this step, each of  <dig> gtr + i + g submodels is fit to the alignment and the model that returns the best aicc score is chosen. next, the algorithm calculates tiger site rates for each site , and uses these rates to classify the sites of the alignment into fast  and slow  sites using the k-means algorithm . the aicc score of a model in which these two subsets are treated independently is then calculated . if the score improves, the split is accepted. the fast  and slow  sites are then used to create two new alignments, and the process is repeated with each new subset. this continues until no more subset splits are accepted. the final step combines all splits that improved the aicc score to create a single partitioning scheme for the dataset.



in step  <dig>  we estimate a tree topology with branch lengths for the dataset. optimizing the tree topology at each step would be computationally intensive, particularly for large phylogenomic datasets. for this reason, we use a fixed tree topology throughout the course of the algorithm. in principle, any method to estimate a starting tree could be used since it has been argued that a non-random tree is likely to be sufficient for model selection  <cit> ; in our implementation, we use the bionj algorithm implemented in phyml  <cit>  to estimate a neighbor joining starting tree, then re-optimize the branch lengths of this tree in phyml using the gtr + i + g model.

in step  <dig>  we define a partitioning scheme in which all sites in the alignment are assigned to a single subset, and we then select a best-fit model of molecular evolution for this subset. the model selection step uses an information theoretic metric  to choose a substitution model from a list of candidate models. here we select the best model from the set of  <dig> submodels of the gtr model available in partitionfinder v <dig> . <dig>  <cit> . these include the gtr model and some of the most popular submodels implemented in phyml, along with the model extensions using discrete gamma distributed site rates  and/or a proportion of invariant sites . during the model selection step, partitionfinder provides two options for estimating branch lengths: ‘linked’ or ‘unlinked’. when the branch lengths are ‘unlinked’, all branch lengths are re-estimated for each model in the list. when branch lengths are ‘linked’, the relative branch lengths are determined by the tree estimated in step  <dig>  and each model is afforded a single rate multiplier which can stretch or shrink all branch lengths in tandem. although ‘unlinked’ branch lengths allow users to better account for heterotachy , in practice, they add so many parameters to the overall substitution that they are rarely preferred. for that reason, in what follows, we use ‘linked’ branch lengths in all of our analyses, although the option to use ‘unlinked’ branch lengths remains.

in step  <dig> we calculate one of two information theoretic scores  for the current partitioning scheme. at the start of the algorithm, when all sites are assigned to a single subset, this score is equal to that of the initial best-fit substitution model.

in step  <dig>  we decide whether to subdivide each of the subsets in the current partitioning scheme. in step 4a, we fit a gtr + g model of molecular evolution to the subset, conditioned on the tree and its relative branch lengths estimated during step  <dig> using maximum likelihood in phyml  <cit> . then we use one of two methods to calculate site-specific rates for each site in the subset,  likelihood-based site rates or  tree independent generation of evolutionary rates  site rates  <cit> . likelihood site-rates depend on the branch lengths and therefore have to be recomputed for each new subset. similarly, tiger rates depend on the composition of site patterns in subsets so also have to be recomputed for each new subset. likelihood site-rates are estimated in phyml using the “--print_site_lnl” option. tiger site rates are calculated using a non-tree based method that estimates the similarity among site patterns as a surrogate for evolutionary rates  <cit> . this method relies on the construction and comparison of set partitions for each alignment pattern. for example, if a given alignment pattern is “aacgga”, the resulting set partition would be p = {{ <dig>   <dig>  6}, {3}, { <dig>  5}}. p  consists of a set of at most four sets, that contain the sequence numbers in the alignment pattern that have, respectively, the nucleotides a, c, g, or t at site i. the number of non empty sets, which we denote by |p| is equal to the number of different nucleotides found in site pattern i. the character partition of each site is then compared to the character partition of every other site. the sites are evaluated for agreement with every other site using a “partition agreement score”, ), which is defined as: pai,j=∑x∈pjax,pipj 

where a) is equal to  <dig> or  <dig> depending on whether x is compatible with the character partition of site i, i.e. if x is a subset of one of the sets in p: ax,pi=1ifx⊆aforsomea∈pi <dig> 

the rate  for the alignment pattern at site i is then obtained by computing the mean partition agreement score across all sites: ri=∑j≠ipai,jn− <dig> 

where n is equal to the number of sites in the alignment. the sites that are more similar to the pool of sites in the alignment are considered slow with rates approaching  <dig>  , while the sites that are less similar to the pool of sites in the alignment are considered fast with rates approaching  <dig>  it is important to note that this method does not take into account the character state in an alignment pattern when the set partitions are compared, e.g. the set partition and resulting site rate of “aacgga” would be identical to that of “ttgaat”. although software exists to calculate tiger site rates  <cit> , we found the existing implementation to be too slow to be useful. instead, tiger site rates are calculated using a fast, c++ based program that we developed  <cit> .

in step 4b, we use the k-means clustering algorithm to divide the sites in the focal subset into two clusters based on one or more of the site-wise parameter estimates from step 4a. k-means is a fast clustering algorithm capable of handling large datasets with high dimensionality  <cit> . it clusters data points by minimizing the within-cluster sum of squares measured between each data point and its closest cluster ‘centroid’. the goal of k-means is to minimize the function: minμ <dig> …,μk∑h=1k∑x∈χh||x−μh|| <dig> 

where k is the number of clusters, μ is the cluster centroid, and x is any given data point, in the case of this study, the site rates, and ‖x‖ is the l <dig> norm, or euclidean length, of x. the algorithm proceeds through two steps:the assignment step, in which each point is assigned to a cluster with its closest centroid.

the update step, in which cluster centroids are moved to the center  of their new clusters.



the number of clusters  is chosen a priori and fixed at  <dig> in our case, and then k centroids are placed within the sample space. the initial placement of centroids is an important step; poor placement can result in an unsatisfactory exploration of the sample space and, although the algorithm may converge, it may only reach a local optimum. to avoid this, we use the k-means++ centroid initialization method, which has been shown to be superior when compared to other centroid seeding techniques such as random placement  <cit> . we perform  <dig> initializations of the k-means algorithm, selecting the initialization that best minimizes the within-cluster sum of squares. following initialization, euclidean distances between each data point and the centroids are calculated and each data point is assigned to a cluster based on its nearest centroid. the centroids are then moved to the mean of their respective clusters  and distances are recalculated. this process is repeated until the centroids no longer move beyond a threshold at the end of the iteration. we used the k-means algorithm from the scikit-learn package implemented in python  <cit> . in theory, any statistic that can be estimated on a site-specific basis could be used for clustering. in what follows, we compare the performance of likelihood site rates and tiger site rates.

in steps 4c and 4d, we use the output of the k-means algorithm to create two new subsets, and then use an information-theoretic metric to decide whether splitting the focal subset improves the overall model of molecular evolution. to do that, we first  estimate the best model for each of the two new subsets from our set of candidate models as described above. we then  calculate the information-theoretic score of two partitioning schemes: one in which the focal subset is retained as a single subset, and one in which the focal subset is divided into two new subsets. if the overall information theoretic score of the latter partitioning scheme is better, we label the focal subset as one that should be divided.

once step  <dig> has been applied to all of the subsets in the current partitioning scheme, we ask whether there are any subset divisions that improved the overall information theoretic score . if there are none, then the algorithm terminates, since we are unable to find a partitioning scheme better than the current scheme. otherwise, we divide all of the subsets that are labeled for division in step  <dig>  then the algorithm iterates.

pragmatic considerations
the algorithm above makes the assumption that likelihoods can be calculated for any collection of sites in an alignment. during the development of the algorithm, we found some cases in which phyml was unable to analyze some subsets. this was usually because the alignments were too small or contained only sites with identical site patterns. since our aim is to produce partitioning schemes that can be used to estimate phylogenetic trees with programs like phyml, and since these problematic subsets are likely to occur during any approach similar to the one we describe here, we designed the following solution. first, we flag the problematic subsets as the algorithm proceeds, and make the conservative assumption that their site-likelihoods will be identical to their site-likelihoods in the larger subset from which they were generated. this allows us to estimate conservative information theoretic scores for partitioning schemes as the algorithm proceeds. at the end of the algorithm , we combine each of the problematic subsets with their nearest neighbor subset, defined as the non-problematic subset with the centroid  that has the shortest euclidean distance to the centroid of the problematic subset. this process is repeated until there are no problematic subsets, i.e. until phyml can successfully analyze all of the subsets in the partitioning scheme.

empirical evaluation
to evaluate the performance of the iterative k-means algorithm, we compared ten partitioning scheme selection approaches on ten different datasets . the approaches comprise five different partitioning methods, each of which was applied with both the bic and aicc . the five methods we compared were:  no partitioning ;  partitioning by gene and codon position/rdna stems and loops ;  optimizing the partitioning scheme from  using the greedy algorithm implemented in partitionfinder  <dig> .1;  iterative k-means with likelihood site-rates;  iterative k-means with tiger site-rates.table  <dig> 
names, references, and clade information for the datasets used in empirical analyses



dataset name
clade 
clade 
paper reference
dataset reference



dataset name
no partitioning
user
greedy
tiger site rates
likelihood site rates


during the empirical evaluation, one dataset, mccormack  <dig>  <cit> , was too large and partitioned into too many pre-defined subsets to analyze with partitionfinder’s greedy algorithm in a reasonable amount of time. for this dataset, we used the relaxed clustering algorithm  <cit>  in partitionfinder  <dig> . <dig>  relaxed clustering is optimized for large datasets and uses raxml  <cit>  for all likelihood calculations. since only two nucleotide substitution models are implemented in raxml  we used a two-step approach. first, the optimal partitioning scheme was selected using the relaxed clustering algorithm for the two raxml models, and second, we reselected models for each subset of the initial partitioning scheme with the ‘user’ option in partitionfinder  <dig> . <dig>  but this time with phyml and considering the full set of models used in every other treatment. this allowed us to directly compare the information theoretic scores of this partitioning scheme with those selected by the other methods.

starting tree bias evaluation
although it has been shown that a starting tree topology is unlikely to negatively affect model selection as long as it the starting tree is non-random  <cit> , it was unclear whether this would be true for the iterative k-means method we develop here. specifically, we were unsure whether site rates calculated under the assumption that the starting tree is true would bias our partitioning schemes toward recovering the starting tree during downstream phylogenetic analyses. thus, we designed a simple test to evaluate starting tree bias.

to test whether the starting tree introduced bias into the estimation of the partitioning scheme, we used a five-step process. first, we estimated a neighbor-joining  tree for the data. second, we created twenty new trees, where each new tree was a single subtree-prune and regraft  move away from the nj tree, giving a set of  <dig> plausible non-random trees for the dataset. third, we used these  <dig> trees as starting trees from which we estimated  <dig> partitioning schemes for each dataset using three methods: the partitionfinder greedy algorithm, iterative k-means with likelihood site-rates, and iterative k-means with tiger site rates . fourth, we estimated a maximum-likelihood phylogenetic tree using raxml for all  <dig> partitioning schemes for each dataset. for each of the three methods, the process resulted in a collection of  <dig> distinct starting trees, and  <dig> estimated ml trees. the final step in the process involved statistically testing whether the starting trees are more similar to their corresponding ml trees than would be expected by chance. to do this, we used a bootstrap test in which the observed test statistic is the sum of the robinson-foulds  <cit>  distances between each starting tree and the corresponding ml tree . for example, in the most extreme case, where each ml tree is identical to its corresponding starting tree, the observed test statistic would be zero. the null distribution of this test statistic is then estimated by re-calculating the test-statistic  <dig> times after randomly shuffling the list of ml trees each time. if the starting tree biases the estimation of the ml tree, then we expect the observed test statistic to be in the lower tail of the null distribution. we calculate the one-tailed p-value from the position of the observed test statistic in a ranked list of the values of the test statistic from the null distribution.

simulation example
while this paper was primarily aimed at evaluating the efficacy of the iterative k-means algorithm on empirical datasets, we also evaluated our method with a simple simulation. first, we simulated a tree under the yule  process in indelible v <dig>   <cit> . we chose a rooted tree and specified the following parameters for the simulation: number of tips- <dig>  birth- <dig> , and death- <dig> with a tree depth of  <dig> . we then simulated a  <dig>  bp alignment using the jukes cantor  <cit>  model. next, we scaled the tree from the first run to a tree depth of  <dig>   and simulated another  <dig>  bp alignment using jukes cantor. finally, we concatenated the alignments  and estimated a partitioning scheme for it using iterative k-means with tiger rates. each step, from tree simulation through partitioning scheme selection was repeated  <dig> times. these conditions were chosen to explicitly test whether the iterative k-means algorithm would 1) assign alignment sites to subsets containing other sites generated from the same model, and 2) find the correct number of subsets.

RESULTS
the primary purpose of this study was to describe the iterative k-means algorithm and evaluate its performance on empirical data as compared to other commonly used partitioning strategies. to do this, we selected partitioning schemes for published empirical datasets using several different methods and compared the relative fit of each partitioning scheme using aicc and bic.

the iterative k-means algorithm substantially outperformed all other partitioning approaches for each of the ten datasets we analyzed, regardless of the details of the k-means approach or the information theoretic metric we used . the set of alignments that we used to test the algorithms comprise a wide range of lengths, number of taxa, and types of molecular markers, confirming the utility of our new algorithm for a wide range of phylogenetic analyses.figure  <dig> 
bic scores for partitioning schemes estimated during empirical testing . the k-means methods presented here outperform traditional methods. “none” is no partitioning, “all” is the user partitioning scheme, “pf-g” is the partitionfinder greedy algorithm, “tiger” is iterative k-means using tiger site rates, “likelihood” is iterative k-means using likelihood site rates. note: the “pf-g” score for the mccormack  <dig> dataset was obtained using the partitionfinder relaxed clustering followed by model selection with phyml as described in the methods, not the greedy algorithm.
aicc scores for partitioning schemes estimated during empirical testing . the k-means methods presented here outperform traditional methods. “none” is no partitioning, “all” is the user partitioning scheme, “pf-g” is the partitionfinder greedy algorithm, “tiger” is iterative k-means using tiger site rates, “likelihood” is iterative k-means using likelihood site rates. note: the “pf” score for the mccormack  <dig> dataset used the partitionfinder relaxed clustering followed by model selection with phyml as described in the methods, not the greedy algorithm.



figures  <dig> and  <dig> show comparisons of the aicc and bic scores achieved by five partitioning methods: using a single partition; partitioning according to structural features of the sequences; optimizing a partitioning scheme based on structural features using partitionfinder; iterative k-means partitioning with likelihood-based site rates; and iterative k-means partitioning based on site rates estimated using the tiger method. figures  <dig> and  <dig> show that both k-means methods we describe here consistently outperform all of the other methods. the figures also suggest that the likelihood-based method is superior, as it consistently outperforms the method based on tiger rates, achieving lower aicc and bic scores. however, the apparent superiority of the likelihood-based method comes at a cost – it is also frequently associated with a bias: phylogenetic trees estimated from partitioning schemes derived from the likelihood-based approach were often more similar to the starting trees than would be expected by chance . in  <dig> out of  <dig> datasets , our test for starting tree bias returned a statistically significant result  for the likelihood-based method.table  <dig> 
p-values and effect sizes for each dataset from starting tree bias analysis



dataset
partitioning method
p-value
effect size


in contrast, when using the tiger based rates we found no evidence for starting tree bias in any of the datasets that we examined. we attribute the difference between these two methods to the fact that the likelihood-based approach relies on a particular starting tree to calculate rates of evolution, whereas the tiger method calculates rates without assuming a particular tree  <cit> . it appears that the dramatic gains in aicc and bic scores achieved using the likelihood-based k-means approach are partially attributable to overfitting the partitioning scheme to the starting tree, and that this overfitting can then bias subsequent phylogenetic analyses. one symptom of this overfitting is that the likelihood-based rates method often selected subsets of sites that consisted entirely of invariant sites of a single nucleotide state. such subsets are difficult if not impossible to justify on biological grounds. together, these characteristics suggest that the likelihood method is problematic, and should be avoided. for the remainder of the paper, we focus only on the results from our study that used rates calculated with the tiger method, which do not show these undesirable characteristics.

one of the primary motivations for this study was to develop a method to select partitioning schemes for datasets that are very large and/or that comprise molecular markers that are not amenable to traditional partitioning approaches, both of which are increasingly common  <cit> . it is encouraging, therefore, to note that the iterative k-means algorithm performed particularly well on the phylogenomic bird dataset   <cit> , which was both very large and comprised solely of uce’s, for which traditional approaches to partitioning are difficult to apply. for example, when each uce was placed in its own subset, the bic score was worse than when all uce’s were grouped into a single subset ). when the partitioning scheme was selected using the relaxed clustering algorithm in partitionfinder, the bic score improved to  <dig>  , but when the partitioning scheme was selected using the iterative k-means method with tiger rates, the bic score improved to  <dig> , a substantial improvement .

the iterative k-means clustering also worked well for datasets consisting of protein coding genes from the standard phylogenetic toolbox. a close examination of the partitioning schemes reveals that the algorithm chooses subsets that reflect the traditional biological partitioning boundaries such as genes and codon positions . for example, in the partitioning schemes selected for the hawaiian fancy-case caterpillar dataset consisting of three protein-coding genes  <cit> , the k-means approach resulted in one large subset that contained almost all first and second codon position sites across all three genes along with some third codon position sites , and eleven smaller subsets which consisted primarily of third codon positions sites from the three loci . insofar as it broadly combines first and second codon positions, and separates out third codon positions, this partitioning scheme is similar to a popular traditional partitioning method that does the same  <cit> . however, although some of the structure of the classical partitioning boundaries exists in the subsets chosen by our algorithm, other subsets include sites from a wide range of genes and codon positions . these results confirm that there is biological value to partitioning by genes and codon positions, but also suggest that relying solely on such boundaries may often fail to capture some of the complex patterns of molecular evolution among sites, potentially limiting the accuracy of downstream phylogenetic analyses.figure  <dig> 
assignment of codon position by gene to subsets selected using the iterative
k
-means algorithm clustered using tiger site rate estimates on the kawahara  <dig> dataset. subsets are ordered by the mean site rate from slowest to fastest. sites from each codon position are spread throughout the subsets with the majority of variation among sites in the 3rd codon position.
subset assignments for the sites from each codon position using the iterative
k
-means algorithm clustering using tiger site rate estimates on the kawahara  <dig> dataset. each row corresponds to a single gene and each column corresponds to a different codon position. the dotted red line represents the total number of sites in each codon position. in each chart, subsets are ordered by the mean site rate from slowest to fastest. first and second codon positions most closely align with “traditional partitioning”, while substantial variation exists among the 3rd codon position sites.



the iterative k-means algorithm provides a useful data-driven method to account for complex patterns of variation in rates of molecular evolution among sites. this is primarily because it tends to group together sites that evolve at similar rates of evolution, reducing the need for additional parameters to describe variation in rates across sites within a given group of sites . for example, in the crocodilian dataset  <cit> , although  <dig> subsets were selected in the partitioning scheme chosen with tiger rates for a dataset with just over  <dig>  sites, models with the gamma model of rate heterogeneity were never chosen, and the proportion of invariable sites parameter was chosen for only three subsets. in contrast, the partitioning scheme chosen with the greedy algorithm included  <dig> subsets with seven that used either gamma or proportion of invariable sites in the model. out of  <dig> total subsets selected using iterative k-means with tiger rates and evaluated with bic during our empirical evaluation,  <dig>  required the additional parameters of gamma, proportion of invariable sites, or both. in contrast, of the  <dig> subsets chosen with the partitionfinder greedy algorithm,  <dig>  of the models included gamma, proportion of invariable sites, or both. these results support recent observations that more flexible models of variation in rates among sites tend to fit the data much better than those that rely on distributional assumptions  <cit> , and suggest that the iterative k-means approach to partitioning may be particularly useful when the variation in rates across sites cannot be adequately modeled using a combination of traditional partitioning  and gamma-distributed rates  <cit> . other methods for accounting for this kind of heterogeneity exist and include the cat model, implemented into the program phylobayes  <cit>  and a “spike and slab” model recently described by wu et al.  <cit>  that has been implemented into beast  <dig>  <cit> . our method provides an alternative to these approaches.figure  <dig> 
average number of parameters per subset for different partitioning scheme estimation methods using bic. each line represents a different empirical dataset. “none” is no partitioning, “all” is the user partitioning scheme, “pf-g” is the partitionfinder greedy algorithm, “tiger” is iterative k-means using tiger site rates, “likelihood” is iterative k-means using likelihood site rates. the parameters per subset decrease for the k-means methods.



we evaluated the partitioning schemes chosen by the iterative k-means with tiger rates for the simulated alignments based on the criteria that, 1) alignment sites generated under the same model would be assigned to the same subsets, and 2) the correct number of subsets would be chosen. our results show that most subsets consisted primarily of sites generated from the same model . for example, in  <dig> out of  <dig> subsets , at least 95% of the sites in the subset were generated under the same model . however, the number of subsets varied from 12– <dig> , far more than the two subsets under which the data were simulated. to further understand this behavior, we examined the partitioning schemes generated after each iteration of the algorithm. we found that the first split often closely approximated the true model, but due to continual increases in the bic score, many more splits were accepted. this suggests that the inability to recover the true number of subsets could be due to the nature of the metrics for the evaluation of model fit. whatever the underlying reason for the over-partitioning of simulated datasets, these results suggest that when using methods like these to select partitioning schemes for empirical studies, it would be prudent to estimate phylogenetic trees under a range of intermediate partitioning schemes as well as the final partitioning scheme. an important next step in investigating these and other approaches to partitioning is a full-scale simulation study which examines a broad range of simulation conditions, and which assesses the effects of each not only on recovering the correct model, but also on recovering the correct tree.figure  <dig> 
assignment of sites using iterative
k
-means and tiger site rates for  <dig> simulated alignments. the different colors represent sites generated under different models. the number of subsets selected is variable while sites are most often clustered with other sites simulated under the same model.
the number of subsets selected using the iterative
k
-means algorithm for  <dig> simulated alignments in which  <dig> independent subsets were simulated



simulation replicate
k
-means subsets
number of subsets consisting of > =95% sites from same model


despite the failure of the k-means method to recover the correct number of subsets in simulated data, three factors suggest that this finding is unlikely to severely compromise the method. first, previous studies have shown that defining too many partitions may have negligible impact on downstream phylogenetic inferences such as tree topologies, bootstrap support, or branch lengths  <cit> . second, on empirical datasets, the k-means method tends to select a relatively modest number of subsets – never more than double the number of features in the dataset itself , and often many fewer. for example, for the mccormack et al. dataset  <cit> , there were  <dig> individual uce’s, and the k-means method selected just  <dig> subsets of sites. third, the k-means method selects partitioning schemes that make biological sense with respect to what we already know about variation in rates and patterns of evolution .

it is important to note that the iterative k-means algorithm represents a heuristic search for an optimal partitioning scheme. as such, it cannot be guaranteed to find the optimum partitioning scheme for any given dataset. furthermore, the k-means algorithm itself is somewhat stochastic in nature, and so it is likely that repeated analyses of the same dataset might lead to the estimation of partitioning schemes with very minor differences. although we have focused on dna sequence alignments in this study, the approach we describe can also be applied to amino acid alignments.

our research suggests that the iterative k-means algorithm is an improvement over traditional approaches to partitioning. accounting for variation of rates among sites has long been viewed as a vital part of modeling in phylogenetics  <cit> , and we have shown that using site rates to inform subset assignments results in substantial improvements in the aicc and bic scores of partitioning schemes, when compared to more commonly used methods. perhaps most importantly, the iterative k-means algorithm provides a data driven method for modeling patterns of molecular evolution in markers such as uce’s that have been difficult to model with traditional approaches.

CONCLUSIONS
partitioning remains the most commonly used method for accounting for variation in the rates and patterns of molecular evolution among sites in phylogenetic analyses. as the size and number of phylogenomic datasets grows, it is increasingly important to fit more realistic partitioned models to those datasets. the algorithm we present in this paper does this by automatically selecting a partitioning scheme for datasets of variable size and type without the need of an a priori determination of partition boundaries or number of desired subsets. although we identified potential pitfalls of using such algorithms , we also showed how these pitfalls could be overcome. these methods provide an important step forward in improving our approaches to modeling molecular evolution, particularly for very large datasets, as well as suggesting fruitful directions for further improvements.

availability of supporting data
all code, alignments, configuration input files, and output files have been uploaded to figshare . the modified partitionfinder code we developed for this manuscript is available at https://github.com/brettc/partitionfinder/tree/paul_develop and the tiger rates code that we developed is available at http://dx.doi.org/ <dig> /zenodo. <dig>  the data sets that were analyzed during empirical evaluation can be found at https://github.com/roblanf/partitionedalignments.

abbreviations
dnadeoxyribonucleic acid

rdnaribosomal deoxyribonucleic acid

rrnaribosomal ribonucleic acid

aicakaike information criterion

aicccorrected akaike information criterion

bicbayesian information criterion

uceultra-conserved element

competing interests

the authors declare that they have no competing interests.

authors’ contributions

pf, bc, and rl conceived the study. pf, bc, cm, and rl developed, coded, and tested the methods. pf performed the analyses on the empirical data sets. pf and rl wrote the manuscript. all authors read and approved the final manuscript.

