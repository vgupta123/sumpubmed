BACKGROUND
the past two decades have witnessed rapid advances in gene expression profiling with the microarray technology, which not only brighten the prospect of deciphering the complexity of disease genesis and progression at the genomic level, but also revolutionize the diagnostic, therapeutic, and prognostic approaches. up to recently, diagnostic classification and prognostic assessment have been based on conventional clinical and pathological risk factors, such as patient age and tumor size, many of which are believed to be secondary manifestation  <cit> . the advent of microarray technology allows researchers to explore primary disease mechanisms by comparing gene expression profiles for malignant and normal cells. the regularity and aberration in the expression patterns of certain genes shed light on their functions and pathological importance  <cit> . studies that seek to identify gene markers to refine diagnostic classification and improve prognostic prediction in the context of gene expression data have enriched the literature  <cit> . in recent years, researchers have realized that gene markers identified from microarrays drawn from difierent studies on the same disease across similar cohorts lack consistency  <cit> . a possibly more effective means to resolve this problem is to employ a network-based approach, that is, to identify markers as gene subnetworks, defined as groups of functionally related genes based on a gene network, instead of treating individual genes as completely independent and identical a priori as in most existing approaches  <cit> . a novel network-based approach proposed recently  <cit>  can be summarized as follows:  randomly searching subnetworks and assigning a score to each subnetwork that characterizes the subnetwork-wise gene expression level;  identifying significant subnetworks that can well discriminate the clinical outcome;  constructing a classifier based on the significant subnetworks with a conventional statistical tool, such as logistic regression. essentially such a network-based approach aggregates gene expression data at the subnetwork level and then identifies and utilizes some significant subnetworks. it has been shown that such a network-based approach not only improves predictive performance and reproducibility, but also sheds biological insights into molecular mechanisms underlying the clinical outcome. however, the above method is largely heuristic without a formal statistical framework; more importantly, it involves a random search over subnetworks, leading to possibly different results from different runs with no guarantee of the optimality of the final result. because of the ever-increasing popularity of penalization methods for high-dimensional data, we propose a novel network-based penalty to be used with the hinge loss, leading to a network-based support vector machine. while maintaining some desirable properties of support vector machine  with the hinge loss function, the network-based penalty directly integrates a biological network to realize more effective variable selection, as compared with generic methods, such as the standard svm  or l1-penalized svm .

the support vector machine  is one of the most popular supervised learning techniques with wide-ranging applications  <cit> . in particular, previous studies have demonstrated its superior performance in gene expression data analysis, especially its ability to handle high dimensional data  <cit> . nevertheless, with categorical predictors, both the std-svm and the l1-svm may have some shortcomings. zou and yuan  <cit>  applied the concept of grouped variable selection and developed an f∞-norm penalized svm to realize simultaneous selection/elimination of all the features derived from the same categorical factor . their numerical examples showed that the f∞-norm svm outperformed the l1-svm in factor-wise variable selection. we extend the idea of variable grouping to gene networks: rather than grouping all the dummy variables created from the same categorical factor, we treat two neighboring genes in a network as one group. the network-based penalty is constructed as the sum of the f∞-norms being applied to the groups of neighboring-gene pairs. with the hinge loss penalized by such a network-based penalty as our objective function, we obtain our network-based svm. the later sections are organized as follows. we begin with a brief review of the svm, and then introduce our proposed network-based svm. we evaluate its performance by simulation studies in both low dimensional and high dimensional data settings as well as two real data applications. the last section concludes the paper with a brief summary.

methods
existing methods
suppose we have training data {}i=1n with xi ∈ ℝp and yi ∈ { <dig>  -1}. define a hyperplane {x : f= xtβ + β <dig> = 0}. the classification rule induced by f  is sign . svm searches for such a hyperplane f^=xtβ^+β^ <dig> that maximizes the margin between the training data points for class  <dig> and class -1:

  max⁡β,β01‖β‖2subject toyi≥1−ξi,∀iξi≥ <dig> ∑i=1nξi≤c 

where ξi are slack variables, and c is a tuning parameter to be determined. the std-svm has an equivalent hinge loss + penalty formulation as an optimization problem  <cit> :

  min⁡β <dig> β{∑i=1n++λ‖β‖22} 

where the subscript "+" denotes the positive part, i.e., z+ = max{z, 0}, ‖β‖22=∑k=1p|βk| <dig>  and λ is the tuning parameter. the solution to  is the same as that to .

the above std-svm forces all nonzero coefficient estimates, which leads to the problem of its inability to conduct variable selection. the l1-svm was proposed to accomplish the goal of variable selection. it can be formulated as

  min⁡β <dig> β{∑i=1n++λ‖β‖1} 

where ‖β‖1=∑k=1p|βk|. the l1-svm wins over the std-svm when the true model is sparse, while the std-svm is preferred if there are not many redundant noise features  <cit> .

zou and yuan  <cit>  pointed out the shortcoming of the l1-norm penalty: even though it encourages parsimonious models, it fails to guarantee successful models in cases of categorical predictors due to the fact that each dummy variable is selected independently. they applied the concept of grouped variable selection and proposed an f∞-norm svm to realize simultaneous selection/elimination of features derived from the same factor so as to accomplish automatic factor-wise variable selection. suppose we have g factors f <dig> ...,fg. from each factor fg, we generate a feature vector x=,⋯,xj,⋯,xng)t.

correspondingly we have the coefficient vector β=,⋯,βj,⋯,βng)t. therefore,

  f=xtβ+β0=∑g=1gxtβ+β <dig> 

define the f∞-norm of fg as

  ‖fg‖∞=‖β‖∞=max⁡j∈{ <dig> ⋯,ng}{|βj|} 

the f∞-norm svm is formulated as

  min⁡β <dig> β{∑i=1n++λ∑g=1g‖β‖∞} 

the most noteworthy property of the f∞-norm svm is its guarantee of sparsity at the factor level. due to the singularity property of the infinity norm: || β ||∞ is not differentiable at β =  <dig>  β will be exactly zero if the regularization parameter λ is properly chosen  <cit> . therefore, the f∞-norm svm automatically eliminates factors that are completely irrelevant to the response, and thus achieves the goal of factor-wise selection. the empirical evidence shows that the f∞-norm svm often outperforms both the l1-svm and the std-svm.

new method
biological observations reveal that neighboring genes in a network tend to function together in biological processes. to incorporate this prior information, a network-based svm for binary classification is proposed to facilitate generating models that extract more biological insight from gene expression data. the penalty term that characterizes the network structure can be specified by implanting the f∞-norm into the context of known functional interrelationships among genes by considering each pair of the functionally related genes as one group.

consider a gene network with s denoting the set of all edges, i.e., the pair of connected genes.

s = { : gene j <dig> and gene j <dig> are connected}

define wk as some weight for gene k. for example, wk = dk where dk is the number of direct neighbors of gene k, or wk = dk, or simply wk =  <dig> for all genes. we propose a novel penalty in the form of

  ∑∈smax⁡{|βj1|wj <dig> |βj2|wj2} 

thus the network-based svm solves the optimization problem as follows.

  min⁡β <dig> β{∑i=1n++λ∑∈smax⁡{|βj1|wj <dig> |βj2|wj2}} 

four properties of the penalty term are noteworthy. first, the regularization is performed at the level of grouped genes with each group containing two neighboring genes in the network. in the case of penalized linear regression, it has been proven that this penalty achieves the goal of eliminating both βj <dig> and βj <dig> simultaneously if  ∈ s  <cit> . the automatic selection of grouped features is due to the singularity of function max{|a|, |b|}  <cit> . this formulation satisfies our assumption that neighboring genes tend to  contribute to the same biological process at the same time. second, the choice of the weight depends on the goal of shrinkage and influences the predictive performance. consider a network comprised of several subnetworks, each with one regulator and ten target genes. because of the singularity of function max at a = b, the weighted penalty in the context of penalized regression, encourages |βj1|/wj1=|βj2|/wj <dig> <cit> . here we examine three weight functions in particular: wk =  <dig>  wk = dk, and wk = dk, where gene k has dk direct neighbors. the new method encourages |βj1|=|βj2| if wk =  <dig>  |βj1|dj1=|βj2|dj <dig> if wk = dk, and |βj1|dj1=|βj2|dj <dig> if wk = dk. therefore, heavier weights  favor genes with more direct neighbors to have larger coefficient estimates; in other words, heavier weights relax the shrinkage effect for those regulators, which are known to be biologically more important. due to this property, the choice of a heavy weight, as a simple strategy, enables us to alleviate the bias in the coefficient estimates from the penalization method and possibly improve the p predictive performance. our default weight is wk = dk. the weight, considered as another tuning parameter, can be determined from cross-validation or an independent validation data set, though we do not consider it here. third, the penalty term, under certain conditions, tends to encourage a grouping effect, where highly correlated predictors tend to have similar coefficient estimates  <cit> . fourth, the penalty is linear, which allows the solution to be found by the linear programming  technique that is computationally convenient.

as usual, the fitted classifier is f^=β^0+xtβ^, and the classification rule is sign). we employ lp to obtain the solutions to  by

  min⁡β <dig> β∈sm) 

subject to

  yi)≥1−ξi,ξi≥0∀iβj+wj+βj−wj≤m,j=j <dig> j2∀∈sβj+≥ <dig> βj−≥ <dig> j=j <dig> j2∀∈s 

where

  ξi=+,i= <dig> ,...,n 

  m=max⁡{|βj1|wj <dig> |βj2|wj2} 

and βj=βj+−βj−, in which βj+ and βj− denote the positive and negative parts of βj. the calculation of the new method can be easily implemented by the r package lpsolve, so is the computation of the l1-svm. the r package e <dig>  is used to obtain the solution to the std-svm.

RESULTS
simulation
we conducted several simulation studies to numerically evaluate the performance of the network-based svm along with the std-svm and l1-svm. the simulation setups were similar to those in  <cit> . we started from a simple network consisting of  <dig> subnetworks, each having a regulator gene t  that regulated  <dig> target genes, leading to a total of  <dig> genes . we assumed that two out of the five subnetworks were informative; that is, the coefficients of  <dig> genes were nonzero and thus informative to the outcome, while the remaining  <dig> noise genes had no effect on the outcome. we generated a simulated data set by the following steps:

• generate the expression level of regulator gene t, xt ~ n , t =  <dig> ...,  <dig>  independently.

• assume that the expression level of regulator gene t and each of its regulated genes follow a bivariate normal distribution with correlation  <dig> . thus, the expression level of each target gene regulated by gene t, xl ~ n, l =  <dig> ...,  <dig> and t =  <dig> ...,  <dig> 

• generate the outcome y from a logistic regression model: logit ) = xtβ + β <dig>  β0=  <dig>  where x is the vector of the expression levels of all the genes, and coefficient vector β=,...,β <dig> ...,β <dig> ...,β10).

four sets of true coefficients, β 's, were specified to reflect four scenarios:

 <dig>  β=..

the effect of one informative subnetwork was the same as the other in magnitude but with an opposite direction.

 <dig>  β=..

both informative subnetworks had positive effects but in different magnitudes.

 <dig>  β=..

target genes in the same informative subnetworks had both positive and negative effects.

 <dig>  β=..

it was similar to but more extreme than scenario  <dig> 

five methods, std-svm, l1-svm, and network-based svm with wk =  <dig>  wk = dk, and wk = dk, were compared based on the results averaged over  <dig> runs under each of the above four scenarios. for each run,  <dig> observations were simulated as training data to build a classifier , another  <dig> for tuning the regularization parameter λ, and the last  <dig>  as test data. each predictor was normalized to have mean  <dig> and standard deviation  <dig>  given any value of λ, we obtained the coefficient estimates from the training set, then applied the classifier to the tuning set to find the classification error. we searched for λ^, from a wide range of prespecified values, which produced the smallest classification error. the classifier corresponding to λ^ was identified as the fitted classifier f^. then we applied f^ to the test set and calculated the test error, the number of misclassifications divided by the test sample size. table  <dig> reports the mean classification error of the test set and its standard error , the standard deviation of the classification errors divided by the square root of the number of runs, for each method over  <dig> runs under each scenario. to evaluate each method's ability to select informative genes, we examined the false negatives, defined as the number of informative genes whose coefficients were estimated to be zero. in addition, we also considered a smaller sample size: we repeated the entire process with  <dig> training data points,  <dig> tuning data points, and again  <dig>  test data points. the network-based svm is named as "new" in the table.

according to our simulation setups, the correct weight function should be w = d. however, we find that the new method with w = d overwhelmingly beat all other methods in all the setups. it consistently made the most accurate classifications and missed no informative genes. the new method with w = d performed the second best: in most cases, it improved the classification accuracy over std-svm and l1-svm; and under all the settings, it produced models that identified more informative genes than the l1-svm. in contrast, w =  <dig> did not bring much gains over the std-svm or the l1-svm. the l1-svm led to models that were too sparse, missing about  <dig> and  <dig> informative genes for n =  <dig> and n =  <dig> respectively. the superior performance and the larger model size of the heavy weight  compared with its counterparts  is presumably due to its relaxation of the shrinkage effect. the penalization methods shrink the β^ toward zero by imposing the constraints  and therefore introduces bias to β^. by grouping neighboring genes, the new method encourages the pairwise weighted absolute coefficients to be equal. therefore, a heavy weight leads to larger |β^| for regulator genes. by choosing a heavier weight, we may overcome over-shrinkage, alleviate biases, and achieve better classification accuracy to some extent at the expense of model sparsity. as shown by table  <dig>  w = d produced the largest |β^| for regulators than its two counterparts. the l1-svm estimates were treated as a yardstick for comparison as to provide an idea of the extent of shrinkage by each weight function. for example, w =  <dig> and w = d overly shrank all the regulators under all scenarios as compared with the l1-svm estimates. note that the binary outcome y was generated from a logistic regression model while β^ was estimated from a linear model, hence e may be different from β even for an unbiased estimator β^ of the linear model.

next, we evaluated the performance of the new method for high-dimensional data with large p. we used the setup of  <dig> observations for training,  <dig> for tuning, and  <dig>  for test data. we assumed that  the network was composed of either  <dig> or  <dig> subnetworks, each having one gene regulating  <dig> target genes;  the first  <dig> subnetworks were informative resulting in  <dig> informative genes;  the rest of the genes had no effect on the outcome, leading to  <dig> noise genes when p =  <dig> and  <dig>  noise genes when p =  <dig>  100; and  the true β was specified as in scenario  <dig>  table  <dig> shows the simulation results averaged over  <dig> runs. again, we see the gains from using a heavy weight . it prevailed over all the other methods in making accurate classifications and selecting informative genes. the w = d ranked the second. however, w = d generated models much larger than those from other methods except std-svm. in this case, the performance of w =  <dig> is no better than l1-svm possibly due to over shrinkage of the effects of the regulator genes.

applications to microarray data
to evaluate its performance in the real world, we applied the new method to two microarray gene expression data sets related to the parkinson's disease   <cit>  and breast cancer metastasis   <cit>  respectively.

parkinson's disease
the data set includes the parkinson's disease status and the expression levels of  <dig>  genes from  <dig> patients   <cit> . we used the same network structure as  <cit> . the network combines  <dig> kyoto encyclopedia of genes and genomes  regulatory pathways and contains a total of  <dig>  genes and  <dig>  edges. the data were randomly split into training , tuning , and test  sets. the expression level of each gene was normalized to have mean  <dig> and standard deviation  <dig> across samples. the tuning parameter was identified from the tuning set and the performance of the method was evaluated on the test set by the mean classification error and its standard error averaged over  <dig> runs. five methods were compared: std-svm, l1-svm, network-based svm with w =  <dig>  w = d, and w = d. to obtain a final model based on the new method with w = d, we combined, for each run, the previous tuning and test data as the new tuning set leading to a sample size as large as  <dig> observations, on which the classification errors were calculated for wide-ranging values of the tuning parameter. then after  <dig> runs, we had an averaged classification error corresponding to each tuning parameter value. the value that generated the minimal averaged error was the one we selected to fit the final model to all the data. note that the classification error rate from the final model was likely to be biased due to the double use of the data for training/tuning and test; the main purpose of fitting the final model was to see the selected genes at the end.

first, we focused on the  <dig>  genes that appeared in the network with the largest variations of expression levels . according to the kegg pathway of parkinson's disease  <cit> ,  <dig> genes play a role in the disease progression, five of which  belong to the  <dig>  genes. in addition to the classification error, we added two additional criteria for method comparison: the number of disease genes identified, and the number of genes identified. table  <dig> shows that std-svm made the most accurate classification, even though the difference with other methods was perhaps non-significant. the w = d ranked the second in predictive performance while produced a model including  <dig>  genes on average. in this case, the w = d gained advantage: it selected more disease genes by a relatively sparse model with a classification error non-significantly larger than std-svm. from the  <dig>  genes, with the final model the new method identified  <dig> genes including one disease gene.

next, to better integrate the biological observation of the kegg pathway and the known network structure of  <cit> , we restricted our analysis to the first- and second-order-neighbors of the  <dig> disease genes on the parkinson's disease kegg pathway whose expression levels and network structure are available. the first-order-neighbor subnetwork  was composed of the  <dig> disease genes and their  <dig> direct neighbors. the second-order-neighbor subnetwork  comprised the pd-1nb-net as well as the direct neighbors of the  <dig> direct neighbors of the disease genes, leading to a total of  <dig> genes. figure  <dig> displays the two subnetworks. we conducted the analysis in the same way as described above. the only difference resided in that this time only genes appearing in the pd-1nb-net and pd-2nb-net were included in the analysis. table  <dig> shows the results.

we see the gains from employing the new method when narrowing down our focus on the pd-1nb-net and pd-2nb-net. for the pd-1nb-net, w =  <dig> and w = d performed equally well. they had the smallest classification error and identified one more disease gene through a model slightly larger than the one obtained from l1-svm. the new method with w = d won over in the case of pd-2nb-net with the best accuracy and most selected disease genes. the w = d ranked the second in terms of the prediction accuracy while detecting  <dig> more disease genes by a model with  <dig> more genes than that of the l1-svm. this means that the new method was able to identify more clinically relevant genes while keeping the same number of noise genes in the model as l1-svm. in both subnetworks, the final models included all the genes.

breast cancer metastasis
the breast cancer metastasis data set  <cit>  contains expression levels of  <dig>  genes for  <dig> patients,  <dig> of whom were detected to develop metastasis within a 5-year follow-up after surgery. tp <dig>  brca <dig>  and brca <dig> are three human genes that belong to the class of tumor suppressor genes, which are known to prevent uncontrolled cell proliferation, and to play a critical role in repairing the chromosomal damage. certain mutations of these genes lead to increasing risk of breast cancer. we explored the protein-protein interaction  network previously used by  <cit> . the ppi network comprises  <dig>  interactions among  <dig>  proteins, obtained by assembling various sources of experimental data and curation of the literature  <cit> . we confined our analysis to the direct or first-order neighbors  of the three cancer genes, and the subnetwork composed of two parts : the direct neighbors of tp <dig>  and the second-order neighbors of brca <dig> and brca <dig>  we fit the final model and compared the four methods in terms of classification error, cancer genes selection, and model sparsity. the cancer genes are the  <dig> known or putative cancer genes with estimated mutation frequencies in cancer samples . a total of  <dig> genes that fell into the bc-1nb-net had observed expression levels, among which were  <dig> cancer genes and  <dig> cancer genes  with mutation frequencies larger than  <dig> . the bc-2nb-net was composed of  <dig>  genes,  <dig>  of them with observed expression levels, including  <dig> cancer genes. besides the  <dig> included in bc-1nb-net,  <dig> additional cancer genes  that had mutation frequencies larger than  <dig>  belonged to bc-2nb-net.

for bc-1nb-net, w = d had the advantage in selecting cancer genes and those with large mutant frequencies . the w = d detected more clinically relevant genes by a sparser model while reaching a comparable classification error rate to that of l1-svm. even though the final model was parsimonious, it included  <dig> cancer genes, one of which had a large mutation frequency. for bc-2nb-net, the new method with w = d detected more cancer genes with equally accurate predictions while maintaining a sparse model compared with l1-svm. the final model included only  <dig> genes out of  <dig> , two of which were cancer genes with one having a large mutation frequency.

CONCLUSIONS
the advancement in the microarray technology has enriched the tool kit of researchers to decipher the complexity of disease mechanisms at the genomic level. studies have been widely conducted to identify genetic markers to better the diagnostic classification and prognostic assessment, largely by ignoring biological knowledge on gene functions and treating individual genes equally and independently a priori. the downside of such an endeavor has been realized; for example, gene markers identified across similar patient cohorts for the same disease in such a way often lack consistency. as a viable alternative, the network-based approach has been gaining popularity. in addition to improving predictive performance and gene selection, the network-based approach extracts more biological insights from high-throughput gene expression data. here we have proposed a network-based svm, with a penalty term incorporating gene network information, as a practically useful classification tool for microarray data. our simulation studies and two real data applications indicate that the proposed method is able to better identify clinically relevant genes and make accurate predictions.

list of abbreviations used
svm: support vector machine; std-svm: standard support vector machine; l1-svm: l1-penalized support vector machine; lp: linear programming; pd: parkinson's disease; bc: breast cancer; kegg: kyoto encyclopedia of genes and genomes; ppi: protein protein interaction

competing interests
the authors declare that they have no competing interests.

authors' contributions
yz implemented the methods, did all the experiments and drafted the paper. xs and wp initiated the project. all participated in the writing of the article.

