BACKGROUND
we are witnessing an unprecedented increase in the number of biomedical abstracts, experimental results and phenotype and gene descriptions being deposited to publicly available databases, such as ncbi's pubmed. collectively, this content represents potential new discoveries that could be inferred with appropriately designed natural language processing approaches. identification of topics that appear in biomedical research literature was among first computational approaches to predict associations between diseases and genes and has become indispensable to both researchers in the biomedical field and curators  <cit> . information from publication repositories is often mined together with other data sources. databases that store relations from integrative mining are for example the omim database on human genes and genetic phenotypes  <cit> , the generif function annotation database  <cit> , the gene ontology  <cit>  and clinical drug information from the dailymed database  <cit> . biomedical mining of literature is a compelling way to identify possible candidate genes through integration of existing data.

a dedicated set of computational techniques is required to infer structured relations from plain textual information stored in large literature databases  <cit> . relation extraction tools  <cit>  can identify semantic relations between entities found in text. early relationship extraction systems relied mostly on manually defined rules to extract a limited number of relationship types  <cit> . later, machine learning-based methods were introduced to address the extraction task by inferring prediction models from sets of labeled relationship types  <cit> . when no labeled data were available, unsupervised systems were developed to extract relationship descriptors based on the language syntax  <cit> . current state-of-the-art systems combine both machine learning and rule-based approaches to extract relevant information from narrative summaries and represent it in a structured form  <cit> .

this paper aims at the extraction of gene regulatory networks of bacillus subtilis. the reconstruction and elucidation of gene regulation networks is an important task that can change our understanding of the processes and molecular interactions within the cell  <cit> . we have developed a novel sieve-based computational methodology that builds upon conditional random fields  <cit>  and specialized rules to extract gene relations from unstructured text. extracted relations are assembled into a multi-relational gene network that is informative of the type of regulation between pairs of genes and the directionality of their action. the proposed approach can consider biological literature on gene interactions from multiple data sources. the main novelty of our work here is the construction of a sequential analysis pipeline for extracting gene relations of various types from literature data . we demonstrate the effectiveness and applicability of our recently proposed coreference resolution system  <cit> . our system uses linear-chain conditional random fields in an innovative way and can detect distant coreferent mentions in text using a novel transformation of data into skip-mention sequences.

we evaluate the proposed methodology by measuring the quality of extracted gene interactions that form the well studied regulatory network of sporulation in bacteria b. subtilis. sporulation is an adaptive response of bacteria to scarce nutritional resources and involves differential development of two cells  <cit> . many regulatory genes that control sporulation or direct structural and morphological changes that accompany this phenomenon have been characterized in the last decade  <cit> . the topology of bacterial sporulation network is stable and suffers no controversy; thus, it is appropriate to serve as a reference network against which the performance of relation extraction algorithms can be compared. our evaluation demonstrates that the proposed approach substantially surpasses the accuracy of current state-of-the-art methods that were submitted to the gene regulation network  bionlp-st  <dig> challenge . the source code of our approach is freely available  <cit> . in this paper we represent a network extraction algorithm, which is an improvement on our winning submission to bionlp  <dig>  <cit> . with these improvements we have been able to further reduce the prediction error from  <dig>  to  <dig> , measured as the slot error rate . this paper substantially extends our previous work  <cit> . below, we discuss motivation for using skip-mention sequences by analyzing distributions of distances between various parts of text  that are used by specialized sieves. we further explain feature functions and rules as they are key components of the system. we analyze the number of relations extracted by each sieve. the approach described here adds a new conditional random fields  sieve to detect direct relations between b. subtilis genes that are "hidden" as target mentions within events. to better address text from biomedicine, we use the biolemmatizer  <cit>  instead of a general lemmatizer. we incorporate an additional knowledge resource - b. subtilis protein-protein interaction network from the string database  <cit> , which is used within the new feature function bsubtilisppi.

we use the term sieve to represent a separate relationship processing component. as we may extract new relationships or delete them in each of the sieve, the term might not be well selected but we left the terminology to comply with the previously published conference paper  <cit>  and the coreference resolution system  <cit>  that inspired the architecture of our proposed system.

related work
research in the field of relationship extraction focuses on extraction of binary relationships between two arguments. new systems are typically tested using social relationships in the automatic content extraction  evaluation datasets  <cit> , where the goal is to select pairs of arguments and assign them a relationship type. machine learning approaches that have been used for relationship extraction include sequence classifiers, such as hidden markov models  <cit> , conditional random fields  <cit> , maximum-entropy markov models  <cit>  and binary classifiers. the latter usually employs support vector machines   <cit> .

the ace  <dig> dataset  <cit>  consists of two-level hierarchical relationship types. a relationship could have another relationship as an argument and a second level relationship can have only non-relationship-like arguments. two-level relationship hierarchies could have a maximum tree height of two. wang et al.  <cit>  proposed a system that uses a one-against-one svm classifier to classify relationships in the ace  <dig> dataset by employing wordnet  <cit> -based semantic features. the grn bionlp  <dig> shared task aimed to detect three-level hierarchical relationships. these relationships are interactions that connect events or other types of interactions as arguments. in comparison to the pairwise technique  <cit> , we extract relationships using linear-based sequence models and manually defined rules.

a relation could be written using forms in unstructured text. machine learning techniques try to learn diverse relations by adapting models against large datasets and by exploiting informative text features. the features are instantiated by a predefined set of feature functions, which are applied on a specific dataset. a technique to overcome a low number of instances of diverse relationship forms was proposed by  <cit> . they proposed lexical-syntactic feature functions based on patterns that are able to identify dependency heads. the proposed solution was evaluated against two relationship types and two languages, where they achieved promising results. in this work we define manually assigned rules to overcome the heterogeneity of the relationship representation.

text used for training a relationship extraction model is most often tagged using the iob  notation  <cit> . in the iob, the first occurrence of the relationship word is labeled as b-rel, second and later consecutive tokens, which also represent relationships are labeled as i-rel, and all other tokens are o. part of the text that most closely identifies a known relationship between the two arguments is referred to as a relationship descriptor. li et al. <cit>  used a linear-chain crf model to label such descriptors. they first changed the subject and object arguments of the accompanying relationships into a specific value . this transformation enabled them to correctly identify direction of a relationship. moreover, they also merged all the tokens from a relationship descriptor into a single token, which enabled them to use long distance features using a linear model representation. we employ an analogous model representation, but transform a sequence of tokens in an innovative way that enables us to extract the target relationship type between the arguments and not just a relationship descriptor. banko and etzioni  <cit>  also employed linear-based classifiers for the open relationship extraction problem, that is, the identification of a general relationship descriptor without regard to any target relationship type. first, they analyzed specific relationship types in the text taking into account lexical and syntactic features and then they learned a crf model against with synonym identification  <cit> . their approach is useful in scenarios where only a very limited number of relationships are known. traditional relationship extraction methods can perform better if our goal is a high value of recall. for this reason we focus on supervised relationship extraction model.

relationship extraction methods in biomedicine have been evaluated at several shared task challenges. the lll - learning language in logic challenge on gene interaction extraction  <cit>  is related to the bionlp  <dig> gene regulatory networks shared task, which includes a subset of the lll data with some additional annotations. for the lll task, giuliano et al.  <cit>  used a svm classifier and proposed a specialized local and global svm kernel that uses neighboring words as contextual information. the local kernel was based solely on mention features, such as words, lemmas or part-of-speech  tags. in contrast, the global kernel used tokens on the left side of, between and on the right side of pairs of mentions that represent candidate arguments. to identify relationships, giuliano et al. processed documents that contained at least two candidate attributes and generated nk example instances, where n was the number of all mentions in a document and k was the number of mentions that constituted a relationship . giuliano et al. used their model to predict either a non-existing relationship, a subject-object relationship or an object-subject relationship. on a related note, we propose the usage of contextual features and syntactic features that depend on neighboring words. however, we predict unoriented extracted relationships and then determine their directionality, i.e., the subject and object arguments, through manually defined rules.

survey of bionlp shared tasks
the bionlp shared task challenges follow an established research-wide trend in biomedical data mining towards the specific information extraction tasks. challenge events have been organized thus far in  <dig>  <cit> ,  <dig>  <cit>  and  <dig>  <cit> , each co-located with the bionlp workshop at the association for computational linguistics  conference. the first event triggered active research in the biomedical community on various information extraction tasks. second shared task focused on generalizing text types and domains, and on supporting different event types. the most recent shared task took a step further and addressed the information extraction problems in semantic web, pathways, cancer-related molecular mechanisms, gene regulation networks and ontology populations.

the bionlp  <dig> entity relations challenge focused on the entity relationship extraction. the best performing system, called tees  <cit> , used a pipeline with svms for the detection of entity nodes and relation prediction that was followed by post-processing routines. it predicted relationships between every two candidate mentions within a sentence. the evalution showed that the term identification step could strongly impact on the performance of the relationship extraction module. in our case, proteins and mentions of entities, these are mentions that represent genes, were identified prior to the beginning of the challenge, and thus, our work here focused on the extraction of events, relations and event modification mentions.

in this work we describe the method that we developed while participating in the bionlp  <dig> gene regulation network shared task  <cit> . we report on several refinements of our approach that were introduced after the shared task ended and that allowed us to further improve its predictive performance. the goal of the grn task was to extract gene interactions from research abstracts and to assemble a gene network, which was informative of gene regulation. training data contained manually labeled texts obtained from research articles that contained entity mentions, events and interactions between genes. entities were text sequences that identified entities, such as genes, proteins or regulons. events and relationships were defined by their type, two connected arguments  and the direction between the arguments. given a test dataset, our goal was to predict relations describing various types of gene interactions. predicted network of extracted gene interactions was matched with the reference gene regulatory network and scored using a slot error rate   <cit> . the ser measures the proportion of incorrect predictions relative to the number of reference relations.

methods
in this section we present our proposed sieve-based system for relation extraction. we start by describing the linear-chain conditional random field  model and proceed by extending it with a novel data representation that relies on skip-mentions. we provide support for transforming data into skip-mention sequences by studying various mention distributions that are used by crf-based sieves. we then overview feature functions used by our model and explain the sieve-based system architecture, which is an end-to-end procedure that consists of data preprocessing, linear-chain crf execution, rule-based relationship identification and data cleaning.

conditional random fields with skip-mentions
crf  <cit>  is a discriminative model, which estimates distribution of the objective sequence y conditioned on the input sequence x, that is, p. following is an example of the input sequence from the grn bionlp  <dig> training dataset, where the potential attributes  are shown in bold:

"spo0h rna and sigma h levels during growth are not identical to each other or to the pattern of expression of spovg, a gene transcribed by e sigma h."

the corresponding objective sequence for this example is y - , which also corresponds to tokens in x - . thus, both sequences are of the same length.

we retrieve additional information for input sequence x and generate sequences xlemma, xparse, xpos that contain lemmas, parse trees, tokens and part-of-speech tags for each corresponding token in x. the crf considers feature functions fj, where j denotes j-th feature function, j =  <dig>   <dig>  . . . , m . feature functions employ text sequences to model target sequence y. the design of appropriate feature functions is the most important step in training crf models. they contribute substantially to the improved performance of the system. we implement feature functions as templates and generate the final feature set by evaluating feature functions on a training dataset. the feature functions used by our model are described in the following section.

training of a crf model involves estimating the most probable objective sequence y ^ˆ given the input x. in particular, we estimate

 y ^=argmaxp,y 

where w is a vector of model parameters, weights, that have to be learned. here, the conditional distribution p is written as

 p=exp∑j=1mwj ∑i=1mfjc, 

where n represents the length of input sequence x, m the number of feature functions and c is a normalization constant over all possible objective sequences y. here, fj  denotes a j-th feature that is fired for i-th place in the input sequence. in our computations we avoid the need of computing normalization constant c. instead of using the exact probabilities we rather rely on ranking of the sequences relative to their probabilities and return a sequence that is ranked first. use features that are fired at least five times on the training data .

the structure of a linear-chain model depends on the references to the target sequence labels that are used by the input feature functions. figure  <dig> shows the graphical representation of the linear-chain crf model. from the figure we can observe that the i-th factor can depend only on the current yi label and the previous label yi− <dig> in a sequence. the training of linear crfs is fast and efficient. this is in contrast to more complex crf models, whose model inference is in general intractable and requires approximate probabilistic methods.

model definition
we formulate the task of relationship extraction as identification of relationships between two arguments. linear-chain crf model with standard data representation lacks the modeling of dependencies between mentions on longer distances . by analyzing the example from the previous section, "gene transcribed by e sigma h", we conclude that untransformed data representation can only identify relationships between two consecutive tokens. thus, we cannot extract all possible relationships using a linear model. rather than extracting relationship descriptors , we would like to extract categorized relationships between pairs of mentions. to overcome the limitation of linear models, we introduce new sequences that contain only mentions. we refer to these sequences as mention sequences. mentions are a type of arguments that can form a relationship. in figure  <dig> we present a conversion of the text excerpt into a mention sequence. transformed sequence x consists of consecutive entity mentions. notice that entity mentions are included in the training dataset.

we label target sequence y with the name of a relationship  or with the none symbol  when no relationship is present. each relationship label represents a relationship between the current and the previous mention.

from the mention sequence generated in figure  <dig>  we cannot identify relationships between mentions that are not consecutive. this limitation becomes exacerbated when mentions that are arguments of a certain relationship appear on longer distances. for example, mentions spovg and e sigma h should be related via the interaction.transcription relationship. however, this relationship cannot be extracted from representation that considers only consecutive mention pairs. furthermore, a linear model can only detect relationships between directly consecutive mentions. to overcome this problem, we introduce a novel sequence representation called skip-mention sequences. the number of skip-mentions defines the number of mentions from the original text that exist between two consecutive mentions in a given skip-mention sequence. thus, the original mention sequence  is a zero skip-mention sequence, because there are zero other mentions between any two consecutive mentions. this is opposed to a one skip-mention sequence, which considers relationships that are one mention apart. for example, to prepare the input data for extracting relationships between every second mention, we create two one skip-mention sequences for each input document. in the example in figure  <dig> we extract relationship interaction.transcription based on one skip-mention sequence.

in a general setting we consider skip-mention sequences for mentions at distance s. for a given skip-mention number, s, we create s +  <dig> mention sequences of length ns. after the sequences are created, one independent linear-chain crf model is trained for each value skip-mention number. as the generated sequences are independent, we can infer prediction models in parallel. from the models we read the extracted relationships between the mentions and form an undirected graph, where each connected component represents a relationship. figure  <dig> shows a high level representation of data flow and relation extraction used in our approach. the time complexity of the proposed method is mainly determined by the time needed for training linear crf models, since other routines can be run in linear time. due to the parallel execution of the for loop , we need to find the longest lasting execution. let us suppose that crf training and inference has time complexity of o  <cit> , where e is the number of edges in the graph, l is the number of labels, and q is the size of the maximal clique. in our type of crf model, we use one label for each relationship type. the number of edges e depends on the sequence input to the algorithm. let further assume there are n mentions in a document, which results in a zero skip-mention sequence with 2n −  <dig> = o edges. moreover, every other generated s skip-mention sequence contains s2ns-1=2n-s=o edges. we conclude that by employing parallelization, crf models would use o = o of time . in addition to other linear time procedures, it is also important to consider the time for initialization of feature functions, which takes on the order of o, where m is the number of input feature functions. figure  <dig> shows the distribution of distances between the relationship mention arguments  from the bionlp  <dig> gene regulatory network training dataset. the labeled arguments represent entity mentions or events, depending on the sieve setting. event is a type of relation that contains only mentions as their attributes. events are extracted using the event extraction sieve. the distribution of distances between mentions is shown in the part a of figure  <dig>  in the sieve  we identify relationships that have only mentions as their attributes . in the training data there are  <dig> relations that have another relation or an event as their attribute. of these, there are  <dig> such relations that have another relation as their attribute. seven contain a regular relation as an attribute, while four represent negated relations, which are not scored. relations that contain events as attributes are extracted by the event relations processing sieve  and the distribution of distances between the attributes is shown in part c of the figure. to use the same approach as for the other sieves, we transform events into mentions  for details). since hierarchies of events or relations are not considered in model evaluation, we include the gene relations processing sieve . sieve  extracts relations only between mentions, that are identified as b. subtilis genes. the distribution of distances between such mentions is presented in part d in the figure. we notice a drop of number of relationships on distance one for parts a, b and c. this is due to the fact of all the mentions we take into account when forming mention sequences. differently, in part d, we take only gene mentions into account which also results in not having a drop at distance one.

from all of the distance distributions we observe that relationships are mostly connected by the attributes on distance of two entity mentions. these distributions demonstrate the need to transform our data into skip-mention sequences. without the transformation the linear-chain crf model would, at best, uncover relations with attributes at zero distance .

for our final results we train the linear crf models against skip-mention sequences from zero to ten skip-mentions. we decide to use this range after observing the distance distributions between attributes of the relations. by using up to ten skip-mentions we can retrieve most of relations and do not overfit the model. the findings in our previous work  <cit>  show that after reaching the tail of distance distributions the results do not further improve.

the feature functions that we consider are thoroughly explained in table  <dig> and table  <dig>  the tables contain short descriptions of the functions and parameters that are used for their instantiation. additionally, the feature function generators generate a number of different functions from the training data and for them we also include the label types from which they are generated.

the feature functions are used by all crf-based sieves for all selected skip-mention crf models. all extracted features are modeled both as unigram and bigram features. unigram features are used for current label factor and bigram features are used for transition factor between two labels.

according to the implementation, different options and observable values, the generators generate specific feature functions using a single scan over training data. the feature functions are used by all crf-based sieves for all selected skip-mention crf models. all extracted features are modeled both as unigram and bigram features . unigram features are used for current label factor and bigram features are used for transition factor between two labels.

data processing components
we introduce a pipeline-like data processing system that combines multiple data processing sieves . each is a separate data processing component. the whole system consists of nine sieves. the first two deal with data preprocessing and data preparation for efficient relationship extraction. the main ones then consist of linear crf-based and rule-based relationship detection. the last one cleans the data before returning it as a result. the whole implementation of this proposed pipeline is available in a public source code repository  <cit> . crfsuite  <cit>  is used for fast crf training and inference.

the proposed system can be easily adapted to another domain or other relation extraction task. in order to use it for other purposes, we would need to adapt the preprocessing part to enable the import of the new data. also, the rule-based processing sieve would need to be discarded or populated with specific rules according to a new problem. all other sieves that extract relations could be the same because they use trained models and those would be specific to a domain and task. we also employed the use of skip-mention sequences to the task of coreference resolution and achieved comparable results to existing approaches  <cit> . the pipeline starts by transforming the input text into the internal data representation, which could be used for further processing and enriches the data with additional labels, such as part-of-speech tags, parse trees and lemmas. after that we detect also action mentions, which are attributes within events. next, we employ linear crf models for event detection. we represent events as a special relationship type. then the main relationships processing sieves detect relationships. we propose several processing sieves for each of the relationship type based on the argument types or hierarchy support. after each relationship extraction step we also use rules to set the agent and target attributes in the right direction. the last relationship processing sieve performs rule-based relationship extraction and therefore detects relationships of higher precision and boosts recall levels. in the last step the extracted data is cleaned and exported.

the sieves of our system are run in the same order as shown in figure  <dig>  we provide detailed description of the processing sieves in the following sections, where we refer to the relationship attributes as subjects and objects, as shown in figure  <dig>  notice that sieves can depend on each other if they use data extracted by sieves executed earlier in the system pipeline  and ). the initial set of the mentions is produced by the mention extraction sieve. this set is then used throughout the system and represent relation attributes used by extracted relations.

preprocessing sieve
preprocessing phase includes data importation, detection of sentences and tokenization of input text. additionally, we tag the data with new labels, which are lemmas  <cit> , parse trees  <cit>  and part-of-speech tags.

mention extraction sieve
the entity mention can belong to any of the following types: protein, genefamily, proteinfamily, proteincomplex, polymerasecomplex, gene, operon, mrna, site, regulon and promoter. entity mentions are provided with the corpus, however, action mentions  are not included in the corpus. we automatically detect action mentions. they are needed to represent relationship arguments within events during the event extraction. to identify action mentions we gather action mention lemmas from the training dataset and select new candidate mentions from the test dataset by exact matching of the lemmas.

event extraction sieve 
an event can be defined as a change in the state of biological entities, such as genes or complexes . we encode events as a special relationship with a type name "event". in the dataset, the event subject types can be of protein, genefamily, polymerasecomplex, gene, operon, mrna, site, regulon and promoter types, while the objects are always of the action mention type , which are discovered in the mention extraction sieve. after the event type relationships are identified, we employ manual rules that change the order of arguments - they set an action mention as the object and a gene as the subject attribute for all extracted events.

relation processing sieves 
due to the existence of different relationships , we extract relationships in four phases . this also enables us to extract hierarchical relationships  in order to achieve higher precision. all the sieves in this step use the novel linear crf-based relationship extraction method. each processing sieve uses specific relationship properties and is executed in the following order :

 first, we extract relationships with only mentions as arguments . mentions can be either of the real or action type. by real mentions we refer to the entities that represent genes, proteins and aggregates, while action mentions could represent only arguments within events .

 in this step, we extract relationships that consist of at least one event in their arguments . before the extraction we map events into mentions, which enables us to use the same approach as in previous step. these mentions consist of two tokens . we treat the newly created event mentions the same as others and also include them in the list of other mentions. their order within the list is determined by the lowest mention token from the event. we train the models using the same techniques as in every other crf-based processing sieve. the new action mentions are treated as other mentions and from them we extract features using the same set of feature functions. lastly, the final relationships are instantiated following the same procedure as in the previous step.

 the goal of the shared task is to extract interaction relations between b. subtilis genes. thus, we select only mentions that represent b. subtilis genes and train the algorithm to predict the appropriate interaction relations . for the mention selection step we exploit a public database of the b. subtilis genes from the ncbi available at http://www.ncbi.nlm.nih.gov/nuccore/al <dig> 

 we propose this new processing sieve in addition to the previous sieves, which we previously introduced in the bionlp challenge submission  <cit> . the goal of the challenge is to extract interactions between genes. when there exists a relationship between a gene g <dig> and and event e, the final result in a grn networks looks exactly the same if our system extracts a relationship between a gene g <dig> and a gene g <dig>  where g <dig> is the object attribute of the event e. by taking into account the latter, we train the models to extract relationships only between b. subtilis genes .

the challenge datasets include seven hierarchical relationship instances, which have another relationship as one of its arguments. due to the small number of instances and newly introduced relationship extraction sieve between genes , we did not extract this type of relationship hierarchies.

additionally, there exist four negated relation instances. the bionlp task considers only positive relations and there is no performance gain if negated relations are extracted. thus, we focus on extracting positive relations. depending on the dataset and performance evaluation measure, we can add a separate sieve that can extract negated relations by applying manually defined rules that search for negation words such as nor, neither, whereas and not.

rule-based processing sieve
the last phase of relationship extraction involves application of the rules to achieve higher precision. the rules operate directly on the input text with recognized mentions and use different data representation than extractors based on crfs. we implemented the following four approaches:

mention triplets: this method searches for the consequent triplets of mentions, where the middle mention is an action mention. as input to the rule we set the matching regular expression that searches for text that action mention must starts with, and a target relation. for example, from text "the rocg gene of bacillus subtilis, encoding a catabolic glutamate dehydrogenase, is transcribed by sigl . . . ", we extract a relation rocg → interaction.transcription → sigl. the mention triplet in this example is rocg, transcribed and sigl, where the middle mention is an action mention matching the regular expression.

consecutive mentions: the method processes every two consequent b. subtilis entity mentions and checks whether the text in-between the mentions matches a specified regular expression used for extracting a target relation. by default, it forms relations that are extracted from active sentences, otherwise it supposes the passive type and changes the order of attribute types within the matched relation. for example, from text "gere binds to a site on one of these promoters, cotx, that. . . ", we extract relation gere → interaction.requirement → cotx. notice that mentions gere and cotx represent the b. subtilis entities and text between the entities matches a regular expression ".*binds to.*".

list of consecutive mentions: this method extends the technique designed for consecutive mentions by allowing potentially many entity mentions on both sides of matched regular expression. the list of mentions must be separated by one of the delimiters ",", ", and" or "and". for example, this rule extracts two relationships from the sentence "the cotg promoter is induced under the control of the sigma k and the dna-binding protein gere."

sentences of consecutive mentions: this method is similar to the rule for consecutive mentions. it first removes subsentences that exist between two mentions and then it extracts relationships. subsentences are defined as parts of text between two commas. for example, the method extracts a relationship gerr → interaction.requirement → spoiiid from the sentence "the sigma factor turns on  <dig> genes, including those for gerr, and spoiiid.".

the interaction relationships are extracted using keywords and regular expressions that depend on the type of interaction. biomedical literature uses many different language forms to express the same type of a genetic relationship. for example, some researchers prefer to repress to to inactivate or to inhibit. we use synonyms of this kind to extract additional relationships that are not identified by linear crf models. the parameters used for rule-based extraction are shown in table  <dig> 

mention triplets
each of the four different rule-based extraction methods takes a target relation name and a regular expression as input. some of them also require to specify whether the extraction should be made from active or passive sentences.

 <dig> the method is called with passive parameter set to true.

data cleaning sieve
the data cleaning sieve removes loops of relationships and eliminates redundancies. we call relationship a loop if and only if both relationship arguments refer to the same entity . for example, the sentence "... sp0h rna and sigma h ..." refers to the mentions sp0h and sigma h. since both mentions refer to the same entity , they cannot form a relationship. removal of the loops improves performance of the system as it contributes to the reduction of undesired insertions in the final prediction. another step in data cleaning phase is removal of redundant relationships. disregarding redundant relationships has no affect on predictive performance of our system but it improves the readability of the output.

experimental setup
bionlp grn  <dig> challenge dataset
the grn dataset consists of sentences from pubmed abstracts, which are mostly related to the topic of sporulation in b. subtilis and from which an appropriate gene regulation network can be reconstructed. it contains annotated text-bound entities that we call mentions. these mentions include biochemical events and relationships that were result of already conducted research work on cellular mechanisms at the molecular level. the goal of bionlp shared task was to identify interactions, which represent relations between biological entities, events or relations and are essential for construction of grn. the interaction relations form a hierarchy of mechanism and effect relation types. we were required to predict the following fine-grained interaction relation classes: regulation, inhibition, activation, requirement, binding and transcription.

in table  <dig> we report on the features of the train, development and test datasets that were used in our study. the test dataset does not include labeled data and thus we cannot perform the evaluation of each sieve against it. in the other two datasets the sentences are manually labeled with relationships, events and entity mentions.

the numbers of the interaction relations that our system reads from the datasets is different than the real ones due to the import technique into our internal data representation. the dev dataset contains  <dig> and training dataset contains  <dig> reference interaction relations. the test data contains  <dig> such relation instances .

evaluation criterion
the official evaluation criterion of the bionlp challenge considers edge resemblance between the predicted and the reference gene regulatory network describing sporulation in b. subtilis. the performance of a relation extraction system is evaluated using the ser measure  <cit> 

 ser=s+i+d/n, 

which is the ratio between the sum of relationship substitutions , insertions  and deletions , divided by the number of edges in the reference network . in short, systems that output as many wrong predictions as correct predictions achieve a ser value of  <dig>  notice that a system, which reports zero extracted relations, produces as many deletions as there are relations in a dataset . when a system extracts a true relation, the number of deletions decreases by one. if it detects a false relation then either the number of substitutions or the number of insertions increases by one. more accurate systems have a lower ser. a perfect system would correctly identify all relations and would achieve a ser of  <dig>  our goal is to maximize the number of matched relations and minimize the number of substitutions, deletions and insertions.

RESULTS
we represent the grn relationship extraction challenge as a two-level task. first, we need to identify relationships among given labeled mentions and secondly, we need to correctly identify the argument types of extracted relationships . for the challenge evaluation procedure, only results that match by relationship type and also by both argument types are counted as correct.

our approach consists of multiple submodules, i.e., sieves, whereas each is developed for extracting a specific relationship type . for the crf-based relation extraction sieves we use skip-mention distances from zero to ten. thus, we first show the overall results and then discuss the contributions of each sieve and subsets of feature functions.

predictive performance
we evaluated the proposed solution against the grn bionlp  <dig> shared task dataset using leave one out cross validation on the development data, where we achieved a ser score of  <dig> , with no substitutions,  <dig> deletions,  <dig> insertions and  <dig> matches. according to the results reported on the development dataset at the bionlp workshop  <cit> , this is improvement for one point in ser due to the additional sieve and new feature functions.

the challenge test dataset consists of  <dig> mentions from  <dig> sentences. we trained the models jointly on the development and train datasets to detect relationships against the test data. the challenge submission results of other participants in the shared task are listed in table  <dig>  according to the official ser measure, our system  was ranked first. the other participants or participating systems were k. u. leuven  <cit> , tees- <dig>   <cit> , irisa-texmex  <cit>  and

the table shows the number of substitutions , deletions , insertions , matches  and slot error rate  metric. best results per metric are highlighted in bold. reported are results announced after the bionlp  <dig> grn challenge was closed.

evex  <cit> . all the participants were trying to achieve a low number of substitutions, deletions and insertions, while trying to increase the number of matched relationships. we obtained the lowest number of substitutions and good results in the other three counters, which resulted in the best ser score. in general also other participants generated a high number of deletions, which is a clear result that the relationships are encoded in many and ambiguous forms in the text. the irisa-texmex achieved the lowest number of deletions and the maximum number of matches but received a low final result due to a high number of insertions and substitutions.

since the submission of our entry to the bionlp challenge, we have introduced some new feature functions and implemented an additional sieve. the new sieve  extracts relations between b. subtilis genes from hierarchically encoded relations in the training dataset. we report the improved results in table  <dig>  they all include new feature functions and are grouped by the inclusion of the new event-based gene processing  sieve and data cleaning sieves. the result without both of them already outperforms our submitted result by one point, with a ser score of  <dig> . the new feature functions extract more relations with increased precision. it is interesting that the inclusion of the sieve  deteriorates the final result by about  <dig> ser points. however, the inclusion uncovers more matches, but it inserts a substantial number of non-correct relations, which results in a higher error rate. thus, the best ser score of  <dig>  was achieved without the sieve  and with data cleaning. compared to our winning result at the bionlp shared task, this may further improve the system by  <dig> ser points.

the table shows results on the test set using new feature functions and the additional sieve  with or without data cleaning. the abbreviations represent the number of substitutions , deletions , insertions  and matches . best results per metric are highlighted in bold.

in figure  <dig> we show the gene regulation network, which is the visual representation of the results of our system against the test dataset. compared to our shared task submission  <cit> , the improved system identifies two additional relations  and deletes one . if the deleted relation is correct, we could merge the results and achieve a ser of  <dig>  with  <dig> substitutions,  <dig> deletions,  <dig> insertions and  <dig> matches, given  <dig> relations in the test set. to the best of our knowledge, this result represents the most accurate prediction on bionlp grn dataset so far. we were able to retrieve 39% of interactions from the data, which suggests that automatic extraction of gene regulatory networks is still a challenging task with open opportunity for future research.

analysis of extractions per sieve
data cleaning results represent the number of loop relations and the number of redundant relations . slot error rate  results are cumulative.

 <dig> due to additional analysis we saw that the event-based gene processing sieve does not improve the final results, therefore we do not employ this sieve on the test data for the final result.

the event extraction sieve uncovers events, which we represent as relations. events are not part of performance evaluation and thus their extraction does not directly affect the ser score. extracted events are given as input to the event processing sieve, which extracts relations having an event as a relation attribute. the first two relation processing sieves  already achieve promising performance on the development dataset, while on the test set they extract seven correct and seven incorrect relations, that is, the ser score remains  <dig>  the next two sieves extract more correct relations on the test set and achieve very good results on the development dataset. the event-based gene processing sieve shows substantial improvements on the development dataset, while there is a minor result change on the test set. the lowest ser score is achieved when not using this sieve for the test set . in this setting there are no further improvements when using rules on the development data. notice that the rule-based sieve contributed importantly on the development data before we introduced the event-based gene processing sieve into the system. we observed that many relations previously extracted by rules are now detected by the event-based gene processing sieve. contrary to development data, rules uncover substantially more relations on the test dataset than event-based sieves.

assessment of subsets of feature functions
the selection of the most informative feature functions is one of the key tasks in machine learning for improving the quality of results. in table  <dig> we show the results on the development data when using different subsets of feature functions. feature functions were grouped into subsets, ranging from more general  to more specific . as expected, the results improve when more feature functions are used. if only basic features  are applied, the system detects one wrong relation, which results in a ser higher than  <dig>  still, when using b. subtilis-related feature functions , the results show no improvement . we notice a reduction of  <dig>  in error rate when prefix and suffix feature functions  were added. thus, we suspect that the improvement results from combining these functions with other feature functions  or it is due to d being generator feature functions that generate larger number of features than the previous  ones. also, the next generator of mention values and mention pairs  substantially improves the result. this is expected, especially if the same type of relations exist in the development dataset and in the training dataset. we confirmed that d and e perform poorly if used separately, achieving a ser of  <dig>  and  <dig> , respectively. if d and e are used together, the system achieves a ser of  <dig> . thus, the inclusion of diverse feature functions is important. it may seem that the feature function subset h does not contribute to the results. this does not hold and can be seen if subset g is excluded. the latter configuration gives a ser of  <dig> .

the table shows the number of substitutions , deletions , insertions , matches  and slot error rate  metric. the results are measured on the development dataset using crf-based sieves only. best results per metric are highlighted in bold. the feature function subsets are selected as follows:  target label distribution, starts upper, starts upper twice, hearst co-occurence, mention token distance,  parse tree mention depth, parse tree parent value, parse tree path,  bsubtilis, isbsubtilis, isbsubtilispair,  prefix value, suffix value,  consequent value, current value,  context value,  previous/next value combination, left/right/between value and  split to values. for their detailed descriptions see table  <dig> and table  <dig> 

CONCLUSIONS
we presented a sieve-based system for relationship extraction from textual data. the system uses linear-chain conditional random fields  and manually defined extraction rules. to enable extraction of relationships between distant mentions we introduced skip-mention linear crf, which extends the applicability of a linear crf model. we form skip-mentions by constructing many sequences of mentions, which differ in the number of mentions we skip.

with a ser score of  <dig>  our approach scored best among the grn bionlp-st  <dig> submissions, outperforming the second-best system by a large margin. we described here a number of improvements of our approach and demonstrated their utility that may be used to further improve the result . the crf-based sieves in our approach are independent processing components and can be trained against an arbitrary data domain for which labeled data exists. we anticipate the utility of our approach in related data domains and for tasks with corpora.

competing interests
the authors declare that they have no competing interests.

authors' contributions
s.z., m.z., b.z. and m.b. designed the experiments. s.z. and m.z. performed the experiments. s.z., m.z., b.z. and m.b. wrote the main manuscript text. all authors read and approved the final manuscript.

