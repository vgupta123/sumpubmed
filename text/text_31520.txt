BACKGROUND
protein-ligand docking is a computational tool that predicts how a ligand binds to a target protein and their binding affinity. hence docking is useful in elaborating intermolecular interactions and enhancing the potency and selectivity of binding in subsequent phases of computer-aided drug design. docking has a wide variety of pragmatic and successful applications in structure-basedvirtual screening  <cit> , drug repurposing  <cit> , lead compound optimization  <cit> , protein cavity identification  <cit> , and protein function prediction  <cit> .

docking consists of two major operations: predicting the position, orientation and conformation of a ligand when docked to the protein’s binding pocket, and predicting their binding strength. the former operation is known as pose generation, and the latter is known as scoring. state-of-the-art docking methods, such as autodock vina  <cit>  and idock  <cit> , work reasonably well at pose generation with a redocking success rate of over 50%  <cit>  on the benchmarks of both pdbbind v <dig> and v <dig>  <cit>  and the csar nrc hiq set  <dig> sept  <dig>  <cit> . however, the single most critical limitation of docking is the traditionally low accuracy of the scoring functions.

classical scoring functions are defined by the assumption of a fixed functional form for the relationship between the numerical features that characterize the protein-ligand complex and its predicted binding affinity. this functional form is composed of the energetic contributions of various intermolecular interactions, and is often additive. the overall binding affinity is calculated as a weighted sum of several physically meaningful terms, while their coefficients are typically derived from standard multivariate linear regression  on experimental data.

cyscore  <cit> , a recently published empirical scoring function, assumes that the overall protein-ligand binding free energy can be decomposed into four terms: hydrophobic free energy, van der waals interaction energy, hydrogen bond interaction energy and ligand’s conformational entropy. cyscore focuses on improving the prediction of hydrophobic free energy by using a novel curvature-dependent surface-area model, which was claimed to be able to distinguish convex, planar and concave surface in hydrophobic free energy calculation.

a recent study on a congeneric series of thrombin inhibitors concludes that free energy contributions to ligand binding at the molecular level are non-additive  <cit> , therefore the modelling assumption of additivity models is error prone. recent years have seen a growing number of new developments of machine-learning scoring functions, with rf-score  <cit>  being the first that introduced a large improvement over classical approaches. rf-score, as its name suggests, uses random forest   <cit>  to implicitly learn the functional form in an entirely data-driven manner, and thus circumvents the modelling assumption imposed by previous scoring functions. rf-score was shown to significantly outperform  <dig> classical scoring functions when evaluated on the common pdbbind v <dig> benchmark  <cit> . despite being a recent development, rf-score has already been successfully used to discover a large number of innovative binders against antibacterial dhqase <dig> targets  <cit> . for the purpose of prospective virtual screening, rf-score-v <dig> has now been incorporated into istar  <cit> , our large-scale docking service available at http://istar.cse.cuhk.edu.hk/idock. a number of subsequent machine-learning scoring functions, including nnscore  <cit> , svr-kb and svr-ep  <cit> , cscore  <cit> , b2bscore  <cit> , sfcscorerf  <cit> , and id-score  <cit> , have also shown large improvements over classical approaches.

in this study we compare the prediction performance of two regression models mlr and rf , and investigate their application conditions and interpretability under various contexts. the methods section introduces mlr and rf, three sets of features, three benchmarks, two kinds of cross validations, and four performance metrics. the results and discussion section analyzes the prediction performance of mlr and rf on the three benchmarks and discusses the conditions of applying mlr and rf. the conclusions section emphasizes the importance of abundance of features and samples for training rf.

methods
multiple linear regression  with cyscore features
cyscore is an empirical scoring function in an additive functional form of four energetic terms, which are hydrophobic free energy Δghydrophobic, van der waals interaction energy Δgvdw, hydrogen bond interaction energy Δghbond and ligand’s conformational entropy Δgentropy . their coefficients kh, kv, kb and ke and the intercept c were obtained by mlr on  <dig> high-quality complexes carefully selected from pdbbind v <dig> refined set. the intercept value was not reported in the original publication, but was included in this study as usual  <cit>  in order to make a quick estimation of absolute binding affinity value, which is the ultimate goal in some real-world applications.
  <dig>  

we use mlr::cyscore to denote the scoring function built with mlr and the  <dig> features from cyscore. it is noteworthy that cyscore is a pure mlr model, unlike autodock vina  <cit>  which is a quasi mlr model because the number of rotatable bonds nrot is in the denominator so as to penalize ligand flexibility  and therefore mlr::vina would require an additional grid search for the weight of the nrot parameter. so this study allows a more direct comparison between mlr and rf.

random forest  with cyscore, autodock vina and rf-score features
a rf  <cit>  is a consensus of a large number of different decision trees generated from random bootstrap sampling of the same training data. during tree construction, at each inner node rf chooses the best splitting feature that results in the highest purity gain from a normally small number  of randomly selected features rather than utilizing all input features. in regression problems, the final output is calculated as the arithmetic mean of all individual tree predictions in the rf. further details on rf construction can be found in  <cit> .

in this study, multiple rfs of the default number of  <dig> trees were built using values of the mtry control parameter from one to the total number of input features. the selected rf was the one resulting in the lowest root mean square error  on the out-of-bag  samples of the training set. only one single random seed was used for training because seed is not a significant impact factor of the prediction performance, and using fewer seeds has the additional advantage of leading to computationally faster training process.

in our experiments we aimed at analyzing how rf responds to varying numbers of features and hence we selected three sets of features: cyscore  <cit> , autodock vina  <cit>  and rf-score  <cit> . cyscore comprises four numerical features: Δghydrophobic, Δgvdw, Δghbond and Δgentropy. autodock vina comprises six numerical features: gauss <dig>  gauss <dig>  repulsion, hydrophobic, hbonding and nrot. rf-score comprises  <dig> features, defined as the occurrence count of intermolecular contacts between two elemental atom types. four atom types for proteins  and nine for ligands  were selected so as to generate dense features while considering all the heavy atom types commonly observed in protein-ligand complexes. table  <dig> summarizes the three combinations of these feature sets used to train rf models. altogether four models  were evaluated in this study.table  <dig> 
the three combinations of three different sets of features used to train rf models in this study




pdbbind v <dig> and v <dig> benchmarks
the pdbbind  <cit>  benchmark is arguably the most widely used for binding affinity prediction. it contains an especially diverse collection of experimentally resolved protein-ligand complexes, assembled through a systematic mining of the yearly releases of the entire pdb  <cit> . for each complex, the experimentally measured binding affinity, either dissociation constant kd or inhibition constant ki, was manually collected from its primary literature reference. the complexes with a resolution of ≤ <dig> Å and with the ligand comprising merely nine common heavy atom types  were filtered to constitute the refined set. these complexes were then clustered by protein sequence similarity with a cutoff of 90%, and for each of the resulting clusters with at least five complexes, the three complexes with the highest, median and lowest binding affinity were selected to constitute the core set. because of the structural diversity of the core set, it is a common practice to use the core set as a test set and the remaining complexes in the refined set as a training set.

on one hand, cyscore was tested on two independent sets: pdbbind v <dig> core set  and pdbbind v <dig> core set , whose experimental binding affinities span  <dig>  and  <dig>  pkd units, respectively. on the other hand, cyscore was trained on a special set of  <dig> complexes carefully selected from the pdbbind v <dig> refined set using certain criteria  <cit>  , ensuring that the training complexes are of high quality and do not overlap with any of the two test sets. in this study we used exactly the same training set and the same test sets in order to make a fair comparison to cyscore.

furthermore, considering the fact that  <dig> classical scoring functions have already been evaluated  <cit>  on pdbbind v <dig> core set and the top performing of them  were trained on the remaining  <dig> complexes in pdbbind v <dig> refined set, we also used these  <dig> complexes as another training set to permit a direct comparison. using predefined training and test sets, where other scoring functions had previously been trained and tested, has the advantage of reducing the risk of using a benchmark complementary to one particular scoring function.

likewise for the pdbbind v <dig> benchmark, we used an additional training set comprising the complexes in pdbbind v <dig> refined set excluding those in pdbbind v <dig> core set. this led to a total of  <dig> complexes. by construction, this training set does not overlap with the test set.

pdbbind v <dig> round-robin benchmark
we propose a new benchmark to investigate how prediction performance of the four models changes in cross validation and with varying numbers of training samples. we used pdbbind v <dig> refined set , which is the latest version and constitutes the most comprehensive and publicly available structural dataset suitable for training scoring functions.

we used 5-fold cross validation, as was used by the recently published empirical scoring function id-score  <cit> , to reduce overfitting and thus generalization errors. the entire pdbbind v <dig> refined set  was divided into five equal partitions using uniform sampling on a round-robin basis: the entire  <dig> complexes were first sorted in the ascending order of their measured binding affinity, and the complexes with the 1st, 6th, 11th, etc. lowest binding affinity belonged to the first partition, the complexes with the 2nd, 7th, 12th, etc. lowest binding affinity belonged to the second partition, and so on. this partitioning method, though not completely random, has two advantages: on one hand, each partition is guaranteed to span the largest range of binding affinities and incorporates the largest structural diversity of different protein families; on the other hand, each partition is composed of a deterministic list of complexes, permitting reproducibility and comparisons in future studies. table  <dig> summarizes the statistics of the five partitions. the pdb ids and measured binding affinities of the complexes in the five partitions are available in the additional file  <dig> table  <dig> 
the statistics of the five partitions of pdbbind v <dig> refined set 




we then used the partition on which the best performance was obtained . see the results and discussion section.) as the test set in pdbbind v <dig> round-robin benchmark, and used the remaining four partitions  to construct four training sets of incremental sizes: the first training set comprises partition  <dig> , the second training set comprises partitions  <dig> and  <dig> , the third training set comprises partitions  <dig>   <dig> and  <dig> , and the fourth training set comprises partitions  <dig>   <dig>   <dig> and  <dig> . therefore this new benchmark provides a way to study how prediction performance varies with training set size. moreover, its test set has a significantly larger number of complexes  compared to pdbbind v <dig>  and v <dig>  benchmarks, making this new benchmark not being a redundant duplication of the previous two benchmarks. table  <dig> summarizes the numbers of test and training samples for the three benchmarks.table  <dig> 
the numbers of test samples and training samples for the pdbbind v <dig>  v <dig> and v <dig> benchmarks used in this study




leave-cluster-out cross validation 
leave-cluster-out cross validation   <cit> , in contrast to standard cross validation, divides the complete set of complexes into protein families instead of random subsets. each protein family, or each cluster, is typically determined by 90% protein sequence identity. protein families with at least ten complexes are treated as individual clusters, labeled as a to w. protein families with four to nine complexes are combined into cluster x. protein families with two to three complexes are combined into cluster y. singletons are combined into cluster z. each cluster is iteratively left out of the training set and used to evaluate the predictive performance of the scoring function. the performance on each cluster can be inspected individually, and the overall performance can be estimated by averaging over all clusters.

so far lcocv has been applied to the assessment of six scoring functions, which are rf-score  <cit> , ddplat+moe  <cit> , cscore  <cit> , b2bscore  <cit> , sfcscorerf  <cit>  and the work of ross et al.  <cit> .

for the purpose of comparison to other scoring functions, pdbbind v <dig> refined set  was used in this study to perform lcocv. the 1xr <dig> entry in cluster x was discarded because its ligand is far away from its protein, thereby leaving  <dig> complexes. the pdb ids and measured binding affinities of the complexes in the  <dig> protein families  and the  <dig> multi-family clusters  are available in the additional file  <dig> 

performance metrics
prediction performance was quantified through standard deviation sd in linear correlation, pearson correlation coefficient rp and spearman correlation coefficient rs between the measured and predicted binding affinities of the test set. these metrics are commonly used in the community  <cit> , and the sd metric is essentially the residual standard error  metric used in some other studies  <cit> . the above three metrics are invariant under linear transformations , so they are mainly for comparative purpose. in some applications, however, the ultimate goal of scoring functions is to report an absolute binding affinity value as close to the measured value as possible. hence we use a more realistic metric, the root mean square error rmse between measured and predicted binding affinities without a linear correlation. lower values in rmse and sd and higher values in rp and rs indicate better prediction performance.

mathematically, equations  <dig>   <dig>   <dig> and  <dig> show the expressions of the four metrics. given a scoring function f and the features  describing the nth complex out of n complexes in the test set,  is the predicted binding affinity,  are the fitted values from the linear model between {y} and {p} on the test set, whereas  and  are the rankings of {y} and {p}, respectively.
  <dig>    <dig>    <dig>    <dig>  

RESULTS
figure  <dig> plots the prediction performance of mlr::cyscore, rf::cyscore, rf::cyscorevina and rf::cyscorevinaelem using different numbers of training samples on pdbbind v <dig> benchmark , pdbbind v <dig> benchmark  and pdbbind v <dig> round-robin benchmark . the raw values are available in the additional file  <dig> figure  <dig> 
prediction performance of mlr::cyscore, rf::cyscore, rf::cyscorevina and rf::cyscorevinaelem trained with varying numbers of samples. first row: root mean square error rmse. second row: standard deviation sd in linear correlation. third row: pearson correlation coefficient rp. fourth row: spearman correlation coefficient rs. left column: pdbbind v <dig> benchmark . center column: pdbbind v <dig> benchmark . right column: pdbbind v <dig> round-robin benchmark .



mlr::cyscore performance does not increase with more training samples
on both pdbbind v <dig> and v <dig> benchmarks, mlr::cyscore performed best when it was trained on the  <dig> carefully selected complexes used by cyscore. its performance dropped when more complexes were used for training. on pdbbind v <dig> round-robin benchmark, mlr::cyscore performance stayed flat regardless of training set sizes.

these results show that mlr::cyscore is unable to exploit large sizes of structural data given only a small set of sophisticated features. feeding more training samples to mlr::cyscore actually increases the difficulty in regressing the coefficients well. generally it would be a good idea to select the training complexes that provide the best performance on a test set, as was the case of cyscore. however, in real applications the binding affinities of the test set are not known and unfortunately selection of training complexes is not performed blindly .

rf performance increases with more structural features and training samples
on all the three benchmarks, given the same set of features, the rf models trained with more samples resulted in higher prediction accuracy. similarly, given the same training samples, the rf models trained with more features resulted in higher prediction accuracy.

these results suggest that rf is capable of effectively exploiting a comprehensive set of structural features and training samples. generally the more training samples, the more knowledge for rf to learn so as to capture the non-linearity of the structural data. likewise, the more appropriate features, the higher probability of choosing the best splitting feature that can result in a high purity gain at non-leaf nodes during rf construction, and hence the higher chance of boosted rf performance.

rf models perform consistently well in cross validation
table  <dig> shows the results of 5-fold cross validation for all the four models. the best performance was obtained on partition  <dig>  in terms of average performance, the relative performance ranking is consistent, where rf::cyscorevinaelem  is better than rf::cyscorevina , which is better than rf::cyscore , which is better than mlr::cyscore .table  <dig> 
cross validation results of the four models on the five partitions of pdbbind v <dig> refined set  in terms of root mean square error rmse, standard deviation sd in linear correlation, pearson correlation coefficient rp and spearman correlation coefficient rs




leave-cluster-out cross validation leads to unrealistically low performance
table  <dig> shows the results of leave-cluster-out cross validation  for all the four models. not unexpectedly, the observed performance is very heterogeneous across the different protein families. these results indeed agree with the lcocv results of six other scoring functions from previous studies . by analyzing the lcocv statistics of all these ten scoring functions, we found that they all performed well in certain clusters  and poorly in some other clusters . the reasons for the large spread of performance across the different clusters are manifold, and a comprehensive analysis for each protein family would be beyond the scope of this study. as pointed out in  <cit> , eliminating all the hiv protease complexes leads to an imbalance between the training and test sets because hiv protease inhibitors are on average much larger than the ligands of the other targets. this illustrates that the lcocv results should not be directly interpreted as performance measures on particular protein families. moreover, the limited size of many clusters and the small range of measured binding affinity values therein make a satisfactory prediction of the ranking rather challenging.table  <dig> 
leave-cluster-out cross validation results of the four models on the  <dig> protein families  and  <dig> multi-family  clusters of pdbbind v <dig> refined set  in terms of root mean square error rmse, standard deviation sd in linear correlation, pearson correlation coefficient rp and spearman correlation coefficient rs







while results on standard cross validation might be too optimistic, results on leave-cluster-out cross validation might be too pessimistic. here we want to emphasize that lcocv is only suitable for estimating the performance of a generic scoring function on a truly new target protein that does not belong to a cluster represented by any of the proteins in the training set, but this constitutes a very uncommon scenario in real-life applications because it is rare for a target protein not to have high sequence similarity to any other protein in a diverse and large training set. in fact, such type of complexes should never be eliminated from a training set. instead, the training set composition should reflect as closely as possible the actual complexes on which the scoring function is to be applied. consequently, lcocv is not appropriate to evaluate generic scoring functions, as previously argued  <cit> .

machine-learning scoring functions are significantly more accurate than classical scoring functions with fixed functional forms
table  <dig> compares cyscore, rf::cyscore, rf::cyscorevina and rf::cyscorevinaelem against  <dig> other scoring functions on pdbbind v <dig> core set , with rf::cyscorevinaelem performing best in terms of rp, rs and sd. it is worth noting that the top four scoring functions are all trained with rf.table  <dig> 
prediction performance of  <dig> scoring functions evaluated on pdbbind v <dig> core set  in terms of pearson correlation coefficient rp, spearman correlation coefficient rs and standard deviation sd in linear correlation on the test set


the scoring functions are sorted in the descending order of rp. rf::cyscorevinaelem and cyscore rank 1st and 9th respectively in terms of rp. the statistics for the other  <dig> scoring functions are collected from  <cit> .



substituting rf for mlr and incorporating more features and training samples strongly improves cyscore
figure  <dig> compares the prediction performance of cyscore and rf::cyscorevinaelem, with rf::cyscorevinaelem improving cyscore by - <dig>  in rmse, - <dig>  in sd, + <dig>  in rp and + <dig>  in rs on the pdbbind v <dig> benchmark, by - <dig>  in rmse, - <dig>  in sd, + <dig>  in rp and + <dig>  in rs on the pdbbind v <dig> benchmark, and by - <dig>  in rmse, - <dig>  in sd, + <dig>  in rp and + <dig>  in rs on the pdbbind v <dig> round-robin benchmark.figure  <dig> 
correlation plots of predicted binding affinities against measured ones. top row: cyscore. bottom row: rf::cyscorevinaelem. left column: pdbbind v <dig> benchmark , with rf::cyscorevinaelem trained on  <dig> complexes. center column: pdbbind v <dig> benchmark , with rf::cyscorevinaelem trained on  <dig> complexes. right column: pdbbind v <dig> round-robin benchmark , with rf::cyscorevinaelem trained on  <dig> complexes.



these results show that rf::cyscorevinaelem performed consistently better than cyscore on all the three benchmarks. it is important to note that, in each benchmark, both scoring functions used the same non-overlapping training and test sets. taken together, these results show that one can develop a much more accurate scoring function out of an existing one simply by changing the regression model from mlr to rf and incorporating more structural features and training samples.

sensitivity analysis of the rf model can determine feature importance
unlike classical scoring functions, rf-based scoring functions can hardly be explicitly expressed as a mathematical equation like eq.  <dig>  therefore it is useful to employ the variable importance tool of rf to estimate the importance of each feature by randomly permuting its training values, and the feature leading to the largest variation in the predicted binding affinity on the oob data can be regarded as the most important for a particular training set. figure  <dig> plots the percentage of increase in mean square error  observed when each of the  <dig> cyscore features used to train rf was noised up. all the  <dig> features turned out to be important , with van der waals interaction energy  and hydrophobic free energy  being relatively more important . correctly estimating variable importance can assist in feature selection and in understanding ligand binding.figure  <dig> 
rf::cyscore feature importance estimated on internal oob data of the  <dig> complexes from pdbbind v <dig> refined set. the four features are hydrophobic free energy , van der waals interaction energy , hydrogen bond interaction energy  and ligand’s conformational entropy . the %incmse value of a particular feature was computed as the percentage of increase in mean square error observed in oob prediction when that features was randomly permuted.



CONCLUSIONS
in this study we have demonstrated that, on one hand, the multiple linear regression  model used in many scoring functions like cyscore does not improve its performance in the presence of abundant training samples. this is a particularly significant drawback for mlr-based scoring functions because they cannot benefit from the future availability of more experimental data. on the other hand, rf-based scoring functions can comprehensively capture the non-linear nature in the data and thus assimilate data significantly better than mlr-based scoring functions. most importantly, feeding more training samples to rf can increases its prediction performance. under this circumstance, improvements with dataset size can only be gained with the appropriate regression model. simply changing the regression model of cyscore from mlr to rf and expanding the feature set and the sample set can significantly increase the prediction accuracy. the performance gap between mlr-based and rf-based scoring functions will be further widened by the future availability of more and more x-ray crystal structures.

moreover, classical empirical scoring functions usually rely on complicated energetic contributions that must be carefully devised from intermolecular interactions, whereas rf-based scoring functions can also effectively exploit features as simple as occurrence count of intermolecular contacts. it has also been shown that functional group contributions in protein-ligand binding are non-additive. this means new features cannot be easily incorporated into an existing mlr model. in this study we have shown that using more structural features appropriately can also substantially enhance the prediction accuracy of rf, as can be seen in the comparison between rf::cyscorevinaelem and rf::cyscore. this further stresses the importance of substituting rf for mlr in scoring function development.

electronic supplementary material
additional file 1:
cv. this csv file contains the pdb ids and measured binding affinities of the protein-ligand complexes in the five partitions of pdbbind v <dig> refined set for cross validation purpose. 

 additional file 2:
lcocv. this csv file contains the pdb ids and measured binding affinities of the protein-ligand complexes in the  <dig> protein families and  <dig> multi-family clusters of pdbbind v <dig> refined set for leave-cluster-out cross validation purpose. 

 additional file 3:
stat. this excel file contains the prediction performance of mlr::cyscore, rf::cyscore, rf::cyscorevina and rf::cyscorevinaelem trained with varying numbers of samples and tested on the pdbbind v <dig>  v <dig> and v <dig> benchmarks and also in the standard 5-fold and leave-cluster-out cross validations in terms of root mean square error rmse, standard deviation sd in linear correlation on the test set, pearson correlation coefficient rp, spearman correlation coefficient rs and kendall correlation coefficient rk. 

 competing interests

the authors declare that they have no competing interests.

authors’ contributions

hl designed the study, ran the experiments, and wrote the manuscript. ksl, mhw and pjb discussed results and commented on the manuscript. all authors read and approved the final manuscript.

