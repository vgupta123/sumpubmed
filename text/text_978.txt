BACKGROUND
accurate classification of patients with complex diseases such as cancer is crucial for successful treatment of the diseases. high-throughput proteomics techniques based on mass spectrometry  have made it possible to investigate proteins over a wide range of molecular weights in a style similar to gene expression studies with dna microarrays. the advancement of these techniques has generated many analytic challenges, among which a central task is to discover 'signature' protein profiles specific to each pathologic state  or differential profiles between experimental conditions  from high-dimensional data  <cit> . the technique of surface enhanced

laser desorption/ionization time-of-flight mass spectrometry   <cit>  has been used in many recent disease studies  <cit> . although it is a convenient method for screening a cohort of samples and finding promising protein markers from serum or plasma samples, it suffers from a relatively low sensitivity and specificity and a high noise level  <cit> .

like in many other biological studies, a key difficulty in such high-throughput studies is the noisy nature of the data, which can be caused by the intrinsic complexity of the biological problems, as well as experimental and technical imperfections. another difficulty arises from the high dimensionality of the data. similar to the situation in microarray studies, typically one proteomics investigation only involves several tens of samples but the measured points on the mass spectrum can be in the thousands or more. even after pre-processing steps such as peak and/or biomarker detection, the dimensionality is usually still larger than or comparable to the sample size. this makes many standard pattern classification algorithms fail. for those that do work theoretically, there is a high risk of overfitting due to the small sample size. thus, there is an algorithmic need for feature selection in addition to the biological need of discovering a manageable set of key molecular factors  behind the disease. as observed in  <cit>  and other investigations, for machine learning methods such as support-vector machines  that can work at high-dimensionality, dimension reduction could still improve the performance dramatically. however, it should be noted that when validating the performance of a classification algorithm with feature-selection steps, the feature selection procedure should also be validated simultaneously to avoid bias in the assessment. also, due to the small sample size, the cross-validation prediction of the algorithm's performance tends to have a high variance. thus, we should pay more attention to properties related to generalization ability rather than the prediction performance per se. we suspect that failing to do so may be a reason why good results published from one investigation may not be reproduced by other investigations.

guyon et. al.  <cit>  proposed a svm-rfe  algorithm to recursively classify the samples with svm and select genes according to their weights in the svm classifiers. we proposed a method r-svm with a similar recursive strategy but used a different criterion to evaluate and select the most important genes  <cit> , and a correct scheme to estimate the prediction performance. in this paper, we describe the r-svm method with a voting scheme for feature selection and compare its performance with svm-rfe on simulation data and two seldi-tof-ms datasets, one on rat liver cirrhosis and another on human breast cancer. we found that cross-validation prediction performances of r-svm and svm-rfe were nearly the same, but r-svm was more robust to noise and outliers in discovering informative genes and therefore has better accuracy on independent test data.

r-svm and svm-rfe represent typical machine-learning-based multivariate approaches for the task. conventional univariate methods are also frequently used for feature selection and classification. we compared the svm-based methods with the weighted-voting  method  <cit>  on simulation studies, and discussed their respective strengths and weaknesses. although our real applications were conducted on ms data, the method can be applied broadly to microarray data and other high-throughput genomics and proteomics data.

RESULTS
simulated data sets
we generated three types of simulated data to investigate the characteristics of the feature selection and classification methods. the basic simulation model for the first two types of data is as follows: each sample contains simulated expression values of  <dig> genes. among all the genes,  <dig> are "informative" ones, each following independently the gaussian distribution n for class  <dig> and n for class  <dig>  the rest  <dig> "uninformative" genes follow independently n for both classes. for each simulation experiment, we generated a training set of  <dig> samples  and an independent test set of  <dig> samples . the two types of simulated data were generated by adding noises to this general model in different ways. the first type  mimics the situation where some of the gene expressions in some samples are outliers. for the informative genes, we randomly chose 5% of expression values in all samples as outliers by making them to follow n for the class- <dig> sample and n for the class- <dig> sample. the second type of simulated datasets  is constructed to contain 5% "outlier samples," which were made by randomly picking 5% of the samples and increasing the standard deviation of every gene in these samples by  <dig> fold. we did  <dig> simulations for each type of the data .

the above two simulation models are over-simplified in many aspects. to mimic more realistic situations, we generated the third type of simulated data based on a real human breast cancer microarray dataset obtained with affymetrix u <dig> plus  <dig>  microarrays. the dataset originally contains  <dig> estrogen receptor positive  cases and  <dig> estrogen receptor negative  cases. the data were normalized by the gcrma algorithm  <cit> , and the gene  expression levels were log2-transformed. according to our previous experiments as well as published work , the er status is one of the most predominant partitioning factors for molecular classification of breast cancer. we therefore took the differentially expressed genes between the two classes as "truly informative" genes. we extracted the genes whose average differences between the two classes are greater than  <dig> , which gave us  <dig>  genes . from these genes, we randomly selected  <dig> to be the "informative genes" in the simulation dataset. then, we randomly took another  <dig> genes and "force" them to be uninformative by permuting their sample labels. by combining the "informative genes" and "uninformative genes", we obtained a simulated dataset in which there are  <dig> informative genes and  <dig> uninformative genes. the correlations among the informative genes and among the uninformative genes are the same as those in the original dataset. from this dataset, we randomly took  <dig> er+ and  <dig> er- samples as the training set, and used the remaining samples as independent test set. we generated  <dig> sets of data by this strategy. we call this type of datasets data-r in our experiments.

real seldi proteomics data sets
we applied the two svm-based methods to two real seldi-tof-ms proteomics datasets. the first dataset is from a rat model used to discover serum biomarkers of liver cirrhosis  <cit> . it contains serum protein profiles of  <dig> normal rats and  <dig> thioacetamide-induced liver cirrhosis rats. the biomarker function of ciphergen proteinchip software  <dig>  detected  <dig> biomarkers from the raw data. they were normalized according to their mean values, small values were truncated to  <dig> and all biomarkers were log transformed. the second proteomics dataset came from a human breast cancer study. we obtained plasma samples from  <dig> breast cancer patients and  <dig> healthy women  <cit> . the plasma samples were ph-fractionated and analyzed by seldi-tof-ms. the ciphergen proteinchip software  <dig>  detected  <dig> biomarkers, which were preprocessed in the same way as for the rat liver cirrhosis data.

comparison of r-svm with svm-rfe on simulated datasets
we first compared the performance of r-svm to svm-rfe on data-g and data-s. for each of these datasets, we applied r-svm and svm-rfe to perform gene selection, built svm models on training data with selected genes, and tested the models on the independent test data. experiments were done  <dig> times for each data type. the following aspects of performances are inspected at each level of gene selection: number of svs  used in the svm model, test error, the percentage of true informative features recovered in the selection, and for data-s the number of outlier samples used as svs. tables  <dig> and  <dig> show the relative improvements of r-svm over svm-rfe with regard to these factors averaged on the  <dig> experiments for each data type, as well as the p-values  of the differences. the cross-validation  errors on training sets are similar between the two methods so the comparison is not shown in the tables .

it can be seen on both data-g and data-s that at most of the selection levels, especially at those lower levels , the number of svs used by r-svm is  <dig> %~ <dig> % fewer than that of svm-rfe, indicating the better generalization ability of r-svm. one can also see that at the same selection level, r-svm recovers significantly more of the informative genes than svm-rfe, and the improvement is about  <dig> %~ <dig> % at lower selection levels. these two factors can explain the observation that independent test errors of r-svm were significantly lower than that of svm-rfe  at lower selection levels. on the data-s with outlier samples, r-svm also shows a better ability to avoid taking the outlier samples as svs .

as simulation models for data-g and data-s are too simplistic, we compared on data-r and the results are shown in table  <dig>  it can be seen that the improvement of r-svm over svm-rfe with regard to both the number of svs used and the number of informative genes recovered are more significant. in terms of testing errors on the independent data, svm-rfe outperformed r-svm initially at high selection levels. when fewer genes were selected, r-svm gradually out-raced svm-rfe, and the improvement of r-svm over svm-rfe became more obvious as the number of selected features decreased. this tendency was also observed on data-g and data-s. it is likely due to the fact that r-svm selected more informative features and that it is more important to include truly informative ones when fewer features are used in a classifier.

application on the rat liver cirrhosis data
we applied r-svm and svm-rfe to the two real seldi-tof-ms datasets. table  <dig> shows the results on the rat liver cirrhosis data. since there is no independent test set and there is no standard answer about the true informative features, we only list the cross-validation errors and the number of svs in table  <dig>  it can be seen that at some selection levels r-svm achieved smaller cv errors, while at others svm-rfe gave smaller cv errors, but the differences are not significant. however, at most levels, r-svm uses fewer svs than svm-rfe. 

the top  <dig> biomarkers selected by r-svm were reported for further biological investigation. they are listed in table  <dig> along with their t-statistics and roc statistics. we see that the top  <dig> markers are all significantly correlated with the sample classification on their own, but not necessarily be ranked at the top according to the univariate criterion. among the top  <dig> biomarkers, the  <dig>  da protein was down-regulated in the liver cirrhosis rats. on-chip purification and tryptic digestion was conducted. combined data from pmf  and maldi-tof/tof ms/ms spectra suggested that this  <dig> da protein shares homology to a histidine-rich glycoprotein  <cit> . it has been reported that the mrna of this gene was found to be specifically expressed only in liver  <cit> . we speculate that down-regulation of histidine-rich glycoprotein in cirrhotic liver may be a manifestation of loss of normal liver function, including secretary pathways upon treatment with thioacetamide  <cit> .

application on the human breast cancer data
the results of r-svm and svm-rfe on the breast cancer dataset are listed in table  <dig>  error rates and sv numbers on this dataset were higher than in the rat liver cirrhosis study, due to the complexity of the problem under study and individual variations among human individuals. still, we found that r-svm tends to use fewer svs at most selection levels and, hence, may have a better generalizability than svm-rfe.

the minimal cv error rate  is achieved at the 8-feature level by r-svm. these  <dig> markers and their t-statistics and roc statistics are listed in table  <dig>  six of the top  <dig> r-svm selected markers are called significant by t-statistics . the top marker, marker- <dig>  has an auc  of only  <dig> , but the  <dig> markers jointly classify  <dig> % of all cases correctly. the auc of the svm model built on the r-svm-selected with  <dig> markers is  <dig> , much larger than that of the best single marker, marker- <dig>  using direct on-chip sequencing provided by ciphergen, one important peptide  was identified and was selected for further study. follow-up biological study showed that this peptide may be an important indicator of the disease status of breast cancer patients  <cit> .

we also applied the random forest  method  <cit>  to this human breast cancer dataset. with the decreaseginidistance criterion  <cit> , we selected the  <dig> most important markers from this data, of which  <dig> are also among the  <dig> markers selected by r-svm. the out-of-bag error reported by the rf method was  <dig> %, which is higher than the minimal cv error of  <dig> % reached by r-svm. however, these two error rates are not directly comparable. the out-of-bag error reported by rf is based on a resampling approach, which makes the effective sample size of its training set smaller than the full size and, therefore, causes the estimated prediction error to be biased upwards. when the sample size is small, the bias of resampling error without any correction can be large. on the other hand, although cross validation error by the cv <dig> scheme  is unbiased, selecting the minimal error among multiple levels of feature eliminations may lead to a slightly over-optimistic estimate. taking these two factors together, we conclude that the two tested methods performed similarly on this dataset.

comparison with the univariate method
many researchers believe that it is beneficial to use multivariate methods to analyze microarray and proteomics data as genes usually work in collaborative ways rather than as independent factors. however, univariate methods are still useful for identifying differentially expression features and continue to play important roles in many applications. therefore it is worthwhile to compare the performances of the svm-based methods with more conventional univariate approaches, of which we take the weighted-voting  method as a representative in this work.

tables  <dig> and  <dig> show the comparison of r-svm and wv on data-g and data-s. it can be seen that r-svm outperformed wv on data-g in both test accuracy and the recovery of informative genes at all selection levels, and the improvement is very significant. however, r-svm was significantly inferior to wv in both aspects on data-s, and the difference was more obvious when fewer genes are selected. these observations suggest that r-svm is more robust than wv to outlier values spreading randomly in the dataset, but suffers more when some samples are entirely corrupted. indeed, although r-svm uses the class means to represent the samples for feature selection, the standard svm uses only boundary samples  in building the classifiers, thus a few outlier samples can make big effect on degrading its performance. 

we should note that all our simulation designs  are favorable to univariate methods since the true underlying classification information is in the differential expression of individual genes. the models haven't consider any collaborative effects , thus they bias in favor of the single-variable approach a priori. we expect that in more complicated situations when combinatorial effects play major roles, svm-based methods will perform even better.

CONCLUSIONS
high-throughput genomics and proteomics data open a new route to the classification of complex diseases. machine learning methods for feature selection and classification have been playing active roles in analyzing such data. we compared two similar methods, svm-rfe and r-svm, both adopting recursive procedures to select features according to their importance in svm classifiers. the major difference between the two methods is the criteria used for evaluating the contribution of genes. although the two methods did not differ significantly in their cross-validation performances, it appeared that r-svm is more robust to severe noise and outliers and can recover more informative genes. the successful application of r-svm on the rat liver cirrhosis seldi data and human breast cancer seldi data show that the proposed strategy can help to identify biologically important markers.

we compared r-svm with a representative univariate method, the weighted-voting method on simulated data. although the comparison is limited in scope , some interesting insights regarding their respective strengths and weaknesses are observed. the svm-based method performs better in terms of the classification accuracy, but univariate methods can reveal more of the differentially expressed individual features. a more systematic comparative study of the univariate and multivariate methods can be helpful for better understanding the nature of the methods and problems.

availability and requirements
ar code package of the r-svm method and a linux-based executable package are freely available.

project name: r-svm 

project home page: 

operating systems: r version: windows xp, linux; linux version: linux.

programming languages: r, c/c++

other requirements: r for the r version; svmtorch  for the linux version

license: free

