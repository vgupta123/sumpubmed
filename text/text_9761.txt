BACKGROUND
the need for more sequence
complete genome sequencing is a major investment and is unlikely to be applied to the vast majority of organisms, whatever their importance in terms of evolution, health or ecology. complete genome sequences are available for only a few eukaryote genomes, most of which are model organisms. the focus of eukaryote genome sequencing has been on a restricted subset of known diversity, with, for example, nearly half of the completed or draft stage genomes being from vertebrates. while arthropoda and nematoda have two completed genomes each, with a dozen others in progress, compared to predicted diversity  current genome sequencing illuminates only small parts of even these phyla. the disparity between sequence data and motivation for biological study is significant. allied to this bias in genome sequence is a bias in functional annotation for the derived proteomes: a vertebrate gene is more likely to have been assigned a function due to the focus of biomedical research on humans and closely related model species such as mouse  <cit> .

shotgun sample sequencing of additional genomes through expressed sequence tags  or genome survey sequences  has proved to be a cost-effective and rapid method of identifying a significant proportion of the genes of a target organism. thus many genome initiatives on non-traditional model organisms have utilised est and gss strategies to gain an insight into "wild" biology. an est strategy does not yield sequence for all of the expressed genes of an organism, because some genes may not be expressed under the conditions sampled, and others may be expressed at very low levels and missed through the random sampling that underlies the strategy. however the creation of est libraries from a range of conditions, such as different developmental stages or environmental exposures, promotes a closer examination of the biology of these species.

the well documented phylogenetic sequence deficit  <cit>  has led us to coin the term "neglected genomes". currently many groups are sequencing ests from their chosen species to perform studies in a wide-range of disciplines, from comparative ecotoxicology  <cit>  to high-throughput detection of sequence polymorphisms  <cit> . the contribution of est projects for neglected but biologically relevant organisms is highlighted in figure  <dig>  as with all sequence data, obtaining high quality annotation requires prior information and is labour intensive. the "partial genome" information that results from est datasets presents special problems for annotation, and we are developing tools for this task.

the need for high quality translation
the partigene software suite  <cit>  simplifies the analysis of partial genomes. ests are clustered into putative genes and consensuses determined. all the data is stored in a relational database, allowing it to be searched easily. while preliminary annotation based on blast analysis of nucleotide sequence can be performed, more robust methods are needed to allow high-quality analysis. the error-prone nature of ests makes application of most annotation tools difficult. to improve annotation, and facilitate further exploitation, a crucial step is the robust translation of the est or consensus to yield predicted polypeptides. the polypeptide sequences present a better template for almost all annotation, including interpro  <cit>  and pfam  <cit> , as well as the construction of more accurate multiple sequence alignments, and the creation of protein-mass fingerprint libraries for proteomics exploitation. high quality polypeptide predictions can be applied to functional annotation and post-genomic study in a similar way to those available for completed genomes.

translating expressed sequence tags
prediction of the correct polypeptide from ests is not trivial:

 <dig>  the inherent low quality of est sequences may result in shifts in the reading frame  or ambiguous bases. these errors impede the correct recognition of coding regions. the initiation site may be lost, or an erroneous stop codon introduced to the putative translation.

 <dig>  ests are often partial segments of a mrna, and as most cloning technology biases representation to the internal parts of genes, the initiation methionine codon may be missed. this is a problem for some of the de novo programs which use the initiation methionine to identify the coding region .

sequence quality can be improved by clustering the sequences based on identity. for each cluster a consensus can be determined  <cit> . this approach, however, will not address the whole problem as poor quality est sequences may not yield high quality consensuses and for smaller volume projects, most genes have a single est representative. therefore additional methods must be applied to provide accurate polypeptide predictions.

similarity-based methods
a robust method to determine the correct encoded polypeptide is to map a nucleotide sequence onto a known protein. this concept is the basis for blastx  <cit> , fastx  <cit>  and protest  <cit> . blastx and fastx use the six frame translation of a nucleotide sequence to seed a search of a protein database. the alignments generated for each significant hit provide an accurately translated region of the est. blastx is extremely rapid, but the presence of a frameshift terminates each individual local alignment, ending the polypeptide prematurely. fastx is able to identify possible frameshifts, but its dynamic programming approach is significantly slower than blastx. these methods require that the nucleotide sequence shares detectable similarity with a protein in the selected database. many genes from both well studied and neglected genomes do not share detectable similarity to other known proteins. for example, the latest analysis of the caenorhabditis elegans proteome shows that only ~50% of the  <dig> predictions contain pfam-annotated protein domains  <cit> , and 40% share no significant similarity with non-nematode proteins in the swissprot/trembl database  <cit> . this feature is not unique to the phylum nematoda, and is likely perhaps to be more extreme for neglected genomes, given the phylogenetic bias of most protein databases.

protest uses a slightly different similarity-based approach  <cit> . a protein sequence is compared to an est database. phrap  <cit>  is used to construct a consensus sequence from the ests found to have significant similarity. these consensuses are then compared to the original sequence using estwise  giving a maximum likelihood position for possible frameshifts. the system is accurate but is not readily adaptable to the high-throughput approach necessary when dealing with very large numbers of ests. more crucially, an est that does not show significant similarity to a known protein is not translated.

'de novo' predictions
to overcome the reliance upon sequence similarity, de novo approaches based on recognition of potential coding regions within poor quality sequences, reconstruction of the coding regions in their correct frame, and discrimination between ests with coding potential and those derived from non-coding regions have been developed  <cit> .

diana-est  <cit> , combines three artificial neural networks , developed to identify the transcription initiation site and the coding region with potential frameshifts. estscan <dig>  <cit>  combines three hidden markov models trained to be error tolerant in their representations of mrna structure . decoder  <cit>  uses an essentially rule-based method for identifying possible insertions and deletions in the nucleotide sequence, as well as the most likely initiation site, and was developed for complete cdna sequence translation.

each of these methods has different strengths in their attempt to identify the precise coding region; all require prior data to train their models. published descriptions of their utility are based on training with human full length coding sequences , and thus tens of thousands of training sequences  were used to achieve optimum results. as stressed above, this amount of prior data is not available for the vast majority of est project species .

new solution – prot4est
prior to this project, nematode ests available through nembase  <cit>  had been translated using decoder, as a preliminary study had suggested that it outperformed the other available methods  .  <dig> out of the  <dig> resulting predicted polypeptides were likely to be poorly translated , and we suspected many more contained errors. this motivated the creation of a solution using several methods to enhance the quality of the polypeptide predictions, exploiting their strengths while recognising their short-comings. prot4est is an est translation pipeline, written in perl, with a user-friendly interface, that links some of these described methods together. it carries out retrieval and formatting of files from online databases for the user. it has been designed to be used as a stand-alone tool, or as an integral part of the partigene process  <cit> .

implementation
decoder
the decoder program  <cit>  was developed to define start codons and open reading frames in full-length cdna sequences. it exploits the quality scores for the sequence produced from base-calling software, such as phred  <cit> , and additional text-based information to identify all possible coding regions. in regions of low sequence quality up to  <dig> nucleotides are removed or inserted, representing possible frameshifts. a likelihood score is calculated for each possible coding sequence , and the one with the lowest score is chosen as the correct cds. the score is computed from the probability of generating a random sequence with a better kozak consensus , atg position and codon usage. decoder requires a codon bias table, which is used to determine the putative coding regions optimal codon usage. a penalty term limits the number of insertions/deletions in the corrected cds.

estscan <dig> 
hidden markov models  can represent known sequence composition in a probabilistic manner  <cit> . this has been exploited recently in applications to find genes in genomic sequence  <cit> , predict domain composition in protein sequences  <cit> , and align multiple sequences  <cit> . estscan  <cit>  exploits the predictive power of hidden markov models by combining three models:

 <dig>  modeling mrna structure: estscan separates the probable cds from the untranslated regions . the core of the coding sequence is represented by a 3-periodic inhomogeneous hidden markov model. flanking this core model are start and stop profiles for the codons observed at these positions. the profiles for untranslated regions flank the start and stop states.

 <dig>  error tolerance: estscan allows insertions and deletions  in the est sequence. for example, if it is more probable that a particular nucleotide is the result of an insertion event then it is omitted from the 'corrected' sequence. conversely, if the hmm probability scores suggest that a nucleotide has been deleted then the model inserts an x into the 'corrected' sequence to denote this prediction.

 <dig>  est structure: estscan recognises that the est may be composed of a combination of 5' utr, cds and 3' utr.

estscan's hidden markov models are trained using complete cds entries from either the embl or refseq databases. scripts included with the distribution parse the data files, extracting the necessary sequence information to produce the model files. the major issue considered at this point is redundancy. if the training data is internally redundant then the resultant model will be fully successful only in finding what is known and will have reduced power in detecting novel transcripts. default parameters were used in estscan for building the hmm and in predicting polypeptides.

hsp tiling
the blastx program  <cit>  allows a nucleotide sequence to be searched against a protein database. the nucleotide query is translated in all six frames and these are used as the query sequences for a blastp search. high scoring segment pairs  are identified that maximise a bit score derived from an amino acid similarity matrix. if a single indel occurs in the nucleotide sequence, causing a frameshift, the hsp is either terminated at this position or continues out of frame. downstream of this frameshift the query sequence may be long enough to result in another significant hsp to the same protein sequence, this time in a different frame. simple extraction of the best blast hsp will miss such features. prot4est implements a rule-based method that considers all the hsps for a match to a database sequence and considers whether a frameshift can be identified. where a frameshift is identified the hsps are joined. where two hsps overlap the sequence with the better bit score is used.

the prot4est pipeline
prot4est is an integrated pipeline utilising freely available software in a tiered, rule-based system .

tier 1: identification of ribosomal rna  genes
the protein databases contain  translations of ribosomal rna genes and gene fragments, and thus it is important to identify and remove putative rrna derived sequences before further processing. a blastn search is performed against a database of rrna sequences obtained from the ribosomal database ii . a blast expect value cutoff of e- <dig> is used to identify matches. the cutoff is a conservative one to reduce the number of false positives. those nucleotide sequences with significant matches are annotated as rrna genes and take no further part in the translation process.

tiers  <dig> and 3: similarity search
the second and third stages are similar. first a blastx search is performed against proteins encoded by mitochondrial genomes. the mitochondrial protein database is obtained from the ncbi ftp site . any sequences with significant hits  are annotated as mitochondrion-encoded genes for the remainder of the process, and the relevant mitochondrial genetic code is used for translation. sequences that do not have significant similarity to mitochondrial proteins are compared using blastx to the swissprot database  <cit> . sequences that yield no significant similarity are moved onto tier  <dig> of the process.

for those sequences that show significant similarity to a protein sequence from either database a hsp tile path is constructed. prot4est then considers whether the nascent translation can be extended at either end in the same reading frame.

tier 4: estscan prediction
the hidden markov models used by estscan to identify the coding region are constructed from embl format files for complete cds using scripts supplied with the package. preprocessing is integrated within prot4est, including the downloading of the embl files. a pair of length threshold criteria are applied to each putative polypeptide before it is accepted. the open reading frame must be at least  <dig> codons in length, and cover at least 10% of the input sequence. polypeptides that satisfy these criteria undergo the extension process described above, sequences that fail any of the criteria are passed onto the next tier. the extension process is carried out on those sequences that exceed the thresholds.

tier 5: decoder prediction
the decoder program is used to predict cds and thus polypeptide translations for the remaining nucleotide sequences. for each sequence a quality file in phrap format is required. when a quality file is unavailable a file with quality values of  <dig> is generated for each sequence. the codon usage table required by decoder can be specified by the user or downloaded from cutg, the codon usage table database  <cit> . by default decoder only processes the forward strand of each sequence, and therefore the reverse complement of each sequence is taken and processed through decoder. two putative polypeptides are generated for each nucleotide sequence. the longer polypeptide is selected as the more probable translation. the polypeptide predictions are checked using the same length threshold criteria as for estscan .

tier 6: longest orf
this last attempt to provide a putative polypeptide translation determines the longest string of amino acids uninterrupted by stop codons from a six-frame translation of the sequence. if a methionine is present in this string it is flagged as a potential initiation site.

output
the primary output from prot4est consists of the putative polypeptides in fasta format, complemented with files containing information describing the translated sequences. this information includes:

position of the translation with respect to the nucleotide sequence, the genetic code used for translation,

position and blast statistics of hsps used in the tile path.

all this additional information is stored in two csv format files, permitting parsing and simple insertion into a database.

speed
this is highly dependent upon the composition and size of the dataset. as a guide, each prot4est run carried out in the benchmarking , took less than an hour for a 2316-sequence input with an athlon  <dig> mhz processor. the blastx searches were carried out separately and used as input to prot4est .

benchmarking est translation methods
we benchmarked five translation methods to test their relative performance. decoder is designed to consider only the forward strand of the nucleotide sequence, as it was originally designed for full-length cdss. when applied to ests it is imperative that both strands are analysed, as both 5' and 3' ests are generated. therefore the reverse complement of each nucleotide consensus was also analysed. decoder_default  considers only the prediction from the forward strand, whilst decoder_best  uses the more accurate prediction. estscan  considers both strands of the nucleotide sequence, and was run as a stand-alone process with default settings.

two arrangements of components within prot4est were tested. prot4est_ed  implements estscan before using decoder on any remaining untranslated sequences. conversely, prot4est_de  uses decoder first followed by estscan. the decoder module in prot4est considers translations on both the foward and reverse strands of the query sequence.

 <dig> data sets
test est dataset for translation
we randomly selected  <dig> caenorhabditis elegans ests from dbest  <cit> . to reduce redundancy, the ests were clustered using clobb  <cit> . phrap  <cit>  was then used to derive a consensus sequence for each cluster. this resulted in  <dig> nucleotide sequences. to ensure that the consensuses corresponded to a coding region, we carried out a blastn search for each consensus against the complete c. elegans cdna dataset available from wormbase   <cit> . significant matches were found for  <dig> consensuses. finally, this set was used to query the c. elegans protein dataset , thus associating each nucleotide sequence with a corresponding reference polypeptide. a final test set of  <dig> consensus sequences was produced.

training datasets
1: caenorhabditis elegans
both estscan and decoder require prior gene sequence. the c. elegans refseq collection was obtained, comprising  <dig> entries . a perl script constructed random training sets giving differing totals of coding nucleotides from  <dig> to  <dig>  four sets were assembled for each level. the build_tables script  was used to filter out sequences  <cit> .

we used the same training sets to build the codon usage tables required by decoder. cusp from emboss  <cit>  was used to build the tables, and a separate perl script written to convert the output to that required by decoder. for any given run of prot4est the estscan hmm training set and codon usage table used were derived from the same training set of c. elegans cdnas.

2: prokaryote genomes
genbank entries from  <dig> complete prokaryote genomes were obtained . a perl script was written to extract the cds entries and construct a refseq-style resource for each prokaryote species . if a taxon's genome consisted of more than one megaplasmid the sequences were combined. cds annotation was not available for  <dig> genomes. we used the cds collections for the  <dig> taxa to determine at content, construct hidden markov models and codon usage tables.

3: arabidopsis thaliana
 <dig> complete cds entries for a. thaliana were obtained from the refseq database  <cit> .

4: spirurida 
we queried genbank for all complete cds entries from species in the nematode order spirurida.

blast databases
swissprot  and trembl   <cit>  were combined to give a swissall database. to recreate the situation facing neglected genome analysis, the accession numbers for all proteins from species in the nematode orderrhabditida were retrieved from the newt taxonomic database  <cit>  and these entries  were removed from swissall.

 <dig> data collection and analysis
comparison of predicted polypeptides to the 'true' polypeptide
we compared each putative polypeptide predicted from the c. elegans test dataset to its cognate reference protein using bl2seq from the ncbi distribution. default parameters were used except for the theoretical database size , set to  <dig>  the size of swissprot. the blast reports were parsed using bioperl modules  <cit> . each c. elegans reference protein sequence was also compared to itself using bl2seq with default parameters. the raw and bit scores were recorded.

calculation of comparison statistics
the raw and bit scores were normalised for length and against their theoretical maximum using equation  <dig>  where:

bitlocal is the bit score of the local alignment between the predicted polypeptide and its cognate reference protein,

bitmax is the bit score for the alignment between the reference protein and its self,

wplength is the length of the wormpep protein that is the reference of the nucleotide consensus translated,

estlength is the length of the nucleotide consensus that has been translated.





RESULTS
to measure the accuracy of translation two statistics were derived from the comparison of the predicted and reference polypeptides. the coverage is the percentage of the predicted polypeptide that aligns with the reference. the bit score represents the total of the alignment's pair-wise scores, normalised with respect to the substitution matrix used to calculate these scores. in this study the bit score was itself normalised to compensate for est length and the maximum possible bit score for each comparison . the number of consensuses translated that had a significant match to their cognate reference c. elegans protein was also recorded for each run.

the influence of number of training codons
both variants of decoder were unable to produce robust translations for over half the nucleotide sequences no matter how many nucleotides were in the training set . as expected, the inclusion of the reverse complement in the decoder analysis improved its performance. the inability of decoder to translate more than 50% of the polypeptides can be traced to its core assumptions. one criterion used is the determination of the most likely initiation methionine. while this is almost always present in full length cdnas , the occurrence of any atg codon in est consensuses is less certain. we noted that decoder will try any atg codon to start its prediction, even if this results in a polypeptide of  <dig> amino acids in length.

the effect of the number of training nucleotides on estscan performance is pronounced. for the majority of the replicates, at each training set size the fraction of predictions that have significant matches to their reference sequence was around 75%, but the number of translations dropped significantly below  <dig> training nucleotides. however, for  <dig> coding nucleotides or less no robust translations are produced. additionally, there was variance in the performance of estscan when there were between  <dig> and  <dig> training nucleotides. examination of these training sets showed no difference in at content compared to larger training sets, but did suggest that fluctuations in codon usage bias might be involved. the replicates that performed less well comprised sequences with shorter mean length, and had codon biases that were at the extremes of the distribution . this variation in sequence composition clearly has an effect on the probabilities that populate the hmm used by estscan. we suspect that the ability of estscan to predict robust translations when trained by datasets of  <dig> to  <dig> coding nucleotides is inflated as a consequence of the random selection of the training set from the complete c. elegans transcriptome. in a genuine situation, when only a small number of full-length cds exist in the public databases, a significant number will be from highly expressed genes with atypical codon bias and structure. this bias will be evident in real-world cds sets with fewer than  <dig> members .

when the training sets contained a large number of non-redundant coding nucleotides , prot4est_ed and estscan performed equally well . when the number of coding nucleotides available for training and codon bias determination were reduced, prot4est translations still showed significant similarity to the correct protein in at least 80% of instances.

the translations produced by prot4est_ed were the most robust across all totals of coding nucleotides, for both coverage and bit score . as the number of coding nucleotides used in training decreased, both measures showed slight reductions.

performance of alternative prot4est architectures
prot4est_ed produced more robust translations for higher numbers of training sequences. however when smaller totals of training nucleotides were used the translations produced by the alternative architecture, prot4est_de, were slightly better , although a smaller proportion of translations were produced with this setup .

the better performance of prot4est_ed was examined by following the fate of individual test sequences through the prot4est pipeline. by employing estscan before decoder, larger training sets allowed the deployment of well trained hmms . all predictions satisfied length and quality filters, and so were accepted as robust. the corresponding decoder predictions, while satisfying length filters, were not as robust. as the training sets decreased in size, the estscan predictions failed the filters and so were ignored, and decoder used instead.

performance of similarity search
seven sequences out of  <dig> were identified as rrna in tier  <dig>  tiers  <dig> and  <dig> of the prot4est pipeline exploit any significant sequence similarity between the query sequence and known proteins for coding region determination. this approach identified coding regions from just under half of the consensuses,  <dig>  nineteen were identified as mitochondrial genome derived. to benchmark the similarity approach against the other probabilistic methods, the accuracy of predictions from  <dig> consensuses were compared. translations derived from prot4est tiers  <dig> and  <dig> were more robust than those from estscan or decoder .

given that an increase in the number of non-redundant coding nucleotides used to train estscan produces more robust translations, we attempted to use coding regions determined thus far to create larger training sets, with the expectation of improved translations. the results from the blastx search against the swissall database were checked for matches where the alignment included the start of the protein sequence. these results contained the information required to construct pseudo-cds entries which can be added to the training set for populating the hmms of estscan. in this study there were only six blastx alignments that provided suitable pseudo-cds, failing to provide any significant increase in the level of non-redundant coding nucleotides. however other species we study have produced higher numbers of pseudo-cds which prot4est uses to give improved translations .

effect of training set and target set sequence composition
as a significant proportion of any est set will not share similarity with known sequences, de novo translation methods need to be trained to as high a level as possible. the question is how this should be done, given the paucity of prior sequence data for individual species. should cds from species considered phylogenetically related be combined or should a large set from a model organism be used? a recent study of gene finding in novel genomes has shown a significant effect of sequence composition upon gene structure prediction, with more closely related model genomes providing poor training if the codon bias differs significantly from the genome of interest  <cit> . the performance of estscan was affected by even slight fluctuations in sequence composition. we examined the effect of at content on the accuracy of translation. the complete cds complements of  <dig> prokaryotes were assembled as described in the methods. this gave a range of at contents from 28%  to 78% , independent of any bias due the organisms' relatedness to c. elegans. the lowest number of non-redundant coding nucleotides was  <dig> , in excess of the minimum number suggested for robust training. to explore datasets from more closely related sources all available cds entries for the nematode order spirurida , and the plant arabidopsis thaliana  <cit>  were obtained.

there was a significant correlation between at content of the training set and the coverage by the putative polypeptides of their reference c. elegans proteins  . the most robust predictions were produced by hmms trained on datasets with an at content similar to that of c. elegans. for the prokaryote training sets, the number of nucleotides used had no significant effect upon performance . we note that some prokaryote training sets with at contents close to c. elegans performed poorly: homogeneity of at content is thus not a panacea. the best performance was obtained using the a. thaliana training set, with significantly better coverage than achieved with the more closely related spirurida. as the plant dataset contained  <dig> times as many coding nucleotides as did the spirurida training set, four random a. thaliana training sets of comparable size to the spirurida were built. these smaller training sets still performed better than the spirurida training set, though not as well as the full cds collection.

CONCLUSIONS
prot4est is a protein translation pipeline that utilises the advantages of a number of publicly available tools. we have shown that it produces significantly more robust translations than single methods for species with little or no prior sequence data. around three quarters of current est projects are associated with training sets of <  <dig> coding nucleotides . thus prot4est offers significant improvement in this real world situation. even with substantial numbers of coding nucleotides, the use of similarity searches means prot4est is able to outperform the best de novo methods. given the increase in protein sequences submitted to swissprot/trembl, prot4est's ability and accuracy can only increase over time. these more accurate translations provide the platform for more rigorous down-stream annotation. currently we are using the prot4est pipeline to translate ~ <dig> nematode consensus sequences from  <dig> species. these translations will then be passed onto other tools we are developing for est analysis and annotation .

availability and requirements
project name: prot4est

project home page: 

operating system: fully tested on linux – redhat <dig> , fedora <dig> .

programming language: perl

other requirements:

estscan <dig>  

decoder rgscerg@gsc.riken.go.jp

bioperl  <dig>  

transeq 

license: gnu gpl

any restrictions to use by non-academics: none for prot4est source code. decoder requires a license. see user guide.

authors' contributions
jw performed all the analyses and wrote all the perl code. mb oversaw the project and suggested additional features.

both authors shared responsibility for writing this manuscript.

