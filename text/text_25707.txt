BACKGROUND
covariance matrices are key tools in the study of the genetics and evolution of quantitative traits. the g matrix, containing the additive genetic variances and covariances for a set of characters, summarizes the genetic architecture of traits and determines their short-term response to multivariate selection along with the constraints this response will face. the more easily estimated matrix of phenotypic variances and covariances p can be used as a surrogate for g, especially in the case of high heritability morphological characters  <cit> . comparisons between covariance matrices are carried out in the study of a wide array of evolutionary problems, such as the stability of g in the presence of selection and drift  <cit> , the role of genetic constraints on determining evolutionary trajectories in adaptive radiations  <cit> , the response of genetic architecture to environmental heterogeneity  <cit> , the evolution of phenotypic integration  <cit> , multi-character phenotypic plasticity  <cit>  and sexual dimorphism  <cit> .

several methods for the comparison of covariance matrices are available . they range from the most mathematically sophisticated, such as those using maximum likelihood  <cit>  or bayesian frameworks  <cit> , to simple methods that are useful for exploratory analyses and are not dependent on distributional assumptions. the simplest methods  <cit>  are based on pair wise comparisons of the matrices’ elements, so that they typically ignore the lack of independence between these values, cannot detect proportionality between matrices, and consider two matrices only. more recent procedures, also using the matrix elements  <cit> , take into account these elements’ lack of independence and permit the joint consideration of several matrices, making it possible to study the contribution of identified external factors to the magnitude of the differentiation.

other simple procedures  <cit>  are based on comparisons between vectors resulting from the product of the studied matrices and sets of test vectors, their rationale being that similar matrices would produce similar results when multiplied by the same sets of vectors. however, most of these procedures result in a single measure of the divergence between matrices that does not provide information about the nature of this divergence. such information is provided by cpca , which uses the flury  <cit>  hierarchy, a maximum likelihood-based procedure to compare the structure of two or more matrices and sequentially test if the matrices are “unrelated” ; if they have some number of eigenvectors in common, if they are proportional , and finally if they are equal . then it determines which of these descriptions best fits to the relationship between the matrices’ structures.

among the limitations of cpca are, first, that it is based on the assumption of multivariate normality, and second, that it results in categorical, not continuously varying measures of matrix similarity  <cit> . the cpca consists in a series of yes/no comparisons between ordered eigenvectors, which allow testing a full series of hypotheses about the relationship between matrices in a hierarchical way, but idoes not have an associated parameter measuring the degree of similarity, relying only on the results of the significance tests. this limitation can be serious in some situations. two matrices are declared as “unrelated” when that is the best fit of all null hypotheses tested, but this result does not preclude the existence of any similarities between them  <cit> . in fact, the procedure may declare two matrices as “unrelated” when many data are available and there is great power to detect differences, even if these differences are trivial from a biological point of view  <cit> . thus, there is no simple relationship between matrix similarities measured by cpca and other matrix comparison procedures  <cit> .

in the present work i propose a new, simple and distribution-free procedure for the exploration of differences between covariance matrices that, in addition to providing a single and continuously varying measure of matrix differentiation, makes it possible to analyse this measure in terms of the contributions of differences in matrix orientation and shape. i use both computer simulation and p matrices corresponding to snail morphological measures to compare this procedure with some widely used alternatives. i show that the new procedure has power similar or better than that of the simpler methods, and how it can be used as the basis for more detailed analyses of the nature of the found differences.

pairwise matrix comparison
the rationale for the comparison procedure is that, when the covariance matrices of two data samples are similar, the eigenvectors obtained in a principal component analysis of any of them will explain similar amounts of variation in both samples. the degree of similarity can be measured by calculating in each sample the individuals’ values and variances for the eigenvectors obtained in the other sample. given that d <dig> and d <dig> are the matrices with the characters’ measures in the two samples and x <dig> and x <dig> the matrices containing in their columns the eigenvectors of these samples’ covariance matrices, the variances of the columns of the products d1x <dig> and d2x <dig> are the corresponding eigenvalues, i.e., the amounts of variance explained by the original eigenvectors, and those of d1x <dig> and d2x <dig>  the amounts of variance explained by the eigenvectors from the compared sample. thus, for each of the n  pairs of eigenvectors obtained in the analysis of the two samples it is possible to calculate vi <dig>  vi <dig>  vi <dig>  and vi <dig>  where vi <dig> is the amount of sample  <dig> total variance explained by eigenvector i from sample  <dig>  vi <dig> the amount of sample  <dig> total variance explained when applying eigenvector i from sample  <dig>  and so on. these n sets of four values are the basic items to measure the similarity in covariance between samples. i define three sums:

 s1=2∑i=1nvi11−vi212+vi12−vi <dig> 

 s2=∑i=1nvi11+vi22−vi12+vi <dig> 

 s3=∑i=1nvi11+vi12−vi21+vi <dig> 

where s <dig> is a general measure of differentiation depending on the ability of the eigenvectors from each sample to explain the variation in the other sample; s <dig> is a measure of the contribution of between-matrix differences in orientation  to s <dig>  and s <dig> that of differences in shape . it can be shown  that s <dig> = s <dig> + s <dig>  figure  <dig> shows a graphical interpretation of these statistics. in this figure, row a shows that s <dig> is the only non-zero component of s <dig> when the matrices differ in shape but not in orientation, and row b that s <dig> is the only non-zero component when the matrices differ in orientation but not in shape. note however that s <dig> and s <dig> do not measure changes in orientation or shape as such. what these statistics measure are the consequences of changes in orientation and shape on the more functionally relevant proportions of variance explained, i.e.,. on the amount of variation available in each multidimensional direction. the effects of these changes are not independent. the comparison between figure  <dig> rows b and c shows that the impact of changes in orientation on the proportion of variance explained by the eigenvectors of the reciprocal matrix depends on the matrices’ shape. the effect is larger for more “elongate” matrices. similarly, the comparison between rows a, d and e shows that the effect of changes in shape depends on the matrices’ orientation, and that changes in matrix shape become irrelevant  when the first eigenvectors of the two matrices are perpendicular . finally, row f shows that there are no differences in orientation when one of the matrices is perfectly “round” . despite this complexity, and as shown in the practical application made below, statistics s <dig> and s <dig> may be very useful to analyse the nature of the differentiation between two covariance matrices.


the s statistics are easier to compare between studies if they are made to vary between zero and one by making them relative to their maximum possible value. this maximum would occur in the extreme situation in which single eigenvectors explain all variation in each of the compared samples, and the eigenvectors of the two samples are orthogonal. in that case, s <dig> is equal to  <dig> times the sum of twice the square of the total variance of the first sample and twice the square of the total variance of the second sample. when the variances explained by each eigenvector are expressed as proportions of the total variance, so that the sum of all the proportions is equal to one, the maximum possible value for s <dig> is equal to eight. in the computer simulation and real data example shown in this article, the explained variances are expressed as proportions, and the s <dig>  s <dig> and s <dig> statistics divided by eight so that they could vary between zero and one.


RESULTS
i contrasted the results obtained with the proposed procedure with those from other widely used ones, namely cpca and two simpler procedures providing single measures of matrix differentiation: one, the random skewers, based on products with test vectors, and the other, the t method, based on the comparison of matrix elements . i followed two approaches. first, i studied the procedures’ power and type i error through computer simulations that considered covariance matrices differing in shape, orientation or both, in different number of variables and sample size situations. second, i compared their ability to detect differences between covariance matrices of shell measures from different morphs and populations of the seashore snail littorina saxatilis.

computer simulation

littorina data
the eigenvectors and eigenvalues of the six samples’ covariance matrices are shown in table  <dig>  the first eigenvectors always had coefficients of the same sign, as expected for size descriptors  <cit> , and were similar for all matrices. the remaining eigenvectors included coefficients of contrasting signs, as expected if they were measuring variation in shell shape. the bootstrap distributions of the rs, t%, s <dig>  s <dig> and s <dig> statistics for  <dig> comparisons are shown in figure  <dig>  not unexpectedly, some basic common trends were observed in all these statistics. matrices in within-sample comparisons tended to be found more similar than those in within morph comparisons, and these, more than matrices in between morph comparisons. also, particular between-samples comparisons resulting in large differences when applying a procedure tended to do the same in the remaining procedures. the t method was, of those shown in figure  <dig>  the least sensitive to matrix differences. in the case of the rs, s <dig> and s <dig> statistics, the bootstrap distributions of the within sample and between morphs comparisons had little overlap, showing that these statistics can easily detect rather modest differences  between matrices using samples of moderate size. the rs method tended to provide a rather definite separation among the results of four kinds of comparisons: within samples, within rbs, within sus and between morphs. the profile of the s statistics’ comparisons was rather different from that obtained with the rs procedure, indicating that the two methods are not equivalent and that they consider matrices’ properties in different ways.



sample
eg1
eg2
eg3
eg4
eg5
eg6
eg7
the columns show each eigenvector’s coefficients for the variables measured in each sample, along with the corresponding absolute eigenvalues  and the same eigenvalues as percentages of the total variance in the sample .

at least in the particular example analyzed here, differences related with matrix shape had the largest weight in the overall measure of differentiation s <dig>  as the comparison results profiles of s <dig> and s <dig> were the most similar. the statistic s <dig> found large differences both between morphs and within the su morph. the largest differences for s <dig> corresponded always to comparisons involving the matrix of the sus from location  <dig>  i.e., comparisons  <dig>   <dig>   <dig>   <dig> and  <dig> in figure  <dig>  this suggested that the shape of this matrix had some particularities, and in fact table  <dig> shows that the first and second eigenvalues for this sample were unusually small and large, respectively. a more detailed analysis of the s statistics  was consistent with this interpretation: the largest contributions to s <dig> were made by the first eigenvector and occurred in comparisons involving the su <dig> sample.


the statistic s <dig> found the most striking contrast between kinds of comparisons in figure  <dig>  differences between morphs being clearly the largest. thus, a large proportion of the differentiation between the two morphs’ covariance patterns was due to a change in matrix orientation not present in the comparisons within morphs. figure  <dig> shows that the largest contributions to s <dig> were made by the second and third eigenvectors. the examination of these eigenvectors’ coefficients in table  <dig> shows that the second eigenvector from the rbs was similar to the third eigenvector from the sus, and vice versa. the second eigenvector from the rbs and the third from the sus could be roughly described as a contrast of measure  <dig> against the rest, and the third eigenvector of the rbs and the second from the sus as a contrast of measure five against the rest. thus, the two morphs’ eigenvectors had very similar structure, but the proportion of variance explained by these eigenvectors was clearly different, to the extent that the ranks of the second and third eigenvectors in one morph got reversed in the other morph. this change in matrix orientation introduced large differences in the proportion of variance explained by the reciprocal eigenvectors. this is illustrated in figure  <dig>  representing the amount of variance explained in a sample by the eigenvectors of the compared sample . it can be seen that the differentiation between the rb and su matrices is related to a reversal in the variances explained by the second and third eigenvectors. in both morphs, the third eigenvector from the reciprocal morph explains more variation than the second reciprocal eigenvector. the figure shows also that the lowest differentiation was in comparisons involving sample su <dig>  again, the inspection of table  <dig> results in an easy interpretation: the second and third eigenvalues were very similar in this sample , so that reversing the order of the corresponding eigenvectors  had very slight effects on the proportion of variance explained. note in figure  <dig> that one of the segments in the circles is nearly horizontal for the comparisons involving sample su <dig>  not all results had a straightforward interpretation in terms of eigenvector coefficients. for example, figure  <dig> shows that the s-statistics comparisons between the rb and su samples from location  <dig> resulted in markedly lower differentiation than all the remaining between morph comparisons. this shows that the s procedures consider aspects of matrix structure beyond the individual values of eigenvector’s coefficients.


the overall agreement between cpca and the other procedures found in the simulations was lost in the analysis of littorina data. . according to the rs, t% and s statistics, the three rb samples had very similar covariance matrices, but the cpc procedure determined that rb <dig> and rb <dig> were “unrelated”, and that rb <dig> and rb <dig> had only one eigenvector in common. the few comparisons finding some eigenvectors in common did not correspond to particularly low measures of differentiation by the other methods. this shows that the cpc analysis did not consider the matrices’ properties in the same way.

discussion
the s statistics constitute sensitive tools for the detection of differences between covariance matrices. in the littorina example used here, it was found that the local differentiation was clearly higher for the su than for the rb morph. this could be due to lower genetic connectivity for populations of the su morph, and also to environmental differences between localities. however, the absence of such differences between the rbs in the same localities would imply that any environmental differences would exist not between whole localities but only between the micro-habitats the su snails use in the mid-shore. the differences between morphs, and specially those due to changes of orientation in the covariance matrices, were the largest in the analysis, and if not of the same size, they were of the same nature in all locations. since shell morphology variation is adaptively important in littorina <cit> , this suggests that these differences in covariance could be relevant for the evolutionary dynamics of the hybrid zone. they could simply result from the environmental differences between the two morphs’ microhabitats within the midshore, but even these environmental differences could have a genetic origin because individuals of the two morphs, even when living within extremely short distances of each other, tend to actively choose microhabitats with different physical characteristics  <cit> .

the computer simulations shown in figure  <dig> are limited to measure the power of the considered procedures to detect the consequences of changes in matrix orientation and shape. they show that such differences can be detected even when moderate in magnitude and when sample sizes are not too large, but cannot be taken as complete or definitive comparisons between the procedures. matrices may differ in many relevant aspects, and different comparison procedures may have different aims and take different aspects into consideration. for example, while the s measures consider the differences in eigenstructure between two matrices, the rs procedure focuses on the related, but not equivalent problem of the differences between the evolutionary responses generated by these matrices. comparisons would be even more difficult with procedures such as the set of evolvability measures proposed by hansen and houle  <cit> , which consider the magnitude of the populations’ responses to different natural selection regimes.

since the s statistics introduced here simply measure what proportion of variation exists in a given sample along the axis of variation defined by the eigenvectors in the compared sample, they are similar to the rs and t% ones in that they do not compare and are not dependent on the matrices’ sizes. they focus instead on the more interesting differences in matrix shape and orientation. in any case, s statistics-based comparisons could use raw covariance components instead of proportions as in the example shown, so that the results would depend on between-matrix size differences. however, in that case the s statistics would not be able to separate the effect of size from those of other sources of differentiation between matrices. similarly, the basic version of the t method proposed in  <cit>  reflects the differences in matrix size, as it is based on raw variance components instead of the proportions used by the t% statistic.

calculating the amount of variance explained by a set of eigenvectors in a given dataset is straightforward in the case of datasets containing the phenotypic measures used to obtain p matrices. in the case of g matrices, the comparison would have to be based on additive genetic value estimates for individuals or families.

since the proposed procedure is limited to two-sample comparisons, it cannot be used to make higher order analyses of the divergence among several populations . however, and as shown in the litttorina example, it can be useful for the study of evolutionary relevant situations such as hybrid zones. the s measures appear to be similar to the measure of distance between covariance matrices used by mitteroecker and bookstein  <cit> , based on the calculation of relative eigenvalues, i.e., the eigenvalues of the product of one matrix premultiplied by the inverse of the second matrix, and therefore on expressing the variances and covariances of one sample relative to those of the other sample. i calculated the correlation of this measure with s <dig>  s <dig> and s <dig> in a computer simulation considering the same cases as in figure  <dig> and found that, while the measures were clearly related, they were not equivalent. for example, in the simulation of four variables and sample size  <dig>  the correlation across replicates of the mitteroecker and bookstein measure with s <dig>  s <dig> and s <dig> were: in the comparison of one population with itself,  <dig> ,  <dig>  and  <dig>  respectively; in the case of divergence in orientation,  <dig> ,  <dig>  and  <dig> ns; in the case of divergence in matrix shape,  <dig> ,  <dig>  and  <dig> ; and in the case of divergence in both orientation and shape,  <dig> ,  <dig>  and  <dig>  .

the littorina example supports the view that cpca might lead to misleading conclusions about the overall similarity between matrices  <cit> , as pairs of matrices found very similar by other procedures were declared as “unrelated” by cpca, and there was no clear correspondence between the two sets of results. the observed between-morphs reversal in importance of the second and third eigenvectors could play a role in this discrepancy. cpca is based on a series of paired comparisons between eigenvectors of the same rank. two matrices may share their axes of variation, but not the amount of variance in each axis. for example, the ith eigenvector of one matrix might be the same as the i+ <dig> eigenvector of the other, and the i+ <dig> of the first, the same as the ith of the second. in that case, the two matrices would have the same eigenvectors, but in a reverse order. a comparison between their ith eigenvectors would find them orthogonal, and this would also be the case for their i+ <dig> eigenvectors. thus there may be considerable similarity between the two matrices, but this similarity is overlooked by the comparison procedure which finds the paired eigenvectors very different. the cpc software  <cit>  enables users to compare the eigenvectors in any order, but this does not fix this particular limitation, as the order chosen for the two samples must be the same. the t and rs methods, based on matrices’ elements and product vectors, would provide a more balanced measure of similarity in this situation because the differences between these elements and these vectors would not depend on the existence of reversals in eigenvector order per se, but on the magnitude of the differences involved. however they don’t allow further analysis of the pattern of differentiation. the three s statistics are affected by different patterns of divergence, so that their joint use provided a deeper view of the differences between the two morphs’ p matrices. the s <dig> statistic is not dependent on the eigenvectors’ ordering per se because it is based on comparisons within eigenvector, i.e., on the difference between the amount of variance explained by one eigenvector from one sample in the original and reciprocal samples. these differences do not change with eigenvector order. but s <dig> changes when the order of eigenvectors in one of the samples is reversed  because this would be considered as a change in matrix orientation. in case the reversal in eigenvectors’ importance was complete, so that there were no changes in overall shape, s <dig> would remain unaffected . however, the reversal of the second and third eigenvectors between morphs cannot fully explain the disagreement between cpca and the remaining methods because the results for s <dig> were not particularly similar to those of cpca . this suggests that other aspects of covariance matrix structure might control the degree of agreement between different comparison procedures.

CONCLUSIONS
the s-statistics procedure provides a simple and continuously-varying overall measure of differentiation that is distribution free and interpretable in terms of changes in matrix orientation and shape. in addition, it makes it easy to study the contribution of the different eigenvectors to the statistics values, which could provide further details on the nature of the differentiation, as was the case of the littorina example used. this procedure could thus fill the gap between simpler statistics such as t% and rs, and more analytical methods like cpca or bayesian mcmc. the s-statistics procedure is not based on a formal analysis of matrices’ properties. instead, it could serve for a simple and fast exploration of the magnitude and nature of the differentiation.

