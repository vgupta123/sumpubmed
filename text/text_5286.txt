BACKGROUND
our ability to obtain genome sequences is quickly outpacing our ability to annotate genes and identify gene functions. the automated annotation of gene models in newly sequenced genomes is greatly facilitated by easy-to-use software pipelines  <cit> . while equally as important as gene the gene models, functional annotation of proteins is usually limited to an automatic processing with sequence similarity searching tools such as blast, followed by extensive manual curation by dedicated database curators and community annotation jamborees. there is increasing demand to quickly annotate newly sequenced genomes so that they can be used for designing microarray analyses, proteomics, comparative genomics, and other experiments. it is clear that manual gene function annotation cannot be scaled to meet the influx of newly sequenced model organisms.

two of the main considerations for developing a protein function classifier are the sources of evidence, or features, used for assigning functional categories to the proteins and the method used for defining the relationships between the features and the categories. a number of automated methods have been developed in recent years that vary both in the sources of features used as evidence for the classifier and in the classification algorithm used to assign protein functions. the most common feature types rely on sequence similarity, often using blast  <cit>  to identify similar proteins from a large database. classifiers may also utilize protein family databases such as pfam and interpro, bio-chemical attributes such as molecular weight and percentage amino acid content  <cit> , phylogenetic profiles  <cit>  transcription factor binding sites  <cit>  as sources of features. several classifiers have also been developed that utilize features from laboratory experiments such as gene expression profiles  <cit> , and protein-protein interactions  <cit> .

a classifier is used to determine the relationships between protein features and their potential functions. the simplest examples are manually constructed mapping tables which represent the curator's knowledge of the relationships between features and functional categories. one example is interpro2go, a manually curated mapping, of interpro terms to go terms. the goa project relies on several such mappings to provide automated go annotations for users  <cit> . more sophisticated classifiers have been developed using machine learning algorithms which can automatically deduce the relationships between the features and the functional categories based on a set of previously annotated proteins that serve as training examples. the use of machine learning algorithms can improve the accuracy of the annotations by discovering relationships between features and the functional categories that were not discovered by human curators  <cit> . in addition, they can be applied rapidly and consistently to large datasets, saving many hours of human curation.

draft genome sequences and machine generated gene models are becoming available for an increasing number of fungal species but machine annotations of protein functions are still limited. we found that pre-existing functional classifiers are either not amenable to genome-scale analyses or are trained only for a small range of taxa  <cit> . in addition, many of the previously published protein function annotation systems utilize features obtained from laboratory experiments such as transcriptional profiling or protein-protein interaction assays. these methods cannot be applied to most fungal proteomes since experimental data are not available.

classifiers that employ features derived from multiple, heterogeneous data sources usually transform the format of each data type into a common format regardless of the properties of each data source  <cit> . for example, an e-value threshold might be applied to blast search results instead of utilizing the e-value directly. the problems with this approach are that the data from each data source is weighted differently and that distinctions between data points can be lost during transformation. meta-learning classifiers  overcome this problem by training independent classifiers  for each heterogeneous data source and then use the decisions from the base-classifiers as features to train a meta-classifier. in this way, weights of each data source can be learned by the meta-classifier. meta-learning classifiers are useful for combining multiple weak learning algorithms and for combining heterogeneous data sources.

we developed a functional classification system called pogo  that enables us to assign go terms to fungal proteins in a high-throughput fashion without the requiring evidence from laboratory experiments. we incorporated several evidence sources that emulate what a human curator would use during manual protein function annotation and we avoided features that require laboratory experiments or that otherwise could not be applied to newly sequenced genomes. during the course of our experiments, we discovered that taxon-specific classifiers outperform classifiers that are trained with larger datasets from a wide array of taxa. we developed a taxon-specific classifier for fungi and we are currently using it to assign go terms to the proteomes of more than  <dig> filamentous fungi. we provide as web application that enables users to annotate proteins through their web browser.

implementation and 
RESULTS
overview of the pogo classifier
protein functional annotation is a multi-label classification problem in which each protein may be assigned one or more go terms. various approaches may be applied to solving multi-label classification problems, but the simplest method, and the one we employed, is to consider each go term as an independent classification problem. thus, pogo is actually a team of independent classifiers. the disadvantage to this approach is that interdependencies among the labels cannot be incorporated into the classifier, thus potentially increasing error. the advantage, however, is that a wide range of binary classification algorithms may be applied to the problem. in addition, this approach is more flexible in that different algorithms and/or datasets may be used to train the classifier for each go term, enabling us to optimize the classification of individual go terms or groups of go terms if necessary. individual go terms or groups of go terms may also be re-trained, as new training data of evidence sources become available. pogo consists of four base classifiers, each of which utilizes distinct data sources, and a meta-classifier for combining the outputs from the base classifiers into a final classification.

design and evaluation of the base classifiers
interpro
interpro terms  <cit>  are defined in the interpro database which is a curated protein domain database that acts as a central reference for several protein family and functional domain databases, including prosite, prints, pfam, prodom, smart, tigrfams and pir superfamily. we have previously showed that interpro is an important source of features for identifying go terms for proteins  <cit> . using the interproscan application  <cit> , we can obtain interpro terms for unannotated proteins. using interproscan, we identified  <dig> interpro terms in the fungal protein dataset. we employed support vector machines  algorithm as the classifier as described previously  <cit> . for each go term, we construct a dataset that is comprised of all proteins that are annotated with the go term  and all proteins that are not annotated with the go term . since the data sets are highly imbalanced, we perform undersampling of the negative class as described previously  <cit> . this step removes members of the negative class so that the training dataset will have the same number of proteins as the positive class. we also apply chi-square feature selection to remove features from the classifier that do not contribute to the accuracy of the classifier. previous results have shown that these steps improve classification accuracy and reduce learning time  <cit> .

sequence similarity
several previous methods including gofigure  <cit> , goblet  <cit> , and ontoblast  <cit>  use sequence similarity based on blast  <cit>  results as features. goanno  <cit>  is also an extension of the similarity-based annotation using hierarchically structured functional categories. in this example, the annotations assigned to the blast hits are used as features. we employ a similar approach. the feature set is comprised of hit obtained in a blast search of a database of go annotated proteins  using an expect threshold  of 1- <dig>  the blast database was constructed by extracting proteins from the taxonomic group fungi from the uniprot database. the resulting feature set contains  <dig> features. we employ undersampling and feature selection as described previously, and also utilize the svm algorithm to train the classifier.

biochemical properties
other authors have previously shown that biochemical properties of proteins, such as a protein's charge, amino acid composition, are useful for functional classification  <cit> . the properties that we use in this study include amino acid content, molecular weight, proportion of negatively and positively charged residues, hydrophobicity, isoelectric point and amino acid pair ratio. we use the pepstat and compseq programs in emboss  <cit>  to compute the biochemical properties based on the amino acid sequence of each protein. unlike the previous datasets, the biochemical properties are not sparse, and are numeric rather than binary. we compared various forms of undersampling and learning methods and found that with this dataset, the adaboosting method using a linear classifier  <cit>  and using an unbalanced dataset provided the best accuracy .

protein tertiary structure
the fourth feature set is protein structure as computed using the hhpred program  <cit>  which is used for protein homology detection and structure prediction using the scop database  <cit> . we use the top  <dig> selected templates as features and the remaining undefined templates are set to zero for each protein. this dataset is comprised of  <dig> binary features. as described previously, we use the svm algorithm combined with dataset undersampling to create the classifier.

design and evaluation of the meta-classifier
the meta-classifier uses the classification decisions from the base classifiers as its inputs . to develop our meta-classifier, we used the combiner meta-learning strategy  <cit>  with a minor modification. meta-learning strategies such as combiner are trained using base-level classifiers in which each base-level classifier has been trained using the same input data and different classification algorithms. in our case, we vary both the type of classification algorithm and the type of features used for training, although the set of proteins used to derive that used to train each of the base-classifiers are identical. an overview of the training and classification system is shown in figure  <dig>  in the case of combiner, the training dataset t is divided into two equal parts t <dig> and t <dig>  the base-level learning algorithms are trained using t <dig> and then the resulting classifiers are used to predict functional categories for t <dig>  then the process is reversed and t <dig> is used for training and t <dig> is used for classification. the predictions are then used as features for the meta-classifier. to evaluate the performance of the set of base-classifiers together with the meta-classifier, we perform ten-fold cross validation by constructing ten datasets in which 10% of the proteins are held out of the training dataset. thus, t <dig> and t <dig> each contain 45% of the training proteins and the remaining 10% are used for evaluation, in each round of cross validation. we tested two statistical pattern recognition algorithms, naíve bayes and support vector machines , as meta-classifiers. since the meta-data are highly imbalanced, we created a balanced dataset by under-sampling as described previously  <cit>  and compared the performance of naïve bayes and svm classifiers trained with both datasets . the naive bayes classifier trained using the unbalanced dataset provided the best overall performance so it was selected as the meta-classifier.

performance represents an average of all classifiers trained with each data set and is measured by 10-fold cross validation.

performance evaluation
performance evaluation of protein function classifiers performance is a complex issue, since no standard methods exist  <cit> . authors typically utilize standard machine-learning metrics such as sensitivity and specificity but the manner in which they are applied vary, which prevents us from directly comparing the performance of classifiers by comparing the performance values reported in publications. most authors compute performance statistics by first calculating the statistics for each protein individually, and then calculating an average over all of the tested proteins. in a traditional machine-learning approach, performance statistics are calculated for each functional category. the issue is further confounded since the test proteins may be annotated with functional categories that were not within the repertoire of categories that can be predicted by the classifier, and these categories may or may not be treated as classification errors during performance evaluation. furthermore, the set of proteins used for evaluation is not consistent among all of the protein function classifiers. authors usually employ cross-validation in order to evaluate their classifier using hold-out sets from the training data set. standard protein data sets have been developed for annual competitions, such as the one that is often held in conjunction with the annual international conference on intelligent systems for molecular biology  but these data sets are not useful for taxon specific classifiers such as pogo.

since we train an individual classifier for each functional category, we evaluated the performance of each classifier individually and then computed an arithmetic mean of all the classifiers. all performance metrics were computed using 10-fold cross validation. when we compare the performance of pogo to other go term classifiers , we use a hold-out set of  <dig> proteins  that were not used in the training. we calculate the performance of each protein individually, and then report the overall average values. the various classifiers that we tested were all trained using different data sources and were developed to annotate different sets of go terms. as performance metrics, we use sensitivity, specificity and f-measure. sensitivity is the proportion of go term annotations in the training dataset that were correctly annotated by the classifier, and specificity is the proportion of available go term annotations not assigned to the proteins that were also not assigned to the protein by the classifier. we also use f-measure to provide a single overall metric for comparing the performance of various classifiers.

considering only the base classifiers, the interpro term classifier had the highest average f-measure value . surprisingly, the blast classifier had low f-measure as well as the lowest specificity, indicating that it results in a large number of false-positive annotations. the f-measure values of the naïve bayes meta-classifiers are higher than the base-classifiers . the highest f-measure for a meta-classifier  was obtained from the naïve bayes meta-classifier using the unbalanced dataset. thus, the meta-classifier trained with the naïve bayes algorithm using the unbalanced dataset provides superior results over any single classifier and was used for further experiments.

the classifiers for go terms that performed poorly were removed by excluding those that have an f-measure below an arbitrary threshold. we tested a range of threshold values and compared the performance to our previous classifier automated annotation of protein functional class   <cit>  which only uses interpro terms as features. we also compared the performance to multipfam2go  <cit>  a classifier that can assign go terms to proteins on the basis of protein domains that are described in pfam. pogo consistently outperformed aapfc over the range of threshold values tested . the f-measure values obtained with pogo were comparable to, although slightly lower than, those obtained for multipfam2go. for example, at a threshold value of  <dig> , the average f-measure for multipfam2go  <cit>  was  <dig>  while the f-measure for pogo was  <dig>  . pogo was able to assign go terms to considerably more proteins than either aapfc or multipfam2go. we attribute this to the additional feature types, such as sequence similarity and protein structure that were included in the dataset. for proteins that to not have matched to the interpro database, these additional feature types are often times still assigned to the protein, and can be sufficient for predicting the go terms.

properties were computed after removing individual go term classifiers with an f-measure below the indicated threshold.

we also compared the performance of pogo to aapfc, multipfam2go as well as to gotcha  <cit> , gopet  <cit> , and interpro2go  <cit>  by randomly holding out 1% of the training dataset  prior to training pogo, and then using the hold-out set to evaluate the performance of all the classifiers. as shown in table  <dig>  pogo outperforms the other classifiers by a considerable margin.

the evaluation was performed using  <dig>  randomly selected proteins that were held out of the training dataset. all go terms annotated to the evaluation proteins were considered when computing the performance metrics.

evaluation of taxon-specific classifiers
since our goal is to develop a classifier for fungal proteins the previous experiments were performed with a training dataset comprised only of proteins from fungi. larger training datasets generally result in more robust classifiers so we evaluated the performance of pogo when non-fungal proteins are included in the training data. we developed a dataset called fungi-expanded by adding non-fungal proteins to the fungi dataset that contain at least one go term that is found in the fungi dataset. we also prepared another dataset called uniprot that includes all go term-annotated proteins in the uniprot database regardless of the taxonomic origin. in all cases, we removed go terms from the proteins that were annotated with the evidence code iea , and those that had less than  <dig> representative proteins. among these datasets, fungi is the smallest. the uniprot set is composed of  <dig>  proteins and  <dig> go terms, which is approximately  <dig> times larger than fungi in proteins and  <dig> times larger in go terms . in a similar manner, we prepared three more taxon-specific datasets representing the taxonomic groups bacteria, viridiplantae, and vertebrata.

performance comparison of the interpro base-classifier on taxon-specific and the uniprot training datasets. no f-measure threshold was applied while computing the performance metrics.

we compared the performance of classifiers trained with each of these data sets using 10-fold cross validation as described previously. because the training and cross-validation time for all the datasets is prohibitively long we performed these experiments using the interpro base classifier. we also measured the performance of the interpro2go mapping by using it to assign go terms to proteins and measuring performance with sensitivity, specificity, and f-measure. all of the pogo classifiers outperformed the interpro2go mapping . adding additional non-fungal proteins to the fungal specific go terms  added more than three times the number of go terms than the fungi dataset with only a slight reduction in performance . in all cases, the classifiers trained with taxon specific datasets performed better than the classifier trained with the uniprot dataset. since the fungi dataset is significantly smaller than the uniprot dataset, we reasoned that the improved performance could be due to overfitting. overfitting can occur when the training dataset is very small, or the number of features in the model is very large. an overfitted classifier can perform well when the instances  are similar to the ones in the training data, but perform poorly when presented with instances that are not similar to the training data. to determine whether the improved performance is due to over-fitting, we trained pogo classifiers with  <dig> smaller datasets containing the same number of proteins as the fungi dataset. these subsamples were prepared by randomly selecting proteins from the uniprot dataset. if the classifier for the fungi dataset was overfitted, we would expect that it would have better performance than the classifiers derived from the sub-sampled datasets. the average f-measure of the subsampled classifiers is  <dig>  while the f-measure for the fungi classifier is  <dig> , indicating that the fungi classifier is not subject to overfitting.

an important point to consider is that each go term is trained independently, and the number of proteins that are used to train each go term classifier varies considerably, depending on the availability of go annotated proteins in uniprot. if small data sets lead to overfitting, then we would expect to see a correlation between data set size and classifier performance  <cit> , where classifiers trained with a smaller number of proteins would tend to have higher performance. we compared the performance vs. dataset size for go terms that were present in the taxon specific and the randomly undersampled datasets but found no correlation in any dataset . the taxon specific datasets may be considered to be a form of data partitioning that removes distantly related proteins from the data. this, in effect, would cause go terms that are never found within a taxonomic group to be removed from the classifier, thus the error of the individual go term classifier does not contribute to the overall error of the system.

web server
we have developed a web server that enables users to annotate protein sequences using the taxon specific classifiers described in this manuscript . the classification results are stored on the server in flat files, and are presented to the user in a table that contains the annotated go terms as well as and interpro terms and links to a detailed page for each protein. the detail page contains a graphical view of the interpro annotations and along with a list of predicted go terms and descriptions including the category. in addition, the tab-separated data files of the go annotations are available for download, enabling to import of the data into other programs. the classification algorithms, undersampling, feature selection and the computation of performance metrics were performed in matlab  <cit>  using the pattern recognition toolbox  <cit> . the web application, as well as several data handling scripts, were written in php. the source code for the web server have been released under the terms of the bsd license and is available in additional file  <dig> as well as on the pogo home page.

CONCLUSIONS
in this paper, we describe a meta-classifier for assigning go terms to proteins using heterogeneous feature sets. by using multiple, heterogeneous data sources, we developed a classifier that offers improved accuracy compared to previously reported annotation methods. more importantly, this system can assign go terms to a higher proportion of proteins than annotation methods that rely only on one feature type. we also found that taxon-specific classifiers have significantly better performance than species independent systems. functional annotations provided by pogo are being used for fungal comparative genomics projects in our research group. we also provide a public web server where users can annotate protein sequences.

availability and requirements
project name: pogo: prediction of gene ontology terms

project home page: http://bioinformatica.vil.usal.es/lab_resources/pogo/

operating system: os independent. use any modern web browser

programming language: matlab, php

other requirements: none

license: bsd

restrictions for non-academic use: none

abbreviations
svm: support vector machines; go: gene ontology.

authors' contributions
jj proposed the learning scheme and implemented the classifier; gy contributed to the implementation of the web site; mt suggested the basic idea and guided the overall system. jj, gy, ss and mt wrote and revised the manuscript. all authors have read and approved the final manuscript.

supplementary material
additional file 1
supplementary tables.table  <dig> - comparison of classifier methods for assigning gene ontology  terms using biochemical properties. table  <dig> - performance comparison of four different taxon-specific data sets and randomly undersampled uniprot data sets.

click here for file

 additional file 2
software source code and data files.

click here for file

 acknowledgements
this research was supported by funds from the united states department of agriculture  and the ministerio de ciencia e innovación of spain . financial support from the ramón y cajal program from the spanish ministerio de ciencia e innovación is also acknowledged.
