BACKGROUND
cloud computing  <cit>  is at the peak of the gartner technology hype curve  <cit> , but there are good reasons to believe that it is for real and will be important for large scale scientific computing:

1) clouds are the largest scale computer centers constructed, and so they have the capacity to be important to large-scale science problems as well as those at small scale.

2) clouds exploit the economies of this scale and so can be expected to be a cost effective approach to computing. their architecture explicitly addresses the important fault tolerance issue.

3) clouds are commercially supported and so one can expect reasonably robust software without the sustainability difficulties seen from the academic software systems critical to much current cyberinfrastructure.

4) there are  <dig> major vendors of clouds  and many other infrastructure and software cloud technology vendors including eucalyptus systems, which spun off from uc santa barbara hpc research. this competition should ensure that clouds develop in a healthy, innovative fashion. further attention is already being given to cloud standards  <cit> .

5) there are many cloud research efforts, conferences, and other activities including nimbus  <cit> , opennebula  <cit> , sector/sphere  <cit> , and eucalyptus  <cit> .

6) there are a growing number of academic and science cloud systems supporting users through nsf programs for google/ibm and microsoft azure systems. in nsf oci, futuregrid  <cit>  offers a cloud testbed, and magellan  <cit>  is a major doe experimental cloud system. the eu framework  <dig> project venus-c  <cit>  is just starting with an emphasis on azure.

7) clouds offer attractive "on-demand" elastic and interactive computing.

much scientific computing can be performed on clouds  <cit> , but there are some well-documented problems with using clouds, including:

1) the centralized computing model for clouds runs counter to the principle of "bringing the computing to the data", and bringing the "data to a commercial cloud facility" may be slow and expensive.

2) there are many security, legal, and privacy issues  <cit>  that often mimic those of the internet which are especially problematic in areas such health informatics.

3) the virtualized networking currently used in the virtual machines  in today’s commercial clouds and jitter from complex operating system functions increases synchronization/communication costs. this is especially serious in large-scale parallel computing and leads to significant overheads in many mpi applications  <cit> . indeed, the usual  fault tolerance model for clouds runs counter to the tight synchronization needed in most mpi applications. specialized vms and operating systems can give excellent mpi performance  <cit>  but we will consider commodity approaches here. amazon has just announced cluster compute instances in this area.

4) private clouds do not currently offer the rich platform features seen on commercial clouds  <cit> .

some of these issues can be addressed with customized  clouds and enhanced bandwidth from research systems like teragrid to commercial cloud networks. however it seems likely that clouds will not supplant traditional approaches for very large-scale parallel  jobs in the near future. thus we consider a hybrid model with jobs running on classic hpc systems, clouds, or both as workflows could link hpc and cloud systems. commercial clouds support "massively parallel" or “many tasks” applications, but only those that are loosely coupled and so insensitive to higher synchronization costs. we focus on the mapreduce programming model  <cit> , which can be implemented on any cluster using the open source hadoop  <cit>  software for linux or the microsoft dryad system  <cit>  for windows. mapreduce is currently available on amazon systems, and we have developed a prototype mapreduce for azure.

RESULTS
metagenomics - a data intensive application vignette
the study of microbial genomes is complicated by the fact that only small number of species can be isolated successfully and the current way forward is metagenomic studies of culture-independent, collective sets of genomes in their natural environments. this requires identification of as many as millions of genes and thousands of species from individual samples. new sequencing technology can provide the required data samples with a throughput of  <dig> trillion base pairs per day and this rate will increase. a typical observation and data pipeline  <cit>  is shown in figure  <dig> with sequencers producing dna samples that are assembled and subject to further analysis including blast-like comparison with existing datasets as well as clustering and visualization to identify new gene families. figure  <dig> shows initial results from analysis of  <dig>  sequences with clusters identified and visualized using dimension reduction to map to three dimensions with multi-dimensional scaling mds  <cit> . the initial parts of the pipeline fit the mapreduce or many-task cloud model but the latter stages involve parallel linear algebra.

state of the art mds and clustering algorithms scale like o for n sequences; the total runtime for mds and clustering is about  <dig> hours each on a  <dig> core commodity cluster obtaining a speedup of about  <dig> using a hybrid mpi-threading implementation on  <dig> core nodes. the initial steps can be run on clouds and include the calculation of a distance matrix of n/ <dig> independent elements. million sequence problems of this type will challenge the largest clouds and the largest teragrid resources. figure  <dig> looks at a related sequence assembly problem and compares performance of mapreduce  with and without virtual machines and the basic amazon and microsoft clouds. the execution times are similar  showing that this class of algorithm can be effectively run on many different infrastructures and it makes sense to consider the intrinsic advantages of clouds described above. in recent work we have looked hierarchical methods to reduce o execution time to o or o and allow loosely-coupled cloud implementation with initial results on interpolation methods presented in  <cit> .

one can study in  <cit>  which applications run well on mapreduce and relate this to an old classification of fox  <cit> . one finds that pleasingly parallel and a subset of what was called “loosely synchronous” applications run on mapreduce. however, current mapreduce addresses problems with only a single  mapreduce iterations, whereas there are a large set of data parallel applications that involve many iterations and are not suitable for basic mapreduce. such iterative algorithms include linear algebra and many data mining algorithms  <cit> , and here we introduce the open source twister to address these problems. twister  <cit>  supports applications needing either a few iterations or many iterations using a subset of mpi - reduction and broadcast operations and not the latency sensitive mpi point-to-point operations.

twister  <cit>  supports iterative computations of the type needed in clustering and mds  <cit> . this programming paradigm is attractive as twister supports all phases of the pipeline in figure  <dig> with performance that is better or comparable to the basic mapreduce and on large enough problems similar to mpi for the iterative cases where basic mapreduce is inadequate. the current twister system is just a prototype and further research will focus on scalability and fault tolerance. the key idea is to combine the fault tolerance and flexibility of mapreduce with the performance of mpi.

the current twister, shown in figure  <dig>  is a distributed in-memory mapreduce runtime optimized for iterative mapreduce computations. it reads data from local disks of the worker nodes and handles the intermediate data in distributed memory of the worker nodes. all communication and data transfers are handled via a publish/subscribe messaging infrastructure. twister comprises three main entities:  twister driver or client that drives the entire mapreduce computation,  twister daemon running on every worker node, and  the broker network. we present two representative results of our initial analysis of twister  <cit>  in figure  <dig> and  <dig> 

we showed “doubly data parallel”  application like pairwise distance calculation using smith waterman gotoh algorithm can be implemented with hadoop, dyrad, and mpi  <cit> . further, figure  <dig> shows a classic mapreduce application already studied in figure  <dig> and demonstrates that twister will perform well in this limit, although its iterative extensions are not needed. we use the conventional efficiency defined as t/), where t is runtime on p cores. the results shown in figure  <dig> were obtained using  <dig> cores . twister outperforms hadoop because of its faster data communication mechanism and the lower overhead in the static task scheduling. moreover, in hadoop each map/reduce task is executed as a separate process, whereas twister uses a hybrid approach in which the map/reduce tasks assigned to a given daemon are executed within one java virtual machine . the lower efficiency in dryadlinq shown in figure  <dig> was mainly due to an inefficient task scheduling mechanism used in the initial academic release  <cit> . we also investigated twister pagerank performance using a clueweb data set  <cit>  collected in january  <dig>  we built the adjacency matrix using this data set and tested the page rank application using  <dig> 8-core nodes. figure  <dig> shows that twister performs much better than hadoop on this algorithm  <cit> , which has the iterative structure, for which twister was designed.

CONCLUSIONS
we have shown that mapreduce gives good performance for several applications and is comparable in performance to but easier to use  <cit>   than conventional master-worker approaches, which are automated in azure with its concept of roles. however many data mining steps cannot efficiently use mapreduce and we propose a hybrid cloud-cluster architecture to link mpi and mapreduce components. we introduced the mapreduce extension twister  <cit>  to allow a uniform programming paradigm across all processing steps in a pipeline typified by figure  <dig> 

