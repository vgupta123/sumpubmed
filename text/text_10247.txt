BACKGROUND
microarray technology allows a comprehensive examination of gene expression profiles and the regulations of their changes at a whole genome level  <cit> . it has great potential in the study of complex human diseases  <cit> . however, the technology is prone to noise and low reproducibility  <cit> . correlations with other platforms including rt-pcr  <cit> , and between different microarray platforms are often unsatisfactory  <cit> . on the other hand, many disease processes involve subtle gene perturbations that require highly accurate gene expression measurements. the noise in microarrays if not adequately reduced, can obscure the true biological variations and presents an obstacle for data-mining tools to distinguish biology from artifacts. for this reason rigorous qc standards are needed for the microarrays  <cit> . this in turn requires a clear understanding of the sources of data variability, so that the contributing factors can be appropriately dissected and most efficiently controlled.

because of the lack of consistent quality control  standards, spotted arrays fabricated in academic laboratories are usually more susceptible to qc problems than commercial arrays  <cit> . their advantages include much reduced fabrication cost and higher content flexibility than commercial arrays. for example, they can be designed to target specific genetic pathways, or to perform promoter analysis for genes of interest  <cit> . therefore developing a generalizable, efficient microarray qc scheme for spotted arrays is important.

we have previously reported a microarray image processing software matarray, which specializes in quantitative qc of data acquisition  <cit> . using it we have shown that several major sources of data variability are readily identifiable from the post-hybridization image, including high or non-uniform noise profiles, low or saturated signal intensities, and irregular spot sizes and shapes. their resultant effect on data reliability can be well characterized through the definition of a set of individual quality scores each measuring the impact of a corresponding factor, and a composite score qcom, which gives an overall assessment of the data quality acquired from each spot on the array  <cit> . through numerous experiments we have demonstrated the advantages of utilizing the ratio-qcom plot for data filtering and normalization  <cit> . nevertheless, there are sources of variability that cannot be directly or quantitatively evaluated from the post-hybridization image. one important example is the quality of array fabrication. the generation of microarray slides involves coating of the glass slides, printing up to tens of thousands of amplified cdna or oligonucleotide "probes" and fixing/blocking of the slide. during this process, variable amounts of material can be deposited and/or retained on the activated glass surface depending a number of variables. when the amount of immobilized probe is inadequate the measurements made on such arrays can be unreliable  <cit> . noise and artifacts introduced to the arrays at this stage will also directly affect the quality of hybridization. until recently, such problems have been difficult to quantitatively evaluate and control for each and every array, since the array is typically "invisible" prior to hybridization  <cit> . to overcome this difficulty, we have made a significant technological advancement in microarray qc by conceiving and developing a three-color microarray platform  <cit> , which we termed third dye array visualization  technology  <cit> . the approach labels the cdna probes printed on the array slides with a cyanine dye-compatible third dye  fluorescein  <cit> , and makes prehybridization quantitative assessment of array quality possible, so that precious samples as well as laboratory and analytical efforts will not be wasted over poor-quality slides. in the last several years, the quantitative third dye threshold for slide qc has been extensively investigated by us  <cit> .

in this report, we further investigate the advantage of tdav in better dissecting the sources for data variability at the spot level, and develop a data filtering and normalization procedure that incorporates the information from the td image. we utilize data from four different microarray experiments to validate our procedure. we evaluate the accuracy of our microarray measurements by comparing them with the known input ratios for spiked in control clones, and with the measurements by quantitative real time rt-pcr.

RESULTS
td intensity and gene-dependent bias in ratio measurements
compression in microarray measurements  have often been observed  <cit> . furthermore, the compression may depend on the ratio or the intensity levels  <cit>  leading to gene-specific biases, which are difficult to calibrate. the exact cause of the compression is still not clear, and its characteristics have not been well quantified. our previous studies have indicated that the amount of immobilized probes on an array available for hybridization can affect the fidelity of the hybridization  <cit> . therefore, we have examined this question in relation to spot td intensity utilizing the spiked-in arabidopsis clones from the rat thymus experiment . details of the design of this experiment are described in the methods. spots either saturated or possessing high background were eliminated using matarray  <cit> . after filtering,  <dig>  data points were available for analysis. figure  <dig> gives the result for transcripts spiked in at 30: <dig> . in a perfect measurement, the ratio of measured fold of change versus actual should be "1". indeed, compression is observed through the whole spectrum of td intensity . moreover, the compression is not constant. when the td spot intensity falls below ~ <dig>  rfu/pixel, the data compression and data variability dramatically increases. this is consistent with our previous studies which suggested that under our array scanning standard, only arrays with mean signal levels >  <dig>  rfu/pixel were able to generate reliable hybridization data  <cit> . in order to quantify this relationship further, we have calculated the mean behavior of the compression using lowess  <cit> , and fit the lowess mean with a piecewise function consisting of two linear segments joined together by a short quadratic function . the quadratic function ensures that non-linear least squares optimization can be used  <cit> . we find that indeed above the threshold value of  <dig>  rfu/pixel the compression is constant; whilst below this value the compression is increasingly more severe with decreasing td intensity . a similar trend exists for other input ratios and the results are summarized in table  <dig>  these results indicate:  the degree of compression both above and below the td intensity threshold is ratio dependent, the higher the fold-of-change is, the more compression the data will exhibit.  significant further compression in ratio measurements below the td intensity threshold occurs for all input ratios.  there is a dye bias in the characteristics of data compression.

we have also examined the data variability in relation to the td spot intensity. at each spiked in ratio, we determined the standard deviation  and the coefficient of variation  in ratio measurements for every data point using its  <dig> nearest neighbors on the ratio-td intensity plot. we find that above  <dig>  rfu/pixel both sd and cv are essentially constants. with deficient td intensities below  <dig>  rfu/pixel sd increases initially followed by a drop at very low td intensities due to the severe data compression in ratio measurements, whilst cv increases monotonically with decreasing td intensity. the cv results for the data points corresponding to the spiked in ratio of 30: <dig> are given in figure 1b. in a real experiment where the transcript abundance for different genes spans a wide range and their folds of change vary, gene-dependent artifacts in measurements will occur. these results revealed that inadequate probe amount is an important major source of data variability that could cause complex features in data compression.

td intensity and the spatial-dependent bias
sources of variation often have localized characteristics across the whole slide. one major type of such spatial-dependent bias is the heterogeneity in the printing characteristics among the pins. its exact cause is not clear and its characteristics not well characterized. normalization methods have been designed to correct spatial bias. for example, the local mean normalization  <cit> , and the pin-dependent localized intensity lowess normalization  <cit> . however they could lead to spurious results when the proportion of differentially expressed genes is high  <cit> . efficient normalization requires proper dissection of the causes for bias and minimization procedures designed accordingly. we have investigated the pin issue using the bb rat thymus data. for each of the  <dig> pins, we determined the mean and sd of the ratio measurements, and correlated the results with the number of spots that fell below the td threshold intensity of  <dig>  rfu/pixel. we found that when there were few poor-quality spots for all pins, they did not show significant difference. most of our arrays  that have passed our pre-hybridization qc  <cit>  are in this category. however, when the amount of failed spots exceeded 20%, a positive correlation between sd in ratio measurements and the percentage of failed spots became evident. figure  <dig> illustrated a typical result from one such hybridization. whilst the mean in log ratio distribution did not change much from pin to pin, clearly data variability increased  with more spots that failed the td intensity threshold cutoff. when the amount of failed spots is very large for most pins, exceeding 50%, the pattern gets more complicated because of the severe data compression . in contrast, no obvious correlation was observed between pin heterogeneity and cyanine intensities . the results here indicate that the amount of material each pin deposits is a major cause for pin difference, and hence it can be better controlled through our tdav technology.

incorporating td information in data filtering and normalization
results in the preceding sections suggest that the td intensity is a major factor that causes spot-level data reliability. in addition, other artifacts on the td image can also influence the accuracy of expression measurements from post-hybridization images, including noise, spot size and shape irregularities  <cit> . based on these observations we formulated a quality measure for every spot from the td image by defining

qtd = qint *qcom, td     

where qcom, td is the composite td image quality score, defined according to signal-to-noise ratio, spot size, and background levels and variation, similarly as given in the equation  of  <cit> . qint is given by:

qint⁡={ <dig> intensity≥thresholdintensity/threshold,intensity<threshold     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgxbqcdawgaawcbagagiyaakmaeioba4maeiidaqhabeaakiabg2da9maaceqabaqbaeaabigaaaqaaiabigdaxiabcycasaqaaiabbmgapjabb6gaujabbsha0jabbwgaljabb6gaujabbohazjabbmgapjabbsha0jabbmha5jabgwmizkabbsha0jabbigaojabbkhayjabbwgaljabbohazjabbigaojabb+gavjabbygasjabbsgakbqaaiabbmgapjabb6gaujabbsha0jabbwgaljabb6gaujabbohazjabbmgapjabbsha0jabbmha5jabc+caviabbsha0jabbigaojabbkhayjabbwgaljabbohazjabbigaojabb+gavjabbygasjabbsgakjabbycasaqaaiabbmgapjabb6gaujabbsha0jabbwgaljabb6gaujabbohazjabbmgapjabbsha0jabbmha5hqaciab=xda8iabbsha0jabbigaojabbkhayjabbwgaljabbohazjabbigaojabb+gavjabbygasjabbsgakbaaaiaawuhaaiaaxmaacawljawaaewaceaacqaiyagmaiaawicacaglpaaaaaa@8983@

in matarray the default threshold =  <dig>  rfu/pixel.

utilizing data from the four microarray experiments we examined the replicate consistency, and the agreement between microarray measurements and the known input ratios for the spiked in arabidopsis clones, with respect to qtd. we found that like qcom , qtd captures well the data variability with higher qtd spots yielding less variation. for example, for each pair of direct replicate hybridizations in this study, we obtained the genes that exhibit differential expression  at p =  <dig> . we divided them into  <dig> bins, and for genes in each bin we determined the mean qtd and qcom, and the pearson correlation in log ratio measurements between the replicates. a typical result is given in figure  <dig>  filtering by either qtd or qcom leads to significant improvement in replicate consistency. notice that majority of the spots have high qtd due to the fact that all the slides we use for hybridization have already been pre-selected using tdav  <cit> . we have also found that there is no significant correlation between qtd and qcom , which validates that they are two non-redundant quality measures each capturing a different major source of data stability, and qc by each is necessary.

incorporating our new quality score definition for the td image, we have developed the following filtering and normalization procedure:

 <dig>  evaluate the log r - qtd plot and decide on a filtering threshold for qtd . normally this will be the value where there is an abrupt increase in data variability, in the same fashion as shown in figure 4a . the quality score for all spots below this threshold will be reset to qtd =  <dig> 

 <dig>  perform a local qtd -dependent normalization for all data points with qtd >  <dig> utilizing the robust scatter plot smoother lowess  <cit> .

 <dig>  evaluate the log r - qcom plot and decide on a filtering threshold for qcom . the quality score for all spots below this threshold will be reset to qcom =  <dig>  as described in figure 4a.

 <dig>  perform a local qcom -dependent lowess normalization for all data points with qcom >  <dig>  the lowess fit for sd will also be determined, and the z-score will be calculated for normalized log r every spot by: z=normalized log⁡rlocal sd
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgabgwcqgh9aqpdawcaaqaaiabb6gaujabb+gavjabbkhayjabb2gatjabbggahjabbygasjabbmgapjabbqha6jabbwgaljabbsgakjabbccagigbcygasjabc+gavjabceganjabdkfasbqaaiabbygasjabb+gavjabbogajjabbggahjabbygasjabbccagiabbofatjabbseaebaaaaa@4c7d@ <cit> . figure 4b gives the log ratio after normalization for the same data set given in figure 4a.

after all normalization a final quality score will be defined

qf = qtd•qcom     

only spots with qf >  <dig> will be retained for further data mining and modeling.

the efficiency of our filtering and normalization procedure
the integration of our technical and analytical developments has resulted in a unique procedure for comprehensive, quantitative microarray data processing and qc. to demonstrate the advantage of our filtering and normalization method, we compared it to a commonly used localized lowess normalization method that utilizes the ratio-intensity dependence  <cit>  . data from all four experiments were processed both by our pipeline  and the ma-lowess  procedure. all spots with qf =  <dig> on any replicate slide were dropped . we calculated the correlation coefficient between replicate hybridizations for common de genes at p=  <dig>  according to both normalization procedures. there were totaling  <dig> pairs of replicate hybridizations and good agreements were observed, with mean replicate correlation r =  <dig>  ±  <dig>  according to z-norm. the difference between the two methods are presented in figure  <dig>  revealing a better  overall performance by our processing pipeline. the improvement by our method is small  due to the already high data quality.

the accuracy of gene expression measurements
arabidopsis clones that were spiked at known ratios in experiment  <dig> were utilized to assess the accuracy of our microarray measurements. a highly linear relationship between the measured and the actual ratios was observed , with the exception of the last data point . there are two possible causes for the discrepancy at this data point:  intensity saturation in one channel can lead to under-estimation of the fold difference between the two dye channels. a close look of the intensity profiles for the spots contributing to this data point revealed that the saturation is insignificant  as we have only included spots with qf >  <dig> in the analysis  <cit> .  ratio measurements can have significant compression at a high fold of change, as we have demonstrated in table  <dig>  we believe this is the major cause for the non-linearity at the last data point. excluding it all normalization procedures led to linear regressions with r <dig> > <dig> , p <  <dig>  over a dynamic range of ~ <dig> fold. overall the measured data exhibited a moderate compression over the actual, with the slope of the linear regression always less than  <dig>  z-norm led to a small, insignificant improvement over ma-norm. again this is likely due to the fact that all arrays used in our experiments were pre-selected using tadv  <cit>  and the data were already of high quality.

in figure 6b the microarray measurements are compared to the rt-pcr results for the  <dig> genes in the rat liver experiments, and an overall good agreement is observed. the rt-pcr platform is generally considered more quantitative and accurate than the microarrays  <cit> .  <dig>  of the  <dig> genes were identified as poor-quality data points as their qf =  <dig> on at least one array. excluding these genes a highly linear relationship  existed for the remaining  <dig>  if we were to calibrate microarray measurements using the rt-pcr results as a standard, we would have rcorrected = q, with a correction factor q~ <dig> = <dig> 
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgxbqccqgg+bgfdawcaaqaaiabigdaxaqaaiabicdawiabc6cauiabiida4iabiwda1iabiwda1aaacqgh9aqpcqaixaqmcqgguaglcqaixaqmcqai3awnaaa@3a21@. this is much better than a q ~ <dig>  previously reported  <cit> . if the  <dig> poor-quality data points were to be included, the agreement between the two platforms drops to r <dig> ~  <dig>  with a correction factor q ~  <dig> . this result strongly indicates that with stringent, efficient qc protocols, cdna microarrays are able to generate high quality, quantitative gene expression measurements comparable to that by rt-pcr.

the impact on the biology
how much impact on the biological interpretation can such improvement in data quality bring about? to answer this question we have performed ontological analysis for the de genes in each experiments using ontoexpress  <cit>  and ease  <cit> . we found no significant difference in the pure number of de gene predictions between data processed by either z-norm or ma-norm . however, at ontology level, a general trend appeared suggestive of z-norm being able to lead to more focused, local biological themes, usually with more significant p-values . for example, in experiment  <dig> at  <dig> hr after drug treatment, the apoptosis progression has been established with at least 40% cells were apoptotic according to the annexin v/pi double staining  <cit> . ease analysis of the de genes after z-norm predicted enhanced presence of genes belonging to regulation of cell cycle, cell proliferation/death, lysosome, lytic vacuole, nucleus, etc, most of which were closely related to apoptosis. using de genes predicted after ma-norm, about one third of these categories were not detected. the interpretation of ontological analysis is a complex issue, with many unresolved problems  <cit> . for example, since most of the ontological categories are not independent, it is still an open question on how to best recap the findings and evaluate significance  <cit> . therefore a quantitative evaluation of our findings awaits further methodology development in the field of ontological analysis.

CONCLUSIONS
in this report we have shown that when the probe amount is inadequate, severe compression in gene expression measurements occur with complex, gene-specific characteristics. likewise, the normal variation in the amount of probes printed and immobilized is a major source for printing pin-dependent artifacts in the data. utilizing tdav, these problems rooted from array fabrication can be effectively dissected from hybridization problems, and efficiently controlled through the definition of a quality score qtd. in addition, we have developed a comprehensive two-step data filtering and normalization procedure based on the log r - qtd and log r - qcom plots, which was found to be more efficient than the commonly used ma-lowess approach. by confirming our microarray data with the known input ratio of spiked in controls clones, and with rt-pcr, we demonstrated that acquiring accurate measurements using cdna microarrays is achievable with our tdav technology and our data qc procedure. furthermore, in a recent study where we compared measurements from our cdna microarrays with those from affymetrix and agilent oligonucleotide array platforms, we observed a high correlation among the three, with no significant differences in terms of data quality. specifically, using anova we have found that the different platforms did not cause more variation than the replicate hybridizations within each individual platform  <cit> . this is in contrast to the recent reports of poor performance by academic arrays which did not use our scheme  <cit> . thus we wish to emphasize that academic labs can still perform high quality, inexpensive microarray experiments with our platform.

our algorithms, although initially developed for cdna arrays, are potentially applicable to spotted oligonucleotide arrays as well. recently we have developed the three-color oligonucleotide array platform by introducing a third-dye labeled universal tracking oligonucleotide into the printing buffer, thus the quality of array fabrication can be quantitatively evaluated through the measurements of the tracking oligonucleotide  <cit> . a high quality microarray platform will allow lab investigators to focus on their biological questions instead of the technical issues of the data, and will allow statisticians and bioinformatics investigators to develop more powerful complex analysis approaches.

