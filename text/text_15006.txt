BACKGROUND
modern technologies like microarrays facilitate the study of expression levels of thousands of genes simultaneously. this study is useful to determine whether these genes are active, hyperactive or inactive in various tissues. the vast amount of microarray data is so important for the applications like disease classification and identifying the genetic networks. solutions for complex problems like identification of cancer types and their subtypes need more accuracy for utilizing them in treating this disease and in preparing more effective therapeutic solution like individual drug design. so, it is important and necessary to select only genes containing the expression data contributing to the problem domain and to filter irrelevant data to increase the performance of the methods used. feature selection is the problem of identifying such genes or features  <cit> . that is, this can be used to identify the important genes with significant information content when the problem is poorly structured. this improves the generalization performance and inference of classification models  <cit>  by overcoming the 'curse of dimensionality'. one important problem with feature selection methods is that both problem relevance and biological relevance of the features selected may not be achieved completely. also, most of the feature selection methods do not fit for the wide range of datasets. they are coupled with a particular classification method and time consuming. statistical methods are in use in this domain for a long time. but, extensive preprocessing and lesser consensus among them are major problems with them. transform oriented signal processing methods are simpler and may provide an alternative platform to the statistical methods. they have been successfully utilized in many domains like image processing. but, they have not been much utilized in the field of bioinformatics. the key advantage of these transform oriented methods is their power of capturing some inherent properties of the data. the aim of this paper is to analyse the capabilities of haar wavelet power spectrum in selecting informative features in microarray data on the basis of the inherent properties captured by them. the present work utilizes some earlier works in feature selection for illustration and analyses the comparability of wavelet strategy with those of earlier works.

feature selection
feature selection can be approached in three ways. first, we may handle feature selection method independently irrespective of the further applications utilizing these features. that is, the features selected may be used for any classifier algorithms. this approach of feature selection is called a filter method. second, features may be selected for a specific classifier algorithm. in this approach called a 'wrapper method'  <cit> , the qualities or accuracies of all possible subsets are analyzed to select the optimal one to the specific classification algorithm. finally, feature selection and classifier design may be accomplished together. this strategy is found in embedded methods. embedded methods are incorporated into the learning procedure, and hence are dependent on the classification model.

systematizations and surveys on feature selection algorithms have been presented in a variety of review articles like blum and langley  <cit> , kohavi and john  <cit>  and guyon  <cit> . so far, a number of variable  selection methods like the support vector machine method  <cit> , the genetic algorithm  <cit> , the perceptron method  <cit> , bayesian variable selection  <cit> , and the voting technique  <cit> , mutual information-based gene and feature selection method  <cit> , entropy based feature selection  <cit>  and many artificial intelligent techniques like hill climbing, best first search  <cit> , simulated annealing  <cit> , backward elimination  <cit> , forward selection and their combinations have been proposed. specific to filter approach, kira and rendell's relief algorithm  <cit>  which selects features based on a threshold of weights assigned to each feature is a good example but it was tested on small set of features.

in case of high dimensional datasets containing thousands of genes, filters are preferred to wrappers due to their independency over the models  <cit> . xiang et al  <cit>  devised a hybrid of filter and wrapper approaches and tested it on high dimensional gene expression data with  <dig> samples and  <dig> features. another such work on high dimensional gene expression data was done by golub et al  <cit>  on the same dataset using correlation measures. califano et al  <cit>  also worked on a high dimensional dataset of  <dig> genes using a supervised learning algorithm. all these works revealed the fact that the result was better while using selected features instead of the whole data set.

most commonly used filters are based on information-theoretic or statistical principles. score based feature selection methods are popular among filters using statistical principles. these methods calculate statistical scores on the gene expression data. they sort genes according the scores assigned and filter them by applying some threshold. χ 2-score, t-test metrics <cit> , wilcoxon rank sum test <cit> , correlation co efficient  <cit>  and b-scatter score are some prominent examples. some other strategies used in feature selection through ranking are snr based ranking used in shipp's approach  <cit>  and sensitivity analysis based ranking used in mean square classifier  <cit> . the strategy of selecting features using sensitivity analysis is to rank a feature according to the change in the value of an objective function caused by the removal of that feature from the dataset. snr method is more capable of detecting and ranking a smaller number of significant variables. apart from ranking methods, several other approaches like relief  <cit> , gini-index  <cit> , relevance, average absolute weight of evidence  <cit>  and bi-normal separation  <cit>  are also in use.

most of the methods of feature selection are complex and consume more time to converge. many of them do not fit for all data types in addition that they require more samples. no consensus among various statistical methods is achieved to use them. the selection of a statistical method for a dataset is a hit and run approach. so, a more generic method which can cope up with a variety of data is in dire need. further, a very few model independent approaches for feature selection are available since most of the methods of feature selection are coupled with classification. in this paper, we analyse the capability of wavelet power spectrum in feature selection and we propose a method of feature selection based on haar wavelet power spectrum. this method is found fit for a wide range of data sets and also works with smaller number of samples. it can be used in conjunction with other classification methods. the algorithm is very simple and requires comparatively less time to be executed. the method is a model independent approach, a filter feature selection method, based on the haar wavelet power spectrum of the microarray data. unlike most of the other methods, it is relatively a very simple algorithm. we observed that the features selected by our method can be used in conjunction with more classification algorithms.

wavelet and its power spectrum
wavelets are a family of basis functions that can be used to approximate other functions by expansion in orthonormal series. they combine such powerful properties as orthonormality, compact support, varying degrees of smoothness, localization both in time or space and scale , and fast implementation. one of the key advantages of wavelets is their ability to spatially adapt to features of a function such as discontinuities and varying frequency behaviour. a wavelet transform is a lossless linear transformation of a signal or data into coefficients on a basis of wavelet functions  <cit> . performing the discrete wavelet transform  of a signal x is done by passing it through low pass filters  and high pass filters simultaneously. down-sampling or decimation by a factor  <dig> is performed after each pass through filters. decimation by  <dig> means removing every alternative coefficient in the function is performed after each pass through filters. figure  <dig> depicts a two level wavelet transform.

mathematically, the wavelet transform of a function x  can be represented by the following formula:

yhigh=∑k=−∞∞x⋅gylow=∑k=−∞∞x⋅h
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqabegabaaabagaemyeak3aasbaasqaaiabdigaojabdmgapjabdeganjabdigaobqabagccqggoaakcqwgubgbcqggpaqkcqgh9aqpdaaewbqaaiabdiha4jabcufabjabdugarjabc2fadjabgwsixlabdeganjabcufabjabikdayiabgwsixlabd6gaujabgkhitiabdugarjabc2fadbwcbagaem4aasmaeyypa0jaeyoei0iaeyohiukabagaeyohiukaniabgghildaakeaacqwg5bqedawgaawcbagaemibawmaem4ba8maem4dachabeaakiabcicaoiabd6gaujabcmcapiabg2da9maaqahabagaemieagnaei4waslaem4aasmaeiyxa0laeyyxictaemiaagmaei4waslaegomaijaeyyxictaemoba4maeyoei0iaem4aasmaeiyxa0faleaacqwgrbwacqgh9aqpcqghsislcqgheispaeaacqgheispa0gaeyyeiuoaaaaaaa@77bc@

where ylow  and yhigh  are responses from low and high pass filters respectively. in matrix form, wt = t where w =  where l and h are impulse responses of low pass and high pass filters and wt is wavelet transform of the one dimensional input signal x. the two filters used at each stage of decomposition must be related to each other by g  = n·h where g and h are the impulse responses of the two filters, l is the filter length in number of points, n is the order of the data points and l is such that  <dig> ≤ n <l. for example, there are two data points for each filter of haar wavelet with n =  <dig>   <dig>  these filters are known as quadrature mirror filters. a wavelet transform of a data after i level of decompositions contains the approximation coefficients at ith level and all detailed coefficients up to ith level. the detailed coefficients at different levels incorporate the variations in information at those levels. level of decomposition is also termed as band.

a number of wavelet families like symlet, coiflet, daubechies and biorthogonal wavelets are already in use. they vary in various basic properties of wavelets like compactness. among them, haar wavelets belonging to daubechies wavelet family are most commonly used wavelets in database literature because they are easy to comprehend and fast to compute  <cit> . haar transform can be viewed as a series of averaging and differentiating operations on a discrete function. the impulse response for high pass filter is given by [1/2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadagcaaqaaiabikdayawcbeaaaaa@2db9@, -1/2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadagcaaqaaiabikdayawcbeaaaaa@2db9@] and for low pass filter, the impulse response is [1/2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadagcaaqaaiabikdayawcbeaaaaa@2db9@,1/2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadagcaaqaaiabikdayawcbeaaaaa@2db9@]. that is, the minimum number of elements in input data should be  <dig>  the input data should always contain the number of elements 2n where n is an integer. in matrix form, the haar wavelet filter can be expressed as


 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawadiqaauaabeqaciaaaeaadawcaaqaaiabigdaxaqaamaakaaabagaegomaidaleqaaaaaaoqaamaalaaabagaegymaedabawaaoaaaeaacqaiyagmasqabaaaaagcbawaasaaaeaacqaixaqmaeaadagcaaqaaiabikdayawcbeaaaaaakeaacqghsisldawcaaqaaiabigdaxaqaamaakaaabagaegomaidaleqaaaaaaaaakiaawufacagldbaaaaa@37f9@

it can be easily examined that both the low pass and high pass filters of haar wavelet are quadratic in nature using the discussion presented in the previous paragraph. for a data having more than two elements, the haar wavelet matrix of can be constructed by diagonally repeating these basic filters to form a matrix of the size of input data. upper part of the matrix is created by repeating impulse responses of low pass filter diagonally and lower part of the matrix is created by repeating impulse responses of high pass filter diagonally. from figure  <dig>  it is evident that the size of the data points to be used for wavelet transform in a level is equal to half of the data points used in the previous level. accordingly, the size of the haar wavelet matrix also reduced. for example, if we use a signal of four data points, the size of the haar wavelet matrix will be  <dig> ×  <dig> in the first step of wavelet transform. from figure  <dig>  it is evident that the number of data points to be used for the second step of wavelet transform is  <dig>  these are the output of low pass filtering operation as shown in figure  <dig>  so, the haar wavelet matrix to be used is of the size  <dig> ×  <dig>  more details of wavelets may be referred at  <cit> .

the minimum number of data points in an input signal should be  <dig> in the case of haar wavelet and the number of data points needed for n times decomposition is 2n. if the number of input data points is less than this required number, 2n, zeros may be padded  at the right end of the input data to compensate the required number. in the present work, the number of data points refers to the number of samples which is equal to the number of columns present in the microarray data matrix. that is, the expression of a gene in a sample is considered as a data point of a one dimensional signal x. accordingly, the columns of the microarray data matrix were prepared so as to be amenable for satisfying the required number criterion. in some experiments, a reduced number of the columns of the microarray data matrix equal to the nearest power of  <dig> were randomly selected and used. since we use the strategy of finding the average value of wavelet power spectrum for each gene per sample, in the present work, the choice of columns selected for replication or reduction is immaterial. we used a random selection of the columns for the purpose of reduction and replication for data input preparation. it was observed that such a random selection of columns did not affect much the robustness and the accuracy of the present method used. in the present work, expression data of each gene across various tissue samples or various experiments is modeled to a one dimensional signal. therefore, the entire microarray data is modeled to a group of m number of one-dimensional signals where m is the total number of genes present in the gene microarray data. more mathematical details of wavelets may be referred at  <cit> .

local wavelet power spectrum at a particular decomposition level is calculated by summing up the squares of wavelet coefficients at that level  <cit> . for a set of wavelet coefficients cj,k, where j is level of decomposition and k is the order of the coefficient, the wavelet power spectrum is given below.

spectrum=∑k=02j−1cj,k2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqqgzbwccqqgwbaccqqglbqzcqqgjbwycqqg0badcqqgybgccqqg1bqdcqqgtbqbcqggbbwwcqqgqbgacqggdbqxcqgh9aqpdaaewbqaaiabboeadnaadaaaleaacqqgqbgacqggsaalcqqgrbwaaeaacqaiyagmaaaabagaee4aasmaeyypa0jaegimaadabagaegomaizaawbaawqabeaacqqgqbgaaawccqghsislcqaixaqma0gaeyyeiuoaaaa@4c5a@

if there are n elements in an array, there will be log <dig> coefficient bands or levels of decomposition for haar wavelet. that is, the power spectrum can be referred as a graphical representation of cumulative information variation at each scale of decomposition. global wavelet power spectrum  <cit>  is the average of such local power spectra.

RESULTS
our proposed algorithm for feature selection has been applied on various datasets and top genes are reported here. in all these experiments, we have used haar wavelets since the number of minimum features for wavelet transformation at lowest level is smaller than that required by the other wavelets. we applied our method on three datasets namely golub dataset, hedenfalk breast cancer dataset and khan srbct dataset. all experiments were carried out without filtering any data to validate the robustness of the method against the noise or outliers in the data.

srbct dataset
first, we focus on feature selection for the small, round blue cell tumors  of childhood. the dataset of srbct used for experimentation here is available at  <cit> . this dataset is composed of  <dig> genes and  <dig> samples from four cancers which includes neuroblastoma  , rhabdomyosacoma  , burkitt lymphomas   and ewing's family of tumors  . originally, khan et al  <cit>  classified this dataset using artificial neural networks on gene expression profiles. the feature selection and classification using this dataset has also been performed by zhou et al using gibb's sampler and smc  <cit> . khan et al  <cit>  selected a list of  <dig> discriminating genes pertaining to classification. this list included some genes being identified important to two classes out of four classes and some genes which were not categorized for any class. our method has identified some of them important for one of the four classes. it has selected almost all these features with comparatively simpler calculations. also, we used only  <dig> samples out of  <dig> sample set. first four samples from each diagnostic category have been selected to form this group of  <dig> samples. it exhibits the possibility of using our methods for datasets with a lesser number of samples. most of the top ranked genes listed in the present work have been used in classification of the dataset in earlier works  <cit> .

a list of top genes selected by our method has been listed in table  <dig>  genes with index ids  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> were also reported to be differentially expressed in ews in khan et al's work  <cit> . gene  <dig> was not allocated to any class in  <cit> . most of the other genes like gene  <dig> reported to be discriminating ews have been selected as important genes for ews but with a lower ranking. the rank of gene  <dig> was  <dig> in our work and its rpv was  <dig> . gene  <dig> was not selected in khan's  <cit>  work but it was ranked fourth in our work. of all genes selected for ews, neural-specific genes  <cit>  like tubb <dig> , anxa <dig> , and noe <dig>  lend more credence to the proposed neural histogenesis of ews  <cit> . most of the top ranked genes are dominant in ews category in comparison with their expression in other classes. this implies that most of the top ranked genes in table  <dig> are highly related to the classification of ews from other categories. when tested with golub's algorithm  <cit> , first  <dig> samples of srbct dataset except sample number  <dig> were categorized as ews and remaining  <dig> samples were categorized as non ews samples.

a list of strongest genes selected for classifying bl versus others using our method has been reported in table  <dig>  in the srbct dataset, the samples of bl category spans from sample number  <dig> to sample number  <dig>  among them, genes  <dig> and  <dig> were reported to be differentially expressed in bl in khan's original work  <cit> . genes with indices  <dig>   <dig>   <dig>   <dig> , <dig> , <dig>   <dig> and  <dig> were also selected in  <cit>  but not assigned to a particular class. of six differentially expressed genes in bl and some other classes, remaining four genes except genes  <dig> and  <dig> were selected for other classes. so, our method selects proper genes effectively with simple algorithm and calculations. most of the top ranked genes are dominant in bl category in comparison with their expression in other classes . this implies that most of the top ranked genes in table  <dig> are highly related to the classification of bl from other categories. expression levels of these genes show that they can be classified using golub's classification algorithm  <cit>  since they are highly correlated to the "idealized expression pattern"  <cit> .

in the case of nb  class, there were no exclusively discriminating genes reported in  <cit> . they were reported to be differentially expressed either with ews or rms. among them, genes  <dig>   <dig>   <dig> and  <dig> have been identified as stronger genes pertaining to ews by our method. genes with index numbers  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> were listed among  <dig> discriminating genes not pertaining to any class. our method has allocated them to be in favour of nb class. a list of such genes selected for classifying nb versus others by original work as well as our method have been listed in table  <dig> 

the fourth class in srbct is rms. originally,  <dig> genes were listed in discriminating genes in  <cit> . all  <dig> genes have been selected in the list of strong genes by our method. but most of them reserve their slots in rank from  <dig> to  <dig>  the genes with index  <dig>   <dig>   <dig>   <dig> and <dig> have come under the top  <dig> strongest genes list. among them igf <dig>  and myl <dig> are specific for muscle tissues which have also been reported in rms in  <cit> . most of the top ranked genes in table  <dig> are dominant in nb category in comparison with their expression in other classes . this implies that most of the top ranked genes in table  <dig> are highly related to the classification of nb from other categories. expression levels of these genes show that they can be classified using golub's classification algorithm  <cit>  since they appear to be highly correlated to the "idealized expression pattern"  <cit> .

acute leukemia data
the experimental setup used for getting acute leukemia data and other details can be found at  <cit> . the data set is publicly available at  <cit> . the microarray data consists of  <dig> human genes and consists of  <dig> samples. the data is split into a training set and a test set. training set consists of  <dig> samples comprising  <dig> aml samples and  <dig> aml samples. test dataset of  <dig> samples comprising  <dig> all and  <dig> aml samples. as a test case, important genes to classify aml versus all are selected on the basis of their relative percentage variations of expression levels between two classes. many genes reported in  <cit>  are listed in table  <dig> but in different order. index number refers clone id here. among these selected genes, genes with index numbers  <dig>   <dig>   <dig> and  <dig> have been reported as important genes in discovering aml class in the original work at  <cit> . also, genes with index numbers  <dig>   <dig>   <dig> and  <dig> have been reported to be important genes at  <cit>  where genes were selected using mutual information.

genes with index numbers  <dig>   <dig>   <dig> and  <dig> have been reported to be important genes selected using t-scores  <cit> . gene  <dig> has been reported as one of the important genes at  <cit> . also, most of the other important genes reported to be important are found to occupy almost the first  <dig> genes in this method. this clearly shows that this method of feature selection is worthy one and may be used in conjunction with different methods of classification.

but, when a dataset with only two classes like the golub data, selecting distinct genes do not workout since this method clearly bisects the genes into two distinct clusters one for each type. so, the number of important genes selected is relatively high in comparison with that for other datasets where the number of classes is more than two. for the datasets having more than two classes the feature selection method proposed here is found to be more useful.

breast cancer dataset
next, we examined our proposed method of feature selection on hereditary breast cancer data from  <cit> . this dataset consists of twenty two breast tumor sample from  <dig> patients. classification of each tumor sample into one of the classes based gene expression data was performed using a compound covariate predictor in  <cit> . in  <cit> , the same classification was performed using smc method and the genes were selected using a gibb's sampler. the genes selected using our method to classify brca <dig> versus others is very close to those selected by gibb's sampler in  <cit> . the genes with indices  <dig>   <dig>   <dig>   <dig> ,  <dig> and  <dig> have been selected among top  <dig> genes using our method  but with little difference in order. some other genes presented in  <cit>  are found within top  <dig> genes selected in our method. among all these genes gene with index number  <dig> is reported as very important for all the methods in  <cit> . it is observed in  <cit>  that only with five or ten genes selected the classification was successful. this suggests that the genes selected by our method are worthwhile to use for classification of brca <dig> versus others since more of them are also found in the list mentioned in  <cit> . gene  <dig> has been identified as one of the top  <dig> strongest genes selected by mutual information  <cit> . genes with index numbers  <dig> , <dig>  and  <dig> which are also selected as the strongest genes in  <cit>  are ranked between  <dig> and  <dig> by our method.

CONCLUSIONS
in the present paper, we have treated the problem of feature selection of microarray gene expression data. we analyzed capability of the wavelet power spectrum using haar wavelet in the domain of feature selection problem. we found that the power spectrum technique has the potential to identify the informative features. we proposed a clustering and feature selection method useful for classification based on haar wavelet power spectrum. the top genes have been selected and have been compared with the results obtained in earlier works. in earlier works, preprocessing methods to remove noise or outliers before applying their methods were used. in the present work, to test the robustness of the dataset, no such preliminary measures were adopted. the method is quite simple in comparison to other feature selection methods and for implementation it needs no special software since the accessibility of wavelets is made quite easier in already available software. each earlier works select different set of genes for classification purpose and proved only few genes are quite enough to approach the classification problem  <cit> . so, the present method can be used in conjunction with many established classification methods with lesser number of samples than that required for other methods. many of the genes selected by our method have been used in the classification of earlier works which proves these genes are informative. the initial results of the idea of using haar wavelet power spectrum in feature selection using microarray data are encouraging and due to its simplicity, speed and effectiveness and fitness for a wide range of datasets, it may be further researched for devising simpler tools with more optimization. a possibility of developing simpler but effective tools in this domain using wavelet power spectrum has been explored. future research may be executed to utilize the power spectrum technique in the area of genomic signal processing using microarrays and its application.

