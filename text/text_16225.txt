BACKGROUND
complex biological systems arise from the interaction and cooperation of a large number of molecular or organismal components. understanding such systems has required, just at the molecular level, the construction and analysis of protein−protein interaction, metabolic interaction, transcription factor binding, and hormone signaling networks. networks are represented by graphs, where vertices are, for example, molecular components and edges represent some relationship among them. understanding such networks mainly requires finding specific topological subgraphs, which entails the application of subgraph isomorphism algorithms  <cit> . such subgraphs are sometimes called network motifs  <cit> . these motifs, which could be repeated in the same network or in different networks, give insight into evolutionary mechanisms . in  <cit> , the authors find network motifs through the following steps:  enumerate all possible subgraphs of the network;  classify them in classes of isomorphic subgraphs;  generate random graphs and enumerate and classify all subgraphs in such graphs ; and  establish as motifs all subgraphs classes that appear with higher frequency in the real network than in the random networks. the second step is repeated many times in real and random networks and entails the usage of subgraph isomorphism algorithms  <cit> . in  <cit> , the authors provides a cytoscape plugin to query networks by drawing and searching known or user-defined subgraphs. it uses the subgraph isomorphism algorithm of  <cit> . the authors in  <cit>  and related publications show their speed-up compared to the algorithm in  <cit>  which is used in  <cit> . in another application, molecular components, such as small or large proteins, are represented as graphs. in such chemical networks, vertices are atoms and edges are the bonds among them. systems such as daylight  <cit>  and its academic version frowns  <cit> , collect a set of molecules represented in two dimensions. then, given a subgraph, they apply a subgraph isomorphism algorithm  <cit>  to determine how many times and where the subgraphs occur in each molecule of the collection. the aim of the above works is to predict or increase the functionality of new or known molecules. in  <cit> , graphs are used to represent proteins in three dimensions. there, vertices and bonds are associated with their positions in space or contact maps are used. contact maps represent protein residues and the cut-off distances among them starting from a three dimensional protein structure. the authors discuss subgraph isomorphism algorithms, graph theoretical properties and the importance of an efficient implementation of such algorithms with the aim of detecting ligands that bind to proteins . finally, in  <cit> , the authors describe the relations among the components and subcomponents of molecules by using hierarchical graphs and making use of subgraph isomorphism algorithms  <cit>  to find common substructures.

finding a solution for the subgraph isomorphism problem is inherently hard  <cit>  and therefore the efficiency of any software using subgraph isomorphism algorithms largely depends on  finding efficient heuristics to make the isomorphism algorithms faster;  reducing the number of subgraph isomorphism calls; and   relaxing the isomorphism conditions.

graph indexing based methods aim to design efficient indexes  or data structures  <cit>  capable of limiting the execution of subgraph isomorphism to only a few candidate graphs; graph mining algorithms  <cit>  reduce the size of indices by identifying frequent subgraphs having at least a specified support; and graph pattern matching algorithms  <cit>  solve a "near" subgraph isomorphism problem by applying more relaxed reachability conditions  <cit> .

this paper introduces a new algorithm for the subgraph isomorphism problem and compares it on synthetic and biochemical data with the most efficient and recent algorithms present in literature  <cit> . notions, concepts and related work are given next.

basic notions
a graph g is a pair , where v is the set of vertices and e ⊆  is the set of edges. let a be a set of labels, the functions lab : v → a and β : e → a assign labels to vertices and edges, respectively. if  ∈ e, v is called a neighbor of u. given g, |v |  indicates the number of vertices . a graph g is dense when the ratio |e|/|v| is high, sparse otherwise.

given a pattern graph g and a target graph g', the problem is to find an injective function, m : v → v', mapping each vertex of g to a unique vertex of g' such that the following isomorphism conditions are satisfied: if  is an edge in g, u has label lab, v has lab, then the corresponding edge  in g' has lab = lab, lab = lab, and β = β. note that there may be an edge  is ∈ e' without any corresponding edge in e; when this happens, the subgraph isomorphism is also called a monomorphism. when g has exactly the edges that appear in g' over the same vertex set, then g is an induced subgraph of g'.

in what follows, we view g and g' as connected graphs and ignore edge labels . moreover, we consider graphs that are directed, that is  ∈ e does not imply that  is also in e. our approach applies as well directly to undirected connected graphs. when needed, we denote an undirected edge with 〈u, v〉.

algorithmic aspects of subgraph isomorphisms methods
a simple enumeration algorithm to find all the subgraph isomorphisms  of a pattern graph in a target graph works as follows: generate all possible maps between the vertices of the two graphs and check whether any generated map is a subgraph isomorphism . whereas this algorithm is inefficient if done naively, it serves as a good starting point.

all the maps can be represented using a search space tree. the tree has a dummy root. each node represents a possible match between some vertex u of the pattern g and some vertex u' of the target graph g' . the path from the root to a given node represents a partial match between g and g' . only certain leaves correspond to subgraph isomorphisms between the pattern and the target graph . during the visit, the isomorphism conditions are applied to verify the partial matches. when the conditions are not satisfied the algorithm prunes the underling branches and backtracks on the parent nodes of the search tree. the size of the above search space tree increases exponentially with the graph size. because the subgraph isomorphism problem is np-complete  <cit>   a cheaper-than-exponential algorithm may not exist. next we sketch the main heuristics in existing subgraph isomorphism algorithms. the common aim is to eliminate unsuccessful mappings as early as possible.

search strategy an important factor influencing an algorithm's performance is the choice of a good variable ordering  of the pattern graph vertices in the branches of the search tree. for example, a variable ordering may begin with a pattern vertex having the highest degree or having the most uncommon label in the target graph  <cit> . a strategy depending on the partial solution could choose the next pattern vertex to be matched such that the number of children in the current search tree's branch is minimized  <cit> . one can choose to maintain the same variable ordering for all the branches of the search tree or can choose different orderings for different branches. these two strategies are called static ordering and dynamic ordering  <cit> , respectively.

an important difference between static and dynamic orderings is that the first one can be chosen a priori, before the search phase. dynamic strategies must be elaborated during the search.

reduce the search space after evaluating a partial solution, an algorithm may backtrack if there is no possible mapping for the remaining unmatched vertices  <cit> . alternatively inference-based techniques can predict future branching of the search tree thus avoiding the need to explore partial solutions that do not result in a match  <cit> . an intelligent matching algorithm orders vertices well and filters well. however, intelligence often comes at a cost.

related work
next, we briefly describe the state of the art. we refer to  <cit>  for a deep treatment of the subject.

a popular algorithm, vflib, was presented by cordella et al. in  <cit> . it uses a dynamic search strategy. given a partial solution, first it chooses unmatched pattern vertices having edges starting from vertices in the partial solution; then it chooses those unmatched vertices having edges ending in vertices in the partial solution. in order to reduce the search space, the approach uses the following two lookahead heuristics. a mapping pair , where u and u' are vertices of the pattern and target graph, respectively, is considered a valid match if it satisfies the following rules:  u and u' are both neighbors of matched vertices;  the number of unmatched pattern vertices which are neighbors of matched vertices and are connected with u must be less than or equal to the number of unmatched target vertices which are neighbors of matched vertices and are connected with u';  the number of vertices connected with u and not in  and  must be less than or equal to the number of vertices connected to u' that are not in  or . the rule  is subdivided into four cases depending on the direction of involved edges between the neighbors of u and the set at .the rule  applies only for induced subgraph isomorphism as opposed to monomorphisms.

subgraph isomorphism may be modeled as a constraint satisfaction problem . given a set of variables  and a set of constraints among them, a solution of a csp for the subgraph isomorphism problem consists of finding an assignment of values  to all variables such that all constraints are satisfied. initially, each pattern variable v is associated with a set of values formed by the set of target vertices that could be matched to v, i.e. lab = lab) and the degree of v is less of degree of m. that set is called the domain of v. constraints guarantee that isomorphism conditions are maintained.

several filtering techniques, such as forward-checking  <cit> , prune the branches of the search tree by propagating constraints to remove values from potential domains .

a branch is pruned when a domain becomes empty. in forward checking, first a variable is assigned, then all constraints involving such variables are propagated to remove values from other domains that are not consistent with the current assignment. this is called inference.

solnon in  <cit>  proposes lad, which combines the constraints that two pattern vertices cannot be matched with the same target vertex into a partial solution, together with the preservation of edge correspondence between the pattern graph and the target graph. such constraints are applied during backtracking and are propagated until convergence . lad defines a dynamic search strategy where the next pattern vertex to be assigned is the vertex with the smallest domain cardinality.

recently, ullmann proposed an algorithm called focussearch  <cit> . the search process is done by a backtracking algorithm that applies a bit-vector domain reduction to each step. before the search starts, it runs two preliminary steps. the first one, called prematch, fills domains by filtering them using vertex invariants based on labels and topology. the second one locally ensures that two pattern vertices cannot be matched to the same target vertex. after the preliminary steps, a static search strategy orders the pattern vertices in the following way: each pattern vertex with a single compatible target vertex is put at the head of the sequence and the next pattern vertex to be matched is the one with the highest number of branches between it and the partial solution. if there are two vertices that are equal candidates to be the the next vertex in the ordering, it chooses the one with the highest sum of the degrees of its neighbors.

other methods in the literatures do not rely on backtracking or csp techniques, but rather apply heuristics based on probabilistic functions, explicit enumeration of matches, and so on  <cit> .

contribution
inference-based methods, which propagate constraints until convergence , reduce the search time to the greatest extent. unfortunately, such inference is done at the price of a greater computational cost. on the other hand, when constraint verification is applied only locally , it is crucial to define a search strategy that tries to prune the search space as much and as early as possible at low cost. this aspect is not addressed by vflib. focussearch applies this concept only partially. it defines a static and partly target-dependent search strategy reflecting the pattern topology. it also performs local inference, minimizing the cost by using bit-vectors. in this paper we present a novel subgraph isomorphism algorithm, called ri . it creates a search strategy based only on the pattern graph topology. the order is chosen to create constraints as early as possible in the matching phase. roughly, vertices having high valence and that are highly connected with vertices previously present in the ordering tend to come early in the final variable-ordering. during the matching phase, ri does not apply any computationally costly pruning or inference rules. this is the first paper that compares all the most recent and used algorithms . we analyze algorithmic aspects including the size of search space, the memory requirement, the timeout of the algorithms, the matching time and the total time, varying the density and dimension of pattern and target graphs, the number and the distribution of the labels. dataset characteristics are typical of molecular biological data. we also used the synthetic data analyzed in the previous work by authors of lad and vflib. in order to validate our strategy, we compare ri and two versions of ri, called ri-ds and ri-dspm. ri-ds computes, after defining the variable order of pattern vertices and before the subgraph isomorphism starts, an initial domain assignment. for each pattern vertex, ri-ds computes its domain and verifies that pattern edges are compatible in the target domains. it does not apply inference or domain reduction during backtracking. this low-priced verification helps in large dense targets, because it reduces the number of candidates to be verified during backtracking. ri-dspm, in addition to ri-ds, uses the prematch phase defined in focussearch, i.e. filters domains by using vertex invariants based on neighbor labels and topology. we show that ri-dspm does not improve performance compare to ri and ri-ds. this behavior is supported by the analysis given in  <cit>  . moreover, it validates the main ideas in ri: a powerful pattern vertex ordering, i.e. strongly dependent only on pattern graph topology, together with light constraint verification, is more efficient than a local or global inference procedure.

RESULTS
compared software we compared ri, along with variants ri-ds and ri-dspm with vflib , lad and focussearch measuring the search space size, the matching time, the memory requirements and the total time.

ri, ri-ds, ri-dspm, vf <dig> and lad are implemented in c/c++. since focussearch has been released in modula <dig>  in order to compare the algorithms under the same platform, we re-implemented focussearch in c++ following the author's guidance and the original source code.

next we describe the pattern and target graphs used to test the algorithms. table  <dig> reports the statistics on the number of vertices, edges and labels of the real target graphs described below. we refer to the additional file  <dig> for details on the synthetic datasets. we consider all graphs to be directed. we transform undirected graphs into directed graphs by replacing each edge connecting two vertices with two edges.

aids
pdbsv1
pdbsv2
pdbsv3
graemlin
ppi
statistics of the number of vertices and number of edges. these describe the minimum, maximum and average number of vertices and edges in the dataset. total labels is the total number of labels in the dataset. avg label is the average number of labels per graph. standard deviations are reported in parentheses.

patterns are searched against all graphs of the dataset, by a one-to-many approach . see the table  <dig> in additional file  <dig> for the average  of the number of subgraph isomorphisms obtained per datasets.

molecular dataset aids dataset contains the topological structures of  <dig> chemical compounds that have been tested for evidence of anti-hiv activity . compounds are graphs where the number of vertices varies from  <dig> to  <dig>  they are small sparse graphs. since the aids dataset contains relatively small graphs, the patterns are graphs of the dataset. patterns were divided into four groups, each group has one hundred graphs, and each graph may have  <dig>   <dig>   <dig> or  <dig> vertices. the topology of patterns was chosen in order to respect the average degree and label distribution of the target graphs. this implies that patterns were often quite complex. patterns of the same size may have different number of matches .

protein dataset pdbsv <dig> dataset contains  <dig> graphs with data from dna, rna, and proteins having up to  <dig> vertices. original structures can be downloaded from http://www.fli-leibniz.de/imglibpdb/pages/entry_list-all.html <cit>  and http://www.rcsb.org/pdb/home/home.do <cit> . in our software package we include the software to convert the original data into graphs. our software makes use of the ball library available at http://www.ball-project.org. the dataset mostly contains large sparse graphs. pattern graphs were extracted from the corresponding target graphs fixing the number of wanted edges. patterns are subgraphs  of their corresponding target graphs. we create six groups of  <dig> random patterns having a number of edges equals to  <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig>  we generated the patterns from the original targets in the following way. starting from an edge, the algorithm adds its closest edges to a list of candidate edges. then, it choses a candidate edge, adds it to the pattern and then adds its neighbors to the candidates list. the process is repeated until the desired number of edges is reached. again, patterns reflect the average degree and label distribution of the target graphs. the number of subgraph isomorphisms for patterns of size  <dig> and  <dig> is larger than for smaller patterns sizes . this may due to the fact that patterns match parts of the backbones and parts of protein surfaces. protein surfaces are rich in atoms of the same type such as hydrogens, leading to an increased number of possible subgraph isomorphisms.

protein backbones dataset pdbsv <dig> dataset contains  <dig> proteins represented by the backbones of the proteins coming from the crystallography downloaded from jena  <cit>  and protein data bank  <cit>  converted to graphs by ball library . they are medium sparse graphs. they may have from  <dig> to  <dig> vertices per graph. pattern graphs were extracted from the corresponding target graphs as we have done for dataset pdbsv <dig>  we create seven groups of  <dig> random patterns, reflecting the typology of the target graphs, having a number of edges equals to  <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig>  again, the number of subgraph isomorphisms for patterns of size  <dig> and  <dig> is larger than for smaller patterns sizes but is much smaller compared to the corresponding results in pdbsv <dig> . this is due to the fact that the graphs are protein backbones.

proteins contact maps dataset pdbsv <dig> dataset contains  <dig> contact maps of the amino acids of the domains of the proteins, retrieved by cmview  <cit> . while for backbones we can have thousands of vertices corresponding to atoms, the number of vertices in the contact maps is relatively small , since they represent relationships among amino acids. contact maps are small dense graphs. in average a graph may have  <dig> vertices.

since target graphs are dense, we extracted from them different types of pattern graphs  to vary the performance comparisons. patterns were generated from the corresponding target graphs giving the number of desired edges. dense patterns are constructed by forcing the number of vertices to be approximately equal to 25% of the number of edges. since patterns are subgraphs  of their corresponding target graphs each pattern tends to reach the desired percentage of vertices. semi-dense patterns have a number of vertices almost equal to 50% of the number of edges. for the sparse patterns the percentage of vertices is set to 90% and cycles avoid simple structures as paths. for each density of patterns, we create seven groups of  <dig> random patterns having a number of edges equals to  <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig>  dense patterns have an average degree of  <dig>  with a standard deviation equals to  <dig> , an average number of labels equals to  <dig>  with  <dig> as standard deviation. semi-dense patterns have an average degree of  <dig> with a standard deviation equals to  <dig> , an average number of labels equals to  <dig>  with a standard deviation  <dig> . sparse patterns have an average degree of  <dig>  with a standard deviation equals to  <dig> , an average number of labels equals to  <dig>  with a standard deviation  <dig> . as we expected, dense patterns have fewer subgraph isomorphisms. the number of matches increases with the size of semidense patterns. however, this is not always true . sometimes, larger patterns may have fewer matches. in real data, this depends on the nature of the data.

graemlin dataset this dataset contains  <dig> microbial networks   <cit> . since each vertex has a unique label, we perform the test using networks without labels, with unique labels, and varying the number of assigned random labeled from  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  to  <dig>  for subgraph isomorphism algorithms, the former corresponds to the hardest and the easiest cases, respectively. in  <cit> , authors prove that the complexity of subgraph isomorphism algorithms is quadratic in the number of vertices on graphs labelled with unique labels. labels are assigned using a uniform distribution. as described for pdbsv <dig> we create sets of  <dig> dense, semidense and sparse patterns for each pattern dimension . the number of matches reflects the density but is not exactly proportional to the the pattern sizes .

protein−protein interaction networks dataset this dataset contains  <dig> networks describing the known and predicted protein interactions downloaded from string  <cit> . we used the following organisms: mus musculus, saccaromyces cerevisiae, caenorhabditis elegans, drosophila melanogaster, takifugu rubipres, danio rerio, xenopus tropicalis, bos taurus, rattus norvegicus, and homo sapiens. they are large dense graphs. since each vertex has a unique label, we perform tests using the networks with unique labels and then with randomly assigned labels varying the number from  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  to  <dig>  labels are assigned using a uniform distribution and a normal distribution . we extracted from the target graphs sets of  <dig> dense, semidense and sparse patterns for each pattern dimension . again, the number of matches depends on the density factor but is not exactly proportional to the pattern sizes . in the additional file  <dig> we present an application of subgraph isomorphism on protein complexes searching on go annotated ppi dataset.

synthetic dataset ri has been compared also on the synthetic dataset distributed by sansone et al.  <cit> . they are pairs of unlabeled graphs having sizes varying from  <dig> to  <dig> vertices. the dataset contains the following kinds of graphs: bounded valence: , mesh , and random . since the synthetic dataset is composed of a pair of  graphs, we do not need to generate patterns for it. we refer to  <cit>  for the detailed statistics of this dataset and to additional file  <dig> for the performance results on the compared softwares.

performance experiments have been conducted on a quadcore intel xeon  <dig>  ghz, with  <dig> physical cores at  <dig> bit with  <dig> mb cache,  <dig> gb of ram, running linux version  <dig> . <dig>  we do use multi-threading. experiments were repeated to get rid of caching effects. we set a timeout of  <dig> minutes to the total execution time of the algorithms . we chose this timer since it reflects in proportion the results reported in  <cit>  . for each dataset we report how many subgraph isomorphism runs each algorithm completes before the timeout. when an algorithm times out, we exclude the related running times from the means of all algorithms. ri shows the best behavior on all datasets. results for vf <dig> on ppi datasets are not reported since they often time out. focussearch times out on dense datasets and lad times out on large graphs.

here, we report the average and standard deviation of space size, memory requirement, matching time and total time on biochemical datasets . all detailed comparisons and plots are given in the additional file  <dig>  in additional files  <dig> and  <dig> we give a comparison of all tested algorithms on aids and pdbs datasets for time and space respectively. additional files  <dig> and  <dig> give the performances on synthetic datasets provided by sansone et al. additional files  <dig>   <dig>   <dig>   <dig> present a comparison of microbial and protein interaction networks varying the number of labels. for each dataset, tests are grouped with respect to the pattern density, number of labels or labels distributions as shown in the plots. we highlighted in bold the algorithm outperforming the others.

the total time needed by an algorithm includes the time to read graphs from files, to build data structures, to run preprocessing operations, to run the real matching phase and so on. therefore, we distinguish between the total time and the matching time. the matching time for ri and vf <dig> pertains only to the matching process; conversely for ri-ds, ri-dspm, lad and focussearch it also includes the preprocessing time. notice that the preprocessing steps are the first parts of the matching processes and they depend on the pattern graphs. the space size is the number of visited nodes of the hypothetical search space tree and the memory size is the number of kilobytes required to store all data structures.

beside total time comparisons, we analyzed space, and memory size due to the fact that comparing the performance of those algorithms means to deal with basic differences that may influence the final results and their applicability on a variety of data.

ri, ri-ds, ri-dspm, vf <dig>  and focussearch store data in adjacency lists. lad uses adjacency matrices. the formers use less memory but require linear time to check the existence of an edge. on the other hand, lad requires quadratic memory to represent the data, but verifies the existence of edges in constant time we refer to the actual implementation of data structures of the algorithms in the released codes. theoretically, efficient matrix implementations for sparse graphs could be used with moderate look-up time sacrifice. the search in adjacency lists could takes logarithmic time on the number of edges if a binary search were used on ordered lists of edges. moreover, each algorithm uses several data structures besides the data structures to store the graphs. therefore the resulting plots do not show the quadratic memory increases of an algorithm compared to another.

ri and vf <dig> do not use initial domains  or make use of variable domains. for this reason, focussearch, ri-ds, ri-dspm, and lad check label similarity between vertices or edges once. on the other hand, ri and vf <dig> compare labels even if they have already done so in some previous step.

since lad, focussearch, ri-ds and ri-dspm, run a preprocessing phase to filter out the variable domains before the matching phase begins, they can potentially generate a smaller search space.

the extensive reduction operations of lad prune the search space well at the price of a greater computational cost. focussearch applies cheaper reduction operations decreasing the running time but generating a larger search space. ri-ds and ri-dspm do not apply inference, therefore only the initial domains are reduced. ri and its two versions apply very light pruning rules and, therefore, they may generate a larger search space. in fact, the aim of our approach is to maintain a balance between the size of the generated search space and the time needed to visit it.

summarizing the results  we observed that

• ri always outperforms vf <dig> 

• ri outperforms all other algorithms in sparse target graphs such as aids, pdbsv <dig>  pdbsv <dig> 

• ri is comparable with lad and focussearch on small dense pattern graphs pdbsv <dig> with dense patterns, and with semidense small-medium patterns.

• ri outperforms lad but not focussearch on small dense pattern graphs pdbsv <dig> with large semidense or sparse patterns.

morever:

• we suggest to use ri-ds on medium or large dense targets . here, ri-ds outperforms  all algorithms across different numbers of labels, pattern dimensions, and densities.

• we do not suggest the use of ri-dspm or any costly inference or pruning rules. ri-dspm does not improve performance compare to ri and ri-ds. this behavior is supported by the analysis given in  <cit>  . since, our algorithm is independent of the used pruning rules, we also tried to run our algorithm with the rules from vflib  <cit> . experiments show that the rules helped to reduce the search space but their contribution were not significant and, in some cases, they increased the total time of the matching process. therefore, we did not deploy those pruning rules.

these considerations validate the main idea in ri: a powerful pattern vertex ordering, i.e. strongly dependent only on the pattern graph topology, together with light constraint verification, is more efficient than a local or global inference procedure.

CONCLUSIONS
subgraph isomorphism is an important functionality of biochemical tools. this paper has made two intellectual and two pragmatic contributions to solving this inherently difficult problem. first, it proposes a new algorithm that outperforms existing algorithms in many though not all settings. second it offers a perspective into when to choose which algorithm. third, it provides implementations of the various leading algorithms. fourth, it compares for the first time all most recent and popular subgraph isomorphism algorithms on biochemical data. in future work, we will apply these algorithms and similar analyses to approximate subgraph isomorphism search.

