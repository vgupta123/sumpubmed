BACKGROUND
nowadays, empirical methods of machine learning are widely used in life sciences and related sciences such as chemistry, biochemistry, pharmacy, and medicinal diagnostics. they can be used to predict the value of a target property in focus such as the competence of a molecular compound to fulfill a specific function. for this purpose, regression methods correlate specific molecular properties  of molecular compounds with the desired target property. in the simplest case, the regression uses an objective function whose number of parameters is as large as the number of considered features. usually the objective function contains also a so-called regularization term. it penalizes model details of unnecessary complexity, focuses on the most relevant features, and thus avoids over-fitting of the data used for training   <cit> . the most commonly used regularization methods are l <dig> regularization, also known as lasso  <cit>  and l <dig> regularization also known as ridge regression  <cit> . as penalty term, the l <dig> regularization adds the sum of the absolute values of the model parameters to the objective function whereas the l <dig> regularization adds the sum of the squares of them. due to its inherent linear dependence on the model parameters, regularization with l <dig> disables irrelevant features leading to sparse sets of features. thus, l <dig> regularization combines efficient feature selection and model generation into one single optimization step.

in recent years, considerable advancements were made in high throughput techniques to generate for a large number of relevant molecular compounds the target values in focus. nevertheless, for many problems, where molecular target values need to be predicted by empirical machine learning methods, the amount of data is often scarce. hence, in a typical prediction scenario the number of compounds with known target values can be very small , while the number of potentially relevant features , which need to be employed initially is often large . under such circumstances, overtraining would be unavoidable unless specific precautions are applied to control and reduce the number of features and thus also the number of parameters. becoming aware of these problems, the biological research community has started to pay increasingly attention to feature selection techniques. there are various feature selection strategies, which all have their pros and contras. recently, feature selection using lasso  regularization has gained research interests due to its simplicity and several modifications of the original l <dig> regularization have been developed  <cit> .

in this study we apply in a two-step regularization procedure where first l <dig> and than l <dig> regularization is applied, using l <dig> regularization for feature selection only. with the remaining selected features, the final model achieves higher accuracy, if it is build with l <dig> regularization only. in spite of its simplicity, l <dig> regularization requires special solvers, since the derivatives of the l <dig> regularization term are not defined at vanishing parameter values. a number of highly optimized solvers are available that can minimize the objective function in presence of l <dig> regularization  <cit> . however, the provided implementations are limited to a certain programming language and loss functions complicating re-implementation and modification of the original algorithm. in this work, the simple rprop algorithm  <cit>  is used to approximate the optimal l <dig> regularized solution leading to good results. we apply the two-step method mentioned above to the prediction tasks of coepra  modeling competition of  <dig>  <cit> . these data sets are characterized by few data points  with a large number of features . the data sets of coepra  <dig> contain octo- and nona-peptides relevant to mhc class i binding which play an important role in the immune response of mammals. the purpose of this competition was to facilitate testing and comparison of various classification and regression algorithms for biological active molecules using blind prediction. the participant groups in coepra applied various methods to the data sets at each task, and the organizers evaluated all collected predictions and then announce the rank of the participants on web site. all data sets can be obtained free of charge from the coepra web site  <cit> .

RESULTS
the training sets have been used to build prediction models using the proposed two-step learning method. the models have then been used to predict the provided test sets. all experiments were performed on a single core of an intel xeon x <dig> cpu running at  <dig> ghz. typically the whole optimization of a prediction model for one of the four coepra regularization tasks required only  <dig> minutes of cpu time. thereby the most cpu time demanding step is the optimization of the regularization parameters λ <dig> , involving the average over five different ten-fold cross validations for the eight and thirteen considered candidate values of λ <dig> and λ <dig>  respectively. the cpu time increases linearly with the number of features and the number of molecules in the training data set. furthermore, the number of cross validation rounds may be decreased for large data sets. hence, the proposed method is also applicable for much larger data sets. finally, using the optimized prediction model requires only milliseconds of cpu time per molecule to be predicted. hence, it can be used as a high through-put method.

we compared the achieved results with the top performing participants of the coepra contest. table  <dig> shows the prediction results in terms of q <dig> values, eq. , on the test sets for all four coepra tasks together with the optimized regularization parameters λ <dig> and λ <dig>  which have been selected using the cross validation technique as described in the methods section. for all four tasks, the number of features could be reduced drastically to about one hundredth of the initial number of features or even less. surprisingly, based on the set of features selected with l <dig> regularization in the first training stage, a better prediction performance is obtained, if in the second training stage l <dig> instead of l <dig> regularization is used. the prediction results of the present study surpass the best performing participants of the coepra contest adopting first rank for task i and second rank for task ii and iii . as one can see from the very low q <dig> values for task iii, the prediction results are not very significant. this is due to a lack of overlap between the target values of the training and the test set as discussed below.

                           a
a numbers in brackets are spearman rank correlation coefficients   <cit> .

b number of features after l <dig> regularization.

prediction results of q <dig> values, eq. , for all four coepra regression tasks using a two-step optimization procedure. first three lines display the results of the three best predictions for the different ceopra tasks. stage 1: only l <dig> regularization is used. all features are removed, where the corresponding parameters have absolute values smaller than 10- <dig> after optimization, stage 2: only l <dig> regularization is applied for all features remaining after stage  <dig>  the regularization parameters λ <dig> and λ <dig> have been determined using  <dig> times a 10-fold cross validation procedure.

the winner of coepra regression task ii and iv, and the second best for task iii is the group of curt breneman. their method combined the provided physico-chemical features with  <dig> recon  descriptors. feature selection has been performed using principal component analysis. their final models contained  <dig>   <dig> and  <dig> features in task ii, iii and iv respectively. wit jakuczun, who ranked first, second and third for task iii, i and iv, respectively, used a random forest approach for feature selection and model building  <cit> . for task iii, his prediction model used  <dig> relevant features. almost all methods used by the coepra participants applied an independent feature selection step before the final model building. it is worth mentioning that none of the participants of the coepra contest was able to rank best for all four regression tasks. our prediction method provides good scores using consistently the same method for all four coepra regression tasks. it serves a double purpose namely a dramatic reduction of the number of used features by feature selection and a subsequent parameter optimization in a two-step learning procedure.

feature selection via l <dig> regularization
how the feature selection with l <dig> regularization works in detail is demonstrated in figure  <dig>  it illustrates the disappearance  and occasional reappearance  of features with increasing λ <dig> values for coepra task i. the left vertical axis represents the λ <dig> values whereas the right vertical axis shows the corresponding number of remaining features. the horizontal axis displays the  <dig> initially available features. features that have been discarded in the selection procedure correspond to white gaps. interestingly, a very small λ <dig> value of  <dig>  in l <dig> regularization leads already to a reduction of the number of feature by more than a factor of ten. the color code of the dots exhibits how often the corresponding features reappear with increasing λ <dig> values after they have been discarded in a previous selection round. with each addition reappearance event, the colored dot that marks the corresponding feature changes stepwise from black to yellow. hence, dots in darker colors represent features that disappeared and reappeared rarely again whereas dots in brighter colors represent features that reappear more often. for technical reasons some dots can be covered by other dots . therefore, the number of visible dots is smaller than the number of selected features. interestingly, features selected at larger λ <dig> values were not necessarily selected in previous steps with lower λ <dig> values. there are no dots consistently appearing for all λ <dig> values. this demonstrates that feature selection by l <dig> regularization possesses some arbitrariness. we should expect such behavior for regression problems with highly redundant feature sets. here, the composition of selected features depends strongly on the used λ <dig> value. therefore, it is crucial to set the regularization parameter λ <dig> to an appropriate value before model building. for feature sets containing only few meaningful descriptors among a large number of meaningless features, we expect these important features to be selected for practically all λ <dig> values. this favorable property of l <dig> regularization suggested that it works like an oracle as discussed in some papers  <cit> .

dependence of number of features and prediction performance
discussion
in this study we applied a two-step approach using first feature selection and subsequent model building. in the first stage l <dig> regularization is used to filter out redundant and irrelevant features. the remaining features are used in a second stage of model building in conjunction with l <dig> regularization. for both steps  an appropriate regularization strength, governed by λp, is crucial. if the regularization strength is too small, unimportant features may get a strong influence on the resulting prediction model. if it is too large, also relevant features will be removed resulting in poorer prediction performance. there are various ways to optimize the λp values. in this work the λp optimization has been done in an additional step using k times n-fold cross validation.

with the l <dig> regularization, we were able to select about one hundred or less features of the initial feature set. however, the set of selected features varied strongly with the regularization strength used to build the prediction model. we explained this behavior by the fact that these regression tasks employ feature sets with high redundancies as we used many physico-chemical features to describe each amino acid. hence, several of these properties may be used equivalently to discriminate between amino acids. however, not only the selected number of features but also the prediction performance of the first stage depends strongly on the parameter value used for l <dig> regularization.

applying l <dig> regularization for the selected features, we were able to achieve an even higher prediction performance for three of the four coepra regression tasks. hence, the feature selection with l <dig> regularization should always be followed by an l <dig> regularized model-building step.

instead of a highly optimized solver to overcome the singular behavior occurring with l <dig> regularization, we made use of the simple rprop algorithm  <cit> . although the rrpop method is not able to set the feature weights exactly to zero the optimal solution is well approximated. setting a threshold near zero and removing all features with an absolute weight below this threshold seems to be a simple alternative and may also be useful for other machine learning approaches.

the choice of loss function, eq. , can have a large influence on prediction results. squared error loss functions, most often used, try to recall each data point of the training set as accurate as possible. hence, errors in the training set may have a large influence on the predictor. in this work a loss function is used which increases very smoothly with increasing prediction errors weakening the influence of potential outliers.

the data sets of the coepra contest are particularly valuable, since they offer the possibility to compare the own approach with a larger number of alternative approaches from different groups on equal footing. in one case  our approach surpasses the best result obtained in the coepra concourse, while in two cases  our results are at second rank. for task iii the feature selection with l <dig> regularization was seemingly too rigorous. hence, the results in the second stage were of lower quality than in the first stage where the second rank was obtained. however, we like to point out the following. although, we did not make explicit use of knowledge on the prediction set, we knew them ahead. this can have a subtle influence on the details of the procedures we were selecting in the present study and may thus have provided a hidden advantage.

previous studies have used reduced sets of descriptors to encode amino acid sequences  <cit> . these small descriptor sets were generated using multivariate statistical methods to reduce the dimension of the feature space by principal component analysis   <cit> . in pca the main assumption is that the variance of a feature represents its information content. hence, pca tries to project the original features into a lower dimensional space such that the variance of data in the lower dimensional space is maximized. the advantage of these generated small feature sets is that they can be used instantly to build models using few descriptors. the proposed l1/l <dig> method on the other hand has to be performed for each specific task . this results in highly optimized feature sets, which we expect to perform better than feature sets obtained with a generally valid reduction scheme. it is also possible to use pca for a specific task similar to conventional feature selection. in this case pca is performed on the training data of the considered problem. we used pca analysis in this manner in previous unpublished studies. pca was able to reduce the number of features drastically. still, using conventional feature selection resulted in better predictive power in most cases. this may be explained by the fact that pca combines the original descriptors such that the variance in the reduced space is maximized. however, features with large variance carry not necessarily the most useful ones for a specific learning task. furthermore, it has to be kept in mind that pca features in the lower dimensional space are linear combinations of the original descriptors. hence, interpretation of selected descriptors may be much less intuitive than by the use of a classical feature selection method.

CONCLUSIONS
the limitations of wet labs to generate larger data sets for a particular problem of interest may be due to various reasons. mostly measurements may be expensive or complicated and time consuming to perform such as in vivo experiments. another reason may be that the regarded problem is a quite new one. in all these cases in silico methods are highly requested as these are able to easily predict the desired target property of new compounds. without precautions many machine learning methods fail in such situations. hence, specialized methods are desired. our proposed method achieved good prediction results for the four coepra regression tasks. furthermore, the number of molecular descriptors has been reduced drastically for the final prediction models. the coepra data sets are representative for many biological classification and regression problems where small data sets of less than hundred are described by thousands of descriptors. hence, we expect the proposed method to be applicable for many other machine learning tasks having same conditions.

