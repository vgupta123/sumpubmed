BACKGROUND
protein-protein interaction  plays an essential role in many cellular processes. in order to have a better understanding of intracellular signaling pathways, modeling of protein complex structures and elucidating various biochemical processes, many high-throughput experimental methods, such as yeast two-hybrid system and mass spectrometry method, have been used to uncover protein interactions. however, these methods are known to be prone to having high false-positive rates, besides their high cost. therefore, great efforts have been made to develop efficient and accurate computational methods for ppi prediction.

many pair-wise biological similarity based computational approaches have been developed to predict if any given pair of proteins interact with each other, based on various properties such as sequence homology, gene co-expression, phylogenetic profiles, three-dimensional structural information, etc. . however, without first principles to tell deterministically if two given proteins interact or not, the pair-wise biological similarity based on various features and attributes can run out its predictive power, as the signals may be weak, noisy, or inconsistent, which can present serious issues even for methods based on integrated heterogeneous pair-wise features, e.g. genomic features, semantic similarities, etc. .

to circumvent the limitations with using pair-wise biological similarity, pair-wise topological features have been used to measure the similarity for any given node pair to make ppi prediction for the corresponding proteins , if a ppi network is constructed with nodes representing proteins and edges representing interactions. moreover, to go beyond these node centric topological features and get the whole network structure involved, variants of random walk  <cit>  based methods  have been developed, but the computational cost of these methods increases by n times for all-against-all ppi prediction. thus many kernels on network for link prediction and semi-supervised classification have been systematically studied  <cit> , which can measure the random-walk distance for all node pairs at once. but both the variants of random walk and random walk based kernels do not perform well in detection of interacting proteins when the direct edge connecting them in the network is removed and the remaining path connecting them is long  <cit> . besides, instead of computing proximity measures between nodes from the network structure explicitly, many latent features based on rank reduction and spectral analysis have been utilized to do prediction, such as geometric de-noise methods  <cit> , multi-way spectral clustering  <cit> , matrix factorization based methods  <cit> . mostly, the prediction task of these methods will be reduced to the convex optimization problem whose objective function should be carefully designed to ensure fast convergence and avoid being stuck in the local optima. furthermore, biological features and topological features can supplement each other to improve the prediction performance, such as by assigning weights to edges in the network based on pair-wise biological similarity scores. then, methods based on explicit or latent features, such as supervised random walk  <cit>  or matrix factorization method, can be applied to the weighted network to make prediction, based on multi-modal biological sources.  <cit> . however, for these methods, only the pair-wise features for the existing edges in the ppi network will be utilized, even though from a ppi prediction perspective what is particularly useful is to incorporate pair-wise features for node pairs that are not currently linked by a direct edge but will if a new edge  is predicted.

therefore, it is of great interest if we can infer ppi network directly from multi-modal biological features kernels that involve all node pairs. it not only can help us improve prediction performance but also provide insights into relations between ppis and various similarity features of protein pairs. yamanishi et al.  <cit>  developed a method based on kernel canonical correlation analysis to infer ppi networks from multiple types of genomic data. however, in that work all genomic kernels are simply added together, with no weights to regulate these heterogeneous and potentially noisy data sources for their contribution towards ppi prediction. meanwhile, it seems that the partial network needed for supervised learning based on kernel cca need to be sufficiently large, e.g., a leave-one-out cross validation is used, to attain good performance. in huang et al.  <cit>  the weights for different data sources are optimized using a sampling based method, abc-dep, which is computationally demanding.

in this paper, we propose a new method to infer de novo ppis by combining multiple data sources represented in kernel format and obtaining optimal weights based on random walk over the existing partial network. the novelty of the method lies in the use of barker algorithm to construct the transition matrix for the training subnetwork and find the optimal weights by linear programing to minimize the element-wise difference between the transition matrix and the adjacency matrix, aka, the weighted kernel from multiple heterogeneous data. then we apply regularized laplacian kernel  to the weighted kernel to infer missing or new edges in the ppi network. a preliminary version of this work was described in  <cit> . relative to that paper, the current work includes extension to handle interaction prediction problem for ppi networks consisting of disconnected components and new results on the human ppi network, which is much more sparse than the yeast ppi network. our method can circumvent the issue of unbalanced data faced with many machine learning methods in bioinformatics by training on only a small partial network. our method works particularly well with detecting interactions between nodes that are far apart in the network.

methods
problem definition
formally, a ppi network can be represented as a graph g= with v nodes  and e edges . g is defined by the adjacency matrix a with v×v dimension: 
  <dig> a= <dig> if∈e <dig> if∉e 

where i and j are two nodes in the nodes set v, and  represents an edge between i and j, ∈e. the graph is called connected if there is a path of edges to connect any two nodes in the graph. given many ppi networks are not connected and has many connected component with various size, we select a large connected component  as golden standard network to do supervised learning. specifically, by adopting the same setting in  <cit> , we divide the golden standard network into three parts: connected training network gtn=, validation set gvn= and testing set gtt=, such that e=etn∪evn∪ett, and any edge in g can only belong to one of these three parts.

a kernel is a symmetric positive definite matrix k, whose elements are defined as a real-valued function k satisfying k=k for any two proteins u and v in the data set. intuitively, the kernel built from a given dataset can be regarded as a measure of similarity between protein pairs with respect to the biological properties, from which kernel function takes its value. treated as an adjacency matrix, a kernel can also be thought of as a complete network in which all the proteins are connected by weighted edges. kernel fusion is a way to integrate multiple kernels from different data sources by a linear combination. for our task, this combination is made of the connected training network and various feature kernels ki, i= <dig> , <dig> ..n, which formally is defined by eq.  
  <dig> kfusion=w0gtn+∑i=1nwiki,whereki=ki∑wki 

note that the training network is incomplete, i.e., with many edges taken away and reserved as testing examples. therefore, the task is to infer or recover the interactions in the testing set gtt based on the kernel fusion. once the kernel fusion is obtained, it will be used to make ppi inference, in the spirit of random walk. however, instead of directly doing random walk, we apply regularized laplacian  kernel to the kernel fusion, which allows for ppi inference on the whole network level. the regularized laplacian kernel  <cit>  is also called the normalized random walk with restart kernel in mantrach et al.  <cit>  because of the underlying relations to the random walk with restart model  <cit> . formally, it is defined as eq. , where l=d−a is the laplacian matrix made of the adjacency matrix a and the degree matrix d, and 0<α<ρ− <dig> and ρ is the spectral radius of l. here, we use kernel fusion in place of the adjacency matrix, generating a regularized laplacian matrix rlk, so that various feature kernels in eq.  are incorporated in influencing the random walk with restart on the weighted networks  <cit> . with the regularized laplacian matrix, no random walk is actually needed to measure how "close" two nodes are and then use that closeness to infer if the two corresponding proteins interact. rather, rlk is interpreted as a probability matrix p in which pi,j indicates the probability of an interaction for protein i and j. 
  <dig> rl=∑k=0∞αkk=− <dig> 

to ensure good inference, it is important to learn optimal weights for gtn and various ki to build kernel fusion kfusion. otherwise, given the multiple heterogeneous kernels from different data sources, the kernel fusion without optimized weights is likely to generate erroneous inference on ppi.

weight optimization with linear programming 
given a ppi network, the probability of interaction between any two proteins is measured in terms of how likely a random walk in the network starting at one node will reach the other node. here, instead of solely using the adjacency matrix a to build the transition matrix, we integrate kernel features as edge strength. then the stochastic transition matrix q can be built by: 
  <dig> q=kfusion 

assuming the network is reasonably large, for a start node s, the probability distribution p of reaching all nodes via random walk in t steps can be obtained by applying the transition matrix qt times: 
  <dig> pt=qtp <dig> 

where the initial distribution p <dig> is 
  <dig> pi0= <dig> ifi=s <dig> otherwise 

the stationary distribution p, when letting t go to infinity, is obtained by solving the following eigenvector equation: 
  <dig> p=qp 

this stationary distribution provides constraints at optimizing the weights. for example, the positive training examples  should have higher probability than the negative training examples . in backstrom et al.  <cit> , this is used as constraint in minimizing the l <dig> norm of the weights for optimal weights. in the work of backstrom et al.  <cit> , a gradient descent optimization method is adopted to get optimal weights, and only the pair-wise features for the existing edges in the network are utilized, which means q is nonzero only for edge  that already exists in the training network. to leverage more information from multiple heterogeneous sources, in our case the q, as defined in eq. , are nonzero unless there is no features for edge i,j in all kernels ka. having many non-zero elements in q makes it much more difficult for the traditional gradient descent optimization method to converge and to find the global optima.

in this work, we propose to solve the weights optimization differently. we can consider the random walk with restarts process shown in eq.  as a markov model, with a stationary distribution p. knowing the stationary distribution, the transition matrix can be obtained by solving the reverse eign problem using the well-known metropolis algorithm or barker algorithm. in this work, we adopt barker algorithm  <cit> , which gives the transition matrix as follows. 
  <dig> qb=pjpi+pj 

now we can formulate weights optimization by minimizing the element-wise difference between qb and q. namely, 
  <dig> w∗=argminw||q−qb|| <dig> 

as the number of elements in the transition matrix is typically much larger than the number of weights, eq.  provides more equations than the number of variables, making it an overdetermined linear equation system. this overdetermined linear equation system can be solved with linear programming using standard programs in  <cit> .

now, in the spirit of supervised learning, given the training network gtn and a start node s, we calculate p′ by doing random walk that start at s in gtn as an approximation of p, and qb=pj′pi′+pj′. note that qb from barker algorithm is an asymmetric matrix whereas q composed from kernel fusion is a symmetric matrix. so, we do not need to use all equations obtained from eq.  to calculate the weights. instead we can just use equations derived from the upper or lower triangle part of the matrices qb and q. this reduction of number of equations will not pose an issue as the system is overdetermined; rather this will help mitigate the issue of being overdetermined. specifically, as shown in fig.  <dig>  for all destination nodes v in gtn, namely reachable from start node s, we divide them into three subsets d, l and m, where d consists of near neighbors of s in gtn with the shortest path between s and nodes di satisfying d<ε1; and l includes faraway nodes of s in gtn with the shortest path between s and nodes li satisfying d>ε2; and the rest of nodes are in subset m. then the system of equations of eq.  is updated to eq. , where u<v indicates lower triangle mapping, and u>v indicates upper triangle mapping. 
  <dig> w0gtn+∑i=1nwiki=qb,ifu,v∈d∪l∧ki!=0∧ fig.  <dig> schematic illustration of node sets d, m and l, with respect to source node s



the optimized weights w∗ can then be plugged back into eq.  to form an optimal transition matrix for the whole set of nodes, and the random walk from the source node using this optimal transition matrix hence leverages the information from multi data sources and is expected to give more accurate prediction for missing and/or de novo links: nodes that are most frequented by random walk are more likely, if not yet detected, to have a direct link to the source node. the formal procedure for solving this overdetermined linear system and inferring ppis for a particular node is shown by algorithm  <dig> 



ppi prediction and network inference
as we discussed in introduction section, the use of random walk from a single start node is not efficient for all-against-all prediction, especially for the large and sparse ppi networks. therefore, it would be of great interest if the weights learned by wolp based on a single start node can also work network wide. actually, it is widely observed that the many biological networks contain several hubs   <cit> . thus we extend our algorithm to all-against-all ppi inference by hypothesizing that the weights learned based on a start node with high degree would be utilizable by other nodes. we will verify this hypothesis by doing all-against-all ppi inference for real ppi network.

we design a supervised wolp version that can learn weights more accurately for the large and sparse ppi network. similarly, if the whole ppi network is connected, then the golden standard network is itself; otherwise, the golden standard network that used to do supervised learning should be a large component of the disconnected ppi network. to do so, we divide the golden standard network into three parts: connected training network gtn=, validation set gvn= and testing set gtt=, such that e=etn∪evn∪ett, and any edge in g can only belong to one of these three parts. then we use wolp to learn weights based on gtn and gvn, and finally use gtt to verify the prediction capability of these weights. the main structure of our method is shown by algorithm  <dig>  and the supervised version of wolp is shown by algorithm  <dig>  the while loop in algorithm  <dig> is used to find optimal setting of d, l and mapping strategy that can generate best weights wopt with respect to inferring and gtn and gvn.





moreover, many existing network-level link prediction or matrix completion methods  <cit>  can only work well on connected ppi networks, but detection of interacting pairs for disconnected ppi networks has been a challenge for these methods. however, our wolp method can solve the problem effectively. because various feature kernels can connect all the disconnected components of the originally disconnected ppi network; and we believe once the optimal weights have been learned based on the training network generated from a large connected component , they can also be used to build the kernel fusion when the prediction task scale up to the originally disconnected ppi network. to do so, we update the algorithm  <dig> to algorithm  <dig> that shows the detailed process of interaction prediction for disconnected ppi networks. given an originally disconnected network g, firstly, we learn the optimal weights by algorithm  <dig> based on a large connected component gcc of g. after that, we randomly divide the edge set e of the disconnected g into training edge set gtn and testing edge set gtt, and use the optimal weights we learned before directly to linearly combine gtn and other corresponding feature kernels to build the kernel fusion, and finally evaluate the performance through predicting gtt. here we call gtn training edge set, because gtn no longer needs to be connected to learn any weights.



RESULTS
we examine the soundness and robustness of the proposed algorithms with use of both synthetic and real data. our goal here is to demonstrate that the weights obtained by our method can help build a better kernel fusion leading to more accurate ppi prediction.

experiments on single start node and synthetic data
a synthetic scale-free network gsyn with  <dig>  nodes is generated by copying model  <cit> : gsyn starts with three nodes connected in a triad. remaining nodes have been added one by one with exactly two edges for each. for instance, when a node u is added, two edges ,i= <dig>  between u and existing nodes vi will be added accordingly. node vi is randomly selected with probability  <dig> , and otherwise vi is selected with probability proportional to its current degree. the parameters we chose is to guarantee gsyn has similar size and density to dip yeast ppi network  <cit>  that we will use to do ppi inference later. then we build eight synthetic feature kernels for gsyn. the feature kernels can be classified into three categories:  <dig> noisy kernels,  <dig> positive kernels and a mixture kernel, which are defined by eq.  
  <dig> knoise=r5093+.∗randdiffkpostive=r5093+.∗randsubkmixture=r5093+.∗randsub+.∗randdiff 

where r <dig> indicates a  <dig> by  <dig> random matrix with elements between , which can also be seen asbackground noise matrix; j <dig> indicates a  <dig> by  <dig> all-one matrix, randdiff is used to randomly generate a difference matrix  =  <dig> in gsyn and  should be  <dig> in the difference matrix) between j <dig> and gsyn with density ρi; randsub is used to generate a subnetwork from gsyn with density ρi; ρi are different for each kernel; η is a positive parameter between  and r <dig> will be rebuilt every time for each kernel.

the general process of experimenting with synthetic data is: we generate synthetic network gsyn, synthetic feature kernels k firstly, and then divide nodes v of gsyn into d, l and m, where d and l can be seen as training nodes, m can be seen as testing nodes. by using gsyn, start node s and k, we can get the stationary distribution p based on the optimized kernel fusion kopt=w0gsyn+∑i=1nwiki. finally, we try to prove that kopt is better than the control kernel fusion kew=gsyn+∑i=1nki built by equal weights, if the p is more similar to p′ based on gsyn, as compared to p″ based on the control kernel fusion kew, where p indicates the rank of stationary probabilities respect to the testing node m. we evaluate the rank similarity between pairs ,p′) and ,p′) by discounted cumulative gain   <cit> .

we carry out  <dig> experiments, each time we select one of the oldest  <dig> nodes as start node, and rebuild synthetic kernel k. in table  <dig>  the results show that dcg@ <dig> between p and p′ is consistently higher than that between p″ and p′ in all  <dig> experiments, indicating that the optimal weights w obtained by wolp can help us build optimized kernel fusion that with better prediction capability, as compared to the control kernel fusion.


experiments on network inference with real data
we use the yeast ppi network downloaded from dip database   <cit>  and the high-confidence human ppi network downloaded from preppi database  <cit>  to test our algorithm.

data and kernels of yeast ppi networks
for the yeast ppi network, some interactions without uniprotkb id have been filtered out in order to do name mapping and make use of genomic similarity kernels  <cit> . as a result, the originally disconnected ppi network contains  <dig> proteins and  <dig>  interactions. the largest connected component consists of  <dig> proteins and  <dig>  interactions, and is used to serve as the golden standard network.

six feature kernels are included in ppi inference for the yeast data. gtn: gtn is the connected training network that provides connectivity information. it can also be thought of as a base network to do the inference. kjaccard  <cit> : this kernel measure the similarity of protein pairs i,j in term of neigbors∩neighborsneighbors∪neighbors. ksn: it measures the total number of neighbors of protein i and j, ksn=neighbors+neighbors. kb  <cit> : it is a sequence-based kernel matrix that is generated using the blast  <cit> . ke  <cit> : this is a gene co-expression kernel matrix constructed entirely from microarray gene expression measurements. kpfam  <cit> : similarity measure derived from pfam hmms  <cit> . all these kernels are normalized to the scale of  <cit>  in order to avoid bias.

data and kernels of human ppi networks
the originally disconnected human ppi network has  <dig> proteins and  <dig> interactions, which is much sparser than the yeast ppi network. the largest connected component that serve as the golden standard network contains  <dig> proteins and  <dig> interactions.

eight feature kernels are included in ppi inference for the human data. gtn: gtn is the connected training network that provides connectivity information. it can also be thought of as a base network to do the inference. kjaccard  <cit> : this kernel measure the similarity of protein pairs i,j in term of neigbors∩neighborsneighbors∪neighbors. ksn: it measures the total number of neighbors of protein i and j, ksn=neighbors+neighbors. kb: it is a sequence-based kernel matrix that is generated using the blast  <cit> . kd: it is a domain-based similarity kernel matrix measured by the method of neighborhood correlation  <cit> . kbp: it is a biological process based semantic similarity kernel measured by resnik with bma  <cit> . kcc: it is a cellular component based semantic similarity kernel measured by resnik with bma  <cit> . kmf: it is a molecular function based semantic similarity kernel measured by resnik with bma  <cit> .

ppi inference based on the largest connected component
for cross validation, like in  <cit> , the golden standard ppi network  is randomly divided into three parts that are connected training network gtn, validation edge set gvn and testing edge set gtt, where gvn is used to find optimal weights for feature kernels and gtt is used to evaluate the inference capability of our method. the table  <dig> shows detailed division for yeast and human ppi networks.
g
tn
g
vn
g
tt


with the weights learned by wolp and using ith hub as the start node, we build the kernel fusion wolp-k-i by eq. . ppi network inference is made by rl kernel eq. , and named as rlwolp-k-i, i= <dig> , <dig>  the performance of inference is evaluated by how well the testing set gtt is recovered. specifically, all node pairs are ranked in decreasing order by their edge weights in the rl matrix, and edges in the testing set gtt are labeled as positive and node pairs with no edges in g are labeled as negative. an roc curve is plotted for true positive v.s. false positives, by running down the ranked list of node pairs. to make comparison, besides the ppi inferences rlwolp-k-i, i= <dig> , <dig> learned by our wolp, we also include other two ppi network inferences: rlgtn and rlew-k, where rlgtn indicates rl based ppi inference is solely from the training network gtn, and rlew-k represents rl based ppi inference is from kernel fusion built by equal weights, e.g. wi= <dig>  i= <dig> ...n. additionally, gset∼n indicates there is n number of edges in the set gset, e.g. gtn∼ <dig> means the connected training network gtn contains  <dig> edges.

the comparisons in terms of roc curve and auc for yeast and human data are shown by fig.  <dig> and  <dig>  the ppi reference rlwolp-k-i, i= <dig> , <dig> based on our wolp method significantly outperforms the two basic control methods, with about  <dig> % increase over rlgtn and about  <dig>  % over rlew-k in term of auc for the yeast data, and about  <dig>  % increase over rlgtn and about  <dig>  % over rlew-k in term of auc for the human data. it is noted that the auc of ppi inference rlew-k based on the equally weighted built kernel fusion is no better or even worse than that of rlgtn based on a really small training network, especially for the yeast data. it means there should be a lot of noises if we just naively combine different feature kernels to do ppi prediction.
fig.  <dig> yeast: roc curves of predicting g
tt∼ <dig> by rlgtn∼ <dig>  r
l
wolp-k-i and r
l
ew-k


fig.  <dig> human: roc curves of predicting g
tt∼ <dig> by rlgtn∼ <dig>  r
l
wolp-k-i and r
l
ew-k




besides inferring ppi network by using weights learned based on the top three hubs in gtn, we also test the predicting capability of ppi inferences by using top ten hubs as start nodes to learn the weights. we make  <dig> repetitions for the whole process: generating gtn, choosing ith, i= <dig> ,... <dig> hub as start node to learn the weights, then using these weights to build kernel fusion and finally to do the ppi inference. for the results based on top ten hubs in each repetition, the average auc of inferring gtt for yeast data and human data are shown in tables  <dig> and  <dig> respectively. and the comparison shows the predicting capability of our method is consistently better than that of rlgtn and rlew-k for both yeast and human data.


effects of the training data
usually, given a golden standard data, we need to retrain the prediction model for different division of training set and testing set. however, if optimal weights have been found for building kernel fusion, our ppi network inference method enable us to train the model once, and do prediction or inference for different testing sets. to demonstrate that, we keep the two ppi inferences rlwolp-k- <dig> and rlew-k obtained before  unchanged, and evaluate the prediction ability for different testing sets. we also examine how performance is affected by sizes of various sets. specifically, while the size of training network gtn for rlgtn increases, sizes of gtn for rlwolp-k- <dig> and rlew-k are kept unchanged. therefore, we design several experiments by dividing the golden standard network into gtni and gtti, i= <dig> ...,n, and building ppi inference rlgtni to predict gtti for every time. to make comparison, we also use rlwolp-k- <dig> and rlew-k to predict gtti. as shown by the table  <dig>  for yeast data, rlwolp-k- <dig> trained on only  <dig>  golden standard edges still performs better than the control methods, even for the rlgtn that employ significantly more golden standard edges. similarly, for the result of human data as shown by the table  <dig>  rlwolp-k- <dig> trained on only  <dig>  golden standard edges still performs better than the control method rlgtn that employ over  <dig>  more golden standard edges.
g
g
g

rlwolp-k-1:gtn∼5394

rlgtn∼7394

rlew-k:gtn∼5394

rlwolp-k-1:gtn∼5394

rlgtn∼8394

rlew-k:gtn∼5394

rlwolp-k-1:gtn∼5394

rlgtn∼9394

rlew-k:gtn∼5394
g
g
g

rlwolp-k-1:gtn∼3310

rlgtn∼3710

rlew-k:gtn∼3310

rlwolp-k-1:gtn∼3310

rlgtn∼4210

rlew-k:gtn∼3310

rlwolp-k-1:gtn∼3310

rlgtn∼4710

rlew-k:gtn∼3310


detection of interacting pairs far apart in the network
it is known that the basic idea of using random walk or random walk based kernels  for ppi prediction is that good interacting candidates usually are not faraway from the start node, e.g. only  <dig>   <dig> edges away in the network. consequently, the testing nodes have been chosen to be within a certain distance range, which largely contributes to the good performance reported by many network-level link prediction methods. in reality, however, a method that is capable and good at detecting interacting pairs far apart in the network can be even more useful, such as in uncovering cross talk between pathways that are not nearby in the ppi network.

to investigate how our proposed method performs at detecting faraway interactions, we still use rlgtn∼ <dig>  rlwolp-k- <dig> and rlew-k for yeast data, and rlgtn∼ <dig>  rlwolp-k- <dig> and rlew-k for human data to infer ppis, but we select node pairs  that satisfy dist> <dig> givengtn from gtt as new testing set and name it gtt>3). figures  <dig> and  <dig> show the results of yeast and human data respectively, which demonstrate that rlwolp-k- <dig> has not only a significant margin over the control methods in detecting long-distance ppis but also maintains a high roc scores of  <dig>   and  <dig>   comparable to that of all ppis. in contrast, in both figs.  <dig> and  <dig>  rlgtn performs poorly and worse than rlew-k, which means the traditional rl kernel based on adjacent training network alone cannot detect faraway interactions well.
fig.  <dig> yeast: roc curves of predicting gtt>3) by rlgtn∼ <dig>  r
l
wolp-k- <dig> and r
l
ew-k


fig.  <dig> human: roc curves of predicting gtt>3) by rlgtn∼ <dig>  r
l
wolp-k- <dig> and r
l
ew-k




detection of interacting pairs for disconnected ppi networks
for the originally disconnected yeast ppi network, we randomly divide the edge set e into training edge set gtn with  <dig> edges and testing edge set gtt with  <dig>  edges. similarity, based on a random division, the number of edges of training edge set gtn and testing edge set gtt are  <dig> and  <dig> for the originally disconnected human ppi network. the detailed information of the originally disconnected yeast and human ppi networks can be found in the subsection of data description. the figs.  <dig> and  <dig> show the predicting results of yeast and human data respectively, which indicate rlwolp-k-i, i= <dig> , <dig> perform steady well on inferring interactions for both yeast and human data and are obviously better than rlew-k. rlgtn is not included in this comparison, because it is not feasible for prediction tasks of disconnected ppi networks.
fig.  <dig> yeast: roc curves of predicting g
tt∼ <dig> by r
l
wolp-k- <dig> and r
l
ew-k for disconnected ppi network

fig.  <dig> human: roc curves of predicting g
tt∼ <dig> by r
l
wolp-k- <dig> and r
l
ew-k for disconnected ppi network



analysis of weights
as our method incorporates multiple heterogeneous data, it can be insightful to inspect the final optimal weights. therefore, we compare the average of weights learned by wolp to the average of weights learned from revised abc-dep sampling method  <cit> , which is more computationally demanding. for the yeast data, the fig.  <dig> shows that these two methods produce consistent results: these weights indicate that ksn and kpfam are the predominant contributors to ppi prediction. this observation is consistent with the intuition that proteins interact via interfaces made of conserved domains  <cit> , and ppi interactions can be classified based on their domain families and domains from the same family tend to interact . for the human data, due to the extreme sparsity of the human ppi network, limited golden standard interactions can be included in the validation set to help optimize weights, which makes the weight optimization problem more challenging, especially for the sampling method. although the result of human data that shown in fig.  <dig> is not good as that of the yeast data, these two methods also produce quite consistent distribution, and ksn is the most predominant contributor. although the true strength of our method lies in integrating multiple heterogeneous data for ppi network inference, the optimal weights can serve as a guidance to select most relevant features when time and resources are limited.
fig.  <dig> yeast: comparison of average weights learned by wolp and abc-dep sampling method

fig.  <dig> human: comparison of average weights learned by wolp and abc-dep sampling method



CONCLUSIONS
in this work we developed a novel and fast optimization method using linear programming to integrate multiple heterogeneous data for ppi inference problem. the proposed method, verified with synthetic data and tested with dip yeast ppi network and preppi high-confidence human ppi network, enables quick and accurate inference of ppi networks from topological and genomic feature kernels in an optimized integrative way. compared to the baseline , our wolp method achieved performance improvement in ppi prediction with over  <dig> % higher auc on yeast data and  <dig> % higher auc on human data, and this margin is maintained even when the control methods use a significantly larger training set. we also demonstrated that by integrating topological and genomic features into regularized laplacian kernel, the method avoids the short-range problem encountered by random-walk based methods – namely the inference becomes less reliable for nodes that are far from the start node of the random walk, and shows obvious improvements on predicting faraway interactions; the weights learned by our wolp are highly consistent with the weights learned by sampling based method, which can provide insights into the relations between ppis and various similarity features of protein pairs, thereby helping us make good use of these features. moreover, we further demonstrated those relations are also maintained when the golden standard network  scale up to the original ppi network that consists of disconnected components. that is to say, the weights learned based on the connected training subnetwork of the largest connected component can also help to detect interactions for the originally disconnected ppi networks effectively and accurately. as more features with respect to proteins are collected from various -omics studies, they can be used to characterize protein pairs in terms of feature kernels from different perspectives. thus we believe that our method can provide us a quick and accurate way to fuse various feature kernels from heterogeneous data, thereby improving ppi prediction.

declarations
publication of this article is funded by delaware inbre program, with grant from the national institute of general medical sciences-nigms  from the national institutes of health. this article has been published as part of bmc systems biology vol  <dig> suppl  <dig> 2016: selected articles from the ieee international conference on bioinformatics and biomedicine 2015: systems biology. the full contents of the supplement are available online at http://bmcsystbiol.biomedcentral.com/articles/supplements/volume-10-supplement- <dig> 

authors’ contributions
lh designed the algorithm and experiments, and performed all calculations and analyses. ll and chw aided in interpretation of the data and preparation of the manuscript. lh wrote the manuscript, ll and chw revised it. ll and chw conceived of this study. all authors have read and approved this manuscript.

competing interests
the authors declare that they have no competing interests.
