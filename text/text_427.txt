BACKGROUND
genome annotation systems provide utilities to identify genes in a new genome sequence, classify each gene according to its most likely functions, and predict biochemical pathways that may exist in the sequenced organism.  these systems have provided the foundation for many bioinformatics processes and analyses, but an in-depth understanding of the algorithms, data processing procedures and hardware requirements utilized is available only to a select few systems maintainers in large genome centers  <cit> .  the advent of next generation sequencing technologies enables individual labs to replicate the sequencing capacity traditionally found only at genome sequencing centers. the capacity to annotate and understand genome sequences  is not easily transferred because of the hardware costs , the bioinformatics expertise required to integrate hundreds of tools, and the expertise of manual annotators.  as systems biology integrates more sources of information, the complexity of annotation will continue to increase.  it is unlikely that even large centres will be able to handle the scope and complexity of what annotation is likely to become, ultimately a computational model of the living cell.

annotation systems such as rast  <cit> , the annotation services at jcvi  <cit> ,  have started to close this gap by providing annotation services online that are free and readily available to individual researchers.  however, it is still difficult to determine the quality, consistency, and interoperability of the outputs from these systems  <cit> .

technologies such as the distributed annotation system   <cit>  provide a good start on these challenges by providing an extensible markup language   <cit>  standard for the annotation data exchange.  this allows sharing data at the syntax level, however the data is not understandable to the machine at the semantic level. semantic interoperability is critical when combining data streams from multiple sources, or when providing different and useful views on data.  semantic interoperability is a critical first step in transforming data into usable knowledge.

recent advances in unambiguous knowledge representation vocabularies  provide promise for recording the underlying semantics in biological data.  gene ontology  provides a controlled set of terms for gene products  <cit> .  the open biological and biomedical ontologies initiative   <cit>  has been collecting a series of ontologies for biological and medical domains such as the sequence ontology  <cit> , and systems biology ontology  <cit> . the comparative data analysis ontology  <cit>  provides a set of concepts for biological comparisons such as alignment and phylogentic trees.  the biopax ontology  <cit>  provides a common framework for relating pathway datasets.  future data source integration will require the ability to tie data and analysis to these common unambiguous biological concept frameworks. more importantly, providing data interoperability and additional utility requires the frameworks be leveraged and connected to data stores under a common standard. 

the owl web ontology language  <cit>  and owl  <dig>  <cit>  are w3c recommended standards for the construction of the semantic web  <cit> .  owl allows data aggregation, integration and construction of a distributed system for automated biological knowledge discovery.  protocols for sharing biological analysis, such as sswap  <cit> , are emerging.  however, many biological ontologies predate the owl standard.  some converters exist for transforming ontologies to the owl standard  <cit> , however not all attributes are easily transferable.  consequently, third party tools link to ontologies in unstructured ways.  there is no guarantee that the go terms referenced are logically consistent with one another or that the reference is consistent with the programmatic model used in the annotation system or with the semantic model intended by the go maintainers  <cit> .  

in the long term, new annotation systems must be built that are more understandable, accessible, and modifiable by biological experts.  the new systems must decouple data from the hardware and algorithms used for analysis.  the first step in this process is robust standards  for sharing information across systems.  the second step is the development of ontologies to share semantic meaning across systems.  both are well underway.  the development of a framework for linking traditional data to owl based ontologies within the context of genome annotation is the next step.  described here is a proof-of-principle that accomplished this next step through an analysis of the data storage resources used in conventional annotation systems, prototyping wrappers that convert the inputs and outputs of algorithms into resource description framework statements , prototyping storage solutions to store and query rdf statement repositories, and prototyping the conversion process from rdf statements into traditional standards.

RESULTS
overview
the oak ridge national lab  genome annotation and analysis  system is a complex software system developed over  <dig> years. in the orgaa annotation system, pipelines are used to predict the locations of genes and functional rna, assign functions to proteins, find functional protein domains, find repeats, characterize transporters and regulators, assign enzyme commission numbers , and create links to external databases.  a high level overview of orgaa is shown in figure  <dig>  

when we refer to a pipeline, we mean software systems that are implemented using the pipes and filters architecture  <cit> .  a pipeline consists of  records – elements for storing data,  filters – elements for transforming data,  sources – a special type of filter that reads data from files or databases,  wrappers – software components that control the input and output to programs and program execution , and  pipelines – software components that can be recursively assembled to construct larger systems.  a wrapper is also a pipeline.  pipelines are stateless, and chained together in linear stages of execution.  architecture patterns such as the pipes and filters architecture are used to manage the complexity of software systems. 

the ornl annotation pipeline shares characteristics with the model organism databases that use genetic model organism database project components   <cit>  to organize data.  however, it is different from systems focused primarily on eukaryotes because the annotators at ornl have embedded the experience from annotating over  <dig> microbes.  consequently, orgaa is more accurately characterized as a general-purpose microbial annotation system.  

a new data representation has been developed herein for use in microbial genome annotation built on advancements in semantic web technology.  a microbial annotation system consists of algorithms for predicting genomic structure, functional features and creating relationships across databases, storage for algorithm results and monitoring and reporting subsystems.  

our results and methods focus on: 

 comparing the features of semantic web storage technologies to traditional annotation methods, 

 discussing current limitations in data representation, 

 illustrating how semantic web technologies can be used along side of conventional storage technologies, and 

 illustrating how wrappers can be used to reformat the input and output of programs to produce data consistent with the semantic web and thus gain abilities not found in conventional systems.  

this provides a demonstration that a genome annotation system can be built on top of semantic web technologies and that these representations provide a layer of abstraction that has specific advantages in software development and are a natural fit for expressing the scientific concepts of annotation.

we intend to use the following terms from the emerging field of information and data quality to illustrate the advantages of this approach:

• correctness:  data is incorrect, when it contains logically inconsistent information or inconsistent formatting that leads to inconsistent results or causes downstream software processes to exhibit unexpected behaviour.

• normalization: data that is not normalized contains undesirable properties most notably, data modification abnormalities that lead to loss of data integrity .

• completeness:  a portion of the knowledge representation, a, is not complete if there exists accessible relevant information, b, that is disjoint from a.

overview of annotation data storage technologies
annotation systems are built using several different data representations.  most common are tab delimited file formats such as gene feature format   <cit> .  tab delimited file formats are easy to parse and easy to use in third party tools such as microsoft excel, however, column definitions are usually documented separately.  free text or semi-structured files such as genbank or embl files  <cit>  are advantageous because they are the current standard for warehousing annotations and are human readable. however, this type of semi-structured data is resistant to change, because small changes in the standard tend to break parsers and legacy code.  consequently, fields such as /note tend to get overloaded with useful information while product descriptions and gene names vary across annotation systems.  these variations make it extremely challenging to build general tools that are reliable.  relational databases are flexible, robust, have a strong theoretical foundation and favourable runtime characteristics.  however schemas implemented in relational database systems tend to be complex and are usually owned by system maintainers.  it is not easy to implement a relational schema for every concept in biology because the focus of relational technology is on storage and retrieval of information and not on describing the information stored.  also, it is difficult for domain experts  to construct and share relational schemas.

other technical solutions are optimal for sharing data and performing queries across data repositories.  xml provides a viable alternative for sharing data across sites, but is difficult for biologists to read.  a more complete overview of relative advantages and disadvantages of data storage and exchange technologies is shown in table  <dig> 

free text is used in repositories such as scientific journals.  tab/line delimited files are used in popular formats such as fasta, gff, and blast.  tab/line delimited files also constitutes the bulk of program output from most bioinformatics software.  mature tools and sequence repositories such as genbank support xml output.  many owl based ontology repositories exist for semantic data integration, however rdf/xml data is currently scarce. relational databases typically do not provide direct access to the data, instead a programming interface is provided for access to the underlying database.  free text is the most flexible, and also the least machine-readable.  relational databases are the most formal structures with the fastest indexing and searching capabilities. relational technology requires the greatest computational expertise investment while free text is the most natural.  xml and rdf/xml are designed for modification over time and in sharing data.   in the rows discussing search speed and update speed, o, o, p and np-hard are computer science terms indicating a range of how fast a computer solution can be obtained to a particular problem.  p indicates a reasonable solution is possible in polynomial time, np-hard means that the solution space explodes relative to the input size.  np-hard problems are expected to not be solvable on a computer in reasonable time. o, o, and p are all solvable efficiently on a computer.  in the rows discussing conversion to and from rdf/xml, turtle, and other semantic aware data storage technologies, loss of information includes schema, constraints, data and formatting.  for example, to convert from a relational schema to tab-delimited files, information is lost because the schema, triggers and views are not representable using tab-delimited files.  so these columns are more than just data, they are data and descriptions surrounding the data for making logical conclusions and for executing computer codes in reasonable time.  in the conversion from free text to semantic standards, assumptions and domain expertise may be lost. 

*assuming all information is in one file.  if multiple files exist, then an indexing system needs to be developed to organize information.

**relational databases are assumed to exist as a single installation on a powerful single resource.  new database technologies have lessened this restriction in recent years.

***cwa – closed world assumption, missing information treated as false. owa – open world assumption, missing information treated as unknown.

****una unique name assumption – each individual has a single unique name.

a fundamental constraint of current annotation systems is that they are constructed with the explicit goal of submitting a sequence to genbank/embl/ddbj, the international nucleotide sequence databases   <cit> .  data warehousing has very different objectives than systems biology research and experimental data integration.  in practical terms, the objective of data submission to the insd has resulted in sequence annotation systems that:  are based on text files that are computationally difficult to manipulate, and  cannot easily accept the expert knowledge possessed at the time of annotation, and  are difficult to merge with other datasets.  

relational databases, free text files, tab-delimited files and xml have been adopted in annotation systems because each has desirable attributes.  tab delimited files are easy to build perl scripts around for rapid prototyping.  relational technology is better suited to create robust and fast production systems with flexible query capability.  next generation data models for biological annotation complement more traditional data storage techniques because they also provide a semantic layer for interoperability. 

rdf/xml and owl have the potential to revolutionize annotation, because they share many of the favorable conventional storage techniques attributes, but also have new capabilities to represent data provenance and workflows, provide semantic data interoperability, allow for greater data transparency , and semantic consistency across systems.  these attributes will make it easier to store information that would otherwise be lost, create views on data , and perform logical inferences.

limitations to conventional representation technologies
as the annotation process proceeds, when additional algorithms are run, when databases are searched and results linked to the query, or when additional data is added to the system, information is created.  the order in which steps are allowed to occur and how the results from each of these steps are stored has significant impact on system behavior.  

data stored in the system can be out of sync with data stored in other parts of the system.  as a simple example, consider updating a blast database.  a blast database is formatted from a collection of fasta records.  blast scoring and results are dependent on the number and types or records in the fasta file.  results  prior to the update has the potential to be different than those obtained after the update.  genomes annotated years ago commonly contain functional and positional annotation that was based on the best knowledge at the time but would be very different if the genome were annotated today.

consider the following excerpts from two annotation files downloaded from genbank representing results for two different annotation systems for the same gene in two closely related strains of escherichia  coli:

/gene="dnaa"

/locus_tag="b3702"

/gene_synonym="eck3694"

/gene_synonym="jw3679"

/function="putative regulator; dna - replication, repair,

restriction/modification"

/note="dna biosynthesis; initiation of chromosome

replication; can be transcription regulator;

go_component: go: <dig> - cytoplasm;

go_process: go: <dig> - dna-dependent dna replication"

/codon_start=1

/transl_table=11

/product="chromosomal replication initiator protein dnaa, dna-binding transcriptional dual regulator"

/protein_id="np_ <dig> "

/db_xref="gi:16131570"

/db_xref="asap:abe-0012103"

/db_xref="uniprotkb/swiss-prot:p03004"

/db_xref="ecocyc:eg10235"

/db_xref="ecogene:eg10235"

example 1: dnaa from the annotation of escherichia coli k <dig> substr. mg1655

/locus_tag="ecdh1_0001"

/product="chromosomal replication initiator protein dnaa" 

/inference="protein motif:tfam:tigr00362"

/note="tigrfam: chromosomal replication initiator protein dnaa" /note="pfam: chromosomal replication initiator dnaa domain; chromosomal replication initiator dnaa" 

/note="smart: aaa atpase" 

/note="sptr: a1ahn <dig> chromosomal replication initiator protein dnaa" /note="kegg: ssn:sson_ <dig> chromosomal replication 

initiation protein  " 

/note="cogs: cog <dig> atpase involved in dna replication initiation" /note="interpro ipr001957:ipr003593:ipr013159:ipr013317" /codon_start= <dig> 

/transl_table= <dig> 

example 2: dnaa from the annotation of escherichia coli dh <dig> atcc 33849

these annotations are from orthologous coding sequences for dnaa from different strains of e. coli.  they will be used to illustrate several known quality deficiencies in insd formatted files.

 <dig>  omission errors. these are a violation of the quality characteristic of completeness. information available to the annotator at the moment of annotation is not stored in the insd file, or it is stored in a way not understandable to other organizations . another type of omission is when dataset fields are sparsely populated, it may be possible to use an insd tag, but it is not used because the insd submission validation script causes an error, because the feature/qualifier is poorly documented, or because the annotation system does not support the functionality.  in the above example, not all tags are shared across annotations .

 <dig>  version control errors: these are violations of the quality characteristic of correctness. under current insd policy it is the responsibility of the scientist who submitted the annotation originally to maintain and update the annotation.  however, usually there is no incentive for updating annotation with small fixes that do not lead to  significant publications or other professional rewards.  consequently many insd annotations are out of sync with data in other databases or with other features unless the annotation is redone.

 <dig>  formatting inconsistencies. these are violations of the quality characteristic of completeness. often two communities develop terminology for the same thing independently.  these divergent syntaxes will both pass the insd validation script.  however it will not be possible for a computer to determine that two fields mean the same thing.  also, annotation systems may produce different syntaxes for the same biological concept over time.  in the above example ‘/note="sptr: a1ahn <dig> chromosomal replication initiator protein dnaa"’ indicates an external reference to dnaa_ecoli  in swisspro/uniprot/tremble as does ‘/db_xref="uniprotkb/swiss-prot:p03004".  another example is the slight inconsistency between the content in the  /product lines.

 <dig>  nonnormalized data. these errors violate the principles of normalization, which attempt to reduce redundant data. some datasets have repeated fields and superfluously duplicated field values derived from other sources.  an example is ‘go_process: go: <dig> - dna-dependent dna replication’.  here  ‘dna-dependent dna replication’ is redundant data that is copied into the insd file from go for the purpose of improving readability.  updating this information in the insd file will cause it to be out of sync with go.  updates to the term in go will cause it to be out of sync with what is in the insd file unless both locations are simultaneously updated which usually is overlooked.

 <dig>  inconsistent data granularity. these are violations of the quality characteristics of consistency and correctness.  notes often contain many important bits of information in free text that are difficult to parse.  for example, /note="cogs: cog <dig> atpase involved in dna replication initiation", contains a database reference , a database id  and expert generated free text.  fields in other records contain only a database reference.

 <dig>  incorrectly labeled records. these are also violations of the quality characteristic of correctness. an example is a gene labeled with an incorrect function, or a group of genes with known function labeled as hypothetical.  as more individual scientists are allowed to annotate their own genomes it will become more difficult to regulate the usage of tags.  it will become harder to define usage, and meaning. this indicates a need for more education and training on information management skills, or an approach that allows more computer automation.  

 <dig>  poorly structured data. these are violations of correct metadata description and relevance. in the example above, both annotations contain tags that are in free text.  this can not easily be compared by a computer.  each annotation uses the /note tag differently.  in the first, /note tags are used to indicate function and to create unstructured links to gene ontology terms.  in the second, /note tags are used primarily to reference functional descriptions in other databases.  in both cases, bioinformaticians must build custom parser rules to fully take advantage of the information.  these parsers will not work on all of the files in the insd, only those files formatted by the same annotation system.  in other descriptions, comparing text on the computer is nearly impossible.  both usages illustrate qualifier tag overloading.

 <dig>  overloading of field function.  overloading, or using for more than one purpose, the function of qualifier tags  in the database structure violates the quality characteristic of correctness.  it is often difficult to untangle the current function of a field if it has been used for many different functions.

 <dig>  overly rigid and inflexible standards:  this makes adoption difficult. the three insd collaborators must agree on common tags, which results in a slow rate of change.  annotation systems have much more information available that is currently lost in the process of creating the insd file.  computational systems also have access to the series of events that created each of the qualifiers in the end file.  currently, annotation systems are not allowed to use additional information or make information easier to parse with computers because the standard is based on consensus.  the result is that useful information is lost in the process of generating a genome annotation.  some of this information is placed on web sites and is not stored in a machine readable way.

semantic web technologies can help resolve some of the inconsistencies between these two representations by linking the representations from multiple annotation groups into a single data representation.  this single representation could also contain information to record the history of information, and thus uncover the methods for obtaining the information.

the semantic web technology stack applied to annotation
it is proposed here that next generation annotation systems should adhere to the following objectives:  create less burdensome software requirements for downstream data users,  provide a data model that is logically consistent with conventional methods,  allow data integration from multiple sources, programs, groups or locations on the web,  provide for a better understanding of how data is created,  allow for a local level of standards that exceeds consensus standards, and thus permit technology advancement, and  not create data loss or loss of functionality during data aggregation.  meeting these objectives leads to an alternative model of genome annotation where each process adds additional information to the system, or creates links between information in the system or across systems.  rdf/xml and owl are technologies designed to build systems of this type.  the owl web language  is a specification for formally describing concepts and relationships between concepts.  this can be applied to the biological concepts or components of a software system.  rdf/xml  <cit> is a method for describing links from one data instances to another or from a data instance to a term  in an ontology.

owl contains three constructs for describing data, the individual, the class, and properties.  an individual, or data instance is an element of data in the data repository.  a sequence is an example individual   each individual has at least one class.aclass is a set of individuals that share some property.  for example both 

rdf/xml and owl are built using triples.  each triple contains a subject , a predicate  and an object.  subjects, predicates and objects are all uniform resource identifiers   <cit> . rdf/xml provides the specification for linking data instances to concepts  in owl ontologies.  rdf triples can be used to link data instances to xml datatypes , data instances to individuals ,  properties to other properties , and data instances to classes .  rdf triples allow for at least two important capabilities:  they allow constraints to be placed on data instances so that data is well formulated and consistent, and  they allow logical reasoning software to traverse ontology relationship hierarchies and thus answer complex questions.

in description logic terminology there is a fundamental modelling concept refer to as an axiom which is a logical statement that relate roles or concepts.  the set of such axioms that define concepts or terminology are referred to as a tbox, and the set of axioms that define assertions or data are referred to as an abox.  a knowledge base consists of the combination of a tbox and abox which can be served with rdf/xml being used as the abox, and owl being used as the tbox  <cit> . annotation knowledgebase systems can be constructed such that they require that each algorithm and database search create rdf statements describing the provenance of data modifications and link discoveries in a semantically consistent way.

a knowledgebase constructed using owl and rdf/xml has several advantages over conventional data representations:  

 <dig>  semantics are explicitly stored in the system, making data unambiguous.

 <dig>  because data is based on xml, data is easily stored and merged across parts of the system or across systems maintained by different groups.

 <dig>  data is structured and can be queried in much the same way as in a relational database .

 <dig>  standard algorithms  can be used to analyze, query data repositories and make logical extensions to existing knowledge.

 <dig>  data repositories of rdf/xml are inherently distributed over the web and thus processed in clustered computing environments and cloud computing.  

these advantages have yet to be realized because ontologies are integrated into data repositories in an ad-hoc manner.  in a semantic-aware knowledgebase system, semantic operations are not difficult to recover because they are imbedded in a data structure and not stored as free text, or embedded in programming logic. 

below are some examples of how rdf/xml can be used to solve each of the problems with insd files identified above.

 <dig>  omission errors. many omission errors of the type identified above can be corrected by linking annotations from different annotation groups.  take the collection of all proteins with a significant alignment to dnaa from e. coli k <dig> as an example. rdf/xml is designed in such a way that it is possible to recover every qualifier that was ever associated to any of the proteins in this set.  therefore omission errors can be greatly reduced by leveraging the strengths and experiences of different research groups and combining complementary datasets.  simply mashing the tags together does not solve this because that does not constrain or classify tags, so there is no way to know how tags are related.  this would eventually result in a large set of redundant tags for each gene with no good method for filtering them.

 <dig>  version control errors: rdf/xml is a web-based technology; it is not a static warehousing based technology.  therefore, when an update happens on part of the data in the system, all of the records in the system reflect the change.  at any given moment, it is possible to construct an equivalent to the annotation file because it is just a view on the rdf data and not a flat file. 

 <dig>  formatting inconsistencies:  rdf allows simple formatting inconsistencies such as the one described above to be resolved by querying the tag collection of highly related sequences and determining co-occurrence of terms.  other formatting inconsistencies can be handled by associating explicit formatting rules to owl classes and deprecating the use of free text in descriptions while enforcing the use of properly formatted data instances that belong to owl classes.

 <dig>  nonnormalized data.  rdf allows data to be normalized by explicitly removing redundant triples. 

 <dig>  inconsistent data granularity. rdf requires that each relationship stated in a predicate links only two things.  concepts linked in this way between well structured classes will not have multiple data instances encoded in a single relationship, unless users violate best practices.  

 <dig>  incorrectly labeled records.  because each instance must conform to a class, incorrectly labeled records can be identified by analyzing property labels of the same type in instances from superclasses/subclasses.  for example, imagine an instance of dnaa is incorrectly labeled as hypothetical.  an analysis of the associated go classes of every near neighbor could be used to identify the inconsistency.  this sort of query is extremely labor intensive when ontology references are encoded as free text or not present.

 <dig>  poorly structured data. regardless of the annotation system or scientific group providing the data, if data is structured such that each instance must belong to a class, and that class is organized in an ontology, then data instances can be formatted to conform to the requirements of a class.

 <dig>  overloading of field function.  rdf and owl allow constraints to be placed on fields such that only instances of a particular type are allowed.  instead of containing free text descriptions, tags can reference instances belonging to a specific class.  because these instances belong to a specific class, it is possible to search properties that relate instances belonging to a particular type.  therefore all known usages of a properties can be organized and described and new properties can be added in a logically consistent manner.

 <dig>  overly rigid and inflexible standards: instead of focusing on a globally accepted lowest common denominator standard, rdf and owl allow individual system maintainers to focus on standards for solving particular problems.  this allows software systems to evolve at a different rate from data warehouses.

example of rdf/xml applied to annotation
xml has not been widely adopted by the bioinformatics community, perhaps because it is not human readable and not easily understood by domain expert biologists.  here, we present turtle  <cit>  showing how a human readable semantic-aware standard can be constructed as an alternative to standard genbank files to overcome this limitation.  

gene    <dig> .18655

/gene="nhaa"

/gene_synonym="eck0020"

/gene_synonym="ant"

/gene_synonym="anta"

/gene_synonym="jw0018"

/locus_tag="b0019"

cds    <dig> .18655

/codon_start=1

/transl_table=11

/gene="nhaa"

/gene_synonym="eck0020"

/gene_synonym="ant"

/gene_synonym="anta"

/gene_synonym="jw0018"

/locus_tag="b0019"

/product="sodium-proton antiporter"

/function="transport; transport of small molecules:

cations"

/note="na+/h antiporter, ph dependent; go_component:

go: <dig> - peptidoglycan-based cell wall; go_component:

go: <dig> - organelle inner membrane; go_process:

go: <dig> - response to ph"

/db_xref="goa:p13738"

/db_xref="interpro:ipr004670"

/db_xref="pdb:1zcd"

/db_xref="uniprotkb/swiss-prot:p13738"

/experiment="n-terminus verified by edman degradation: pmid

8381959"

/protein_id="aac <dig> "

/translation="mkhlhrffssdasggiiliiaailamimansgatsgwyhdfletp

vqlrvgsleinknmllwindalmavffllvglevkrelmqgslaslrqaafpviaaigg

mivpallylafnyadpitregwaipaatdiafalgvlallgsrvplalkiflmalaiid

dlgaiiiialfytndlsmaslgvaavaiavlavlnlcgarrtgvyilvgvvlwtavlks

gvhatlagvivgffiplkekhgrspakrlehvlhpwvaylilplfafanagvslqgvtl

dgltsilplgiiaglligkplgislfcwlalrlklahlpegttyqqimvvgilcgigft

msifiaslafgsvdpelinwaklgilvgsissavigyswlrvrlrpsv"

example 3: the gene nhaa as it is stored in a traditional genbank file.

below is a turtle representation of the same record.  this example contains three subjects, gene:example_gene, cds:examplecds, and experiment:exampleexperiment.  

@prefix an:  <http://compbio.ornl.gov/annotate.owl#> .

gene:example_gene

        a:hasstart integer: <dig>  ;

        a:hasend   integer: <dig>  ;

        a:hasstrand strand:+      ;

        a:obtainedfrom prodigal v <dig> 

        a:hasgenename string:nhaa ;

        a:hasgenesynonym string:eck <dig> ;

        a:hasgenesynonym string:ant ;

        a:hasgenesynonym string:anta ;

        a:hasgenesynonym string:jw <dig> ;

        a:hasgenesynonym string:sof ;

        a:haslocustag string:b <dig> .

cds:examplecds

        a:hasgene gene:example_gene ;

        a:codon_start integer: <dig>   ;

        a:transl_table integer: <dig> ;

        a:haslocustag string:b <dig> ;

        a:hasproductdescription string:sodium-proton antiporter ;

        a:hasfunctiondescription string:transport; transport of small 

   molecules: cations ;

        a:note string: na+/h antiporter, ph dependent ;

        a:hasgo_component go:0009274;

        go: <dig> a:hasdescription peptidoglycan-based cell wall;

        a:hasgo_component go:0019866;

        go: <dig> a:hasdescription organelle inner membrane ;

        a:hasgo_process go:0009268;

        go: <dig> a:hasdescription response to ph ;

        a:hasdb_xref goa:p <dig>  ;

        a:hasdb_xref interpro:ipr <dig> ;

        a:hasdb_xref pdb:1zcd ;

        a:hasdb_xref uniprotkb/swiss-prot:p <dig> ;

        a:hasexperiment experiment:exampleexperiment ;

        a:protein_id="aac <dig> " ;

        a:hastranslation


vqlrvgsleinknmllwindalmavffllvglevkrelmqgslaslrqaafpviaaigg

mivpallylafnyadpitregwaipaatdiafalgvlallgsrvplalkiflmalaiid

dlgaiiiialfytndlsmaslgvaavaiavlavlnlcgarrtgvyilvgvvlwtavlks

gvhatlagvivgffiplkekhgrspakrlehvlhpwvaylilplfafanagvslqgvtl

dgltsilplgiiaglligkplgislfcwlalrlklahlpegttyqqimvvgilcgigft

msifiaslafgsvdpelinwaklgilvgsissavigyswlrvrlrpsv"""  .

experiment:exampleexperiment

        a:eve string:n-terminus verified by edman degradation ;


        a:hasdb_xref pmid: <dig> .

example 4: the gene nhaa as it is stored in turtle, a rdf/xml equivalent.

the primary objective of this example is to illustrate how an rdf/xml representation can be displayed in a human readable way that is roughly equivalent to an insd file; it may not be immediately obvious to the reader that a format conversion of this type is something significant.  however, it is significant in at least the following ways:  in it’s potential for correcting data representation errors such as omission errors, version control errors, formatting inconsistencies, nonnormalized data, inconsistent data granularity, poorly structured data, and overloading of field function,  in it’s capabilities for allowing for software and data abstraction and in alleviating overly rigid and inflexible standards,  in that constraints can now be placed on fields because well formed fields must be linked to terms in an ontology.

in the above example, the domain and range of each predicate is constrained such that each data instance is a member of a class and not just considered as free text.  in this example, gene:example_gene and cds:examplecds are linked through the relationship cds:examplecds a:hasgene gene:example_gene.  the predicateannotate:hasgene requires that the object referenced is of type gene.  these types of relationships can not be encoded in the insd representation because tags in the insd file are not ontology terms.  higher level concepts such as what an a:gene is, and constraints on this data type are defined in the ornl annotation owl ontology, and, in this case, mapped to the ‘gene’ class in the sequence ontology .  constraints on the predicates in the example also exist in the annotation ontology.  

these constraints make it possible to normalize data, for example each gene_synonym in the genbank example is repeated for both the cds and the gene when in reality they only apply to the gene.  in rdf, because the cds is associated with the gene, each synonym for the gene can also be associated with the cds through the rdf graph.  

omission errors can also be handled with this approach because rdf/xml allows us to merge data from any other data sources on the web such as kegg  <cit>  that already have rdf/xml accessible data  <cit> . this allows everything known about nhaa in any dataset to be linked to the annotation.  

problems in the genbank record with formatting inconsistencies, poorly structured data, inconsistent data granularity, and overloading of field function are all addressed in the turtle representation to a much greater degree.  the best example of this is the turtle alternative to the /note tag.  instead of a free text description with go identifiers sprinkled throughout, the turtle representation illustrates hard links to go classes and free text descriptions of those terms.  this representation is easier to compute because the go classes are easily distinguishable from free text by the computer. another example is that, in the insd file, experimental validation is represented in free text.  in the turtle representation annotators can represent both an evidence code that can be classified and placed in an evidence ontology   <cit> , and they can link the data entry to the evidence method.  it is clear that this data representation scheme is better at handling the traditional problem of data loss than the insd standard because statements do not have to pass a central regulatory body to be placed in the system.  statements that would have been lost in the past can each be encoded relative to a local ontology.  at a later date, the meaning of these statements can be mapped to other ontologies as they develop, or the local ontology can be shared and become a standard.

rdf also addresses version control and normalization errors.  the turtle example above is actually an rdf result from a sparql query.  this separates the way the data is stored from the way the data is viewed.  in the case of the insd file, the file contains the data.  this leads to data replication and loss of data integrity.  a much better approach is to have a file that contains references to other sources of information, as the referenced information improves, the annotation also improves.  implementing a data cache can resolve performance issues.

the rdf example also imports relevant mapping to other ontologies .  this is done via importing the ornl annotation ontology.  concepts in the ornl annotation ontology such as programs and databases can be defined locally independent of a central standards body.  an example of this is a:obtainedfrom prodigal v <dig> . this states that the region was obtained using the gene prediction program prodigal.  the classification hierarchy that places prodigal as a gene prediction program is stored in the ornl annotation ontology, and is not deposited in insd. this allows concepts to be declared and used rapidly local to the annotation system.  these concepts allow for a greater degree of abstraction.  tool developers and data curators can declare concepts of importance to the task at hand without the burden of understanding the structure and function of annotation systems.

discussion
in the past, most work in bioinformatics focused on algorithms; however as more high-throughput experimental techniques become available, managing data complexity is an ever increasing challenge.  bioinformatics systems are growing so quickly that it is no longer possible for a single source to fund data collection and analysis.  better methods are needed to increase data quality and interoperability.  advances can come from utilities that allow faster access, more intuitive access, more robust and sharable data structures, or the ability to provide new, increasingly detailed descriptions of data that are precise and machine readable even if manually contributed.  rdf/xml and owl can be used in each of these ways, and have great potential to change the way data modelling is performed.  

it is important to recognize that rdf/xml, owl, and other semantic web technologies are complementary to traditional database technology, xml, programming languages  and other current techniques.  resources built on these technologies need not be abandoned for the successful implementation of the semantic web; rather it is important that traditional technologies are linked in a logically consistent manner.  if this can be achieved, the semantic web will enable better software engineering practices, easier evaluation of system performance characteristics, and most importantly, a better understanding of the software processes and data used in systems biology.

an illustrative example of some of these advantages in annotation can be found in the current propagation of low quality product assignments.  consider a protein sequence, p.  assume that p has been determined biochemically to have a known function, f.  annotation systems commonly copy the functional properties of p to putative proteins based on a sequence similarity cut-off.  imagine that an annotation system encounters a hypothetical protein, hp <dig>  that’s closest match is p and assigns f to hp <dig>   now hp <dig> is forever associated with f in insd.  at a later date, another sequence annotation system performs a search for another hypothetical protein, hp <dig> and assigns f as its function because the nearest neighbor to hp <dig> is hp <dig>   this process continues until eventually a hypothetical protein, hpx which may have a function other than f is assigned the function f.  semantic annotation allows more than the association of evidence codes to each annotation.  it also allows a trail of evidence  to be constructed so that the methods and series of deductions leading to the functional assignment can be catalogued and ranked in a systematic way.  this allows for better characterization of algorithm performance and better understanding of systematic error.  more importantly, it allows such errors to be corrected.  it is possible that a system based on conventional storage technologies could implement this kind of data tracking in a local fashion; however, it is impossible for the inferences to be shared unless all users have nearly identical technology stacks.  the advantage of the semantic web approach is that it allows users with vastly different technology infrastructure to share and process inferences at a detailed level in a consistent and automated way.

CONCLUSIONS
we have presented a conceptual framework for constructing a description logic and web standard based knowledgebase for genome annotation systems.  the core components of this system were introduced along with examples that illustrate the implementation details and advantages.  some of the stated advantages include the ability to record a series of operations that are semantically consistent across groups and technologies, the ability to share data and semantics across research communities, greater data transparency , and logical consistency.

semantic web technology is still in its infancy and many important details need to be worked out to translate informatics research problems into production systems.  although there are currently many implementations of core utilities, we do not yet know how to build systems that meet all of the stated expectations.  large scale prototypes are needed to characterize performance issues and understand potential pitfalls.  

our approach to creating a semantic annotation system will meet these challenges, but many underlying details are still being considered.  combining multiple rdf data sources is a challenge because of the lack of available systems with such capabilities and researchers with sufficient expertise to build and demonstrate the benefits of such systems for biological research. when the web was first developed it was not immediately obvious how html formatting was better than text files.  over time, as more web pages came online, the advantage of hyperlinks for navigating content became apparent.  rdf/xml and owl create a standard for linking data.  this capability will increase in usefulness in a similar way as these methods are adopted and data accessibility increases.  the critical first step is to create rdf/xml data repositories that can be leveraged; annotation is a central resource in bioinformatics and is therefore an intuitive place to start.

for the most part, ontologies are not as useful as they could be because the analysis routines and data are not always linked in the most meaningful ways.  ontologies developed with data that references and uses them will be more accurate, more robust, simpler to understand and more elegant.

