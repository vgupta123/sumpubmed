BACKGROUND
the collection of technologies described as second- or next- generation sequencing  platforms are characterised by the synthesis of complementary strands of dna from clusters of homologous templates  <cit> . the chemistry used differs between the platforms but that for the life technologies corporation solid platform is particularly interesting since there is not a one-to-one correspondence between measurements made during sequencing and nucleotides of the sequence being read. instead the primary output of the solid platform is a ‘color sequence’, an encoded form of the nucleotide sequence, that has advantages for calling snps when comparing the reads to a reference  <cit> . with the advent of the  <dig> series of machines in november  <dig>  an improved ‘exact call chemistry’  was introduced that changes the way that the sequence is encoded by the platform and allows mistakes in the measurements to be corrected, hence producing more accurate reads  <cit> .

the solid sequencing chemistry consists of multiple rounds where probes, consisting of eight bases and a fluorophore, are sequentially ligated to the template sequence to build up a complementary strand. each round consists of a priming step followed by a repeated cycle of ligating probes to the template, exciting the fluorophores and imaging the resulting emission, then cleaving the flurophore and part of the probe ready for the next cycle. the probe/fluorophore combinations are designed so that the probes interrogate the first two of the eight ligated positions in the template, with each of four fluorophore colors used to indicate four of the  <dig> possible nucleotide pairs at these positions. the color of the fluorophore for each template is recorded and used later to determine the sequence of the read. after imaging, those templates to which a probe failed to ligate have their previous probe decapped  so they will not be extended on future cycles. this reduces problems analogous to ‘phasing’ on the illumina platform  <cit> . the fluorophore and last three bases are cleaved from the probe, leaving the strand ready to be further extended in the next cycle. the repeated ligation of eight additional bases and then cleaving the end three mean that the pair of bases of the template sequence that is interrogated moves on by five positions every cycle. after a specified number of cycles, the round is stopped and the complementary strand melted from the template to leave the template ready to be primed for the next round. each round uses a different primer so that the positions interrogated by the probes change each time; for example, on the first round the first position of the probes on cycles  <dig>   <dig>  … corresponds to positions  <dig>   <dig>  … of the template sequence; on round two these become positions  <dig>   <dig>  …, etc.

after five rounds, probes have been ligated so that every position of the template sequence has been the first position of a probe. the colors recorded and prior knowledge of one of the bases, typically the last adapter base adjacent to the template, are sufficient to determine the nucleotide sequence assuming that no errors have been made. conventionally, one of the rounds is primed so that it starts one base before the beginning of the template sequence and so the first base interrogated is the last base of a known adapter sequence.

the solid exact call chemistry  augments the ‘two-base-encoding’ chemistry with an extra round where a different set of probes is ligated. each cycle of this round interrogates positions  <dig>   <dig> and  <dig> of each five-nucleotide block of the template. the same four fluorophores are used, each now indicating the presence of one of  <dig> of the possible  <dig> combinations of nucleotides at the positions interrogated. these color calls can be used to detect and recover from miscalls made in previous rounds.

directly decoding the colors from the first five rounds into nucleotide sequence allows catastrophic failures to occur where a single calling error creates errors in every position from then on  <cit> . as well as allowing some errors to be corrected, the additional round also enables the calls to be translated into nucleotide sequence in a manner that behaves more gracefully in the presence of mistakes  <cit> .

the mathematical basis for the error correcting properties of the solid ecc is the theory of convolutional codes  <cit> , a class of codes which are incorporated into some of the most powerful error-correcting codes in general use, for example those used in the voyager deep-space exploration program  <cit> . convolutional codes are a category of linear codes  <cit>  and we use the theory of linear codes to analyse the error correcting properties of the solid exact call chemistry, describing the types of sequencing mistakes it can correct and those cases where the presence of an error can only be detected. for isolated mistakes that cannot be unambiguously corrected, we show that the type of substitution can be determined and its location can be narrowed down to two or three positions in the read, leading to a significant reduction in the the number of plausible alternative reads. if the read is ‘corrected’ at the wrong location, we show that the nucleotide read ultimately produced contains errors that have a distinctive pattern. finally, we apply the same techniques used to analyse the exact call chemistry to look at hypothetical alternative chemistries and show that some of them have superior characteristics, being able to correct more errors and inducing simpler patterns of error in the decoded nucleotide sequence when reads are miscorrected.

methods
convolutional code theory
we start by defining the terminology we need to analyze the solid code. the chemistry of the sequencing reactions and imaging mean that each fragment of dna, or ‘source sequence’ , is encoded into a ‘code sequence’  of equal or greater length. the original five rounds produce as many observed colors as there are nucleotides in the fragment; the ecc round adds one-fifth as many additional color calls. the code sequence is then observed with potential errors . since the code sequence is longer than the source sequence, not all possible code sequences correspond to an encoded source sequence; those that do correspond are termed ‘valid’. given an observed sequence, some procedure is used to find the closest valid code sequence  and the nucleotide sequence that produces this code sequence is the decoded sequence. note that the source sequence and decoded sequence are sequences of nucleotides , whereas the code, observed and corrected sequences are colors .

in terms of the theory to be presented, colors and nucleotides are just different representations of the same things, members of a set of four elements over which a finite field  is defined . to prevent confusion between the various matrices and vectors studied and their concrete representation as nucleotides or colors, all calculations in the text are described in terms of elements gf <dig> and the more usual notations are reserved for when an actual sequence of nucleotides or colors is meant.

0
1
2
3
for the purposes of the coding theory presented here, both nucleotides and colors represent elements of the galois field over four elements  and the correspondence between them is shown below. for example, the color ‘
2
’, the nucleotide ‘g’ and the element ‘α’ are considered to be equivalent for the purposes of calculation. a field consists of a set of elements and rules on how to add  and multiply  them together; the results of combining two elements are expressed by the cayley tables above; for example, α⊕β= <dig> and α⊗α=β. the standard rules for associativity and commutativity for multiplication and addition still apply in finite fields, and multiplication is still distributive over addition  <cit> . one notable difference from ordinary arithmetic is that all elements are self-invertible under addition in gf <dig>  so addition and subtraction are equivalent operations.

the ecc code, and any linear code in general, is defined in terms of a k×ngenerator matrix g with elements in gf <dig>  a source sequence s is a row vector of length k consisting of elements of gf <dig>  and its code sequence c is sg where the vector-matrix multiplication is understood in the usual sense but addition and multiplication of elements is carried out within gf <dig> . codes are termed linear because any linear combination of valid code sequences is also a valid code sequence: if 
s <dig> and 
s <dig> are source sequences with corresponding code sequences 
c1and 
c <dig>  and 
w <dig> and 
w <dig> are elements of gf <dig> then 
w1
c <dig> + 
w2
c2=g so c=
w1
c <dig> + 
w2
c <dig> is a valid code sequence corresponding to the source sequence 
w1
s <dig> + 
w2
s <dig> 

rather than expressing errors in terms of a specific color miscall, we consider types of errors defined by what happens when an element of gf <dig> is added to a call. for example, the error type ‘ + β’ transforms the color 
1
 into 
2
 since 1⊕β=α, and transforms 
0
 into 
3
 since 0⊕β=β. although we will be considering color miscalls in the coding sequence, it is instructive to examine the action of these error types on nucleotides to show that they can have a concrete interpretation: + β complements the nucleotide, + αresults in a transition, + <dig> preserves whether the nucleotide is an amino- or keto- acid  and + <dig> leaves the nucleotide unchanged. the group structure ensures that all possible substitutions can be encoded in this form and, when errors are rare, the sequence of error types will consist mostly of 0s. adding the sequence of error types to the observed sequence results in the recovery of the  code sequence; likewise, adding the error types to the code sequence results in the observed sequence.

each generator matrix has an associated parity check matrix h that can be used to determine whether a code sequence is valid. the parity check matrix uses a subset of the elements of observed sequence to call a putative decoded sequence and then calculates the expected value of the remaining ‘parity’ elements by treating the decoded sequence as true. the expected and observed parity sequences are then summed element-wise to create the parity check sequence. h is constructed so that the parity check sequence is the sequence-matrix product xh, where x is the observed sequence. by construction, the parity check of valid code sequence is a sequence of 0s; invalid code sequences have a non-zero parity check. when errors have occurred, the observed sequence can be expressed in terms of the code sequence for the source sequence and a sequence of error types e: x=c + e. since the parity check of a valid code sequence is a sequence of zeros, the parity check of the observed sequence only depends on the sequence of errors and not the source sequence as xh=h=eh. the value of eh=xh is termed the ‘syndrome’ of the error, a sequence in gf <dig> of length n−kthat partitions the set of possible errors into equivalence classes.

syndromes provide a simple and efficient method of decoding an observed sequence under the assumption that errors are rare and so the most plausible corrected sequence is the valid coding sequence which requires the fewest changes from the observed sequence. an error consisting of multiple changes may belong to the same equivalence class as one with fewer changes but the simpler error is the more likely and so is the best predictor of what error actually occurred. before decoding begins, a table is constructed mapping every syndrome  to the simplest possible sequence of error types that has it ; this can be done by enumerating all single errors, followed by doublets, triplets, etc. until every syndrome has been observed at least once and all error types up to a given complexity have been considered. the syndrome table is a function of the code structure only and can be calculated once and distributed with the probe sets. to decode each observed sequence x, its syndrome xh is calculated and then located in the table to find the simplest sequence of error types 
e
xh
 that could cause it. the sequence x + 
e
xh
is then a valid code sequence which can then be decoded. if there are several equally simple sequences of error types, then an error has been detected but the correction is ambiguous.

the ecc code is defined by the architecture shown in figure  <dig>  the encoding ‘machinery’ looks at windows of length five of the source sequence, moving along one element at a time, and performing the specified additions and multiplications. two streams of symbols are emitted. for the first stream, corresponding to the standard color encoding, the first two elements of the window are added together. for the second stream, the ecc encoding, only every fifth symbol is recorded, a practice known as puncturing or perforating. this stream multiplies the second and fourth elements of the window by β and sums them with the first element. in terms of the solid chemistry rounds, the color calls from each of the first five rounds correspond to every fifth element of the first stream with a different offset for each round and the calls from the final round correspond to the punctured second stream.
a <dig> 
a <dig> … is passed through the encoder, progressing one position at a time, and the color obtained from the additions and multiplications indicated is emitted from each stream. the top ‘color stream’ is that produced by the two-base-encoding chemistry and the bottom ‘solid ecc stream’ is punctured so that only every fifth color member of the sequence is used.

the calculation for both the first and second streams  can be expressed as the dot product a·
ρ
i
 where a is a row vector of the bases in the window and the 
ρ
i
 describe the calculations to be done, with 
ρ1= <dig> and 
ρ2=1β0β <dig>  the 
ρ
i
are the ‘probe generators’ for the chemistry since they define the color of fluorophore that each probe has attached to it: the probe that interrogates the sequence b has color b·
ρ
i
.

the architecture of the ecc code imposes the block structure shown in figure  <dig> on the source sequence. the read is partitioned into blocks of length five, each of which can be uniquely recovered from the indicated five elements of the encoded sequence. the remaining elements of the encoded sequence each span two blocks and are in effect parity colors that can be used to detect the presence of errors. ignoring the parity colors, the generator for each block is the invertible matrix 

  gblock=1  0  0  0  11  1  0  0β0  1  1  0  00  0  1  10  0  0  1  0gblock−1=0  1  1  1  1β  β  α  α  αβ  β  β  α  α0  0  0  0  11  1  1  1   <dig> 
b1i

b2i

b3i

b4i

b5i
, showing how the two-base-encoding and ecc color calls are split into five ‘data’ colors 
c1i

c2i

c3i

c4i

c5i
, from which the block can be called, and a ‘parity’ color  which straddles the block and its downstream neighbour. the data colors are used to determine the nucleotide sequence of the blocks and the parity color is used to detect whether an error has occurred. note that the data colors are a mixture of both color streams, with the parity color coming from the color stream of the code.

which can be used to freely convert between a sequence of five nucleotides and its encoding. for example, the nucleotide sequence acgat  has the color encoding 
13233
 since 
gblock=. conversely, gblock−1=.

after reordering the encoded sequence appropriately, the generator for the full code, 
gecc, can be expressed in terms of the generators for each of the blocks and an additional column for each of the parity colors: 

  gecc=gblock0…0gblock0…⋮0gblock⋱⋮⋱⋱u50…u1u5⋱0u1⋱⋮⋱⋱=g∗|p 

 where 
u
i
is a column vector with five elements consisting of 0s except for a  <dig> in the 
ithposition, with g* and p being defined appropriately. the dimensions of 
gecc are k×n, where k is the length of the read  and n the total number of color calls produced in both the two-base-encoding and ecc chemistry rounds.

the parity-check matrix, 
hecc, corresponding to generator 
gecc, is 

  hecc=g∗−100in−kpin−k 

 where 
i
m
 is the m×midentity matrix. by considering the action of this parity-check matrix on an encoded sequence, the calculation of the syndrome can be given concrete form: the first matrix of the factorised form of 
hecc takes the observed sequence and inverts it block by block to get a putative nucleotide decoding, with the values of the parity colors preserved. the second matrix calculates the parity colors for the putative decoding and adds them element-wise to the parity colors actually observed. the resulting sequence of length n−k is the syndrome. if the syndrome is composed entirely of 0s, i.e. the parity-check has been passed, then the code sequence and its putative nucleotide decoding are valid. if the syndrome has non-zero entries then an error has been detected.

when an error is known to have occurred but is ambiguous, it may be mistakenly ‘corrected’ by applying the wrong error type. the equivalence class contains the possible simple error types that could have caused the observed syndrome but only one of them is the one that actually occurred; applying any to the observed sequence will result in a valid code sequence. we can determine the pattern of nucleotide errors that mistaken correction will result in: if the observed sequence with  error 
e <dig> is c + 
e <dig> and the correction 
e2≠
e <dig> is applied, then converting back to a nucleotide sequence using the inverse of the block generator results in g− <dig>  the difference between the nucleotide translation of the corrected sequence and the correct nucleotide sequence is g− <dig> — the pattern of error induced. note that the pattern does not depend on the correct nucleotide sequence, but only on the two changes being considered. by examining all possible combination of error types belonging to an equivalence class, the full set of patterns that may occur can be determined. the pattern of error may be a single base change, or a more complicated multi-base change.

practical implementation
the theory described examines the worst case where an error can occur anywhere and there is no additional information about which site it is likely to have affected. real-life performance of the code depends on additional factors. firstly, the distribution of where errors occur is not uniform, depending for example on the chemical and physical characteristics of the sequencing process, and may not even be independent between positions; the performance of a code may change depending on the error profile. secondly, but related, the sequencing platform provides quality information in the form of phred scores  <cit>  that can be used to help locate the position where an error occurred. values quantifying the probability that a given call is wrong are known as ‘soft information’, compared to the ‘hard information’ of the observed sequence not being a valid encoding.

a convenient way to deal with these extra complications is to simulate encoded sequence under a realistic model of how errors occur and then decode using dynamic programming, on a ‘trellis’ graph that defines all possible decodings and their relative probabilities, to find the most probable call for each position of the decoded sequence  <cit> . in practice, we need to consider three classes of differences: those due to variants between sequence under study and the reference it is mapped against, those caused by mutations in the original sequence due to polymerase errors during sample preparation  and errors made calling the encoded sequence. the first type of difference is of interest and will affect many reads mapping to a single location. of the remaining two types of difference, the former occurs before the sequence is encoded, so encoding provides no protection and limits the maximum accuracy of the platform; errors of the latter class may be correctable.

in the absence of empirical data for how the distribution of calls and miscalls changes over probes and rounds, and how errors are correlated between positions, we have assumed that errors occur independently for every element of the code sequence and errors are picked uniformly from the three possibilities. the probability of an error for a particular round and cycle of ligation is taken to be equivalent to the quality score from a read sampled at random from a real set of data, implicitly assuming that the error characteristics for alternative probe sets  will be the same as for the ecc probe set. the simulation scheme is as follows: 

 <dig>  sample a fragment from genome .

 <dig>  mutate bases of fragment independently with equal probability .

 <dig>  encode mutated fragment .

 <dig>  sample qualities from a set of real data.

 <dig>  mutate the encoded sequence with probabilities defined by quality values.

this scheme outlined has an error model similar to that implicitly assumed when analysing real data since information about both alternative calls and correlation between calls has already been lost during processing into a color-space sequence with a single quality value for each position. the scheme does not simulate the occurrence of insertions and deletions but these are relatively rare compared to substitutions and are most likely to be due to errors introduced during sample preparation rather than calling errors.

RESULTS
if a read is error-free then it can be decoded unambiguously and the calls from the two-base-encoding chemistry and ecc codes will agree. when an error occurs in any of the two-base-encoding chemistry rounds, it translates into multiple base miscalls. the structure of the ecc code allows such errors to be detected, recovered from, and, in certain circumstances, corrected. the syndrome equivalence classes are defined by the types of error that can occur and so, by examining them, we can classify the cases where errors can be corrected unambiguously and those where additional information is needed to resolve the ambiguity.

since the elements of the syndrome consist solely of the summation of the observed and expected parity colors, there is a one-to-one correspondence between them and parity colors. an error in a block can only affect two elements of the syndrome, those corresponding to the two parity colors that overlap the block’s first and last elements. we refer to these as the upstream and downstream syndromes, respectively, so it is sufficient to concentrate on how errors occur in only one block. all statements we make about the error correcting properties of the encoding are on a per-block basis, so a code that can correct one error per block can correct multiple errors if spread between blocks. the final block only has a upstream syndrome and so has more limited error correction.

the value of the syndromes for all possible single-color errors that could occur in a block or its parity colors are shown in table  <dig>  for example, an error of + <dig> in the second color produces the syndrome βα, identical to that produced by an error of + <dig> in the third position. if a syndrome is unique  then, assuming only a single error has occurred in that block or parity color, that error can be determined and so corrected. note that while errors in parity color 
p−, overlapping a block and the previous one, appear to have unique syndromes, this parity color is also 
p +  for the previous block and so the syndrome can actually be caused by multiple different errors.


p
−


c1

c2

c3

c4

c5

p
 + 

c <dig> 
c <dig> 
c <dig> 
all syndromes caused by a single error in a block of five code letters  and two parity letters  of the solid ecc code. each row in the table corresponds to a specific type of error at the given position of the code word, with the ‘interpretation’ of an error type being the effect it would have when considered as a nucleotide substitution. the table entries are the values of the relevant elements of the syndrome, corresponding to the upstream and downstream parity checks for the block for each error type. the error type + <dig> is not shown since it represents no error; its syndromes would be  <dig> at all positions. the equivalence classes are listed separately and do not correspond to specific error types. note that 
p−is also 
p + for the preceding block.

the syndrome equivalence classes show the types of single-color error that cannot be disambiguated without additional information; a error has been detected but is not correctable. for example, the syndromes 0β, 0α and  <dig> can be generated by single errors at 
c <dig>  
c <dig> or 
p + , and so errors at these positions cannot be distinguished. the equivalence classes define the possible simple changes to the observed sequence that will produce a valid sequence that can be decoded into a string of bases, only one of which produces the correct sequence. when an error is wrongly corrected, the code sequence is changed in two places, the original error and the correction; the structure of the code ensures that the changes should be complementary .

using the inverse of the block generator matrix, the pattern of changes that are induced in base-space by miscorrections to the observed sequence can be derived. if errors occur and are miscorrected, such that the difference between the correct and observed values of the data colors is d, then the pattern of differences p induced in base-space is given by p=dgblock− <dig>  for example, an error of type + αat the second position might be erroneously corrected by an error of type + α at the third position, so the differences are 0αα <dig> and the pattern induced in base-space is 00α <dig> . if this occurred to the nucleotide sequence atgcg then the sequence atacg would result.

all patterns induced by single-color errors are listed in table 3; for example, an error of type + α at the first position may be wrongly corrected at either the fourth position or the parity color, leading to the triple error 0ααα <dig> or the quadruple error 0αααα, respectively. note that no pattern ever affects the first position of the block; the structure of the matrix gblock− <dig> shows that this position can only be changed by errors in the second, third or fifth positions: single-color errors at the fifth position can be always be unambiguously corrected, and errors at the second or third positions result in either corrections or changes that cancel at the first position.


c <dig> 

c <dig> 

c <dig> 

c <dig> 
all possible patterns of error caused in corrected base-space sequence by a single wrongly corrected error in the observed sequence of type +d . the ‘positions’ column indicates all possible pairs of positions at which an error can occur and be wrongfully corrected; it is not necessary to identify which member of the pair corresponds to which form of error as the resulting pattern is the same. in all cases the error pattern should be multiplied element-wise by the error type  to get the actual pattern .

one notable feature of these more complex errors is that the error type is the same at all affected positions, a property that might help distinguish sequencing errors from genuine variants if the reads are later mapped to a reference. by adding the mapped read to the reference in gf <dig>  the pattern should be evident if it was due to simple miscall. if the pattern is not evident, the differences are either due to sequence variants or multiple miscalls.

while the consideration of syndromes provides useful information about a code’s properties and syndrome decoding is computationally efficient, it is not a replacement for probabilistic methods of decoding since the latter incorporates the quality information about each call and can use it to make better decisions about correction. however, syndrome decoding techniques may still be of use in conjunction with the more computationally expensive probabilistic methods since the syndrome provides a quick test of whether the observed sequence is valid . syndrome equivalence classes could also be used to restrict the paths through a trellis to a plausible set, providing a heuristic to speed up the dynamic programming algorithm to find the most probable decoding.

theoretical bounds
we have shown the type of errors that the ecc code can correct but have not yet addressed whether it is optimal. before examining specific alternative codes, it is interesting to look at what can possibly be achieved and there are several mathematical results that restrict the performance of any code. firstly the hamming  <cit> , johnson  <cit>  and singleton  <cit>  bounds place an upper limit on the number of errors that a code can hope to detect or correct but codes meeting these bounds may not exist; the gilbert–varshamov bound  <cit>  is a lower limit on the performance of the best code that does exist.

for reads of  <dig> bases encoded into  <dig> letters , the lower bound guarantees that a code exists that can detect any two errors and correct any single error. the upper bounds show that no code can guarantee to correct more than three errors. for reads of length  <dig> bases encoded into  <dig> letters, then a code exists that can detect and correct any two errors but no code can guarantee to detect and correct more than four errors. there is no guarantee that a convolutional code can meet this bound and the two most common classes of codes that come close to attaining these bounds, turbo codes  <cit>  and low-density parity-check codes  <cit> , require long-range dependencies between positions in the sequence and so cannot be implemented in any plausible sequencing chemistry.

alternative chemistries
examining the syndromes in table  <dig>  we notice that, despite many possible single errors having ambiguous syndromes, not all syndromes are present: the three syndromes αβ, β <dig> and 1α do not occur. the missing syndromes are not truly unused, being generated by multiple errors, but the failure to use them to distinguish single errors suggests that there may exist alternative codes with a greater ability to correct single calling errors. the advantage would derive from using the extra syndromes to partition single-color errors more evenly into equivalence classes. such alternative codes can be analysed using the same techniques as the ecc code.

rather than consider all possible convolutional codes, we will focus our attention on a subset that satisfy some reasonable restrictions inspired by the reality of the solid platform. it is desirable that any new chemistry would be backwards compatible with the two-base-encoding chemistry, meaning that the first five rounds of sequencing must use an unaltered two-base-encoding probe set. this backwards compatibility restriction is equivalent to requiring that a new code must have an unpunctured color stream.

while the number of rounds and probe sets could be varied, and probes of differing lengths could be used, to remain comparable to the current ecc chemistry we will focus only on chemistries where a single additional round  will be used and the probes remain based on pentamers. analogous to the code structure shown in figure  <dig>  alternative codes that can be implemented with a single additional round are punctured so only every fifth element of the second stream is produced.

by redefining the boundaries of the block structure, the number of different alternative codes that need to be considered can be further reduced: a code whose probe generator starts with one or more 0s is identical to one starting at the first non-zero element with the tail padded with zeros. the probe generator of any code that starts with α or β can be written as αw or βw, where w  is the probe generator of a code starting with  <dig>  and the linearity of convolutional codes ensures that the two codes have identical sets of code words: although the mapping between nucleotide sequence and encoded sequence will differ, the error correcting characteristics will be the same.

the block generator for alternative codes that satisfies all the restrictions, and its inverse, can be written as 

  galt=1  0  0  0p11  1  0  0p20  1  1  0p30  0  1  1p40  0  0  1p5galt−1=1  0  0  0x1y−10  1  0  0x2y−10  0  1  0x3y−10  0  0  1x4y−10  0  0  0y−1l 

where 
p1
p2
p3
p4
p <dig> is the generator for the second set of probes, l is the matrix whose upper triangle consists of  <dig> and whose diagonal and lower triangular elements are all  <dig>  x=lp and y=
x <dig>  the ecc code has the probe generator 
p1
p2
p3
p4
p5=1β0β <dig>  the inverse generator only exists when y is invertible ; when y is not invertible, blocks of colors cannot be individually inverted and the syndrome analysis is not applicable. due to the structure of l, requiring y to be invertible is the same as the sum of the elements of the probe generator not being zero.

one further restriction will be placed on the probe generator of alternative codes: the ecc probe generator contains  <dig> at its fifth position and this is probably a consequence how accurately pentamers with differing final bases can be ligated to the sequence. codes whose generators use the final position might theoretically have better error correction properties but the increased rate of calling errors, due to incorrect probes being ligated, may cancel any improvement their use may offer; consequently, we will require 
p5= <dig> 

there are  <dig> probe generators satisfying all the restrictions and the invertibility condition, of which the syndromes and equivalence class for two interesting alternatives are shown in table  <dig>  the first code has generator 
p1
p2
p3
p4
p5=10β <dig> and has similar equivalence classes to the ecc code but only uses the first three positions of the generator. while the set of syndromes for single-color errors is also incomplete  and thus the error correcting properties will be similar to the ecc code, the shorter length of the generator means that calculations on the trellis, to determine the most probable decoding, can be carried out four times quicker.


p
−


c1

c2

c3

c4

c5

p
 + 

c <dig> 
c <dig> 
c <dig> 
c1},{
c <dig> 
c <dig> 
all syndromes caused by a single error in a block of five code letters  and two parity letters  for codes with the specified generator. each row corresponds to a specific type of error at the given position of the code word and the table entries are the values of the relevant elements of the syndrome corresponding to the upstream and downstream parity checks for the block for each error type. the error type + <dig> is not shown since it represents no error. the equivalence classes are listed separately and do not correspond to specific error types. note that 
p−is also 
p + for the preceding block.

the second alternative code shown in table  <dig>  with probe generator 1β <dig>  uses all possible syndromes and potentially has better error correcting properties than the ecc code. comparing the equivalence classes of the new code to those for the ecc code, the new code uses the extra syndromes to split the largest class into two. whereas the ecc code is unable to distinguish errors at the first, fourth or parity positions, the new code can unambiguously correct errors at the first position and the ambiguity of fourth and parity position errors is reduced.

c <dig> 
c <dig> 
c <dig> 
c <dig> 
c <dig> 
c <dig> 
all possible patterns of error caused in corrected base-space sequence by a single, wrongly corrected error in the observed sequence for code with given generator. an error occurs at one of the positions in the ‘positions’ column; the other member of ‘positions’ is where the observed sequence is wrongfully corrected. as in table  <dig>  all error patterns should be multiplied element-wise by the error type  to get the actual pattern.

simulations
the theory described suggests that the code with probe generator 1β <dig> is more powerful than the ecc code but this does not necessarily mean that it performs better in practice since actual performance will depend on the distribution of different types of error and at which positions they occur. to help quantify the difference in performance between codes, they were compared on artificial data, simulated so that it had an error profile similar to real data but for which the original sequence of bases is known, the error profile being estimated by mapping to a snp-corrected genome.

one million fragments were sampled uniformly from the positive strand of the genome of e. coli dh10b with qualities being sampled from a real sequencing run of the same genome. the probability of generalised error was set to be equivalent to 
q <dig>  close to observed values for prepared samples. three sets of reads were produced, using different codes on the same set of fragments to generate the ecc information, and sequence was called into both base-space and color-space using the maximum a posteriori  criterion. reads were then mapped to an appropriately encoded reference using the bwa aligner  <cit>  with an edit distance of five. results, in terms of the total proportion of reads mapping and the proportion mapping with a given number of errors, are shown in table  <dig> 

percentage of reads mapped , and mapped with a given number of errors, for one million simulated reads using the codes with probe generators as specified. for comparison, figures are also given using only the two-base-encoding probe set . since color-space reads have their first position trimmed before mapping to produce reads  <dig> colors long, the percentages of mapped reads and reads with a given number of errors are slightly inflated compared to those given for base-space reads.

using the ecc information  to help call reads makes a considerable improvement in both color-space and base-space. without any correction,  <dig> % of simulated reads map to the reference in color-space and only  <dig> % of these are perfect, with a further  <dig> % having one error. after correction,  <dig> % of reads are perfect, with  <dig> % containing one error. in base-space, the number of mapped reads increases from  <dig> % to  <dig> %. this large increase is due to correcting single-color errors that would otherwise induce multiple base errors and prevent the read from being mapped using the ‘five differences or fewer’ criterion.

all three codes perform better than uncorrected sequence in both color-space and base-space. there was little difference between their performance in absolute terms, although the code generated by 1β <dig> produced more error-free reads than the other two. this small absolute difference disguises a larger increase in the proportion of corrected reads: under the 1β0β <dig> code,  <dig> % of reads were corrected to being perfect compared to  <dig> % for the 1β <dig> code, an  <dig> % relative improvement even though the absolute improvement is only  <dig> %. the 10β <dig> code produces more mappable base-space sequence than either of the other codes and the reason for this may be in the pattern of errors in base-space that single-color errors make: comparing the patterns in tables  <dig> and  <dig> shows that, for the two alternative codes, single-color errors predominantly cause a single base error when wrongfully corrected, rather than more complex errors, and so there are fewer base-space errors in total.

while the code generated by 1β <dig> does perform better than the ecc code, the improvement is not especially dramatic despite the syndrome equivalence classes suggesting superior error-correcting properties. the decoding algorithm uses soft information as well as the hard information from color calls when determining the most probable base at each position of decoded sequence and this is a possible explanation for the small difference between the performance of the codes. the equivalence classes narrow down the possible errors but, rather than randomly picking the correction, map decoding uses the quality information to guide the choice and the right correction might be picked the majority of the time even without this extra assistance.

CONCLUSIONS
the addition of the exact call chemistry to the solid platform enables many sequencing errors to be detected that would otherwise would pass unnoticed; this in itself provides useful information about the accuracy of reads. without using quality information, the ability of the exact call chemistry to correct sequencing errors is limited but the number of possible options can often be drastically reduced. the quality of the calls for the encoded sequence can then be used to choose between the options for correction, leading to genuine ability to correct sequencing mistakes. the five-base length of the block places a limit on how frequently errors can occur before the encoding can no longer offer protection. in our simulations, the number of perfect color-space reads increases by 27% when error correction is performed, the majority of these corrections being single errors that might otherwise degrade the base-space translation of the read. the error correction capability offered by the ecc results in measurable gains for the solid platform but there is a trade-off between the extra time and expense and the improvements possible through increasing coverage using additional sequencing runs.

the complex triple and quadruple errors that can be induced in the final nucleotide reads by miscorrection of the observed sequence are not well represented by the phred error model  <cit> , where each site is assigned an individual quality independent of other sites. this may have consequences for downstream analyses that assume the phred model is a good representation of the probability that a particular site is in error. the patterns of error induced by the largest equivalence class of the ecc code may allow some sequencing errors to be distinguished from genuine variants after mapping to a nucleotide reference. the power of this approach is unlikely to be good and so it will not be an adequate replacement for mapping and variant calling using a more appropriately encoded reference. one possibility would be to encode the reference using the ecc code and match directly against that, using all the available information but making variant calling difficult due to the complex structure of the data. a simpler alternative would be to decode into two-base-encoding colors, using the extra ecc calls to correct the color calls. the corrected color reads can then be mapped against a color-encoded reference using the many tools already developed.

the variation in the ability of the ecc to correct errors at different positions in each block suggests a simple improvement to the platform that would enable it to recover from problematic rounds. consider, for example, that one of the initial five rounds has experienced some form of gross failure, perhaps due to a bad wash or incomplete melting of the previous primer and sequence from the template, so that every cycle in that round is of poor quality. the priming of the final round could be adjusted to start at a different position of each block, chosen to maximise the chance of correcting previous errors. for the ecc code, the priming would be chosen to ensure that calls from the bad round coincide with positions two or three of the final round ; the alternative code generated by 1β <dig> would be primed so that the errors coincide with the first position and correction could be guaranteed. this technique could also help recover from transient errors, like bubbles in the buffer preventing calls being made for a large number of clusters on a particular cycle.

the alternative codes that we describe have equal or superior error correction ability to that of the ecc but the creation of a new probe set is a considerable investment and a more thorough search of alternatives codes should be undertaken before the expense is incurred. all the alternative codes considered satisfy a number of restrictions, several of which could be relaxed to improve the error correction properties. firstly, more rounds could be used, slowing the sequencing process down but providing more redundancy with which to correct errors. secondly, the architecture of the encoder was constrained to be backwards compatible with the two-base-encoding sequencing chemistry, completely defining one unpunctured stream. a completely new chemistry could vary both probe sets and also change the puncturing matrix, allowing much more flexibility over the design of the code architecture and potentially creating codes that are capable of unambiguously correcting multiple errors.

while many sequencing errors can be corrected, their occurrence is non-uniform, being more frequent in the later cycles of each round. this leads to a tendency for the poorest reads to contain multiple errors in close proximity. such bursts of errors cannot be successfully corrected, so the advantage of the ecc is limited to generally high quality reads containing a few sparsely distributed errors. perhaps the major advantage of the new chemistry is that the sequencing platform can produce reliable nucleotide sequence, without the possibility of a single error causing a catastrophic decoding failure, which allows the reads produced to be analysed with the wealth of tools available that assume nucleotide sequence.

software
software reimplementing the decoding algorithm for the two-base- and four-base- encodings , as well as the alternative encodings described, on a trellis to find both the maximum likelihood  and maximum a posteriori  <cit>   nucleotide sequence is available at http://www.ebi.ac.uk/goldman-srv/solid/, distributed under version  <dig> of the gnu general public licence, as is software to simulate two- and four-base encoded reads and some utility functions to manipulate convolutional codes in gf <dig>  this software is provided solely for the purposes of reproducibility.

competing interests
this work was supported in part by wellcome trust technology development grant wt088151ma and a grant to the ebi from life technologies corporation. life technologies corporation had no input into the development of the study, article preparation or decision to publish. no other competing interests are declared.

authors contributions
the study was conceived and planned by both authors. the analysis of the exact call chemistry coding was done by tm, who also drafted the manuscript. both authors read, edited and approved the final manuscript.

