BACKGROUND
it may happen that researchers have to take into account the results obtained from different independently handled statistical tests of the same null hypothesis. it is then desirable to combine all tests into a single one in order to make the most accurate decision. this is typically the case when one wants to combine the results from different published articles and obtain a global p-value over all the tests for global decision making or, in population genetics studies, when the statistical results from different loci or from different kinds of samples must be combined. for instance, it may be desirable to test for genetic differentiation between males and females, between infected and non-infected hosts from different populations or between parasites collected from different host species sampled in sympatry in different locations. let p <dig>  p <dig> ... pk be the k p-values obtained. the question asked becomes: "is the k tests series significant as a whole?". beside the bonferroni procedure and its sequential derivatives  <cit>  that are not appropriate in that matter , one procedure, the fisher's method  <cit> , is classically used in the literature to combine these k p-values into a single one. as already discussed  <cit>  bonferroni is very conservative, and is inappropriate if the goal is to obtain a global p-value and not to identify which p-values are significant, which is really a very different question . fisher's procedure was held responsible for being sensitive to deviations from uniformity of the distribution of the partial p-values by goudet  <cit>  who then proposed a randomization procedure to test for symmetry around  <dig>  using the geometric mean of p-values as a statistic . fisher's method was also blamed to suffer from asymmetry by whitlock who proposed stouffer's z-transformed test  <cit> . to quote rice  <cit> , "while useful in many applications", fisher's test is "inappropriate when asking whether a set of tests, on balance, supports or refutes a common null hypothesis" as it is the case explored in the present paper. an alternative exists that was first introduced by wilkinson  <cit>  and first applied  to population genetics data by prugnolle et al.  <cit> . at a given type i error rate α of say  <dig> , if k tests are undertaken under the null hypothesis, it is expected that there are about 5% of p-values that should be equal or inferior to  <dig>  . then an exact binomial test with  <dig>  expectation, k <dig> , the number of observed p-values not greater than  <dig>  in k trials, should provide the exact probability that a number as great or greater of significant p-values can be observed under the null hypothesis. a generalisation of this simple principle was proposed by teriokhin et al.  <cit> .

in the present note we describe "multitest v <dig> " that implements this generalized binomial procedure. we propose a performance comparison analysis between fisher, generalised binomial, sgm and z-transformed procedures on simulated population genetics data with randomisation tests where all tests address the same null hypothesis and are all looking at deviations in the same direction. finally, the comparison is also undertaken on several real data sets. these procedures were never compared before, especially so with randomisation tests on frequency  data for which minimum p-values are bounded by sample size, genetic diversity and randomisation number.

implementation
parameters used for the generalised binomial procedure
the different parameters we will use here are the following:

s: a series of independent tests;

k: the number of tests in s;

α: the chosen level of significance over all the k tests;

ssorted: the k tests from s sorted in increasing order, p <dig> the lowest and pk the highest;

k': the number of tests in ssorted that need to be equal or under a given level so that h <dig> is rejected at level α for s;

α': the level to which all p-values from the first to the k'th in ssorted must stay equal or inferior , so that h <dig> can be rejected at level α;

kα': the number of tests that are significant at level α';

: the minimum value required for α that leads to reject h <dig>  for a given k' or α'.

the software
multitest v <dig>  is a windows application developed with delphi  <dig> . the algorithm, detailed procedure and the quick-basic source can be consulted in  <cit> . the program , the code  and help file  are provided as additional files  <dig>   <dig> and  <dig> respectively . the philosophy behind the test is that the k independent p-values of the same null hypothesis h <dig> should be distributed according to a uniform distribution with mean  <dig>  and limits  <cit> . the software was designed to deal with two distinct situations. in the first situation one chooses k', the number of partial significant tests that will define, for a given α, the level α' at which the k' tests need to be significant , so that s is significant at level α. for this situation we recommend to always use k' = k/ <dig> or in any case to define k' before anything else is undertaken . in the second situation one chooses α' that will determine the required number of tests k' that need to be significant , so that s is significant at level α. this second situation is particularly useful when the exact p-values are unknown and levels of significance are indicated by symbols such as "ns" , "*" , "**"  and "***" .

while running multitest you are asked to provide several quantities. the first quantity is the desired level of significance. classically  <dig>  is chosen, but you might be more or less severe, particularly if you are looking for , the "exact" threshold p-value for the k tests series. the second quantity corresponds to the total number of tests you want to combine . then you are asked to choose either to fix k', and search for α', or to fix the value of α', and search for k', under the chosen overall significance level α. if you choose to fix k' then the software will outputs α' that should be not greater than pk' . if α' <pk' then s is not significant at level α. if you choose to fix α', the software outputs k', the number of tests that must display a p-value not greater than α'. if k' > kα' s is not significant at level α. the precision can also be chosen . finally, you are asked to choose an output file where all the results are stored in a text file presented as a table sheet. we advise using the .mul extension but this is left to the user's preference.

let us see one example as illustration. let us assume that we obtained the following p-values after testing for genetic differentiation between males and females of a given imaginary species from ten different localities :  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig>  and  <dig>  . we want to obtain the p-value =  corresponding to h <dig> that there is no differentiation between males and females across the k-tests series. we set α =  <dig> , k =  <dig> and choose to test for k' = k/ <dig> =  <dig>  from there the result is α' =  <dig> , meaning the series is significant at α =  <dig>  if it contains at least five tests with p-value not greater than  <dig> , which is indeed the case as our fifth smallest p-value, p <dig> =  <dig> . a much lower level of significance α can be chosen for the series. here, the minimal level of significance is in fact ≈ <dig> , which outputs α' =  <dig>  ≥ p <dig> =  <dig> . consequently,  represents the p-value  over all the k tests.

evaluating performances of combining procedures with simulations
all simulations were made under easypop v  <dig>  

simulations of controlled null hypotheses
we simulated  <dig> island models  with free migration  of  <dig> randomly mating populations of  <dig> monoecious individuals each, 10- <dig> mutation rate,  <dig> independent loci with u = 10- <dig> mutation rate into  <dig> possible allelic states, starting with maximum diversity and for  <dig> non-overlapping generations. we then tested for genetic differentiation across populations using a random sample of  <dig> populations of  <dig> individuals each. the test used was the g-based  randomisation test  <cit> . the statistic g is computed on contingency table of allelic frequencies from the different subsamples and randomisation based on multilocus genotypes . for each individual test  h <dig> was "there is no differentiation between populations" or, more specifically, "observed g, computed on contingency table of allelic frequencies, is not above 95% of g's generated while randomizing individuals across subpopulations". this test was implemented with fstat  <dig> . <dig>  that also executes a global test across the  <dig> loci using the additive property of g . it thus provides a "true" p-value that takes into account the information from all loci, weighted with sample sizes and allelic frequencies. for each replicate  we combined the  <dig> tests across the  <dig> loci with the different methods. note that in genepop  <cit> , fisher's method is used to combine p-values across loci. please also note that the tests are not g-tests but randomisation tests using g as a statistic. the p-values obtained are thus unbiased estimate of exact p-values  <cit> . this test was deeply investigated  <cit>  and is expected to generate "standard" p-value distributions: uniform under h <dig> and progressively skewed to lower p-values under increasing deviation from h <dig>  it was undertaken to test and compare the correct behaviour of the different procedures under a realized null hypothesis.

simulations with controlled alternative hypothesis
we chose the randomisation test of linkage disequilibrium  between paired loci of fstat  <dig> . <dig>  citing fstat  <dig> . <dig> help file, this option allows testing the significance of association between genotypes at pairs of loci in each sample. the statistic used to test the tables is the log-likelihood ratio g-statistic or, more accurately, the only part of this statistic that changes when randomising tables:  

where xijkl represents the number of individuals in the sample with genotype ij at the first locus and genotype kl at the second locus and where n and m are the number of alleles at the first and second loci respectively. the p-value of the test is obtained as follows. genotypes at the two loci are associated at random a number of times and the statistic is recalculated on the randomised data set. the p-value is estimated as the proportion of statistics from randomised data sets that are larger or equal to the observed. an overall sample statistic is obtained by summing the g-statistics overall samples. the overall test is obtained by comparing this overall statistic with that obtained from randomised tables . the advantage of this test is that each sample is weighted by its "information" content. the p-value in a sample where the two loci are nearly monomorphic  should not be given the same weight as a p-value from a sample where the two loci are very polymorphic and hence the significance of genotypic association can be thoroughly tested. it thus provides a "true" p-value that takes into account the information from all subsamples, weighted with sample sizes and allelic frequencies. ld was chosen because it is probably the population genetics test that generates the most non-standard p-value distributions  , thus the closest to natural imperfect data. for all simulations, parameters were  <dig>  non-overlapping generations, in an island model with n =  <dig> subpopulations, n =  <dig> individuals per subpopulation, m =  <dig>  migration rate, two loci with u =  <dig>  mutation rate with  <dig> possible allelic states. all simulations were replicated  <dig> times. alternative hypotheses of increasing strength were obtained by increasing the clonal rate c =  that generates a corresponding increase in ld between loci  <cit> . for all simulations  <dig>   <dig> or  <dig> subpopulations of  <dig> individuals each were sampled, in order to get different values for k. some simulations ended with a few less than k p-values because some tests were not feasible in some subpopulations . please note that though the strength of deviation from h <dig> is controlled for, h <dig> can itself never be simulated. a full independence between loci would require an infinite population size with free recombination for an infinite number of generations. thus, a signal  is expected even with random union of gametes .

procedures to combine the k p-values
the binomial probability  was looked after with multitest v <dig> . note that α' is bound to  <dig> . when pk' >  <dig> , increasing α  invariably outputs α' =  <dig> . in such cases we simply used the actual value pk' as the global p-value. this has no incidence on the results presented in the present paper as we only were interested in  ≤  <dig>  p-values.

fisher's procedure is simply obtained by a chi-square test with 2×k degrees of freedom on the quantity:   

the sgm procedure was implemented by the eponym computer program kindly provided by j. goudet. it uses a randomisation procedure to test the symmetry around  <dig>  of the geometric mean of the k p-values.

for stouffer's z transform test, each p-value pi is transformed into its standard normal deviate zi, which, for instance, can be obtained by the normal inverse function of excel™, with a maximum value of  <dig>  for pi when pi =  <dig> .

zi is used for the computation of the statistic zs  <cit> :   

zs is then compared to the normal standard distribution  in excel).

a logistic regression exploring the model  ~c + k + method + k: method + constant was finally undertaken under s-plus  <dig> professional release  <dig> , where pi ≤  <dig>  means "significant at the  <dig>  level is true", c is the clonal rate , k the number of tests to be combined, method the kind of procedure  and : stands for "interaction" between parameters. a stepwise procedure was used to select for the best model following the akaike information criterion and remaining parameters tested with a chi-square.

real data sets
four data sets were used: two data sets on mussel  allozymes from  <cit>  and  <cit> ; one data set on schistosome flukes  microsatellites  <cit>  and one data set on the opportunistic fungus candida albicans allozymes  <cit> . we undertook ld tests on these data to compare natural results to our simulations using examples where the exact g-based test was significant as a signature for false h <dig> 

finally we used some non ld-based real datasets to give examples of application when no global test is available. two data sets are from  <cit>   for viviparity in fishes  <cit>  and branch length in angiosperms  <cit>  where contradictions were found between fisher and sgm procedures and where publication biases may interfere with final results. two data sets concern examples of combination of non parametric correlation tests: one data set studies the correlation between limpet abundance and cockle shell size on which they settled in new-zealand shores  <cit>  and one data set examines the correlation between the presence of two pathogenic bacteria in tunisian cattle individuals  <cit> . a fifth data set combines test for bottleneck signatures  on population genetics data in wild rusa deer populations from new-caledonia  <cit> . the last data set concerns the results obtained on the relatedness between male and female cattle ticks found as pair on different hosts and different farms in new-caledonia  <cit> .

RESULTS
simulations of controlled null hypotheses
the global g outputs  <dig> significant tests at α =  <dig>  , and fisher, binomial, sgm and z outputted  <dig>   <dig>   <dig> and  <dig> significant tests respectively. none of these values significantly deviates from the expected 5% .

to conclude, all procedures are fine under h <dig> and give rather equivalent results.

simulations with controlled alternative hypothesis
the first important result, though beyond the scope of the present paper, is that the power of ld test is weak as it can be observed from figure  <dig>  a substantial amount of significant tests only arise for c =  <dig>  . the second result is that, in case of non-standard p-value distributions, combinatory procedures are very conservative. the third observation resulting from figure  <dig> is that all procedures perform more or less equally at least for these tests and simulations. the logistic regression kept c with the strongest  impact, method, because sgm seemed apparently less powerful than the others and k with a positive effect. globally, g, fisher, binomial sgm and z respectively displayed  <dig>   <dig>   <dig>   <dig> and  <dig> significant tests. the slightly lower power of sgm probably comes from the fact each time a p-value is close or equal to unity, it becomes almost impossible for the procedure to output a significant result, even when a substantial proportion of tests in the s series are very small. in fact this test is especially conservative in case of u-shaped p-value distributions. it was indeed designed for combining published p-values on the same null hypothesis, in which case a publication bias is expected and thus for which more weight for non-significant results may be desirable . because of the nature of ld tests, when h <dig> is far from true, u-shaped distributions are likely to occur because in some populations polymorphism will be insufficient at one locus, leading to very high p-values and to very small p-values in subpopulations where polymorphism is high enough. this is also likely to occur often in many population genetics data sets where the power of the different tests in a series will rarely be identical and most of the case highly variable because of uneven sample sizes  and variable genetic diversity across sub-samples.

real data sets
for ld tests, only independent series  for which the global g-based test provided a significant p-value are presented. a glance at table  <dig> confirms the lack of power of combining procedures and that the different procedures do not necessarily lead to the same decision, hence the choice is far from neutral. this general tendency is confirmed with the non ld-based data sets . for literature based data, sgm interestingly outputs non-significant results in opposition to other procedures. here, publication bias might be interfering and the most conservative sgm may be more appropriate, providing the several p-values close to unity are not due to low power tests. it may happen that some tests were made in samples verifying h <dig> and others h <dig>  mixing  <dig> p-values from our simulated h <dig> with  <dig> p-values from ld tests on our simulations with the maximum expected signal  did not spectacularly dropped the proportion of significant global tests but for the binomial . there is indeed no reason that such situations would generate more p-values very close to  <dig> than expected under full h <dig> and such phenomena are not expected to affect sgm much.

k: number of tests combined

significant combined p-values are in bold

smm: stepwise mutation model 

significant combined p-values are in bold

CONCLUSIONS
"fisher's testing procedure represents a test against broad alternatives. it specifically tests whether at least one component test is significant, and can yield a significant combined test statistic when the component tests, on balance, strongly support h <dig>  this is an undesirable characteristic when asking whether a group of tests collectively supports the same h0"  <cit> . bonferroni  is specifically designed for identifying which tests are significant in a series or, to phrase it in a more statistical way, it is designed to test family wide significance of individual p-values  <cit> . to illustrate this, a  <dig> tests series with a single p-value = 10- <dig> and where the remaining  <dig> tests follow a uniform distribution with mean  <dig>  will output  <dig>  with fisher, 10- <dig> with bonferroni,  <dig>  with the generalised binomial,  <dig>  with stouffer's z and  <dig>  with sgm. here, if the alternative hypothesis is that a signal exists across all tests, generalised binomial, stouffer's z or sgm are more appropriate, knowing that a strong lack of power will be met each time the s series will deviate from uniformity . if h <dig> is "there is at least one significant test" then fisher and even bonferroni are more appropriate and will provide a very different result . it is noteworthy signalling that a weighted version of z, more powerful, was also proposed  <cit> . for population genetics data, weighting is a complex interaction between sample sizes and allelic frequencies, but an interesting trail to follow may come from there. note that we did not study the effect of uneven sampling sizes that might also change some conclusions. for published p-values combination, the conservative sgm procedure might be preferred when a publication bias is suspected, but users should be aware that this test will always be very conservative when one or few tests are close to unity. choosing which procedure should be preferred will require further more sophisticated approaches and thus stays a matter of personal convenience. nevertheless, one advantage of the binomial approach is that it can work even when the exact values of probabilities are unknown but only their significance at a given level, a property not shared by any of the other procedures that all require numerical inputs. one disadvantage of the generalised binomial is its lack of symmetry, especially so when the number of tests is small . for instance, when k =  <dig> with p <dig> =  <dig>  and p <dig> =  <dig> , the generalised binomial will output p-value =  <dig>  instead of  <dig>  . in such very particular cases , it will probably be wiser using stouffer's z.

availability and requirements
project name: multitest

project home page: http://gemi.mpl.ird.fr/sitesgass/sitetdm/programs

operating systems: windows 

programming language: delphi  <dig> 

abbreviations
h0: null hypothesis; h1: alternative hypothesis; ld: linkage disequilibrium between loci; smm: stepwise mutation model .

authors' contributions
tdm wrote the program, undertook and analysed the simulations, re-analysed real data sets and wrote the paper. jfg contributed in the guidance of the project and corrected the manuscript. att wrote the algorithm, contributed in the guidance of the project and corrected the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
the executable program file, "multitestv1- <dig> exe".

click here for file

 additional file 2
the source code in text format, "multitestlisting.txt".

click here for file

 additional file 3
the help file, in adobe acrobat format, giving all the instructions needed to use the program, "noticemultitestv1- <dig> pdf".

click here for file

 acknowledgements
the work was partially supported by rfbr . we thank m. whitlock for his patience and time spent explaining to tdm the z-transformed test, j. goudet for useful discussions and for providing the program sgm and two anonymous referees whose help considerably improved the manuscript. the program multitest v <dig>  is available at http://gemi.mpl.ird.fr/sitesgass/sitetdm/programs/multitestv1- <dig> exe and the corresponding notice at http://gemi.mpl.ird.fr/sitesgass/sitetdm/programs/noticemultitestv1- <dig> pdf.
