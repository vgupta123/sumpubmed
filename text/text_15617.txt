BACKGROUND
web-based information sources such as online news media, government websites, mailing lists, blogs and chatrooms provide valuable epidemic intelligence by disseminating current, highly local information about outbreaks, especially in geographic areas that have limited public health infrastructure. these data have been credited with providing early evidence of disease event, such as sars  and avian influenza  <cit> . the availability of open source and freely available technology has spawned a new generation of disease "mashups" that scour the web and provide real-time outbreak information. healthmap  <cit>  is one such system that monitors, analyzes and disseminates disease outbreak alerts in news media from all around the world. each hour, the system automatically queries over  <dig>  sources using news aggregators such as google news, for relevant reports. it filters the retrieved documents into several taxonomies and provides on its website, http://www.healthmap.org, a geographic and disease-based display of the ongoing alerts. healthmap provides a starting point for real-time intelligence on a broad range of emerging infectious diseases, and is designed for a diverse set of users, including public health officials and international travelers  <cit> .

this real-time surveillance platform is composed of a number of information retrieval and natural language processing modules, such as outbreak alert retrieval and categorization, information extraction, etc. in the present work we are interested in a critical task of the last phase of the information processing scheme: the geographic parsing   <cit>  of a disease outbreak alert or the extraction from one such textual document of its related geographic information. this information is needed for the precise geographic mapping, as well as for the identification/characterization of the particular disease outbreak described in the alert. indeed each alert is uniquely characterized by its disease category, a set period in time and its precise geographic location. a good characterization of the outbreak allows the system to discriminate between duplications of an alert and new alerts. it is also essential for the evaluation of the system and for long-term analysis. most importantly, the automated high resolution geographic assignment of alerts aids rapid triaging of important information by system users, including public agencies such as the world health organization and the us centers for disease control and prevention that use these data as part of their daily surveillance efforts.

so far, healthmap assigns incoming alerts to a low resolution geographic description such as its country, and in some cases its immediately lower geographic designation . the system uses a rule-based approach relying on a specially crafted gazetteer, which was built incrementally by adding relevant geographic phrases extracted from the specific kind of news report intended for mapping. the approach consists in a look-up tree algorithm which tries to find a match between the sequences of words in the alert and the sequences of words in the entries of the gazetteer. it also implements a set of rules which use the position of the phrase in the alert to decide whether or not the phrase is related to the reported disease.

the gazetteer contains around  <dig>  key phrases, some of which refers to geographic locations with several resolution levels , some are negative phrases  as well as phrases that are specific to the kind of data processed .

while the current limited gazetteer has proven useful for a high level view of ongoing threats, there is a public health need to develop a method that provides the highest resolution geographic assignments, especially for public health practitioners that require this information for outbreak verification and follow-up. the approach presented in the following section is an attempt at producing a geo-parser using the prior knowledge encoded in the gazetteer as a base. at first glance, it would seem that in order to increase the resolution of the healthmap geo-parser, expanding the size of the gazetteer should be enough. however in our experience, adding new terms to the gazetteer, without careful supervision often results in an upsurge of false positives for the system. the ability to predict statistically if a word in a sentence is a geographic reference is a valuable feature for a geo-parser. indeed no matter the size of the gazetteer, it cannot contain every geographic reference, and even words in the gazetteer might be false positives in the alert . the typical way of obtaining a statistical predictor is to have access to annotated training data. however, data annotation is a very time consuming and expensive task, and thus the approach we present is an attempt at circumventing it by using the already available gazetteer.

the inspiration behind the present approach is based on the intuition that a human reader presented with a text containing a phrase that is out of his vocabulary would most likely be able to guess whether this phrase refers to a geographic location or not. this reader would infer the semantic role of the phrase with a certain accuracy, because he has prior knowledge of the syntactic context in which geographic references appear, maybe also of their particular character distribution or the fact that they generally begin with a capital letter, etc. our approach in some sense simulates this situation with a learning algorithm in the guise of an artificial "reader." we use the healthmap gazetteer as the supervision  for training the reader. some of the location words in the training texts are purposely hidden from the reader's vocabulary in order to divert the attention of the learning algorithm to the context on which these words appears instead of the words themselves. previous related natural language processing approaches reported in the literature, like ours, use a limited knowledge base to generate a broader one.

the task we are trying to solve, namely finding geographic location references in a text, falls into the more generic natural language processing problem of information extraction  <cit> , which involves automatically selecting sub-strings, containing specific types of information, from a text. it is in particular closely related to the named entity recognition task  <cit> , in which texts are parsed in search of references  to persons, organizations and locations or more recently gene name  <cit> . however, we are here interested in more than just named entities, since any hint of location, even, for example, adjectives  or public health organizations , can provide us with desired information.

there have been a number of approaches to named entity recognition and more generally to information extraction problems , exploiting as we do, syntactic and contextual information. they however usually rely on supervised approaches, which require heavily annotated datasets to account for the human experience. building these annotated corpora, is extremely time-consuming, expensive and results in a so-called knowledge-engineering bottleneck. on the other hand, large numbers of unlabeled texts are easily available through, for example, the internet.

in order to take advantage of the unlabeled data, while avoiding the cumbersome need of annotation, a number of approaches, sometimes referred to as automatic knowledge acquisition  <cit>  have been developed. the domains to which each of these methods is applied are very diverse. some concentrate specifically on the named entity classification problem  <cit> , while others, like ours, have a different information extraction scope  <cit> . whether they use a few rules  <cit>   or a small lexicon  <cit>   as seeds for the information to be extracted, all approaches, including ours, begin with a corpus partially labeled. these approaches are related to semi-supervised learning, where a few labeled examples are used in conjunction of a large number of unlabeled ones. the goal of all these models is to exploit the redundancy of language and to learn a generalization of the context in which labels appear.

strategies on how to use these few labeled examples diverge. most of these approaches go through their training sets in several steps and incrementally add inferred labels to their labeled examples, in a bootstrap fashion,  <cit> . of course, the addition of every inferred labeled example  can quickly produce a drift towards a noisy solution. these approaches have thus heuristics to decide which examples to add at each iteration. other approaches, like ours, are built to learn everything they need from the initial input only  <cit> .

while our present work focuses on geo-parsing an english-language corpus, healthmap surveillance so far covers alerts in  <dig> more languages , and plans to expand to other languages. most of these approaches,  <cit> , use elaborate linguistic knowledge either to represent the words or to target groups of words to tag. relying on complex linguistic features requires language-specific expert knowledge difficult to obtain. our approach relies on low level syntactic features making it easily portable to other languages. in addition, it is based on statistical machine learning principles , as opposed to rule-based ones, which also reduces the need of expert knowledge.

RESULTS
core idea
the core idea behind our approach is to have a dataset of alerts tagged with the gazetteer-based algorithm as well as with more general linguistic knowledge , and then to use this dataset with tags partially hidden to learn a generalization of the parsing. in the example of figure  <dig>  a sentence is annotated with its corresponding part-of-speech tags and gazetteer-based geo-parsing tags . in order to learn a generalization of the geo-parsing scheme, the same sentence would be used in our training dataset with the specific identity of the word new caledonia hidden, but its part-of-speech preserved.

additional features
our dataset consists of english-language disease outbreak alerts retrieved in  <dig> by the healthmap system. we used the healthmap rule-based approach to tag the words in the alert that match geographical references found in the gazetteer. to enrich the dataset with syntactic information, we tagged the alerts with a part-of-speech tagger . provided that, in english, location names often begin with capital letters or appear as acronyms, we assigned to the words in the alerts, in addition to their part-of-speech tags, a capitalization status, ie none, first character, upper case.

from a more formal perspective, our dataset  is composed of p examples {, ..., }. each example  is composed of an alert x =  with a certain number l of words xi, and their corresponding "location" labels y = , yi ∈ {loc, none}, that is, yi is  a location reference or not. the words xi are represented by their part-of-speech tag, their capitalization status and occasionally by their index in a dictionary d, extracted from the training dataset. figure  <dig> illustrates the vectorial representation of words. the |d|  first components of xi correspond to the dictionary indices and are all equal to zero, except for the position coinciding with the word index in d. similarly, the next k features of xi correspond to the part-of-speech tag indices in the k part-of-speech tag list. and finally, the last three features stand for the three possible capitalization values.

hiding words
as explained previously, one important characteristic of this experiment is the fact that the words  are only partially accessible to the learning algorithm. this is implemented by the choice of the dictionary d mentioned in the previous paragraph. as we will explain shortly, some words in the corpus are purposely left out of the dictionary. consequently, the first components of their sparse representation, the ones referred to as dictionary index in fig.  <dig>  are all set to zero. such words are thus represented only by their part-of-speech tag and their capitalization status.

how we choose which words to hide is critical to the generalization power of our learning algorithm. indeed, naively deciding to hide the words tagged as location by the healthmap gazetteer would be equivalent to giving away the label information to the learning algorithm. it would lead it to learn "by heart" what it is that the gazetteer considers a location, while what we are interested in is making it discover new location patterns. the strategy we implemented is based on the idea that words referring to specific locations have a lower frequency  ) in the corpus than "typical" words ). for example, the word authorities appears  <dig> times in our  <dig>  alerts corpus  while, the word australia appears only  <dig> times. continuing the metaphor of the artificial reader, we decide to cut words out of its vocabulary indiscriminately, based solely on their frequency. we make the reasonable assumption that if a word is rare then it is less likely to be "known" by the reader. figure  <dig> shows that applying this strategy indeed leads to hiding more location words than "typical" words. as shown in the figure, for a given word's frequency cutoff, the percentage of hidden words out of all words in the corpus  is lower than the percentage of hidden location words out of all location words . the graph shows, for example, that cutting words that appear with a frequency lower than  <dig>  × 1e- <dig>  hides roughly 10% of the words in the corpus, but close to 25% of the location word occurrences . lets us call this lower bound on the word frequency λ. we apply λ onto the corpus to decide whether or not a word index is to be hidden, so that only frequent word indices are visible to the model. another justification of our approach is the following. the list of words present in the training set represents the largest vocabulary the algorithm has access to. in general, it would seem a good idea to diversify and augment this vocabulary, by increasing the size of the training set, so that the algorithm is presented with examples of usage of the new words - as is typically done in a fully supervised approach. since we do not have access to fully annotated corpora, but rely on a finite gazetteer to label the data, increasing the size of the training set would eventually be redundant. from another point of view, no matter the size of the training set, there is no guarantee that all words in a new set of texts will fall within the algorithm's dictionary. as an example, figure  <dig> shows the percentage of out-of-vocabulary words among the unique words of an evaluation set, for several vocabulary sizes. our approach reduces the dictionary available to the learning algorithm, placing it, during the training, in the situation that it would eventually face when confronted with new data. in particular, since we reduce the dictionary in a way that respects the distribution of words, we make the hypothesis that the location words that we purposely keep out of the dictionary can work as surrogates of the new location words we want to discover.

using the data just described, we trained an artificial neural network to output a probability estimate of the label value yi for the ith word xi in an alert x,  

given a window  of preceding and following words. a threshold on nn allows us to decide if the input is a location or not. the neural network was trained by negative log-likelihood minimization using stochastic gradient descent. an extensive description of the learning algorithm architecture and optimization is provide in the "methods" section.

evaluation
the same lack of labeled data that we were faced with for training the geo-parser applies to the question of how we can test the performance of a trained model. indeed, to measure the accuracy of the outputs of the geo-parser, we would need "correct" annotations to compare with. ideally, we would even have a test corpus annotated by several humans independently and thus be able to measure the difficulty of the task we are trying to solve. as stated previously, annotating such a dataset is a tedious and expensive task and we thus consider it as last resort. in order to nonetheless evaluate the performance of the algorithm we devised two approximate solutions.

• first, as in the training phase we can reuse the healthmap gazetteer to tag the test corpus and then evaluate how much of the gazetteer annotation the neural network geo-parser  is able to recover. it might seem that retrieving the locations that were used for training is an easy task. however, the same lexical information that was hidden in the training corpus would be hidden in the test dataset as well. as a consequence, both training and test examples provide only general context information, and thus rediscovering the healthmap gazetteer labels is not such a trivial task.

• in a second experiment, we used a comprehensive  commercial geo-parser  to tag  <dig> alerts with what we would consider "true" location references. note however that despite the fact that there is good overlap between the healthmap gazetteer and the metacarta tags, there are also a certain number of tags that are found only in the healthmap annotation. taking both sets of tags as a whole:  <dig> % are found metacarta's only, both geo-parsers agree on  <dig> % of the tags and the remaining  <dig> % come from the healthmap gazetteer. some of the healthmap only tags are due to minor uninteresting variations in the annotation schema, while others suggest a specialisation of the healthmap gazetteer to public health content that the more generic metacarta geo-parser is obviously not trained to provide.

we trained several neural network geo-parsers on the three datasets t <dig> , t <dig>  and t <dig> , with extracted dictionaries of varying sizes according to our lower bound λ. given the approximate nature of the solution found when training neural networks by stochastic gradient descent, we repeated the learning process for each condition  <dig> times to estimate the variance. the evaluation corpus contains  <dig> disease outbreak alerts subsequent to the ones used for training, in respect of the temporal nature of the healthmap surveillance process. this represents  <dig>  words to tag among which  <dig>  are words that are considered locations by the healthmap gazetteer approach,  <dig>  are considered locations by the commercial geo-tagger, of which  <dig>  by it alone .

 and the recall of the system  

:  

to show that this approach is not just a memorization of the gazetteer-based approach, the results are sliced into the f <dig> scores of words inside and outside the algorithm's dictionary, ie words that were represented with and without their dictionary index feature. as the value of λ increases, the size of the dictionary we allow the nn geo-parser to see decreases and thus the number of words in the evaluation corpus that are out of the algorithm's dictionary increases. this makes the task of identifying which words in the text refer to locations more difficult, and as a visible consequence the overall performance of the system decreases . on the other hand, however, the increase in out-of-dictionary examples greatly improves the ability of the system to correctly identify locations that are out of the algorithm's dictionary , ability that is non-existent when the whole training set vocabulary is available to the learning algorithm . as explained in previous sections, the idea behind this approach is to consider those purposely "hidden" location words as surrogates of the location words unknown to the gazetteer, those we want to be able to discover. the observed increase in performance in the retrieval of those words suggests that this approach would be appropriate for this task and without having too high a loss in performance for "visible" words .

it is worth noting that there are words that are here considered as false positives  that are in fact generalizations of those words that healthmap sees as location but that the more generic geo-parser ignores.

discussion
we have presented an approach to the geo-parsing of disease outbreak alerts in the absence of annotated data. the identification of precise geographic information in the context of disease outbreak surveillance from informal text sources  is essential to the characterization of the actual outbreaks, and would increase the resolution of a visualization scheme such as the one proposed by healthmap. such precise information extraction typically requires a dataset of texts with the desired information carefully annotated by a human expert. a corpus of this kind is expensive and time-consuming to create. instead, we propose a statistical machine learning approach which generalizes the existing healthmap rule-based geo-parsing by making use of the lexical and syntactic context in which the existing gazetteer phrases appear. we have demonstrated that the described model has indeed the ability to discover a substantial number of geographic references that are not present in the gazetteer. in addition, our approach also limits the number of false positive it produces.

nevertheless, there is still a portion of those geographic references that remains a challenge to retrieve. there are several components of this approach that could be refined in order to improve the performance of the algorithm. for example, a more sophisticated word-withholding strategy could be implemented. in addition to relying on the term frequency, the strategy could also account for the part-of-speech of the word to hide , and on the alert frequency of the term . another place for improvement could be the representation of words. other features, aside from part-of-speech and capitalization, could provide additional information about the semantic status of the word. however, these features should be simple enough to implement in several languages, in order to comply with the requirement of portability formulated previously. another further area of exploration, is the weight given to words tagged as none during training. some of those words are actually unknown locations that, during the learning process, are strongly supervised as not being locations. even if we compensate for this error by tuning the decision threshold, it should be also possible to act on the problem by a relaxation of the supervision during the training stage. finally, the geo-parsing of an alert is only an intermediate step. finding which terms in an alert are geographic references is crucial, but the final goal is to identify which among these terms is the definitive disease outbreak location and be able to disambiguate it . ideally, we would like to integrate these different tasks into one, so that the information that is learned from one can benefit the others.

from a more general point of view, the presented approach also describes a way of incorporating the prior knowledge encoded in a rule-based procedure into a more general statistical framework. this could be adapted to the extraction of other types of information that would also prove useful in the characterization of an outbreak. for example, we are interested in the extraction, from the alerts, of attributes related to the individuals involved in the outbreak such as age, sex, setting, clinical outcomes when specified.

CONCLUSIONS
we have presented an approach to the geo-parsing of disease outbreak alerts in the absence of annotated data. the results of this analysis provide a framework for future automated global surveillance that reduce manual efforts and improve timeliness of reporting. ultimately, the automated content analysis of news media and other nontraditional sources of surveillance data can facilitate early warning of emerging disease threats and improve timeliness of response and intervention.

