BACKGROUND
finding structural genomic variations  has become an active research subject recently. it is commonly believed that some structural variations may be linked to complex diseases  <cit> . now high throughput sequencing  technologies  become more available. sequence data can potentially reveal nearly all genetic variations, including structural variants. thus, great efforts have been made for discovering structural variations in populations using sequence data. for example, the ongoing  <dig> genomes project has released called structural variations for several human populations from hundreds of sequenced individuals in the pilot studies  <cit> .

many current sequence datasets are consisted of pairs of reads. these pairs can be mapped to a reference genome using read mapping tools such as bowtie  <cit>  and bwa  <cit> . usually both reads of the same pair can be successfully mapped to two different locations of the reference genome. the distance in between is called insert size, whose value depends on the library mean and standard deviation. abnormal insert size  may indicate the presence of some genomic structure not present in the reference genome. such pairs are called discordant pairs, which can be useful in locating structural variations. there are many methods that detect svs by analyzing the insert size of discordant pairs, such as pemer  <cit> , breakdancer  <cit> , gasv  <cit>  and variationhunter  <cit> . a drawback of these methods is that only approximate positions of the breakpoints of the svs can be found, while the high resolution of break points is useful in sv classification and annotation  <cit> . read depth methods  belong to another type of method that does not show the exact breakpoints.

assembly and split-read mapping methods are the alternative approaches that can find exact breakpoints of svs. one representative method using split-read mapping is the program pindel  <cit> . sometimes a reads mapping program cannot properly map a pair of reads. there are multiple causes for unmapped reads, e.g. errors in sequence reads. the presence of svs may also cause some reads to be unmappable. in the case of deletions, for example, when a read contains breakpoints of a deletion site, the read will contain two parts: one from the region prior to the deletion site and one from the region following the deletion site. the read may be unmappable because the read is a concatenation of the two parts and is not contained in the reference genome. the pairs with one read mapped and the other read unmapped are used in the split-read mapping methods. the mapped read in the pair is used as an anchor. the other read is split in the middle and then the two parts are attempted to map to the reference genome. if mapped correctly, the mapped split reads may reveal where the deletions occur. recently there are more methods dedicated to find exact breakpoints. sric  <cit>  is a split-read method mainly works on longer single reads like the sanger and  <dig> reads. age  <cit>  maps an assembled contig to a reference genome to detect the exact breakpoints of multiple svs. there are also methods  that do not detect the breakpoints themselves but rely on the exact breakpoints provided by mapping tools . a disadvantage of split-read mapping is that mapping split reads with a large gap is usually less efficient. moreover, split reads may be mapped to wrong locations due to noises in the reads. also, for the svs with a breakpoint in a repetitive region, mapping may fail.

despite there are increasing number of developed methods, calling structural variations from real sequence data remains a challenging computational problem. the challenges for calling structural variations with real sequence data include:  sequence data tends to be short and noisy ,  much current sequence data is at low coverage, and  the volume of sequence data is often large. therefore, much work is still needed to develop more accurate and efficient approaches for structural variation calling with low-coverage sequence data.

recently, we have developed a computational approach for calling deletions from low-coverage sequence data  <cit> . this approach  integrates two existing deletion calling approaches , and thus in principle it utilizes more information contained in the reads than the pure split-read mapping approaches. since sequence data tends to be noisy, it is important to utilize more information contained in the data when calling deletions. briefly, svseq <dig> first tries to split a read  and maps the prefix and suffix parts in two regions. the gap between the two mapped regions of the split read may correspond to a deletion. since there may be more than one way of splitting for some reads and some mapped split reads may only be artifacts of sequence and/or mapping errors, we filter the candidate deletions  using discordant insert size analysis. that is, we call a candidate deletion a true deletion only when the candidate deletions are supported by the discordant insert size analysis. simulation results in  <cit>  show that our method outperforms an existing method  <cit> .

our work in  <cit>  makes progress toward improving deletion calling from sequence data. however, we notice that it has several disadvantages. the most severe issue is that it is difficult to determine the best way for splitting reads: due to noise in reads, there may be many equally good ways for splitting the reads. this not only leads to longer running time , but also may introduce false positives. moreover, split-read mapping tends to be slow especially for genome-scale data. at last, only deletion calling is supported in  <cit>  and obviously other types of structural variations  may also be of interests to many downstream applications.

in this paper, we present our recent work that improves upon svseq <dig>  <cit> . our new approach is implemented in the program svseq <dig>  the following lists the main features of svseq <dig> 

 <dig>  like svseq <dig>  svseq <dig> calls deletions  with exact breakpoints.

 <dig>  svseq <dig> achieves more accurate calling through split-read mapping on focal regions. svseq <dig> also has a much desired feature: there is no need to specify the maximum deletion size, which is often needed by other methods . svseq <dig> is also much faster because it only needs to examine a small number of ways of splitting the reads.

 <dig>  svseq <dig> utilizes new features of sequence reads mapping tools. latest sequence reads mapping  provides partial reads mapping . these partially mapped reads are often provided in the sequence data. svseq <dig> relies on the soft-clip mapping provided by reads mapping tools in part of the split-read mapping. this makes svseq <dig> faster than svseq <dig> and some other similar deletion finding programs .

 <dig>  svseq <dig> is also easier to use: it only needs mapped sequence data  and reference genome  as input.

 <dig>  svseq <dig> supports insertion calling from low-coverage sequence data.

methods
svseq <dig> is mainly designed to reduce the number of falsely mapped split reads. in our previous method svseq <dig>  <cit> , split-read mapping is performed on a genomic region whose length depends on the maximum size of deletions to detect. then, each mapped split read introduces a candidate deletion, which is then filtered through discordant pair analysis. suppose one wants to find deletions up to  <dig> mb long, svseq <dig> needs to search for a region roughly  <dig> mb long on the reference genome. due to errors in reads and repeats in the genome, there may be many "hits" when split reads are mapped. many falsely mapped splits reads are filtered with discordant pairs, but some may happen to pass the filtering step. also, when the number of hits is large, it can be slow in finding all the hits and evaluating them. svseq <dig> takes a different approach in calling deletions:

 <dig>  the mapped segment of a split read  is used as the starting point of split-read mapping. this utilizes new features of read mapping tool and speeds up the computation.

 <dig>  to locate the soft-clipped segment of the split read, we infer a focal region  using the discordant read analysis. we will explain in the following how this step is performed.

 <dig>  the focal region is usually much shorter and thus there is less chance to introduce false positives. we then search for the occurrence of the second segment within the focal region using a semi-global alignment algorithm.

for insertions, svseq <dig> also uses soft-clip mapping in locating the likely insertions. we now give a more detailed description on how svseq <dig> calls deletions and insertions.

deletion calling
svseq <dig> relies on two types of patterns formed by split reads to detect deletions.

• type i pattern: the segment facing the anchor end is mapped .

• type ii pattern: the segment away from the anchor is mapped .

for type i pattern, the mapped segment of a split read based on soft-clip mapping faces the anchor. we denote the mapped location of the mapped segment as  . to discover a deletion, the soft-clipped segment needs to be mapped to some region  . we denote the length of the soft-clipped segment as ls = d - c +  <dig>  because the length of the true deletion is not known, some existing split-read mapping methods  have a parameter on the maximum distance to search for the second  segment. instead of searching in a large region, svseq <dig> only searches a focal region by the guidance of spanning pairs. our goal here is to infer where the soft-clipped segment is likely to start . our first observation is: even with low-coverage sequence data, a deletion is still likely to have at least one paired-end read whose two ends are located on different sides of the deletion . suppose there is a read pair whose two ends are mapped to  and  respectively on the reference genome , and this pair is a spanning pair for the deletion, whose location is determined by the mapping of the soft-clipped segment of the split read. we let li be the expected insert size and let σ be the standard deviation of the insert size. note that li measures the outer distance of the pair . we denote the length of the two reads of the spanning pair as l <dig> and l <dig> respectively. suppose the minimum deletion size to be detected by svseq <dig> is md. svseq <dig> sets md to be  <dig> 

we first show where to find spanning pairs for a given split read.

lemma  <dig> for type-i pattern, s <dig> ≥ a, and with high probability, we have s <dig> ≤ a - l <dig> - l <dig> + li + 3σ.

proof  <dig> if s <dig> < a, then a is not a breakpoint. this does not agree with our underlying assumption that the mapped segment  corresponds to a deletion.

to give an upper bound on s <dig>  note that a is the position of the right breakpoint. the rightmost position of e <dig> on the reference is a - ldel, where ldel is the length of the deletion. now since with high probability, the distance between s <dig> and e <dig> is at most li - l <dig> - l <dig> + 3σ + ldel on the reference. so with high probability s <dig> ≤ +  = a - l <dig> - l <dig> + li + 3σ.

lemma  <dig> states where the spanning pairs are very likely to be located. for a given split read, svseq <dig> searches for reads mapped on the reverse strand within this region for spanning pairs.

now suppose we find one spanning pair for the given split read. recall the spanning pair is mapped to  and . the following lemma specifies the range of c .

lemma  <dig> for type-i pattern, e <dig> - ls ≤ c ≤ a - md - ls . moreover, with high probability, we have c ≤ e <dig> + li - ls - l <dig> - l <dig> + 3σ.

proof  <dig> note that the rightmost position a deletion can end is md bases to the left of a on the reference, because the minimum deletion size is md. so c ≤ a - md - ls. since the spanning pair  spans the deletion, we know the deletion must occur to the right of . the leftmost position of the deletion is thus at least e <dig>  since the length of ls is to be mapped  from the left end of the deletion, we have c + ls ≥ e <dig> 

we now estimate how large c can be. note that on the alternative chromosome , the left and right breakpoints of the deletion become the same, and the left breakpoint of the deletion must be to the left of the starting position of the right end of the spanning pair. thus, on the reference chromosome, with high probability, the left breakpoint is no bigger than e <dig> + li - l <dig> - l <dig> + 3σ.

lemma  <dig> states that we only need to search for the second segment of the split read within the region . this region is called the "focal" region for the split read being mapped and a spanning pair. in most current sequence data, the focal region is relatively small. for example, suppose ls =  <dig> , l <dig> = l <dig> =  <dig>  li =  <dig> and σ =  <dig>  then the width of the focal region is not larger than  <dig>  this is much smaller than the focal region that the original split-read mapping would have searched . also, from lemma  <dig>  the width of the region for spanning pairs is at most  <dig> 

the processing of split reads with type ii pattern is similar in many aspects to that of type i pattern. a main difference between type i and type ii patterns is that type ii pattern does not need additional spanning pairs because the paired-end read itself is a spanning pair. this imposes an additional constraint on the focal region. suppose the mapped segment of the split read is located at  and the mapped anchor is located at  . we let  be the location of the soft-clipped segment of the split read on the reference . we let ls = d - c +  <dig> be the length of the soft-clipped segment. we let l <dig> and l <dig> be the length of the two reads . md, li and σ are defined as before. the following lemma specifies where the soft-clipped segment is allowed to map.

lemma  <dig> b + md ≤ c ≤ s. also, with high probability, we have s - li - ls + l <dig> + l <dig> - 3σ ≤ c ≤ s.

proof  <dig> note that the leftmost position a deletion can start is md bases to the right of b on the reference, because the minimum deletion size is md . also, the breakpoint cannot go to the right side of s for there is no split on the anchor.

note that the position of soft-clipped segment is constrained by the anchor position and the insert size. so the second inequality follows the same reasoning as in lemma  <dig> 

lemma  <dig> states that we only need to search for the second segment of type ii pattern of the split read within the focal region . for the cases when the split read is on the reverse strand, the method applied on them is essentially the same as when they are on the forward strand.

our experience indicates that type i pattern is usually more reliable then type ii pattern, because less errors are expected at the head of illumina reads. thus svseq <dig> gives type i pattern higher weights than type ii pattern when calling deletions. the weight of type i pattern is set to  <dig>  and the weight is set to  <dig> for type ii pattern. a cutoff value on the total weight  is used by svseq <dig>  the default cutoff value is set to  <dig>  i.e. at least one type i pattern read is required or at least three type ii pattern reads are required when there is no type i pattern read.

to search for the occurrence of a soft-clipped segment within a inferred focal region, svseq <dig> uses a semi-global alignment algorithm, as illustrated in figure  <dig>  briefly, we want to map the entire soft-clipped segment within the focal region. thus, the gaps outside of the aligned positions for the focal region are without penalty, while we set the gap penalty within the read to  <dig>  the similarity score is  <dig> for matches and - <dig> for mismatches. since the focal region and the read are relatively short , split-read mapping with sequence alignment can be performed relatively fast.

since the above split-read mapping method starts from soft-clip mapping, its accuracy depends on how accurate the soft-clip mapping is. because soft-clip mapping is found through local sequence alignment, sometimes errors can be introduced by soft-clip mapping. we observe that one possible error in soft-clip mapping occurs when there is a gap in the soft-clip mapping. as shown in figure  <dig>  soft-clip mapping may align a segment longer than that in the true split read by introducing a false gap, while the true alignment can be achieved without gap by mapping a longer soft-clipped segment to a later position. when this occurs, the length of the detected deletion can be different from the real length. svseq <dig> addresses this potential problem by using an adjustment step, which tries to find an optimized mapping of the entire read by avoiding errors . during the adjustment step, we examine all supporting split reads for some deletion. if each of the split reads can be adjusted to achieve a better mapping , then svseq <dig> removes the gaps within these reads and adjusts the length of the deletion accordingly. if the reads do not agree with each other in terms of splitting positions, then svseq <dig> takes a voting scheme by choosing split reads with higher alignment scores.

in practice, there may be more than one spanning pairs for a candidate deletion . when the deletion is heterozygous in a diploid genome, some spanning pairs may originate from the copy without the deletion while others from the copy with the deletion. some other spanning pairs may be due to mapping errors. one possible scheme is to find a "consensus" focal region by combining information provided by multiple spanning pairs. svseq <dig> simply takes the union of all the focal regions from all the possible spanning pairs. this is because there could be mapping errors in the spanning pairs, and thus svseq <dig> takes a conservative estimate of the focal region. our experience shows that the overall focal region is still relatively small and searching for split read can be performed relatively efficiently.

insertion finding
svseq <dig> uses the reads with head segments mapped with low quality  to detect insertions.  in particular, svseq <dig> uses type iii pattern: two mapped segments of split reads overlap but the two whole reads cannot be aligned well . both reads are from properly mapped pairs, and both have low quality mapping or soft-clip at the head segment. we consider a split read that has its tail mapped on the reverse strand. if another split read is from the other direction of the insertion, its split is very likely to be only located in a small region near the known breakpoint. for example, in figure  <dig>  knowing that read  <dig> is mapped with a possible breakpoint, then only the reads that have split in the short region  <dig> have to be examined. as shown in figure  <dig>  if there is an insertion, then the heads of the reads are not from the reference genome. thus, the overlapped portions of the two reads are unlikely to be aligned well. on the other hand, if there is no insertion, then the overlapped portions come from the same genomic region and should be aligned well. because the not well mapped segments are from the heads of the illumina reads, less errors are expected in these segments and their alignment is more reliable.

svseq <dig> relies on pair wise sequence algorithm to align two overlapped reads. the parameters are the same as for the deletion case. if the score of the mapping over the length of the overlap is less than  <dig>  then the pair is treated as evidence of a possible insertion. the default cutoff value of reads supporting an insertion for svseq <dig> is  <dig>  that is, at least another read in this region has the same split and passes the alignment test with the read in this pair on the different strand.

RESULTS
we apply svseq <dig> on both simulated datasets and real datasets, comparing with svseq <dig>  <cit>  and pindel  <dig> .4d  <cit>  on accuracy and efficiency. for deletion finding, the three methods are run on simulated population data, real individual and pooled data. for insertion finding, simulated individual data is used. the real sequence datasets  consist of the alignment files of  <dig> individuals on chromosome  <dig>  nine of the individuals are from the ceu population and the others are from the yri population. these alignment datasets are mapped using bwa with soft-clips on ncbi human genome  <dig>  the accuracy is evaluated according to the results by the  <dig> genomes project ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/working/20110719_merged_sv_calls/. the results contain assembled deletions and the ones found by five sv detection tools of more than  <dig> individuals . the methods include breakdancermax <dig>   <cit> , cnvnator  <cit> , genomestrip v <dig>   <cit> , embl/delly and pindel  <cit> . since not all of the methods are able to provide exact breakpoints of deletions, evaluation of accuracy of methods is based on both a strict criterion and a less strict criterion. a called deletion is viewed correct by the strict one, if the length of the called deletion is the same as a deletion in the results by the benchmark. the less strict one only requires that a called deletion overlaps with a deletion in the benchmark, and at least 50% of the bases of the called deletion are supported. 

finding deletions using simulated pooled data
the simulated datasets with read length  <dig> from  <cit>  are used in this paper to compare svseq <dig> to svseq <dig> and pindel in terms of accuracy and sensitivity. the datasets are simulated from the sequence of chromosome  <dig>  of ncbi human genome  <dig>  the results of the copy number variation release paper of the  <dig> genomes project  <cit>  are based on this version of genome. the deletions of the  <dig> individuals from the ceu population reported by  <cit>  are introduced to the simulation datasets . since the haplotypes of the deletions are not inferred in the file, for the heterogeneous deletions we arbitrarily place one such deletion to one of the two haplotypes of an individual. since the deletions are usually far apart from each other, this may not have big effects on the accuracy of the simulation. a tool called wgsim https://github.com/lh3/wgsim is used with the "-h" option to generate paired-end reads from the two copies of genomes of an individual. single nucleotide polymorphisms and small indels on each genome are simulated using the default parameters. all the datasets are generated with base error rate 2%. paired-end reads are simulated with read length  <dig> and "outer distance"  <dig>  three datasets with coverage  <dig> ×,  <dig> × and  <dig> × are used. bwa, which provides soft-clips, is used with default parameters to map these simulated paired-end reads to the entire ncbi human genome  <dig> 

the performance of finding deletions is compared among svseq <dig>  svseq <dig> and pindel, on these pooled datasets. the results are shown in table  <dig>  we can see that svseq <dig> usually has the highest accuracy and sensitivity. pindel usually has a high accuracy but lower sensitivity comparing with the other two methods. mapping a soft-clipped segment of a split read to a focal region reduces the chance that this segment is mapped to wrong positions. since the mapping approach of svseq <dig> is more accurate, it does not need a higher cutoff to call deletions . we can see that when coverage is higher , the sensitivity of svseq <dig> and svseq <dig> is similar. but when coverage is lower , the sensitivity of svseq <dig> is higher than svseq <dig>  when the coverage or the frequency of a deletion is very low, svseq <dig> may have a better chance of detecting it than using the other two methods.

reads of length  <dig> on chromosome  <dig> with  <dig> deletions are simulated. the cutoff value of svseq <dig> is  <dig>  the cutoff value is  <dig> for svseq <dig> and pindel. number of findings and true positives in each setting are reported.

a called deletion is viewed correct in the comparison in table  <dig> if the length of a called deletion is the same as the simulated length. the split-read approaches have the advantage of high resolution of breakpoints, while different approaches such as read depth and read pair methods are not suitable to find the exact breakpoints. the  <dig> deletions introduced into the simulation are only from  <dig> individuals of the ceu population on one chromosome, but the frequencies of the lengths of these deletions  show the same trend with the frequencies of the deletions found by the  <dig> genomes project . svseq <dig> is able to detect both smaller and larger deletions. for example, in the  <dig> × coverage setting, svseq <dig> finds all the  <dig> larger deletions with length >  <dig>   <dig> and pindel misses one deletion. for the  <dig> deletions with length in range  <dig>   <dig> to  <dig>   <dig>  svseq <dig> finds  <dig> and pindel finds  <dig> deletions. for the  <dig> smaller deletions with length <  <dig>   <dig>  the numbers are  <dig> and  <dig>  respectively. the effectiveness of the sv finding methods may also be affected by sequence coverage. for example, cnvnator  <cit>  is a read depth method that is very accurate on the  <dig> genomes project's trio data. the resolution of breakpoints is also high when it is applied on high coverage data. but it does not perform as well on the low-coverage datasets, e.g. when it is applied on the  <dig> × coverage data using bin size of  <dig>   <dig>   <dig> and  <dig>   <dig>  cnvnator reports  <dig>   <dig>   <dig> and  <dig> deletions, with  <dig>   <dig>   <dig> and  <dig> correct  respectively.

finding deletions using real individual data
the sequence data of five individuals from yri population used by the  <dig> genomes project is used to compare svseq <dig>  svseq <dig> and pindel on real individual data. the number of findings and true positives are shown in table  <dig> 

svseq <dig> is run with cutoff values  <dig> and  <dig>  the cutoff value is  <dig> for svseq <dig> and pindel. f stands for "findings", se for "supported by exact breakpoints" and so for "supported by overlap". numbers in the parenthesis are accuracies.

using individual sequence data, svseq <dig> is able to utilize split reads to call more deletions than svseq <dig> and pindel even when the coverage is very low. with cutoff value  <dig>  svseq <dig> finds the largest number of deletions and a large portion has supports by the benchmark. if a higher cutoff value  <dig> is used, most of the called deletions are supported by the benchmark. the number of findings is still larger than svseq <dig> and pindel, when using cutoff value  <dig> 

finding deletions using real pooled data
sequence reads from  <dig> individuals from ceu population are pooled together, and reads from  <dig> individuals from yri populations are pooled together. svseq <dig>  svseq <dig> and pindel are tested using these two pooled datasets. results are shown in table  <dig> 

svseq <dig> is run with cutoff values  <dig> and  <dig>  the cutoff value is  <dig> for svseq <dig> and pindel. f stands for "findings", se for "supported by exact breakpoints" and so for "supported by overlap". numbers in the parenthesis are accuracies.

using pooled data, all three methods are able to find more deletions than using individual data. svseq <dig> finds more deletions using cutoff value  <dig> but false positive rate is increased too. quite a portion of deletions found by svseq <dig> using cutoff value  <dig> are missed by using cutoff value  <dig>  even pooling nine individuals together, many less frequent deletions still belong to single individuals. because the sequence coverage is low, only one split read with soft-clipped mapping covers such a deletion . the quality of soft-clipped mapping provided by the mapping tools matters in finding svs. if a mapping tool fails to perform soft-clip mapping on a split read, then this read is not used by svseq <dig>  by pooling more data from more individuals, more deletions are likely to be found by svseq <dig> 

simulation results for insertion
there are fewer insertion finding methods than deletion finding. also, fewer insertions have been called and released than deletions. to simulate insertion, the release  of the na <dig> individual is used in this paper. this individual has been sequenced at high coverage and the  <dig> genomes project has released some inserted sequences of this individual. chromosome  <dig> of ncbi human genome  <dig> is used in the simulation, since there are  <dig>  inserted sequences on this chromosome in the release for this individual. each insertion is treated to be heterozygous and added into an arbitrary haplotype. illumina reads with 20× coverage  are simulated using wgism https://github.com/lh3/wgsim with insert size  <dig> and read length  <dig>  the reads are mapped using bwa. both svseq <dig> and pindel  <dig> . <dig> d are tested to find insertions. svseq <dig> finds  <dig> insertions with  <dig> true positives. pindel finds  <dig> insertions, both of which are correct. one is found as "li" and the other is found as "si". the large insertion reported by pindel as "li" has  <dig> split reads supporting it, where  <dig> out of  <dig> are from the forward strand and the other  <dig> are from the reverse strand. even at 20× coverage, split reads of type iii pattern are not common in this simulation study. this simulation shows that svseq <dig> is able to use fewer supporting reads to call insertions.

running time
because the mapping of svseq <dig> is performed on focal regions, the algorithm of svseq <dig> is usually faster than svseq <dig> and pindel. the run time of svseq <dig>  svseq <dig> and pindel is compared in this paper on one dataset. the file  from the  <dig> genomes project is used. the chromosome is  <dig>   <dig>   <dig> bps in length and the file is about  <dig> × coverage. running time of the three methods with different settings is shown in table  <dig>  each method is run using one thread on a  <dig> mhz intel xeon workstation. it can be seen that svseq <dig> is the fastest among the three methods in calling deletions. svseq <dig> also runs faster in calling insertions than pindel. also note that the running time of svseq <dig> and pindel depends on the maximum event size. it can be seen that, if the maximum event size is set higher, both svseq <dig> and pindel will take even longer time to run. before running svseq <dig> and pindel, running some additional scripts is needed to collect inputs for these two programs. such preprocessing may take several minutes, which are not included in the table. svseq <dig> takes the bam file as input and there is no additional preprocessing.

del stands for deletion, ins for insertion and td for tandem repeat.

CONCLUSIONS
there are four types of methods that use high throughput sequencing data to call svs. read depth methods and assembly methods usually need data with higher coverage. read pair methods and read depth methods are not able to find exact breakpoints of svs. split-read mapping methods may find exact breakpoints of some svs with low-coverage data. however, split-read mapping alone usually leads to significant false positives. combining split-read mapping with other types of methods may increase the power in finding svs. in this paper we describe an improved split-read mapping method to call svs using low-coverage sequence data. we show that by using read pairs with discordant insert sizes, split-read mapping can be applied as mapping a segment of a split read on a focal region. using the lemmas in the methods section, we show that the length of the focal region can be much smaller than the maximum deletion size. mapping split reads within a small focal region reduces the chance that a segment is aligned to incorrect positions. thus, mapping split reads within focal regions leads to both higher accuracy and shorter running time. applying on several datasets, we show that svseq <dig> outperforms some other methods in both accuracy and efficiency. svseq <dig> is more powerful compared to these methods when using very low coverage sequence data.

the split-read mapping approach in svseq <dig> can still be improved, e.g. to better model the error patterns of high throughput sequencing data. for the situation when there are repeats in focal regions, insert size analysis might be helpful in finding correct mapping.

availability
the program svseq <dig> can be downloaded at http://www.engr.uconn.edu/~jiz08001/.

competing interests
the authors declare that they have no competing interests.

authors' contributions
jz designed algorithms, developed software, performed analysis and experiments, wrote the paper. jw contributed to performing analysis and experiments. yw designed the algorithms, wrote the paper and supervised the project. all authors have read and approved the final manuscript.

