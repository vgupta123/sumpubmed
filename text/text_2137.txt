BACKGROUND
the aim of feature selection is to form, from all available features in a dataset, a relatively small subset of features capable of producing the optimal classification accuracy. this subset is called the predictor set. the following are past and current stances on the use of feature selection for multiclass tissue classification:

• feature selection does not aid in improving classification accuracy  <cit> , at least not as much as the type of classifier used.

• feature selection is often rank-based, and is implemented mainly with the intention of merely reducing cost/complexity of subsequent computations , rather than also finding the feature subset which best explains the dataset  <cit> .

• studies proposing feature selection techniques with sophistication above that of rank-based techniques resort to an evaluation procedure which is prone to giving overly optimistic estimate of accuracy, but has the advantage of costing less computationally than procedures which yield a more realistic estimate of accuracy  <cit> .

in short, there are three ways in which feature selection has been, and still is regarded for multiclass microarray datasets: 1) should not be considered at all, 2) as simple rank-based methods for dataset truncation, and finally, 3) as more complicated methods with sound theoretical foundation, but with doubtful empirical results.

a feature selection technique is made of two components: the predictor set scoring method ; and the search method . the technique becomes wrapper-based when classifiers are invoked in the predictor set scoring method. otherwise, the technique is filter-based. filter-based techniques, which are the focus of this study, have several advantages over wrapper-based techniques: 1) filter-based techniques cost less computationally than wrapper-based techniques. 2) filter-based techniques are not classifier-specific. 3) more importantly, unlike the typical 'black-box' trait of wrapper-based techniques, filter-based techniques provide a clear picture of why a certain feature subset is chosen as the predictor set through the use of scoring methods in which inherent characteristic of the predictor set  is optimized. the last advantage is particularly crucial since the predictor set scoring method in a filter-based technique can explain the prediction ability of the predictor set, whereas in a wrapper-based technique, the score of goodness of the predictor set is its prediction ability itself, and hence the term 'black-box'.

an important principle behind most filter-based feature selection techniques can be summarized by the following statement: a good predictor set should contain features highly correlated with the target class distinction, and yet uncorrelated with each other  <cit> . the predictor set attribute referred to in the first part of this statement, 'relevance', is the backbone of simple rank-based feature selection techniques. the aspect alluded to in the second part, 'redundancy', refers to pairwise relationships between all pairs of genes in the predictor set.

previous studies  <cit>  have based their feature selection techniques on the concept of relevance and redundancy having equal importance in the formation of a good predictor set. we call the predictor set scoring methods used in such correlation-based feature selection techniques equal-priorities scoring methods. on the other hand, guyon and elisseeff   <cit>  demonstrated using a 2-class problem that seemingly redundant features may improve the discriminant power of the predictor set instead, although it remains to be seen how this scales up to multiclass domains with thousands of features. a study was implemented on the effect of varying the importance of redundancy in predictor set evaluation in  <cit> . however, due to its use of a relevance score that is inapplicable to multiclass problems, the study was limited to only binary classification.

from here, we can state the three levels of filter-based feature selection for multiclass tumor classification as follows: 1) no selection, 2) select based on relevance alone, and finally, 3) select based on relevance and redundancy. thus, currently, relevance and redundancy are the two existing criteria which have ever been used in predictor set scoring methods for multiclass tumor classification.

contributions of this study
we propose to go one step further, by introducing a third criterion: the relative importance placed between relevance and redundancy. we call this criterion the degree of differential prioritization . ddp compels the search method to prioritize the optimization of one of the two criteria  at the cost of the optimization of the other. unlike other existing correlation-based techniques, our proposed feature selection techniques do not take for granted that the optimizations of both elements of relevance and redundancy are to have equal priorities in the search for the predictor set  <cit> .

having introduced the element of differential prioritization, we stress the importance of applying a more appropriate evaluation procedure which gives more realistic estimates of accuracy than the internal cross validation  procedure used in several feature selection studies for gene expression data  <cit> . this is done by evaluating our feature selection techniques using the f-splits evaluation procedure.

in this paper, we investigate the efficacy of two ddp-based predictor set scoring methods on nine multiclass microarray datasets. each of the two methods is differentiated from the other method by the measure of correlations between genes used in the method. the first method is termed the antiredundancy-based wa,s scoring method. the measure of antiredundancy, us, is used as the measure of correlations between genes in the wa,s scoring method. in the second method, called the redundancy-based wr,s scoring method, the measure of redundancy, rs, is used as the measure of correlations between genes. the ddp parameters for the wa,s and the wr,s scoring methods are denoted as α and ρ respectively. larger ddp means more emphasis on optimizing relevance, vs, and less emphasis on minimizing correlations between genes in the predictor set. conversely, smaller ddp indicates more emphasis on minimizing correlations between members of the predictor set and less emphasis on optimizing its relevance.

the main contribution of this study is to show that a degree of freedom in adjusting the priorities between maximizing relevance and minimizing redundancy is necessary to produce the best classification performance . a secondary contribution is to determine which one of the two measures investigated in this study is the better measure of correlations between genes in the predictor set .

RESULTS
nine multiclass microarray datasets are used as benchmark datasets . the brown  dataset, first analyzed by munagala et al.   <cit> , includes  <dig> broad cancer types. we also analyzed another version of this dataset, denoted as brn <dig>  where one class  is excluded due to its small sample size.

the gcm dataset  <cit>  contains  <dig> tumor classes. for the nci <dig> dataset  <cit> , only  <dig> tumor classes are analyzed; the  <dig> samples of the prostate class are excluded due to small class size.

the pdl dataset  <cit>  consists of  <dig> classes, each class representing a diagnostic group of childhood leukemia. the srbc dataset  <cit>  consists of  <dig> subtypes of small, round, blue cell tumors . in the 5-class lung dataset  <cit> ,  <dig> classes are subtypes of lung cancer; the fifth class consists of normal samples.

the mll dataset  <cit>  contains  <dig> subtypes of leukemia: all, mll and aml. the aml/all dataset  <cit>  also contains  <dig> subtypes of leukemia: aml, b-cell and t-cell all.

except for the brn, brn <dig> and srbc datasets , datasets are preprocessed and normalized based on the recommended procedures in  <cit>  for affymetrix and cdna microarray data. except for the gcm dataset, for which the original ratio of training set size to test set size used in  <cit>  is maintained to enable comparison with previous studies, for all datasets we employ the standard 2: <dig> split ratio.

different values of α and ρ ranging from  <dig>  up to  <dig> with equal intervals of  <dig>  are tested. predictor sets ranging from size p =  <dig> to p = pmax are formed in each split. the number of splits, f is set to  <dig> in this study. the choice of the value of pmax is based on previous studies on feature selection in tumor classification such as  <cit> , where it is observed that there is no significant change in accuracy at values of p beyond  <dig>  therefore, for datasets with larger number of classes , we set pmax to  <dig>  while for datasets with  <dig> or less classes, the value of pmax is decreased accordingly in proportion to k . the rationale for this is that datasets with smaller number of classes need less predictor genes to differentiate samples from different classes than datasets with larger number of classes. it is easier to distinguish among say,  <dig> classes than telling apart samples from  <dig> different classes .

two feature selection experiments were run on each dataset: one using the wa,s scoring method and the other, the wr,s scoring method. the dagsvm classifier is used in evaluating the performance of all resulting predictor sets from both experiments. the dagsvm is an all-pairs svm-based multi-classifier which uses substantially less training time compared to either the standard algorithm or max wins, and has been shown to produce accuracy comparable to both of these algorithms  <cit> .

two parameters will be used to evaluate the performance of the wa,s and the wr,s scoring methods. the first is the best estimate of accuracy. this is simply taken as the largest among the accuracies obtained from figure  <dig> at all tested values of α or ρ at p = pmax. by taking the accuracy at fixed value of p , we further exclude the possibility of leaking information from the test set into the training process of forming the predictor set in each split. if a draw occurs in terms of the estimate of accuracy, we take the average of the values of α or ρ giving the largest accuracies as the optimal α or ρ.

for multiclass classification problems, merely attaining a good estimate of accuracy does not represent excellent classification performance. there is also the need to ensure that samples from all classes are predicted with equally good rate of accuracy. this is especially true when class sizes are greatly unequal among the classes, which is often the case for multiclass microarray datasets. a predictor set may achieve high estimate of overall accuracy by simply predicting test samples as belonging to one of the classes with large class size at a high frequency. the end results will be that samples belonging to certain classes will be correctly predicted most of the time, while samples from other classes will be wrongly classified at a high rate.

this calls for the second parameter, the range of class accuracies, in evaluating the performance of the predictor set scoring methods. for each class, class accuracy denotes the ratio of correctly classified samples of that class to the class size in the test set. each class accuracy is computed from the ddp value which produces the best estimate of accuracy at p = pmax in the first place. the range of class accuracies is the difference between the best class accuracy and the worst class accuracy among the k class accuracies in a k-class dataset. in an ideal situation, overall accuracy being exactly  <dig>  each class accuracy is  <dig>  so the perfect range of class accuracies is  <dig>  hence, the lower the range of class accuracies, the better the classification performance.

best estimate of accuracy
overall the wa,s scoring method outperforms the wr,s scoring method by giving better accuracy in six out of nine datasets . only in three datasets, gcm, nci <dig> and srbc datasets, does the wr,s scoring method give the same accuracy as the wa,s scoring method.

range of class accuracies
the wa,s scoring method gives better performance than the wr,s scoring method by yielding smaller range of class accuracies for five datasets: gcm, nci <dig>  pdl, mll and aml/all datasets . the wr,s scoring method turns out lower range of class accuracies for only two datasets: the lung and brn <dig> datasets. for the remaining two datasets , both methods yield the same performance.

comparing the wa,s and the wr,s scoring methods
by taking the rightmost columns of tables  <dig> and  <dig>  we assign the overall superior method for each of the nine datasets in table  <dig>  at p = pmax, the wa,s scoring method is superior to the wr,s scoring method for six out of nine datasets . four of these six datasets contain large number of classes . the overall superior method is undecided for two datasets , while for the srbc dataset, both methods produce equal performance in terms of both best estimate of accuracy and range of class accuracies.

both methods have been briefly compared in a previous work  <cit>  using only one dataset . here we add the binomial test recommended in  <cit>  for comparing classifiers in order to compare both methods for all values of p ranging from  <dig> to pmax. for each predictor set size p =  <dig> ,...,pmax, we identify the ddp  value which gives the best accuracy  for each scoring method. in each split, a classifier is constructed using the p-gene predictor set obtained at this optimal ddp value from each scoring method. we then compare the two resulting classifiers across splits using the test sets of all f splits.

out of these pmax-  <dig> comparisons, for each dataset we record the number of times, a, we reject the null hypothesis that both scoring methods are equal, in favor of the hypothesis that the wa,s scoring method is better than the wr,s scoring method at the  <dig>  significance level . the outcome of the comparisons does not seem impressive until we take into account the fact that the number of times, b, we reject the null hypothesis that both scoring methods are equal, in favor of the hypothesis that the wa,s scoring method is worse than the wr,s scoring method, is  <dig> for all datasets. moreover, we observe a strong correlation between the training set size  and a – the larger the training set, the higher the frequency at which the null hypothesis can be rejected in favor of the wa,s scoring method as the superior method . therefore, we believe that with sufficiently large training set, it can be irrefutably proven that the wa,s scoring method is the superior method.

to reinforce the results from the binomial test, we further conduct the wilcoxon signed rank test  <cit>  on accuracies from both methods obtained at the best ddp for each predictor set size p =  <dig> ,...,pmax. the rightmost column of table  <dig> contains the number of times, c, the right-sided p-value is below the significance level of  <dig> . the right-sided p-value in this case is the probability that t <dig> is less than or equal to the wilcoxon signed rank test statistic, t.

t=∑p=2pmax⁡sign⋅rank     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgubavcqgh9aqpdaaewbqaagqaaiab=nhazjab=lgapjab=dganjab=5gaujabcicaoggaciab+h7axnaabaaaleaacqwgqbauaeqaaogaeyoei0iae43tdg2aasbaasqaaiabdcfaqbqabagccqggpaqkcqghfly1cqwfsbgucqwfhbqycqwfubgbcqwfrbwacqggoaakcqwgqbaucqggpaqkcawljagaaczcaiabcicaoiabigdaxiabcmcapawcbagaemiuaalaeyypa0jaegomaidabagaemiuaa1aasbaawqaaigbc2gatjabcggahjabciha4bqabaaaniabgghildaaaa@5637@

where θp is the number of test samples the wa,s scoring method predicts correctly at predictor set size p for all f splits, and ηp is the number of test samples the wr,s scoring method predicts correctly at predictor set size p for all f splits. rank is the rank of . t <dig> is computed in a similar way to t, except that the expression sign  is replaced with independent random sign. this means that the right-sided p-value represents the probability that the wa,s scoring method is only superior to the wr,s scoring method by random chance, given available observations of the classification performance of the two scoring methods. in other words, a right-sided p-value that is near zero  indicates a high likelihood that the wa,s scoring method is indeed significantly superior to the wr,s scoring method.

the left-sided p-value represents the probability that t <dig> is greater than or equal to t. supporting the results from the binomial test, the number of times, d, the left-sided p-value is below the significance level of  <dig> , is also  <dig> for all nine datasets. moreover, as in the case of a, c is also proportionate to the training set size, mt, of the corresponding datasets . indeed, as shown in figure  <dig>  the ratio of c to pmax -  <dig> has stronger correlation to mt than the ratio of a to pmax -  <dig> 

discussion
comparisons to other studies
detailed comparisons to previously reported results will only be made for the four datasets with the largest k. two of them, the gcm and nci <dig> datasets, have been extensively analyzed in previous studies and have been known to consistently produce low realistic estimates of accuracy   <cit> . since the wa,s scoring method has been shown to outperform the wr,s scoring method, we shall compare results from the wa,s scoring method against results from other studies.

for the gcm dataset, with a 150-gene predictor set, an accuracy of  <dig> % is achievable with our wa,s scoring method when the value of α is set to  <dig> . this is a significant improvement compared to the 78% accuracy obtained, using all available  <dig> genes, in the original analysis of the same dataset  <cit> . however, strict comparison cannot be made against this 78% accuracy of  <cit>  and the  <dig> % accuracy  achieved in  <cit>  since the evaluation procedure in both studies is based on a single  split of the dataset. we can make a more appropriate comparison, however, against a comprehensive study on various rank-based feature selection techniques  <cit> . the study uses external 4-fold cross validation to evaluate classification performance. in  <cit> , the best accuracy for the gcm dataset is  <dig> %, when no feature selection is applied prior to classification!

for the nci <dig> dataset, using the wa,s scoring method, the best 10-splits accuracy of 68% occurs at α =  <dig> . this is only marginally better than the best accuracy obtained from the two studies employing evaluation procedures similar to ours  <cit> . in  <cit> , the best averaged accuracy is around 67% , whereas the study by li et al.   <cit>  gives similar performance with an accuracy of  <dig> % achieved using the sum minority rank-based feature selection technique with the same number of genes as our predictor set,  <dig>  a more recent study on a wrapper-based feature selection technique  <cit>  found a loocv  accuracy of  <dig> % for the nci <dig> dataset. unfortunately no separate test set has been formed to validate the efficacy of the 30-gene predictor set which has achieved this accuracy.

the classification of samples from the nci <dig> dataset is well-known for being a difficult problem. major culprits include the small class sizes and the heterogeneity of some of the classes   <cit> . using the current interval size between the ddp values , our improvement of accuracy for this dataset is small. however, we hope that further refinement to our feature selection technique  will bring about a significantly better accuracy.

the discriminative margin clustering method used on the brn dataset in  <cit>  is geared towards discovering sub-classes in broad histological types – but manages to yield good class accuracy  for four tumor classes . not surprisingly, the class accuracy produced by our wa,s scoring method for each of these classes ranges from  <dig> to 100%. for this dataset, we obtain a  <dig> % accuracy using a 150-gene predictor set found at α =  <dig> .

this is better than the results reported in  <cit>  where an  <dig> % loocv accuracy on the brn dataset is achieved using a wrapper-based ga/svm feature selection technique. however, if the loocv accuracy itself is used as the ga fitness function, as is the case in  <cit> , an external test set should have been set aside to evaluate the performance of the technique. it is the accuracy from this test set that provides a realistic estimate of accuracy for the feature selection technique . again, similar to the situation in an aforementioned study on the nci <dig> dataset  <cit> , no such evaluation procedure has been implemented in  <cit> .

in  <cit>  the authors have eliminated the skin tissue samples from the 15-class brn dataset , also possibly, as in our case, due to small class size . in that study, the nearest shrunken centroid classifier yields a 10-splits accuracy of  <dig> % using  <dig> genes for the brn <dig> dataset. this is slightly lower than the  <dig> % accuracy from the wa,s scoring method . more importantly, we use a much smaller predictor set  to achieve a better accuracy.

therefore, compared to previous studies which used realistic evaluation procedures similar to the f-splits evaluation procedure, rather than the potentially overly optimistic icv procedure, our wa,s scoring method has produced better classification performance on highly multiclass datasets such as the brn, brn <dig>  gcm and nci <dig> datasets.

the importance of the ddp
in both scoring methods, it is worth noting from figures  <dig> and  <dig>  and table  <dig> that the best classification performance  is not always achieved at values of the ddp where the technique becomes equal-priorities scoring method  or rank-based . therefore, without varying the ddp so that it takes any other values aside from  <dig>  or  <dig>  the optimal classification performance would not have been achievable for most of the datasets. for datasets where the optimal value of the ddp happens to be exactly  <dig>  , it is due to the fact that some characteristic of the dataset dictate that the optimal value of the ddp for the dataset should be  <dig> .

in  <cit> , we have hypothesized that for a given scoring method, the value of the ddp leading to the best estimate of accuracy is dataset-specific. successfully predicting such optimal value of the ddp for a dataset gives us savings in terms of computational cost and time . linking the optimal value of the ddp to dataset characteristic is the first step towards successful prediction of the optimal value of the ddp for any future untested datasets.

since our feature selection technique does not explicitly predict the best p from the range of , in order to determine the value of the ddp most likely to produce the optimal accuracy, we use a parameter called size-averaged accuracy, which is computed as follows. for all predictor sets found using a particular value of the ddp, we plot the estimate of accuracy obtained from the procedure outlined in figure  <dig> against the value of p of the corresponding predictor set . the size-averaged accuracy for that value of the ddp is the area under the curve in figure  <dig> divided by the number of predictor sets, . the value of α or ρ associated with the highest size-averaged accuracy is deemed the empirical estimate of α* or ρ* . if there is a tie in terms of the highest size-averaged accuracy between different values of α or ρ the empirical estimate of α* or ρ* is taken as the average of those values of α or ρ  <cit> 

the overall trend in figure  <dig> implies that as k increases, in order to achieve the optimal classification performance, the emphasis on

• maximizing antiredundancy  or

• minimizing redundancy 

needs to be increased at the cost of the emphasis on maximizing relevance. conversely, maximizing antiredundancy  becomes less important as k decreases – thereby supporting the assertion in  <cit>  that redundancy does not hinder the discriminant power of the predictor set when k is  <dig>  the α* - k plot follows this trend more closely than the ρ* - k plot.

since the measure of antiredundancy, us, and the measure of redundancy, rs, play increasingly important roles compared to relevance, vs, as k increases, the better performance of the wa,s scoring method compared to the wr,s scoring method for majority of datasets with larger k  must be due to the superiority of the measure of antiredundancy, us, over the measure of redundancy, rs, in measuring correlations between predictor genes.

the statement above can be substantiated by comparing the corresponding value of α* to ρ* for each of these datasets in figure  <dig>  the value of α* is always less than the value of ρ* for all datasets. the role of α or ρ is such that the smaller the value of α or ρ, the more the emphasis on maximizing us or minimizing rs, respectively . this implies that us is more useful than rs as a criterion in finding the optimal predictor set for datasets with large k. moreover, we observe from figure  <dig> that the estimate of accuracy from the wr,s scoring method at small ρ is much lower than accuracy from the wa,s scoring method at small α, again underscoring the reliability of us over rs in finding the optimal predictor set.

most frequently selected genes
since the wa,s scoring method has been shown to outperform the wr,s scoring method, we perform the analysis on the most frequently selected genes using the optimal predictor sets found from the wa,s scoring method. the optimal predictor sets consist of the pmax-gene predictor set obtained in each split of training and test sets using the value of α which gives the best estimate of accuracy . the most frequently selected genes are identified by surveying the optimal predictor sets for genes that are selected  <dig> times out of  <dig> splits of training and test sets. we then rank these genes based on their split-averaged position in the predictor set. higher rank is assigned to genes with consistently high position in the predictor set in each split. the biological significance of the top  <dig> genes is briefly described in tables  <dig>   <dig> and  <dig> for the brn, brn <dig> and gcm datasets respectively .

for the brn and brn <dig> datasets, we first identify which of the  <dig> genes have been found to be markers for specific tumor types against normal tissues in the originating study by munagala et al.  <cit> . we discover that out of the  <dig> genes,  <dig> and  <dig> genes are included in lists of genes which differentiate specific tumor types from normal tissues in case of the brn  and brn <dig>  datasets respectively. these lists are available at the website  <cit>  of the authors of  <cit> .

based on existing literature regarding them  <cit> , the remaining  <dig>  and  <dig>  genes can be divided into four groups. the first group is similar to the aforementioned  <dig> and  <dig> genes; this group marks a specific cancer class against normal tissues. thus it is probable that genes which mark a specific tumor type against normal tissues also differentiate that tumor type from all other tumor types.

the second group comprises genes which are known to either promote or inhibit tumor in general . our results suggest that these genes are expressed variably among different tumor types as well as between tumor tissues and normal tissues.

the third group contains genes which are tissue-specific . examples are genes # <dig>  # <dig> and # <dig> in table 6; and genes # <dig>  # <dig> and # <dig> in table  <dig>  this is expected, as the classification problem involves distinguishing among different broad tumor types, each of which originates from a distinct tissue type.

the fourth group is made of unknown sequences and genes with either still-unidentified function  or general housekeeping roles such as production of normal proteins and gene regulation . in other words, these are genes that ostensibly play no role in influencing the development of tumor in general or specific tumor types. however, the identification of these genes as predictor genes for multiclass tumor classification points to the possible cascade effect of these genes in development of specific tumor types, especially in case of gene # <dig> in table  <dig> , which is involved in regulating the expression of other genes.

for the gcm dataset, we also first compare our top  <dig> genes to the marker genes identified in the originating study by ramaswamy et al.  <cit> . the authors of  <cit>  have provided a list of ova  marker genes for each tumor type at the paper website  <cit> . each list contains the top  <dig> genes which distinguish a specific tumor type against all other  <dig> tumor types in the gcm dataset. these genes are ranked based on their significance, which is computed using the permutation test elucidated in  <cit> . out of our top  <dig> genes,  <dig> genes  are included in the top  <dig> of one or more of ramaswamy et al.'s lists of ova marker genes .

of the remaining  <dig> genes, only  <dig> gene is not listed in any of the lists of the top  <dig> ova marker genes of  <cit> . this is gene # <dig> in table  <dig>  which belongs to the second of the four groups of genes defined previously. the other  <dig> genes belong to the first of the four groups of genes   <cit> .

CONCLUSIONS
for majority of the benchmark datasets, using the optimal value of the degree of differential prioritization gives an accuracy higher than accuracies obtainable using equal-priorities scoring method  or rank-based technique . therefore, instead of limiting ourselves to a fixed universal set of priorities for relevance and antiredundancy/redundancy  for all datasets, a suitable range of α or ρ should be chosen based on the characteristics of the dataset of interest in order to achieve the optimal accuracy.

furthermore, the study demonstrates the advantages of using the measure of antiredundancy over the measure of redundancy for measuring gene correlations, especially for datasets with large number of classes. based on the criteria of best estimate of accuracy and range of class accuracies, the antiredundancy-based predictor set scoring method performs better than the redundancy-based predictor set scoring method for majority of the benchmark datasets. furthermore, the antiredundancy-based predictor set scoring method is the superior method of the two in four of the datasets with the largest number of classes. these are the brn, pdl, gcm and nci <dig> datasets, the last two of which remain the most difficult datasets to work on in the area of tissue classification.

finally, a large portion of the genes most frequently selected into the optimal predictor sets has been identified by the originating studies on the corresponding datasets as marker genes of specific tumor types. majority of the most frequently selected genes have also been discovered to be involved in either development or suppression of specific tumor types by other studies. these findings confirm the practical value of our feature selection technique for the analysis of gene expression data.

