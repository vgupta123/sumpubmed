BACKGROUND
the fast progress in analyzing variations in the genome by deep sequencing has led to a plethora of high density genotyping arrays in many livestock species. thereby also, the amount of single nucleotide polymorphism  data that is available for analyzing the genetic relationships between different samples is constantly growing. one common approach to handle this type of data and to identify e.g. subpopulations, is the application of dimension reduction methods such as principle component analysis . currently pca is established to be the standard approach in clustering genotype data, see e.g.  <cit> . however, as we will demonstrate with an simulation example, there are drawbacks and pitfalls in the pca approach. in a pca, the principle components are ordered according to the variance they explain, but there is no theoretical justification that the component with the largest explained variance also contains the information required, e.g. to separate subgroups within the data. a vivid counterexample is a large hamburger. if it is large enough, the component with the largest variation goes through the diameter of the burger, but to separate the subgroups, one would need a direction from bottom to top. that means, even in this simple three-dimensional case the interesting component would be only the second one. hence, the interesting components might explain only a small fraction of the variance and consequently are easily missed by checking only the few first or last components. for an overview and a more theoretical background on the application of pca in genotype data, see  <cit>  or  <cit> .

other dimension reduction methods, such as invariant coordinate selection , are not commonly applied to genomic data. ics is a modern multivariate method originally introduced as generalized pca in  <cit> , but then established as ics in the seminal paper  <cit>  to avoid a name mismatch with a different generalized pca approach, see e.g.  <cit> .

the basic idea of ics is to use two different scatter matrices and to compare how they differ. different choices of scatter matrices lead then to different applications of ics. currently, ics has been e.g. applied to near-real time retrieval of low stratiform cloud coverage  <cit> . further, ics was used to enhance the discrimination between snow and ice clouds and detection of broken, thin clouds  <cit>  and also for studies of developmental canalization and the identification of divergent and stabilizing selection  <cit> .

to discuss possible problems with pca, we will first present a basic counterexample to show that pca does not necessarily identify cluster structures in a dataset and after that apply pca and ics on a real example genotype dataset. further, for both datasets we will compare the two methods also with other methods used by the bioinformatics community. for that we apply also t-distributed stochastic neighbor embedding   <cit> , isomap  <cit> , locally linear embedding   <cit> , kernel pca   <cit>  and diffusion maps   <cit>  to the simulated and the real data. for completeness, we also check the performance of a linear discriminant analysis  for the simulation example.

methods
simulated data
first we simulated a dataset as an example that pca is not always capable of detecting clusters in high-dimensional data. consider three 10-variate normal populations with n
 <dig>  where


μ
1=⊤,


μ
2=⊤ and


μ
3=⊤ and


Σ=diag <dig> , <dig> , <dig> , <dig> , <dig>  with μ
∗= <dig>  from each population we simulated then  <dig> samples. in order to hide the clearly visible subpopulations, we further rotated the simulated observations with a random orthogonal matrix. note that the rotation has no impact on the performance of pca as the method is rotation invariant.

additional file 1: figure s <dig> shows the simulated data before the rotation and additional file 1: figure s <dig> the data after it. in the latter one, the groups are clearly not visible anymore.

chicken data
the high-density genotype data consists of  <dig> chicken from  <dig> generations. the last generation is the largest group with  <dig> samples. the other generations contain  <dig>   <dig> and  <dig> samples. the data consists of sequence based variation data from  <dig> genomic regions, covering approx. 35% of the genome. the regions have been preselected based on previous studies as containing loci affecting egg-quality traits, see  <cit>  and  <cit> . as reference genome we used galgal <dig> 

in total there were  <dig>  genotypes measured in those regions. see additional file 1: figure s <dig> for the locations of the used regions on the chicken reference genome.

in addition to the genotype data, also a set of  <dig> different breeding values was available for all chicken. these were, besides others, egg production in period  <dig> to  <dig>  egg production in period  <dig> to  <dig> and feed intake. we use this data as real data example and will follow up on the biological findings only for one detected subgroup in order to keep the focus on the method.

invariant coordinate selection
to explain ics we need first to introduce the concept of scatter matrix. for a p-variate random vector x any p×p matrix-valued estimator s is called a scatter matrix if it affine equivariant in the sense that 
 s=asa⊤,  for any full-rank p×p matrix a and any p-variate vector b. clearly the regular covariance matrix cov is a scatter matrix. but especially in the robust statistics literature many other scatter matrices were introduced. for more details about how scatter matrices generalize the covariance matrix and many related references see  <cit> . a scatter matrix we will use later is the so-called scatter matrix of fourth moments 
 cov4=1p+2er2))⊤,  where r=||cov−1/2)|| and ||·|| denotes the frobenius norm.

the main idea of ics is to compare two different scatter matrices s
 <dig> and s
 <dig> by solving the following eigenvector-eigenvalue problem 
 s1−1s2b⊤=b⊤d,  where d is then the diagonal matrix containing the p eigenvalues of s1−1s <dig> in decreasing order. the rows of b contain then the corresponding eigenvectors. for convenience of notation we will denote from now on s
1=s
 <dig>  s
2=s
 <dig>  b=b and d=d.

the ics equation above can be seen as the problem of jointly diagonalizing the two scatter matrices, i.e. find b and d such that 
 bs1b⊤=ipandbs2b⊤=d. 


an interpretation can then be given as follows. first s
 <dig> is used to whiten the data, i.e. uncorrelate the variables and standardize the scales. then perform on the whitened data pca using s
 <dig>  therefore the idea is to see if s
 <dig> finds still some interesting structure after removing second order information as measured by s
 <dig> 

the transformation bx yields then an invariant coordinate system in the sense that 
 bx=b,  where equality holds up to marginal signs for any full rank p×p matrix a. the new vector z=bx is then usually referred to as the invariant coordinates.

the univariate concept of kurtosis can be seen as the ratio of two  scale measures and similarly s1−1s <dig> can hence be seen as a multivariate extension of this concept. therefore the eigenvalues contained in d can be interpreted as generalized kurtosis values as measured by s
 <dig> and s
 <dig>  in the special case of s
1=cov and s
2=cov <dig> it can be shown that the diagonal elements in d are a linear function of the classical measures of kurtosis of the components in z  <cit> .

and for example when searching clusters it is well-known that large clusters can be found often in directions with small kurtosis and outliers and small clusters in directions with large kurtosis. this means that invariant coordinates are very suitable for searching for groups as the components are ordered according to their  kurtosis. as actually  <cit>  show, in the context of mixtures of elliptical distributions with proportional scatter matrices, ics finds fisher’s linear discriminant subspace without knowing the group memberships. hence, when using ics for exploratory data analysis usually most attention is paid to the components with extreme generalized kurtosis values, like for example the first 3– <dig> and last 3– <dig> components. for more details about ics see  <cit> .

as practical considerations we would however like to point out that there is no general best combination of scatter matrices and the performance might depend on the choice of s
 <dig> and s
 <dig>  the choice s
1=cov and s
2=cov <dig> is however well-established and for example also a solution to the independent component problem  if x follows it . ica has been applied in the context of genetic data e.g. in  <cit> .

furthermore, ics is however currently limited to the case when p<n− <dig> as otherwise scatter matrices are always proportional to each other, see  <cit>  for details. therefore if p≥n− <dig>  then one can for example first perform dimension reduction using pca, resulting in a n×n matrix where the n-th eigenvalue is zero. then ics is only applied to a subspace which is known to have variation and is of smaller dimension than n− <dig>  this is for example standard practice in many multivariate methods which are limited to the p≥n− <dig> case, like for example the high-dimensional noisy ica approaches  <cit> .

distance measure, distance groups and statistical testing
for the simulated data the classification decision based on the scatterplot matrices from pca and ics was done by applying a k-means algorithm to the desired components. the classification results of the different dimension reduction methods were then evaluated using the adjusted rand index  <cit> . in the real data example, the classification decision was done by visual inspection of the figures.

in order to calculate the genetic distance of two different groups in a region of interest, we followed a basic approach. assuming two subpopulations a and b have been identified in the data, we determined first at each loci l= <dig> ,… the most common genotypes for both groups and denote these g~a,l respective g~b,l. then, we compared if these genotypes match between the two groups, by setting g
l= <dig>  if g~a,l=g~b,l and  <dig> else. afterwards we calculated a moving average of length  <dig> across the data and calculated in each window the average level of agreement. let w=w
 <dig> w
 <dig> … be the set of all windows of length  <dig> with w
1=l
 <dig> …,l
 <dig> w
2=l
 <dig> …,l
 <dig>  the average level of agreement in window i is then x¯wi=∑∀l∈wigl/ <dig>  for the sake of simplicity, we calculated the moving average also across chromosomal borders.

for all windows w
i with level of agreement between two subpopulations x¯wi≤ <dig> , the individual distance of each individuum in the one group was calculated to the average of the other group. for that, we use again the most common genotype for each loci in the subpopulation coded as  <dig> , <dig> and then we calculated the manhattan distance of each individuum from the standard population to that.

testing for differences in the breeding values between the two subpopulations has been done by applying a two-sided mann-whitney test. significant breeding values  are further investigated with a directional test, as proposed by  <cit>  and implemented in the r-package gmwt  <cit> . the individual distance measure of the chicken from the main population to the subpopulation showed three types of chicken, those which are genetically close , those that are medium  and those that are far  away from the subpopulation. let f
p,c, f
p,m and f
p,f be the distributions of the three groups for a given phenotype p, we have then the following two testing problems in mind 
 h0:fp,c=fp,m=fp,fvsh1:fp,c≼stfp,m≼stfp,f 


or 
 h0:fp,c=fp,m=fp,fvsh2:fp,f≼stfp,m≼stfp,c  with ≼st being the stochastical ordering of the two distributions. two distributions f
 <dig> and f
 <dig> are stochastically ordered, if f
1≥f
 <dig> ∀x∈r and we write f
1≼st
f
 <dig>  these directional hypotheses have been used to test for a directional relationship between the similarity group and the different phenotypes.

RESULTS
to evaluate the performance of the different dimension reduction methods to unravel the original cluster structure, we first clustered the plain simulated data using k-means with the constrain of three classes . for the classification result, we then calculated the adjusted rand index for the 3× <dig> table between the original class labels and the result of the k-means clustering. next, we performed a pca, followed again by a k-means clustering using the first two components for classification. also for this classification result table we calculated the adjusted rand index. then we applied ics onto the dataset and calculated in the same way again the rand index for the k-means applied to the last two components. to compare the results to other popular dimension reduction methods, we applied also t-sne, isomap, lle, kpca and dm to the simulation data and calculated the corresponding rand indices. further, we searched with each dimension reduction method the same dimensionality, that was d= <dig> for the simulated and d= <dig> for the real chicken data.

the rand index for the clustering using the original data is  <dig> , the index for pca is  <dig>  and for ics it is  <dig> . in other words, the k-means clustering applied to the raw data does not detect any of the original groups and the pca only detects two groups, but mixes the second and third one. the ics method, however, recovers the original cluster structure to a large extent, indicated by an adjusted rand index of nearly one. see also fig.  <dig> that visualizes the cluster labels in the projected datasets for the k-means classifications applied to the different methods.
fig.  <dig> cluster labels of the k-means clustering for mixed data , the first two principle components  and the last two ics components . the true class labels are colored accordingly and the k-means classification is represented with different symbols




the results for the other dimension reduction methods were rather weak. whereas the t-sne method was almost as good as the ics , the four others clearly were outperformed by these two mothods. isomap had a rand index of  <dig> , lle had a value of  <dig> , dm had also only  <dig>  and the kpca method had with  <dig>  even a value smaller than the pca had. that means, none of these methods was able to fully recover the original data. the corresponding figures s4–s <dig> can be found in the additional file  <dig>  to calculate the t-sne we used the r-package tsne  <cit> , isomap is implemented in the r-package rdrtoolbox  <cit>  and lle in lle  <cit> . for kpca we used the kernmap package  <cit>  and for dm the destiny package  <cit> .

the lda function applied to the simulation data resulted in an error-free separation of the data and had consequently a rand index of  <dig>  however, the ics method is with  <dig>  not too far away from that optimum. in absolute numbers,  <dig> out of  <dig> observations were mislabeled using the ics function. lda cannot be applied to the real example data, as the identification of subgroups is done without any prior knowledge and as such supervised methods like lda cannot be applied to the problem.

to analyze the real chicken data using pca, we applied the snpgdspca function of the snprelate  <cit>  r-package to it. figure  <dig> shows the scatter plot matrix of the ten first components, but no particular subgroup could be identified. the pca identifies only two strongly deviating individuals. next we determined the number of eigenvectors that account for a total variance of 80%.
fig.  <dig> scatterplot matrix of the pca analysis. no particular subgroup could be identified. the first component detects only two outlying observations




we plugged the corresponding matrix with the first  <dig> eigenvectors from the eigen-decomposition of the pca into the ics function of the ics  <cit>  r-package. we applied the ics function using the regular covariance matrix and the covariance matrix of forth order moments , as described above. by using this method we could clearly identify two subgroups in the last components of the ics as well as deviating individuals in the first components. one subpopulation is separated by the antepenultimate component . this subpopulation of  <dig> individuals is marked in red and green in the scatterplot matrix of the ics components, see fig.  <dig>  further, we could also identify another possible subgroup of size  <dig> by projecting the data onto the penultimate component , indicated in blue. we do not follow up on the individual outliers identified in the first components as the current goal was subgroup detection.
fig.  <dig> scatterplot matrix of the ics analysis. clear subgroups could be identified in component  <dig> and  <dig>  all members of the subgroup  <dig> have the same father, but two different mothers, indicated by red and green . another subgroup could be identified in component  <dig> 




before analyzing the phenotypical particularities of the identified subgroup, we also test the performance of the other dimension reduction methods on the real chicken data. here, kpca and lle are able to identify the same clusters as ics does, but t-sne and isomap fail to identify any clear cluster structures. in case of t-sne we tried both k= <dig> and k= <dig>  but in neither case any obvious subgroup could be identifed. diffusion map, however, apparently identifies another subgroup. the corresponding scatterplot matrices can also be found in the additional file  <dig>  we used the default settings and protocols as provided by the different packages. that means, e.g. for lle we calculated the optimal number of neighbors as  <dig> 

members of the red subgroup, identified by the ics method are all offsprings from the same father and mainly from the same mother. from the  <dig> members of the subgroup only one individual  has a different mother. the subpopulation indicated in blue is also formed by a family. seven chicken from this population have the same father and mother. further, the father of those  <dig> chicken can also be found in this group.

a region of approximate length 4mb , containing  <dig> snps was identified by calculating the genetic similarity between the deviating  family and the remaining population. the genetic similarity was calculated with a moving average using windows of size  <dig> kb. areas, where the average level of agreement drops below  <dig>  are considered to be the major cause for the difference between the divergent red family and the main population. additional file 1: figure s <dig> shows the level of agreement across the considered chromosomal regions. also for the blue subpopulation we could identify in a candidate region a similar way.

next, we calculated for each chicken within the main population the manhattan distance between the mode genotype values of the deviating red family in the region of interest and the individual genotypes. there we could clearly identify three subgroups within the main population, see additional file 1: figure s <dig>  we denote those subgroups as close, intermediate and far.

when breeding values of  <dig> production values were compared between the red subpopulation and the main population, significant differences were seen in  <dig> traits . these were then tested further using a generalized mann-whitney test for directional alternatives. this means, we tested for a directional trend of the phenotypes with respect to the close, the intermediate and the far group.

for six breeding values a directional relationship in the main population could also be verified. especially the production values followed a directional order, see the corresponding boxplots in fig.  <dig>  in details that means that the red subgroup had a significant higher egg production compared to the main group and within the main group the chicken that are genetically closer to the subgroup in an identified region also had a higher production compared to those that are genetically further away. however, the increased production values occurred with a higher feed intake.
fig.  <dig> boxplot of production values p <dig>  and p <dig>  a clear directional relationship between the subpopulation and the three distance groups close, medium and far. in both production periods have chickens that are in the identified region closer to the subpopulation also higher production values




discussion
we applied the modern dimension reduction method ics to a simulation example and compared it to the commonly used pca method to visualize some deficiency of the pca approach. further, we applied the other, modern dimension reduction methods t-sne, isomap, lle, kpca and dm to the simulation data. here, in the controlled environment we could clearly see that the pca method was not able to identify all three true groups in the simulated data, but the ics method, however, was. from the other tested methods, only t-sne was able to recover all three subgroups, but all other four tested methods failed doing so. some of them separated a single subgroup, but mixed the remaining two groups into a single large cluster. when the methods were then applied to a high-density genotype chicken data, the pca method could not identify any subgroups. the ics method clearly identified two subgroups consisting of  <dig>  respective  <dig> samples, that share the same family background. two  of the five other methods, however, also detected the same subgroups in the real chicken data. the other three methods failed to identify any clear cluster structures.

in the scatterplot matrices some outlying observations could be identified by t-sne , but not as evident as in the ics case. a closer look at component  <dig> showed e.g., that some of the chicken with a value larger  <dig> are related but the most of them are unrelated. in terms of calculation times, the ics needed around  <dig>  s, whereas the t-sne run took around three minutes. the other used methods needed at most only a few seconds for the calculation.

we considered also the red subgroup identified by ics closer. it was superior in more than half of the available breeding values compared to the standard chicken population. within the standard chicken population we could identify three subgroups that were either genetically close, intermediate or far away from this subgroup based on the most deviating chromosomal region. in addition, these three groups of the main population showed a directional trend in many traits, especially in the important production values p <dig> and p <dig>  also the blue subgroup is deviating in five breeding values from the main population, including the production values p <dig> 

there were no other combinations with those parents in the data available so that no further investigations could be conducted to identify the reason for the subgroups to behave in such a different way. the biological explanation for the difference is beyond the scope of this paper.

the identification of three groups  within the data is remarkable. as all the chicken originate from the same line, one would not assume any subpopulation structures and by applying a pca to it, we did not identify any. ics identified two subpopulations that were thereafter also seen to differ from the main population for some of the phenotypes for production traits.

further, we could identify strongly deviating genetic regions between the subpopulations and the main group and followed exemplary up on the one that corresponds to the red subgroup. within that, we calculated then the genetic distance of the remaining chicken to the identified subpopulation and could see that chicken genetically more similar with regard to the deviating region to the subpopulation also have better production values. moreover, we could identify a directional relationship between the genetic similarity in that region and certain production values.

CONCLUSIONS
we presented here an alternative dimension reduction method that is already used in other scientific fields, but that has not yet made its way to the genomic community. however, although ics is superior over pca in the current scenario, its purpose is not to replace pca or any other dimension reduction method, but it is rather considered to be another tool in the dense genotype data analysis toolbox.

its good results for both, the simulation and the real dataset encourage its use also for other genomic datasets to further evaluate its performance in a larger scale. compared to other, modern dimension reduction methods, we saw that there is a large variation in the performance of each method, depending on the dataset. for our data, only ics showed good results in the simulation as well as in the real data set, isomap and diffusion map had the weakest results for both setups. t-sne only performed well in the simulation setup and lle only for the real data.

additional file

additional file  <dig> the supplemental material contains additional figures. included figure files are provided below. figure s <dig>  scatterplot matrix of the unmixed simulation data. figure s <dig>  scatterplot matrix of the rotated simulation data. figure s <dig>  visualization of genomic regions that have been used for the analysis. figure s <dig>  scatterplot with cluster labels of the k-means clustering for t-sne, isomap and the lle. figure s <dig>  scatterplot with cluster labels of the k-means clustering for diffusion maps and kernel pca. figure s <dig>  scatterplot matrix for t-sne output with k= <dig> applied to the real chicken data. figure s <dig>  scatterplot matrix for t-sne output with k= <dig> applied to the real chicken data. figure s <dig>  scatterplot matrix of the first  <dig> components of the lle output applied to the real chicken data. figure s <dig>  scatterplot matrix of the first  <dig> components of the isomap output applied to the real chicken data. figure s <dig>  scatterplot matrix of the first  <dig> components of the kpca output applid to the real chicken data. figure s <dig>  scatterplot matrix of the first  <dig> components of the dm output applied to the real chicken data. figure s <dig>  the values of the level of agreement across the regions of interest. figure s <dig>  the individual genetic distances between main population to the mode of the subpopulation. 




abbreviations
dmdiffusion maps

icaindependent component analysis

icsinvariant coordinate selection

kpcakernel pca

ldalinear discriminant analysis

llelocally linear embedding

pcaprinciple component analysis

snpsingle nucleotide polymorphism

t-snet-distributed stochastic neighbor embedding

