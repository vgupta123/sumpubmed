BACKGROUND
recent advances in high-throughput sequencing technology have provided a mechanism to gain genomics insight on species without a complete genome sequence by generating expressed sequence tags collections . ests are single-pass, partial sequences obtained from randomly selected complementary dna  clones and need to be processed and annotated to provide a biologically relevant data set. they include low-quality and vector regions that must be identified and removed to obtain high-quality, clean sequences suitable for further analysis. in addition, due to the random selection of the sequenced cdna clones, a clustering step is needed to obtain a non-redundant set of unique consensus sequences, or unigenes. finally, functional and structural annotation of the unigenes is required in order to add relevant biological information to the sequences. all these data must be conveniently stored and organized in a structured database, and interfaces must be set up for end-users to efficiently retrieve and mine all these data.

due to the high number of sequences in most ests datasets, different computer-based methods are required to process, annotate, record, display, and retrieve the data. these methods are applied sequentially from the input raw sequence data to the final searchable, fully annotated est database, and knowledge of computing science is needed to arrange them in an efficient and reliable analysis pipeline. the usual approach to this problem is to build an in-house prepared set of script programs that semi-automates the analysis. this solution requires highly skilled bioinformatics staff capable of programming and using the scripts, and is inefficient because a different pipeline should be prepared for each project, and because the resulting semi-automated process is difficult to maintain and lacks reproducibility. it would therefore be more convenient to use a well tested, freely available automatic tool. in our opinion, this application should ideally have the following features: 1) to be fully automated in a pipeline covering all the steps from the input chromatogram files to a clean, annotated web-searchable est database, 2) to be highly modular and adaptable, 3) to be able to run in parallel in a personal computer  cluster, thus benefiting from the multiprocessing capabilities of these systems, 4) to use third-party freely-available programs, in order to ease the incorporation of the improvements made by others programmers, 5) to include a highly-configurable and extensible user-friendly interface to perform data mining by combining any search criteria, fitting the final user needs, and 6) to be based on an open source license to allow a continuous development by a community of users and programmers, as well as its customization for the particular needs of different projects.

some applications, including pipeonline  <cit> , estap  <cit> , partigene  <cit> , estima  <cit> , est-page  <cit> , parpest  <cit> , garsa  <cit> , or opensputnik  <cit> , have been proposed and they fulfill a certain number of the desired characteristics. however, as far as we know, none of these packages are endowed with all the requirements indicated above, especially the code availability, enabling costumization, and the automatic creation of a user-friendly web site to perform complex queries ready to be deployed in a production environment.

in an attempt to fulfill the need for an analysis software which accomplishes all the mentioned requirements, we have developed a software package, namely est2uni . this pipeline has been tested through three genomics projects which we are involved in: citrus functional genomics project  <cit> , chillpeach project  <cit> , and spanish melon genomics project  <cit> . est2uni uses a set of chromatogram files as input to produce a structured and annotated est database, as well as a web site to perform complex queries and data mining. configuration of the pipeline is done by just editing a single well-documented text file. after initial set up configuration, the pipeline is completely automated, and can be run in parallel in a pc cluster using the load distribution tool condor  <cit> . its modular structure provides an easy way to adapt the analysis to the special requirements of individual projects. furthermore, the code is designed to easily integrate well-tested, widely-used, freely available third-party tools, either as locally installed programs  or as web services . the software package is freely distributed  <cit>  under a gpl license, and can be easily installed in a standard linux system running apache http server, perl scripting language, mysql database management system, and php language.

implementation
the est2uni package consists of: 1) a set of perl modules that perform both the est analysis and the database creation, 2) a set of php scripts to generate the browseable database-interacting web pages, and 3) a set of php modules with the functions called by the php scripts. the est analyses are performed by the main perl script, which manages the execution of several third-party, freely-available, standard tools commonly used for est analysis, as well as a number of home-made analyses. the running parameters used by the pipeline and the external tools are stored in a well-documented, plain text configuration file which can be modified with a text editor. this configuration file contains all the required information to run the pipeline, like file paths, analyses to be performed, and paths and parameters for the programs installed to run the analyses. the software package is distributed with a single minimal-analysis example configuration file where default parameters used by est2uni and external tools can be inspected and modified if desired. running parameters considered as default ones by developers of the external tools used have been selected as a starting point in this example configuration file. each annotation analysis is controlled by a different perl module, and the results from different analyses are stored at independent tables in the database to facilitate the future addition of new methods and analyses. as a consequence, all analyses can be run independently at different times, and the annotation modules could be re-run at any time with updated external databases. parallelization is also managed by an independent perl module which splits the analysis in small tasks, manages their execution in different nodes, and join the results, using the load distribution tool condor, as explained above. it should however be easy to hack the code in order to use a different load distribution tool.

the package is distributed with a complete working web site built by est2uni using php. the visual design is controlled by using css, and special attention has been paid to the modularity of the php code, so that it is very easy to add new functionalities to the web pages and to customize their appearance. this adaptation should be easily performed by any project administrator because the php code appearing on the browseable web pages is minimal . mysql authentication can be used to maintain the privacy of the data of the different projects.

RESULTS
est analysis
a standard est analysis pipeline includes the following steps: 1) est pre-processing, 2) est assembly, and 3) functional and structural annotation of the resulting unigenes. below we describe the analysis and methods currently implemented in est2uni . however, the modular design of the package makes it easy to include new analyses. furthermore, it is possible to run only specific parts of the pipeline just by indicating it either in the configuration file or in the command line. next, we describe the steps followed by the est analysis pipeline.

pre-processing
pre-processing is the first step performed, and includes base calling, vector and low-quality regions removal, masking of repeats and low-complexity regions, and contaminant sequence detection and removal. the result is the generation of a clean, high-quality est sequence set. both chromatogram and fasta sequence files with or without quality scores are accepted as entry point to the analysis. base calling and quality score assignment from chromatograms are performed with phred  <cit> . low quality and cloning vector regions are removed from the sequences with lucy  <cit> , and repetitive elements and low complexity regions are masked with repeatmasker  <cit>  and seqclean  <cit> , respectively. for repeat masking, a taxon-specific repeats database can be used. unexpected vector sequences, probably coming from contamination by manipulation at the laboratory, are also removed with seqclean, using ncbi's univec database  <cit> .

assembly
clean, high-quality est sequences obtained in the pre-processing step are assembled in contigs and singletons in order to eliminate sequence redundancy and a non-redundant unigene set is obtained. either cap <dig>  <cit>  or tgicl  <cit>  can be used for the assembly step, and consensus sequences for the contigs are determined using the quality scores of the individual ests to resolve sequence discrepancies. a set of unigene clusters , grouping different unigenes with extensive sequence overlapping, can also be obtained. these superunigenes could represent gene families, alternative splicing or polymorphism. poly tails and open reading frames , predicted using estscan  <cit> , are used to reverse the sequences when necessary.

to assist in the design of a cdna microarray derived from the est collection, one cdna can be selected as the best representative for each unigene or superunigene. the criteria to choose this cdna are based on a number of user-defined restrictions  and/or preferences . when no clone in an unigene satisfies all the above mentioned restrictions, these can be progressively relaxed until a representative is selected. since clones from different unigenes in a superunigene are very similar, they are expected to have the same mrna targets under standard hybridization conditions when used as probes in cdna microarrays  <cit> , and superunigene representatives can also be selected to be printed in the microarray, which reduces spot cross-hybridizations.

structural annotation
unigenes can be annotated using different modules, following user specifications in the configuration file. ssr microsatellites can be detected with sputnik  <cit> . putative single nucleotide polymorphisms  can be found by using a locally developed algorithm. since ests have frequent sequencing errors, only positions with a quality score above a user-specified threshold value are considered, and sequence discrepancies between ests in the same contig are marked as putative true snps only if the polymorphism is confirmed by more than one est in the contig. when a file including a set of primer pairs is provided, in-silico pcr experiments can be performed with ipcress  <cit>  to integrate information about existing pcr-based molecular markers and associate these markers to specific unigenes. rflp information can also be integrated with the unigenes via the cdna clones. an rflp is a clone proven to be useful to identify differential hybridization band patterns when used as a probe against dna samples coming from different sources and digested with an specific restriction enzyme. a file with rflp names and corresponding cdna clones and restriction enzymes can be used to annotate unigenes coming from these rflp clones. finally, when cdna libraries are constructed using oligo-dt as a primer for the reverse transcriptase reaction, unigenes can be aligned with protein databases from species with a complete genome sequence to predict if there are full-length clones for each unigene.

functional annotation
for functional annotation, different similarity search tools can be used. unigene comparisons against a set of user-defined nucleotide and/or protein databases are carried out by blast  <cit>  using the search parameters indicated in the configuration file. the descriptions of the blast hits obtained with the different blast runs are then parsed to yield a descriptive annotation for each unigene. descriptions containing some word from a user-defined list including words as "unknown" or "hypothetical" are considered uninformative and unsuitable for annotation of the unigenes. the annotation is in the form "similar to" or "highly similar to", depending on the e value of the alignment with the corresponding blast hit. gene ontology classification  can also be obtained from blast against a set of user-defined go-annotated databases. a hmmer  <cit>  search against a pfam database  <cit>  can be used to locally search protein domains in each unigene. a bi-directional blast comparison can also be performed with a number of user-defined species-specific sequence databases in order to obtain a set of putative orthologs. in this analysis, two sequences are considered orthologs when each one is the first hit in a blast search with the other. finally, when est collections can have contaminant sequences coming from species other than the intended one, e.g. fungus sequences in plant est libraries made with infected plants, a blast analysis can be done using taxon-specific databases to predict the source organism and flag the putative contaminant sequences.

microarray expression integration
est2uni is ready to incorporate microarray expression data for each unigene. normalized expression data coming from different arrays and experiments can be easily added to the database and referred to the corresponding unigenes. database can also store details about biological samples, tissues and conditions used for each experiment, while referencial integrity is required to reproduce the standard work flux in microarray experiments. there are several ways of retrieving this expression data from the web site depending on the number of unigenes involved. a graphical representation of expression data in all the microarray experiments done is created for each unigene, which is accesible from the individual unigene web pages. bulk data retrieval is also posible, so that expression data for all unigenes or for any set of unigenes obtained in a query can be easily obtained. furthermore, this data can be directly sent to the gepas analysis suite  <cit>  or downloaded as a text file ready to be used with the mev software  <cit> .

parallelization
execution times in standard est analysis pipelines are usually very long because of the high number of sequences to be processed and the big computational costs of some analyses performed. est2uni can achieve a reduction in these execution times by parallelization, because it can be run either in sequential mode or in a parallel environment using a load distribution tool. when run in parallel mode, est2uni divides the analysis in several tasks that are asigned to different processing nodes and manages the execution of these tasks with the aim of keeping the different nodes busy, thus taking maximum advantages of the processing resources available in the system. these tasks are created by splitting the set of sequences to analyze in several chunks, and running independent subanalysis in parallel. est2uni sends these jobs to the load distribution tool cue and checks that they run as expected. the pipeline is distributed with the code necessary to use condor  <cit>  as the load distribution tool. however, the high modularity of the code makes it easy to adapt est2uni to use another load distribution tools, like openmosix  <cit>  or openpbs  <cit> .

although some of the work load is done by third-party tools with its own parallel implementations , we decided to use this est2uni-driven unique model of parallelization for the whole because this approach enables to run in parallel even those analysis tools which are not designed to be run in that way. moreover, because the different parallel tools have different parallelization infrastructures and requirements, the parallel code implemented in est2uni is directly applied to their sequential implementations, so avoiding use of the parallel versions. anyway, if a load distribution tool cannot be installed, the pipeline is also able to take advantage of parallelization by using the parallel versions of the third-party capable software like mpiblast  <cit>  or hmmer using pvm . this optimal use of the available resources significantly reduces the time required to complete an est analysis. figure  <dig> compares the time required by est2uni to completely perform the analysis of  <dig>  ests in sequential mode  with respect to the time required in parallel mode when using  <dig>   <dig>  and  <dig> biprocessor nodes. clearly, the time is considerably reduced when the number of nodes is increased. when  <dig> nodes are used, the time required is  <dig>  times lower than when the analysis is performed in sequential mode. this time is  <dig>  and  <dig>  times lower when using  <dig> and  <dig> nodes, respectively. this represents an almost linear reduction in the processing time as the number of nodes grows, which shows the high scalability achieved by the est2uni application.

regarding the ability of est2uni to properly manage parallelization in the analysis of big est collections, figure  <dig> shows the time required to finalize a complete analysis of different numbers of input est sequences using the pipeline in parallel mode with  <dig> biprocessor nodes. a set of  <dig>  ests were completely processed, assembled, and annotated in two days, and more than  <dig>  ests in just about one week, showing the robustness of the parallelization implemented. notice that while the number of input ests increases by a factor of  <dig>  the time needed to complete the full analysis increases only by a factor of  <dig> . the main reason for this is that some analyses are performed on unigenes rather than ests, and unigene number is not increased by the same factor than est number due to higher redundancy of bigger est collections. it should be noted, however, that an important part of the parallelization performed by est2uni is done before unigenes assembly, which highly contributes in that time reduction.

database
est2uni creates and populates a structured mysql relational database where the results of the different analyses are stored automatically by the pipeline. this database is key element for the pipeline, since the different analysis modules used in the pipeline get their input data from the database and write their output to it. the database contains information about all the data obtained, from cdna libraries, isolated clones, and raw sequences to analysis results, as well as additional information about genetic maps and markers or expression data provided by users, or the representative clone selected for each unigene. it can also include information about the researchers and research centers involved, as well as journal publications or public accession numbers of sequences derived from the project. access to the database can be password-protected in order to keep the data private.

web site
est2uni includes a web site interface to the database through a powerful data mining tool with an advanced querying interface and high integration among all kinds of data . it is not just a collection of simple tables showing the data, or a simple query form to search by using sequence identifiers or keywords. on the contrary, it allows combination of almost every different functional and structural annotation criteria in the queries. for example, it would be very easy to look for unigenes fulfilling all the following criteria: 1) to have ests coming from several given libraries, 2) to be annotated as transcription factors, 3) to have sequences with tri-nucleotide ssrs or snps, and 4) to be represented by a putative full-length clone. additionally, bulk queries using a file with a list of unigene names or orthologs are implemented.

the unigenes obtained as query results can be inspected individually, but also bulk downloads of the sequences, names or orthologues are allowed. in order to globally analyze the gene ontology terms of the result set, it is possible to automatically submit the orthologues in some model species to babelomics  <cit> . in a similar way, a text file with the expression data for the unigenes in the result set can be obtained, which can be used as a input for expression data analysis at specialized web sites like gepas  <cit> , a well-known, web-based, microarray data analysis service. the individual unigene web page view shows graphical and textual summaries of the assembly and annotation results . hyperlinks to the first hits of the external databases searched with blast are provided, as well as their descriptions and e values . the full blast results can also be retrieved. gene ontology annotation results are also shown in a table with links to the go-annotated similar external sequence used to transfer the go annotation. another table shows the sequence discrepancies among the ests in the unigene, so that putative snps can be evaluated by visual inspection . expression data for the unigene is also provided in this page. finally, primer <dig>  <cit>  is integrated into the web site, and it is possible to design primers and see the graphical results with just two mouse clicks.

the web site also provides some statistics of the full est collection, like contigs/singletons distribution, unigene length distribution, number of ests per unigene distribution, or number of unigenes annotated with the different functional and structural criteria. it also provides statistics for each single library, like number of ests, singletons, contigs, unigenes, novelty, and redundancy. tables for global and library-specific gene ontology annotations are also provided where unigenes annotated in each functional category can be directly retrieved, and libraries can be compared for the gene ontology annotation of their unigenes.

comparison with other tools
although other tools which perform est analysis and database creation have been developed, none of them accomplishes all the characteristics we consider essential for the research community . first, they must be freely available for download and local installation, as is the case for est2uni. some other tools , although promising, are not available for free download and local installation, and they must be used in collaboration with their maintainers under specific agreements. second, they should be based, when possible, on freely-available, open-sourced, widely-used standard external tools. all the functionalities provided by est2uni are based on such software tools, reducing its implementation costs and facilitating the incorporation of future improvements in these programs. this is not the case for some existing packages, which use proprietary tools like oracle  or repeatbeater  <cit>  . third, regarding the time efficiency of the software, only parpest  <cit>  and est2uni allow reduction of execution times by running the pipeline in a parallel environment. fourth, est2uni includes all the necessary steps for est analysis, i.e., pre-processing, clustering and annotation, arranged in a sequential, completely automated pipeline. similar existing tools provide at most different script programs  and/or different interfaces  which need to be applied sequentially to perform the different steps in the standard est analysis. other tools  provide only the database and its web interface . fifth, although all the similar existing tools perform some kind of functional annotation of est collection, none allows structural annotation based on searching for snps and microsatellites, as does est2uni. sixth, only est2uni provides a powerful data mining tool capable of complex queries by combining any structural and/or functional criteria to retrieve data. similar existing tools only provide static tables showing different statistics related to the est collection, and simple web forms to retrieve data with simple queries, such as est/unigene id or length, and blast hit description keywords. lastly, none of the other tools integrate expression data storage. to sum up, the est2uni package is, as far as we know, the only application able to meet all the requirements of the research community.

1it means that software package can be free and directly downloaded from internet for local installation and use.

2it means the ability to perform complex queries to the database using any combination of est/unigene annotations as a condition in the query clause.

CONCLUSIONS
we have developed an est analysis tool capable of converting, in a fully automatic and time-effective way, a set of trace files or plain sequences in a highly structured and annotated est database with a user-oriented web interface for efficient data mining. the est analysis pipeline includes standard pre-processing, clustering and annotation programs, and can incorporate gene expression data to the database. the software is highly modular, which facilitates the incorporation of new methods and analyses, meeting the needs of different est projects. running options are also easily adapted to local needs by simply modifying an extensively documented single configuration text file which provides the parameters to be used by the different analyses. once configured, the pipeline runs without user assistance, from the input files to the final annotated est database. the est analysis can be run either in a single standard computer, or in a pc-cluster, thus taking advantage of the multiprocessing capabilities of these systems, which allows reduction of the time required to complete the est analysis. the web site deployed is a powerful data mining tool with a complex, yet easy to use, query interface, that also provides functionalities for bulk data retrieval and download. it also eases the use of several tools, like primer design and blast searches against the database. access to the data can be restricted by passwords to keep the data private. the development team of est2uni is continuously improving the end-user interface, the quality of the analyses, and the integration with other tools. we have set up a public subversion server  <cit> , and a mailing list  <cit>  to allow collective development of the code, which everyone is invited to join.

to sum up, we conceive this bioinformatics tool as an open and evolving project, and all the bioinformatics community is invited to participate, using and improving the tool that we have created for our specific est projects .

availability and requirements
est2uni can be freely downloaded from internet  <cit> . detailed installation instructions are provided on the download web page and are also included in the software package. the application is free in the sense that it has been released under the gpl license, and its development is open and collaborative. any researcher is free to use it, to modify it, and to deploy their own web site with the results.

no hardware or memory restrictions are imposed by this software package other than those that apply to the external programs used in the pipeline. in general, it runs without problems in any standard medium-level ix86-based equipment using the linux operating system. we have tested and configured correctly this software in computers running the following gnu/linux distributions: fedora core  <dig>  ubuntu  <dig> , suse linux  <dig> , and debian sarge. it should run without problems in any unix installation as long as all the required software is installed. the following software, which come with any gnu/linux distribution, is absolutely required to run the est2uni pipeline: perl, apache, mysql, and php. est2uni also requires certain additional perl modules, specifically bioperl and dbi. in addition, the following external tools and resources are used to perform the different analysis: phred, lucy, repeatmasker, repbase, seqclean, ncbi's univec, tgicl, estscan, sputnik, ncbi blast, ipcress, and the perl module go-perl. for parallel processing, the load balancing tool condor must be installed. detailed instructions for installing each of these external programs are also provided with the package and at the est2uni download web site.

competing interests
the author declares that there are no competing interests.

authors' contributions
jmb and jf conceived and designed the pipeline structure, the database schema, and the web site, wrote the code, tested est2uni as end-users, and drafted the manuscript. fg wrote the code for parallelization, tested its efficiency, and drafted the parallelization section of the manuscript. ar managed the parallelization project, and critically revised the manuscript. vc and fn critically revised the manuscript, and coordinated the citrus functional genomics project and the spanish melon genomics project, respectively, which provided the est data which were utilized in the development and testing of est2uni. jmb guided and coordinated the development of est2uni. all authors read and approved the final manuscript.

