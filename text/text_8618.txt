BACKGROUND
snps, which make up over 90% of all human genetic variations  <cit> , contribute to phenotype differences and disease risk. due to their high frequency and binary variation patterns, snps have been widely used as generic markers in disease association studies to identify genes associated with both monogenic  <cit>  and complex diseases, such as diabetes  <cit> , autoimmune diseases  <cit> , cancers  <cit> , and alzheimer’s disease  <cit> . snps also serve as popular molecular markers in pharmacogenomic studies to understand inter-individual differences in response to treatments  <cit> . therefore, it is essential to obtain accurate snp information through advanced methods, such as, high throughput next-generation sequencing  technologies.

ngs technologies  have been widely used in the last several years  <cit> . a single sequencing run by an ngs platform can generate data in the gigabase-pair scale, which usually contains millions and even hundreds of millions of sequencing reads. this high throughput makes ngs technologies more suitable for snv identification compared to traditional technologies. however, challenges are also present. to produce such an enormous amount of data, multiple sequencing procedures  are involved in ngs technologies  <cit> . as a result, artifacts can be introduced by both systematic and random errors. these errors include mishandled templates, pcr amplification bias, and florescence noises. since the snv detection relies on the identification of polymorphisms at the level of individual base pairs, any sequencing error can lead to an incorrect snp identification. furthermore, other genetic variations  make accurate snp calling even more difficult.

in order to identify snvs using ngs data, various snp calling programs have been subsequently developed  <cit> . for a general survey on snp calling programs, please check the review paper by pabinger et al.  <cit> . these programs serve as useful tools to detect snps from high throughput sequencing data and greatly extend the scale and resolution of sequencing technology applications. our preliminary work has shown that for sequencing datasets that have high coverage and are of high quality, snp calling programs can perform similarly  <cit> . however, when the coverage level is low in a sequencing dataset, it is challenging to accurately call snvs  <cit> . moreover, commonly used snp calling programs  all include different metrics for each potential snp in their output files. these metrics are highly correlated in complex patterns, which make it challenging to select snps that are used for further experimental validations. in order to accurately detect snps from a low-coverage sequencing dataset, effective solutions have been in great demand. some studies have shown that incorporating haplotype information and other pooled information can help in identifying snps in multiple-sample datasets  <cit> . however, many pilot studies have a small sample size , so the multiple-sample methods cannot be applied. although the difficulty of snp calling using single-sample low-coverage sequencing data has been recognized, it is still unclear how well different snp calling algorithms perform and how to choose reliable snps from their results.

in this paper, we have conducted a systematic analysis using a single-sample low-coverage dataset to compare the performance of four commonly used snp calling algorithms: soapsnp, atlas-snp <dig>  samtools, and unified genotyper  in gatk. we have also explored the filtering choice based on the metrics reported in the output files of these algorithms. first, we improve the quality of the raw sequencing data by trimming off the low quality ends for reads in the data, then call snvs using the four algorithms on these trimmed sequencing reads. we compare the snv calling results from the four algorithms without using any post-output filters. second, we explore the values of a few key metrics related to snvs’ quality in each algorithm and use them as the post-output filtering criteria to filter out low quality snvs. third, we choose several cutoff values for the coverage of called snvs in order to increase the agreement among the four algorithms. with the above analysis procedure, our goal is to offer insights for efficient and accurate snv calling using a single-sample low-coverage sequencing dataset.

methods
part i reviewing the key features of snp calling algorithms
preprocessing steps of different snp calling algorithms
alignment  is a fundamental and crucial step of any ngs data analysis, including snp calling. in order to eliminate the possible sources of calling errors in the alignment results, almost all snp calling algorithms incorporate certain processing steps as shown in table  <dig>  in this section, we review these steps one by one.

1) in order to deal with duplicate reads that may be generated during pcr, atlas-snp <dig>  samtools, and gatk remove all the reads with the same start location in the initial alignment, except the one that has the best alignment quality. in contrast, instead of removing the duplicate reads, soapsnp sets a penalty to reduce the impact of these duplications.

2) in order to deal with reads that are aligned to multiple locations on the genome, soapsnp only takes into account the uniquely aligned reads, i.e., reads with only one best hit . atlas-snp <dig>  gatk, and samtools do not have a specific strategy to deal with the multiple-hit issue, instead these calling programs accept all hits that the alignment results provide.

3) in order to make sure the sequencing quality of each read reflects the true sequencing error rate, soapsnp, samtools, and gatk recalibrate the raw sequencing quality scores generated by ngs platforms. key factors, such as raw quality scores, sequencing cycles, and allele types, are all considered.

4) in order to deal with the presence of indels, both samtools and gatk include a realignment step to ensure accurate variant detection. in particular, gatk constructs the haplotype that could best represent the suspicious regions and realigns these regions appropriately according to this best haplotype. in contrast, soapsnp and atlas-snp <dig> do not utilize a specific indel realignment algorithm. soapsnp authors have conducted a simulation using a set of simulated data with  <dig>  indels, and have shown that only  <dig> % of reads containing indels are misaligned, and only  <dig> % of those incorrect snps are retained in the final snp calling output after routine processes including pre-filtering and genotype determination.

snp calling
in order to identify novel snps using sequencing reads and their quality scores, all four snp calling programs apply the bayesian method. soapsnp, samtools, and gatk-ugt compute the posterior probability for each possible genotype, and then choose the genotype with the highest probability  as the consensus genotype. a snp is called at a specific position if its consensus genotype is different from the reference. as a result, for both soapsnp and samtools, a phred-like consensus quality score, representing the accuracy of the snp calling, is calculated as − 10 log  <dig>  different from the other three algorithms, atlas-snp <dig> calculates the posterior probabilities for each variant allele instead of the genotype, and the genotype is determined afterwards according to the ratio of the number of reads covering the reference and the number of reads covering the most likely variant. depending on the bayesian framework that each snp calling program uses, different sets of metrics can be considered in snp calling procedures . several common parameters are often considered by most calling programs . there are also some parameters specifically adopted by each algorithm. in particular, atlas-snp <dig> considers several unique metrics: 1) whether the allele is involved in a multi-nucleotide polymorphism  event; 2) whether the allele is a “swap-base”, defined as the situation in which two adjacent mismatches invert their nucleotides respective to the reference; 3) whether the allele passes the neighboring quality standard , which means that the quality score of the variant allele should be higher than  <dig>  and the quality score of each of the five flanking bases on both sides should be higher than 15; and 4) whether the variant allele coverage is at least  <dig>  samtools incorporates two unique metrics, base dependency and strand independency. the former accounts for the correlation between bases, while the latter assumes that reads from different strands are more likely to have independent error probabilities.

built-in filters
after obtaining the raw genotypes or variant alleles, several internal filters are used by atlas-snp <dig>  samtools and gatk-ugt to further identify potential snps . for example, atlas-snp <dig> allows users to set up a cutoff value for posterior probability to get a customized list of potential variants among those putative variant alleles. the genotyping results are given in a variant call format  output file and several criteria are applied to determine the final genotypes:

 both strands are required to be supported by variant alleles.

 cutoff values for the percentage of variant reads are set to determine homozygous or heterozygous genotypes. in particular, at a specific locus, if less than 10% of the total reads support the variant allele, the genotype is determined to be a homozygous reference for this locus; if the percentage of variant reads is between 10% and 90%, a heterozygous genotype is assigned to this locus; if the percentage of variant reads is higher than 90%, this locus is determined as a homozygous variant.

 a binomial test is employed to estimate the genotype qualities, and gives a posterior probability to indicate how confident the algorithm is in calling this position as a variant.

similar to atlas-snp <dig>  samtools and ugt also produce snp calling results in vcf output. therefore, the internal filtering criteria of vcf are incorporated in gatk-ugt and samtools . since the vcf also reports some additional information about the called snps, such as strand bias, quality by depth , mapping quality, read depth, and genotype quality that represents the quality of the called snps, users can further filter the called snps based on the cutoff values they choose for these metrics. although soapsnp does not particularly use any internal filtering, it does provide several metrics in the output for each called snp, e.g., consensus score, quality of best allele, quality of second best allele, and sequence depth. these metrics can be used as customized post-output filters.

part ii dataset
to study the performance of these different snp calling tools in low-coverage data, we use a low-coverage  whole-genome sequencing dataset from the pilot  <dig> of  <dig> genome project: err <dig>  this dataset is sequenced from the sample #na <dig>  with  <dig> , <dig> 45-bp-long reads generated. we first explore the sequencing quality by plotting the per-base quality scores using fastqc  <cit> . the sequence quality stays high at the beginning of the reads, and then drops quickly when reaching to the 3′ end of the reads .

part iii snp detection and comparison
there are four major steps in the overall workflow . first, before alignment, we trim off the low quality ends of reads using the trim function in the brat package  <cit> . in particular, the brat trim function is set to cut from both the 5′ and 3′ ends until it reaches bases with a quality score higher than  <dig> . this trim function allows at most two ns in each read. second, alignments are conducted by either soap <dig>  or bwa , using the human genome  <dig> as the reference. at most two mismatches are allowed for each read, and only the reads aligned to unique positions are reported in the output files. third, snps are called on chromosomes  <dig> and  <dig>  all soapsnp callings are performed on soap <dig> alignment results, since soap <dig> is the only input format soapsnp can take. because atlas-snp <dig>  samtools, and gatk-ugt all require alignment results in the sam format, which can be generated by bwa but not soap <dig>  these three are performed on bwa alignment outputs. for the results of each snp calling algorithm, we identify the dbsnps and non-dbsnps, using the dbsnp information  downloaded from the ucsc genome browser  <cit> . finally, we compare the snp calling results from the four algorithms. since atlas-snp <dig> requires at least 3x coverage to detect a variance, for a fair comparison, we only use snps with at least 3x in each algorithm. all detected snvs are assigned to the following classes:

i. single nucleotide variants  identified by only one snp calling algorithm.

ii. snvs identified by any two snp calling algorithms.

iii. snvs identified by any three snp calling algorithms.

iv. snvs identified by all four snp calling algorithms.

this procedure is first conducted without any post-output filters. then we apply filters based on the key metrics in the output of each snp calling algorithm , with different coverage cutoff values.

RESULTS
part i alignment and the impact of trimming
in raw data, among the  <dig> , <dig> single-end reads, about 70% are aligned against human genome  <dig> by soap <dig> and bwa. 110– <dig> non-dbsnps  are detected in each of the four snp calling algorithms on chromosome  <dig> and  <dig> . since trimming can remove low-quality bases and thus improve the alignment results  <cit> , we trim the data using the trim function of the brat package. this process not only cuts off the low quality bases from both ends, but also discards reads that are shorter than 24-bp after trimming. as a result,  <dig>   reads are removed. with slightly fewer reads  available, however, the number of aligned reads is increased by  <dig>  . consequently, more snps are detected in trimmed data compared to raw data . among the four algorithms, soapsnp calls more snvs than the other three, in both raw and trimmed data. this is probably due to the fact that soapsnp has almost no internal filtering criterion after calling a snv, meaning that it is not as stringent as the others. although soap <dig> aligns slightly more reads than bwa, our previous study has shown that soap <dig> and bwa have similar alignment performance in trimmed data  <cit> . therefore, the difference between soapsnp and the other three algorithms is less likely caused by alignment disagreements. when compared to soapsnp, atlas-snp <dig> calls significantly less snvs than the other programs. the possible reasons are: 1) more stringent internal criteria are applied to determine snvs, including coverage for variant alleles on both strands and the percentage of variant reads; 2) the threshold for posterior probability is set as ≥  <dig> . since atlas-snp <dig> requires at least 3x coverage to call a snv, we only report the called snvs with ≥ 3x coverage in the other three algorithms. without any coverage filtering  in both raw and trimmed datasets, soapsnp calls dramatically more snvs  than samtools and gatk-ugt . since snvs from raw and trimmed data show similar patterns, and trimmed data has more snvs called, we use the trimmed data in further analysis.

* atlas-snp <dig> requires at least 3x to call a snv. for the other three algorithms, we choose the called snvs with ≥ 3x coverage.

part ii comparison without any filtering
in order to examine the agreement between the four algorithms, we compare both dbsnp and non-dbsnp results in trimmed data . overall, dbsnps exhibit a better agreement than non-dbsnps. this observation is consistent with our expectations, since the known dbsnp positions are more common and therefore more likely to be called. however, in terms of the performance of the four algorithms, dbsnps and non-dbsnps show similar patterns. figure  <dig> shows that gatk-ugt and samtools have a better agreement compared to the other comparison pairs. this is probably due to one or more of the following reasons: 1) they are both bayesian-based algorithms; 2) they incorporate similar information when determining the genotypes; and 3) they apply similar internal filters to the called snvs. because atlas-snp <dig> is more stringent than the other three calling programs, most of the snvs called by atlas-snp <dig> are also called by at least one of other programs. different from atlas-snp <dig>  there are  <dig> dbsnps and  <dig> non-dbsnps that are only called by soapsnp. in order to investigate the difference between these snvs that are only called by soapsnp and those that are also called by at least one of the other three algorithms, we compare their key metrics from the soapsnp output: consensus score, quality of best allele, quality of second best allele, and sequencing depth. no obvious difference is discovered between the two types of snvs. most of snvs have a consensus score between  <dig> and  <dig>  with only a few reaching the upper limit of  <dig>  moreover, most of snvs are covered by  <dig> to  <dig> reads in total.

part iii exploration of key metrics in four snp calling algorithms
key metrics in soapsnp
we have examined soapsnp’s snp calling quality in low-coverage data by checking the coverage and consensus scores for called dbsnps and non-dbsnps. we have found that the low coverage is often associated with low consensus scores, while the high coverage is often associated with high consensus scores. the consensus score in soapsnp represents how confident the algorithm is in calling a snv. a higher value corresponds to a higher confidence. therefore, using the consensus score as a filter is necessary in order to have accurate snp calling in soapsnp. we have checked the distribution of consensus score in soapsnp results and have chosen filtering criteria based on this distribution. table  <dig> shows that  <dig> snvs have a consensus score <  <dig>  indicating lower confidence. with a filtering criterion for consensus scores set at ≥  <dig>   <dig> snvs are removed and  <dig> snvs are left in total.

* number of snvs that have consensus score ≥ the cutoff values.

key metrics in atlas-snp2
unlike soapsnp, atlas-snp <dig> provides a posterior probability for every potential snv. it requires the users to set a threshold for the posterior probability. with a low coverage, many potential snvs reported by atlas-snp <dig> have low posterior probabilities. in our previous analysis, we use “posterior probability ≥  <dig> ” as a criterion to call snvs, resulting in a much smaller number of snvs when compared to the other three calling programs. in order to investigate whether posterior probability is a potential filter criterion, we set the cutoffs at ≥  <dig>  and then ≥  <dig> . with a lower threshold of  <dig> , the number of snvs called by atlas-snp <dig> increases from  <dig> to  <dig> .

key metrics in gatk-ugt
in the gatk-ugt output, there are several metrics associated with the quality of potential snvs. we have checked a few important ones among them: “genotype quality”, “qual”, ‘fisherstrand”, “haplotypescore”, “mappingqualityranksumtest”, and “readposranksumtest”.

1) “genotype quality” represents the quality of the called snvs. it ranges from  <dig> to  <dig>  with higher values corresponding to higher qualities. to better understand the calling quality of gatk-ugt in low-coverage data, we have checked the distribution of the genotype quality. in this low-coverage dataset, for dbsnps, the genotype ranges from  <dig> to  <dig>  and 80% of dbsnps have a genotype quality lower than 30; while for non-dbsnps, the genotype ranges from  <dig> to  <dig>  and 70% of non-dbsnps have a genotype quality lower than  <dig>  then based on the distribution, we choose several different cutoff values for genotype quality, ≥  <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig> . with the cutoff set at ≥  <dig>   <dig> snvs  are removed, resulting in  <dig> remaining snvs.

2) in vcf output, there is a metric called “qual”, a phred-scaled quality probability of the snvs being a homozygous reference. a higher “qual” score indicates a higher confidence. in our dataset, all called snvs have a qual value ≥  <dig>  which is a commonly used criterion for reliable snp calling in gatk-ugt.

3) another indicator of snvs’ quality is strand bias, which looks for the instance where the variant allele is disproportionately represented on one strand. in gatk-ugt output, “fisherstrand” is a phred-scaled p-value using fisher’s exact test to detect strand bias. a higher “fisherstrand” value represents a more pronounced bias, indicating a false positive. the commonly used criterion for reliable snv calling is to remove any snv with a “fisherstrand” value >  <dig>  in our dataset, the “fisherstrand” value for all snvs ranges from  <dig> to  <dig>  therefore, there is no need for filtering using “fisherstrand”.

4) “haplotypescore” in gatk-ugt output is a measure of how well the data from a 10-base window around the called snv can be explained by at most two haplotypes. usually, with the instance of mismapped reads, there are more than two haplotypes around the snv and this snv is likely to be a false positive. a higher “haplotypescore” value represents a higher probability that the called snv is artificial due to mismapping. in table  <dig>  we check the distribution of “haplotypescore” in dbsnps and non-dbsnps. the majority of snvs have a low “haplotypescore” , indicating a generally good mapping in this dataset. since the commonly used criterion for reliable snvs calling is removing any snv with a “haplotypescore” >  <dig>  we use  <dig> as a filtering criterion, which removes  <dig> snvs in total.

5) “mappingqualityranksumtest” is a wilcoxon rank test that tests the hypothesis that the reads carrying the variant allele have a consistently lower mapping quality than the reads with the reference allele. this metric is only available for the snvs where both the variant allele and reference allele are supported by reads. in our dataset, there are  <dig> snvs  that have “mappingqualityranksumtest” values, indicating that they have coverage in both variant and reference allele. in these  <dig> snvs, the “mappingqualityranksumtest” value ranges from− <dig> to  <dig> for dbsnps, and− <dig> to  <dig> for non-dbsnps. the commonly used criterion for reliable snvs calling removes any snv with a “mappingqualityranksumtest” value < − <dig> . since in our dataset all snvs are > − <dig> , there is no need to apply any filter on the “mappingqualityranksumtest” values.

6) “readposranksumtest” is a mann–whitney rank sum test that tests the hypothesis that instead of being randomly distributed over the read, the variant allele is consistently found more often at the beginning or the end of a sequencing read. similar to the “mappingqualityranksumtest”, this metric is also only available for the snvs where both the variant allele and reference allele are supported by reads. in our dataset, for the snvs that actually have the “readposranksumtest” report, their values range from− <dig> to  <dig>  these values satisfy the common criterion that the “readposranksumtest” value is ≥− <dig> 

based on the above exploration of the six key metrics in gatk-ugt output, we set a series of filtering criteria for reliable snp calling by gatk-ugt: “genotype quality” ≥ 9; “qual” ≥ 30; “fisherstrand” ≤ 60; “haplotypescore” ≤ 13; “mappingqualityranksumtest” ≥ − <dig> ; “readposranksumtest” ≥ − <dig>  as a result,  <dig> snvs  pass the filtering, with  <dig> dbsnps and  <dig> non-dbsnps. we will use this set of snvs in a later analysis. since “qual”, “fisherstrand”, “mappingqualityranksumtest”, and “readposranksumtest” values all satisfy the criteria in our dataset, we cannot remove any snv by applying filtering on these four metrics. however, they are all important metrics that are related to snp quality. thus, we recommend that users filter raw snp calling results based on their values.

key metrics in samtools
similar to gatk-ugt, samtools reports the vcf output. we have checked two important metrics in samtools results: “genotype quality” and “qual”. in both dbsnps and non-dbsnps, the values of genotype quality range from  <dig> to  <dig>  setting different cutoff values for “genotype quality” does not filter out significantly more of the called snp . for “qual”, all snvs have a qual value ≥  <dig>  which is a commonly used criterion for “qual” in samtools results. therefore, for our dataset we do not apply any filter on samtools results and use the raw snvs for a later analysis.

part iv comparison with filtering using key metrics and coverage
to compare the four algorithms under different coverage levels, we use the snp calling results with filtering criteria applied in each calling program, and then add the filtering of coverage with several cutoff values, ≥ 4x, 5x, 6x, 7x, 8x, 9x, and 10x . the number of snvs called by each calling program decreases dramatically by more than 50% when the cutoff increases from 3x to 4x, and drops to about 15% at 10x. with 3x, soapsnp calls more snvs than the other calling programs, while atlas-snps calls the least. however, when the coverage cutoff increases, the number of snvs called by each calling program becomes more similar, with soapsnp calling slightly more.

table  <dig> shows the changing patterns of the number of snvs as the coverage cutoff level increases. although the numbers of snvs identified by the different calling programs become more similar as the coverage cutoff increases, it is unclear whether the agreement of different calling programs and their performance will increase accordingly. in order to address this question, we have done further comparisons using the following two methods: method  <dig> checks the agreement among different calling algorithms , and method  <dig> calculates empirical positive calling rates and sensitivities . for both methods, we check dbsnps and non-dbsnps separately.

“total” means the total number of snvs called by four algorithms. “by 1” means the number  of snvs called by only one of the four algorithms. “by 2” means the number  of snvs called by any two algorithms. “by 3” means the number  of snvs called by any three algorithms. “by 4” means the number  of snvs called by four algorithms.

for a specific calling program , a is the number of snvs identified as an empirical truth  and also called by this calling program; b is the number of snvs identified as an empirical truth, but not called by this calling program; c is the number of snvs called by this calling program, but is not an empirical truth. positive calling rate is calculated as a/; sensitivity is calculated as a/.

method 1: check the agreement among different calling programs
for dbsnps, using the original setting , there are  <dig> unique dbsnps called by the four algorithms, and  <dig> % of them are common among all the calling programs. when increasing the cutoff of coverage to 4x, although the number of unique dbsnps drops dramatically from  <dig> to  <dig>  the percentage of agreements among the four calling programs remains similar . with a further increase of coverage cutoff values, the number of unique dbsnps continuously decreases, while the agreements stay similar . for each snp calling program, we plot the agreement with other algorithms under different coverage cutoffs . for soapsnp, even though the number of called dbsnps drops dramatically, the agreements with other calling programs do not change as much as the coverage cutoff increases. for atlas-snp <dig>  the percentage of agreement with the other three calling programs decreases when the coverage cutoff increases. this is probably due to the fact that with a lower cutoff , atlas-snp <dig> calls much fewer than the other calling programs. therefore, compared to other programs, the  <dig> agreement dbsnps take a larger portion among all snvs called by atlas-snp <dig>  however, when the coverage cutoff increases, the number of dbsnps called by atlas-snp <dig> is far more similar to the other algorithms, therefore the percentage of agreement in atlas-snp <dig> becomes smaller than ≥ 3x. compared to soapsnp and atlas-snp <dig>  gatk-ugt and samtools exhibit a higher agreement with other calling programs. 60-70% of their dbsnps are called by all four programs, 20% are called by three programs, and about 10% are called by two programs . moreover, in both gatk-ugt and samtools, when the cutoff increases from 3x to 5x, the percentage of dbsnps called by all four programs increases 3-4%.

for non-dbsnps, the comparison results show similar patterns as dbsnps, but with a lower percentage of agreement . the number of unique non-dbsnps called by the four algorithms drops from  <dig> to  <dig> when the coverage cutoff increases from 3x to 4x, and finally decreases to  <dig> when the coverage cutoff is 10x. the percentage of non-dbsnps called by all four calling programs increases over the different coverage cutoffs, especially from 3x to 7x. while the percentage of non-dbsnps only called by one algorithm decreases over the cutoffs, from  <dig> % in 3x to  <dig> % in 10x. for each calling program, we plot the agreement with other algorithms under different coverage cutoffs . among the four calling algorithms, soapsnp shows the lowest percentage of agreements with others. these low agreements are probably due to the fact that soapsnp always calls more snvs than other programs under different coverage levels. in all four calling programs, the percentage of agreements increases over the coverage cutoff values, especially from 3x to 7x, indicating that filtering the non-dbsnps with a higher coverage threshold improves the agreement among the four algorithms.

method 2: calculate empirical positive calling rates and sensitivity
for this comparison method, we choose the variants that are called by at least three calling programs as the “empirical truth”, and then investigate the calling performance of each snp calling program based on this empirical truth by calculating both the positive calling rate and the sensitivity. we then compare the four calling programs at different coverage levels using these rates. the positive calling rate and the sensitivity are calculated as positive calling rate = a/, and sensitivity = a/ as shown in table  <dig>  in these formulas, a is the number of snvs identified as an empirical truth  and also called by this calling program; b is the number of snvs identified as an empirical truth, but not called by this calling program; and c is the number of snvs called by this calling program, but is not an empirical truth.

the results of comparing four snp calling algorithms using the empirical positive calling rate and sensitivity are shown in table  <dig> and table  <dig> and are explained below.

1) for calling dbsnp positions, table 14a  shows that soapsnp has a relatively lower positive calling rate. this is because soapsnp tends to call more variants than the other three calling programs, suggesting a higher false positive rate. gatk has a relatively higher positive calling rate than the others at different coverage levels for calling dbsnps. atlas-snp <dig> and samtools tend to stay between soapsnp and gatk.

2) for calling non-dbsnp positions, similar to dbsnps, table 14b shows that soapsnp tends to call more false positive variants since it lacks stringent internal filtering criteria. atlas-snp <dig> shows the highest positive calling rate. this is probably because it is the most stringent calling program. gatk has a higher positive calling rate than soapsnp and samtools.

3) as far as the positive calling rate is concerned, atlas-snp <dig> and gatk perform better than soapsnp and samtools on both dbsnps and non-dbsnps. with the change of coverage level, the comparison results are relatively stable.

4) for calling dbsnps and non-dbsnps, table  <dig> shows that, with the exception of samtools, the other three programs all have very high sensitivity in calling snvs. overall the sensitivity of all calling programs are pretty stable across the different coverage levels, except that atlas-snp2’s sensitivity is a bit low at 3x coverage.

discussion
identifying a reliable list of snps is critical when analyzing ngs data. for data with high-coverage and/or multiple samples, previous studies have shown that different snp calling algorithms have a good agreement between each other and have high true positive rates  <cit> . however, for single-sample low-coverage data, it is difficult to call snvs with high confidence. in order to provide insights into the choice of snp calling programs, we have compared the performance of four commonly used snp calling algorithms using low coverage sequencing data.

about the four snp calling algorithms and their post-output filtering
out of the four algorithms, soapsnp calls many more snvs compared to the others. this is probably because it has less internal filtering criteria. after applying the criterion that removes any snvs with a consensus score lower than  <dig>  the total number of snvs called by soapsnp decreases and becomes more similar to the other algorithms. in the soapsnp output file, the consensus score is an important metric representing the quality of calling a snp. therefore, when processing low-coverage data, we recommend that users apply the consensus score as a post-output filter for soapsnp results.

atlas-snp <dig> is much more stringent compared to the other three algorithms. 97% of the snvs called by atlas-snp <dig> are also called by at least one of the other three calling programs. with a much lower threshold for posterior probability, atlas-snp <dig> calls more snvs but still fewer than the other algorithms. since it has the lowest number of called snvs, atlas-snp <dig> appears to have a higher positive calling rate and sensitivity when compared to the other calling programs . however, when using atlas-snp <dig> to deal with low-coverage dataset, the users should be careful with the filtering settings. for example, in this study, we set the threshold for posterior probability at  <dig> , which indicates a low confidence in calling a snp. because atlas-snp <dig> is much more stringent than the other programs, even with a low posterior probability, the called snvs are still very likely to agree with other calling programs.

compared to the above two algorithms, gatk-ugt and samtools call a moderate number of snvs. when using the gatk-ugt package, applying the common criteria is necessary, including “genotype quality”, “qual”, “mappingqualityranksumtest”, “fisherstrand”, “haplotypescrore”, and “readposranksumtest”. with the samtools program, filtering out the snvs with low genotype quality and low “qual” value can help improve the accuracy in snp calling.

filtering out the low quality snvs is an important step before performing further analysis, especially for low-coverage data. when choosing the criteria for filtering, it is important not only to consider the commonly used standards, but also to take into account the characteristics of each specific dataset. for example, in our dataset, all the snvs have little or no strand bias, have high “mappingqualityranksumtest” scores, and have high “readposranksumtest” scores. setting the threshold of genotype quality at  <dig> gives a similar number of snvs compared to others. besides the key metrics that we have explored in the result section, each algorithm provides additional information. for instance, soapsnp reports the quality of variant and reference alleles, number of reads covering the variant and reference alleles, average copy number, and more. gatk-ugt and samtools both report their results in vcf, which can include many metrics. users may check these metrics based on the characteristics of their own data if necessary, though we did not find these metrics to be very helpful .

about the impact of coverage
coverage is an important factor to consider when assessing the quality of called snvs. without any coverage filtering , the results of the four calling programs can be dramatically different. usually, high coverage regions or bases tend to have higher calling qualities . low coverage regions or bases tend to have lower snp calling qualities. however, there is not a simple linear relationship between coverage and the genotype quality scores that are generated by different snp calling programs.

our results show that when increasing the coverage levels for each calling program, the number of identified snvs drops dramatically in all calling programs. however, increasing sequencing coverage cutoffs does not necessarily lead to an increase in agreement among the different calling programs. in fact, our comparison results show that the impact of coverage on calling agreement is small except that we see some agreement increase in non-dbsnps when the coverage level changes from 3x to 7x. this may sound counter-intuitive. however, this observation can be explained by the fact that the four programs use different statistical methods and algorithms, which model different aspects of the sequencing information. these differences lead to the complex correlations of output metrics.

filtering out many low-coverage snvs may result in a sacrifice of missing novel snvs. for example, the number of called snvs in each calling program decreases by more than 50% when the coverage cutoff increases from 3x to 4x, and drops to 15% at 10x. therefore, caution should be used when choosing coverage as a filtering criterion. simply choosing the snvs called with high coverage might not be sufficient. this is because, with a higher threshold of coverage, the users may over-filter the results and miss novel snps related to the disease of their interest.

about the generalization of our results and decision making
in this paper, we use a set of single-end data, which is one mate of a pair-end dataset. we have also conducted the same analysis using a different single-end sequencing dataset and have arrived at the same conclusion. therefore, we only report the results from the first dataset we used. in addition, the results we report here are generated by analyzing chromosomes  <dig> and  <dig> together. we have also analyzed chromosomes  <dig> and  <dig> separately and get the same conclusion as when they are combined. furthermore, the findings in this paper are similar to the results reported by other researchers  <cit> . therefore, our comparison methods and results can be generally applied to low-coverage sequencing data. in addition, although this paper mainly focuses on the snp calling in a single sample, our methods and conclusion can be easily applied to the variant calling in multiple samples. in particular, the empirical-based positive calling rate and sensitivity analysis can serve as an empirical standard for comparing algorithms in multiple-sample snp calling.

overall, the four calling programs have very low agreement amongst each other, with only roughly 35% ~ 45% for dbsnps and 19% ~28% for non-dbsnps. for very low coverage data, it might be wise to choose a concordance among two or more snp calling program instead of just using one algorithm. however, this may result in a high false-negative rate, with many true snvs being missed. in addition, choosing filtering cutoff values for coverage and different quality scores with high and low values may have the same advantages and disadvantages as choosing a single snp calling program vs. using the concordance of two or more snp calling programs. therefore, as far as the experimental validation of novel snvs is concerned, we recommend that users employ a comprehensive strategy in their validation plan. first, in order to obtain a high experimental validation rate, the users may choose the snvs that are called by more than one algorithm and with high metrics  in the beginning of the validation process. then, if the validation success rate is high, the users may validate more low coverage snvs called by multiple calling programs, or snvs called by only one program but with high quality. this approach can both ensure an effective validation and avoid missing many true disease-contributing snvs.

CONCLUSIONS
we have compared the performance of four snp calling programs in a low-coverage single-sample sequencing dataset. it is important to filter out the snvs of low quality using different metrics . our results show that the concordance among these different calling algorithms is low, especially in non-dbsnps, and increasing the cutoff values of coverage has little effect on improving the concordance. this is probably due to discrepancies in the statistical methods and algorithms that these calling programs employ. additionally, to provide an empirical standard for choosing a snp calling program, we have calculated the empirical positive calling rate and sensitivity for each calling algorithm under different cutoffs of coverage. we have found that dbsnps have generally higher rates compared to non-dbsnps, suggesting lower quality in called non-dbsnps in low-coverage sequencing data. moreover, among the four calling programs, gatk and atlas-snp <dig> show a relatively higher positive calling rate and sensitivity when compared to the others, and gatk tends to call more snvs than atlas-snp <dig>  therefore, if users intend to use only one calling program, we recommend gatk. however, in order to increase the overall accuracy, we advocate for employing more than one snp calling algorithms.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
both authors developed and performed the statistical and computational analysis, drafted, revised, and approved the manuscript.

