BACKGROUND
as a result of their ability to detect the expression levels of tens of thousands of genes simultaneously, dna microarrays have quickly become a leading tool in diverse areas of biological and biomedical research. given this popularity and the associated accumulation of numerous microarray analysis methods, there is a critical need to know the accuracy of microarray technology and the best ways of analyzing microarray data. important advances toward this goal were made by the microarray quality control  project launched by us food and drug administration  <cit> . for the maqc study, two distinct reference rna samples were mixed together at specified ratios and then hybridized to different microarray platforms at multiple test sites. this design enabled the maqc consortium to evaluate the reproducibility of microarray technology and the consistency between platforms. the study demonstrated that high levels of both intraplatform and interplatform concordance can be achieved in detecting differentially expressed genes  when the microarray experiment is performed appropriately. however, as the exact identities of the individual rnas in the reference samples were unknown, the maqc project was not able to address questions regarding the overall accuracy of microarray technology and analysis methods.

spike-in experiments are designed to address questions about the correctness of microarray data and have been used extensively to compare among different analysis methods. currently there are four major spike-in datasets available for the affymetrix microarray platform: the affymetrix spike-in dataset for cross platform comparisons  <cit> , the affymetrix latin square dataset  <cit> , the gene logic spike-in dataset  <cit>  and the golden spike dataset  <cit> . different from the other spike-in studies where a small number of spike-in rnas were mixed with large unknown background rna samples, the golden spike dataset contains a defined background sample of over  <dig> rnas and over  <dig> spike-in rnas that differ by known relative abundance with fold changes from  <dig>  to  <dig>  this dataset was used to determine the preferred choices at each step in microarray analysis to achieve optimal deg detection  <cit> , and subsequent work by a variety of researchers has extended these findings and proposed improved analysis alternatives  <cit> .

although it has enjoyed wide-spread use in the bioinformatics community, concerns have been raised over two aspects of the golden spike dataset: differentially expressed rnas are more abundant in the spike arrays than in the control arrays  <cit> , and the experiment consists of only a limited  set of technical replicates  <cit> . the former condition violates a main assumption of most microarray normalization methods, which presuppose that up-and down-regulation of genes is balanced and total rna amount is equivalent in both samples. in order to address both of these concerns as well as to explore further questions regarding ways of analyzing microarray data, we have constructed a new wholly-defined affymetrix genechip control dataset, the "platinum spike" dataset. this new dataset consists of a total of  <dig> microarrays with evenly balanced up and down gene expression between conditions. we used the platinum spike dataset to compare over  <dig>  possible analysis routes derived from combining different methods in individual steps of the analysis procedure in order to determine the optimal path for deg detection and to pinpoint the most critical steps affecting analysis accuracy. we find that how normalization is conducted has a dramatic effect on performance and depends on the balance of gene expression between the sets of arrays being compared. the highly balanced platinum spike dataset, and comparisons between it and the unbalanced golden spike dataset, provide a valuable resource for developing and testing analysis procedures to handle a range of distributions of degs in affymetrix genechip experiments. the platinum spike raw data are available at http://www.ccr.buffalo.edu/halfon/spike/index.html, and through ncbi's gene expression omnibus   <cit> .

RESULTS
experimental design
the platinum spike dataset consists of  <dig> affymetrix drosophila genome  <dig>  microarrays representing two different "conditions" , each of which contains the identical  <dig> crnas, but at different defined relative concentrations. we generated three independently labeled samples for each condition, and hybridized each sample to three arrays. therefore the nine arrays for each condition consist of three sample replicate groups, and each group contains three technical replicates . for each condition, the total amount of crna is the same, and there are similar numbers of up-and down-regulated crnas:  <dig> and  <dig> individual rnas are up-and down-regulated in condition a versus condition b respectively, with known fold changes varying between  <dig>  and  <dig> fold, and  <dig> rnas are identical in abundance between the two conditions . our previous golden spike dataset  <cit>  was criticized for having higher-than-typical hybridization signal intensities  <cit> . we calibrated the amount of rna hybridized to the arrays in the current experiment so that gene intensities fell within the range commonly seen in experiments stored in geo .  <dig> additional rnas were spiked in at defined  concentrations to provide further validation that our rna concentrations fell within a reasonable range . the platinum spike dataset therefore differs from the earlier golden spike dataset in four critical ways: it includes  <dig> additional individual rnas ; it is balanced with respect to total labeled rna amount and extent of up-and down-regulation for each experimental condition; the observed probe intensities are more in line with what is typically seen in an affymetrix experiment; and it consists of both technical and sample replicates.

pool
*there are  <dig>   <dig> and  <dig> probe sets assigned to clones present in two, three and four different pools respectively.

assessment of present/absent calls
the affymetrix drosophila genome  <dig>  arrays measure the levels of over  <dig>  transcripts using probe sets composed of  <dig> oligonucleotide probe pairs per transcript. each probe pair contains two  <dig> bp dna oligonucleotide probes: the perfect match  probe, which is exactly complementary to the target crna, and the mismatch  probe, which is almost identical to the pm probe except that the central nucleotide has been changed to the complementary base. affymetrix estimates whether or not the crna target of a probe set is present in a sample based on the mas  <dig>  detection algorithm  <cit> , as follows: to obtain the present/absent call for each probe set in an array, a discrimination score / is calculated for every pm, mm probe pair, and then a wilcoxon signed rank test is performed to test whether the median of the score values is greater than a pre-specified value τ . as we knew in advance what crnas were in fact present or absent in the platinum spike dataset, we could evaluate the performance of the detection algorithm by using receiver operating characteristics  curves.

as background correction and probe normalization could change the probe intensities and therefore affect the results of the detection algorithm, we evaluated the performance of the present/absent call for multiple analysis routes obtained from different combinations of popular methods for these two steps . eight different ways of background correction were used, including no background correction, rma  <cit> , mas  <dig>    <cit> , and five different scenarios of gcrma  <cit> . probe normalization was either not performed, or performed using one of the six normalization methods coupled with one of three "normalization groups." the arrays in the platinum spike experiment fall into two distinct conditions. each condition contains three sample replicate groups, and each sample group contains three technical replicates. at the normalization step, therefore, we could normalize among technical replicates and perform six normalizations , normalize among arrays under the same condition and perform two normalizations , or normalize once using all arrays  . the latter, in which all arrays are used for normalization, is the typical choice in microarray analysis and is based on a popular assumption that there is an equal amount of up-and down-regulation in the samples leading to comparable intensity distributions from all arrays. however, we and others have argued that this assumption may often not be justified  <cit> .

in the platinum spike dataset, there are  <dig>  empty probe sets and  <dig> probe sets whose crna targets are present in the samples . the relative performance of all routes from different combinations of background correction and probe normalization was assessed based on a summary statistic of the roc curve, rauc <dig> . this statistic measures the area under the curve  relative to the maximum auc value  when the false positive rate  is less than or equal to  <dig> . that is,  where pauc <dig>  is the partial auc  <cit>  when the false positive rate  ≤  <dig> . we define fpr as the number of probe sets that are incorrectly called "present" divided by the number of probe sets whose targets are truly absent, while true positive rate  is the number of probe sets that are correctly called "present" divided by the number of probe sets whose targets are truly present in the samples. we used a conservative fpr cutoff , as researchers are typically interested in performance when the number of false positives is relatively small. a larger rauc <dig>  value corresponds to a higher true positive rate and lower false positive rate, and therefore better performance. as the mas  <dig>  detection algorithm is applied on each individual array, we obtained  <dig> rauc <dig>  values corresponding to the  <dig> arrays and used the average value () for evaluation.

a total of  <dig> different routes were compared, and most routes generated reasonably high   =  <dig> , figure 3a). at a false positive rate not greater than 5%, the best route on average calls "present" approximately 85% of probe sets whose crna targets are truly present. gcrma background correction using maximum likelihood estimation  generally outperformed other background correction methods by a small margin, although it could also lead to very poor results when used with the wrong combination of other steps . different normalization methods mostly performed similarly to no probe normalization. the exception to this is invariantset normalization, which showed larger variation than the others, and whose best performance was substantially worse than the best performance of any other normalization method . choice of normalization group also had little effect on overall performance . the limited contribution of probe normalization might be due to the fact that the detection algorithm is designed to be applied on an individual chip basis.

detection of differentially expressed genes
one of the primary uses for microarrays is to detect the genes whose expression levels have changed between compared conditions. deg detection using affymetrix data requires a series of analysis steps including background correction, probe normalization, pm correction, probe summarization, probe set normalization and deg testing . for each of these steps, we picked several representative methods based on different algorithms, and assessed all possible combinations of these methods. we also tested several methods which themselves already combine multiple steps. in total, we examined  <dig>  different routes and evaluated the performance of each combination using rauc <dig> .

most of the routes performed well on the platinum spike dataset , with the best 1% of routes all yielding a tpr <dig>  >  <dig> . . the alchemy and goldenspike methods, which previously had been shown to fall among the top methods for the golden spike dataset  <cit> , perform strongly on the platinum spike dataset as well, with alchemy falling within the top ten .

*the number of correctly detected degs at false positive rate cutoff  <dig> .

somewhat surprisingly, we found that the choice of method at most steps in the analysis had relatively little impact on overall performance. for instance, while the overall best route used the gcrma-reb background correction method, and while the gcrma-rml and gcrma-lml background correction methods were significantly more enriched in the top 1% of routes , the effect was only marginal . similarly, although invariantset normalization was found to be highly enriched in the routes of the top 1% , the top two routes used scaling and vsn normalization, respectively. probe set normalization was also found to be of minimal utility, consistent with the suggestion by choe et al.  <cit>  that this step is of importance mainly when the log fold changes are not centered around zero as a result of unequal crna amounts between conditions, which is not the case in the platinum spike dataset . choice of statistical test for deg detection also had little bearing on performance. interestingly, the simple fold change method performed extremely well and was in fact marginally enriched in the top 1% of routes . we treated technical replicates and sample replicates as part of a single 18-array experiment in order to take advantage of the resulting larger sample size. although such a choice in theory could lead to underestimation of the overall variance, we consider any such effect to be negligible in this case as the variation introduced by independent sample preparation can be seen to be much smaller than other sources of variation . the relatively large number of replicates and overall low variance in the platinum spike dataset may be at least partially responsible for this surprisingly strong performance achieved by simply ranking probe sets by their fold change levels.

the most important factors affecting deg detection appear to be the probe normalization group, pm correction method, and summarization method, as the options selected at these steps had the largest impact on performance. probe normalization among all arrays  was always more likely to produce large rauc <dig>  values than using either the other normalization groups or no probe normalization at all . we speculate that the improved performance seen when using all arrays might merely be the result of increased sample size for normalization; performance decreases in proportion with the number of arrays normalized such that all  >conditional  >technical  ≈ none. pmonly and medianpolish were consistently the best methods in the pm correction and summarization steps, respectively. not only did the best route use these two methods, but routes using these two methods always yielded a higher percentage of large rauc <dig>  values . the superiority of pmonly over mas pm correction is consistent with results reported by  <cit>  but in contradiction to those of  <cit> , likely reflecting differences in the nature of the various datasets being analyzed .

in order to test whether the performance of deg detection depends on the magnitude of the fold change between conditions, we calculated rauc <dig>  values for all routes at each fold change level. the crnas spiked in at each individual fold change level were considered to be true degs, while the fold change =  <dig> and empty probe sets were considered as non-degs. consistent with previous findings using the golden spike dataset  <cit> , degs with fold changes below  <dig> × were poorly detected by most of the routes . although the best route for detecting high fold-change degs did not work well for identifying small fold-change degs, the overall best route performed equally well at all fold change levels, with detection of small fold-change degs only slightly worse than observed for higher fold changes.

comparison with the golden spike dataset
we  <cit>  and others  <cit>  have used the previously developed golden spike dataset  <cit>  to determine optimal methods for microarray analysis. interestingly, however, many of the methods that were shown to perform strongly on the golden spike dataset did not fall among the best routes identified here for the platinum spike dataset. one reason for this could be that some methods did not exist, or were not considered, at the time of those analyses. on the other hand, the platinum spike dataset and the golden spike dataset are two distinct datasets and might require different analysis strategies. we addressed this issue in two ways:  we analyzed the platinum spike dataset using only those methods that had previously been applied to the golden spike dataset, and  we applied all of the methods evaluated here for the platinum spike dataset to the golden spike dataset.

using only those methods that had previously been applied to the golden spike dataset in the choe et al. study  <cit> , we found that optimal methods for the platinum spike dataset were still different from choe et al.'s findings . . thus differences in the datasets, rather than in available algorithms, appear to be responsible for the differences between the two studies.

to further investigate the effect of different datasets on method performance, we also evaluated the deg detection performance of  <dig>  routes on the golden spike dataset using the same methods--including those not available at the time of the choe et al. study--we assessed for the platinum spike dataset.  whereas with the platinum spike dataset a large number of methods gave reasonably good results, most of the routes we tested performed poorly on the golden spike dataset . we did identify a number of strongly-performing routes, including ones that significantly outperformed alchemy and goldenspike, which were reported to be the best methods in previous studies  <cit> . importantly, however, we found that these routes do not correspond to the most strongly performing routes identified using the platinum spike dataset. gcrma-lml and gcrma-rml were superior to other background correction methods, and the constant normalization method was slightly better than others for probe normalization step. in stark contradiction to the better performance observed when normalizing using all arrays in the platinum spike dataset, we found normalization among all arrays to be deleterious in the golden spike dataset for both probe normalization and probe set normalization, with clearly improved performance seen with either technical/conditional group normalization or no normalization at all . we additionally saw that cybert is the optimal deg test method for the golden spike dataset, consistent with  <cit> .

although the optimal analysis routes identified for each dataset are clearly different, we were able to identify routes that gave acceptable performance on both datasets as long as the appropriate normalization groups were used . this suggests that the predominant factor affecting analysis of the two datasets is how the arrays are grouped for normalization. the opposite performance of the normalization groups between the platinum spike and golden spike experiments can be seen to result from the respective designs of the two datasets. the degree of up-and down-regulation in the platinum spike dataset is balanced, and a similar amount of labeled crna was hybridized to each array. therefore the signal intensity across all  <dig> arrays is similar, even for arrays from different conditions, and normalization using all arrays is beneficial . on the contrary, the golden spike dataset is imbalanced and more labeled crna was added to the spike arrays than the control arrays, leading to higher relative signal intensities in the former. when all arrays are used for normalization, the true intensity difference between the two conditions is improperly diminished, leading to degraded performance. when only arrays from the same condition are used for normalization, the true intensity difference between the spike and control arrays is maintained. the difference in balance between conditions in the two datasets likely contributes to the other differences we observed in optimal analysis methods as well, including choice of background correction, pm correction, summarization and deg testing. these steps may be affected by the differences in background hybridization intensity for the empty probes and the amount of cross-hybridization caused by the presence of more labeled crna in the spike versus control arrays.

false discovery rate
a common practice in microarray analysis is to estimate the false discovery rate , the expected proportion of false positive results among the detected degs, typically expressed as a q-value for each gene  <cit> . a number of methods have been proposed to estimate this statistic in which the actual q-value is mathematically guaranteed to be below the estimated q-value. although statistically sound, it is unknown how well these q-value estimations perform in real-world microarray experiments. we have shown previously for the golden spike dataset that the q-values appear to underestimate the true fdr  <cit> . taking advantage of the full knowledge of the crna identities in the platinum spike dataset, we compared the actual fdr and the estimated q-values by using the results from the overall top ten deg detection routes . the estimated q-values were calculated either by permutation or by the benjamini and hochberg method  <cit>  . our results show that when using the benjamini and hochberg method, the predicted q-values  understate the true fdr in four out of the five evaluated routes for q =  <dig>  . the permutation-based method worked slightly better, as the true fdr was successfully controlled in six of the ten routes . nevertheless, these results show that care must be taken in assessing the fdr in microarray experiments, as the true fdr is frequently not successfully controlled.

control of the fdr requires that the p-values from statistical evaluation of the genes not differentially expressed  be uniformly distributed in the interval . dabney and storey  <cit>  show that this is not the case for the golden spike data and suggest that non-uniform null p-value distribution is the cause for the underestimation of the fdr for those data observed in  <cit> . they also claim that the non-uniform p-values are due to specific design flaws in the golden spike experiment, a claim we have disputed  <cit> . fodor et al.  <cit>  lend support to our view with their finding that the affymetrix latin square experiment also shows a non-uniform distribution of the null gene p-values. the null gene p-values in the current dataset also fail to form a uniform distribution , despite the great care we have taken to ensure that the platinum spike experiment closely approximates the conditions of a "typical" microarray experiment. this observation lends further support to the idea that the skewed distribution is not dataset specific but rather due to more general factors, perhaps from the analysis procedures themselves  <cit> .

sample size study
due to high costs and/or limited biological samples, microarray experiments are frequently performed using three or even fewer replicates. we find that in the platinum spike dataset, the variation introduced by sample preparation and hybridization are similarly small , allowing us to treat all of the arrays from each experimental condition together as a single 9-fold replicated experiment. by randomly drawing and analyzing differently sized subsets of the nine arrays, we addressed the question of how many arrays are needed to identify degs with high accuracy and specificity  <cit> .

we evaluated the performance of deg detection when using as few as two up through seven arrays per condition. in each case, we applied nine analysis routes  on  <dig> samples formed by randomly selecting arrays from the total nine "a" and nine "b" arrays . for five of the nine routes being tested, we found that using five arrays per condition can reach at least 95% of the performance  seen when all nine arrays are used, while for two routes four arrays were sufficient . these results suggest that at least five replicate arrays are required to achieve near-optimal deg discovery in a microarray experiment. as the variation among arrays in "real" experiments is likely to be greater than in the platinum spike dataset, which lacks potential additional variation introduced by using different biological samples, an even larger number of replicates may be necessary. on the other hand, the magnitude of the change in correctly identified degs when moving from four to five replicates is relatively small , suggesting that four replicate arrays will frequently give acceptable results. the commonly-used three replicates, while clearly sub-optimal in absolute terms, also tends to give reasonable results with an average change in degs of  <dig> ±  <dig>  when compared to using four arrays .

ap-values for testing whether the relative auc values corresponding to a specific number of arrays is less than 95% of the relative auc value achieved by the same route in the platinum spike dataset.

bthe routes used for sample size study. they are modified from nine of the top  <dig> routes  assessed in the platinum spike dataset by using all arrays for normalization.

cns: not significant, based on the multiple hypothesis correction using holm's method.

CONCLUSIONS
the wholly-controlled platinum spike dataset provides an important addition to the available spike-in control datasets available for assessing affymetrix microarray analysis methods. our analysis of over  <dig>  analysis routes on this dataset reveals that in general the state of affymetrix analysis is in good shape: most commonly used methods perform strongly. the best performance when using default settings for mas  <dig> , rma, and gcrma yielded between 84%-87% sensitivity at a 5% false positive rate, close to that achieved by the best overall routes. choice of probe normalization group, pm correction method, and summarization method were the key factors affecting outcome. our sample size study suggests that while five or more replicates is the preferred choice, four or even three replicates, typical sizes for microarray studies, produce reasonable outcomes.

at the same time, our data reveal several areas that remain in need of further development. as noted previously  <cit> , methods for assessing the false discovery rate tend to underestimate its size, providing a false sense of confidence about the specificity of the results, even in the highly controlled, idealized platinum spike dataset. a proposed solution to this problem  <cit>  did not appear to be effective with the platinum spike dataset , suggesting that continued investigation of this important issue is warranted. perhaps more critically, our current data suggest that accuracy in microarray analysis is significantly affected by the nature of the transcriptomes being compared in an experiment. in this respect, the platinum spike dataset serves as a useful complement to our previous golden spike dataset. together, they represent extremes of highly balanced  and imbalanced  changes of gene expression between the compared experimental conditions and illustrate the importance of designing and choosing analysis algorithms appropriate for the underlying rna distributions in a microarray experiment. methods that work well on the platinum spike dataset perform far less well on the golden spike dataset, which is much more sensitive to choice of analysis route. although analysis routes that give acceptable performance on both datasets can be identified, they are suboptimal and moreover still depend on appropriate choice of normalization group for each dataset. in the golden spike dataset, which has a considerable degree of imbalanced gene expression between the two compared conditions, using the "conditional" normalization group gives superior performance. however, normalizing in this manner carries the obvious risk that differences introduced by significant technical variation between the conditions--experimental artifact--will be artificially exaggerated rather than eliminated by normalization. the problem lies in the fact that with real experimental data, one cannot know which is more severe, technical variation between the conditions or the degree of "imbalance" in the true rna distributions. therefore, means to properly assess underlying rna imbalances and other dataset-specific issues, and methods to allow for proper normalization among arrays that can account both for imbalance and for technical artifact, are urgently needed in order to guide researchers to the most effective choice of analysis route for their particular experiment. our data suggest that a single "best" route for all microarray experiments may not exist.

