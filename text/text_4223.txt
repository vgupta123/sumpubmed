BACKGROUND
as an integral part of the modencode project , the primary role of the dcc has been to collect, validate, and release data from the  <dig> sub-projects. this is a challenge; not only does the data span a wide range of data types ranging from chromatin profiling to gene model annotation  <cit> , but a large variety of methods and platforms were used to collect and analyze the data over the project’s five year life span, including several changes to the reference genome builds. in addition to d. melanogaster and c. elegans, the modencode data also includes information collected from seven other drosophila and four other caenorhabditis species. to bring order to this diverse data set, the dcc built a multi-step pipeline to automate the submission and validation process. this pipeline uses controlled vocabularies to describe both project metadata and experimental protocols. the pipeline then collects and links submitted data together based on these common controlled vocabularies and executes a semi-automatic qc process to ensure the consistency and completeness of the submitted data.

once both metadata and data pass qc, they are released for use by the public. figure  <dig> shows the data flow of the modencode dcc pipeline from data submission to data release. table  <dig> lists the number and type of data sets released by modencode.

implementation
the modencode dcc provides public access to the modencode data set in a variety of formats, and is also charged with releasing convenient tools for manipulating the data. with respect to data availability, all data released by modencode is available for bulk download from data.modencode.org, and for interactive browsing and mining from intermine.modencode.org. in addition, we have made the complete data set available on amazon cloud  as snapshots  and on the bionimbus cloud  as a shared directory . both cloud environments allow users to compute over the complete  <dig> tb of data without first having to download it to a local compute resource.

for a single user, although galaxy can easily be installed and used locally, , there are two major challenges:  to setup each of the tools and supporting data types;  to configure galaxy to work with a local compute cluster provided one is available. these procedures can take an estimated one to two weeks to complete. additionally, transferring multiple terabytes of data from amazon cloud to the local galaxy will take a significant amount of time and may require additional storage. without a cluster, processing large quantities of data using just a local galaxy server is not practical due to the amount of time necessary for analysis to complete. using galaxy on amazon cloud is an effective alternative as disk space and compute resources can be allocated and terminated dynamically as needed.

the amazon cloud is pay-as-you-go, while bionimbus is an academic cloud environment that is available to researchers via an application process . a getting-started guide for accessing the data set on amazon can be found at http://data.modencode.org/modencode-cloud.html.

galaxy offers bioinformatics analysis reproducibility. any analysis run by a user should be reproducible by another user. galaxy supports this feature via ’workflows’, which users can create and share with each other. each workflow is a sequence of one or more analysis steps with a pre-defined set of parameters and a fix number of inputs. the initial release of galaxy workflows  <cit>  for modencode has focused on peak calling in chip-seq data, taking advantage of the fact that the encode and modencode projects have jointly adopted a uniform peak calling pipeline that provides uniform analysis of human, mouse, worm and fly. the chip-seq workflows use uniform input and output formats, making them easy to use and simplifying the process of performing comparisons among data sets, across species, and between public and private data. the chip-seq tools are available for public use in the galaxy toolshed  and are also preinstalled on the amazon modencode galaxy image described in a later section.

as of february  <dig>  the following dcc-provided chip-seq analysis tools were publicly available in the galaxy toolshed:

•macs <dig> - model-based analysis of chip-seq , written by tao liu 

•peakranger - multi-purpose, ultrafast chip-seq peak caller, written by xin feng 

•idr - reproducibility and automatic thresholding of chip-seq data, written by anshul kundaje 

•spp - cross-correlation analysis package 

•bamedit - merging, splitting, filtering, and qc of bam files, written by ziru zhou

the choice of peak calling tools reflects discussions among the encode and modencode data analysis working groups. the uniform peak calling pipeline adopted by these groups uses macs <dig> for calling broad peaks such as histone marks, spp for calling punctate peaks such as transcription factor binding sites, and idr to rank peaks based on reproducibility among replicates. peakranger was used during early iterations of the uniform peak calling pipeline, but was eventually dropped. its main feature is superior performance and software stability, making it useful for a “quick look” at large genomes that would otherwise take a long time to process using the standard peak callers.

for each of the tools in the galaxy toolshed, we provide full instructions on how to install and use these tools in galaxy  <cit> . except for a few minor changes in the spp source codesa, the same versions of all the tools are installed on the cloud machine images and on galaxy. this provides consistency across platforms regardless of which platform users choose to use for their analysis. update of any of the tools will be done in all platforms to maintain consistency. all previous versions of machine images, tools, and wrappers on galaxy are kept for backward-compatibility.

we have built galaxy workflows for calling peaks and performing consistency analysis across both 2- and 3- replicate chip-seq experiments. this greatly reduces the complexity of executing peak calling. for example, the standard modencode uniform peak calling pipeline, which begins with raw fastq files and ends with peak call bed files, involves  <dig> analysis steps among four software tools. the corresponding galaxy workflow can be executed in just a few clicks to select the input files and the species under consideration. these workflows, along with instructions on how to use them, are available on modencode dcc github  <cit> . we encourage users to use these workflows and fine-tune parameters of tools in the workflow as they see fit.

launching modencode galaxy on amazon cloud
the modencode galaxy instance may be launched as a single virtual machine, or as a cluster of machines that work together to speed up computations; in the latter case, the galaxy user and admin interfaces run on the galaxy instance, and the actual computation is done on a series of “worker nodes.” to run galaxy on the amazon public cloud, users must have an amazon web services  account, and the credentials needed to launch instances in the elastic compute cloud . to simplify the process of creating and configuring one or more galaxy instances, we provide the perl script modencode_galaxy_create.pl and its ec <dig> api command line tools dependencies, which are all available from our modencode galaxy github https://github.com/modencode-dcc/galaxy, in the “bin” directory. to run this script, users will need perl installed on their local machine . prior to running the script the first time, users will need to modify the accompanying configuration file to hold their ec <dig> credentials, and preferences regarding the type, number and location of the galaxy instances they wish to run. in addition to its basic function of launching galaxy clusters in the amazon cloud, the modencode_galaxy_create.pl script also provides status and location information for the launched instances, information on logging into the instance, and convenient tagging of all resources used by the galaxy cluster.

once galaxy is up and running, users must install the modencode tools onto their galaxy cluster in either of two ways:  from toolshed via the galaxy graphical administration interface; or  by running the auto install script from the command line to install all of the modencode tools at once. using the galaxy administration interface, tools can be installed one by one as needed ). this is suitable if users need only a handful of tools from the modencode tools collection. however, we recommend installing all the modencode tools so that multi-step workflows, such as the uniform peak calling pipeline, will run successfully.

the auto_install.sh script, which is also found in modencode galaxy github in the “bin” directory, takes about  <dig> minutes to complete as the downloading and installing are done in parallel. the auto_install.sh script also keeps track of worker nodes with tools downloaded and installed, so when one or more new worker nodes are added to the cluster, auto_install.sh will only download and install tools on these new additional computing nodes. complete detailed and step-by-step instructions on how to launch galaxy on amazon cloud and how to install tools on galaxy are available on our github  <cit> . figure  <dig> panel  shows modencode galaxy after installations of tools and their dependencies. tools put together by modencode dcc are listed under ‘modencode tools’ menu in the tools panel. tutorials on how to run these tools are also available in  <cit> .

figure  <dig> shows one of the main features of modencode galaxy, which allows users to use our faceted browser, search for the data sets they want to use and then import the results directly into their galaxy for further analysis.

RESULTS
the uniform peak calling workflows take a fasta reference genome and either  <dig> or  <dig> pairs of fastq files representing the chip and control  experiments. the workflows then perform the following steps:

•convert each of the input fastq files to sanger format if it is not already in sanger format 

•use bwa  <cit>  to align each of the sanger fastq files against the fasta reference genome. output files are in sam format 

•convert sam format output files to bam format files

•use macs <dig> to call peaks on each pair of chip and control bam files

•use idr to compare the consistency between the replicates, for example, replicate  <dig> vs. replicate 2

•use idr-plot to plot the consistency analysis from the previous step

this section shows how to run the 2-replicate uniform processing/peak calling workflow starting from the raw fastq files. for convenience, below are urls that can be used to obtain the raw fastq files, reference fasta file, and a 2-replicate workflow that were used in this exercise:

# chip and input fastq files for replicates  <dig> and 2

http://data.modencode.org/modencode_test_data/fastq/3381/3381_snyder_daf-12_gfp_l3_rep1_acgt.fastq.gzhttp://data.modencode.org/modenco'de_test_data/fastq/3381/3381_snyder_daf-12_input_l3_rep1_acgt.fastq.gzhttp://data.modencode.org/modencode_test_data/fastq/3381/3381_snyder_daf-12_gfp_l3_rep2_tgct.fastq.gzhttp://data.modencode.org/modencode_test_data/fastq/3381/3381_snyder_daf-12_input_l3_rep2_tgct.fastq.gz

# worm reference genome ws220

http://data.modencode.org/modencode_galaxy/test_data/fasta/ws <dig> fasta

# 2-replicate uniform workflow

http://data.modencode.org/modencode_galaxy/workflows/galaxy-workflow-2-rep-peak-call-bwa-macs2-idr.ga

 <dig>  from the galaxy instance’s web-based user interface, import the above fastq and fasta files by clicking on ‘get data’ and ‘upload file’. cut and paste the above urls into the url/text box and click on ‘execute’. once the uploaded is done, there should be  <dig> entries in the history panel as shown in figure  <dig> 

 <dig>  import the 2-replicate uniform processing/peak calling workflow by clicking on ‘workflow’ and ‘upload or import workflow’

 <dig>  once the workflow is uploaded, click on the workflow and select ‘run’. select the input data for the workflow as follows:

a. step  <dig> input dataset: data entry  <dig> 

a. step  <dig> input dataset: data entry  <dig> 

a. step  <dig> input dataset: data entry  <dig> 

a. step  <dig> input dataset: data entry  <dig> 

a. step  <dig> input dataset: data entry  <dig> 

 <dig>  execute the workflow by clicking on ‘run workflow’.

depending on the load, the workflow should take  <dig> to  <dig> hours to complete. some of the important files output by the workflow are:

•aligned files produced by bwa in bam format

•narrowpeak files produced by macs <dig> 

•overlapped peaks between the two replicates and above threshold generated by idr

•the last step in the workflow is to produce a result pdf file, which shows the consistency between the two replicates.

figure  <dig> shows the visualization of peak output files from the workflow. the top and middle tracks show chromosome ii peaks for chip vs. input for replicate  <dig> and  <dig>  the bottom track is the gene annotation in gtf format for c. elegans.

to run the same workflow on your own data, simply use the galaxy upload features to load your own set of chip/control fastq files. this will provide you with peaks that have been called and qc checked with methods identical to those used adopted by the modencode and encode projects.

amazon cloud & bionimbus cloud
uploading/downloading large data results to/from galaxy via its graphical user interface can be slow ; furthermore, when a galaxy cluster is terminated, all its data and results are deleted. experienced users may prefer doing all analysis via the unix command line rather than using galaxy. to accommodate these users, we have created machine images on both the amazon public cloud and the bionimbus community cloud containing all the tools needed for the encode/modencode uniform processing/peak calling pipeline. to use these images, researchers simply launch them, log into them via the secure shell, and run the pipeline using the provided scripts. because both the amazon and bionimbus cloud images come with a complete copy of the modencode data set, there is no need to copy the data before starting to work with it. step-by-step documentations on how to use either amazon cloud or bionimbus cloud are available at github  <cit> .

other online resources provided by modencode dcc
other modencode computing resources available publicly are modmine, the faceted data browser and download server, and gbrowse. all can be reached from the modencode project portal at http://www.modencode.org.

modmine 
the modmine database and web interface  <cit>  are based on the intermine data warehousing system  <cit>  to provide researchers with a powerful infrastructure to query the modencode metadata and data. data produced by the  <dig> sub-projects are integrated with information from other public data sources in order to increase their utility. for instance, by including mappings to orthologous genes in other organisms  <cit> , the opportunity to carry out comparative studies is provided. other external data sources integrated in modmine included: genome annotations from wormbase  <cit>  and flybase  <cit> , gene ontology  annotations  <cit> , physical and genetic interactions  <cit> , protein information  <cit> , and protein domains  <cit> .

apart from its ability to integrate data from multiple sources, modmine has other useful features:  the ability to work with lists  and to provide basic analysis for them ;  to access a library of commonly used search tasks available as ‘search templates’;  to be able to extract data from a defined list of chromosomal locations ; and , keyword searching with logical operations and faceted results. modmine also provides extensive web-services and code-generation, which users can utilize to extract data programmatically from modmine directly to their local computers. documentation and tutorials on how to use modmine are available from http://www.modencode.org.

faceted data browser and download server 
the faceted browser  is a lightweight web-based application that allows users to search and explore modencode data by applying a combination of filters using a shopping catalogue metaphor. the filter system gives the user a flexible and intuitive mechanism for visualizing the diversity of data sets available, and narrowing them down to the subset that is relevant to his or her work. currently, the following filters are available on the faceted browser: organism, project category, genomic target element, technique, principle investigator, assay factor, developmental stage, strain, cell line, tissue, compound, and temperature. each filter displays the number of data sets that would be selected if that filter is turned on, and the effects are interactive. for example, after we select “rna-seq” from technique, and “d. melanogaster” from organism, then the development stage filter shows that there are  <dig> adult female experiments,  <dig> adult male experiments,  <dig> late embryo experiments, and so forth. when a search is performed or when one or more filters are selected, the faceted browser displays its results  in a uniform format thus making it straightforward to compare between data sets and/or across species. furthermore, once the list of data sets is narrowed down to the user’s satisfaction, he or she can press one of the navigational buttons to send the results to external applications such as galaxy, gbrowse, modmine, or to create an archive to download the data files to their local computers.

traditional ftp access to the modencode data set is available from http://ftp://data.modencode.org. directories on the ftp server are organized into species and further divided into different data categories and data types. at the lowest level of directories are data files. all data files follow a common naming pattern, which is described in detailed at http://ftp://data.modencode.org/readme.

it is also possible to launch a personal instance of the faceted browser and ftp server within the amazon cloud, letting users create customized instances of these services. information on how to do this is available at http://data.modencode.org/modencode-cloud.html.

generic genome browser  
all level  <dig> modencode data sets can be browsed and analyzed in a genome browser using the gbrowse  <dig>  software. the browser offers track-level configuration, stable user accounts, and a data track uploading facility. in addition to using the public modencode gbrowse instance, advanced users can run a copy of the genome browser server within the amazon cloud, thereby allowing them to host a complete copy of the modencode data set and to add their own private data sets to the corpus. instructions for doing this can be found at http://data.modencode.org/modencode-cloud.html.

CONCLUSIONS
we provided ready-to-use tools and workflows on galaxy for the uniform processing/peak calling pipeline, which has been adopted by both the modencode and encode communities. using the resources we put together not only save users time from installing tools and reference data but also provides consistent and reproducible analysis results. our tools and workflows are designed to work with human, mouse, worm, and fly genomes and thus cross species comparisons are possible. for advanced users, we also provide machine images on both amazon and bionimbus clouds so the same consistent and reproducible analysis can be done from the command line.

future work
although the worm and fly genomes are relatively small, the encode/modencode uniform processing/peak calling pipeline may take several hours to complete on a single machine. the human and mouse genomes, which are over 20× larger, take proportionately longer to run. to speed up the analysis, we are parallelizing the tools in the galaxy-based encode/modencode uniform processing/peak calling pipeline so that the work can be split among multiple worker nodes. similarly, we are adding support for sun grid engine to the amazon and bionimbus modencode machine images, thereby simplifying the task of running the pipeline on a virtual compute cluster.

availability and requirements
project name: modencode galaxy

project homepage:https://github.com/modencode-dcc/galaxy

operating system: unix

programming language: perl

other requirements: ec <dig> api tools, galaxy amazon machine image 

license: all modencode galaxy scripts are available free of charge to academic, government, and non-profit institutions.

endnote
achanges in spp source code are mainly:  to make spp work on galaxy;  to generalize parameters of spp so that it works with all four species .

abbreviations
dcc: data coordinating center; idr: irreproducible discovery rate; go: gene ontology; modencode: model organism encyclopedia of dna elements; nih: national institutes of health; qc: quality control.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
qmt and lds are responsible for the conception, design, and writing of the manuscript. fj, zz, and kmc are responsible for the designs and implementations of scripts to launch galaxy on amazon cloud and configurations of tools on galaxy toolshed. mdp is responsible for the curation of modencode data. etk is responsible for the main modencode pipeline. sc is responsible for the development and maintaining of modmine. pr is responsible for the development and maintaining of gbrowse. all authors read and approved the final manuscript.

