BACKGROUND
the ability of microarray technology to simultaneously measure expression values of thousands of genes has brought major advances. the measurement of gene expression may be done within a relatively short time to quantify genome-wide expression levels. on the other hand, statistical analyses to extract useful information from such high dimensional data face well known challenges. common mistakes in conducting statistical analyses were reported  <cit> . particularly class prediction studies are subject to concerns about reliability of results  <cit> , where genes involved in predictive models depend heavily on the subset of samples used to train the models. this is related to the likelihood of false positive findings due to the curse of dimensionality in microarray gene expressions datasets  <cit> .

methods for aggregating gene expression data across experiments exist  <cit> . data standardization is proposed as a preliminary step in cross-platform gene expression data analyses , as raw gene expression datasets are recommended to be used  <cit>  and gene expression values may be incomparable across different experiments. meta-analysis is known to increase the precision of the effect estimate and to increase the statistical power to detect a certain effect size . in class prediction, meta-analysis methods can have different objectives, ranging from methods for combining effect sizes  <cit>  or combining p values  <cit>  to rank-based methods  <cit> . however, there is no meta-analysis method known to be generally superior to others  <cit> .

in this study, we compared the performance of classification models on a given gene expression dataset between gene selection through meta-analysis on other studies and conventional supervised gene selection. a single gene expression dataset with less than a hundred samples is likely not enough to determine whether a particular gene is an informative gene  <cit> . thus, gene selection based on multiple microarray studies may yield a more generalizable gene list for predictive modeling. we used raw gene expression datasets from six published studies in acute myeloid leukemia  to develop predictive models using  <dig> different classification functions to classify patients with aml versus normal healthy controls. in addition, a simulation study was conducted to more generally assess the added value of meta-analysis for predictive modeling in gene expression data.

methods
as a starting point, we assume d gene expression datasets are available for analysis. first, the d raw datasets are individually preprocessed. next,  <dig> classifiers are trained on expression values from the j
th study  by incorporating variable selection procedure via limma method and externally validated on the remaining d- <dig> gene expression datasets. we refer to these models as individual-classification models.

to aggregate gene expression datasets across experiments, d gene expression datasets are divided into three major sets, namely  a set for selecting probesets ,  for predictive modeling using the selected probesets from set <dig>  and  for externally validating the resulting predictive models . the data division is visualized in fig.  <dig>  we next describe the predictive modeling with gene selection via meta-analysis -classification model). first, significant genes from a meta-analysis on set <dig> are selected. next, classification models are constructed on set <dig> using the selected genes from set <dig>  the models are then externally validated using the independent data in set <dig>  the ma-classification approach is briefly described in table  <dig> and is elaborated in the next subsections.fig.  <dig> data division to perform cross-platform classification models building and their characteristics. 





data extraction
raw gene expression datasets from six different studies were used in this study, as previously described elsewhere  <cit> , i.e. e-geod- <dig>  <cit>  , e-geod- <dig>  <cit>  , e-geod- <dig>  <cit>  , e-mtab- <dig>  <cit>  , e-geod- <dig>  <cit>   and e-geod- <dig>  <cit>  . five studies were conducted on affymetrix human genome u <dig> plus  <dig> array and one study was performed on u133a . the raw datasets were pre-processed by quantile normalization, background correction according to manufacturer’s platform recommendation, log <dig> transformation and summarization of probes into probesets by median polish to deal with outlying probes. we limited analyses to  <dig>  common probesets that appeared in all studies.

meta-analysis for gene selection
we aggregated d- <dig> gene expression datasets to extract informative genes by performing a random effects meta-analysis. this means meta-analysis acts as a dimensionality reduction technique prior to predictive modeling. for each probeset, we pooled the expression values across datasets in set <dig> to estimate its overall effect size. let y
ij and θ
ij denote the observed and the true study-specific effect size of probeset i in an experiment j, respectively. the random effects model of a probeset i is written as: yij=θij+εij,whereθij=θi+δijfori= <dig> …,pandj= <dig> …,d− <dig>  


where p is the number of tested probesets, θ
i is the overall effect size of probeset i, ε
ij ~ n with σ
ij <dig> as the within-study variance and δ
ij ~ n with τ
i <dig> as the between-study or random effects variance of probeset i. the study-specific effect size θ
ij is defined as the corrected standardized mean different  between two groups, estimated by:  <dig> θ^ij=x¯ij0−x¯ij1sij1−34nj0+nj1− <dig>  


where x¯ij0x¯ij <dig> is the mean of base- <dig> logarithmically transformed expression values of probeset i in group  <dig> . s
ij is originally defined as the square root of the pooled variance estimate of the within-group variances  <cit> . this estimation of σ
ij, however, is rather unstable in a small sample size study. we utilized the empirical bayes approach implemented in limma to shrink extreme variances towards the overall mean variance. thus, we define s
ij as the square root of the variance estimate from the empirical bayes t-statistics  <cit> . the second component in eq. is the hedges’ g correction for smd  <cit> . the estimation of between-study variance τ^i <dig> was performed by paule-mandel  method  <cit>  as suggested by  <cit> 

for each probeset, a z-statistic was calculated to test the null hypothesis that the overall effect size in the random effects meta-analysis model is equal to zero . to adjust for multiple testing, p-values based on z-statistics were corrected at a false discovery rate  of α = 5%, using the benjamini-hochberg  procedure  <cit> . we considered probesets that had a significant overall effect size as informative probesets. for each informative probeset i, the estimated overall effect size θiθ^i is:  <dig> θ^i=∑jwijθ^ij∑jwij, 


where wij=1/τ^i2+sij <dig> 

classification model building
the following classification methods were used to construct predictive models: linear discriminant analysis , diagonal linear discriminant analysis   <cit> , shrunken centroid discriminant analysis   <cit> , random forest   <cit> , tree-based boosting   <cit> , l2-penalized logistic regression , l1-penalized logistic regression   <cit> , elastic net  <cit> , feed forward neural networks   <cit> , support vector machines   <cit>  and k-nearest neighbors   <cit> . a detailed description of the classification methods, model building procedure as well as the tuning -parameter was presented in our previous study  <cit> . the class prediction modeling process for both individual- and ma-classification models was done by splitting the dataset in set <dig> into a learning set ℒ and a testing set t. the learning set ℒ was further split by cross validation into an inner-learning set and inner-testing set, to optimize the parameters in each classification model. the optimal models were then internally validated on the out-of-bag testing set t. henceforth, we referred to the testing set t as an internal-validation set v <dig> 

for ma-classification models on set <dig>  we used all the probesets identified as differentially expressed by meta-analysis procedure in set <dig>  except for lda, dlda and nnet methods, which cannot handle a larger number of parameters than samples. for these methods, we incorporated top-x probesets to the predictive modeling, where x was less than or equal to the sample size minus  <dig>  the top lists of probesets were determined by ranking all significant probesets on their absolute estimated pooled effect sizes  from eq.. as the number of probesets to be included was itself a tuning parameter, we varied the number of included probesets from  <dig> to the minimum number of within group samples. for other classification functions, we used the same values of tuning parameter as described in our previous study  <cit> .

for the individual-classification approach, we optimized the classification models based on a single gene expression dataset . here, we applied the limma procedure  <cit>  to determine top-x relevant probesets, controlling the false discovery rate at 5% using the bh procedure  <cit> . the optimum top-x was selected among{ <dig>   <dig>   <dig>  200} for classification methods other than lda, dlda and nnet. we used the same number of selected probesets for the three aforementioned classification methods as in the ma-classification approach. in each case, we evaluated the classification models by the proportion of correctly classified samples to the number of total samples, known as a classification model accuracy.

model validation
the optimal classification models obtained from the previous step were externally validated on set <dig>  the log <dig> expression values of the data in set <dig> for the probesets used in the classification models were scaled to the log <dig> expression values of the data in set <dig>  so that the learning and the validation sets had comparable range. for each probeset i, we assumed the expression values were in the interval [a
i, b
i] in set <dig> and [c
i, d
i] in set <dig>  a log <dig> expression value x
is of probeset i in sample s from set <dig>  was scaled to the scale of set <dig> by the following transformation formula:  <dig> fxis=ai+bi−aixis−cidi−ci,di≠ci. 


predictive models were then applied to the scaled log <dig> gene expression data in set <dig> 

for individual-classification, we rotated the single learning dataset and validated the models on the other d- <dig> datasets. for ma-classification, we rotated the datasets used for selecting informative probesets  as well as learning  and validating  classification models. for each possible combination of d- <dig> datasets, we repeated step 3– <dig> of our approach . due to a small number of samples in data <dig>  we omitted the predictive modeling process when it was selected as set <dig>  hence, the possible gene expression datasets in set <dig> were data <dig>  data <dig>  data <dig>  data <dig> and data6; and gene expression datasets in set <dig> were data <dig>  data <dig>  data <dig>  data <dig>  data <dig> and data <dig>  rendering thirty possible combinations to divide d =  <dig> datasets to three distinct sets.

simulation study
we generated synthetic datasets by conducting simulations similar to that described by jong et al  <cit> . we refer to the publication for more detail description of each and every parameter stated in this sub-section. among parameters to simulate gene expression data , we applied these following parameters for all simulation scenarios, i.e.  the number of genes per data set ;  the pairwise correlations of noisy genes were set equal to zero ,  the proportion of differentially expressed genes  and;  the parameter of an exponential distribution to draw the variances of the genes . further, the number of samples per dataset , the log <dig> fold changes of differentially expressed  genes  and pairwise correlations of de genes  were varied as follows: n =  <dig>   <dig>  150; Δ =  <dig> ,  <dig> ,  <dig> ; and ρ =  <dig> ,  <dig> ,  <dig> , respectively. we define pairwise correlation of noisy  genes as the correlation between any and every two pairs of noisy  genes. table  <dig> shows nine combinations from these parameters, which reflect the amount of information in each simulated gene expression dataset. in the first block  for instance, the dataset generated by parameters in simulation # <dig> contains less information than the dataset generated by parameters in simulation # <dig>  which is caused by the low degree of log <dig> fold changes and high correlation of de genes.table  <dig> parameters to generate simulated gene expression datasets

n
ρ
a
b
symbols. n: the number of samples in each generated dataset; Δ: the log <dig> fold changes of differentially expressed  genes. ρ: pairwise correlation of de genes


athe number of genes that were stated as differentially expressed  genes by ma approach from  <dig> cumulative studies. all the selected genes are true positives


bthe number of true de genes among the top- <dig> de genes selected by limma procedure




for each scenario mentioned in table  <dig>  we simulated data that consisted of n* <dig> samples from the same population. the data was then randomly divided into  <dig> different sub-datasets of n samples each . next, the sub-datasets were randomly chosen to be considered as  set1: a set of fifty datasets for selecting probes via meta-analysis;  set2: a dataset for predictive modeling;  set3: a dataset for validation. in the ma-predictive modeling, we estimated classification model accuracies when the number of studies for variable selection were ranging from  <dig> to  <dig> studies.

random effects linear regression
we quantified the difference in performance between classification models that were optimized with and without incorporating information from other studies in the simulation study by a random effects linear regression model. the difference of model accuracy between ma- and individual-classification procedure for a classification model c based on a simulation scenario s is denoted as d
csm. such differences were calculated when ma-classification procedure incorporated m studies  to select features. having rescaled the d
csm to be in the range of  <dig> and  <dig> by 1+dcsm <dig>  we then transformed d
csm using the logit function to get unbounded and more approximately normally distributed outcome values. given in each simulation setting we calculated d
csm for different number of m studies for feature selection in ma approach, we used a fully crossed random effects model, where simulation setting s and the number of studies for ma-approach m acted as clustering factors or random effects. additionally, since the same classification methods were applied to build prediction models, classifier c was added as a random effect term.

we then tested three determinants  that might contribute to the difference in performance of classification models that were trained by two approaches , namely the number of samples per dataset , the log <dig> fold changes of differentially expressed  genes  and pairwise correlations of de genes . each of the determinant was individually evaluated in the random effects model. more formally, the random effects model for the k
th determinant is written as: d'csm=β0+ϑ0c+ϑ0s+ϑ0ms+β1xk, 


where d ' csm is the logit transformation of the scaled d
csm; ϑ
0s, ϑ
0m and ϑ
0c are the random intercepts with respect to the simulation setting s ), the number of studies for meta-analysis m  ~ n) and classification model c ) respectively.


software
all analyses were performed in r statistical software using these packages: affy for preprocessing procedures  <cit> ; meta for meta-analysis  <cit> , cma for predictive modeling  <cit> , lme <dig> for the random effects linear model  <cit>  and ggplot <dig> for data visualization  <cit> .

RESULTS
we first present the performance of classification models when each individual study was used to optimize the classification functions  in aml datasets. as the first illustration, we considered the case for which data <dig> was used for optimization. to start with, we compared the distribution of expression values in the validation sets data <dig> to data <dig> to the expression values in data <dig>  there seemed to be a considerable difference in the distributions of expression values between studies, with data <dig> having a lower range than other experiments, indicating that data standardization across studies was necessary . gene expression values in data <dig> to data <dig> were effectively scaled by using eq. so that they had comparable ranges as in data <dig> . the classification models optimized in data <dig>  were validated with data <dig> to data <dig>  the classification models performed poorly in all  <dig> validation sets, notably worst in data <dig> and data <dig> . when data <dig>  data <dig>  data <dig> and data <dig> were used to optimize the classifiers, we found similar results .fig.  <dig> the distribution of expression values after pre-processing step from the first three samples in six experiments. the expression values are in log <dig> scale




the comparison of the accuracies of classification models that were trained by ma- with individual-classification procedures based on optimization with data <dig> is shown in fig.  <dig>  in most cases, ma-classification models outperformed individual-classification models. the difference of model accuracies between ma- and individual-classification approach was considerably larger when data <dig> was used as a validation set. on average, classification methods that require the number of features to be smaller than the number of samples , seemed to improve with the ma-classification approach. when validated against data <dig>  all models seemed to benefit from the ma-classification approach.fig.  <dig> plot of the difference of classification model accuracies between ma- and individual-classification approach, when data <dig> was used as a training data




in the other cases , we noticed that ma-classification approach did not outperform the individual-classification models when the models were validated on data <dig>  the ma-classification approach reduced the classification model accuracies by up to 50%, as compared to individual-classification models. as the ma-classification approach mostly resulted in a lower number of genes used in the predictive models than individual-classification approach, it might be hard for ma-classification models to outperform individual-classification models when validated on data <dig>  as de genes in this dataset  had a low degree of log <dig> fold change . on the other hand, most of ma-classification models outperformed individual-classification models when they were validated on data <dig> . given that  the ma-approach was better in selecting the “true” de genes  and more importantly  the average log <dig> fold change of the de genes in data <dig> was considerably high, i.e.  <dig> , in most cases the classifiers benefited from the ma-approach. incorporating information from other experiments in these datasets did not consistently improve the predictive ability of classification models when externally-validated. the simulation study was conducted to evaluate the difference of classification model accuracies between the ma- and individual-classification approach more generally. the results showed that the ma-classification approach was more likely to improve the classification model accuracy when it was conducted in a set of less informative datasets . we defined a less informative dataset as a dataset with a small number of samples, a low degree of log <dig> fold changes of the de genes and a high level of pairwise correlation of de genes. in this type of dataset, feature selection via limma method hardly selected the true de genes in the individual-classification approach. among the true  <dig> de genes in each simulated dataset, the limma procedure could select  <dig> to  <dig> de genes. meanwhile, all selected genes by ma approach were truly de genes . as we observed in the aml data, classification methods that require the number of features less than the number of samples  performed better with the feature selection prior to predictive modeling via meta-analysis.fig.  <dig> plot of the difference of classification model accuracies between ma- and individual-classification approach in the simulated datasets, when Δ =  <dig> , γ =  <dig>  and  n =  <dig>   n =  <dig>   n =  <dig> . the aforementioned simulation parameters resulted in the less informative datasets




factors that might contribute to the difference of classification model accuracy between the ma- and individual-classification approach, were individually evaluated by random effect models. this resulted in the log <dig> fold changes and pairwise correlation between de genes as the significant factors. both factors were consistent with the finding that a set of less informative datasets benefited from the ma-classification approach . further, there was no additional variation in the difference in performance between ma- and individual-classification approach that was associated with the number of datasets used to select features in meta-analysis approach . a possible explanation of this finding could be that five datasets used in ma-classification approach were enough to select relevant variables so that the quality of the variable selection was not further increased by the increasing the number of datasets. this might also explain all the true positive genes selected by ma-approach in the simulation study. table  <dig> results of the random effects models

σ
0c
σ
0s
σ
0m

n

ρ
each factor was evaluated individually in the random effects linear regression model. the coefficients were inverse transformed to the original scale of the difference of classification model accuracy between ma- and individual classification approach


abbreviations: ll lower limit, ul upper limit

symbols: n: the number of samples in each generated dataset; Δ: the log <dig> fold change of differentially expressed  genes. ρ: pairwise correlation of de genes. σ
0c, σ
0s and σ
0m are the standard deviation of the random intercepts with respect to classification model, scenario in the simulation study and the number of studies used for selecting relevant features via meta-analysis approach. see method section for more details regarding the random effect models




discussion
this study applied a meta-analysis approach for feature selection in predictive modeling on gene expression data. selecting informative genes among massive noisy genes in predictive modeling faces a great challenge in microarray gene expression data. dimensionality reduction is applied to reduce the number of noisy genes as well as to reduce the possibility of predictive models choosing clinically irrelevant biomarkers. an extra step to generate a gene signature list is usually applied in practice , including predictive modeling via embedded classification methods . selected informative genes may depend on the sub-samples used in the analysis  <cit> , which may lead to the lack of direct clinical application  <cit> .

previous research on the application of meta-analysis in differential gene expression analysis showed that a single study might not contain enough samples to make a conclusion whether a particular gene is an informative gene. among  <dig>  common genes from  <dig> combined samples,  <dig> to 90% of the genes needed more samples in order to draw a conclusion  <cit> . a very low sample size as compared to the number of genes can cause false positive finding  <cit> . involving thousands of samples is a straight forward solution but it can be very costly and time consuming. a possible solution to increase the sample size is by combining gene expression datasets with a similar research question through meta-analysis.

meta-analysis is known as an efficient tool to increase statistical power and to obtain more generalizable results. although a number of meta-analysis methods have been used as a feature selection technique in class prediction, no method has been shown to perform better than others  <cit> . in this study, we combined the corrected standardized effect size for each gene by random effects models, similar to a study conducted by choi et al  <cit> . however, we estimated the between-study variance by paule-mandel method, which outperforms the dersimonian-laird method in continuous outcome data  <cit> . we used a broad selection of classification functions to build predictive models in order to evaluate the added value of meta-analysis in aggregating information from gene expression across studies.

six raw gene expression datasets resulting from a systematic search in a previous study in acute myeloid leukemia   <cit>  were preprocessed,  <dig>  common probesets were extracted and used for further analyses. we assessed the performance of classification models that were trained by each single gene expression dataset. the models were then validated on datasets obtained from other studies. classification models that were externally validated might suffer from heterogeneity between datasets, due to, for instance, different sample characteristics and experimental set-up.

for some datasets, gene selection through meta-analysis yielded better predictive performance as compared to predictive modeling on a single dataset, but for others, there was no major improvement. evaluating factors that might account for the difference in performance of the two predictive modeling approaches on real-life datasets could be confounded by uncontrolled variables in each dataset. as such, we empirically evaluated the effects of fold change, pairwise correlation between de genes and sample size on the added value of meta-analysis as a gene selection method in class prediction with gene expression data.

the simulation study was performed to evaluate the effect of the level of information contained in a gene expression dataset. for a given number of samples, we defined an informative gene expression data as a dataset with large log <dig> fold changes and low pairwise correlation of de genes. the simulation study shows that the less informative datasets  benefited from ma-classification approach more clearly, than the more informative datasets. the limma feature selection method on a single dataset had a higher false positive rate of de genes compared to feature selection via meta-analysis. incorporating redundant genes in the predictive model may weaken the performance of a classification model on independent datasets. while conventional procedures use the same experimental data, meta-analysis uses a number of datasets to select features. thus, the chances of sub-samples-dependent features to be included in a predictive model are reduced in ma- than in individual-classification approachand the gene signature may be widely applied.

for ma, we defined the effect size as a standardized mean difference between two groups. although we individually selected differentially expressed probesets , we incorporated information from all probesets by applying limma procedure in estimating the within-group variances ). this empirical bayes moderated t-statistics produces stable variances and it is proven to outperform ordinary t-statistics  <cit> . marot et al implemented a similar approach in estimating unbiased effect sizes  in  <cit> ) and they suggested to apply such approach to estimate the study-specific effect size in meta-analysis of gene expression data.

we analyzed gene expression data at the probeset level. when more heterogeneous gene expression data from different platforms are used, mapping probesets to the gene level is a good alternative. annotation packages from bioconductor  <cit>  and methods to deal with multiple probesets referring to the same gene may be considered, if such mapping is applied in a cross-platform gene expression study. a point to consider in cross-platform analysis of microarray experiments is data standardization. the same genes may have different signal in different experiments, due to e.g. different array technology and scanning process. we investigated the distributions of expression values across experiments and found incomparable ranges of expression values across experiments. despite its simple nature, the scaling formula in eq. produces common ranges of gene expression values across experiments. some methods to scale gene expression across experiments were proposed  <cit> . we do not expect that different scaling methods give significantly different findings as presented here, although it may be interesting to study.

we individually pre-processed the selected gene expression datasets, adjusted by the microarray platform in each and every study. a different preprocessing method may lead to different results of the prediction models, but it is not covered in this study. the predictive ability of a classification model may depend on a set of samples that is used in the preprocessing and normalization step. the rank-based genes is preferred over raw expression values to generate gene expression data  <cit> . although we do not expect the present conclusions to change, it could be interesting to investigate this procedure further in this context.

CONCLUSIONS
a meta-analysis  approach was applied to select relevant features from multiple studies. based on the simulation study, the ma approach was better in terms of variable selection than the predictive modeling by using a single dataset. in particular, a less informative dataset  was likely to benefit from feature selection via meta-analysis for class prediction. this also held for classification methods that require a smaller number of features than samples. given the present public availability of omics datasets, meta-analysis approach can be used more often as an alternative gene selection method in class prediction.

additional files

additional file 1: a supplementary material file. 

 
additional file 2: r scripts. 

 
additional file 3: the simulated datasets. this folder contains synthetic datasets that were generated by using parameters described in table  <dig>  

 


abbreviations
amlacute myeloid leukemia

dedifferentially expressed

dldadiagonal linear discriminant analysis

knnk-nearest neighbors

lassol1-penalized logistic regression

ldalinear discriminant analysis

mameta-analysis

nnetfeed forward neural networks

rfrandom forest

ridgel2-penalized logistic regression

scdashrunken centroid discriminant analysis

smdstandardized mean difference

svmsupport vector machines

tbbtree-based boosting

