BACKGROUND
statistical analysis of high dimensional data, i.e. those for which the number of parameters p is much larger than the number of samples m, often involves testing of multiple hypotheses. this is because models typically associate one parameter to each feature on the microarray used, which may represent a part of or a whole gene  or any genomic section. so if classic statistical methods are used to analyse the data per feature, computed p-values must be jointly corrected for multiple testing  <cit> . of course, the larger the number of hypotheses tested, the stronger the correction for multiple testing must be in order to keep the error rate acceptably low.

to decrease the penalty incurred by multiple testing correction, some articles  make a selection of features prior to the data analysis that, it is hoped, are more likely to not conform with the null hypothesis. for clarity, we call such features non-null, in contrast with those that follow the null hypothesis which we call null features. by having a weaker correction for multiple testing, it is also hoped to improve power. however, such selection may have undue effect on results. firstly, by leaving some features out of the analysis altogether, some non-null features will also be left out, therefore putting a bound on potential power. secondly, it is impossible to select only features that do not follow the null hypothesis - had we known which ones these were, we would not have needed testing in the first place. so many features, including some null, are left in, and as such multiple testing correction must still be used. however, commonly used multiple testing correction methods rely on the assumption that the null p-values follow a uniform distribution, which may no longer be the case amongst the selected features. this may introduce bias on the corrected p-values.

the effect of feature selection on power to detect probes that are differentially expressed between two groups can also be assessed. feature selection yields an increase of the p-value significance threshold and, as a consequence, the power is largely expected to increase too. however, as we will show this is not always the case.

thus feature selection methods may affect both power and overall error rate estimation. however, neither can be evaluated in practice, as they require knowledge of which features conform  with the null hypothesis. so it is not straightforward to know in practice the exact effect of feature selection. in this paper we shall evaluate the effects of feature selection procedures, first theoretically and subsequently illustrated by both a simulation study and publicly available experimental data. these selection procedures are often referred to as "filters", because they are meant to "filter out" some of the noise  of the data. we shall consider multiple testing correction methods that estimate the false discovery rate , under the assumption that null p-values follow a uniform distribution . we shall describe common types of filter used in the methods section, and subsequently study their impact on both overall estimated error rates and power.

methods
filtering violates fdr methods' assumption
here we assume a study setup commonly found in practice, involving gene expression profiles of two groups of independent samples, with the null hypothesis h0i : μia = μib representing no differential expression between the two groups a and b, for any given gene i, and a corresponding two-sided alternative hypothesis hai : μia ≠ μib. let {vi}, {ri}, i =  <dig>  ..., m be sets of binary variables taking values in { <dig>  1}, such that vi =  <dig> if gene i follows h <dig>  and ri =  <dig> if gene i is left in the data after filtering. let us also consider the filter statistic w = w , so that gene i is filtered whenever w  ≥ w for a chosen value w, where z represents a test statistic. then we can write ri = i{w  ≥ w} for all i. for more details about this setup, see section "study design" in additional file  <dig> 

in practice, multiple testing correction still needs to be used after filtering. methods that aim at handling the fdr typically assume that, under h <dig>  the p-values yielded by the statistical test satisfy p~u <cit>  in general, so multiple testing correction after filtering requires that their null distribution, represented by g <dig>  after filtering is represented by g0w and also satisfies g0w=u for a given filter statistic w. its cumulative distribution function  is

  g0w=pr{piw≤u|h0}              =pr{pi≤u|vi= <dig> ri=1}, 

thus the equality g0w=g <dig> for all u, holds if, and only if,

  pr{pi≤u|vi= <dig>  ri=1}=pr{pi≤u|vi=1} 

which means that ri must be independent of vi and of pi. in other words, if the filter selects null features from the entire range  <cit>  with equal probability, then the null distribution of the p-values remains u <cit> . this assumption is notably difficult to check, as it is not known which of the remaining p-values follow  u <cit> . note also that this is not required from alternative features.

filter statistics
various criteria are used by researchers to filter features out of a dataset. we aim at evaluating filter effects on error estimates and power and, as such, will consider a few filter types used in practice, but these are not intended to cover all possible filters.

a commonly used method involves leaving out of the dataset features with measurement very close to, or less than, background. we shall refer to this as the signal filter, and we base it on the average signal observed for the feature over the two groups, i.e. ws=/ <dig>  a second type of filter commonly used is based on the absolute value of the  fold change, i.e. wfc= |x¯−y¯|. it aims at leaving out of the analysis features with too small a fold change to be biologically interesting, and we shall refer to it as the fold change filter. a third type of filter of practical interest leaves out of the analysis features that overall vary less than a certain given threshold. this variance filter assumes that the feature-specific variance reflects how much discrimination that feature may yield between the groups, and we shall express it as wv=sz <dig> 

the aforementioned statistics will be used to filter out uninformative features here.

effect of filters on multiple testing correction
multiple testing correction can be done in a variety of ways. essentially, methods aim at controlling/estimating either one of two different error types, namely family-wise error rate  and false discovery rate . in most gene expression data analysis applications, it is of interest to handle the fdr, as it makes more sense to talk about the proportion of false positives in a list of genes declared differentially expressed, instead of the probability of making at least one mistake in all tests. for this reason, we focus on methods that aim at handling the fdr, most of which assume that null p-values follow a u <cit> , so can be biased by the use of a filter that affects the validity of this assumption.

thresholds for significance yielded by multiple testing methods increase as the number of hypotheses tested m decreases. for example, consider the original benjamini and hochberg  <cit>  step-up procedure for  control of the fdr for independent test statistics, which can be described as follows. in order to control the fdr at level ϕ, reject the null hypothesis h0i whenever the p-value pi is no greater than /m for each i =  <dig>  ..., m. when some filter is applied to the data, resulting in γm features retained for further analysis, the new fdr threshold becomes /, which is larger than the one for all features as  <dig> <γ <  <dig>  this suggests that an improved power may result from filtering, as a larger threshold may select more truly differentially expressed genes. however, the actual effect on power and on achieved fdr depends on the filter statistic used. to consider those analytically it is convenient to write the multiple testing procedure as an explicit function of the empirical cdf of the p-values.

the commonly used fdr-controlling multiple-testing procedures suggested by y. benjamini and co-authors can all be expressed in terms of the empirical cdf of the p-values gm  <cit> . indeed, consider the general procedure of selecting p-values pi satisfying pi ≤ u*, where

  u*=maxu{g≤gm, 0≤u≤1}, 

and u represents possible values for the random variables {pi} . then different functional forms of g will yield the different fdr methods: benjamini and hochberg  <cit> 's step-up procedure uses g = u/ϕ , the adaptive fdr of benjamini et al.  <cit> , which corrects for the proportion of null features π <dig>  uses g = u/ϕ , both independent of i, and benjamini and yekutiely  <cit> 's method uses g=u/ϕ∑ji1/j. for more details see section "fdr methods and p-values distributions" in additional file  <dig> 

the effect of filtering on power can then be evaluated by using the relation gm = π0g <dig> - ga. doing so the expression on the right-hand side of  <dig> becomes maxu{/ ≤ ga ,  <dig> ≤ u ≤ 1}. for example, if after filtering the original benjamini-hochberg fdr is to be used, the power is given graphically by the intersection gaw=, where π0w represents the proportion of null features after filtering .

for one example of such a development, see section "fdr and power as function of fraction filtered out" in additional file  <dig> 

for some methods it is not possible to express them as explicit functions of the p-values' cdf. then numerical methods can be used to evaluated the effect of filtering, as done in our simulation study in section "simulation study".

student's t test
to further evaluate the effect of filtering, null and alternative distributions of the test statistic, before and after filtering, must be known. a commonly used statistic in the study setup used here to test the null hypothesis h <dig> : μx = μyagainst ha : μx≠ μy is

  t=x¯nx−y¯ny1/2=x¯nx−y¯nysp1/nx+1/ny, 

where we assume for simplicity that σx2=σy2=σ <dig>  so sp <dig> represents the pooled variance. under h <dig> the distribution of t is a student's t distribution with v = nx + ny -  <dig> degrees of freedom.

the effect of filtering can be evaluated via conditioning on the filter statistics. for example, if a fold change filter is used, the conditional cdf can be written as

  ftw=pr{t≤t|   |x¯nx−y¯ny|   ≥w}              =pr{t≤t,|x¯nx−y¯ny|  ≥w}pr{|x¯nx−y¯ny|  ≥w}. 

similar expressions can be derived for the other filter statistics .

once an expression for the pdf of the test statistics after filtering is obtained, we can obtain the pdf and cdf of the p-values using the relation p =  <dig>  which holds since p = p{t > |t0|} =  <dig> - f + f and f is assumed to be symmetric . similar relationships can also be obtained for non-symmetric f. for expressions corresponding to some of the filter statistics, see section "filtering and p-values distribution" in additional file  <dig> 

based upon such expressions obtained with the various filter statistics, we display the effect of each filter on the null and alternative distributions of p-values on figure  <dig>  from it we can see that the fold change filter leaves out mostly features with p-values near  <dig>  whilst the variance filter leaves out more p-values near  <dig>  suggesting the latter is more likely to leave out non-null features.

a test for filtering-induced fdr bias
since the bias on the fdr is yielded by the effect of a filter w on the null p-values distribution g <dig>  we propose to compare an estimate of the distribution g0w to the expected uniform. in the setup used here, g0w can be estimated by performing the same statistical analysis on the dataset where row and column labels are permuted, and subsequently applying the filter w. then g^0w can be compared to the uniform using a kolmogorov-smirnov test, yielding a p-value q. in fact, we use a benjamini-hochberg fdr-correction for the p-values, that is equivalent to a one-sided kolmogorov-smirnov and thus will be less conservative. this process can be repeated a number n <dig> of times, so yielding an empirical p-value distribution, gq say, for the comparison between the filtered null p-value distribution and the u <cit> . by comparing gq to the uniform using again a kolmogorov-smirnov test, it can be concluded whether filtering affects the null p-value distribution and, as such, fdr estimates. if so, researchers may wish to either consider other types of filter, or avoid using a filter altogether. for more details, see section of the same name in additional file  <dig> 

simulation study
study setup
to investigate the properties of the filters described in subsection "filter statistics" a simulation study is carried out. we use a setup that mimics a microarray experiment where thousands of features are measured simultaneously, based upon a setup first suggested by langaas et al.  <cit>  and described in detail in additional file  <dig>  briefly, per feature a two-sample student-t test statistic was calculated and converted to a two-sided p-value accordingly. the p-value list was then filtered using each one of the filter statistics considered here, and subsequently fdr-corrected by either one of the following methods: benjamini-hochberg  <cit> , benjamini-yekutieli  <cit> , adaptive benjamini-hochberg  <cit>  and storey  <cit> , represented respectively by bh, by, abh and qv. to guarantee comparability, the same fraction  of p-values was removed in all cases. for each simulated list, features are declared differentially expressed yielding an fdr of 5%, and subsequently both the achieved fdr  and the observed power  were calculated.

note that fdr estimation using the qvalue method relies on p-values taking values on the entire  <cit>  interval. thus, we only used the variance and signal filter statistics with this method.

illustration of filter statistics
we shall show how each filter statistic affects power and fdr estimates using one of the simulated datasets chosen at random. first, the filter statistics used have little association with each other, as evidenced by the lack of pattern described by the dot clouds . it is also clear that, when using any of these filter statistics, there is always a fraction of the truly differentially expressed features that is wrongly left out, which can never be declared differentially expressed.

this can also be seen in figure  <dig>  where the empirical distribution of the filter statistics grouped by the underlying hypothesis is displayed. the gray vertical lines indicate deciles of the filter, increasing from left to right, so that if 50% of the data is to be left out then all features with filter statistic up to the fifth gray line from the left are neglected. thus, filter statistics that have the least overlap between their distributions under the null  and alternative  hypotheses are expected to improve power, which is the case with the fold change filter. however, the opposite is true for the variance and signal filters, implying that these filter statistics tend to leave many non-null features out of the dataset. this is natural, as the fold change filter makes use of the group labels, which the other ones do not.

filtering and fraction removed
an ideal filter only removes null features, thus decreasing the chance of making false positives. as a consequence the proportion of true null hypotheses, π <dig>  decreases when compared to the whole set of features. so it is interesting to compare the filter statistics based upon the behaviour of π <dig>  as the proportion of features retained γ varies from  <dig> to  <dig> 

as references, we consider both the best filter possible, which leaves out null features until there are none left, and a random filter, which leaves out null and non-null features with equal probability . in figure 3 π <dig> for the best and random filters serve as bounds below and above for all others filters. amongst realistic filters, the best performances are obtained with the fold change filter, although it does not perform as well as the best filter. the variance filter performs worse than those, which is not surprising as it leaves proportionately more features out with small p-values than the others . the worst performance is yielded by the signal filter, its performance slightly better than the random filter.

fdr and power
since some filters are more likely to leave out alternative features than others, they will also have a different effect on power and achieved fdr. we evaluated the mean of each of these quantities across all  <dig> simulated datasets.

as the fdr is controlled at 5% in all cases, the achieved fdr is expected to remain constant as the proportion of filtered out features increases, but this is not always observed . indeed, when using either bh or abh, the fold change filter yields an increase on the achieved fdr that gets larger as more features are filtered out. interestingly, the variance and signal filters do not have this effect on the achieved fdr. a decrease on achieved fdr, while not as bad a problem as an increase since it means more conservative results, is also undesirable and is observed in some degree with all fdr methods. it is also noteworthy that two of the fdr methods  overestimate it even with no filtering, while one  underestimates it over the entire range. the variance and signal filters showed the smallest induced bias on the fdr, for all methods considered. from the viewpoints of both bias at no filtering  and trend for increasing values of x, using bh with variance or signal filter yielded the best results: the achieved fdr was closest to the required 5% for most filtered-out proportions x.

our intuition also suggests that power should increase after filtering. again here this is proven to not always hold . for example, the power yielded after using the signal filter almost invariably decreases, so that it is worse to use this filter than to not filter at all. the power yielded after using the variance filter increases slightly compared with no filtering, but the amount depends on the fdr method used. the fold change filter seems here to be the best, yielding considerable increases of power when used.

by considering both measures together a better picture emerges of the cost-benefit relationship of using each filter statistic. we construct an equivalent to a roc curve for this  using bh. to start with, the signal and variance filters always yield less power after any amount of filtering, a trend that gets stronger as the fdr threshold increases. on the other hand, the test statistic and fold change filters yield an improved power for each fdr thresh-old after filtering, for commonly used fdr threshold values up to  <dig>  and filtered out ratios not larger than  <dig> . interestingly, the cost-benefit relationship between observed power and achieved fdr is constant for the fold change filter, regardless of the fraction filtered out x, whilst it deteriorates for the other filters as x increases .

testing for filtering-induced fdr bias
the test for filtering-induced bias proposed  can be easily applied to one of the simulated datasets. all filter statistics considered here are used, with a range of filtered out fractions . after permutation and filtering, signal and variance filters yield null p-values approximately uniformly distributed for all filtered-out fractions, whilst the fold-change filter does not, even if only 10% of the features are filtered out. correspondingly, the fdr bias  is larger for the two latter and negligible for the two former filter statistics, relatively for each fdr method. note that the achieved fdr curves are very similar to those shown in figure  <dig>  as expected. in a similar way, the corresponding observed power  follows the same curves as the ones shown in figure s <dig> of additional file  <dig>  with the p-values from the fdr bias test given, we can see that the situations where bias is introduced are correctly picked up.

based upon these results, we conclude that the signal and variance filters introduce no significant fdr bias, but also yield little power gain. the fold change filter, on the other hand, does introduce bias on the fdr, but may yield improved power for ϕ <  <dig>  and  <dig> - γ <  <dig> . so, no filter statistic displays superior results all-round.

experimental data: childhood leukemia
in contrast to a simulation study, in this case it is not known which features are null or alternative. since these are essential to measure achieved power and false discovery rates, we take the same approach as that from van wieringen and van de wiel  <cit> , which is to choose an experiment with plenty of samples in each group to be compared, from which small subsets are selected and analysed. the idea here is to use the dataset with all available samples as the truth, so that the achieved power is estimated as the number of features found in the subset as well as in the whole data, divided by the total number of features found in the whole data. similarly, the achieved fdr is estimated as the proportion of features selected in the subset that was not selected in the whole dataset.

we use a leukemia gene expression dataset described and first analysed by den boer et al.  <cit> . briefly, the dataset consists of peripheral blood samples, from which rna was isolated and hybridized to affymetrix u133a microarrays, according to the manufacturers' protocol, and the data was subsequently pre-processed as described by den boer et al.  <cit> . we shall use the data corresponding to samples with the tel-aml translocation  and hyperdiploid  of their training cohort . to compare the groups, we apply an empirical-bayes linear regression model as implemented in the bioconductor package limma <cit> , and the yielded p-values are corrected for multiple testing by benjamini-hochberg's fdr  <cit> . we evaluate power and fdr bias in three study sizes  to check if sample size may affect results.

the true positive rate and the achieved fdr were calculated for various filter thresholds ranging from 0- <dig>  using the fold-change, variance and signal filters . with the fdr level fixed at 5%, as the fraction filtered out increased the achieved fdrs with both the signal and variance filters remained stable around 5%, but increased with the fold change filter, in agreement with the simulation study results . interestingly, this fdr bias seemed invariant to the sample size. on the other hand, there is a strong relationship between sample size and observed power , in spite of the model being used taking advantage of the large number of genes in the study to improve power for detection of differential expression. for each fixed sample size, however, results are similar to those for the simulation study, with the fold change filter improving the power but the signal and the variance filter having no or the opposite effect. these confirm our conclusions that power increase via filtering is linked to an fdr bias.

RESULTS
filtering features is a common practice in high-dimensional data analysis, aimed at minimizing the penalty due to multiple testing correction and, consequently, increasing power. as we have shown in this paper, an increase in power is linked to introducing bias on the fdr. this is because any filter statistic that filters out only noise is bound to be associated to the test statistic and, therefore, affects the null distribution of p-values and introduces fdr bias.

the fact that filtering introduces fdr bias is evident from published articles. for example, querec and co-authors  <cit>  used a fold-change filter to increase their list of  <dig> differentially expressed genes to  <dig>  both obtained with 5% fdr. of the longer list  <dig> genes are validated by rt-pcr, of which  <dig> are confirmed, yielding in fact an fdr between  <dig> and 20%. the sought power increase could have been achieved instead by using a more suitable analysis model for their data than a per-gene anova, such as the one proposed by smyth  <cit>  and implemented in the bioconductor package limma.

we have assumed in this study that a student's t test, or an empirical bayes linear regression model, is used to find differential gene expression between the two groups. however, results are not specific to these models. indeed, had the wilcoxon rank-sums test been used similar results would be produced. to illustrate this, we show power and achieved fdr computed in our simulation study using this test statistic and the benjamini-hochberg fdr . the fdr curves  and the power curves  are very similar to the ones produced using the t-test statistic.

the conclusion that fdr bias may result from the use of filtering is also not dependent upon the shape of the alternative hypothesis. indeed, in much the same way as for the two-sided alternative, for a one-sided alternative such as is the case when an f test is used in anova, it still holds that whenever gw ≠ g the f-test p-values do not follow a uniform distribution under h <dig> 

filtering can also affect the fit of models that estimate the distribution of one  parameters across all genes, such as empirical bayes models like the one proposed by smyth  <cit> . indeed, such models rely on a large number of features with certain common characteristics, and if for example half of the features with small variance are left out, it can be that the distribution for the sample variance may no longer be well-described by the model.

other authors have also attempted to handle the effect of filtering on multiple testing correction. mcclintick et al.  <cit>  used permutations to estimate the number of false positives, but ignored the fact that if a filter is used the null distribution of p-values may be affected. hackstadt and hess  <cit>  also propose a framework that makes objective use of the p-value distribution, but assumed without criticism that power is increased after filtering. our proposed framework allows us to not only demonstrate that an fdr bias may be introduced by filtering, yielding important understanding about the problem, but also to evaluate this bias, and its effect on power, using a variety of fdr formulations and filter statistics.

to the best of our knowledge, we are the first to propose a statistical test to check if filtering introduces fdr bias. it can be used in any application for any combination of statistical model, filtering setup and fdr method. in our simulation study filter statistics that use group information are found to introduce bias, whilst those approximately independent from group information do not, as expected. we suggest researchers use this test to make decisions of whether or not to apply any filtering to the data.

our fdr bias test differs from examining the density of the left-out values as mentioned elsewhere  <cit> . we believe that, as these p-values typically include a  set corresponding to non-null features, even if the filtered-out p-values do have an empirical distribution close to the uniform, it can still be that the fdr is biased.

a violation of the uniform null distribution assumption also occurs when there is correlation among features, as previously pointed out by  <cit> . this served as motivation to propose resampling-based fdr procedures which preserve the original dependence structure among features. we checked if these fdr-estimating methods would be affected by filtering in our simulation study . our conclusion is that the methods tested do not avoid introducing fdr-bias as a result of filtering in the context considered. further research would be needed to better understand the behaviour of these fdr methods when filters are used.

on the basis of our results, we believe it is unlikely that a two-step approach involving testing and filtering improves power and does not bias the fdr. our conclusion is thus that two-step approaches should be avoided in general, extending to a general microarray study the conclusions of pounds and cheng  <cit>  that "the use of even the best filter may hinder, rather than enhance, the ability to discover interesting probe sets or genes", obtained for filters such as present/absent calls  using simulation and experimental data.

it often occurs that researchers wish to prioritize features via fold change, say, from a compiled list of differentially expressed genes, estimated to contain a fixed percentage ϕ false positives, with the goal of making a shorter list for in-lab validation. while this does not bias the multiple testing correction as is done a posteriori, researchers should be aware that the shorter list is no longer expected to have the same percentage ϕ of false positives. here we note that, in our simulation study, for some combinations of fdr estimation method and filter the fdr was preserved after post-fdr filtering, but no power improvement resulted . a better alternative would be to incorporate the fold-change filter threshold into the statistical model used, as suggested by mccarthy and smyth  <cit>  and zhang and cao  <cit> . a similar approach could be used to derive a statistical test that combines a two- or multiple-group comparison and the variance filter, based upon the f statistic. in general, however, for each choice of statistical model and filter statistic a new combined model needs to be worked out.

alternatives, for any generic filter and test, to avoid filtering-induced fdr-bias would be to adapt the multiple testing correction method to relax the assumption of uniform distribution for the null features in a way that filtering-induced bias is avoided, or to devise a way of correcting the fdr bias. these issues deserve further research if two-step approaches are to yield correct results.

CONCLUSIONS
we showed both in theory and in applications that, when a statistical test follows a filter to prioritize features for further analysis, power increase is linked to an fdr bias, making results look too optimistic. our proposed statistical test for fdr bias can be used to guide researchers in their decision as to whether or not to filter, and as to the filter setup to use, such as the filter statistic and the proportion of features filtered out.

software
all the computations were performed using r version  <dig> . <dig>  <cit>  and the bioconductor  packages multtest , qvalue  and genefilter . all the figures were made using basic r graphics and packages geneplotter , lattice  and rcolorbrewer . all scripts used here are available from the authors upon request. r scripts and functions implementing the simulation and reproducing the figures and results presented here can be found at: http://www.humgen.nl/microarrayanalysisgroup.html.

authors' contributions
mvi has developed the framework and performed all computations. jmb participated in discussions and coordination. rxm gave the original idea and supervised the work. all authors collaborated in writing, read and approved the final manuscript.

supplementary material
additional file 1
this document contains details of some of the theoretical developments in the article, as well as figures.

click here for file

 acknowledgements
we thank p.a.c. 't hoen, j.j. goeman, j.t. den dunnen, m. van de wiel and w. van wieringen for fruitful discussions, and comments and suggestions of two anonymous reviewers which helped clarify the text. this study was supported by a grant from the centre for medical systems biology within the framework of the netherlands genomics initiative /netherlands organisation for scientific research . this work is part of the biorange bioinformatics research program supported by the netherlands bioinformatics centre .
