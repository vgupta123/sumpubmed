BACKGROUND
a clique is a fully-connected subgraph in a finite, simple graph. the problem of determining whether or not a graph has a clique of a given size, called simply clique, is one of the best known and most widely studied combinatorial problems. although classically formulated as an np-complete decision problem  <cit> , where one is merely asked to determine the existence of a certain size clique, the search and optimization formulations are probably most often encountered in practice, where one is asked to find a clique of given size and largest size respectively. in computational biology, one needs to look no farther than pubmed to gauge clique's utility in a variety of applications. a notable example is the search for putative molecular response networks in high-throughput biological data. popular clique-centric tools include clique community algorithms for clustering  <cit>  and paraclique-based methods for qtl analysis and noise abatement  <cit> .

a clique is maximal if it cannot be augmented by adding additional vertices. a clique is maximum if it is of largest size. a maximum clique is particularly useful in our work on graphs derived from biological datasets. it provides a dense core that can be extended to produce plausible biological networks  <cit> . other biological applications include the thresholding of normalized microarray data  <cit> , searching for common cis-regulatory elements  <cit> , and solving the compatibility problem in phylogeny  <cit> . see  <cit>  for a survey of additional applications of maximum clique.

any algorithm that relies on maximum clique, however, has the potential for inconsistency. this is because graphs often have more than just one maximum clique. idiosyncrasies between algorithms, or even among different implementations of the same algorithm, are apt to lead to an arbitrary choice of cliques. this motivates us to find an efficient mechanism to enumerate all maximum cliques in a graph. these can then be examined using a variety of relevant criteria, for example, by the average weight of correlations driven by strain or stimulus  <cit> .

we therefore seek to solve the maximum clique enumeration  problem. unlike maximal clique enumeration, for which a substantial body of literature exists, very little seems to be known about mce. the only exception we have found is a game-theoretic approach for locating a predetermined number of largest cliques  <cit> .

while very little prior work seems to have been done on mce, the problem of maximal clique enumeration has been studied extensively. since any algorithm that enumerates all maximal cliques also enumerates all maximum cliques, it is reasonable to approach mce by attempting first to adapt existing maximal clique enumeration algorithms. an implementation of an existing maximal clique enumeration algorithm also provides a useful runtime benchmark that should be improved upon by any new approach. besides maximal clique enumeration algorithms, another potential strategy is to compute the maximum clique size and then test all possible combinations of vertices of that size for connectivity. while this approach may be reasonable for very small clique sizes, as the maximum clique size increases the runtime quickly becomes prohibitive, and we mention it only for completeness, and focus our efforts on modifying and extending existing algorithms for enumerating maximal cliques.

current maximal clique enumeration algorithms can be classified into two general types: iterative enumeration  and backtracking . iterative enumeration algorithms, such as the method suggested by kose et al  <cit> , enumerate all cliques of size k at each stage, test each one for maximality, then use the remaining cliques of size k to build cliques of size k +  <dig>  the process is typically initialized for k =  <dig> by enumerating all vertex subsets of size  <dig> and testing for connectivity. in practice, such an approach can have staggering memory requirements, because all cliques of a given size must be retained at each step. in  <cit> , this approach is improved by using efficient bitwise operations to prune the number of cliques that must be saved. nevertheless, storage needs can be excessive, since all maximal cliques of one size must still be made available before moving on to the next larger size. figure  <dig> shows the number of maximal cliques of each size in one of the graphs near the median size in our testbed. this graphic illustrates the enormous lower bounds on memory that can be encountered with iterative enumeration algorithms.

many variations of backtracking algorithms for maximal clique enumeration have been published in the literature. to the best of our knowledge, all can be traced back to the algorithms of bron and kerbosch first presented in  <cit> . some subsequent modifications tweak the data structures used. others change the order in which vertices are traversed. see  <cit>  for a performance comparison between several variations of backtracking algorithms. as a basis for improvement, however, we implemented the original, highly efficient algorithm of  <cit> . we made this choice for three reasons. first, an enormous proportion of the time consumed by enumeration algorithms is spent in outputting the maximal cliques that are generated. this output time is a practical limitation on any such approach. second, a graph can theoretically contain as many as  <dig> maximal cliques  <cit> . it was shown in  <cit>  that the algorithm in  <cit>  achieves this bound in the worst case. no algorithm with a theoretically lower asymptotic runtime can thus exist. third, and most importantly, the improvements we introduce do not depend on the particulars of any one backtracking algorithm; they can be used in conjunction with any and all of them.

RESULTS
using the seminal maximal clique enumeration algorithm due to bron and kerbosch  <cit>  as a benchmark, we designed, implemented, and extensively tested three algorithmic improvements, the last based on observations about the nature of graphs produced by transcriptomic data. along with describing these improvements, we will describe our existing tool for finding a single maximum clique, based on the theory of fixed-parameter tractability   <cit> . such a tool is essential for all three improvements, since the first two rely on knowledge of the maximum clique size, and the last uses the maximum clique finding tool as a subroutine. all codes are written in c/c++ and compiled in linux. for testing, we use  <dig> graphs derived from  <dig> different datasets which are publicly available on geo. we concentrate on transcriptomic data, given its abundance, and eschew synthetic data, having learned long ago that effective algorithms for one have little bearing on the other.  in an effort to improve performance, we scrutinize the structure of transcriptomic graphs and explore the notion of maximum clique covers and essential vertex sets. indeed, we find that with the right preprocessing we are able to tailor algorithms to the sorts of data we routinely encounter, and that we can now solve instances previously considered unassailable.

algorithms
in the following sections, we describe each of the mce algorithms we implemented and tested. the first is the algorithm of bron and kerbosch  <cit> , which we call basic backtracking and use as a benchmark. since all our subsequent improvements make use of an algorithm that finds a single maximum clique, we next describe our existing tool, called maximum clique finder , which does just that. we next modify the basic backtracking algorithm to take advantage of the fact that we only want to find the maximum cliques and can quickly compute the maximum clique size. we call this approach intelligent backtracking, since it actively returns early from branches that will not lead to a maximum clique. we then modify mcf itself to enumerate all maximum cliques, an approach we call parameterized maximum clique, or parameterized mc. in a sense this is another backtracking approach that goes even further to exploit the fact that we only want to find maximum cliques. finally, based on observations about the properties of biological graphs, we introduce the concepts maximum clique covers and essential vertex sets, and apply them to significantly improve the runtime of backtracking algorithms.

basic backtracking
the seminal maximal clique publication  <cit>  describes two algorithms. a detailed presentation of the second, which is an improved version of the first, is provided. it is this second, more efficient, method that we implement and test. we shall refer to it here as basic backtracking. all maximal cliques are enumerated with a depth-first search tree traversal. the primary data structures employed are three global sets of vertices: compsub, candidates and not. compsub contains the vertices in the current clique, and is initially empty. candidates contains unexplored vertices that can extend the current clique, and initially contains all vertices in the graph. not contains explored vertices that cannot extend the current clique, and is initially empty. each recursive call performs three steps:

• select a vertex v in candidates and move it to compsub.

• remove all vertices not adjacent to v from both candidates and not. at this point, if both candidates and not are empty, then compsub is a maximal clique. if so, output compsub as a maximal cique and continue the next step. if not, then recursively call the previous step.

• move v from compsub to not.

note that not is used to keep from generating duplicate maximal cliques. the search tree can be pruned by terminating a branch early if some vertex of not is connected to all vertices of candidates.

vertices are selected in a way that causes this pruning to occur as soon as possible. we omit the details since they are not pertinent to our modifications of the algorithm.

the storage requirements of basic backtracking are relatively modest. no information about previous maximal cliques needs to be retained. in the improvements we will test, we focus on speed but also improve memory usage. thus, such limitations are in no case prohibitive for any of our tested methods. nevertheless, in some environments, memory utilization can be extreme. we refer the interested reader to  <cit> .

our basic backtracking implementation serves as an initial benchmark upon which we can now try to improve.

finding a single maximum clique
we use the term maximum clique finder  to denote the software we have implemented and refined for finding a single clique of largest size  <cit> . mcf employs a suite of preprocessing rules along with a branching strategy that mirrors the well-known fpt approach to vertex cover  <cit> . it first invokes a simple greedy heuristic to find a reasonably large clique rapidly. this clique is then used for preprocessing, since it puts a lower bound on the maximum clique size. the heuristic works by choosing the highest degree vertex, v, then choosing the highest degree neighbor of v. these two vertices form an initial clique c, which is then iteratively extended by choosing the highest degree vertex adjacent to all of c. on each iteration, any vertex not adjacent to all of c is removed. the process continues until no more vertices exist outside c. since |c| is a lower bound on the maximum clique size, all vertices with degree less than |c - 1| can be permanently removed from the original graph. next, all vertices with degree n -  <dig> are temporarily removed from the graph, but retained in a list since they must be part of any maximum clique. mcf exploits a novel form of color preprocessing  <cit> , used previously in  <cit>  to guide branching. this form of preprocessing attempts to reduce the graph as follows. given a known lower bound k on the size of the maximum clique, for each vertex v we apply fast greedy coloring to v and its neighbors. if these vertices can be colored with fewer than k colors, then v cannot be part of a maximum clique and is removed from the graph. once the graph is thus reduced, mcf uses standard recursive branching on vertices, where each branch assumes that the vertex either is or is not in the maximum clique.

intelligent backtracking
given the relative effectiveness with which we can find a single maximum clique, it seems logical to consider whether knowledge of that clique's size can be helpful in enumerating all maximum cliques. as it turns out, knowledge of the maximum clique size k leads to a small, straightforward change in the basic backtracking algorithm. specifically, at each node in the search tree we check if there are fewer than k vertices in the union of compsub and candidates. if so, that branch cannot lead to a clique of size k, and so we return. see figure  <dig>  while the modification may seem minor, the resultant pruning of the search tree can lead to a substantial reduction in the search space. in addition to this minor change to branching, we apply color preprocessing as previously described to reduce the graph before submitting it to the improved backtracking algorithm. color preprocessing combined with the minor branching change we call intelligent backtracking.

paramaterized enumeration
given that mcf employs a vertex branching strategy, we investigated whether it could be modified to enumerate not just one, but all maximum cliques. it turns out that mcf, also, lends itself to a straightforward modification that results in enumeration of all maximum cliques. the modification is simply to maintain a global list of all cliques of the largest size found thus far. whenever a larger maximum clique is found, the list is flushed and refreshed to contain only the new maximum clique. when the search space has been exhausted, the list of maximum cliques is output.

we must take special care, however, to note that certain preprocessing rules used during interleaving are no longer valid. consider, for example, the removal of a leaf vertex. the clique analogue is to find a vertex with degree n -  <dig> and remove its lone non-neighbor. this rule patently assumes that only a single maximum clique is desired, because it ignores any clique depending on the discarded vertex. therefore this particular preprocessing rule must be omitted once branching has begun.

maximum clique covers
if we view mcf as a black box subroutine that can be called repeatedly, it can be used in a simple greedy algorithm for computing a maximal set of disjoint maximum cliques. we merely compute a maximum clique, remove it from the graph, and iterate until the size of a maximum clique decreases. to explore the advantages of computing such a set, we introduce the following notion:

definition  <dig> a maximum clique cover of g =  is a set v' ⊆ v with the property that each maximum clique of g contains some vertex in the cover.

the union of all vertices contained in a maximal set of disjoint maximum cliques is of course a maximum clique cover , because all maximum cliques must overlap with such a set. this leads to a useful reduction algorithm. any vertex not adjacent to at least one member of an mcc cannot be in a maximum clique, and can thus be removed.

in practice, we find that applying mcc before the earlier backtracking algorithms yields only marginal improvement. the concept of mcc does, however, lead to a much more powerful approach based on individual vertices. since any improvement made by mcc is subsumed by the next approach, we do not test mcc by itself.

essential vertex sets
our investigation of the mcc algorithm revealed that it typically does not reduce the size of the graph more than the preprocessing rules already incorporated into mcf. for example, mcf already quickly finds a lower bound on the maximum clique size and removes any vertex with degree lower than this bound. upon closer examination, however, we found that for  <dig> of  <dig> graphs that we initially tested for the conference version of this paper, only one clique was needed in an mcc. that is to say, one maximum clique covered all other maximum cliques. and in our current testbed of  <dig> graphs, in every case a single maximum clique suffices for an mcc. in fact this coincides closely with our experience, in which we typically see high overlap among large cliques in the transcriptomic graphs we encounter on a regular basis. based on this observation, we shall now refine the concept of mcc. rather than covering maximum cliques with cliques, we cover maximum cliques with individual vertices.

we define an essential vertex as one that is contained in every maximum clique. of course it is possible for a given graph to have no such vertex, even when it contains many overlapping maximum cliques. but empirical testing of large transcriptomic graphs shows that an overwhelming number contain numerous essential vertices. and for purposes of reducing the graph, even one will suffice. an essential vertex has the potential to be extremely helpful, because it allows us to remove all its non-neighbors. we employ the following observation: for any graph g, ω >ω if and only if v covers all maximum cliques, where ω is the maximum clique size of g.

we define an essential set to be the set of all essential vertices. the essential set  algorithm, as described in figure  <dig>  finds all essential vertices in a graph. it then reduces the graph by removing, for each essential vertex, all non-neighbors of that vertex. the es algorithm can be run in conjunction with any of the backtracking mce algorithms, or indeed prior to any algorithm that does mce by any method, since its output is a reduced graph that still contains all maximum cliques from the original graph. as our tests show, the runtime improvement offered by the es algorithm can be dramatic.

implementation
we implemented all algorithms in either c or c++. the code was compiled using the gcc  <dig> . <dig> compiler on the ubuntu linux version  <dig> . <dig> operating system as well as the gcc  <dig> . <dig> compiler under debian linux version  <dig> . all timings were conducted in the latter debian environment on dedicated nodes of a cluster to ensure no affect on timings from concurrent processes. each node had a dual-core intel xeon processor running at  <dig>  ghz and  <dig> gb of main memory.

testing
in the conference version of this paper, we used three different datasets at  <dig> thresholds each to derive a total of  <dig> graphs on which to test our algorithmic improvements. while these graphs certainly sufficed as an initial proof of concept, two concerns could be raised regarding them. first, one might argue that three datasets are not a sufficiently large sample size to provide a true sense of the overall nature of transcriptomic data or an algorithmic improvement's general effectiveness on such data, the large number of thresholds notwithstanding. and second, since the three datasets are proprietary and not publicly available, the results were not as readily reproducible as they might otherwise have been. obtaining de-identified versions, while feasible, was an unnecessary obtacle to reproducibility.

we address such concerns here by creating a new suite of transcriptomic graphs on which to test our algorithmic improvements. the suite consists of graphs derived from  <dig> datasets obtained from the gene expression omnibus   <cit> , a publicly accessible repository. for each dataset, graphs were created at four different thresholds, for a total of  <dig> graphs. the datasets were selected to provide a reasonably diverse sampling of experimental type, species, and mrna microarray chip type. they cover  <dig> different species and a number of different experimental conditions such as time series, strain, dose, and patient. since our graphs are derived from thresholding correlation values, we excluded from consideration any dataset with fewer than  <dig> conditions. thresholding correlations calculated using so few conditions can produce unacceptably large rates of false positives and false negatives. the number of conditions range from a low of  <dig> to a high of  <dig>  nine of the datasets had not been log-transformed, in which case we performed log-transformation. four of the datasets contained missing values; in these cases we used correlation p-values rather than correlations for the threshold. see table  <dig> for a listing of the geo datasets used for testing.

the  <dig> datasets obtained from the gene expression omnibus   <cit> . all datasets were retrieved between 4-04- <dig> and 4-23- <dig>  each dataset was log-transformed if it had not been already. for each dataset, four different correlation thresholds were used to build unweighted graphs.

from the expression data, we first constructed weighted graphs in which vertices represented probes and edge weights were pearson correlation coefficients computed across experimental conditions. we then converted the weighted graphs into unweighted graphs by retaining only those edges whose weights were at or above some chosen threshold, t. for each dataset, we chose four values for t. all size/density values were within the spectrum typically seen in our work with biological datasets. the smallest graph had  <dig>  vertices and  <dig>  edges; the largest had  <dig>  vertices and  <dig> , <dig> edges.

the number of maximum cliques for the graphs in our testbed ranged from  <dig> to  <dig>  as seen with our previous testbed, there was no discernible pattern based on graph size or density. one might ask why there is such wide, unpredictable variability. it turns out that the number of maximum cliques can be extremely sensitive to small changes in the graph. even the modification of a single edge can have a huge effect. consider, for example, a graph with a unique maximum clique of size k, along with a host of disjoint cliques of size k -  <dig>  the removal of just one edge from what was the largest clique may now result in many maximum cliques of size k -  <dig>  edge addition can of course have similar effects. see figure  <dig> for an illustrative example.

for each algorithm on each graph, we conducted timings on a dedicated node of a cluster to avoid interference from other processes. if the algorithm did not complete within  <dig> hours, it was halted and the graph was deemed to have not been solved. we chose thresholds to spread the runtimes of the graphs out over the five algorithms we were testing. the largest  threshold was selected so that a majority of the algorithms, if not all, solved the graph. the smallest  threshold was selected so that at least one of the algorithms, but not all, solved the graph.

on each graph we timed the performance of basic backtracking, intelligent backtracking, and paramaterized mc. we then reduced the graphs using es and retested with intelligent backtracking and parameterized mc, in which case the runtimes include both the reduction and the enumeration step. as expected, basic backtracking was found to be non-competitive. both intelligent backtracking and parameterized mc showed a distinct, often dramatic, improvement over basic backtracking. figure  <dig> shows the runtimes of each of the five methods on all  <dig> test graphs. on some of the easier graphs, ones taking less than three minutes to solve, the overhead of es actually caused a minor increase in the overall runtime. but on the more difficult instances its true benefit became apparent, reducing runtime by an order of magnitude or more. and in all cases where two or fewer algorithms solved the graph, the algorithm was either es with intelligent backtracking, es with parameterized mc, or both.

CONCLUSIONS
es serves as a practical example of an innovative algorithm tailored to handle a difficult combinatorial problem by exploiting knowledge of the input space. it succeeds by exploiting properties of the graphs of interest, in this case the overlapping nature of maximum cliques. more broadly, these experiments underscore the importance of considering graph types when testing algorithms.

it may be useful to examine graph size after applying mcc and es, and compare to both the size of the original graph and the amount of reduction achieved by color preprocessing alone. figures  <dig> and  <dig> depict original and reduced graph sizes for five graphs we originally tested.

while mcc seems as if it should produce better results, in practice we find it not to be the case for two reasons. first, the vertices in an mcc may collectively be connected to a large portion of the rest of the graph, and so very little reduction in graph size takes place. and second, any reduction in graph size may be redundant with fpt-style preprocessing rules already in place.

contrast to random graphs
it would have probably been fruitless to test and design our algorithms around random graphs.  in fact it has long been observed that the topology of graphs derived from real relationships differs drastically from the erdös-rényi random graph model introduced in  <cit> . attempts to characterize the properties of real data graphs have been made, such as the notion of scale-free graphs, in which the degrees of the vertices follow a power-law distribution  <cit> . while work to develop the scale-free model into a formal mathematical framework continues  <cit> , there remains no generally accepted formal definition. more importantly, the scale-free model is an inadequate description of real data graphs. we have observed that constructing a graph so the vertices follow a power law  degree distribution, but where edges are placed randomly otherwise using the vertex degrees as relative probabilities for edge placement, still results in graphs with numerous small disjoint maximum cliques. for instance, constructing graphs with the same degree distribution as each of the  <dig> biological graphs in our original testbed resulted in maximum clique sizes no greater than  <dig> for even the highest density graphs. compare this to maximum clique sizes that ranged into hundreds of vertices in the corresponding biological graphs. other metrics have been introduced to attempt to define important properties, such as cluster coefficient and diameter. collectively, however, such metrics remain inadequate to model fully the types of graphs derived from actual biological data. the notions of maximum clique cover and essential vertices stem from the observation that transcriptomic data graphs tend to have one very large highly-connected region, and most  of the maximum cliques lie in that space. furthermore, there tends to be a great amount of overlap between maximum cliques, perhaps as a natural result of gene pleiotropism. such overlap is key to the runtime improvement achieved by the es algorithm.

future research directions
our efforts with mce suggest a number of areas with potential for further investigation. a formal definition of the class of graphs for which es achieves runtime improvements may lead to new theoretical complexity results, perhaps based upon parameterizing by the amount of maximum clique overlap. furthermore, such a formal definition may form the basis of a new model for real data graphs. we have noted that the number of disjoint maximum cliques that can be extracted provides an upper bound on the size of an mcc. if we parameterize by the maximum clique size and the number of maximum cliques, does an fpt algorithm exist? in addition, formal mathematical results may be achieved on the sensitivity of the number of maximum cliques to small changes in the graph.

note that any mcc forms a hitting set over the set of maximum cliques, though not necessarily a minimum one. also, a set d of disjoint maximum cliques, to which no additional disjoint maximum clique can be added, forms a subset cover over the set of all maximum cliques. that is, any maximum clique c ∉ d contains at least one v ∈ d. see figure  <dig>  to the best of our knowledge, this problem has not previously been studied. all we have found in the literature is one citation that erroneously reported it to be one of karp's original np-complete problems  <cit> .

for the subset cover problem, we have noted that it is np-hard by a simple reduction from hitting set. but in the context of mce we have subsets all of the same size. it may be that this alters the complexity of the problem, or that one can achieve tighter complexity bounds when parameterizing by the subset size. alternately, consider the problem of finding the minimum subset cover given a known minimum hitting set. the complexity of this tangential problem is not at all clear, although we conjecture it to be np-complete in and of itself. lastly, as a practical matter, exploring whether an algorithm that addresses the memory issues of the subset enumeration algorithm presented in  <cit>  and improved in  <cit>  may also prove fruitful. as we have found here, it may well depend at least in part on the data.

competing interests
the authors declare that they have no competing interests.

authors' contributions
jde wrote the software employed in this study. cap produced and collected problem instances from transcriptomic data and performed exhaustive timings. glr provided background investigation and coordinated data presentation. mal conceived of the project and coordinated manuscript preparation. all authors participated in algorithm design and analysis, and read and approved the final manuscript.

