BACKGROUND
genetic association studies at the population level are one of the most promising ways to discover the genetic basis of subtle human phenotypes such as complex diseases or drug responses  <cit> . the aim of these studies is to map genetic factors underlying such phenotypes by comparing genetic information and phenotypes of individuals sampled from a population. as whole-genome sequencing for each individual remains currently impossible, the genetic information is typically assessed through a set of genetic markers that carry information on their neighborhoods due to linkage disequilibrium . single nucleotide polymorphisms , the most common type of polymorphisms in the human genome, are markers of great interest in this context. in fact, they are so common that the information they carry seems highly redundant in high ld regions of the human genome. consequently, it makes sense to select a small fraction of the snps, the tag snps, for mapping purposes. this can significantly reduce genotyping effort without much loss of power  <cit> . one of the main goals of the international hapmap project is to acquire the knowledge of the ld structure needed for the choice of efficient tag snps  <cit> .

the stake in the choice of the tag snps has driven the development of numerous methods for their selection during the past few years. these methods were reviewed by halldórsson et al.  <cit> . in their view, methods differ mostly in two major aspects: the quality or correlation measure used for the definition of tag snps and the algorithm used for the minimization of the final number of tag snps. for instance, we can seek a subset of snps such that every snp that does not belong to the tag set  has a r <dig> measure of pairwise ld with a tag snp greater than a given threshold  <cit> . some studies suggest that the human genome is composed of haplotype blocks with high ld and relatively limited haplotype diversity, separated by short regions of low ld  <cit> . the concept of block has immediately received a great deal of attention in the context of tag snp selection because a block may contain a large number of snps, but a few snps are enough to uniquely identify the haplotypes in a block. a straightforward block-based strategy consists of two separate steps:  identify haplotype blocks and  select the tag snps  <cit> . in one of the most popular tag snp selection approaches introduced by zhang et al.  <cit> , both the selection of block boundaries and the choice of tag snps are optimized jointly to capture the majority of haplotype diversity within blocks. the idea is implemented in the hapblock software  <cit> . here, rather than a "hard" definition of blocks, we describe ld by "soft" parameter values in an appropriate probabilistic model. our choice of a block-free perspective is motivated by numerous observations on block partitioning documented in the literature. the most important reason is that although some regions of the human genome seems to conform quite well to a description in terms of blocks, other regions do not  <cit> . it also appears that the definition of a block is not straightforward and very different partitioning are obtained depending on the adopted definition, leading to the selection of very different number of tag snps  <cit> . in addition, block partitioning has been reported to be affected by many factors such as the snp density, the number of observed sequences and the choice of a particular set of markers  <cit> . a good probabilistic model of the haplotype sequences can better capture ld patterns than haplotype blocks or simple measures of pairwise ld do. we also find two other benefits in adopting a model-based point of view. first, it allows us to tackle the tag snp selection problem as a data compression problem using the widely accepted shannon's measure of information content. second, a probabilistic model provides the machinery to predict tagged snps from tag ones. effectiveness of tag sets can then be evaluated through their information content and their ability to predict other snps, both measurements requiring a good probabilistic model. this direct measure of performance makes it possible to compare various methods, including model-based and other tagging methods. similar ideas can be found in some previous studies. in particular, the formulation of the haplotype tag snp selection as a data compression problem and the idea of evaluating tag sets through their prediction performances are advocated in halldórsson et al.  <cit>  and shannon entropy based criteria to measure ld and to select tag snps are described in  <cit>  and  <cit> , respectively. however, overall, little attention has been paid to selection and use of good probabilistic models for tag snp selection.

the coalescent model  <cit>  and its generalizations are the most appealing approaches to relating genetic variation to recombination rate and other genetic and demographic factors. however, the inference based on coalescence is computationally challenging. rather, we consider approximations that are computationally tractable. we start with a simple markov model that has fast algorithms for exact solutions and can serve as a baseline reference. next we consider several hidden markov models . in particular, we study the model introduced by li and stephens in the context of recombination rate estimation  <cit> . we show how entropy under different models of haplotype sequences can be maximized in practice. as another reference, we propose a greedy method that maximizes entropy without the linear structure embedded in markov models and hmms.

model-based snp tagging hinges on the issue of model comparison. criterion such as aic  <cit>  and bic  <cit>  have been proposed in the literature for model selection. in this article, to deal with various complicated models, we adopt the principle of minimum description length , which unifies the likelihood theory and shannon's theory  <cit> . according to the mdl principle, the best model gives the shortest description code-length of the data. analytical forms of description code-lengths are only available for relatively simple models  <cit> . the two-stage coding scheme has been adopted to delineate haplotype blocks using mdl after formulating the task as the choice of a model whose parameter space size increases smoothly with the number of blocks  <cit> . here, we compare a relatively small number of models with very different parameter spaces and thus we prefer to evaluate the description code-length of the models by using a cross-validation version of the predictive coding scheme for bit-counting that automatically penalizes over-fitting.

we compare the performances of the models and of the resulting tag sets on haplotype data from the international hapmap and encode projects. the performances of the tag sets selected by our model-based method are also compared with tag sets selected by the hapblock software  <cit> , with tag sets selected by the method proposed by carlson et al.  <cit> , with snps chosen at random, and with evenly spaced snps. finally, we assess the loss of predictive power of the tag sets due to the typing of genotypes instead of haplotypes.

RESULTS
model-based tag snp selection
let x = {xj, j =  <dig>   <dig>  ... , n} denote the random variable that corresponds to a haplotype sequence. when selecting a subset of tag snps indexed by j ⊂ { <dig>   <dig> ..., n}, we want to minimize the loss of information when we experimentally assess the subset j instead of the whole set of snps. in other words, we would like to find the optimal compression xj of x. shannon has shown that the information content, or randomness, of x is well measured by its entropy defined as h = -e
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaatuudjxwak1uy0hmmaehbfv3yslgzg0uy0hgiud3bagabaiab=ri8fbaa@388c@x log ℙ  <cit> . for any given subset j, the information content of x can be decomposed into two parts using the chain rule of entropy: h = h + h, where the entropy h is the information carried by the subset j and the conditional entropy h = -e
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaatuudjxwak1uy0hmmaehbfv3yslgzg0uy0hgiud3bagabaiab=ri8fbaa@388c@x log ℙ is the information loss due to the residual randomness of the snps j˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgkbgsgaacaaaa@2dd8@ that do not belong to j. in this framework, we seek a set of tag snps j that maximizes h subject to the constraint on the number of markers in j. the same subset j minimizes h and maximizes the mutual information between xj and x.

the method of entropy maximization requires the specification of a probabilistic model for x. better models provide more accurate quantification of the information content contained in sets of snps and thus are also expected to allow the selection of better tag snps. in the methods section we describe the models along with tractable tag snp selection procedures for each of them.

model comparison using description code-lengths
each model provides a coding system that can be used to encode haplotype sequences. when averaged over many new sequences, the length of this code gives an assessment of the quality of the model: the shorter the code-length, the better the model . code-lengths are closely related to the information content of the data . both quantities are usually expressed in bits and any achievable code-length is an upper bound to the real information content of the data. code-length equals to the information content only in the idealistic case where the "true" model is used for encoding. in table  <dig>  we show the cross-validated estimates of the code-lengths computed as the negative cross-log-likelihoods , for the ten encode data sets and the chromosome  <dig> data set . code-lengths are expressed in bits and thus are directly comparable to the number of snps in each data set. for instance, the li and stephens model that accounts for variable recombination rates gives a 52-bit long description per haplotype of the  <dig> snps in the enr <dig> region from the ceu population: the same description length as that of the outcome of  <dig> fair coin tosses.

the simple unconstrained two-state hmm shortens the code-lengths by about 30% compared to the simple markov model. the model introduced by daly et al.  is a better choice. it is better than the "greedy" model with context-size  <dig>  on chromosome-scale data sets but worst on most encode regions suggesting that the relative performances of the two models depend on snp density and/or minor alleles frequencies of the snps. the "greedy" model with context-size  <dig>  systematically outperforms these two models.

in all cases, li and stephens models with homogeneous recombination rate  and with heterogeneous recombination rate  stand out by their short description lengths. the shortest description length is always achieved by ls-het that shortens the code-lengths by about 40% compared to the gr- <dig> model. the amplitudes of the differences between code-lengths associated to ls-het and ls-hom vary greatly depending on the genomic region. it is less than 2% in the enm <dig> and enrl <dig> regions and more than 8% in the enrl3l and enr <dig> regions, being around 5% for the chromosome-scale data set . these differences may reflect variations in the pattern of recombination. in addition to demonstrating a strong superiority of the li and stephens hmms, code-lengths also reveals some interesting features of the data. we note in particular that the information content, or randomness, as assessed with the best model  is systematically higher for yri haplotypes than for ceu haplotypes. for instance, the difference is 44% on the chromosome  <dig> data set. we also observe that the information content measured in bits is always much smaller than the number of snps: a prerequisite for the existence of small tag sets capturing most of the information.

informativeness versus number of tag snps
next we check how informativeness increases as we select more tag snps. we emphasize that the calculation of informativeness does not necessarily depend on the tagging method and should in fact be measured according to the best available model  when comparing tag sets. figure  <dig> shows the information content versus number of tag snps in the enr <dig> region from the ceu population. we do not present the results for other regions as they look qualitatively similar.

in figure  <dig> , the information content is computed by the same model as used to select the tag snps. as expected the information content increases as more snps are selected. let us look at the two extreme cases. in the li and stephens models, the information curve increases at a much slower pace after  <dig> snps are selected. on the other end, in the simple markov model, the information content keeps growing rapidly as more snps are selected. with the results of model comparison in mind, we know that li and stephens models better describe the true information pattern. this supports the hypothesis of high redundancy in snp information. those extra bits seen in the simple markov model reflect its relatively low efficiency in encoding haplotypes.

in figure  <dig> , the information is computed with the ls-het model. as expected, tag sets selected with ls-het are the most informative. the informativeness of a tag set can be thought as a measure of its ability to describe snp data. if we compare models according to this measure, the results are consistent with those obtained using cross-validated code-lengths although differences between tag sets are smaller than differences between models. averaging information content over all encode regions from both ceu and yri, we find that  <dig> snps selected with ls-het capture the same amount of information as  <dig> snps selected with ls-hom or  <dig> snps selected with gr- <dig>  next come gr- <dig>  and hmm-4d .

we also study tag sets made of evenly spaced snps and randomly picked snps ). they appear to be less informative. the poor performance of these tag snps is particularly evident for small tag sets. the information content of the 20-snps tag set selected by the heterogeneous li and stephens model is about  <dig> bits whereas it is only about  <dig> bits for random or evenly spaced snps. when a model is used to select tag snps, the information increases at the rate of about one bit per snp at the very beginning because any model starts by choosing approximately independent snps with high minor allele frequencies. the rates slow down as more snps are selected: the snps that do not belong to the tag set tend to have strong dependence with tag snps or lower minor alleles frequencies.

prediction of tagged snps
a good tag snp set should allow us to predict the allelic status of the tagged snps with high accuracy. explicitly or implicitly, a prediction procedure is usually associated with a probabilistic model. here, we take the most probable allele, given a model and the haplotype known at the tag snps, as our prediction. these probabilities are easy to compute in the hmm framework. predictions based on greedy models rely on the observed frequencies in  <dig> simulations of the tagged snps given the haplotype at the tag snps. figure  <dig> shows the fraction of correctly predicted snps in the enr <dig> region from the ceu population. in figure  <dig> , predictions are based on the same model as used to select the tag snps. the relative standings of these curves are similar to those in figure  <dig>  although their curvatures are different. once again, the li and stephens models stand out. the correct prediction rates are as high as  <dig> % and  <dig> % with  <dig> tag snps when ls-hom and ls-het are used for both snp tag and snp prediction, respectively. this rate is  <dig> % for the gr- <dig> model and next it drops to  <dig> % and  <dig> % for the hmm-4d and gr- <dig> models.

the comparisons between tag sets on the other data sets show similar patterns for the relative standings of the rate of correct prediction. however, it is worth mentioning that rates correct prediction achieved on ceu data sets are higher than those obtained on yri data sets with the same number of tag snps. for instance, on average over all encode regions,  <dig> ls-het tag snps allow a prediction accuracy of  <dig> % on ceu encode regions but only  <dig> % on yri encode regions where about  <dig> snps are needed to achieved an average of  <dig> % of correct predictions. as expected, the fraction of snps that need to be retained in the tag set to obtain a desired level of correct prediction depend also strongly on the density of snps. on the chromosome  <dig> data set, where the average spacing between snps is about  <dig> times higher than in encode regions, about  <dig> snps are needed to obtain a  <dig> % correct prediction rate on the ceu data set .

comparison with other methods
we compare our best tag snp selection method based on the ls-het model with two methods previously described in the literature: the block-based dynamic algorithm implemented in the hapblock software  <cit>  and the carlson et al. method  <cit> . figure  <dig> presents results obtained on the different regions with different sets of user-defined parameters.

in addition to the better predictive power, the information content as measured by ls-het was always higher for our tag sets than for the tag sets selected using either the hapblock or the carlson et al. method .

genotyping versus haplotyping
the methods described above assume known haplotype information. however, most current typing platforms directly measure genotypes but not haplotypes. this means that the genotype at each typed snp is experimentally known but not the phasing between alleles at different loci. optimizing the tag set in the genotyping context may have to take into account the possibility that some snps are more informative than others for inferring haplotypes at the tag snp loci. such an optimization is not in the scope of this paper and would be computationally challenging if we do not want to sacrifice the necessary sophistication of a good haplotype model. instead, we propose to assess the loss of predictive power due to phase uncertainty at the tag snp loci by comparing the ability of our tag set to predict tagged snp genotypes from either haplotypes or genotypes at the tag snp loci.

the task of genotype modelling does not fundamentally differ from haplotype modelling as a genotype is simply a pair of haplotype whose phase is unknown . therefore, the best genotype model is just a modified version of the best haplotype model. naturally we use the ls-het model in this context: the ls-het model generalizes rather easily to genotype sequences ; prediction is performed after computing the posterior probability of each genotype at each snp with the forward-backward algorithm for hmms.

discussion
it has been proposed to use shannon entropy to assess ld  <cit>  and to select tag snps  <cit>  without relying on explicit models of haplotype sequences. the model-free approach relies on direct estimation of the entropy from the empirical haplotype frequencies and is not scalable to large set of snps. when snp sets are large enough any haplotype is observed at most only once which causes the empirical estimate of shannon entropy to plateau to log q, with q being the number of sequences sampled. in comparison, explicit modelling of the haplotypes makes it possible to estimate shannon entropy whatever the size of the snp set. in this context, appropriate models can bypass the concept of blocks and still account for simultaneous correlations between multiple markers.

the li and stephens models are becoming widely used in contexts including recombination rate estimation  <cit>  and haplotype phase reconstruction  <cit> . this study is the first that reports their use in the context of tag snp selection and it confirms their strengths. they outperform the other models in several aspects: description code-length of data, informativeness of tag snps, and prediction of tagged snps. furthermore, we show that the heterogeneous version of the model, accounting for fine scale variations in the recombination rate, is actually better than the homogeneous version. this was not a priori obvious because the heterogeneous model has many more parameters than the homogeneous model and their inference could be unreliable. it should be noted that these parameters are closely related to the recombination rates  <cit>  which are known to be hard to estimate  <cit> . however, we do not see a large difference between the heterogeneous and the homogeneous li and stephens model for the practical purpose of tag snp selection. this may suggest that inference of the fine scale variations in the recombination rate, and in particular of the location and the intensity of the recombination hot-spots  <cit> , is not very important in the context of tag snp selection. the relatively good performances of our models of context-size  <dig> or  <dig> might encourage more work on related models such as graphical models for snp data  <cit> .

in this study we consider the problem of haplotype tag snp selection assuming that haplotypes are available. in reality, the data are usually just genotype. genotype trios give much more information on the haplotypes but they necessitate more genotyping and are impractical in most large case-control studies because of difficulties to find suitable trios. as in most of the works on haplotype tag snp selection, the problem can be circumvented by using haplotypes inferred from genotype data. examples of methods for inferring haplotypes from genotype data, or phase reconstruction, include those proposed by excoffier and slatkin  <cit> , niu et al.  <cit>  and stephens and scheet  <cit> . however, these methods are not error-free and this may cause a problem at two levels. first, the choice of the tag snps relies on a set of haplotypes and could be sensitive to phase errors in these haplotypes. we do not believe this to be a very important concern as  a high density of snp will typically be assessed and thus will permit accurate haplotype reconstruction and  trio genotyping is possible if haplotype reconstruction actually appear to be a problem. second, the set of tag snps may not be optimal for phase reconstruction. our results rule out this concern by showing that the impact of phase uncertainty on the ability to predict tagged snps from a good tag set is marginal. even a very hypothetical tag set that could simultaneously remove any phase uncertainty and preserve the maximal amount of information on the tagged snps would not perform more than a few percent better than our tag sets. in addition, one may argue that if the tag set were genotyped in many individuals, these genotypes may be used to further reduce the phase uncertainty at the tag snp loci.

our results show that tagged snps can be predicted with a very high accuracy using a good tag set and a good prediction method. for instance,  <dig> snps mapping a  <dig> kbp long encode regions from ceu population can be predicted with an average error rate of 1% using  <dig> tag snps. rough computations suggest that if the  <dig> gbp of whole genome was mapped at the same snp density ,  <dig>  tag snps would be enough to predict the other snps with a 1% average error rate. due to the more complex haplotype structure of african populations  <cit> , achieving the same rate for yri population could require about twice as many tag snps. it would be interesting to investigate whether or not one should use these predictions in association studies. the answer may not be evident. on one hand, working with a smaller set of snps reduces the dimension of the analysis and thus it could be advantageous to restrict the analysis to the tag snps. on the other hand, tag snp selection relies on knowledge about the haplotype structure that is latter lost if predictions are not used. moreover, when the association study rely on single marker analyses where each snp is tested separately, then it is probably worth to first predict tagged snp to increase the probability of finding a single marker heavily linked to the causal snp. throughout this study we select tag sets by maximizing entropy given a model. the use of this quantity is motivated by strong arguments from information theory. if we compare how entropy and global prediction accuracy measure the power of tag sets, results suggest that entropy is a more sensitive quantity than prediction accuracy. for instance, the comparison between the average rate of correct prediction achieved with a tag set selected using the best model  and relatively simple models  show quite similar performances for each tag sets whereas the superiority of the li and stephens model is clearly established both in terms of code-lengths and predictive ability. we also see that the pace at which entropy increases with the number of tag snps does not slow down as quickly as the rate of correct prediction. we note that both measures are adopted in the popular machine learning tool of classification and regression tree   <cit> . namely, entropy is used to generate more partition nodes while prediction  accuracy is used to prune decision trees. an important issue in association studies is the choice of the number of tag snps. it may be a good idea to adopt the strategy used in the cart methodology for tag snp selection. that is, after having pre-selected a tag set  on the basis of entropy, we could determine the final number of tag snps by shrinking the original set on the basis of the prediction accuracy. in this context, it is worth mentioning that tag snps selected from a given collection may better predict snps from this collection than other snps  <cit> . therefore, it could be sound in future works to set aside a fraction of the snps during the tag snp selection process that could latter be used in order to choose the final number of tag snps on the basis of the prediction accuracy.

finally, we were able to provide evidence of the advantage of our best tag snp selection method over both the methods implemented in the hapblock software and the method proposed by carlson et al.  <cit> . we did that by comparing the performances of tag sets on their abilities to predict the tagged snps . these comparisons are meaningful because they rely on a good probabilistic model. in the future, this scheme that allows direct comparison of the intrinsic performances of tagging sets should be useful to further clarify the issues associated to the choice of tag snps.

CONCLUSIONS
our study provides strong evidence that the tag sets selected by our best method, based on li and stephens model, outperform those chosen by several existing methods. the results also suggest that information content evaluated with a good model is more sensitive for assessing the quality of a tagging set than the correct prediction rate of tagged snps. besides, we show that haplotype phase uncertainty has an almost negligible impact on the ability of good tag sets to predict tagged snps. this justifies the selection of tag snps on the basis of haplotype informativeness, although genotyping studies do not directly assess haplotypes.

availability and requirements
a program that implements tag snp selection, entropy computation, and tagged snp prediction based on li and stephens hmms is freely available under the terms of the gnu public licence at  and is also attached to this publication . the programs implementing the other tag snp selection methods described in this paper will be made available upon request to the authors. requirements of the software: source code is in c++ language and was compiled on i <dig> linux platforms; numerical maximization relies on routines of the gnu scientific library  that needs to be installed to compile the program.

