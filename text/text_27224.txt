BACKGROUND
analysis of variance  is a commonly used statistical technique in experimental biology. often one or more of the independent/predictor variables such as dose, time, or age, can be treated as a continuous numeric variable rather than a categorical variable during analysis, even if experimentally it is treated as a category. for example, animals may be randomly assigned to one of several different groups, each of which receives a different dose of a drug . this would commonly be analysed with a one-way anova, with one control group and several experimental groups. dose would be treated as a categorical variable when testing whether the drug had any effect on the response variable, such as performance on a behavioural test. another example is killing animals at different ages in order to assess how age affects anatomical or physiological variables of interest. animals could be killed at perhaps three different ages , and again this would be traditionally analysed with a one-way anova. alternatively, dose or age could be treated as a continuous variable and these analyses would proceed as a simple regression analysis, with both the response and predictor variables being numeric. pharmacologists and toxicologists routinely treat dose as a numeric variable and fit nonlinear dose-response curves to the data, but apart from these specific disciplines, this method of analysis is not common in experimental biology . there are a number of advantages of such an approach when used appropriately, such as greater statistical power due to more precise estimates, a simpler and more informative interpretation of the results, a more parsimonious explanation of the data with fewer parameters, and transformations of the predictor variable are possible. to simplify the discussion, the first type of analysis will be referred to as the anova analysis and the second as the regression analysis , as most readers will be familiar with these terms. however, the only difference between them is whether the predictor variable is treated as a categorical factor or a continuous numeric variable, and both are specific cases of a linear model  <cit> .

this paper will discuss the advantages of using a regression analysis instead of the more common anova analysis, why these advantages occur, and when this analysis is, and is not, appropriate. in addition, an example is provided illustrating how the incorrect conclusion can be reached using the standard anova analysis.

RESULTS
increased power when treating dose as a continuous variable
twenty rats were randomly assigned to four groups and given either  <dig>   <dig>   <dig>  or  <dig> mg/l of fluoxetine in their drinking water. after four weeks, performance on the forced swim test  was assessed, and the amount of time spent immobile was the main response variable of interest  and once as a continuous numeric variable .

it is evident that there is a dose-dependent relationship between fluoxetine and immobility time , with decreased immobility associated with higher doses of fluoxetine. a standard method of analysing this data is with a one-way anova, with one control and three treatment groups. this analysis leads to the conclusion that there is no significant effect of fluoxetine . however, when dose is treated as a continuous variable, the effect of fluoxetine becomes significant . to understand why this occurred, it is necessary to understand how the two analyses are implemented. the difference between them is that the anova analysis estimates four parameters from the data , while the regression analysis only estimates two parameters . since one degree of freedom  is lost every time a parameter is estimated, the anova analysis has lost two more dfs compared to the regression analysis. the anova tables of both analyses are presented in table  <dig>  the sum of squares  for dose is slightly greater with the anova than the regression method , indicating that the anova analysis accounted for slightly more variation in immobility times. this is also reflected in the residual ss, with the regression analysis having a slightly larger ssresidual, indicating a greater amount of unexplained variation. it would appear therefore that the anova analysis is preferable because it accounts for slightly more of the variation. however, the anova method is evaluated on  <dig> and  <dig> df, while the regression method is evaluated on  <dig> and  <dig> df, and this makes all the difference. the mean square  for dose is calculated as msdose = ssdose/dfdose and having  <dig> df in the anova analysis reduces ssdose by a third, whereas in the regression analysis msdose = ssdose . the opposite occurs with the residuals: because ssresidual is divided by a bigger number in the regression analysis  than the anova analysis , msresidual will be smaller with the regression analysis. this is important because the f-value is calculated as f = msdose/msresidual, and therefore the higher msdose and the lower msresidual, the higher the f-value. as can be seen, the f-value is  <dig> / <dig>  =  <dig>  times bigger using the regression analysis. the critical value of f  is the number that the calculated f-value has to exceed in order to be significant at the  <dig>  level, and it is different for each method because it is based on the degrees of freedom. f-crit for the regression analysis is  <dig> / <dig>  =  <dig>  times bigger, but is still less than the change in f-value, and explains why the regression method is more powerful. this will be true in general, and more formally, the linear regression model

the difference in the last digit of total ss between the anova and regression analysis is due to rounding error.

  yi = α + βxi + εi 

where α is the y-intercept and β the slope of the regression line, is a special case of the ordinary 'factor' model

  yij = μ + θi + εij 

where μ is the grand mean and θi is the treatment effect of the i th group . in each case, the residuals  are normally distributed with a mean of zero. we would always choose  in preference to  if the data allow this , as this allows the parameters to be estimated more accurately .

the fst is a standard behavioural screen for antidepressant drugs  <cit> , and the effects of fluoxetine on this test are well documented  <cit> . kulkarni and dhir estimated  <dig> mg/kg of fluoxetine as the dose that produces a response in 50% of rodents , when given intraperitoneally  <cit> . this corresponds to the  <dig> mg/l group in the present experiment. the lack of statistical significance therefore reflects the reduced power of the anova analysis compared to the regression analysis; using the anova analysis, we would falsely retain the null hypothesis that fluoxetine has no effect on this behavioural test .

readers might wonder if the regression method will increase the chance of false-positives , since it can have greater power and therefore provides lower p-values than the anova method. the answer is no, because the type i error rate is set by the researcher before looking at the data . the preference for the regression method  is in line with the 'classical' neyman-pearson method of analysis commonly used in experimental biology: set the probability of falsely rejecting the null hypothesis when it is true , and then use the test with the highest probability of rejecting the null hypothesis when the alternative hypothesis is true   <cit> . the aim is to minimise type ii errors , subject to the constraint on type i errors  <cit> . the problem with the anova analysis is that it has reduced power to detect linear relationships , which can lead to increased type ii errors.

a simpler and more informative interpretation
the regression analysis also lends itself to a simpler interpretation. the effect of fluoxetine on immobility time on the fst can be stated thus: for every  <dig> mg/l increase in fluoxetine, immobility time decreases by  <dig> seconds . this is true for doses between zero and  <dig> mg/l, and extrapolating to higher doses should be done with caution. this states the relationship between the two variables, and the confidence interval provides an estimate of uncertainty. contrast this with interpreting the anova analysis; one can only describe qualitatively that there was a decrease in immobility time with higher doses of fluoxetine , but this is much less informative than quantifying this relationship as in the regression analysis. the amount of variation in the response variable accounted for by the predictor variable could also be mentioned , but this is still not as informative. post hoc tests are routinely applied in such situations, whether the overall anova was significant or not. these are problematic  because they reflect lack of power rather than a lack of effect, particularly if corrections for multiple comparisons are used. for example, the only significant difference was between the control and  <dig> mg/l group when uncorrected pairwise comparisons are used . this is no longer significant when correcting for multiple tests.  comparing the control group to all the fluoxetine groups gives p =  <dig>  for this comparison  or blindly doing all pairwise comparisons gives p =  <dig>  . each pairwise comparison uses half the total sample size  and correcting for multiple comparisons raises the bar for significance, making the power of such tests greatly reduced. all of this can be avoided with the regression analysis; not only is the interpretation simpler, but more informative.

another drawback of the anova analysis is that the results are invariant to the ordering of the groups. for example, the above anova result would be identical if the data from the  <dig> and  <dig> mg/l groups were swapped, such that the  <dig> mg/l group now has the lowest immobility time. there is now no consistent decrease in immobility time with increasing dose  and therefore less evidence for an effect of fluoxetine, yet the p-value remains the same . this is because '0', '80', '160' and '240' are just labels in the anova analysis, which could just as easily have been the labels a to d. the regression analysis has the advantage of respecting the order of the data, where zero means no drug, and  <dig> mg/l is twice  <dig> mg/l.

greater parsimony
other things being equal, it is generally accepted that a simpler explanation is preferable to a more complex one  <cit> . the regression analysis describes the data with just two parameters: the slope and intercept of the regression line. in contrast, the anova analysis requires four parameters: the mean of the control group , and the difference between the control and each of the fluoxetine groups . the anova analysis therefore uses twice as many parameters to describe the data, and therefore other things being equal, the regression analysis is preferred because it is simpler.

transformations of the predictor are possible
since the predictor in the regression analysis is treated as a number and not a category, it is possible to transform it. for example, a toxicology study may use doses that span several orders of magnitude , but it is not predicted that the response will have such a wide range , and therefore it is unlikely that the relationship between the two will be linear. however, taking the log <dig> of the above doses gives values of - <dig>  - <dig>   <dig>   <dig>  and  <dig>  which are more likely to be related linearly to the response variable. treating the predictor as a continuous variable therefore provides added flexibility by allowing transformations that the anova analysis does not.

when not to use the regression analysis
the first requirement is that the predictor variable must in fact be continuous, and a true categorical variable such as different types of drug, arbitrarily labelled from  <dig> to  <dig>  cannot be treated as a continuous variable. second, the regression analysis requires the relationship between the response and predictor variables to be linear. nonlinearity could be handled by transforming one or more of the variables , but it may be preferable to use the anova analysis in this case if it makes the interpretation simpler. alternatively, the relationship between the response and predictor might be 'u'- or inverted 'u'-shaped, in which case the anova analysis would be preferable . the greater the extent of nonlinearity, the less power  the regression analysis will have compared to the anova analysis, and the optimal fit for the regression line would occur when it passes through the mean of each group.

the relationship between the response and predictor variables can be established by plotting the data as in figure 1b. however, plotting the fitted values against the residuals will also provide information on the lack of fit of the regression model. crawley  suggests that the lack of fit of the regression model can be tested by using both the continuous and categorical variables in the same analysis; entering the continuous variable first, and then the categorical factor. if p <  <dig>  for the categorical factor, then the anova model is preferred to the regression model. note that this is a sequential , where the continuous variable is entered into the model first, and the categorical variable accounts for any additional variation not accounted for by the continuous variable. for some statistical software, the default ss will have to be changed to 'type i' before the analysis is carried out. it is also possible to compare the two models directly with an f-ratio using the following equation

  f=/ssanova/dfanova 

where ssreg and ssanova are the residual sums of squares from the regression and anova analyses, and dfreg and dfanova are the residual degrees of freedom. this follows an f-distribution with dfreg - dfanova degrees of freedom in the numerator and dfanova degrees of freedom in the denominator. with the present data, the results are f <dig>  =  <dig> , p =  <dig> , indicating that the anova model does not provide a significantly better fit than the regression model.

in addition, the two models can be compared using akaike's information criterion   <cit> , and a discussion of this approach can be found in motulsky and christopoulos  <cit>  . briefly, aic is a measure of how well a model fits the data. the more parameters a model has, the better the fit; however, aic penalises superfluous parameters and thus represents a trade-off between goodness-of-fit and the number of parameters. the model with the lowest aic value is preferred, and aic for the anova method was  <dig> , while for the regression method it was  <dig> , indicating that the regression method is preferred. a related approach is the bayesian information criterion , which tends to penalise complex models more heavily than the aic, and therefore gives greater preference to models with fewer parameters  <cit> . the bic value is interpreted in a similar manner to the aic , and bic for the anova method was  <dig> , while for the regression method it was  <dig> . therefore, using the f-ratio, aic, and bic, it is possible to compare both the anova and regression models directly to see which is preferable.

the final requirement is that the groups must be independent, with different animals  in each group . neither the one-way anova nor the regression analysis are appropriate if the same subjects give values at more than one level of the factor; for example, if a response was measured at more than one time point. in this case, a repeated-measures anova or a mixed-effects model  <cit>  should be used.

extensions and further applications
the example provided had only a single predictor variable, but the results and the general approach also apply to higher order designs where one or more variables could be treated as continuous rather than categorical, leading to analysis of covariance  or multiple regression-type analyses. for example, if the present data contained both males and females, separate regression lines could be fit for each sex, each with its own slope and intercept. it is important to note that if there is no significant interaction between the continuous variable  and the groups , then the interaction term should be removed from the model, which constrains the regression lines to have equal slopes. if this is not done, the interpretation of differences between sexes becomes difficult, and can lead to erroneous conclusions  <cit> . in addition, the response variable may be counts, proportions, percentages, or other types of data that would normally be analysed with a generalised linear model, and the advantages of treating the predictor variable as continuous rather than categorical are similar for these analyses  <cit> . a related issue is that of 'data carving'  <cit> , where a continuous numeric  variable such as age is binned into a few categories; for example, age may be dichotomised into young and old groups based on a median-split, or perhaps a middle group would be included as well, and the data would then be analysed with an anova. there is little to be gained from such an approach and it is not recommended  <cit> .

in general, the greater the number of groups, the greater the usefulness of using the regression analysis; for example, if there were eight groups ssdose would be divided by  <dig> -  <dig> =  <dig> df for the anova analysis, but msdose would still equal ssdose  for the regression analysis. at the other extreme, the two analyses would always produce identical results if there are only two groups .

the analysis of datasets such as the one presented in this paper are not limited to anova and regression analyses, computationally intensive methods , non-parametric tests  and tests for trend  are also available, and each have their associated advantages, disadvantages, and caveats. the advantages of the regression analysis are well known to statisticians, but relatively unknown to experimental laboratory-based biologists. this is likely due to regression and anova being treated as separate topics in undergraduate statistics courses aimed at natural scientists . in addition, many modern point-and-click statistics packages maintain this distinction, with the tests located under different sub-menus. however, regression and anova are not two fundamentally different analyses; they both fit  linear models to the data, and it is up to the analyst to decide which model is preferable in each case.

CONCLUSIONS
as demonstrated above, treating an experimental variable as continuous rather than categorical during analysis has a number of advantages. first, it will generally have greater statistical power. second, because fewer parameters are used to describe the data, it is more parsimonious. third, it often provides a simpler interpretation , and this is usually more informative as well. finally, there is the added flexibility of allowing transformations of the predictor variable. because of these advantages, treating independent variables as continuous should be the method of choice in the first instance, with anova being used if regression analysis is not appropriate .

failing to use the optimal analysis runs the risk of missing significant effects; in the example provided, the anova analysis did not reject the null hypothesis while the regression analysis did. it is not known how many published studies failed to find significant effects, or how many studies have not been published due to lack of significant results  because an anova analysis was used when a regression analysis would have been more powerful. but given the ubiquity of anova in experimental biology, it is likely a non-trivial number. it is hoped that, when feasible, readers will employ this approach in their own research to improve the power of their analyses and arrive at a better understanding of their data.

