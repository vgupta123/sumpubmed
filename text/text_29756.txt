BACKGROUND
the understanding of the molecular basis of complex diseases such as cancer has been greatly enhanced in present time by genomic sequencing and other omics-approaches. genomic biomarkers have been applied to disease screens , cancer subtype classification , and to predict drug response . as large numbers of biomarkers can be measured simultaneously at a relative small cost, the bottleneck for such omics studies has become the expansion of the number of samples collected. unfortunately, for many current studies, the number of subjects is much smaller than the number of genetic markers measured, which has ranged from thousands of genes to millions of genetic variants. thus how to identify the relevant variables or biomarkers precisely in a high-dimensional data set has become a challenge for the further advancement of the development of precision medicine and personalized treatment.

traditionally, variables were identified by univariate analysis, followed by multiple-testing adjustment such as bonferroni’s p value correction or false discovery rate  procedure  <cit> . for example, in genome-wide association studies , single nucleotide polymorphisms  are screened site-by-site to test the association between diseases and complex traits. however, this approach ignores the underlying correlation structure between genomic markers, leading to the absence of identification of the joint impacts of biomarkers on phenotypes. to address the joint impacts, popular variable selection methods such as lasso  <cit> , adaptive lasso  <cit> , and scad  <cit>  have been established over the past decades. such methods, however, are beset with high computational costs when p is as large as an exponential of the sample size n. to overcome these high computational costs in analyzing such ultra-high dimensional data, an effective solution is to conduct pre-screening of variables. for example, fan and lv proposed the sure independence screening  approach in which prescreening is based on marginal correlations  <cit> . tibshirani et al. proposed a method to prescreening-based on a lasso penalization under the cox model  <cit> .

another way to tack this “large p small n” paradigm is to collect multiple datasets . one popular approach is to pool datasets together and then perform further analysis as if they originated from a single study. this approach demands the data to be fully comparable and it’s often not feasible to integrate datasets from different sources of genomic information. other data integration methods have been developed by incorporating hierarchical and network-based models to integrate different omics data. shen et al. proposed an icluster approach to assign cancer subtype by integrating multiple levels of omics data with introducing a latent variable  <cit> . aure et al. identified in‑trans process associated genes in breast cancer by integrated analysis of copy number and expression data  <cit> . akavia et al. identified driving cancer mutations and the processes that they influence by integration of copy-number variation and gene expression  <cit> . in a recent nci-dream challenge, various integration methods, such as bayesian multitask multiple kernel learning , have been applied to identify biomarkers for drug response  <cit> .

such methods however are often associated with a few problems. first, most of them are very complex and sometimes difficult to apply without possession of specialized statistics knowledge. secondly, since these methods may be designed for specific cases, they are potentially inflexible and hard to modify in order to apply to another study. lastly, and most importantly, all of them require the access to the raw data, which often is unavailable.

the goal of this study is to develop a general procedure for variable selection with knowledge integration. the basic idea of our method is to guide the pre-screening procedure by taking prior knowledge into account, and then after prescreening, sophisticated variable selection techniques such as lasso could be applied.

the only input required for our method is a rank of genomic biomarkers obtained from external information, which is certainly a desirable feature for the users without accessibility to raw data. for example, in one possible application, summary statistics of psychiatric disorders could be found at the psychiatric genomics consortium  website  <cit>  and used to develop a ranking. this ranking could be then applied to pre-rank the snps in gwas studies related to psychiatric disorders. in other applications, an association between genes and other biological terms could be obtained through text mining of the literature  <cit> , and genes could be ranked based on this association. similarly, the genes reported to have interaction with a drug or compound  <cit>  can be placed on the top of the list  when predicting drug response. in the top of lists when predicting the drug response. more commonly, a candidate list could already exist before the high-through measurement procedure takes place and it is then reasonable to give these candidates a higher priority. in the most extremist case, only candidate biomarkers were measured  instead applying a genome-wide measurement. to distinguish our method from others, we call this “knowledge integration”.

a simulation study was conducted to examine the performance of our method. we also compared it to the other popular approaches. we then applied our method in a drug response analysis. our method outperformed a commonly used marginal correlation based screening procedure.

method
sure independence screening
suppose we have a genomic dataset , where y
i is the response and x
i =  is the vector of p covariates, for i =  <dig>   <dig>  …, n. in real applications, y could be measurements of some phenotypes or quantitative traits, such as weights, drug response, etc. x could be some high-dimensional omics-measurements, such as gene expression, cpg methylations, etc. in a typical genomic setting, p could be far larger than n. to deal with high dimensionality, effective variable selection techniques are required.

the sure independence screening  method introduced by fan and lv  <cit>  is a two stage approach. first, it selects significant predictors by sorting the corresponding marginal likelihood , thus fast reducing the ultra-high dimensionality to a relatively large scale d ). subsequent to sis, a more sophisticated lower dimensional model selection technique such as scad  <cit> , the dantzig selector  <cit> , lasso  <cit> , or adaptive lasso  <cit>  could be applied to perform the final variable selection and parameter estimation. apparently, sis could dramatically speed up variable selection when the p is extremely large. fan and lv proved sis enjoys the sure screening property and model selection consistency under certain conditions.

screening with prior knowledge integration
we noted that the idea of sis is based on marginal correlation to first select important variables. based on this idea, we proposed an novel approach, screening with prior knowledge integration , to select variables in the first stage. the basic procedure of ski is drawn in fig.  <dig>  the idea of the ski is to rank the variables not only based on marginal correlation but to also incorporate external information. the rationale here is that the variables supported by both marginal correlation and external information are more likely to be important features, and thus should be included in the second stage for parameter estimation with larger probability.fig.  <dig> a brief description of ski procedure. for each variable, two ranks are generated, one based on prior knowledge , the other based on marginal correlation . a predefined α,  is used to control the weight of prior knowledge. variables are then sorted by weighted geometric mean of two ranks. ski first reduces the variable number from p to d, and then a more sophisticated method such as scad is used to further refine the model to size d ’ and estimate the parameters. iski updates the marginal correlation based rank  by regressing residues over the rest p − d ’ variables. the procedure is repeated until the desired number of parameters obtained




besides the same settings in sis, now suppose we also have an external ranking of variables r
 <dig>  which is of length p, obtained from prior knowledge. we define a new rank for gene j as the weighted geometric mean of two ranks: rj=r0jα×r1j1−α for i =  <dig>   <dig>  …, p. r
0j is the rank of gene j obtained from prior knowledge, and r
0j is the rank of gene j obtained by sorting marginal correlation. here α is a parameter controlling the importance of prior knowledge. here, we restrict 0 < α <  <dig>  to limit the influence of prior knowledge so that it could not be stronger than the data at hand and we will estimate it by data . but in practice, α could be a value, in range from  <dig> to  <dig>  predetermined by users or estimated by data. if we set α =  <dig>  the genome-wide measure becomes the targeted-region measure.

the initial ranking represents the importance of each variable known ahead of the ongoing study. for example, if the goal of this study is to predict drug response based on gene expressions, other genetic measurements such as copy number variants  might be available. we could first rank each cnv by its marginal correlation with drug response obtained by univariate linear regression and then we map cnv ranks back onto the genes to get an initial rank of genes. more commonly, we could rank genes based on their importance scores obtained by expert domain knowledge or literature searching.

typically, we require that each variable has an initial rank. for those variables with no information, an average rank can be assigned. for instance, among  <dig> predictors,  <dig> of them are found associated with response from existing knowledge. we could assign ranks  to these  <dig> predictors based on their association strength and  <dig> for the rest. alternatively, if we don’t know the association strength, we could set the ranks of  <dig> predictors as the average of  <dig> to  <dig>  which is  <dig> 

estimation of α
as mentioned above, the selection of α could control the relative strength of influence imposed by prior knowledge, which is essential for the success of the proposed methods. unfortunately, there is no pleasant way for tuning this parameter. lasso or elastic-net  <cit> , uses cross-validation strategy to select α with lowest internal prediction errors. however, the problem we face here is a ultra-high dimensional problem, where the number of covariates p is already much larger than sample size n. cross validation will require us to further spit the sample into training and testing, which can make the ultra-high dimensionality issue worse. to alleviate these concerns, we develop the following alternative strategy.

we first generate a sequence of α = . for each α, we re-rank the variables as its weighted geometric mean rank. we then select the top d ranked variables as inputs for a ridge regression model  <cit> . after fitting a penalized ridge regression, we calculate the fraction of null deviance explained as. dev.ratio=1−loglikesat−loglikeloglikesat−loglikenull 


here loglike
sat refers the log-likelihood or the saturated model . and loglike
null refers to the intercept model. we compare the dev. ratio across different α’s, and select the α yields largest dev. ratio as the final α.

the rationale of this method is that if one set of variables is more biologically meaningful than the other, the better it could fit a ridge regression model. we do notice that the number of variables selected d will affect the performance of ski in terms of estimation of α. in the most extreme case, if only one variable is selected , then the estimated α will always be zero. but our experiences suggest the number of variables selected won’t affect the results significantly if this number is not too small. although some methods have been proposed to tune this parameter  <cit> , how to determine the number of variables is out of the scope for this study.

extension: iterative ski
fan and lv demonstrated that when too many predictors are involved, the basic sure screening methods might miss some important variables due to collinearity issues. in their paper they developed an iterative version of sis to use fully the joint information of the covariates rather than marginal information. briefly, in the first step, a subset of k
 <dig> variables is selected using an sis-based method. next, a n -vector of residuals are obtained from regressing the response y over k
 <dig> variables are treated as new responses and the same method is applied to the remaining p − k
 <dig> variables. the process is repeated until desired number  of variables is selected or  maximum iteration is reached.

we extend this idea to ski and developed an iterative version of ski . the similar procedure was used. in the first step, the rank of each variable is obtained as weighted geometric mean of knowledge-based rank and the sorting marginal correlation between responses and predictors. for the rest of the steps, the rank is weighted geometric mean of the knowledge-based rank and the sorting marginal correlation between residuals and predictors.

RESULTS
simulations
we adopted a similar simulation in ma  <dig>  <cit> . in total n
x =  <dig> samples  were simulated, with gene number p =  <dig>   <dig>   <dig> clusters were simulated independently, and  <dig> genes in each cluster were simulated from a multivariate normal distribution with μ =  <dig>  σ
2 =  <dig> and ar correlation structure ρ =  <dig>   <dig>   = ρ
|i − j|). this is to mimic a real gene expression studies with taking pathway structure into account. in each cluster, the coefficients β’s of first ten genes were simulated from a uniform distribution with minimum  <dig>  and maximum  <dig>  all other β’s were set to be zeros. this is consistent with the parsimonious assumption that only few genes and pathways were associated with phenotypes or diseases. continuous responses were generated from linear regression models with σ
x2 =  <dig> .

another n
z =  <dig> samples  with gene number p =  <dig>   <dig> were simulated to mimic an external gene expression study, where our prior knowledge was drawn from. gene expressions and responses were simulated from the same structure as described above. but the non-zero coefficients β were simulated to have  <dig>   <dig>  and 100% overlap with non-zero β in the internal settings. this is to mimic the situation that the prior knowledge completely disagrees, partially agrees and exactly agrees with our true experiment settings.

to better evaluate the performance of the proposed approach, we also consider other alternatives:select genes without external knowledge available. genes were based on marginal correlations between x and y
x. 

select genes based on the proposed methods, where the prior ranks of genes generated based on marginal correlation between z and y
z. 

select genes based on pooling two dataset together and conduct analysis as one dataset. genes were ranked based on marginal correlations. 




in table  <dig>  we summarize the results of variable selection by generating  <dig> datasets. as expected, under the same settings of ρ, σ
x <dig>  and σ
z <dig>  the estimated α was increased as the percentage of non-zero β that overlapped between internal and external datasets increased. the proposed methods selected consistently more true positive genes when prior knowledge partially or exactly agrees with internal settings . when the prior knowledge is completely noisy , the performance of the proposed methods is comparable with only using an internal dataset. although, the performance of pooling two datasets is better than the proposed methods when the prior knowledge is useful, the performance will drop dramatically when the prior knowledge is not useful. more importantly, as stated before, the focus of this study is to develop a strategy to integrate biological knowledge. obviously, the applied range of the proposed methods is much broader.table  <dig> simulation results compared the number of true positives among different methods

positivea
%b
σ
x2
c
σ
z2
d
α
e

atop  <dig>   <dig> and 10% variables were selected respectively under different settings


bthe percentage of non-zero β’s overlapped with each other in two datasets


c
σ
x <dig> : the variance added in internal dataset to generate response y
x



d
σ
z2: the variance added in external dataset to generate response y
z



e
α: the estimated value of α which control the weight of two ranks in geometric mean


fsis: variables were sorted by marginal correlation using only internal dataset


gski: variables were sorted by weighted geometric mean of two marginal correlation based ranks using two dataset


hpool: two dataset were pooled together and treated as a single dataset, and then variables were sorted by marginal correlation




we also investigated the performance of the extension of the proposed approach , by compared it with non-iterative version of the proposed approach , sis and isis methods. the last two methods were proposed by fan  <dig> to select important variables without considering prior knowledge. the extension methods were proposed to solve the issue of strong collinearity between genes. so we simulated different ρ  to investigate its performances under different correlation settings. since both isis and iski are very computation intensive, we fixed σ
x2 =  <dig> and σ
x2 =  <dig>  we also set the maximum iteration to three to reduce computing time. scad was used to fit the model in the second stage. all the other settings were kept the same as before. table  <dig> summarizes the number of true positives when the top 1% genes were selected. as expected, isis included more true variables than sis, and iski performs even better than isis when the external information are useful.table  <dig> simulation results compared the number of true positives among iterative and non-iterative approaches when top 1% variables were selected

%a
ρ
b
α
c

a%: the percentage of non-zero β’s overlapped with each other in two datasets


b
ρ: correlation coefficients between two neighbor variables in each cluster


c
α: the estimated value of α which control the weight of two ranks in geometric mean


dsis: variables were sorted by marginal correlation using only internal dataset


eisis: iterative version of sis


fski: variables were sorted by weighted geometric mean of two marginal correlation based ranks using two dataset


giski: iterative version of ski




real application: drug response analysis
we next applied the ski procedure to a drug response study and compared it to the results obtained with the sis procedure. selumetinib  is a drug used to treat various types of cancer such as non-small cell lung cancer . it is a potent, highly selective mek <dig> inhibitor. unfortunately, despite intensive studies, the genetic mechanism for selumetinib resistant remains controversial . we applied the ski procedure to identify the potential biomarkers of response to selumetinib. we downloaded the drug response data  from the cancer cell line encyclopedia  project  <cit>  together with its baseline omics measurement, which includes gene expression, mutation data, and copy numbers. in total there were  <dig> cell lines and  <dig>  genomic features measured. for a single feature, we assign a specific gene annotation on it. we then searched the drug2gene database  <cit>  to acquire prior knowledge of association between selumetinib and genes. drug2gene is an integrative knowledge base reporting relations between genes/proteins and drugs/compounds including bioactivity data where available. the data has been collected from  <dig> public databases and integrated to provide a 'one-stop shop’ for identifying tool compounds for genes or finding all known targets of a drug. in total,  <dig> genes were identified to have associations with selumetinib. we gave an initial rank to  <dig>  genomic features based on whether its annotated genes have a known association with selumetinib. for  <dig> features with annotated genes having association with selumetinib, we set their ranks as  <dig>  and for others, we set the ranks as  <dig> .

the ski and sis procedure were used for variable selection, respectively. the top  <dig> features were selected and scad was used to fit the final model. in other studies, external information  are used to judge whether the variables identified are accurate. since here we already used this knowledge in ski, it is unfair to judge the results by this criteria. so we used leave-out-out cross validation  to compare the prediction squared error of these two methods.

the average of α estimated in ski was  <dig> , indicating that the prior known associated genes are very informative in variable selection. in fig.  <dig>  we showed the loocv prediction square error of two methods. in general ski methods outperforms sis in terms of small prediction error. the median  prediction square errors are  <dig>   and  <dig>   for sis and ski, respectively. by integrating prior known information, ski selects the variables more accurately.fig.  <dig> boxplot of squared error for selumtinib response prediction using two methods. whiskers indicate min/max, upper box limit 75% percentile, low box limit 25% percentile and line the median




we also investigated the features identified by these two methods. those features identified by ski procedure, with known association with selumtinib ahead, are summarized in table  <dig>  the mean absolute value of marginal correlation for all variables is  <dig> , while this number increases to  <dig>  for variables with previous known association. despite the fact that genes with known association with selumtinib were highly enriched in the top of the ranked list generated by marginal correlations, only one variable, mutation of braf, could be recruited by using common marginal correlation based screening methods when top  <dig> variables were selected. but by applying the ski procedure, we rescued  <dig> variables whose marginal correlations are not high enough, but supported by external knowledge in our final model.table  <dig>  <dig> variables selected by ski procedure when top  <dig> variables were selected, whose association with selumetinib could be found in database

r
sis
a
r
ski
b

a
r
sis: rank by marginal correlation


b
r
ski: rank by prior knowledge integrated




discussion and 
CONCLUSIONS
in a typical omics study such as gene expression analysis or gwas, a common scenario is that first a candidate list is generated based on some statistical test procedures , and biomarkers are selected for downstream analysis or validation based on expert domain knowledge. in this study, we developed a variable selection framework, screening with prior knowledge integration , to integrate two steps into one statistical framework. inspired by sure independence screening  method, we break the procedure into two stages: first a geometric average combining the marginal information and external information together is used first to reduce the huge number of parameters to a relative small number; and then a more sophisticated methods such as lasso are used to refine the model.

the rationale of ski is to increase the sample size while limiting the noise by selecting a proper α. incorporating external knowledge could lead to more stable results since the prior knowledge is drawn from long-time accumulated studies, and thus rescue the signals overwhelmed by random artifacts in the data at hand. the knowledge relevance is evaluated by carefully selecting α to avoid arbitrariness. the similar idea could be found in machine learning techniques such as weighted ensemble predictors  <cit> .

the proposed approach is general and is not limited to any specific type of prior knowledge as long as the variables could be ranked based on some external criteria. in this study, we showed an application example in drug response prediction. since the only input for our method is a pre-ranked feature list, it could be easily modified to accommodate other applications. though, the method was developed for knowledge integration, it is suitable for data integration. in our simulation, we showed if the data heterogeneity is strong, the performance of the proposed method is even better than analysis by dataset pooling.

bergersen et al. has proposed a weighted lasso  procedure with data integration, which shared a similar idea of our approach  <cit> . however, there are three major differences between ski and wlasso. first, wlasso incorporates the external information in the penalty terms of lasso, making it similar to adaptive lasso. users have to carefully select the weight terms since it will affect the model fitting directly. our rank based method is introduced in the screening procedure; it only promotes variables into the model, but will not affect the final model fitting. second, our approach is more general for knowledge integration. it is difficult to generate a weight function for some abstract biological and medical knowledge, but it is always feasible to give a priority. finally and the most importantly, one of the purposes to design sure independence screening is to accelerate the data analysis. the computing of complexity is o smaller than lasso’s complexity, which is o. ski enjoys the same advantage as sis in terms of low computing complexity when dealing with ultra-high dimensional datasets.

sis has extended to more generalized fields such as generalized linear models, additive models, cox models, and model-free feature selections. in this study, we only discuss the linear and generalized linear model. but, as a screening-based method, ski is apparently flexible to extend to more generalized fields, too. on the other hand, li et al. proposed a variant methods, robust rank correlation screening  method, which is based on the kendall τ correlation coefficient between response and predictor variables rather than the pearson correlation of sis  <cit> . they showed the rrcs procedure could be more robust against outliers and influence points in the observations. it is also feasible for us to implement an rrcs-based ski by replacing the pearson marginal correlation by kendall’s marginal correlation, which will be the focus of future work.

