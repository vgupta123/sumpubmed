BACKGROUND
metagenomics is an emerging discipline focused on genomic studies of complex microorganismal population. in particular, metagenomics enables a range of analyses pertaining to species composition, the properties of the species and their genes as well as their influence on the host organism or the environment. as the interactions between microbial populations and their hosts plays an important role in the development and functionality of the host, metagenomics is becoming an increasingly important research area in biology, environmental and medical sciences. as an example, the national institute of health  recently initiated a far-reaching human microbiome project  <cit>  which has the aim to identify species living at different sites of the human body , observe their roles in regulating metabolism and digestion, and evaluate their influence on the immune system. the findings of such studies may have important impacts on our understanding of the influence of microbials on an individual’s health and disease, and hence aid in developing personalized medicine approaches. another example is the sorcerer ii global ocean sampling expedition  <cit> , led by the craig venter institute, the purpose of which is to study microorganisms that live in the ocean and influence/maintain the fragile equilibrium of this ecosystem.

there are many challenges in metagenomic data analysis. unlike classical genomic samples, metagenomic samples comprise many diverse organisms, the majority of which is usually unknown. furthermore, due to low sequencing depth, most widely used assembly methods – in particular, those based on de bruijn graphs – often fail to produce quality results and it remains a challenge to develop accurate and sensitive meta-assemblers. these and other issues are further exacerbated by the very large file size of the samples and their ever increasing number. nevertheless, many algorithmic methods have been developed to facilitate some aspects of microbial population analysis: examples include megan   <cit> , a widely used tool that allows for an integrative analysis of metagenomic, metatranscriptomic, metaproteomic, and rrna data; and picrust   <cit> , developed to predict metagenome functional contents from 16s rrna marker gene sequences. although suitable for taxonomic and functional analysis of data, neither megan nor picrust involve a data compression component, as is to be expected from highly specialized analytic software.

in parallel, a wide range of software solutions have been developed to efficiently compress classical genomic data . specialized methods for compressing whole genomes have been reported in , building upon methods such as modified lempel-ziv encoding and the burrows-wheeler transform. compression of reads is achieved by mapping the reads to reference genomes and encoding only the differences between the reference and the read; or, in a de novo fashion that does not rely on references and uses classical sequence compression methods. quip  <cit>  and cram  <cit>  are two of the best known reference-based compression algorithms, whereas recoil  <cit> , scalce  <cit> , mfcompress  <cit> , and the ncbi sequence read archive method compress data without the use of reference genomes. reference-based algorithms in general achieve better compression ratios than reference-free algorithms by exploiting the similarity between some predetermined reference and the newly sequenced reads. unfortunately, none of the current reference-based method can be successfully applied to metagenomic data, due to the inherent lack of “good” or known reference genomes. hence, the only means for compressing metagenomic fasta and fastq files is through the use of de novo compression methods.

as a solution to the metagenomic big data problem, we introduce metacram, the first de novo, parallel, cram-like software specialized for fasta-format metagenomic read compression, which in addition provides taxonomy identification, alignment and assembly information. this information primarily facilitates compression, but also allows for fast searching of the data in the compressive domain and for basic metagenomic analysis. the gist of the classification method is to use a taxonomy identification tool – in this case, kraken  <cit>  – which can accurately identify a sufficiently large number of organisms from a metagenomic mix. by aligning the reads to the identified reference genomes of organisms via bowtie <dig>  <cit> , one can perform efficient lossless reference-based compression via the cram suite. those reads not aligned to any of the references can be assembled into contigs through existing metagenome assembly software algorithms, such as velvet  <cit>  or idba-ud  <cit> ; sufficiently long contigs can subsequently be used to identify additional references through blast   <cit> . the reads aligned to references are compressed into the standard cram format  <cit> , using three different integer encoding methods, huffman  <cit> , golomb  <cit> , and extended golomb encoding  <cit> .

metacram is an automated software with many options that accommodate different user preferences, and it is compatible with the standard cram and samtools data format. in addition, its default operational mode is lossless, although additional savings are possible if one opts for discarding read id information. we report on both the lossless and “lossy” techniques in the “methods” section. metacram also separates the read compression process from the quality score compression technique, as the former technique is by now well understood while the latter is subject to constant changes due to different quality score formats in sequencing technologies. these changes may be attributed to increasing qualities of reads and changes in the correlations of the score values which depend on the sequencing platform. for quality score compression, the recommended method is qualcomp  <cit> .

metacram offers significant compression ratio improvements when compared to standard bzip and gzip methods, and methods that directly compress raw reads. these improvements range from 2– <dig> fold file size reductions, which leads to large storage cost reductions. furthermore, although metacram has a relatively long compression phase, decompression may be performed in a matter of minutes. this makes the method suitable for both real time and archival applications.

the paper is organized as follows. the “results” section contains an in-depth performance analysis of metacram with respect to processing and retrieval time, and achievable compression ratios. the “discussion” section describes the advantages of using metacram for data compression compared to other general-purpose methods, and describes directions for future algorithmic improvements. the “methods” section contains detailed information about the methodology behind the metacram algorithmic blocks and it also outlines the way constituent algorithms are integrated and their purposes in the pipeline.

RESULTS
the block diagram of the metacram algorithm is given in fig.  <dig>  and the operation of the algorithm may be succinctly explained as follows. the first step is to identify suitable references for compression, which is achieved by identifying dominant taxonomies in the sample. the number of references is chosen based on cut-off abundance thresholds, which themselves are chosen using several criteria that trade-off compression ratio and compression time. once the references are chosen, the raw reads are aligned to their closest references and the starting positions of the reads are statistically analyzed to determine the best integer compression method to be used for their encoding. furthermore, reads that do not align sufficiently well with any of the chosen references are assembled using idba_ud, and the contig outputs of the assembler are used to identify additional references via blast search. reads not matched with any references after multiple iterations of the above procedure are compressed independently with the mfcompress suite. the results associated with each of the described processing stages are discussed in the next subsections. note that here and throughout the paper, we use standard terms in genomics and bioinformatics without explanations.
fig.  <dig> block diagram. the block diagram of the metacram algorithm for metagenomic data processing and compression. its main components are taxonomy identification, alignment, assembly and compression



we tested metacram as a stand-alone platform and compared it to mfcompress, a recently developed software suite specialized for fasta files, and bzip <dig> and gzip  <cit> , standard general purpose compression tools . other software tools for compression of sequencing data such as scalce and quip, and samzip  <cit>  and slimgene  <cit> , were not tested because they were either for fastq or sam file formats, and not fasta files.

as already pointed out, metacram does not directly process fastq file formats for multiple reasons: 1) the quality of sequencers are improving significantly, reaching the point where quality scores may contain very little information actually used during analysis; 2) reads with low quality scores are usually discarded and not included in metagenomics analysis – only high quality sequences are kept; 3) there exist software tools such as qualcomp  <cit> , specifically designed for compressing quality scores that users can run independently along with metacram.

taxonomy identification and reference genome selection
as the first step of our analysis, we compared two metagenomic taxonomy identification programs, kraken and metaphyler in terms of computation time and identification accuracy on synthetic data, as it is impossible to test the accuracy of taxonomy identification on real biological datasets. for this purpose, we created mixtures of reads from  <dig> species, listed in the additional file  <dig>  the two illumina paired-end read files were created by metasim  <cit>  with  <dig> % error rate, and they amounted to a file of size  <dig>  gb. kraken finished its processing task in  <dig> min and successfully identified all species within the top  <dig> most abundant taxons. on the other hand, metaphyler ran for  <dig> min and failed to identify acetobacterium woodii and haloterrigena turkmenica at the genus level. this example illustrates a general trend in our comparative findings, and we therefore adopted kraken as a default taxonomy retrieval tool for metacram.

when deciding how to choose references for compression, one of the key questions is to decide which outputs of the kraken taxonomy identification tool are relevant. recall that kraken reports the species identified according to the number of reads matched to their genomes. the most logical approach to this problem is hence to choose a threshold for the abundance values of reads representing different bacterial species, and only use sequences of species with high abundance as compression references. unfortunately, the choice for the optimal threshold value is unclear and it may differ from one dataset to another; at the same time, the threshold is a key parameter that determines the overall compression ratio – choosing too few references may lead to poor compression due to the lack of quality alignments, while choosing too many references may reduce the compression ratio due to the existence of many pointers to the reference files. in addition, if we allow too many references, we sacrifice computation time for the same final alignment rate. it is therefore important to test the impact of the threshold choice on the resulting number of selected reference genomes.

in table  <dig>  we listed our comparison results for all five datasets studied, using two threshold values:  <dig>  and  <dig> . for these two choices, the results are colored gray and white, respectively. we observe that we get slightly worse compression ratios if we select too few references, as may be seen for the files err <dig> and err <dig>  still, the processing time is significantly smaller when using fewer references, leading to  <dig> to  <dig> minutes of savings in real time. it is worth to point out that this result may also be due to the different qualities of internal hard drives: for example, the columns in gray were obtained running the code on seagate barracuda st <dig>  while the results listed in white were obtained via testing on western digital nas.
comp. 
processing time
align. %
no. files
columns in bold represent a threshold of  <dig> species, while the columns not bolded correspond to a cutoff of  <dig> species. the results are shown for metacram-huffman. “align. %” refers to the alignment rates for the first and second round, and “no. files” refers to the number of reference genome files selected in the first and second iteration. processing times are recorded row by row denoting real, user, and system time in order



many of the most abundant references may be from the same genus, and this may potentially lead to the problem of multiple alignment due to subspecies redundancy. the almost negligible effect of the number of reference genomes on alignment rate implies that combining them to remove the redundancy would improve computational efficiency, as suggested in  <cit> . nevertheless, extensive computer simulations reveal that the loss due to multiple alignment is negligible whenever we choose up to 75– <dig> references. therefore, our recommendation is to use, as a rule of thumb, the threshold  <dig> in order to achieve the best possible compression ratio and at the same time provide a more complete list of genomic references for further analysis.

compression performance analysis
our chosen comparison quality criteria include the compression ratio , as well as the compression and decompression time, as measured on an affordable general purpose computing platform: intel core i5– <dig> cpu at  <dig>  ghz, with a  <dig> gb ram. we present test results for five datasets: err <dig>  srr <dig>  err <dig>  srr <dig>  and srr <dig>  including metagenomic samples as diverse as a human gut microbiome or a richmond mine biofilm sample, retrieved from the ncbi sequence read archive  <cit> . additional file  <dig> contains detailed descriptions of the datasets tested.

the comparison results of compression ratios among six software suites are given in table  <dig> and fig.  <dig>  the methods compared include three different modes of metacram, termed huffman, golomb and extended golomb metacram. these three techniques differ from each other with respect to the integer compression scheme used. the schemes will be described in detail in the next sections, although we remark that the three methods are chosen to illustrate various compression ratio and decompression time trade-offs.
fig.  <dig> compression ratio. the compression ratios for all six software suites, indicating the compression ratio

191
263
948
703
137
for short hand notation, we used“mch” = metacram-huffman, “mcg” = metacram-golomb, “mceg” = metacram-extended golomb, “mfcomp” = mfcompress. mch <dig> is the default option of metacram with huffman encoding, and mch <dig> is a version of metacram in which we removed the redundancy in both quality scores and the read ids. “align. %” refers to the total alignment rates from the first and second iteration. minimum compressed file size achievable by the methods are written in bold case letters. minimum compressed file size achievable by the methods are written in bold case letters



the result indicates that metacram using huffman integer encoding method improves compression ratios of the classical gzip algorithm 2– <dig> fold on average. for example, metacram reduces the file size of srr <dig> to only  <dig> % of the original file size. observe that metacram also offers additional features that go beyond compression only, such as taxonomy identification and assembly. users have the options to retrieve the alignment rate, list of reference genomes, contig files, and alignment information in sam format. this list produced by metacram may be stored with very small storage overhead and then used for quick identification of files based on their taxonomic content, which allows for selection in the compressive domain. information regarding gene profiles was not included in the pipeline output, as gene analysis does not directly contribute to the quality of the compression algorithm.

in the listed results, the column named “qual value ” provides the estimated size of the quality scores for each file, after alignment to references found by kraken. in our implementation, we replaced these scores with a single “*” symbol per read and also removed the redundancy in read ids. the result shows that these two options provide better ratios than the default ratio, as shown in table  <dig> column “mch2”. however, since read ids may be needed for analysis of some dataset, we also report results for the default “mch1” mode which does not dispose of id tags.

in terms of the processing time shown in table  <dig>  the metacram suite is at a clear disadvantage, with processing time 150-fold slower than bzip <dig> in the worst case. figure  <dig> presents the average runtime of each stage for all five datasets tested, and illustrates that assembly, alignment, and blast search are computationally demanding, accounting for  <dig> percentage of the total time. this implies that removing the second and subsequent assembly rounds of metacram reduces the processing time significantly, at the cost of a smaller compression ratio. table  <dig> compares the compression ratios of metacram with one round and with two rounds of reference discovery, and indicates that removing the assembly, alignment and blast steps adds 1– <dig> mb to the compressed file size. thus, the user has an option to skip the second round in order to expedite the processing time.
fig.  <dig> average runtime of each stage of metacram. detailed distribution of the average runtimes of metacram for all five datasets tested. we used “_1” to indicate the processes executed in the first round, and “_2” to denote the processes executed in the second round

for short hand notation, we used“mch-2rounds” = metacram-huffman with  <dig> rounds, “mch-1round” = metacram-huffman with  <dig> round. we also used the shortcut “mfcomp” = mfcompress and “align. %” refers to the percentage of reads aligned during  <dig> rounds and  <dig> round, respectively, for mch-2rounds and mch-1round



likewise, table  <dig> illustrates that the retrieval time of metacram is longer than that of bzip <dig>  gzip, and mfcompress, but still highly efficient. in practice, the processing time is not as relevant as the retrieval time, as compression is performed once while retrieval is performed multiple times. for long term archival of data, metacram is clearly the algorithm of choice since the compression ratio, rather than processing or retrieval time, is the most important quality criteria.


we also remark on the impact of different integer encoding methods on the compression ratio. huffman, golomb, and extended golomb codes all have their advantages and disadvantages. for the tested datasets, huffman clearly achieves the best ratio, as it represents the optimal compression method, whereas golomb and extended golomb compression improve the real and system time as a result of computation efficiency. however, the parallel implementation of metacram makes the comparison of processing time of the three methods slightly biased: for example, if we perform compression while performing assembly, compression will take much more time than compressing while running an alignment algorithm. as the processing and retrieval time is not consistent among the three methods, we recommend using huffman coding for archival storage.

discussion
in what follows, we comment on a number of useful properties of the metacram program, including compatibility, losslessness, partial assembly results and compressive computing.

compatibility. metacram uses well established and widely tested genomic analysis tools, and it also follows the standard genomic data compression format cram, hence making the results of downstream analysis compatible with a current standard for genomic compression.

lossless compression principle. by its very nature, metacram is a lossless compression scheme as it encodes the differential information between the reference and the metagenomic reads in a  <dig> % accurate fashion. nevertheless, we enabled a feature that allow for some partial loss of information, such as the read id tags. it is left to the discretion of the user to choose suitable options.

cram versus mfcompress. mfcompress achieves good compression ratios when compressing highly redundant reads. metacram consistently achieves a rate proportional to the alignment rate because it only encodes the small difference between the reference genome and the read. as more microbial genome become available, metacram will most likely offer higher compression ratio than other tools in general. note that only on one data file - srr <dig> - did mfcompress achieve better compression ratios than metacram, most likely due to the redundancy issues previously mentioned.

metagenomic assembly. metagenomic assembly is a challenging task, and there is a widely accepted belief that it is frequently impossible to perform meaningful assembly on mixture genomes containing species from related genomes. nevertheless, we are using assembly mostly as a means for identifications, but at the same time its output provides useful contigs for gene transfer analysis and discovery. in the case that assembly fails on a dataset, we suggest skipping the assembly step so as to trade off computation time with discovery of new reference genomes and contigs.

compressive computing. there has been an effort towards computing in the compressed domain, in order to eliminate the need for persistne compression and decompression time when all one needs to perform is simple alignment  <cit> . similarly, metacram offers easy retrieval and selection based on the list of references stored as an option. for example, suppose we perform metacram on all available human gut metagenome data. if we want to analyze the datasets with a concentration of escherichia coli, we avoid sacrificing retrieval time by quickly scanning the list of reference files and only retrieving the datasets with e. coli.

methods
pre-processing
metacram accepts both unpaired and paired-end reads. if paired-end reads are given as an input to metacram, then the first preprocessing step is to append the read ids with a “_1” or a “_2” indicating that the read came from the first or second mate, respectively. another preprocessing step includes filtering out the quality scores in case that the input file is in fastq format. this filtering process allows for using new and emerging quality score compression methods without constantly updating the metacram platform. note that the paired end labeling is done automatically, while filtering can be implemented outside the integrated pipeline by the user, based on his/her requirements for quality score lossy or lossless compression goals.

metacram uses as a default fasta files that do not contain quality values, in which case the resulting sam file contains the symbol “i” repeated as many times as the length of the sequence. these symbols amount to about  <dig> bytes per read, and this overhead increases proportionally to the number of reads . in order to reduce the size of this unnecessary field, metacram replaces the sequence of “i”s with a single symbol “*”, complying with the standard sam format. likewise, read ids are highly repetitive in nature: for instance, every read id starts with the data name such as “srr <dig> ”, followed by its unique read number. rather than repeating the data name for every read, we simply store it once, and append it when performing decompression. both versions of metacram – one incorporating these two options – and another one without the described features are available to the user. the former version of the methods requires a slightly longer compression and decompression time.

taxonomy identification
given the labeled read sequences of a metagenomic sample, the first step is to identify the mixture of species present in the sample. there are several taxonomy identification methods currently in use: the authors of  <cit>  proposes to use the 16s rrna regions for bacterial genome identification, metaphyler  <cit>  scans for unique markers exceeding length  <dig> and provides a taxonomy level as specific as the genus. on the other hand, a new taxonomy identification software known as kraken  <cit> , based on exact alignment of k-mers to the database of known species, often outperforms metaphyler and other methods both in terms of speed and discovery of true positives, as indicated by our tests.

metacram employs kraken as a default tool in the pipeline. kraken produces an output report which is automatically processed by metacram. part of the report contains information about species present in the sample, as well as their abundance. we rank order the species in from the most abundant to the least abundant, where abundance is based on the number of reads identified to match a species in the database. for downstream analysis, metacram selects the “most relevant” species and uses their genomes as references. the default definition of “most relevant” is the top  <dig> species, but one has the option to choose a threshold for the abundance value or for the number of references used. as an illustration, table  <dig> lists the results of an analysis of the impact of different thresholds on the processing time and the compression ratio.

alignment and assembly
after a group of reference genomes is carefully chosen based on the kraken software output, alignment of reads to the reference genomes is performed. this task is accomplished by using bowtie <dig>  a standard software tool for ultra-fast alignment of short reads to long genomes. the alignment information is stored in a sam  file format and subsequently used for compression via reference-based algorithms.

due to the fact that many species in a metagenome sample have never been sequenced before, some reads will not be aligned to any of the references with high alignment scores, and we collectively refer to them as unaligned reads hereafter. in order to discover reference genomes for unaligned reads, we assemble the unaligned reads in a relatively efficient, although often time consuming manner using a metagenomic assembler. our metagenomic assembler of choice is idba-ud  <cit> , given that in our tests it produced the largest number of contigs leading to new reference identification. alternatives to idba-ud include the ray meta software  <cit> .

when the reads have high sequencing depth and large overlaps, the contigs produced by the assembler may be queried using blast to identify the organisms they most likely originated from. the user may choose to blast only the top n longest contigs, where n is a user specified number, but in our analysis we use all contigs . subsequently, we align the unaligned reads to the newly found references.

in some rare cases, the assembler may fail depending on the number of species in the metasample and the sequencing depth, in which case one may want to skip the assembly step and compress the unaligned reads in a reference-free manner. for reference-free compression, the software of choice in metacram is mfcompress  <cit> . as long as the assembler is successful, one can reduce the volume of unaligned reads by iterating the process of assembly, blast, and alignment as illustrated at the top right hand of fig.  <dig>  all our demonstrations and results are based on two iterations of the described algorithmic loop.

distribution of read starting positions
we empirically studied the distribution of integers representing the read positions, variation positions, and paired-end offsets in order to choose the most suitable compression method. as an example, the distribution of the starting positions for the reads that aligned to jh <dig>  in the dataset srr <dig> is shown in fig.  <dig>  this distribution was truncated after achieving a  <dig> % coverage of the data . the empirical distribution is shown in yellow, while a fitted power law distributions is plotted and determined according to  <cit> , with pi=2−logmi12i, where i is the integer to be encoded, and m is the divisor in the extended golomb code. the parameter choose shown is m= <dig> and  <dig>  the negative binomial distribution is fitted using maximum likelihood estimation , while the geometric distribution is fitted by two different means: using mle and ezfit, a matlab script which performs an unconstrained nonlinear minimization of the sum of squared residuals with respect to various parameters.
fig.  <dig> integer distribution. distribution fitting of integers to be encoded, truncated at  <dig> % of the integer data



for single reference alignment methods, it was reported that the best fit for the empirical distribution is a geometric distribution or a negative binomial distribution  <cit> . however, due to sequencing errors and non-uniform distributions of hydrogen bond breakage, the empirical data often deviates from geometric or negative binomial distributions  <cit> . in addition, for metagenomic samples, there exist multiple references which may have good alignments with reads that did not originally correspond to the genomic sample of the reference. this creates additional changes in the read starting position with respect to the geometric distribution. moreover, one has to encode not only the read positions but also the variation positions and paired-end offsets, making it difficult to claim any one of the fitted distributions is better than others. this observation is supported by fig.  <dig>  since there is no known efficient optimal encoding method for a set of integers with negative binomial distributions, and golomb and extended golomb encoding are optimal for geometric distributions and power law distributions, respectively, we use these two methods with m= <dig>  the parameter m is chosen based on extensive experiments, although the user has the freedom to adjust and modify its value.

as the number of unaligned reads that remains after a few iterations of metacram is relatively small, these reads were compressed using a reference-free tool such as mfcompress  <cit> , which is based on finite-context models. furthermore, the sam files produced after running bowtie <dig> are converted to the sorted and indexed binary format of a bam file using samtools  <cit> . each bam file is compressed via reference-based compression against its representative to a standard cram format. we tested three different modes of the cram toolkit  <cit> : huffman, golomb, and extended golomb encoding, all of which are described in the next section. note that the extended golomb encoding method is our new addition to the classical cram method, as it appears to offer good compromises between compression and decompression speed and compression ratios.

intrinsically, sam files contain quality values and unique read ids for each read, which inevitably account for a large file size: quality values are characters of length as long as the sequence, and read ids often repeat the name of the dataset. by default, metacram preserves all quality values and read ids as designed in cram.

compression
compression in the reference-based mode is accomplished by compressing the starting points of references with respect to the reference genomes and the base differences between the reads and references. as both the starting points and bases belong to a finite integer alphabet, we used three different integer compression methods, briefly described below.

huffman coding is a prefix-free variable length compression method for known distributions  <cit>  which is information-theoretically optimal  <cit> . the idea is to encode more frequent symbols with fewer bits than non-frequent ones. for example, given an alphabet a= and the corresponding distribution p=, building a huffman tree results in the codebook c=. decoding relies on the huffman tree constructed during encoding which is stored in an efficient manner, usually ordered according to the frequency of the symbol. due to the prefix-free property, huffman coding is uniquely decodable coding and does not require any special marker between words. two drawbacks of huffman coding that make it a costly solution for genomic compression are its storage complexity, since we need to record large tree structures for big alphabet size which arise when encoding positions in long sequences and the need to know the underlying distribution a priori. adaptive huffman coding mitigates the second problem, at the cost of increased computational complexity associated with constructing multiple encoding trees  <cit> . in order to alleviate computational challenges, we implemented so called canonical huffman encoding, which bypasses the problem of storing a large code tree by sequentially encoding lengths of the codes  <cit> .

golomb codes are optimal prefix-free codes for countably infinite lists of non-negative integers following a geometric distribution  <cit> . in golomb coding, one encodes an integer n in two parts, using its quotient q and remainder r with respect to the divisor m. the quotient is encoded in unary, while the remainder is encoded via truncated binary encoding. given a list of integers following a geometric distribution with known mean μ, the dividend m can be optimized so as to reduce code length. in  <cit> , the optimal value of m was derived for m=2k, for any integer k. the encoding is known as the golomb-rice procedure, and it proceeds as follows: first, we let k∗=max <dig> +log2loglogμμ+ <dig>  where ϕ= <dig>  unary coding represents an integer i by i ones followed by a single zero. for example, the integer i= <dig> in unary reads as  <dig>  truncated binary encoding is a prefix-free code for an alphabet of size m, which is more efficient than standard binary encoding. because the remainder r can only take values in { <dig> ,…, m-1}, according to truncated binary encoding, we assign to the first 2k+1−m symbols codewords of fixed length k. the remaining symbols are encoded via codewords of length k+ <dig>  where k=⌊log2⌋. for instance, given n= <dig> and m= <dig>  we have 7=2×3+ <dig>  implying q= <dig> and r= <dig>  encoding  <dig> in unary gives  <dig> and  <dig> in truncated binary reads as  <dig>  hence, the codeword used to encode the initial integer is the concatenation of the two representations, namely  <dig> 

decoding of golomb encoded codewords is also decoupled into decoding of the quotient and the remainder. given a codeword, the number of ones before the first zero determines the quotient q, while the remaining k or k+ <dig> bits, represents the remainder r according to truncated binary decoding for an alphabet of size m. the integer n is obtained as n=q×m+r.

golomb encoding has one advantages over huffman coding in so far that it is computationally efficient . one does not need to the distribution a priori, although there are clearly no guarantees that golomb coding for an unknown distribution will be even near-optimal: golomb encoding is optimal only for integers following a geometric distribution.

an extension of golomb encoding, termed extended golomb  <cit>  coding, is an iterative method for encoding non-negative integers following a power law distribution. one divides an integer n by m until the quotient becomes  <dig>  and then encodes the number of iterations m in unary, and an array of remainders r according to an encoding table. this method has an advantage over golomb coding when encoding large integers, such is the case for read position compression. as an example, consider the integer n=1000: with m= <dig>  golomb coding would produce q= <dig> and r= <dig>  and unary encoding of  <dig> requires  <dig> bits. with extended golomb coding, the number of iterations equals m= <dig> and encoding requires only  <dig> bits. as an illustration, let us encode n= <dig> given m= <dig>  in the first iteration, 7=2×3+ <dig>  so r1= <dig> is encoded as  <dig>  and q1= <dig>  since the quotient is not  <dig>  we iterate the process: 2=0×3+ <dig> implies r2= <dig>  which is encoded as  <dig>  and q2= <dig>  because the quotient is at this step  <dig>  we encode m= <dig> as  <dig> and r=r2r1= <dig>  and our codeword is  <dig> 

the decoding of extended golomb code is also performed in m iterations. since we have a remainder stored at each iteration and the last quotient equals qm= <dig>  it is possible to reconstruct the original integer. similar to golomb coding, extended golomb encoding is computationally efficient, but optimal only for integers with power law distribution.

there are various other methods for integer encoding, such as elias gamma and delta encoding  <cit> , which are not pursued in this paper due to the fact that they do not appear to offer good performance for the empirical distributions observed in our read position encoding experiments.

products
the compressed unaligned reads, cram files, list of reference genomes , alignment rate , contig files  are all packaged into an archive. the resulting archive can be stored in a distributed manner and when desired, the reads can be losslessly reconstructed via the cram toolkit. additional file  <dig> contains software instructions, and detailed descriptions of created files and folders by metacram processing are available in additional file  <dig> 

decompression
lossless reconstruction of the reads from the compressed archive is done in two steps. for those reads with known references in cram format, decompression is performed with an appropriate integer decompression algorithm. when the files are converted back into the sam format, we retrieve only the two necessary fields for fasta format, i.e., the read ids and the sequences printed in separate lines. unaligned reads are decompressed separately, through the decoding methods used in mfcompress.

post-processing
the two parts of reads are now combined into one file, and they are sorted by the read ids in an ascending order. if the reads were paired-end, they are separated into two files according to the mate “flag” assigned in the processing step.

effects of parallelization
one key innovation in the implementation of metacram is parallelization of the process, which was inspired by parallel single genome assembly used in tiger  <cit> . given that metagenomic assembly is computationally highly demanding, and in order to fully utilize the computing power of a standard desktop, metacram performs meta assembly of unaligned reads and compression of aligned reads in parallel. as shown in table  <dig>  parallelization improves real, user, and system time by 23– <dig> %.


availability of supporting data
the datasets supporting the results of this article are available in the national center for biotechnology information sequence read archive repository, under accession numbers err <dig> , srr <dig> , err <dig> , srr <dig> , srr <dig> .

CONCLUSIONS
we introduced metacram, the first parallel architecture for reference-based, lossless compression of metagenomic data. the compression scheme is compatible with standard cram formats and offers significantly improved compression ratios compared to the existing software suites, compressing file to 2- <dig> percent of the original size. furthermore, it provides the user with taxonomy and assembly information, allowing for fast selection of relevant files in the compressed domain. thus, metacram may represent an important processing platform for large metagenomic files.

additional files
additional file  <dig> 
comparison between kraken and metaphyler. we compare two taxonomy identification tools and decide that kraken outperforms metaphyler in terms of both speed and accuracy. additional file  <dig> contains a table of randomly selected set of  <dig> species used for the comparison. 



additional file  <dig> 
datasets used for testing metacram. this file has detailed descriptions of the  <dig> datasets used for testing metacram, with unique id specified in the ncbi sra and the type of library used. 



additional file  <dig> 
software instruction. this file includes specific commands to compress or decompress metacram, including options available. it also contains default software commands used for each step of metacram pipeline. 



additional file  <dig> 
outcome of metacram. additional file  <dig> illustrates detailed outcome of metacram, such as files and folders produced after compression and decompression, and an example of console output. 



competing interests

the authors declare that they have no competing interests.

authors’ contributions

mk, xz, jl, ff, vv and om contributed to the theoretical development of the algorithmic method. mk, xz and jl implemented the algorithms, while mk, xz, and ff, tested it on a number of datasets. mk also co-wrote the paper and suggested a number of components in the execution pipeline. om conceived the works, proposed the compression architecture and wrote parts of the paper. all authors read and approved the final manuscript.

