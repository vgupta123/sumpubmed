BACKGROUND
the increasing adoption of semantic web technologies and formalisms in biomedical and biomolecular areas is often driven by the need to interoperate between ever more complex data stores and between the applications that process them  <cit> . as the pace of adoption quickens, a distributed infrastructure is emerging that is starting to satisfy the two properties of a von neumann architecture, also known as "stored-program computer": that it can store both data and the applications. the mingling of data and ready to run applications is particularly tightly woven in web services that rely on padded json calls , following on proposals for crossdomain json calls such as  <cit> , and are now used by major web  <dig>  services  <cit> . in those systems, if properly configured  <cit> , there are no syntactic barriers to workflows that pull data and code from different machines and then transfer the results to further use elsewhere.

by breaching the same origination restriction of url calls in the conventional  ajax model, json-based systems also subvert the client-server equation. specifically, json calls are function calls where the data is passed as the input argument and the function name is specified as a callback parameter in the url call. this signifies that data and code can be invoked freely from multiple originations and can be made part of arbitrary workflows, as in the node.js project  <cit> . this outcome, illustrated in figure  <dig>  was also anticipated by the view of the semantic web as leading to an ecosystem of usages accessing a shared "rdf-bus"  <cit> . it also suggests an architecture for distributed bioinformatics infrastructures that literally delivers the "web as a computer", that is, a von neumann machine. indeed, the work described here can be construed as an attempt to build the minimal set of server-side features that facilitate the integrated management of data and of its analysis in a web infrastructure.

in a strict sense, a von neumann architecture comes with a von neumann bottleneck  <cit>  in data access. however, the client side applications described in figure  <dig> are hosted in independent machines, that is, in machines with their own memory where data can be cached for ready access by the cpu. therefore this architecture is more accurately described as a von neumann hybrid supporting non-uniform memory access . in a nutshell, the distributed computing enabled by web-like architectures have fundamental advantages for scalability that stem from the memory access architecture and the reliance on functional programming  along lines anticipated by john backus  <dig> turing lecture, which are now key to data-intensive scientific discovery  <cit> .

the use of server-side only as a standardized representation layer for scientific research applications is not original. it is, for example, at the core of cloud computing based systems such as google wave  <cit> . in such systems, the computational intensive data processing components can be deployed as client-side services that regularly consult the representation of the domain they were written to process. in the illustrative google wave example those client-side services are designated as "robots"  <cit> . what is originally proposed here, and is illustrated with prototype applications, are minimal abstractions that will support the requirements of a distributed bioinformatics infrastructure.

design criteria
the architecture described in this report specifically targets biomedical applications, which places two requirements on a web-based infrastructure. first, it needs to accommodate the fluidity that is intrinsic to the biology knowledge domain  <cit> . second, individual variability is driving the redesign of biomedical information management systems to allow mixing private and public data in personalized medicine applications  <cit> .

the first design criterion of flexible handling of fluid and heterogeneous domains suggests that the biology domain expert should control the description of the domain that is being experimentally explored. in fact, the results of data analysis often require the redesign of the original data acquisition effort. the redesign happens so patently that, for example, it is now explicitly exploited to speed up target identification and drug discovery through the use of adaptive designs in clinical trials  <cit> . redesign is also often triggered in biomedical research by advances in the analytical methods and can become the major challenge to the use of new technologies, as is currently the case for next generation sequencing  <cit> . in either case that redefinition of domain has to be achieved without compromising the consistency of the data already acquired. another consequence of this domain fluidity is the need for co-existence of a myriad of sub-fields and subcommunities which are not necessarily in agreement with each other. this property alone suggests that bioinformatics infrastructure should support bottom-up, collaborative, data acquisition and representation linked to multiple descriptions of the same domain. the motivation for this design criterion is therefore the accommodation of the widest range of data acquisition efforts in the same web-based infrastructure. the resulting resource would be useful as the starting point for the identification of logical models, while not being constrained by them.

the second design criterion, that of fine grained management of access permission, calls for a generic mechanism to describe the relationship between the user and each data element. that description could then be used by the infrastructure to decide what types of access to the data are authorized for each user. a literal reading of this requirement would be to document that relationship for each element, individually, and for each user. this absurd solution would of course increase the size of the data repository several fold. a more scalable alternative is therefore needed which allows for that relationship to be also defined for the description of the domain, and then propagated to its observational instantiation. accordingly, the identification of a markov model that propagates user relationships among the s3db entities is the second key feature of the core model described in this report.

methods
the abstractions described in the results resort to w3c formalisms, are illustrated by an accompanying library and are partially deployed by a web-service:

core model entities
the description of the core entities of the s3db core model was pursued with recourse to the world wide web's consortium  resource description framework   <cit> , including related schema language rdfs  <cit> , and owl web ontology language  <cit> .

propagation of user operators
s3db operators describe relationships between users and entities of the core model. any operator predicated on a user as a subject, and on any of the seven s3db core entities as an object, will be propagated as a markov process. the propagation model described in the results section was originally coded as a finite state automata  using matlab  and consists of three functions: merge, migrate and propagate. these m-functions were written to be also executable in less sophisticated open source m-code interpreters such as freemat http://freemat.sourceforge.net. the three functions were then also coded in javascript to support web browser based applications such that their inner workings can be explored without need for specialized programming environments. these applications, with links to the m and js source code, are made available at http://s3db-operator.googlecode.com. the markovian process described by them was used in the prototype web service  to calculate the independent propagation of each of the three permission operators supported by that particular implementation - view, edit and use - for each of the three permission states considered - none, self and all.

web service prototype
the identification of the s3db core model has been pursued for five years as an iterative exercise where tentative new features in the core model were exposed to communities of biomedical and molecular epidemiologists to collect usage feedback  <cit> . this feedback typically came with suggestions for desirable behaviors that informed the next round of core model re-design. a regularly updated version of this prototype web service is available for download with open source at the http://s3db.org project web site and also at http://s3db.googlecode.com. the webservice's api is exposed through a rest protocol, s3ql, documented at http://s3db.org/documentation/s3qlsyntax. a javascript library for cross domain json requests is also provided at http://s3dbcall.googlecode.com.

RESULTS
the advantages of a "sloppy", evolvable, data representation distinguishing between domain and instantiation was first argued in  <cit>  using a relational diagram. that argument was expanded and a first draft of the core model was subsequently used to integrate distributed data sources for a lung cancer spore  <cit> , and to enable the realtime analysis of dna copy number variation  in glioblastoma multiforme tumor samples  <cit> , and to support a standards based proteomic repository  <cit> . a complete model, designated as "s3db core model", needs to merge that draft logical model with the markovian propagation of user operators used to assign user permissions to s3db entries.

separating domain from its observational instantiation
the separation of domain from instantiation is centered on the pattern described in lower half of figure  <dig>  where the representation of domain as triples is shown to be the predicate of the statements that instantiate that domain with observations. the  <dig> core entities and  <dig> logic relationships between them, outlined graphically in figure  <dig>  are more formally described in table  <dig>  in a nutshell, the use of the s3db model ultimately consists of declaring all the data elements associated with a given observation as being types of s3db entities.

 <dig>  s3db:rsubject owl:inverseof rdf:subject; rdfs:domain
 <dig>  s3db:robject owl:inverseof rdf:object; rdfs:domain
 <dig>  s3db:rpredicate owl:inverseof rdf:predicate; rdfs:domain
 <dig>  s3db:ssubject owl:inverseof rdf:subject; rdfs:domain s3db:item;
 <dig>  s3db:sobject owl:inverseof rdf:object; rdfs:domain s3db:item;
all relationships except for s3db:operator  are s3db:relationship . the inversion of rdf subject, predicate and object in relations 5- <dig> may appear capricious at this point but it will simplify the identification of automata for the propagation of s3db:operator states in the next section. specifically, it will allow the definition of equation  <dig> such that the direction of the arrows in figure  <dig> is the same as the propagation of s3db:operator states.

the core entities
the root entity of each s3db representation is the deployment, which is identified by the location of the s3db service. a deployment is directly related  with users and projects , with the latter providing granularity for sets of collections and rules . the collections are used as subjects and as objects of rules, which represent the domain one wants to instantiate with observations . for example the concept that "people live in places" would be represented by a rule associating the collection of people with the collection of locations . the collections in turn delimit sets of items . for example, mary could be an item of the collection of people. in some cases the object of the rule is best not confined by a collection and instead can be left as a literal, as in, for example, the instantiation of the rule "people have names". note no cardinality is imposed for any relationship so many-to-many scenarios are allowed. for example, the same item can be a member, s3db:ci, with multiple collections. on the contrary, we found it useful to restrict the predicates of the rules, such as live in and have above, to be items of collections , as will be clearer after describing the propagation of user operators.

data submission to a s3db service corresponds to instantiating those rules with the observations, through the use of statements. for example, the statement "mary lives in houston" would have a rule like "people live in places" as a predicate associating an item of the collection of people  with an item  of the collection place. one could then continue weaving the description with further assertions by first identifying new components of the domain, for example, by creating a rule to the effect that "places have addresses" and then asserting those addresses as items of the collection of places. it is important to recall that each non-literal element is identified by a universal resource identifier , necessary to make assertions using rdf. in conclusion, the purpose of the s3db core model is just to provide a template where to aggregate data elements that may already be available, either as their own pre-existing uris, or, otherwise, by generating those uris within s3db.

the illustration of the previous paragraph will now be repeated using the formalism of notation  <dig>  and the rdf, rdfs and owl vocabularies , with reference to the list of  <dig> relationships described in table  <dig> 

we can now return to the example that "mary lives in houston" and use the s3db template to generate the triples to be submitted to the s3db service. starting with a deployment hosting an instance of a s3db:project, p_example, and using notation  <dig> ,

a) create collections of people and places:

:p_example s3db:pc :c_person.

:p_example s3db:pc :c_places.

b) insert mary and houston as items of the respective collections:

:c_person s3db:ci :i_mary.

:c_places s3db:ci :i_houston.

c) describe the domain we are about to instantiate, that people live in places, as a

s3db:rule:

:p_example s3db:pr :r_people_in_places.

:c_person s3db:rsubject :r_people_in_places.

:c_place s3db:robject :r_people_in_places.

:i_lives_in s3db:rpredicate :r_people_in_places.

.

d) insert the new data:

:i_mary :r_people_in_places :houston.

this example illustrates a very simple mechanism to store descriptions of the domain and the data that instantiates them in such as way that they can be edited as required by the fluidity of the life sciences domain  <cit> . the actual identifiers, such as ":i_mary", ":c_person" or ":p_example" in reality are random or sequential alphanumeric strings such as the unique indexes generated by the s3db service. of course :i_mary has a name, which we will use as an example to illustrate how literal values are asserted through instantiation of a rule, without the need for, say, mediation by a collection of names:

@prefix foaf: <http://xmlns.com/foaf/ <dig> />.

:p_example s3db:pr :r_people_have_ names.

:c_person s3db:rsubject :r_people_have_names.

foaf:firstname s3db:robject :r_people_have_names.

:i_has s3db:rpredicate :r_people_have_names.

which then allows inserting the corresponding literal information,

:i_mary :r_people_have_names "mary".

note in this last assertion that neither the object of the rule, foaf:firstname, nor the object of the statement, "mary", are s3db entities. as noted in figure  <dig> and discussed in its legend, when the object of the rule is not a collection, the core model allows for any type of content  to be associated to either object in the rule/statement instantiation. in the implementation followed by the reference s3db prototype, these two non-s3db entities are simply stored as literals in a variable length string  database field.

more importantly than the data type of the rule and statement objects is that if mary changes her name that doesn't affect the information about where she lives or that she has a first name. the reverse is also true, if what she has is no longer designated foaf:firstname, it could even be replaced by another literal such as "name", that editing does not affect the existing assertion: whatever the new designation is, it is still instantiated in the same statement, with the same uri, by the same rdfs:resource, in this case the literal "mary". this is far from being an esoteric scenario. in molecular epidemiology surveillance it is quite common to have, for example, the identity of a microbial isolate, which is an instance of a class, be used as the predicate of the molecular typing methodology. there is of course nothing new here; this modularity is intrinsic to the dyadic predicated nature of the rdf framework. however, what was achieved by tying the submission of new data to the s3db core model was to restrict the use of rdf such that an explicit distinction of domain and observational instantiation is preserved throughout the process. as will be discussed later, this was achieved purely through the assertion  <cit>  of a design pattern, that is, without the computational overhead of description logics and the need of reasoners for subsequent information retrieval.

propagation of s3db operator states
the last relationship in table  <dig>  s3db:operator, is the point of entry for the second component of the core model, the embedded finite state automata  that propagates the states of any such operator. this component allows the assertion of a generic relationship between a user and a component of the domain, for example, collections and rules, and then expect to find it automatically propagated for its instantiation, for example, as items and statements . inversely, s3db operators can also be used to define exceptions to broader relationships, for example, by using operator states to describe relationships between the user and individual statements, or items, without affecting the remainder entries in the same set, that is, for other statements on the same rule or items of the same collection. this model was identified in very generic terms as to allow for the definition of complex relationships to be described succinctly, without the need to particularize them for each entry. throughout the different projects where s3db was used, we have found the critical need for a solution that is balanced between the two extreme scenarios of having a system where all user permissions are indiscriminately set at the point of access, and the extreme alternative of having user group permissions for every entry. accordingly, the embedded propagation of user relationships was first devised narrowly as a solution for the challenges of mixing public and private data, as well as mixing data instantiating distinct, even contradictory, descriptions of a domain in multiple investigator initiatives. it was only in the later stages of the project that the opportunity for a generic solution of propagating unspecified operator states became apparent.

an s3db operator, f, is a discrete variable with a set of n ordered states. the elements of the set can exist in two different forms, a upper case or dominant form, Φ, and a lower case or recessive form, ϕ. for example, the capital form of the ith state of the operator f, would be represented as Φi, where i ε. accordingly, the description of such relationship between a user and some s3db:entity is defined using the state of the operator, as represented in equation 1:   

the three functions described below, merge, migrate and percolate, are used in the resolution of state propagation between data elements. that description is best followed by testing different scenarios using the accompanying tool at http://s3db-operator.googlecode.com.

merging
as illustrated in equation  <dig>  for each user, u, and for each instance, e, of any of the seven types of s3db:entity, the nature of the relationship can be described by an arbitrary number of states of the operator f , by simply declaring the {u f e} triple. however, regardless of such statements having been made between a user and an entity, the f state assigned as predicate in those statements is not necessarily the effective state of that relationship. other states may also be indirectly asserted to the relationship by directly assigning them for relationships with entities upstream of the target entity. the resolution of what state is effective for the relationship between a given u and e is resolved by merging all the assigned states, directly or indirectly, as defined in equation  <dig>  in this equation, a is the vector of indexes of assigned dominant  states,Φ; and a is the index vector of assigned recessive  states, ϕ. as for the other definitions, the behaviour and implementation of merge can be verified using the accompanying tool at http://s3db-operator.googlecode.com.   

the numeric indexes of the vectors a and a, are integers between  <dig> and n. however, because numbers are symbols with no upper and lower case, it is easier to represent the resolution of equation  <dig> using the alphabetic indexes instead. the argument for using alphabetic indexes is that their case can distinguish between a dominant and a recessive merged state, therefore allowing a and a to be represented together as a single vector. two illustrative examples - for an operator with three states indexed as {'b','c','d'}, merge = merge =  <dig> and merge = merge =  <dig>  the case of the merged state, 'd' and 'c' in the example, is of no consequence to the operator itself, which will respond only to its position in the ordered state vector,  <dig> and  <dig> respectively. if, in this example, the operator was something like view_query_results() and the index of the ordered states were {'noview','thecountonly','yes'}, the result of the first merging might be returned as 'yes' and of the second as view 'thecountonly'. however, if further operations are to be made on the merged result then the case of the merged state is important and needs to be retained. it is patently easier to return 'd' and 'b', or even 'yes' and 'thecountonly' than to have to specify that the merged i =  <dig> was a recessive outcome, whereas i =  <dig> was dominant, and as a result the state index  <dig> >  <dig> when equation  <dig> is used.

migration
the direction of the relationships between s3db entities  was conveniently defined to be the same as the propagation of operator states from domain to its instantiation . this allows the definition of a boolean transition matrix, equation  <dig>  that can be applied to any instance of one or more of the seven types of s3db entity, e, ordered using their initials as . the numbers between brackets in the transition matrix indicate the logical tests  that individual instances of the seven types of entities can have between each other .   

as described in equation  <dig>  this simplifies the computation of the transition of states between entities as the external product of the corresponding boolean square matrix and the vertical vector of states assigned to each entity. for example, if a state of a s3db:operator is used to describe a user relation with a certain s3db:collection, and this collection happens to both have items and to be the subject of a rule, then this state will be passed to those rules and to those items, using relationships  and , respectively.   

the process by which states are passed from one instance of an entity to another before being merged at the end of each iteration  is designated as state migration and is described in equation  <dig>  the simplest example is the migration of a singular state - if the state of an instance of a s3db:entity, e, is described as a singular value, say 'a', then 'a' will be passed on for the relationships verified in equation  <dig>  however, if the state of the operator, f, is described by more than one value, l> <dig>  then the additional expressivity in state propagation can be achieved, as described by equation  <dig>  that generalization consists of specifying that if a state is singular , then it will be passed as is. if, on the other hand, it is plural, then the first state is used as the effective state of the subject entity and only the remaining states are passed on to the entity that is object of the valid relationship, as described in equation  <dig>  for example, starting with singular migration, if the state of an instance is 'a', and this instance is subject of one or more of the  <dig> relationships , then the object state will merged with the migrated state 'a'. however, if the subject instance has a plural state, say 'abcd', then only 'bcd' will migrate. note that both dominant and recessive cases are considered in the vector f in equation  <dig>    

one last generalization of the migration process was also found to increase expressivity. the procedure described in equation  <dig> was vectorized to allow simultaneous migration of states of multiple s3db operators. this is achieved by defining a second input argument for the migrate procedure which identifies how many operators fj, j =  <dig> ...,m, are having their states migrated simultaneously. since the states of each operator define m-tupples inside the state of n states, this is equivalent to identifying the migrating states of fj as being fj =f. accordingly, equation  <dig> is equal to equation  <dig> when m =  <dig>  that is, when only one operator is being considered.   

the enhanced expressiveness of the representation of multiple operator states described in equation  <dig> is most useful for s3db operators that share the same states. for example, states that identify groups of users could be used as the states of multiple operators such as "view" and "edit", as is the case for the s3db prototype . as can be verified in the tool accompanying this manuscript, the multiple state migration allows for very short descriptions of states that span multiple operators. for example, migrate = 'aaa', which allows for a single assignment that spans several operators. this is achieved without affecting the migration of individual statements - for example migrate = 'abc'- while at the same time allowing for a sweeping assignment of migrated states as in migrate = 'bbb'. note also in equation  <dig> that when a operator state at position i is not specified, it is borrowed from the operator immediately to the left, position i- <dig>  this implies that the order of the operators can be used to simplify assignments that just span a subset of them, as in migrate being 'bcc'. as always, the behavior and implementation of this functionality can be verified using the accompanying tool.

percolate
the third and last function used by the state propagation procedure brings together the merge and migration functions to find the steady state solution of equation  <dig>  that is, when the migration of states, equation  <dig>  has progressed to the point where the effective state of the operator, for each and every s3db entity, no longer changes:   

in the accompanying web tool this resolution is made available for any boolean transition matrix. although for the specific purposes of the s3db prototype, the transition matrix t in equation  <dig> is equal to ts3db in equation  <dig>  there is no reason not to define, and test using the tool, the percolation of s3db operator states more broadly for arbitrary transitions.

discussion
the s3db framework comprises a core model with an embedded markov process propagating user operator states. the two key properties of the resulting construct are the explicit separation between domain and its experimental instantiation, and the accommodation of a very flexible and fine tuned description of the relationship between the users and its contents. most features of this framework were put to use in an open source prototype available at s3db.org. they have also been validated with practical applications developing multiple investigator information management infrastructure such as  <cit> . the potential usages and configurations of the s3db framework described here are nevertheless much broader and can be described as a set of restrictions placed on rdf representations. recalling from the results section, the use of the s3db model consists of declaring all the data elements associated with a given observation as being types of s3db entities. the interoperability with the resulting construct is ideally delivered as a rest web service protocol, which is not covered in this report. for an illustrative implementation see the prototype's documentation for the query language s3ql, to which sparql queries can also be mapped http://sparql.s3db.org.

core model
as described in the box diagram at the bottom of figure  <dig>  the key feature of the core model is the representation of value triple statements  predicated on triple statements that describe the domain instantiated . this design pattern was specifically devised to allow domain experts to incubate the description of their own domain of expertise  <cit> . as proposed in that report, and verified here, that pattern establishes a specific relationship between the objects and subjects between the two triples, which allows the autonomous editing of the domain description and of its value instantiation.

the three core s3db entities peripheral to the nuclear square of collections, items, statements and rules  create additional management modeling opportunities without which the core model would be little more than a flexible data format. the first of these three entities is the deployment, which was conceived as a pointer to the url of the s3db web service. as all model entities, its usage is not conditioned, nor does it condition, the location of the other entities  linked to it. this design seeks to support the distribution of the information management infrastructure. this can be achieved for example, by dereferencing. the deployment url address can point to a central registry that resolves it to the actual web service. in that case, the content hosted can be distributed between multiple machines for purposes of, say, load balancing, or in general to enable intermediate tools that may aid in content discovery. another example is the compartmentalization of content by hosting collections and rules in multiple machines, distinct from the hosting projects . specifically, because the project uri is resolved by the hosting deployment, the relationship s3db:dp  can be used to associate deployments with arbitrary projects, which do not have to be in the same machine that hosts the subject deployment. furthermore, by defining uri's for individual entities as url calls to the deployment this discussion is extensible to all other relationships in table  <dig>  this can also be verified by following this link  <cit>  to a s3db project , with content retrieved from the cancer genome atlas . note the resolved uri's at the lower left corner of the web application.

the last of the four peripheral entities is the user which is rdf:subject of a class of operators with states that were conceived for fine grained definition of relationship between users and the corresponding content. as discussed above for deployments, collections and rules, the uri of a user can also be resolved to any deployment by s3db:du . this implies that authentication and user management are architecturally decoupled. the users have also specified a relationship between themselves, s3db:uu , conceived to support a flexible definition of grouping. when two users are linked by s3db:uu, the operator states can migrate between them to extend or restrict the relationships with the corresponding content. this enables the use of s3db:uu to design flexible user management systems. for example, if one user entity is used as a hub to which multiple uu relationships converge, this is akin to the conventional definition of groups. the reverse would be closer to the conventional definition of roles. a rich variety of groupings can be envisioned between these two conventional extremes. note that contrary to the operator percolation features discussed in the two previous paragraphs, between-user percolation is presently not supported, and therefore was not tested, by the reference s3db prototype. two ongoing projects in particular, http://aguia.googlecode.com and http://cnviewer.googlecode.com, provide s3db-based, javascript-coded, platforms where those features are being explored, respectively, in the context of clinical trial management and of dna copy number variation in tumor samples.

s3db operators
the idea of s3db operators as a class of functions with states that describe the relationship between a user and the entities of a data model was conceived independently from its specific application to the s3db core model. accordingly, this second component of the core model is applicable to any rdf schema provided that the direction of state propagation is defined . the generic nature of the s3db user operators and of the markov process that propagates its states is apparent in equations  <dig> - <dig>  similarly, although the s3db prototype provides an illustrative example using three s3db operators , each with the same three ordered states , the range of applications is open ended and is not necessarily associated with permission management. for example, an operator could be defined to represent priority in the retrieval of query results, which could differ between users. by using the states of this operator to define the relationship between a user and the model entities one could configure, for that user, that clinical records with a specific outcome would be, for instance, graphically highlighted. retrieval priorities, or, for that matter, the choice of graphic interface features, could therefore be personalized by associating them to operator states pointed to the appropriate semantic content.

the graphic presentation beyond access control could also include workflow components. for example, it could be used by a quantitatively minded researcher to have statistics tools automatically applied to the construction of a specialized interface. more interestingly, the concept of user could be used more broadly as that of usage. quite literally, data analysis procedures could be configured as users. by treating usages and analytical workflows as users, the corresponding procedure would be automatically executed for content with specified semantics. the same line of discussion can also lead to the observation that there is nothing in the definition of an s3db:operator that restricts its use to describe the relationship between a user and the entities of the s3db core model. that restriction is described by the construction of the transition matrix which for this core model happens to be the one defined in equation  <dig>  therefore, a different core model would just have to identify a different transition matrix of logical tests and the same state propagation mechanism defined in equations  <dig> - <dig> would be automatically applicable. in summary, the key feature of the user relation propagation component of the s3db core model is the articulation of the three functions, merge, migrate and percolate, applied to a set of states that can take a dominant or recessive form. that articulation is defined by those equations and is also illustrated by the accompanying browser-based application at http://s3db-operator.googlecode.com.

CONCLUSIONS
the dyadic predicated nature of resource description framework  has emerged as the shared representation of a variety of semantic web formalisms and technologies. in this report we describe a core model that mediates the generation and management of the rdf triples by and for domain experts. this model abstraction is the result of five years of bioinformatics infrastructure development in biomedical and molecular epidemiological contexts. the underlying approach/hypothesis is that by explicitly distinguishing description of the domain from its instantiation with observational data, one allows domain experts to freely evolve the former without compromising the actuality of the latter. the other, complementary, critical feature of the s3db core model is the markov process that propagates the relationship between users and the entities of a core model. this ability to propagate operators from the description of the domain to its instantiation has already found an immediate application in the management of access permissions. finally, at the very center of the s3db abstraction is a two tiered modeling pattern where instances of a class describe the relationship between two classes and, in turn, instances of the resulting triple, which are also triples, host the observed values. this modeling pattern underlies the s3db schema but may be of more general applicability.

authors' contributions
jsa identified the s3db core model and drafted the manuscript. hfd developed the php prototype. wm uncovered and analyzed the model's logical patterns. all authors read and approved the final manuscript.

