BACKGROUND
the impact of the genome sequencing projects will emerge from elucidating protein function. diseases result from protein dysfunction and are managed with drugs that alter protein function. although protein function has been inferred from sequence similarities, it has not been directly studied in most cases. there is a substantial need to develop high throughput  tools that will accelerate the study of the thousands of proteins not yet examined, a field referred to as functional proteomics.

protein function studies start by producing proteins using cloned copies of the genes that encode them. recognition of this has led to the production of large collections of cloned genes configured in a protein expression-ready format . to ensure accurate conclusions, the coding sequences must first be validated, a process that includes comparing the clones' sequences to the expected sequences at the nucleotide and amino acid levels. yet, despite the well-recognized importance of sequence verifying cloned genes, it has been performed on less than a handful of protein-expression clone collections currently in existence  <cit> .

assembling clones is now well-established, relatively inexpensive and automated. however, several key steps – oligonucleotide synthesis, reverse transcription, and pcr – are unavoidably mutagenic, emphasizing the importance of sequence verification. the ultimate goal of validation is to determine if a given clone is "acceptable" for use in biological experiments. acceptance criteria may vary based on the experiment but typically a clone is rejected when its coding sequence contains one or more mutations that might adversely affect protein activity.

in contrast to building the clones, the process of verifying their sequences is still handled manually. whereas excellent software has been written to manage and automate the de novo sequencing of dna, and although elements of that software can be employed , there is no software that manages the process of validating clone sequences, particularly for large projects. there are a number of attributes unique to the clone validation process that distinguish it from denovo sequencing and require the development of novel software. first, whereas the goal of de novo sequencing is to make the best sequence prediction for an unknown, the goal of validation is to determine if an unknown matches a pre-defined reference sequence. second, clone validation software should provide an automated mechanism for sorting clones into either the accepted or rejected categories. third, the evaluation process must also consider the polypeptide sequence because, in general, the amino acid sequence dominates in determining the clone's overall value. fourth, in genomic sequencing, discrepancies among reads arise primarily due to sequencing errors, whereas with clone validation, discrepancies arise not only from sequencing errors but also because of polymorphisms in the source material, pcr errors, oligonucleotide synthesis errors, reverse transcription errors and even mistakes or ambiguity in the reference sequence. finally, the availability of a reference sequence affects the strategies that software would employ in managing the projects, for example, favoring primer walk strategies over transposon and shotgun alternatives for full length sequencing.

software for clone validation should thus be able to: 1) determine the sequence of each clone accurately; 2) identify if and where that sequence varies from the intended target sequence; 3) evaluate and annotate the polypeptide consequences of any variations; and 4) determine if these observed differences are acceptable based on user defined criteria.

thus far, software developed to aid in dna sequencing has focused on the de novo sequence determination of dna fragments, such as in genomic sequencing projects. some basic sequencing programs handle pre-processing of sequence trace files, base calling  <cit> , quality clipping, vector trimming and removal  <cit> , contig assembly  and primer design  <cit> . these programs are often used as integral parts of sophisticated software packages  which include additional functionality like clustering, gene annotation and finishing tools . several packages have been developed to manage expressed sequence tag  projects . recently comprehensive modular software packages have been developed that provide a graphical user interface  for biologists with minimum software experience to define their own processing pipelines for sequencing, managing sequencing results and performing basic analyses like sequence alignment and computing quality statistics . these packages typically rely upon the basic bioinformatics programs mentioned above to perform unit tasks and relational databases as backend storage for processed data.

software tools used in "re-sequencing" projects  bear some similarity to the concept of clone validation . however none of these programs consider polypeptide consequences of differences nor do they provide a mechanism for applying acceptance criteria within the workflow. moreover, snp discovery software has been designed to operate with dense sequence coverage of specific target regions whereas clone validation software must deal with the minimum possible coverage achieved for many different genes .

in this paper, we present the automated clone evaluation  system, which is an automated software application that simultaneously manages the process of sequence validation for thousands of plasmid clones. ace has been used successfully to evaluate more than  <dig>  clones at the harvard institute of proteomics, providing full automation for all processes and assisting in building sequence verified clone collections useful for ht proteomic studies.

implementation
conceptual approach
the two central requirements for automated clone validation software are the abilities:  to identify discrepancies between the clones' determined sequence and the expected sequence at the nucleotide and polypeptide level; and  to automatically sort clones into acceptability categories  based on user defined criteria. a discrepancy is any difference between a clone and its expected sequence and may arise because of cloning artifacts, mistakes in determining the clone's sequence, natural occurring polymorphisms or errors in the reference sequence. moreover, discrepancies will have varying effects on the predicted polypeptide, from silent  changes to truncations. both the causes and consequences of discrepancies are important to end users. thus, for the software to make informed decisions regarding the acceptability of clones, it must also collect and use this ancillary information.

the strategy employed by ace is to describe each clone as a list of multidimensional discrepancy objects. ace populates multiple properties of discrepancy objects including: discrepancy type , sequence confidence level , and position on the reference sequence. additionally, discrepancy objects fully describe the predicted consequences on both the nucleotide and polypeptide levels .

to automatically sort the clones, ace compares the list of discrepancy objects for each clone against the user's acceptance/rejection criteria for discrepancies of various types. the existence of any discrepancy is always deleterious; but users consider some more deleterious than others. for example, some users require an exact match to their expected sequence  whereas others would allow one amino acid change and still others might also allow that change only if it represents a natural polymorphism. it is also important to consider the base confidence of discrepancy objects because the most common cause of low confidence discrepancies is base calling errors .

the strategy of describing clones as lists of discrepancy objects separates the clone annotation process from the clone sorting process. annotation defines the discrepancies, which are inherent and objective features of the clones. the sorting process compares lists of discrepancies according to a subjective value system defined by the user. thus, the same set of clones can be sorted based on different user criteria for different applications. finding the "best" among several clones for the same gene is reduced to comparing their discrepancy lists. moreover, it allows these comparisons to be made at the level of polypeptide consequences, which is most relevant for the functional proteomics purposes.

application overview
ace is a comprehensive, multi-platform and multi-user, web-based sequence verification software system. the key novel aspects of ace include:  finding and annotating discrepancies between clone and reference sequences, including clones with incomplete sequence assemblies;  describing each clone sequence as a list of multidimensional discrepancy objects;  implementing an automated decision making process that compares each clone's discrepancy object list to user-defined clone acceptance criteria;  embedding in each discrepancy object information about the polypeptide consequences of the discrepancy;  automating selection of the best isolate when multiple isolates for a target sequence are available.

ace provides an integrated environment that relieves the user from routine process management tasks, such as bookkeeping of all project- and clone-related information, re-entering of parameters and criteria, and history tracking. it automates all steps of sequence verification, including primer design, sequence contig assembly, gap mapping, demarcating low confidence regions in sequence coverage and identifying polymorphisms.

whenever possible, ace implements existing well-established algorithms and utilizes third-party programs via custom wrappers that adapt them to the specific tasks. for example, we modified a phred/phrap  <cit>  script to allow users to remove low-quality reads from an assembly and/or to trim tails of the reads according to user defined criteria. ncbi blast  <cit>  and needle  <cit>  are used, respectively, for local and global sequence alignments. primer <dig>  <cit>  is launched iteratively using the clone's reference sequence as a guide to enable primer design for automated primer walking.

ace is structured for maximum flexibility to support different approaches to clone validation and sequencing management. users do not need to follow a single path in clone sequence verification, but rather can invoke each module individually. a typical ace-based workflow used in our laboratory is shown in figure  <dig>  a project begins with end read sequencing of one or more clonal isolates per target. end reads are acquired, assigned to their corresponding clone , and then processed by the assembler to determine if end reads alone are sufficient to yield a complete contig assembly . whether or not the assembly yielded a single contig covering the full-length cds, clone contig are analyzed to detect differences or "discrepancies" compared with the reference/target sequence . ace can compare any discrepancies with one or more sequence database, such as genbank, to determine if they correspond to naturally occurring polymorphisms . during the final decision process, users can optionally configure the software to avoid penalizing clones for discrepancies that represent polymorphisms . if more than one isolate exists for a given clone, an optional module  can rank isolates based on user-defined preferences specified in the form of penalties associated with different types of discrepancies.

clones that failed to assemble into a single contig covering the cds can be scanned to find the remaining gaps in sequence coverage . in addition, sequence regions of low confidence can be analyzed to demarcate their boundaries . subsequently, clones with low confidence regions or gaps in sequence coverage can be processed to define appropriate internal sequencing primers to cover those regions . at any stage during the clone verification process, a set of clones can be processed by the decision tool in order to determine how far each clone has progressed in the analysis pipeline and its acceptance/rejection status.

representative output files and user interfaces for each of the key modules in this application can be found in additional files .

software architecture
in its implementation ace comprises core classes, wrapper classes and processing modules. core classes represent subject domain objects, such as clone or plate, as well as process control objects, such as project or analysis spec . a relational database is used as a repository of all persistent data except for trace files. trace files are stored on a server hard drive .

core classes serve as a layer of abstraction to this database by encapsulating sql queries for retrieving their data and for updating the database as processing proceeds.

wrapper classes encapsulate third party programs and are responsible for adjusting their parameters and parsing results into objects that can be processed by processing modules.

processing modules contain user interfaces and encapsulate processing algorithms. they are responsible for interpreting user requests, creation, modification or deletion of core objects and providing feedback to the user. to prevent accidental loss or corruption of data, several levels of access rights are defined in ace and some functionality is available only to the users with higher level of privileges. typically, processing modules operate on user-specified clones or collections of clones. although ace does not impose a strict workflow on the user, some restrictions on the order of operations still apply. most processing modules check whether necessary conditions are met for a selected group of clones. for example, neither decision tool nor isolate ranker can process the clones if their available sequence coverage was not analyzed first by discrepancy finder.

the first group of processing modules is responsible for data entry and direct data management by the user. the creation of a new project requires entering the project description and defining the cloning strategy, which includes a description of the cloning vector, linkers, and the intended start and/or stop codons. multiple projects can refer to the same cloning strategy. data load modules are responsible for loading:  clone related information using xml files ;  trace files ;  clone sequences that may be provided in fasta format.

the second group of modules is responsible for the analysis of clone data, annotating the clone, assisting the user with any required additional sequencing and finishing the clone if necessary. these modules constitute the core of ace and are described in more detail in the next section.

the third group of modules generates ace views and reports. ace provides user feedback in the form of interactive views, reports and email notifications. for processes that involve manageable screen content or that require real-time management, interactive views give user-friendly access to currently available information. these include: plate viewer  that gives accesses to complete information for specific plates and the designed primers viewer to select which automatically generated primers to use , etc. for processes involving thousands of clones or whenever a documented result is requested, each analysis tool or report can be launched in an asynchronous way, so that the user is notified at the end of the operation by email with attached report. some reports represent direct database queries, whereas others, like the decision tool, trace file quality assessment, and mismatch report include complex data processing.

a detailed structure and the relationships among the most important core classes are presented in figure  <dig>  for simplicity, classes that support process control and primer design are omitted from the figure.

in ace we define a class hierarchy for the description of cdna sequences. the basesequence class is a nucleotide string. refsequence is a description of the target sequence to be verified; it contains a set of literature identifiers associated with the target sequence, such as gi, genbank accession number, gene name. reference sequences frequently represent genbank records, where the record sequence is longer than the clone's insert sequence. consequently, refsequence in ace also contains the coordinates of the target sequence on the nucleotide sequence. the scoredsequence class is derived from basesequence by adding the confidence score  to each nucleotide. analysedsequence is derived from the scoredsequence and adds the collection of discrepancydescription objects, the collection of low confidence regions  as well as the mapping of analysedsequence on the range of the target sequence. analysedsequence is the parent class for clonesequence that represents full clone coverage and specifies how the sequence was obtained.

a discrepancy in ace refers to any mismatch between scoredsequence and corresponding target sequence. the discrepancy class aggregates nucleotide and polypeptide level descriptions of the mismatch. in general, discrepancy objects that describe nucleotide substitutions correspond to one codon and, hence, can contain up to three nucleotidediscrepancy objects and one proteindiscrepancy object. table  <dig> describes the correspondence between nucleotide and protein discrepancies. in some cases , proteindiscrepancy is absent and discrepancy consists of a single nucleotidediscrepancy. discrepancy objects are created by the discrepancy finder and are discussed in more detail in the corresponding section.

the clone object aggregates all information about the insert subject to validation. data in the clone object are accumulated in the course of processing. the class clone contains:  physical location data ;  reference sequence information ;  one or two endread objects;  contig collection;  clone sequence.

plate objects represent physical plates of predefined format .plate contains multiple sample objects, where a sample is a physical substance sitting in a particular well. the sample can be a control or a clone. it is important to underscore that a single clone object corresponds to a single sample. if the workflow involves creation of several candidate isolates for the same target gene, each isolate is represented as a separate clone object. the isolategroup object corresponds to a set of isolates with a common target.

core modules
 <dig>  clone analysis
during a large scale sequence validation project, the analysis of clones is an ongoing and iterative process. each cycle includes acceptance of good clones, the elimination of failed clones and identification of gaps in the remaining, potentially acceptable clones.

 <dig>  discrepancy finder
this module detects mismatches between the clone contig and reference sequence, creating a list of discrepancy objects for each clone.

discrepancy finder first builds a global alignment between the clone's contig and its reference nucleotide sequence using the needleman wunsch algorithm  <cit> . mismatches are identified via base-by-base comparison and a nucleotidediscrepancy object is created for each, except that contiguous mismatches  are grouped together to form a single nucleotidediscrepancy object. when up to three adjacent nucleotidediscrepancies belong to one codon inside the orf sequence and none of them is an ambiguous substitution, we create a corresponding proteindiscrepancy object .

ace assigns low-confidence status to a discrepancy if the phred confidence score  <cit>  of at least one base used to define the discrepancy or one out of four bases on either side of the discrepancy is below the user-defined threshold. all discrepancy information is stored in the ace database.

 <dig>  polymorphism finder
this optional module determines if discrepancies are attributable to natural sequence variations of the gene, which is particularly relevant for human genes cloned using different tissue samples from those used to make the reference sequences. the process requires two steps:  a relatively short sequence segment comprising the discrepancy and its flanking sequence is compared using ncbi blast  <cit>  to data from all user-selected genbank databases to find an identical match;  each 100% hit is verified by comparing the entire clone target sequence with the hit sequence to ask if it came from the same gene using pairwise blast  <cit> . a match provides good evidence that the observed sequence variation occurs naturally, but the absence of a match is inconclusive  <cit> .

the polymorphism finder in ace will analyze all discrepancies detected for a set of clones and store the ncbi gi number for each verified hit as part of the discrepancy description. as this operation requires sending numerous blast queries against large databases, its implementation requires a local copy of the appropriate genbank database installed on a dedicated high-performance computer or cluster.

 <dig>  decision tool
this tool sorts clones into a finished-and-accepted group, a rejected group, and a set of groups slated for further processing. fundamentally, the decision tool accomplishes this by comparing each clone's discrepancies list to a user-defined list of acceptance criteria, which define how many discrepancies of each type are permitted. users can set thresholds to:  automatically accept clones that meet some high level criteria and  automatically reject clones that fail to meet some minimal criteria. moreover, users can set independent thresholds for high and low confidence discrepancies .

any clones not accepted nor rejected remain incomplete until additional processing has been performed. the flowchart in figure  <dig> illustrates the logic path for assigning these pending clones to different action groups. the goal here is to bin clones into groups that require the same next step in processing and report them in a single flat file . this tab-delimited output can be used as input for the next processing step.

users may wish to apply alternate acceptance criteria to the same set of clones for different experimental purposes. user specifications for acceptance criteria are stored as named sets in the software and can be invoked and applied to any collection of clones.

 <dig>  isolate ranker
some cloning workflows produce several isolates for the same gene with the expectation that at least one of these isolates will be of acceptable quality. isolate ranker selects the best isolate to carry forward by comparing isolates based on end reads, or  partial or full sequencing data, and applying user defined penalties for different discrepancy types. for each combination < discrepancy type, confidence > , the user specifies two values: maximum permitted number of such discrepancies and the penalty per discrepancy combination . isolates that exceed the maximum permitted number of mutations of at least one type are rejected. for each remaining isolate, the overall score is computed by normalizing the sum of the penalties over the number of bases covered. these scores determine the rank order among the surviving isolates of the same gene and are displayed as a color coded virtual plate map  so that users can quickly identify the best candidate clones.

 <dig>  clone sequencing support
the automation of clone sequence validation requires tools that manage the sequencing process itself. unlike genomic sequencing projects that are often tasked with combining all available sequence reads into a single large contig, clone validation can be likened to thousands of independent micro-sequencing projects that must be maintained and managed separately.

 <dig>  trace file distributor
clone validation projects often include sequence reads from different clones representing similar genes . it is essential that reads from closely related clones do not end up in one another's analysis. the ace package creates a hierarchical directory structure and stores all files related to a single clone in a directory specific to that clone, as required by the phred/phrap package  <cit>  .

the trace file distributor parses identifier information encoded in the filename for each trace file, embedded there by the sequencing facility, that indicates which clone it belongs to . however, because different facilities utilize different naming conventions, the trace file distributor stores each facility's format and uses it to convert the filename into a format that ace can use to process these files automatically. this approach has worked efficiently for files from five different sequencing centers.

 <dig>  end read processor
end reads are treated slightly differently from internal reads because some users employ them to select the best candidate from multiple isolates for a clone. we have found that poor quality end reads will lead to the rejection of about 25–35% of good clones based on discrepancies that turn out to be sequencing errors. to mitigate this the end read processor assesses read quality and disregards any end reads that do not meet minimum quality criteria, whereas end reads of sufficient quality are submitted into the database. to satisfy minimum quality criteria, a read must be longer than the user-defined minimum length  and the average confidence score for all non-ambiguous bases between the first and last base  must be above the user-defined minimum confidence score . the trace files for the rejected end reads can be optionally added later as internal reads for inclusion in the clone sequence assembly.

 <dig>  assembly wrapper
this tool automates contig assembly by calling the phred/phrap  <cit>  package for every clone on a user-submitted list. contig assembly for multiple clones can be a significant bottleneck in ht projects. when simultaneously assembling thousands of clones, experience has demonstrated that some fraction of clones will fail to assemble despite the availability of adequate sequence coverage. however, by adjusting a variety of settings involving trimming and quality requirements, many of them can be encouraged to assemble.

vector trimming during contig assembly is performed using cross_match  <cit> . in some cases, trimming is essential for contig assembly, whereas in others, it interferes with it. for example, a high degree of similarity between gene sequence and sequences in the vector library causes phred/phrap to mask valid  sequence when vector trimming is applied blindly. this problem can be partially alleviated by editing vector sequences down to about  <dig> bp of insert-flanking sequence and removing irrelevant vector sequences from the vector library. occasionally it is necessary to turn off vector trimming altogether to get an assembly for particular clones. we also found that aggressive quality- and/or read length-based trimming helps to improve contig assembly. when prompted, ace trims all reads prior to assembly by removing bases prior to base  <dig> and after base  <dig> . reads with a low average confidence score or below a minimum length can also be excluded from the assembly.

 <dig> clone finishing
for practical reasons, sequence coverage for validating clones is usually much lower than that used for de novo sequencing projects . thus, failed reads lead readily to gaps in coverage or regions of low sequence confidence. as there are often hundreds to thousands of clones to track, software is needed to automate the finishing process. autofinish <cit>  is excellent software for finishing in de novo sequencing projects. however, this program is not the optimal tool for the verification workflow, because it assumes dense coverage produced by shotgun sequencing and it does not exploit the existing reference sequence.

 <dig>  gap mapper
this module uses a strategy that exploits the existence of the reference sequence to demarcate gaps in sequence coverage that arise from short or failed reads . the module uses the assembler to align clone trace files with the reference sequence, which is included as a "pseudo-read" with the same preset confidence score used for every base in its sequence . this ensures complete assembly without forcing the contig to be identical to the target sequence. the assembler output is parsed to determine the alignment of each sequence read relative to the reference sequence. using this positional information, a two-dimensional matrix is created wherein each position is described by base and confidence score . at each position along the matrix , a consensus base is determined for the clone by assessing all of the bases at that position and their confidence scores using a naïve bayesian calculation  <cit> . the reference sequence and assembled contig are disregarded at this step, so the computed confidence scores reflect the actual clone sequence. the resulting contigs are optionally trimmed at both ends to remove bases with low confidence scores . tight trimming ensures that the new coverage will include the junction regions. once the contigs are assembled, trimmed and mapped to the reference sequence, the gaps are defined as stretches of reference sequence not covered by contigs. gap and contig information is stored in the database and used to assess clone status and quality, and can be passed to primer designer to design primers for clone finishing.

 <dig>  low confidence regions finder
not surprisingly, discrepancies most frequently occur where sequence confidence is low. the low confidence regions finder was designed to identify low confidence regions  in contigs by applying a 'sliding window' algorithm. the user defines the width of the window , cut-off phred score for low-confidence bases  and maximum allowable number of low-confidence bases . using the default values, an lcr is defined as a region in which a window of  <dig> consecutive bases contains at least three low-confidence bases. lcrs located close  to each other are joined . like gaps, lcrs can be processed by the primer designer to design a primer walk to obtain better coverage for these regions.

 <dig>  primer designer
this set of modules exploits the availability of the reference sequence to allow users to:  design gene-specific sequencing primers;  select specific primer for placement on a vendor order;  view all primers  and their design specifications using a convenient user interface, and  track and manage primer plates and individual primers. in practice, the primer designer tool is among the most utilized tools in this application. it can be used a priori to design gene-specific primers for a complete primer walk or, more efficiently, to design only those primers needed to complete coverage  or to re-sequence regions of low confidence defined by gap mapper or low confidence regions finder. the module collects essential information including:  type of coverage desired ;  primer sequence related parameters ;  sequence processing parameters ; and which sequences to cover . after removing any sequence covered by universal primers , the module breaks the remaining sequence into fragments which are provided to primer <dig>  <cit>  for primer prediction. fragment size takes into account the expected distance between the primer and reliable sequence, expected high quality read length, and the primer <dig> window size provided as part of user specification for primer designer. the primer design module then collates the output of primer <dig> 

RESULTS
in order to test the performance of ace, we compared the processing times and outcomes for an experienced operator using ace with a researcher experienced at manual analysis using commercial software to analyze one plate of clones for  <dig> unique genes ranging in length from  <dig> to  <dig> bp flanked by common linkers of ~ <dig> bp. we did not include project setup time for either study arm so that the focus was on processing time for the traces. to simplify the comparison, we designed and ordered internal sequencing primers for all clones with an insert length above  <dig> bp at the start of the project based on the reference sequences and a uniform read spacing . although commonly used for manual analysis projects, pre-ordering all of the internal primers is not ordinarily needed for ace projects. instead, internal primers are typically ordered as needed based on the gaps identified which significantly reduces the need for additional primers and reads.

the acceptance criteria that were used are shown in figure  <dig>  basically clones were allowed to have no more than a single amino acid change. these final acceptance criteria are indicated by the far left column  and were used by both analysis methods. the other  <dig> columns were used by the software to automatically reject unacceptable clones or to indicate clones that need further analysis.

manual analysis
the manual analysis was performed by an experienced researcher, who analyzed all clones using sequencher™  software and tracked the results in excel® . each clone was individually analyzed and annotated according to the observations made at the time. any internal or repeat reads for a clone were uploaded into the previously generated file and the analysis was repeated where necessary. insufficient coverage and only high confidence discrepancies were annotated by indicating the position based on the entire target sequence  where the first base of the 5' linker was listed as '1'. the researcher passed clones based on the criteria indicated above ; everything else was failed or listed as pending.

in summary, the researcher accepted  <dig> clones;  <dig> clone was provisionally accepted after confirming that an in-frame deletion could be found in other genbank entries by blast analysis .  <dig> clones were rejected . there were  <dig> clones that did not have adequate quality sequence coverage to allow a decision about the insert. one additional clone had a region requiring additional sequencing due to ambiguous nucleotides that could not be resolved. to resolve these pending clones in the manual workflow, the researcher will need to manually design and order additional sequencing primers, as well as execute the necessary re-array to repeat the sequencing.

#: special handling includes the manual analysis of  <dig> clones in the ace workflow

automated ace analysis
the same plate of clones analyzed manually was used for the automated analysis following the general workflow outlined in figure  <dig>  after sequence traces were loaded and those of acceptable quality were distributed into their appropriate clone directories, we assembled the end reads after trimming them based on the vector sequences, on the read length , and on the base calling confidence . we then performed a discrepancy analysis and used the isolate ranking feature to identify clones that did not match their reference. based on the report for this initial processing, we culled all of the accepted clones and repeated the procedures for the remaining clones without vector or quality trimming. altering the trimming parameters often captures clones that were missed in a first assembly pass. we used the same rules for automatic acceptance/rejection listed in figure  <dig> .

in an ordinary project, it would be necessary at this point to design internal sequencing primers to cover any gaps or lcrs; however, as mentioned above, in this project all of the internal reads had been designed and pre-ordered based upon the reference sequence. nevertheless, to capture the actual time needed to execute the project, we went through the exercise of identifying regions requiring additional coverage  for clones that were either not fully covered with end reads due to the insert length  or listed as having 'persistent low confidence discrepancies'. we then designed internal primers theoretically needed to complete this coverage , though these were not actually ordered from a vendor. this approach required less than one third of the number of primers and reads than were needed for the upfront design approach.

after uploading the pre-ordered internal reads and any repeated end reads into ace, we performed a "first pass" analysis. this lead to  <dig> accepted clones,  <dig> rejected clones, and  <dig> that required further review . based on the report generated by ace,  <dig> of the accepted clones had at least one discrepancy, but within the range allowed. of the rejected clones,  <dig> exhibited no match to the target sequences and  <dig> had high quality discrepancies leading to frame shift or truncation mutations. the  <dig> clones slated for further review were all handled in sequencher and resulted in acceptance of  <dig> additional clones. the remaining  <dig> clones had either regions that were not covered by sufficiently good traces , or no traces of quality were present in the first place .

discussion
when the outcome of the two analyses is compared, only a small number of clones show differences. the same clones were accepted by both approaches. one clone harboring an in-frame deletion was accepted because a blast analysis against genbank indicated that it represents a documented variant for that gene. all clones manually rejected were also rejected in ace; however, ace rejected  <dig> additional clones that were not rejected in the manual analysis. in ace these  <dig> clones were reported as being 'no match' with their target sequence, and upon automatic blast analysis in ace were marked as showing only significant homology to the cloning vector. a review by a senior researcher confirmed the call made by ace.

the somewhat higher rejection rate by the ace software is fairly typical, as the parameters used for this study tuned the software to be conservative about accepting clones. the parameters demanded high sequence confidence across the clone and required resolution of all low confidence discrepancies. this has the effect that projects done with the automated workflow will end up with a subset of clones that require manual review. obviously, by defining different parameter sets, users can employ less stringent criteria and obtain a different outcome. one of the advantages of using ace is that every analysis has a well-defined and well-documented set of acceptance criteria attached to it. there are no ambiguities about why a clone was included or excluded.

the automated analysis took close to half the amount of time of the manual approach. this was surprising because such a large difference might not be expected for a small project . as projects grow in size, the relative time savings grows considerably. the time it takes to analyze clones manually increases linearly with more clones. there is no advantage to increasing the scale; ten plates of clones take about  <dig> times longer than one plate of clones. however, this is not true for the automated analysis. although there are some exceptions , most of the ace steps demonstrate no significant difference in the user operational time whether the operation is performed on one plate or ten. as the projects increase in size, the amount of researcher time spent per clone drops precipitously. because the operational steps are straightforward, most large automated projects include a second round of automated analysis before referring pending clones for manual analysis, resulting in about 90% of the clones processed automatically. because there were only  <dig> clones requiring further processing, all of them were done manually in this project.

there are some additional advantages to the automated analysis that might not be evident from this comparison. first, the automated analysis captures information on all clones including detailed annotation about all discrepancies for all available sequence. the manual review process focuses only on finding discrepancies that will cause a clone to be rejected. once these are discovered, the clone is dropped and the researcher moves onto the next clone. second, ace produces a detailed report about all discrepancies and their polypeptide effects in a format that is easy to upload into a database for further analysis. this report is useful for understanding where mutations occur and improving cloning conditions. in cases of manual analysis one runs either the risk of human miss-annotations or having to export all assembled, finished sequences to cross-check them with a text comparison tool to create a standardized annotation for any discrepancy. finally, ace automatically manages the file transfers, the data tracking and the management of all internal sequencing primers. this includes automatically designing any necessary clone re-arrays for additional sequencing steps. in large scale projects, these processes are both very tedious and error prone when executed manually.

with the increasing construction of large cdna and orf clone sets slated for use in protein-based experiments comes the need to fully sequence validate the clones. sequencher <cit>  and the staden package <cit>  are widely used software tools for sequence assembly, editing and orf analysis. sequencher is a commercial desktop application which runs only on windows and macintosh os. it was not designed to handle high-throughput processing for thousand clones and cannot be automated for this purpose. the staden package has been developed to help automate the task of deriving a consensus sequence for genome sequencing efforts  <cit> , but cannot be used to automate the process of validating plasmid clones. this is due, largely, to the fact that sequence derivation and sequence validation are two fundamentally different goals. whereas de novo sequencing software focuses on producing the best consensus sequence, validation software must first determine a consensus sequence for each clone and then compare it to its appropriate existing reference sequence to determine whether the clone meets the user's standards.

to accomplish this goal, a novel strategy was employed in which each clone is described as a list of discrepancy objects. essentially, this may be considered as alternative language to describe relationships between clone sequences and their reference standards.

in this strategy, clone validation is divided into the interplay between two separate processes: annotation and sorting. annotation is an objective process that merely documents which discrepancies exist and what are their properties . in contrast, the sorting process is by its nature a subjective one. in this setting, each discrepancy becomes a liability for the clone that may be used to determine whether the clone should be rejected. the penalty appropriate for each type of discrepancy depends upon the intended experimental application. indeed, even which attributes dominate the decision making process may vary – for some applications it is most important to achieve precise polypeptide matches whereas for others the emphasis is on clone sequences that occur naturally. thus, the same objective list of discrepancies can be evaluated using different subjective scoring criteria.

the ace software application was designed to meet the challenges of high-throughput clone verification projects. this software has dramatically reduced the amount of time and labor required to evaluate clone sequences, enabling many weeks worth of manual validation to be completed in a few hours. moreover, the results are stored in a database that allows users to reassess the same set of clones based on different acceptance criteria and create detailed reports on accepted and rejected clones. for institutional reasons, we chose oracle as a backend database for this project, though it could be adapted to other sql-based databases. nevertheless, given the number of clones, sequence traces, discrepancies and contigs that are typically tracked in these projects, some form of relational database is required. this is generally not a problem as most groups engaged in the validation of a large set of clones are likely to have database capabilities already.

an advantage of using automated tools is significantly reducing the number of missed sequence discrepancies, which can be overlooked by researchers during manual evaluation. however, the cost of this stringency is that the software will maintain some clones as "needs further review"  that would otherwise get accepted by manual analysis, because human eyes can sometimes resolve errors made by the automated base caller. it is also true that for some sequence reads, manual analysis can resolve subtle issues  that are considered discrepancies by the automated tool. although this problem can be mitigated to some extent by read trimming and by using confidence scores to indicate which sequences to trust, it is clear that there is still a role for manual analysis for about 10% of a typical large scale clone analysis project. fortunately, these manual checks can be facilitated by ace, which informs the user of which specific discrepancies and/or regions to check in order to expedite the manual review.

to accommodate the many possible workflows used in different projects, the ace package was designed as a set of modules that can be invoked individually. for example, if full length sequences are obtained elsewhere, users can skip the sequencing workflow and proceed directly to contig analysis. the assembled sequences can be submitted as text files  and compared to the expected sequences to generate a discrepancy list and sorted for acceptance .

CONCLUSIONS
the ace software application was designed to meet the challenges of high-throughput clone verification projects. it uses a novel strategy to describe each clone as a list of multidimensional discrepancy objects that can be used to automatically determine the acceptability of the clone based on user defined acceptance criteria. its major advantages include reducing the number of sequencing reads needed to achieve adequate coverage for making decisions on clones, reducing the need for manual analysis of numerous clone sequences, reducing the process time required to complete a project and significantly reducing human error when annotating discrepancies on nucleotide and protein level. ace has being used by the harvard institute of proteomics for finishing and validating of over  <dig>  clones with orfs ranging in size from  <dig> to  <dig>  bp. finished and validated clones are available to the community at plasmid information database  <cit> .

availability and requirements
project name: ace – automated clone evaluation

project home page: 

project help and installation instructions are supplied as an additional file .

operating system: linux: redhat ; windows: microsoft windows server 2003

programming language: java, jsp, javascript

other requirements: oracle 8i or 10g, sun's j2se version  <dig> .1_ <dig> or above, tomcat  <dig> . <dig>  ncbi blast  <dig> . <dig>  emboss  <dig> . <dig> , primer <dig> and phred/phrap package, cygwin for installations on windows server.

any restrictions to use by non-academics: all the analysis tools are freely available for academics.

validated clone web page: 

authors' contributions
et was the lead architect of the system, designed database schema and implemented application. ar was the lead alpha tester of this software who provided critical input on its accuracy and usability and suggestions for database design. jw and yh were additional alpha testers. dz contributed to database design and manuscript revision. sm contributed to documentation for the software and manuscript revision. jl conceived of the project, conceptualized its overall structure, guided its development and edited this manuscript.

supplementary material
additional file 1
screen-shots of ace interfaces. figure `. isolateranking report. figure  <dig>  request for approval of specific primers. figure  <dig>  decision tool execution. figure  <dig>  create new set ofparameters for clone ranking. figure  <dig>  online example of gap mapperresult. figure  <dig>  online example of low confidence region finder results. figure  <dig>  parameter settings for sequencing primer design.

click here for file

 additional file 2
samples of ace reports. the file contains sample reports for decision tool, primer designer, primer order, gap mapper and low confidence finder.

click here for file

 additional file 3
ace data import and export. the file contains the organization and type of data that is either imported into or exported from ace.

click here for file

 additional file 4
xml file with reference sequence descriptions. example of xml file contains descriptions of reference sequences for all clones on the plate 'ygs000374-1'.

click here for file

 additional file 5
xml file with clone mapping information. example of xml file contains clone mapping data for the plate 'ygs000374-1'.

click here for file

 additional file 6
help file. this is the pdf version of the online help files in the software. it includes both a tutorial and an overview of the software, as well as installation instructions.

click here for file

 acknowledgements
we would like to thank fontina kelley, seamus mccarron, dan jepson, jason kramer, yili li and ashleigh ranczuch for their hard work in building and sequencing clones to produce the data used to test and develop this software. we would like to thank wilson endege for his critical comments on alpha testing the software. and we are indebted to michael collins for tireless efforts in systems support, without which this work would not be possible. this work was supported by a grant from nih/nhgri r01hg <dig> 
