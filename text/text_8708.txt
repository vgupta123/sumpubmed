BACKGROUND
high throughput gene expression analyses carried out by using innovative microarrays are increasingly used for the identification of biomarkers and for their combination in prognostic or predictive classifiers, so called signatures  <cit> . the usual design procedure of such studies consists in the following three steps: i) feature selection by statistical or artificial intelligence approaches, ii) development of the classifier on a training set, and iii) validation of the signature  <cit> . suitable data sets are more and more available in microarray data repositories, mainly geo   <cit>  and arrayexpress  <cit> .

these data sets can be used to study the correlation of clinico-pathological and follow-up data with gene expression both for the definition of signatures and to test hypotheses on the effects of over/under expression of single genes. this is particularly important in the light of the vast literature on potential biomarkers that are often tested on very limited patient cohorts and are not validated on independent data sets.

different tools, implementing distinct algorithms, are available for diverse platforms.

web-based software would avoid this problem, but up to date, apart from some exceptions , most of web tools are spread over different servers and employ different file formats.

gepas  <cit>   is an integrated web-based pipeline for the analysis of gene expression patterns aimed at putting the most popular tools together through an integrated interface. this allows end users to make transparently profit from the various tools, once their data has been uploaded. this solution, then, provides a powerful set of statistical services for gene expression analysis to users. however, the single analysis, for example survival analysis, presents some limitations concerning, e.g., availability of various tools' options, flexibility in the analysis flow and support for large datasets. moreover, in order to customize their analyses, users should design ad-hoc pipelines, by making use of workflow management features, and this would imply more complex requirements and constraints.

another tool, asterias  <cit> , provides a similar suite of applications, even if it has different implementation language and application interface; its emphasis is put on parallel computing.

we therefore set out to develop a flexible tool that allows to investigate large microarray datasets – or subsets thereof – by using standardized statistical approaches aimed at finding out the correlation of clinical data with gene expression results. in the final setting, researchers should be able to select either single or several datasets, or even single samples from different datasets, to create their own unique dataset. they would then be able to determine, on the basis of this new dataset, correlations between the expression of single genes or gene lists and clinical data.

to this aim, those limitations that are posed by current tools must also be taken into account. in fact, most of current statistical analyses on gene expression data is performed by using the r language, with the support of domain specific extension packages, such as bioconductor  <cit>  for life sciences. it is known that r poses serious limitations in dealing with large amount of data; for example, it indicates a memory failure when the required amount of memory exceeds the address-space limit for the process or, more likely, because the system is unable to provide a large enough contiguous block, even if there is enough free memory available. so, we designed a structured and engineered procedure that fully exploits available hardware and software resources thus avoiding many of the problems that are faced by researchers.

this limitation can be particularly relevant for normalization of microarray data and calculation of gene expression. we therefore developed a parallel version of dchip  <cit>  for linux that is able to handle datasets of the order of thousands of microarrays  <cit> .

microarray data analyses procedures are usually composed of several steps ranging from pre-processing  to the statistical analysis of differential gene expression. this may involve different computer programs, ranging from specialized tools and statistical software environments to simple spreadsheets. it is therefore difficult to embed them all into a single program or script. even when possible, this requires time and skills, it is hard to maintain as well as unsuited for the dissemination to the scientific community. one of the most common solutions proposed to solve this issue is based on computerized scientific workflows. in this approach, users can design their virtual experiments using a specialized software that is able to make access to standardized services and to properly visualize resulting data. web based solutions able to guarantee versioning and reproducibility of experiments have also been presented in order to build coherent collections of experiments to be used and re-used by the community. however, this approach implies that all services needed must already be available and provided either by the developer or by a third party. additional concerns derive from overall maximum execution time and the amount of data that can be handled by services, determined by the protocols used for inter-processes messaging  and for data transfer . ftp  and http protocols, that are presently widely used on the internet, were not designed for the transfer of large datasets. if used for large data set, transfer is slow due to the limits inherent in their processing overheads and the transport layer protocol, tcp . this also poses limitations to tools that rely on such data transfer solutions .

the goal of this research is to develop, set-up and validate a service for automating functional genomics analyses used in breast cancer survival studies and prognosis assessment. we present the strategy used for organizing heterogeneous computational steps by taking benefit from high performance computing  hardware and offering a simple user interface through a web portal. the current implementation only exploits local resources; further work is underway to integrate the service within existing e-science grids, such as enabling grids for e-science   <cit> .

RESULTS
the service
in this work, we present a methodology and an example of application successfully used to translate an existing procedure into an automated and consolidated web based process which can benefit from existing high performance computing resources.

for the validation of our methodology, we explored the possibility of obtaining information on the correlation of single genes or lists of genes with disease free survival after surgical removal of primary breast cancers, as described in the methods section, and implemented a web tool that we called survival online, the main processing steps, and other intermediate manual data preparation steps, that are necessary for such analyses, were analyzed, converted into web services and made available through the bmportal, a portal developed at the biolab of the department of communication, computer and system sciences  of the university of genoa, by using the enginframe framework  <cit> . this tool was chosen because it is the platform adopted by the laboratory of interdisciplinary technologies for bioinformatics  project  <cit>  and, as a consequence, all newly developed services can be deployed without any additional effort in the litbio portal  <cit> , thus reaching a large user community.

the automated procedure is shown in figure  <dig> that includes the diagram of the sequence of the processing steps involved and their relationship. in the first step, the parallel version of dchip is used to pre-process microarray data  and to determine gene expression . the following steps further evaluate expression values by means of several r/bioconductor methods. first, under/over-expressed genes are filtered so that only a user defined set of genes is conserved. then, specific phenodata are associated to samples. finally, a cox regression analysis, possibly combined with a multivariate analysis, is performed and the kaplan-meier survival plot is computed.

in figure  <dig>  the user interface for entering input parameters and selecting analysis options is shown. users can browse and select data remotely or upload it from their machine. in figure  <dig>  the execution spooler of the portal is shown. both result files and execution status are visible.

testing
the automated procedure has been tested in order to assess its added value. to this end, a series of gene lists that were recently identified  <cit>  was used. multi-gene analyses, as those performed here, are the first step in signature development and need to be validated on independent data sets: this extended validation was not performed in this case. results achieved by using survival online were identical to those obtained on local resources, thus indicating the correct functionality of the service.

results were classified according to appropriate indicators. increase of performance compared to the classic desktop-based processing was assessed by using quantitative indicators, such as time spent for analysis against different sizes of dataset, that showed a clear advantage in using survival online. tables  <dig> and  <dig> shows the execution times obtained running the test analysis with both the traditional procedure and survival online . table  <dig> presents a comparison between different analysis tools such as dchip, rma  and gcrma 

this table shows execution times related to the execution of the automated procedure by using a desktop pc, the web based service, the web based service with the parallel dchip using five parallel jobs. execution on the desktop is carried on by an experienced operator on a system with all needed software already up and running. it includes intermediate manual operations and access to data stored locally on the pc.

web execution times are calculated starting with the same operator already logged in to the service. analysis is carried out on the same data, already available on the portal.

this table shows the skill of dchip, rma and gcrma to execute the analysis based on increasing number of microarrays. gcrma doesn't even allow to analyze  <dig> microarrays, while dchip succeeds in analyzing more than  <dig> microarrays.

increase in usability, that is a key point in this work, since the designed service is intended to be used as an actual and effective support to translational research, was also assessed. usability tests have been performed by staff of the functional genomics laboratory of the national cancer research institute of genoa, whose domain expertise afforded a real evaluation of the service. as shown in table  <dig>  a qualitative assessment of usability of the traditional manual procedure and of survival online was carried out. users involved in the testing phase were asked to choose the level of satisfaction  with reference to three indicators: ease of use, repeatability and accessibility.


                              indicator
                              desktop pc
                              web based service
qualitative comparison between traditional approach and the equivalent web based service is based on the opinions given by the users involved in the evaluation process. evaluation scale is based on three levels: poor, medium, good. the comparison shows a clearly better usability of the survival online tool.

discussion
expression analyses using complex oligonucleotide microarrays has profoundly changed biomedical research. classical single gene studies necessarily start with a hypothesis, most often based on pre-existing or preliminary evidence or just on an intuition. instead, high throughput approaches, such as microarray gene expression analyses, yield data sets that are usually created and queried for a specific scope, yet the data is available for other, completely unrelated analyses. single gene studies for the identification of biomarkers for disease prognostication or prediction of treatment response have therefore led to an overwhelming number of publications where validation is usually limited to a single study on a single dataset  <cit> .

however, prioritization of the many biomarkers for in depth validation and possible developments towards clinical application is difficult. this difficulty can be overcome by using publicly available microarray datasets with relevant clinical-pathological data that allow for in-silico validation of potential biomarkers for the selection of bona fide candidates for further development. similarly, the same datasets can be used to analyze the classification power of multigene classifiers .

we presented here a web based system, survival online, where such analyses can easily be carried out by bio-medical researchers without special computation skills. at present, the system allows to select a whole dataset or a subset of it for a straightforward analysis of correlations between clinical and gene expression data. classical statistical methods  are combined in a pipeline to automate the manual procedure daily performed by researchers. the analytical tool box will be further enriched through the addition of advanced feature selection tools and of classification methods based on artificial intelligence approaches.

the validation of a classifier is better achieved when more independent data sets are taken into account. for this reason, our service is designed to harbour many different datasets to be used separately or for the generation of specific, user defined data sets. at present, this is limited to the widely used affymetrix genechip arrays. other platforms can also be integrated, but under the condition that probesets can be matched between the different arrays by using specific conversion tools and by correcting for batch effects.

analyses that use very large datasets or several smaller datasets together are normally limited by the computation power available. in our case, this limitation is overcome by the implementation of the service on a high performance platform and by the parallelization of the microarray data analysis suite dchip.

we have tested our system using a breast cancer dataset  retrieved from geo. we were able to monitor the classification power of both gene sets selected from gene ontology categories and single genes selected from literature  <cit> . the data shows that the systems allows for straightforward testing of potential biomarkers.

functional analyses of single genes or gene lists should always include an assessment of their effect on clinical outcome of cancer patients. figures  <dig> and  <dig> illustrate that survival online enables the non expert biomedical researchers to perform such analyses using existing data sets.

in some cases, the proposed approach could be considered better than a classical workflow approach. first, the execution of the analysis is not demanded to a single execution engine on the computer of the user  or on a server . approaches to execute workflows on distributed environments as a set of services exist  but they are not implemented in hpc environments and are often affected by middleware overhead and network constraints. since this case study requires a high level of interaction and usability, the best solution revealed to be the submission of a collection of single processes on hpc infrastructure synchronized by a scheduling system manager.

results show that our approach can scale up in the size of processed data well above current available desktop computer limitations  just by exploiting a cluster made up of three nodes, each composed by a low end server . obviously, by using more resources it is either possible to achieve better results or, in some cases, to overcome the single machine limitations. however, since current trends in information technology  are to offer software as a service and to concentrate computing power into data centers, due to cooling, power and skills requirements, it is essential to adequate existing procedures and applications accordingly. this will also avoid the spread of investments in computing equipment inside non-it departments and will help to better exploit resources due to their concentration.

CONCLUSIONS
survival online, allows for a straightforward execution of complex data correlations aimed at the identification of biomarkers and multi-gene classifiers. the system enables bio-medical researchers to perform validations on several independent microarray gene expression datasets in a simple and reliable fashion by using robust classical statistics. the collaboration between molecular oncologists and software engineers allowed for the optimisation of the system without loosing flexibility. the system relies on widely used analytical tools. results obtained with the web-based approach are identical to those achieved by using the traditional methods.

future developments are planned to enhance service usability by using a standard wsdl  interface. in fact, the adopted framework is able to provide services both through a web interface and through a web service, exploitable from any web services enabled client application.

orchestrating multiple processing steps in a workflow strategy opens a new scenario where users can design complex experiments, tailored to their needs, just by using portal services as workflow building bocks.

this is possible, e.g., by importing wsdl descriptions of services into a client application, such as the taverna workbench, and by letting the portal to execute and monitor the workflow life cycle, thus avoiding client side enacting limitations.

in the future, we plan to implement additional feature selection and classification tools and to add further datasets. the development of inter-array comparison tools is also planned.

