BACKGROUND
biological networks are context‐specific and dynamic in nature  <cit> . under different conditions, different regulatory components and mechanisms are selectively activated or deactivated  <cit> , <cit> . one example is the topology of underlying biological network changes in response to internal or external stimuli, where cellular components exert their functions through interactions with other molecular components  <cit> , <cit> . thus, in addition to asking “which genes are differentially expressed”, the new question is “which genes are differentially connected”  <cit> , <cit> . studies on network-altering events will shed new light on whether network rewiring is a general principle of biological systems regarding disease progression or therapeutic responses  <cit> , <cit> . moreover, due to inevitable experimental noise, snapshots of dynamic expression, and post-transcriptional or translational/post-translational modifications, systematic efforts to characterize biological networks must effectively distinguish significant network rewiring from random background fluctuations  <cit> .

almost exclusively using high-throughput gene expression data and focusing on conserved biological networks, various network inference approaches have been proposed and tested  <cit> , including probabilistic boolean networks  <cit> , state‐space models  <cit> , <cit> , and probabilistic graphical models  <cit> . however, since these methods often assume that there is a static network structure, they overlook the inherently dynamic nature of molecular interactions, which can be extensively rewired across different conditions. hence, current network models only present a conserved cellular network averaging across all samples. to explicitly address differential network analysis  <cit> , <cit> , <cit> , some initial efforts have been recently reported  <cit> . in our previous work, zhang et al. proposed to model differential dependency networks between two conditions by detecting network rewiring using significance tests on local dependencies across conditions  <cit> , <cit> , which is a substantially different method from the one proposed in this paper where experimental data and prior knowledge are jointly modeled. the approach was successfully extended by roy et al. to learn dynamic networks across multiple conditions  <cit> , and by gill et al. to assess the overall evidence of network differences between two conditions using the connectivity scores associated with a gene or module  <cit> . pioneered and reported in  <cit> , correlation and partial correlation are used to construct network graphs, and differential pathway analysis is developed based on graph edit distance. the temporal evolution of network structures is examined with a fused penalty term to encode relationship between adjacent time points in  <cit> . furthermore, recent efforts have also been made to incorporate existing knowledge about network biology into data-driven network inference  <cit> . wang et al. proposed to incorporate prior knowledge into the inference of conserved networks in a single condition by adjusting the lasso penalties  <cit> . yet, the inherently dynamic wiring of biological networks remains under-explored at the systems level, as interaction data are typically reported under diverse and isolated conditions  <cit> .

there are at least five unresolved issues concerning differential network inference using data-knowledge integrated approaches:  the solution  space is usually large while sample sizes are small, resulting in potential overfitting;  both conserved and differential biological networks are complex and lack closed-form or efficient numerical solutions;  “structural” model parameters are assigned heuristically, leading to potentially suboptimal solutions;  prior knowledge is imperfect for inferring biological networks under specific conditions, e.g., false positive “connections”, biases, and non-specificity; and  most current methods do not provide significance assessment on the differential connections and rigorous testing of the type i error rate.

to address these challenges, we formulated the inference of differential dependency networks that incorporate both conditional data and prior knowledge as a convex optimization problem, and developed an efficient learning algorithm to jointly infer the conserved biological network and the significant rewiring across different conditions. extending and improving our work on gaussian graphical models  <cit> , <cit> , we designed block-wise separable penalties in the lasso-type models that permit joint learning and knowledge incorporation with an efficient closed-form solution. we estimated the expected error rate due to “random” prior knowledge via a novel sampling scheme. based on that scheme, we developed a strategy to fully exploit the benefit of this data-knowledge integrated approach. we determined the values of model parameters that quantitatively correspond to the expected significance level, and evaluated the statistical significance of each of the detected differential connections. we validated our method using synthetic datasets and comprehensive comparisons. we then applied our method to yeast cell line and breast cancer microarray data and obtained biologically plausible results.

methods
formulation of knowledge-fused differential dependency network 
we represent the condition‐specific biological networks as graphs. suppose there are p nodes  in the network of interest, and we denote the vertex set as v. let g = ) and g = ) be the two undirected graphs under the two conditions. g and g have the same vertex set v, and condition‐specific edge sets e and e. the edge changes indicated by the differences between e and e are of particular interest, since such rewiring may reveal pivotal information on how the organisms respond to different conditions. we label the edges as common edges or specific to a particular condition in graph g =  to represent the learned networks under the two conditions.

prior knowledge on biological networks is obtained from well-established databases such as kegg  <cit>  and is represented as a knowledge graph g
w
 = , where the vertex set v is the same set of nodes  and the edge set ew over v is translated from prior knowledge. there are many alternatives to extract existing domain knowledge, e.g., string, hprd, or manual construction. the adjacency matrix of gw, w ∈ ℜ
p × p
, is used to encode the prior knowledge. the elements of w are either  <dig> or  <dig>  with x
ji
 =  <dig> indicating the existence of an edge from the jth gene to the ith gene , where i, j =  <dig>   <dig>  ⋯, p, i ≠ j. w is symmetric if the prior knowledge is not directed.

the main task in this paper is to infer from data and prior knowledge gw the condition‐specific edge sets e  and e). the method is illustrated in figure  <dig> 

we consider the p nodes in v as p random variables, and denote them as x <dig>  x <dig>  ⋯, x
p
. suppose there are n <dig> samples under condition  <dig> and n <dig> samples under condition  <dig>  without loss of generality, we assume n1 = n2 = n. under the first condition, for variable x
i
, we have observations xi1=x1i <dig> x2i <dig> ⋯,xni1t, while under the second condition, we have xi2=x1i <dig> x2i <dig> ⋯,xni2t, i =  <dig> ,⋅⋅⋅,p. further, let x1=x <dig> x <dig> ⋯,xp <dig> be the data matrix under condition  <dig> and x2=x <dig> x <dig> ⋯,xp <dig> be the data matrix under condition  <dig> 

denote  yi=xi1xi <dig> x=x100x <dig>  

and  βi=βi1βi2=β1i <dig> β2i <dig> ⋯,βpi <dig> β1i <dig> β2i <dig> ⋯,βpi2t, with the non‐zero elements of βi <dig> indicating the neighbours of the ith node under the first condition and the non‐zero elements of βi <dig> indicating the neighbours of the ith node under the second condition.

the problem of simultaneously learning network structures and their changes under two conditions is formulated as a regularized linear regression problem with sparse constraints and solved by convex optimization. for each node  x
i
, i =  <dig> ,⋅⋅⋅,p, we solve the optimization with the objective function  fβi=12yi−xβi22+λ1∑j=1p1−wjiθβji1+βji2+λ2βi1−βi <dig> 

the non‐zero elements in w introduce knowledge to the objective function , and θ is a ℓ <dig> penalty relaxation parameter taking value in .

the solution is obtained by minimizing ,  βi=argminβifβi=argminβi <dig> βi212yi−xβi22+λ1∑j=1p1−wjiθβji1|+|βji2+λ2βi1−βi21s.t.βii1= <dig> βii2= <dig>  

both the cost function 12yi−xβi <dig> and two regularization terms ∑j=1p1−wjiθβji1|+|βji <dig> and βi1−βi <dig> co-existed in the objective function are convex, and this convex formulation leads to an efficient algorithm. the structures of the graphical model under two conditions are obtained jointly by solving  sequentially for all nodes. the inconsistency between βi <dig> and βi <dig> highlights the structural changes between two conditions, and the collection of differential edges form the differential dependency network.

given the vast search space and complexity in both conserved and differential networks, it is crucial for kddn to identify statistically significant network changes and filter the structural and parametric inconsistencies due to noise in the data and limited samples. this objective is achieved by selecting the proper model specified by λ <dig> and λ <dig> that best fits the data and suffices the statistical significance. λ <dig> is determined by controlling the probability of falsely joining two distinct connectivity components of the graph  <cit>  and λ <dig> is found by setting differential edges to a defined significance level. we refer readers to additional file 1: s <dig>  for a detailed discussion of model parameter-setting approaches.

with parameters specified, problem  can be solved efficiently by the block coordinate descent algorithm presented in additional file 1: s <dig> , algorithm s <dig> 

incorporation of prior knowledge
the prior knowledge is explicitly incorporated into the formulation by w
ji
 and θ in the block-wise weighted ℓ <dig> -regularization term. w
ji
 =  <dig> indicates that the prior knowledge supports an edge from the jth gene to the ith gene and  <dig> otherwise. a proper θ will reduce the penalty applied to βjic, c =  <dig>   <dig>  corresponding to the connection between x
j
 and x
i
 with w
ji
 =  <dig>  as a result, the connection between x
j
 and x
i
 will more likely be detected.

θ is a weighting parameter on the influence of prior knowledge, determining the degree of the knowledge incorporation in the network inference. when θ =  <dig>  the algorithm ignores all knowledge information and gives solely data-based results; conversely, when θ =  <dig>  the edge between x
j
 and x
i
 will always be included if such an edge exists in the prior knowledge. therefore, the prior knowledge incorporation needs to find a proper balance between the experimental data and prior knowledge to achieve effective incorporation, as well as limit the adverse effects caused by any spurious edges contained in imperfect prior knowledge.

here we propose a strategy to control the adverse effects incurred in the worst‐case scenario under which the given prior knowledge is totally random. in this case, the entropy of the knowledge distribution over the edges is maximized and the information introduced to the inference is minimal. incorporating such random knowledge, the inference results will deviate from the purely data-driven result. we want to maximize the incorporation of relevant prior knowledge, while at the same time making sure the potential negative influence of irrelevant prior knowledge is under control so that the expected deviation is confined within an acceptable range in the worst‐case scenario. to properly set the value of θ, we assess the actual influence of prior knowledge for each value that θ may take, and developed theorem  <dig> to determine the best degree of prior knowledge incorporation. this approach guarantees robustness even when the prior knowledge is highly inconsistent with the underlying ground truth.

to quantify the effects of prior knowledge incorporation, we use graph edit distance  <cit>  between two adjacency matrices as a measurement for the dissimilarity of two graphs. let gt =  denote the ground‐truth graph with edge set e
t
, gx =  denote the graph learned purely from data, i.e., w =  <dig>  and gx,wr,θ=v,ex,wr,θ denote the graph learned with prior knowledge. wr indicates that the prior knowledge is “random”. let d denote the graph edit distance between two graphs. further, let |e| be the number of edges in the graph g.

our objective is to bound the increase of inference error rate associated with the purely data-driven result, dgt,gx,wr,θ/et−dgt,gx/et, within an acceptable range δ even if the prior knowledge is the worst case by finding a proper θ.

since gt is unknown, we instead control the increase in the error rate indirectly by evaluating the effect of random knowledge against gx, the purely data‐driven inference result. specifically, we use a sampling‐based algorithm to find the empirical distribution of dgx,gx,wr,θ, and choose the largest θ ∈  that satisfies:  θ^=maxθs.t.edgx,gx,wr,θ/ex≤δ, where e is the expectation of the graph edit distance between graphs g <dig> and g <dig>  with respect to its empirical distribution.

a natural question is whether using gx instead of gt to control the increase in the error rate induced by random knowledge is legitimate. to answer this question, we show in theorem  <dig>  that the θ obtained in  in fact controls an upper bound of edgt,gx,wr,θ/et, i.e. the increase in the network inference error rate induced by random prior knowledge , under the assumption that the number of false negatives  in the data-driven result gx is smaller than the number of false positives . as we adopt a strategy to control the probability of falsely joining two distinct connectivity components  <cit> , this assumption generally holds.

theorem  <dig> establishes the relationship between prior knowledge incorporation θ and the adverse effects of prior knowledge on network inference, quantified by δ, under the worst-case scenario . for example, δ =  <dig>  indicates that the user can accept at most 10% performance degradation if the prior knowledge is completely noise. with the estimate of θ at δ =  <dig> , even the prior knowledge is totally random, the performance will decrease no more than 10%, while the relevant portion of the real prior knowledge  can greatly improve the overall network inference performance.

theorem 1
for a given δ ∈ [ <dig>  1), if the prior knowledge incorporation parameter θ satisfies the inequality  edgx,gx,wr,θex≤δ, then the increase in the error rate induced by incorporating random prior knowledge is bounded by δ, more specifically,  edgt,gx,wr,θet≤dgt,gxet+δ 

given the number of edges specified in the prior knowledge, procedures to compute θ are detailed in algorithm s <dig> in additional file 1: s <dig> .

RESULTS
we demonstrated the utility of kddn using both simulation data and real biological data. in the simulation study, we tested our method on networks with different sizes and compared with peer methods the performance of overall network structure recovery, differential network identification and tolerance of false positives in the prior knowledge.

in a real data application, we used kddn to learn the network rewiring of the cell cycle pathway of budding yeast in response to oxidative stress. a second real data application was the study of the differential apoptotic signaling between recurring and non-recurring breast cancer tumors. applications to study muscular dystrophy and transcription factor binding schemes are included in additional file 1: s <dig> 

simulation study
we constructed a gaussian markov random field with p =  <dig> nodes and  <dig> samples following the approach used in  <cit> , and then randomly modified 10% of the edges to create two condition‐specific networks with sparse changes. the details of simulation data generation procedure are provided in additional file 1: s <dig> . the number of edges in prior knowledge m was set to be the number of common edges in the two condition‐specific networks, and δ was set to  <dig> .

to assess the effectiveness of prior knowledge incorporation and robustness of kddn when false positive edges were present in prior knowledge, we examined the network inference precision and recall of the overall network and the differential network at different levels of false positive rate in the prior knowledge.

both false positives and false negatives in the prior knowledge here are with respect to the condition-specific ground truth from which the data are generated. thus, although false positives in prior knowledge may contribute more learning errors, false negatives will not worsen network learning performance .

starting from prior knowledge without any false positive edges, we gradually increased the false positive rate in prior knowledge until all prior knowledge was false. at each given false positive rate in the prior knowledge, we randomly created  <dig>  sets of prior knowledge, and compared the performance of kddn in terms of precision and recall with two baselines:  a purely data-driven result, corresponding to kddn with θ= <dig>  i.e., without using any prior knowledge in the network inference ; and  a naïve baseline of knowledge incorporation by superimposing  the prior knowledge network and the network inferred purely from the data.

the results of the overall network  learning are shown in figure  <dig> and . the dot-connected lines are averaged precision or recall of the network learned with  <dig>  sets of prior knowledge. the box plot shows the first, second and third quartiles of precision or recall at each false positive rate in prior knowledge . the blue squared lines, brown circle lines, and red diamond lines indicate the mean performance of kddn, purely data-driven baseline, and naïve baseline, respectively. narrower lines with the same colors and line styles mark the one standard deviation of the performance of the corresponding approach.

precision reflects the robustness to the false positive edges and efficiency of utilizing the information in prior knowledge. figure  <dig> shows that, as expected, the false positive rate in prior knowledge has a limited effect on the precision of kddn . with more false positives in the prior knowledge, the precision decreases but is still around the purely data-driven baseline  and much better than the naïve baseline . the naïve baseline suffers significantly from the false positives in prior knowledge, because it indiscriminately accepts all edges in prior knowledge without considering evidence in the data. this observation corroborates the design of our method: to control the false detection incurred by the false positives in the prior knowledge. at the point where the false positive rate in the prior knowledge is 100%, the decrease of precision compared with the purely data-based result is bounded within δ.

recall reflects the ability of prior knowledge in helping recover missing edges. figure  <dig> shows that when the prior knowledge is 100% false, the recall is the same as that of the purely data-driven result, because in this case the prior knowledge brings in no useful information for correct edge detection. when the true positive edges are included in the prior knowledge, the recall becomes higher than that of the purely data-based result, because more edges are correctly detected by harnessing the correct information in the prior knowledge. the naïve baseline is slightly higher in recall, since it calls an edge as long as knowledge contains it, while kddn calls an edge only when both knowledge and data evidence are present. the closeness between kddn and naïve baseline demonstrates the high efficiency of our method in utilizing the true information in prior knowledge.

we then evaluated the effect of knowledge incorporation solely on the identification of differential networks following the same protocol. the results are shown in figure  <dig> with the same color and line annotations.

for differential network recovery, the naïve baseline is almost identical to purely data-driven results because the prior knowledge seldom includes a differential edge in a large network with sparse changes. while similar advantages of kddn apply, our method has better precision and recall, and bounds the performance degradation when knowledge is totally wrong. unlike the naïve baseline where knowledge and data are not linked, we model the inference with knowledge and data together, so knowledge is also able to help identify differential edges. performance evaluation results in additional file 1: s <dig>  studied networks with varying sizes, reaching consistent conclusions.

depending on specific conditions, false positives in prior knowledge may not distribute uniformly, but tend to aggregate more towards certain nodes. experiments with biased knowledge distribution shown in additional file 1: s <dig> , figures s10-s <dig> indicate no difference or little improvement compared to random knowledge, confirming that random knowledge represents the worst-case scenario and the bound according to random knowledge is sufficient.

performance comparisons with peer methods
we compared our joint learning method kddn with four peer methods: 1) ddn   <cit> , 2) cslearner   <cit> , 3) meinshausen’s method   <cit> , and 4) tesla   <cit> . cslearner can learn more than two networks but we restricted the condition to two. meinshausen’s method learns the network under a single condition, and we combined the results learned under each condition to get conserved network and differential network. tesla learns a time-evolving network, but can be adapted to two-condition learning as well. only kddn can assign edge-specific p-values to differential edges.

parameters in kddn are automatically inferred from data as described in additional file 1: s <dig> . for the competing methods in the comparison, we manually tested and tuned their parameters to obtain their best performance. we set ddn to detect pair-wise dependencies. the number of neighbors in cslearner is set to “4” . meinshausen’s method uses the same λ <dig> as inferred by kddn, as it is a special case of kddn under one condition without prior knowledge. tesla uses the empirically-determined optimal parameter values, since the parameter selection was not automatic but relies on user input.

to assess the impact of prior knowledge, we ran kddn under three scenarios: data-only , data plus true prior knowledge , and data plus “random” prior knowledge . only kddn is able to utilize prior knowledge.

the ground truth networks consisted of  <dig>   <dig>   <dig>   <dig> and  <dig> nodes, respectively, and correspondingly  <dig>   <dig>   <dig>   <dig> and  <dig> samples. for each network size,  <dig> replicate simulation networks were generated. we evaluated the performance of inferring both overall and differential edges of the underlying networks using the f-score  and precision-recall averaged over all datasets under each network size.

the results are presented in figure  <dig> using bar plots. the color annotations are: orange-cslearner, golden-ddn, olive green-kddn.dt, aquamarine-kddn.fk, blue-kddn.tk, purple-meinshausen, magenta-tesla. we used one-sided t-tests to assess whether kddn performs better than the peer methods across all network sizes. the null hypothesis is that there is no difference between the mean of f-score of kddn and the peer methods. the alternative hypothesis is that kddn has the greater mean of f-score. the detailed results are included in tables s <dig> and s <dig> in additional file 1: s <dig> , which shows that kddn.tk performs significantly better than peers in all cases, and kddn.dt and kddn.fk performs better than peers in  <dig> of  <dig> cases.

figure  <dig> compares the ability of recovering overall networks. we see kddn.tk consistently outperforms all peer methods, and kddn.dt and kddn.fk performs comparably to tesla . the independent learning methods, ddn and meinshausen’s method, place third due to their inability to jointly use data.

figure  <dig> shows the comparison of performance on recovering differential edges. kddn consistently outperforms all peer methods under all scenarios. while kddn and tesla share some commonalities, they use different formulations. where tesla uses logistic regression, kddn adopted linear regression to model the dependency. such a difference also has implications for the subsequent solutions and outcomes. the fact that kddn determines λ <dig> according to the statistical significance of differential edges helps kddn outperform tesla in differential edge detection. it is also clear that a single-condition method cannot find the differential edges correctly and has the worst performance.

in figures s <dig> and s <dig> in additional file 1: s <dig> , the performance of these methods is compared in terms of precision and recall; we reached the same conclusions.

through these comparisons, we show that kddn performs better than peer methods in both overall and differential network learning. high-quality knowledge further improves kddn performance, while kddn is robust enough even to totally random prior knowledge. joint learning, utilization of prior knowledge, and attention to statistical significance all helped kddn outperform the other methods.

pathway rewiring in yeast uncovers cell cycle response to oxidative stress
to test the utility of kddn using real biological data, we applied the kddn to the public data set gse <dig>  this data set used budding yeast saccharomyces cerevisiae to study the genome-wide response to oxidative stress imposed by cumene hydroperoxide . yeast cultures were grown in controlled batch conditions, in 1l fermentors. three replicate cultures in mid-exponential phase were exposed to  <dig> mm chp, while three non-treated cultures were used as controls. samples were collected at t =  <dig>  and at  <dig> , <dig> , <dig>  and  <dig> min after adding the oxidant. samples were processed for rna extraction and profiled using affymetrix yeast genome s <dig> arrays. there were  <dig> samples in total, evenly divided between the treated and the non-treated groups.

we analyzed the network changes of cell cycle-related genes with structural information from the kegg yeast pathway as prior knowledge. we added the well-studied yeast oxidative stress response gene yap <dig> <cit> - <cit>  to the knowledge network and related connections gathered from the saccharomyces genome database  <cit> . the learned differential network result is shown in figure  <dig>  in which nodes represent genes involved in the pathway rewiring, and edges show the condition-specific connections. red edges are connections in control and green edges are connections under stress.

oxidative stress is a harmful condition in cells, due to the failure of the antioxidant defense system to effectively remove reactive oxygen molecules and other oxidants. the result shows that yap <dig>  rho <dig> and msn <dig> are at the center of the network response to oxidative stress; they are activated under oxidative stress and many connections surrounding them are created . yap <dig> is a major transcription factor that responds to oxidative stress  <cit> - <cit> . msn <dig> is considered as a general responder to environmental stresses including heat shock, hydrogen peroxide, hyper-osmotic shock, and amino acid starvation  <cit> , <cit> . rho <dig> is known to resist oxidative damage and facilitate cell survival  <cit> - <cit> . the involvement of these central genes captured the dynamic response of how yeast cells sense and react to oxidative stress. the edge between yap <dig> and ctt <dig> under stress grants more confidence to the result. ctt <dig> acts as an antioxidant in response to oxidative stress  <cit> , and the coordination between yap <dig> and ctt <dig> in protecting cells from oxidative stress is well established  <cit> . this result depicted the dynamic response of yeast when exposed to oxidative stress and many predictions are supported by previous studies. this real data study validated the effectiveness of the methods in revealing underlying mechanisms and providing potentially novel insights. these insights would be largely missed by conventional differential expression analysis as the important genes rho <dig>  msn <dig>  yap <dig> and ctt <dig> ranks  <dig>   <dig>   <dig> and  <dig> among all  <dig> involved genes based on t-test p-values. in a comparison with data-only results in additional file 1: s <dig> ,  <dig> different differential edges are found. we also applied a bootstrap method in  <cit>  to assess the robustness of the findings as detailed in additional file 1: s <dig> .

apoptosis pathway in patients with early recurrent and non-recurrent breast cancer
network rewiring analysis is also applicable to mechanistic studies and helps identify underlying key players that cause phenotypic differences. for example, 50% of estrogen receptor-positive breast cancers recur following treatment, but the mechanisms involved in cancer recurrence remain unknown. understanding of the mechanisms of breast cancer recurrence can provide critical information for early detection and prevention. we used gene expression data from a clinical study  <cit>  to learn differences in the apoptosis pathway in primary tumors between patients with recurrent and non-recurrent disease. we compared the pathway changes in tumors obtained from patients whose breast cancer recurred within  <dig> years after treatment and from patients who remained recurrence-free for at least  <dig> years after treatment. there were  <dig> and  <dig> tumor samples in the recurring and non-recurring groups, respectively. gene expression data were generated using affymetrix u133a arrays. we used the apoptosis pathway from kegg as prior knowledge.

following the same presentation as in the yeast study, red edges are connections established in patients with recurrent disease, and green edges are connections unique to patients without recurrent disease. differences in the signaling among genes in the apoptosis pathway between patients whose cancer recurred or who remained cancer-free are shown in figure  <dig> 

three inflammatory/immune response genes  that are all linked to increased resistance to breast cancer treatment were identified in the recurrent tumors. these genes formed a path to inhibit proapoptotic casp <dig> and ppp3r <dig> <cit> , and to activate the pro-survival genes pik3r <dig> or csf2rb that maintain cell survival. in contrast, green edges that were present in non-recurrent tumors form paths to both anti-apoptotic xiap/akt <dig> and proapoptotic bax and bad gene functions.

when we overlaid the differential network over the kegg  <cit>  apoptosis pathway we noticed additional differences in the signaling patterns. using the same color-coded presentation, we show the learned differential network in figure  <dig>  in the recurrent breast cancers , the molecular activities mainly affect the initial apoptotic signals outside the cell and within the cell membrane , while inside the cell there is no clear signaling cascade affected to determine cell fate. the only route affected within the cell is il1b-induced inhibition of proapoptotic casp <dig>  in non-recurrent breast cancer, the affected network involves both signals received from activation of the membrane receptors and a cascade of signaling pathways inside the cell to promote both apoptosis and survival. since a balance between apoptosis and survival is necessary for damaged cells to be eliminated and repaired cells to survive  <cit> , it is logical that both pathways would be activated concurrently. interestingly, the imbalance of apoptotic and survival signals and the inhibition of casp <dig> in recurrent cancer both lead to the resistance of cell death, reported as a major hallmark of cancer  <cit> .

in conclusion, the apoptosis pathway rewiring analysis identified key mechanistic signaling differences in tumors from patients whose breast cancer did or did not recur. these differences provide a promising ground for novel hypotheses to determine factors affecting breast cancer recurrence.

CONCLUSIONS
to address the challenges concerning differential network inference using data-knowledge integrated approaches, we formulated the problem of learning the condition‐specific network structure and topological changes as a convex optimization problem. model regularization and prior knowledge were utilized to navigate through the vast solution space. an efficient algorithm was developed to make the solution scalable by exploring the special structure of the problem. prior knowledge was carefully and efficiently incorporated in seeking the balance between the prior knowledge support and data-derived evidence. the proposed method can efficiently utilize prior knowledge in the network inference while remaining robust to false positive edges in the knowledge. the statistical significance of rewiring and desired type i error rate were assessed and validated. we evaluated the proposed method using synthetic data sets in various cases to demonstrate the effectiveness of this method in learning both common and differential networks, and the simulation results further corroborated our theoretical analysis. we then applied this approach to yeast oxidative stress data to study the cellular dynamic response to this environmental stress by rewiring network structures. results were highly consistent with previous findings, providing meaningful biological insights into the problem. finally, we applied the methods to breast cancer recurrence data and obtained biologically plausible results. in the future, we plan to incorporate more types of biological prior information, e.g., protein‐dna binding information in chip‐chip data and protein‐protein interaction data, to improve the use of condition-specific prior knowledge.

abbreviations
kddn: knowledge-fused differential dependency network

tp: true positives

fp: false positives

kddn.dt: kddn with no knowledge

kddn.tk: kddn with true knowledge

kddn.fk: kddn with random knowledge

competing interests
the authors declare that they have no competing interests.

authors’ contributions
yt and bz designed the method and drafted the manuscript along with zz and yw. yt and bz carried out the experimental validation of the methods. eph, rc, ims, jx and dmh helped with the methodology development and provided important biological interpretation of the results. rc and dmh helped with the yeast study. rc, zz and ims helped with the breast cancer study. eph helped with the muscular dystrophy study. all authors read and approved the final manuscript.

additional file
supplementary material
additional file 1:
supplementary methods and experimental results. details of theoretical proofs and algorithms, more synthetic and real data comparisons, and validations are included in this file.

click here for file

 acknowledgements
this work is supported in part by the national institutes of health under grants ca <dig>  ca <dig>  ns <dig>  ca <dig> and hl <dig>  this work is also supported in part by grant u24ca <dig>  from the national cancer institute clinical proteomic tumor analysis consortium .
