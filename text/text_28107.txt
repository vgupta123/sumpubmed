BACKGROUND
assigning functions to unannotated genes, identified by genome sequencing and other methods, is the goal of functional genomics. many approaches have been proposed for large-scale prediction of gene function  <cit> . these approaches are largely based on physical association, genetic interaction, sequence relationships and patterns of gene expression. predicting gene functions based on large-scale gene expression measurements is an attractive strategy since many pathways display coordinated transcriptional regulation  <cit> . although previous studies show that supervised learning methods can be used to predict gene function based on gene expression in microorganisms such as the yeast saccharomyces cerevisiae and in mammals such as mice  <cit> , it remains unknown to what extent this is true in plants.

with the a. thaliana genome completely sequenced  <cit> , functional annotation of the genes remains a key challenge for biologists. currently, approximately 50% of the  <dig>  genes have not been assigned any function  <cit> . thus, the extent to which supervised learning methods can be used to infer gene function in a. thaliana is a timely and important question. little work has been done in this area, two exceptions being  <cit> .

in  <cit> , a method is developed to infer gene function from microarray data and predicted protein-protein interactions. the method is similar to nearest neighbor algorithms  <cit>  in that the predicted function of a gene are based on the function of nearby genes. here, the "nearness" of one gene to another is based on a normalized pearson correlation of their expression profiles and on putative interactions of their protein products. in addition, the method is extended to the discovery of biological pathways, and is applied to predicting the signaling pathway of phosphatidic acid as a second messenger in a. thaliana.

in  <cit> , a decision tree algorithm is applied to the problem of predicting the function of protein sequences in a. thaliana. six sources of data were used: sequence, expression, scop, secondary structure, interpro and sequence similarity. one conclusion of the study is that the decision tree algorithm was unable to extract much information from the expression data. the authors suggest that this is because the expression data came from unrelated and highly-specific experiments with just a few readings per gene each. they also suggest that because many more expression data sets are now available for a. thaliana, results may improve when using this type of data in the future.

the present study aims to identify unannotated genes in a. thaliana that are potentially involved in plant response to stress. in the context of plants, a stress  causes a decrease in plant growth or yield. we investigated the prediction of gene function in a. thaliana based solely on gene expression data using a variety of basic supervised learning methods, namely logistic regression , linear discriminant analysis , quadratic discriminant analysis , naive bayes  and k-nearest neighbors . we also investigated the effect on the learning methods of preprocessing the expression data using principal component analysis . finally, we improved the performance of the basic learning methods by combining them using a weighted voting  scheme. this work has enabled our collaborators, biologists in the department of cell and systems biology at the university of toronto, to carry out directed biological experiments for determining gene function. in addition to these biological results, the paper illustrates how various machine-learning methods have had to be adapted to fit this bioinformatics application.

RESULTS
microarray data and the gene ontology
in this study, we used two microarray data sets: one from the botany array resource at the university of toronto  <cit> , and the other from the atgenexpress consortium  <cit> , archived at nascarrays  <cit> . these data sets include over  <dig> expression-level experiments for arabidopsis, and using all of them would give a data set with dimensionality over  <dig>  since the performance of statistical and machine-learning methods tends to decrease with dimensionality, we chose only those experiments that are specifically stress-related. even so, the covariance matrix of the resulting data set is singular, which is a problem for many of the machine-learning methods. the singularity is probably due to dependencies between the expression levels under control conditions, since removing the controls from the data sets solved the problem. to compensate, we tried applying the learning algorithms to expression-level ratios . however, we found that the results were better when ratios were not used . this is probably because the classifiers look for genes that respond similarly to the known stress-associated genes, so it is not so important to include the controls. in addition, since many of the features are time-courses, there is still a "time zero" control included for the values, providing a baseline measurement. the results reported in this article are therefore based on absolute expression levels without controls.

from the toronto data set, we selected  <dig> features corresponding to experiments conducted primarily to study plant environmental and stress physiology, plant physiology, plant-microbe and plant-insect interactions. from the atgenexpress data set, we selected  <dig> features, including various abiotic stresses . we combined the selected features into a single data set. the resulting data set consists of gene expression levels for  <dig>  genes under  <dig> +  <dig> =  <dig> different experimental conditions.

we used terms from the gene ontology for biological processes  to represent gene function. for example, the gobp term go: <dig>  refers to genes that respond to stress. in general, the gene ontology  provides a dynamic controlled vocabulary for describing genes and gene products in any organism  <cit> . "biological process" is one of three broad go categories . gobp terms are organized into a directed acyclic graph  to reflect the hierarchical relationships between the terms. parent gobp terms are subdivided into increasingly specific child gobp terms.

since our study focussed on stress, we were concerned with gene functions at or below the term go: <dig>  in the gobp hierarchy. this gobp term has  <dig> child terms, such as go: <dig> , go: <dig> , and go: <dig> . since gene function becomes more and more specific as we move down the gobp hierarchy, fewer and fewer genes have any given annotation. the result is that for specific types of stress, our data set contains many negatives and few positives. in the best case, for the term go: <dig> , over 97% of the training data consists of negatives. the typical case is even worse. in fact, looking at all  <dig> types of stress,  <dig> types have no positives at all, and of the remaining  <dig> types, the median number of negatives is  <dig> % of the training data. this highly unbalanced data made accurate prediction of gene function difficult. for this reason, we narrowed our study to the top stress term, go: <dig> . to get positive training samples for this term, we propagated all genes in its offspring upward to it in the hierarchy. after up-propagation, the top stress term has  <dig>  genes, or almost 9% of the total genes in the training data. the training data therefore contains 9% positives and 91% negatives.

using gobp terms to annotate all genes in a. thaliana is an ongoing project started in  <dig> by tair  <cit> . the gene annotations  can be downloaded from tair  <cit> . the predictions reported in this paper are based on the version for march  <dig>   <dig>  using these annotations, we categorized the genes into annotated genes and unannotated genes. the annotated genes are those which have at least one gobp annotation; the unannotated genes are those which have no gobp annotations. in addition, a gene was treated as unannotated if its only annotation is the top gobp category, go: <dig> , since the function of such a gene is unknown. the result was  <dig>  annotated genes and  <dig>  unannotated genes in our data set.

the annotated genes formed the training data, in which a gene was called positive if it is annotated as a stress gene, and negative otherwise. the unannotated genes formed the prediction data. it should be noted that this approach probably introduces some false negatives into the training data, because genes not known to have a particular function are considered to be negative, even though future experiments could reveal them to have that function. that is to say, "unknown" is treated as "negative". however, the number of such false negatives should be small, since only a small number of genes participate in any given biological process. that is, most negatives are true negatives.

predicting gene function using basic learning methods
using a variety of basic learning methods, we trained a number of classifiers to distinguish between genes that do and do not respond to stress, based on their patterns of gene expression in the training data. we then applied each classifier to the prediction data to estimate the function of the unannotated genes. in addition, we used cross validation to evaluate the performance of each classifier and to estimate the precision of each prediction.

we used five supervised learning methods: logistic regression , linear discriminant analysis , quadratic discriminant analysis , naive bayes  and k-nearest neighbors   <cit>  . these methods were chosen because they are representative of the most basic supervised learning methods, the goal being to explore simple methods first. these methods are widely understood, take little computation time, and the results provide a benchmark against which more sophisticated methods can be compared. moreover, as we show below, the results provided by these methods are good enough to enable biologists to conduct targeted laboratory experiments.

each of the five methods is discriminative. that is, the classifiers learned by the methods assign a real number  to each gene, reflecting the classifier's certainty that the gene responds to stress. formally, a discriminative classifier is a function, f^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgmbgzgaqcaaaa@2e11@, from genes to discriminant values. in our case, each gene is represented as a 290-dimensional vector, x, whose components are the expression levels of the gene under the  <dig> experimental conditions. thus, if x is a vector representing a gene, then dv = f^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgmbgzgaqcaaaa@2e11@ is the discriminant value assigned to the gene by the classifier. finally, a decision threshold, τ, is chosen, and the gene is predicted to respond to stress if and only if dv > τ.

unsupervised, semi-supervised and transductive learning
in addition to these supervised learning methods, we preprocessed the gene expression data using principal components analysis , a form of unsupervised learning, to reduce the dimensionality of the data . for this purpose, we combined the expression-level measurements for all genes  into one large data set, and applied pca to the entire set. we are therefore doing a form of semi-supervised learning  <cit> , in which unsupervised learning uses the entire data set , and then supervised learning uses the annotated data. this increases the effectiveness of learning by increasing the amount of training data used in the unsupervised phase  <cit> . in our case, the unannotated data is also the prediction data, which means that information about the prediction data is used during  training. this is possible because we know all the prediction data in advance. that is, we know the expression levels for all the genes in arabidopsis whether they are annotated or not. we are therefore doing a form of transductive learning  <cit> , in which the entire prediction set is known during training and is exploited to predict its annotations. this has the added computational advantage of simplifying the way pca is done during cross validation .

estimating classifier performance
to evaluate the performance of discriminative classifiers, it is common to use receiver operating characteristic  curves  <cit> . a roc curve plots the true positive rate  of a classifier against the false positive rate  for various decision thresholds. it therefore shows the quality of a classifier not at one threshold, but at many, and provides more information than a simple miss-classification rate . in practice, however, biologists are not usually interested in having more than a few dozen false positives, especially in unbalanced data such as ours, in which the number of false positives can rapidly overwhelm the number of true positives. we therefore use so-called roc <dig> curves  <cit> , a variant of roc curves in which the horizonal axis only goes up to  <dig> false positives. the area under a roc <dig> curve is the roc <dig> score  <cit> , and is a measure of the overall usefulness of a classifier.

to estimate roc <dig> curves for our classifiers, we used 20-fold cross-validation . because cross-validation relies on a random split of the training data into folds , there is a certain randomness to the estimated roc <dig> curve. to provide more accurate results, we performed cross-validation ten times, each time with a different  20-fold split of the data . each 20-fold split results in a slightly different roc <dig> curve. in some cases, we plot all ten of these curves, to give an idea of the uncertainty in classifier performance . in cases where this would result in overly cluttered graphs, we simply present the average of the ten roc <dig> curves .

we generated roc <dig> curves for each supervised learning method combined with various amounts of dimensionality reduction. using pca, we reduced the original  <dig> dimensions to  <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> dimensions, respectively. in this way, the original data set was transformed into six separate data sets of various dimensions. each basic learning method  was applied to the original data set and to each of the six reduced data sets. thus, for each basic learning method , we trained and tested seven different classifiers. in the case of knn, we used only the original, unreduced data, but with five different values of k. altogether, we trained and tested a total of  <dig> ×  <dig> +  <dig> =  <dig> different classifiers. figures  <dig> to  <dig> show the estimated performance of these basic classifiers. each figure shows a number of roc <dig> curves, each derived using cross-validation averaged over a number of random splits of the data, as described above. unlike traditional roc curves, the axes of these curves give the number of true and false positives, instead of the proportion. the red dash-dot line near the bottom of each figure shows the expected performance of a random classifier . the roc <dig> scores for the curves are shown in the legend of each figure.

as the figures show, in some cases the classifiers perform not much better than random, but in most cases they perform significantly better. the figures also show that the performance of each classification method depends heavily of the amount of dimensionality reduction used. notice in particular that in some cases, the classifier trained on the reduced data has a much higher roc <dig> score than the classifier trained on the original, unreduced data. this is especially true for nb and qda. for instance, the classifiers trained on the original data have low roc <dig> scores of  <dig>  for nb and  <dig>  for qda. this is comparable to the random classifier, whose roc <dig> score is  <dig> . however, reducing the dimensionality of the data to  <dig> increases their roc <dig> scores to  <dig>  and  <dig> , respectively. this shows the importance of dimensionality reduction. in contrast, knn performs well for all the values of k that we used.

improving prediction accuracy by combining classifiers
combining different classifiers in prediction can be thought of as combining different opinions in decision making. the advantage is that a group opinion is better than a single opinion if the single opinions are correctly weighted and combined. in machine-learning systems, classifiers are often combined by weighted voting, in which the discriminant value of the combined classifier is a linear combination of the discriminant values of the individual classifiers. formally, given a set of basic classifiers, f^ <dig> …,f^m
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgmbgzgaqcamaabaaaleaacqaixaqmaeqaaogaeiilawiaes47iwkaeiilawiafmozaymbakaadawgaawcbagaemyta0eabeaaaaa@3599@, and a set of weights, w <dig>  …, wm, the combined classifier, f^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgmbgzgaqcaaaa@2e11@, is defined by the equation f^=∑mwmf^m
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgmbgzgaqcaiabcicaogqabiab=hha4jabcmcapiabg2da9maaqababagaem4dac3aasbaasqaaiabd2gatbqabagccuwgmbgzgaqcamaabaaaleaacqwgtbqbaeqaaogaeiikagiae8heagnaeiykakcaleaacqwgtbqbaeqaniabgghildaaaa@3ec3@. in our case, m =  <dig>  as described above.

by judiciously choosing the weights, w <dig>  …, wm, the performance of the combined classifier can be maximized. various methods are available for doing this, such as model averaging and stacking  <cit> . using these methods on our data sets, we found that the roc curve of the combined classifier was usually better than the roc curves of the basic classifiers, as expected. unfortunately, we also found that the roc <dig> curve of the combined classifier was usually worse . we hypothesized that this is because our data sets are highly unbalanced. intuitively, model averaging and stacking try to choose weights so as to correctly classify as much data as possible. in our case, this means trying to correctly classify the vast number of negative samples in our data sets, even if this means misclassifying the small number of positives. in other words, these methods try to minimize the total number of false positives, even though we only care about the first fifty.

to choose appropriate weights for our combined classifier, we used the heuristic that classifiers that perform well should be given more weight than classifiers that perform poorly. in our case, since we want to maximize the roc <dig> score of the combined classifier, we want to give high weight to classifiers with high roc <dig> scores. there are many ways to do this, but we found that it was sufficient to estimate and normalize the roc <dig> score of each basic classifier, and use this as its weight. that is, we used wm=s^m/∑ms^m
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwg3bwddawgaawcbagaemyba0gabeaakiabg2da9iqbdohazzaajawaasbaasqaaiabd2gatbqabagccqggvawldaaeqaqaaiqbdohazzaajawaasbaasqaaiabd2gatbqabaaabagaemyba0gabeqdcqghris5aaaa@3b09@, where s^m
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgzbwcgaqcamaabaaaleaacqwgtbqbaeqaaaaa@2fba@ is an estimate of the roc <dig> score of classifier fm. note that with these weights, if each f^m
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgmbgzgaqcamaabaaaleaacqwgtbqbaeqaaaaa@2fa0@ is a number between  <dig> and  <dig> , then so is f^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgmbgzgaqcaaaa@2e11@. also, this method automatically gives low weight to classifiers that use an inappropriate amount of dimensionality reduction, since such classifiers have low roc <dig> scores. in this way, the combined classifier incorporates not only the best combination of supervised learning methods, but also the best amounts of dimensionality reduction for each method.

to train and evaluate the combined classifier, we used two sets of validation data. after the basic classifiers were trained, one validation set was used to estimate their roc <dig> scores. the combined classifier was then constructed using these scores, and the second validation set was used to estimate its roc <dig> curve. thus, the validation data for the basic classifiers is part of the training data for the combined classifier. to do this in a cross-validation setting, we used what amounts to nested cross-validation . as shown in figure  <dig>  the resulting combined classifier has a higher roc <dig> score than any of the basic classifiers from which it is made.

roc and roc <dig> curves plot the number of true positives against the number of false positives. however, in applications such as ours, the precision is also of interest. precision is the proportion of true positives  among the predicted positives .  precision is important since each prediction is a potential experiment, and as a matter of economics, a biologist needs an estimate of how many of the experiments will succeed. this is especially important in situations, such as ours, where the number of real negatives is much greater than the number of real positives, and so there is a real possibility of having a huge number of failed experiments.

stress-response predictions
we trained the combined classifier on our arabidopsis data set, using all  <dig>  genes for principal components analysis, and the  <dig>  annotated genes for supervised learning, as described above. we then applied the classifier to the  <dig>  unannotated genes, to get a set of  <dig>  predictions . table  <dig> shows the top fifty predictions. each row in the table is a prediction: the first  entry is the rank of the prediction ; the second entry identifies a gene; the third entry is a discriminant value ; and the fourth entry is the estimated precision of the prediction and all predictions above it . as an example, consider the 23rd row of the table, the row for gene at1g <dig>  since the estimated precision in this row is given as  <dig> , we expect that about 70% of the top  <dig> genes respond to stress, i.e.,  <dig> genes.

pr, estimated precision; dv, discriminant value.

figures  <dig> and  <dig> provide visual evidence supporting these predictions. each figure shows a heat map. these maps, known as "electronic northerns" , were generated using the expression browser tool of the botany array resource  and the atgenexpress stress series  data set <cit> . the program contains expression data for more than  <dig>  genes across more than  <dig> samples collected from nascarrays, atgenexpress consortium, and the department of botany at the university of toronto  <cit> . each row in an e-northern is a gene, and each column is an experiment. the colour at a point represents the relative expression level of the gene during the experiment. more specifically, the colour represents the log <dig> of the ratio of the average of replicate treatments relative to the average of corresponding controls. yellow means that under the experimental conditions, the gene had the same expression level as the control.  red means that the gene had a higher expression level than the control , and blue means it had a lower expression level . a gene that shows significant up-regulation  under stress conditions is likely to be involved in response to stress. thus, unlike cross validation, electronic northerns provide a means of evaluating the quality of predictions based on the prediction data, not just the training data. the e-northerns of figures  <dig> and  <dig>  for instance, are based entirely on prediction data. in these e-northerns, the experiments exposed the plant to various stress conditions, such as heat, cold, drought, uv-b radiation, etc. figure  <dig> is the e-northern for the top- <dig> predictions of our combined classifier, i.e., for the  <dig> genes predicted to most likely to respond to stress. for comparison, figure  <dig> is the e-northern for  <dig> genes chosen at random from the prediction set. note that there is much more colour in figure  <dig> than in figure  <dig>  especially red. this suggests that our combined classifier has indeed extracted meaningful gene expression patterns for genes that respond to stress.

gene knockout experiments
from the predictions of the combined classifier, three genes were chosen for biological analysis using gene knockout experiments. here, we present the results for one of these genes, at1g <dig>  which show it to be necessary for the normal response to temperature and nacl. our results also confirm that the other two genes, at1g <dig> and at4g <dig>  are involved in a variety of stress responses .

the criteria used to choose candidate genes for subsequent biological analysis were: 1) the gene must be expressed in either root or shoot, 2) gene expression should be strongly increased in response to abiotic stress, such as cold, drought, osmotic and salt stresses, 3) t-dna knockout lines – in which a given gene's expression has been eliminated – should available from the salk institute  <cit> , and 4) the gene should not have an annotated function nor be present in any patent database. further bioinformatics analysis was performed using athena for promoter motif prediction  <cit> , expression angler for co-expressed gene analysis  <cit>  and efp browser for electronic representation of gene expression patterns  <cit> .

stress response
the increased presence of anthocyanin levels in plants lacking a functional copy of the at1g <dig> gene during cold stress of 4c indicates that this gene is involved in cold stress response . the same effect is seen at 30c, indicating that this gene is also associated with response to heat stress . interestingly, at1g <dig> is normally expressed during the later stages of seed maturation, towards seed dessication, and hence may play a role in seed dormancy. this sort of bifunctionality is seen with other stress response genes, which have documented roles in the cold, heat and salt stress pathways, e.g. rd29a  and lea  protein  <cit> . these proteins have also been found to accumulate during seed maturation  <cit>  and are in fact co-expressed with at1g <dig> under stress conditions and during seed maturation, as determined using the expression angler algorithm  <cit> .

in addition to modulating a response to temperature, plants lacking a functional at1g <dig> exhibit a defective root growth phenotype under increasing salt concentrations . this phenotype, combined with previous microarray studies  <cit> , which found at1g <dig> induction at  <dig> mm nacl, gives clear indication that at1g <dig> is also part of the salt stress response pathway.

CONCLUSIONS
in this study, we evaluated and compared five basic supervised learning methods  for gene function prediction in a. thaliana based solely on gene expression data. the major advantage of supervised methods over unsupervised methods is that by including prior knowledge of class information, supervised methods can ignore uninformative features and select informative features that are useful for separating classes. in this study, we focussed on finding genes that respond to stress, as represented by the term go: <dig>  in the gobp hierarchy. using a training set of genes of known function, we used the basic learning methods to predict the stress response of genes of unknown function. we estimated the accuracy of the predictions using roc <dig> scores derived through cross validation. we found, for instance, that knn performs well for various values of k. for the other learning methods, the performance depends greatly on whether the data is preprocessed using pca, and on how much its dimensionality is reduced. using various values of k and various amounts of dimensionality reduction, we trained and tested a total of  <dig> basic classifiers.

we also investigated combining the basic classifiers using weighted voting. our method of constructing the combined classifier chooses not only the best combination of supervised learning methods, but also the best amount of dimensionality reduction for each method. our results show that the combined classifier outperforms all the basic classifiers in predicting whether a gene responds to stress. this can be attributed to the relative robustness of methods for combining classifiers. intuitively, any single learning method represents a single view of the data, while a combination method represents multiple views strategically combined. the proper choice of combining method is important to the success of a combined classifier. for example, model averaging and stacking are well-known methods for combining classifiers  <cit> ; however, we found that while they did improve on the overall roc curves of the basic classifiers, the roc <dig> curve was often worse . in contrast, our weighted voting method using roc <dig> scores as weights is simple, provides improved accuracy in predicting stress response in a. thaliana, and we would expect it to provide improved accuracy in other organisms and for other gene functions.

using electronic northern analysis, we observed significant up-regulation and down-regulation of many of our predictions. the strong up- and down-regulation are also present among the stress-response genes in the training data . in contrast, randomly selected genes show much less up- and down-regulation. this visually confirms that the combined classifier is able to distinguish between stress and non-stress genes. moreover, unlike cross-validation, this confirmation is based on the prediction data, not the training data.

using gene knockout experiments – in which a given gene's expression is eliminated – we tested three of our predictions. we presented the results for one of these genes, at1g <dig>  which show it to be involved in the stress response pathways to cold , chill  and nacl. we have also confirmed the biological stress responsive roles of the other two genes, at1g <dig> and at4g <dig> . further biological studies will determine the pattern of expression in specific cell and tissues types of the plant and the exact physiological role of these genes.

