BACKGROUND
technological advancements in sequencing technologies have enabled fast sequencing of millions of reads at a rapidly reducing cost. sequencing-by-synthesis, including illumina’s platforms based on reversible terminator chemistry and 454’s pyrosequencing platforms, is taking us closer to affordable routine sequencing tasks. however, inherent uncertainties in the ensemble-based sequencing-by-synthesis, along with data acquisition noise, present a major bottleneck in the quest for reads that are as long and reliable as those provided by the conventional sanger sequencing method.

sequencing-by-synthesis on illumina’s platforms typically involves the following steps. first, multiple copies of the genome being sequenced are broken into short fragments in a random fashion, followed by ligation of sequencing adapters to the fragments. in the next phase, the dna sample is introduced into one of the  <dig> lanes  split into multiple tiles each containing a lawn of primers covalently bonded to the surface to generate clonal clusters of the captured forward and reverse dna strands. in particular, captured strands hybridize to neighboring primers to form so-called u-shaped bridges, followed by the process of bridge amplification which is repeated ≈  <dig> times to generate clusters containing ≈  <dig> molecules. in the final stage, “sequencing”, fluorescently labeled reversible terminators  are introduced and incorporated into the complementary strands of the dna templates. imaging of the fluorescently labeled clusters is followed by cleavage and unblocking of the incorporated nucleotides, and the labeled reversible terminators are added anew to proceed with synthesis of the complementary strands.

images acquired at the end of each sequencing cycle are processed, and the resulting signals are analyzed to determine the type of nucleotides incorporated into the complementary strands. the problem of inferring the order of nucleotides in a template from the noisy signals is referred to as base calling. for base calling, illumina uses its in-house software bustard. fundamentally, bustard attempts to reverse the effects of various uncertainties on the signal and then makes base calls. while it is computationally very fast, the error rates resulting from bustard’s calls may be significantly improved by more sophisticated algorithms  <cit> .

several base calling strategies for the illumina platform have been proposed in recent years such as  <cit> . in  <cit> , a parametric model of the sequencing-by-synthesis process was proposed, and a monte carlo method for base calling was presented. while having better performance than bustard, this scheme is computationally intensive and impractical for processing tens of millions of reads generated by today’s sequencers. kao and song  <dig>  <cit> , the follow-up paper, suggested a modification leading to a computationally feasible base calling albeit with slight degradation in performance compared to  <cit> . in  <cit> , an algorithm achieving significant improvement in speed at the cost of a minor deterioration of base calling error rate as compared to  <cit>  was presented.

following a simplification of the parametric model of illumina’s sequencing-by-synthesis platform proposed in  <cit> , in this paper we present a formulation of the base calling problem amenable to being solved by dynamic programming methods. in particular, we derive the forward-backward and soft-output viterbi algorithm  for solving the base calling problem. the performance of the proposed algorithms is demonstrated on experimental reads acquired from illumina’s genome analyzer ii and hiseq <dig> and compared with several recent base calling techniques.

methods
in this section, we present the mathematical model that leads to the dynamic programming formulation of the base calling problem, and present algorithms for base calling and parameter estimation.

comprehensive model of sequencing-by-synthesis in illumina’s platforms
a detailed mathematical model of illumina’s platforms which describes various sources of uncertainty in the sequencing process and data acquisition step was introduced in  <cit> . for the sake of self-contained presentation, we briefly review this model here while omitting details for brevity.

the 4-dimensional observation vector yi  acquired in the ith cycle  of the sequencing-by-synthesis process can be expressed as 

  yi∼ni= <dig> yi|yi−1∼ni= <dig> ....n, 

where k is the 4× <dig> cross-talk matrix quantifying overlap of the emission spectra of the four fluorescent tags used to label nucleotide bases, α is the parameter accounting for empirically observed signal leakage between consecutive cycles, xi is the signal generated in the ith cycle, and Σi=∥xi∥22Σ is the variance describing multiplicative measurement noise that primarily originates from fluctuations in k. the generated signal, xi, is affected by phasing and pre-phasing. in an ideal setting, addition of the four terminating base nucleotides during the sequencing step should lead to a single base getting incorporated into each of the complementary strands. however, nucleotide incorporation is not perfect and phasing  or pre-phasing  may occur. these effects are modeled probabilistically: it is assumed that no base is incorporated with probability pii, while with probability pcf more than  <dig> base is incorporated. for the sake of tractability of the final model and practical feasibility of base calling, we assume that at most two bases may be incorporated into a complementary strand in a single cycle. define an × transition matrix p with entries 

  pi,j=piiifj=i,ifj=i+ <dig> pcfifj=i+ <dig> otherwise, 

where pi,j is the probability of a complementary strand extending from length i to length j in any given cycle. then the signal generated in the ith cycle, xi, can be expressed as 

 xi=λizi=λii, 

 where s is a 4×l matrix  representing the template sequence of length l, e is an n×l matrix having entries ei,j= <dig> j equal to the probability that the length of a strand after i cycles is j, and λi is a random variable describing empirically observed signal decay caused by the dna loss due to primer-template melting, digestion by enzymatic impurities, dna dissociation and misincorporation, 

  λi∣λi−1∼nλi− <dig> λi−12σ2), 

where d is a constant droop factor over all cycles and all reads and σ is the standard deviation of λi.

illumina’s base calling approach
prior to base calling, bustard  estimates cross-talk using signals generated by synthesizing the first  <dig> bases of all reads, evaluating entries of k as the median of the estimates obtained using individual read signals. bustard then infers x by inverting k and multiplying it with y. next, it calculates a tile-wide average scalar x¯i=∑xj,i and renormalizes the signal by multiplying xi by x¯1/x¯i. this procedure corrects for the signal droop. matrix e is estimated from the first  <dig> bases, inverted and multiplied by the normalized xi values. this compensates for phasing/prephasing. finally, for each cycle, base calling is performed by selecting the base inferred as having the highest corrected signal.

parameter estimation and basecalling approach of bayescall and naivebayescall
bayescall relies on the comprehensive model reviewed in this section to perform base calling and significantly reduce error rates compared to bustard. however, it suffers from two major computational bottlenecks. first, the lack of a closed form expression for the solution to the e-step of the em algorithm used for parameter estimation necessitates a computationally intensive numerical optimization. hence, the parameter estimation stage is time consuming, requiring ≈ <dig> minutes per iteration on an  <dig> core machine. consequently, bayescall performs a single parameter estimation step that uses reads from all the tiles in a lane to generate a single set of parameters for the entire lane. detailed analysis of bayescall and naivebayescall error rates indicates that using a single set of parameters for an entire lane results in serious performance degradation for tiles where the parameters significantly differ from the ones used by the base calling algorithms . moreover, base calling in bayescall is performed by relying on simulated annealing. being a computationally intensive algorithm, the times for base calling via simulated annealing are prohibitively high. in order to overcome this issue, in the follow-up paper  <cit> , the authors propose a simplified heuristic which reduces base calling times to ≈ <dig> hours per lane with a small reduction in performance compared to bayescall. however, since the parameter estimation step used by naivebayescall is the same as the one used by bayescall, tiles with parameters which significantly differ from the single parameter set computed for the entire lane have very small performance improvements over bustard.

our model refinements for fast tractable base calling
while providing detailed description of various sources of uncertainty, mathematical model of the sequencing process overviewed in the previous section leads to computationally demanding base calling algorithms. to simplify the model and enable practically feasible base calling, we approximate λi by its mean. such an approximation is justified by the analysis of experimental data which shows that the coefficient of variation  of λi in  is small   <cit> . on the other hand, it is desirable that the model allows variations in the droop factor from one cycle to another. therefore, we describe the decay as λi=λ∏j=2i, where λ denotes a read-dependent transduction coefficient mapping synthesis events to the generated signal intensity, and d¯j denotes cycle-dependent droop factors.

note that the signal generated in the ith cycle of the sequencing step, i, can be expressed as 

  i=∑j= <dig> j≠ilβi,jsj+si, 

where βi,js are dependent on pii and pcf. based on the initial parameter estimates obtained using monte carlo methods, we observe that pii is very small. it is also observed that if we choose to retain only those terms in a given row of e that are at least 10% of the maximum entry, there is at most one base ahead of the tested base that contributes significantly to the signal xi. consequently, we may approximate i in  as 

  i≈si+βi,i+1si+ <dig>  

where βi,i+ <dig> is a cycle-dependent parameter which allows us to essentially approximate the nonlinear dependence of e on pcf and hence facilitate efficient base calling.

for any given cycle i, the intensity of the signal in  is a function of si and si+ <dig>  such a finite memory approximation enables search for the optimal path s <dig> s <dig> …,sl using dynamic programming principles. graphically, this can be interpreted as the search on a 16-state trellis , where the states at the ith stage of the trellis represent all possible pairs of bases in the ith and th position of a read. we denote the states of the trellis by ti, where i is the cycle number. the states can take one of  <dig> possible values in the set {aa,ac,ag,…,tt}, 1≤j≤ <dig>  note that not all state transitions are feasible. in particular, a transition from a state in cycle i to a state in cycle i+ <dig> is valid if the second symbol of the state in cycle i is the same as the first symbol of the state in cycle i+ <dig>  figure  <dig> illustrates two consecutive stages of the trellis. for the sake of tractability of the illustration, only  <dig> of the possible  <dig> states are shown. arrows indicate valid transitions between the states that are included in the illustration.

final model
based on the discussion in the preceding section, the final model  is of the form 

  yi∼ni=1yi|yi−1∼nkixi+αiyi− <dig> )2∥xi∥22Σi)i= <dig> ....n 

where xi=si+βisi+1), and βi,i+ <dig> is relabeled as βi for the simplicity of notation. let us collect all the parameters into a vector Θ. given Θ, λ and y <dig> y <dig> …,yn, the goal of base calling is to determine s <dig> s <dig> …,sl. in our approach, we first obtain estimates of the parameters Θ using an unsupervised learning scheme. then, posterior probabilities of si are determined using either forward-backward or soft-output viterbi algorithm. details of the proposed scheme follow.

parameter estimation
we infer parameters of the mathematical model  by relying on an unsupervised estimation scheme. unsupervised estimation need not be aided by a reference genome nor does it require analyzing a known sequence in a control lane. our scheme also has the advantage of being implemented as an online, as opposed to a batch, algorithm. this allows parameter estimation  of a previous window to be performed while the experiment is still in progress, resulting in smaller latency between the end of the run and basecalling results. in particular, we employ the online expectation-maximization  algorithm  <cit>  which relies on a training set of r= <dig> reads randomly selected across a tile. the optimization problem that the em algorithm solves in an iterative fashion can be stated as 

  Θn=argmaxΘe|Θn− <dig>  

where the scalar coefficient λ and the template sequence matrix s are latent variables, Θ={α,β,k,Σ,d¯j} is the set of parameters which need to be determined, and logp denotes the log-posterior function. in the absence of any prior information, this is also the log-likelihood function. the expectation in  needs to be evaluated with respect to λ and s given the current estimates of Θ.

the results from  <cit>  indicate that base calling may be significantly improved by allowing the parameters to be cycle dependent. we observe the same and thus divide a sequencing run into windows of length w= <dig>  and estimate model parameters window-by-window. parameters for window l are initialized using the values of the parameters estimated in the previous window, l− <dig>  to prevent over-fitting,  is optimized over two windows, l and l+ <dig>  and the resulting Θ is used as the set of parameters for window l. a window length w= <dig> was found to be short enough to capture time variations in the parameters and still maintain run times of the parameter estimation and base calling low.

initialization for the first window
the em algorithm requires initialization of the parameters in the first time window. we can reliably call the first two bases in a template by simply identifying the channels having the largest signal in the first two test cycles from which an initial estimate of k is obtained. as done by bustard, multiple estimates of the columns of k can be computed from the first two signals of each read in a tile, and then the median of all these estimates can be used to provide an initial estimate k^. subsequently, we find the mean of each column and add this to the diagonal entries. we then use the inverse of the resulting matrix to call bases again and iteratively refine the estimates of the entries of the matrix. the number of iterations is set to  <dig> 

given k^, an empirical estimate of σ is obtained by computing the difference between the intensity vector and the column of the cross talk matrix that corresponds to the called base. the covariance matrix Σ^l is computed from this for each read. finally, the estimate Σ^ is formed as the median of Σ^l. parameters αi and βi are negligible in the early cycles and are initialized as zeros. to estimate droop coefficients d¯i, we start by calculating k^−1yil for each read and summing up the resulting vector elements to obtain the total signal xil acquired in the ith cycle. the droop for the ith cycle is then calculated as d¯^il=xil/xi−1l for each individual read. finally, the median value of all d¯^il is chosen as the initial value of d¯i. details of this step are omitted and the interested reader is referred to  <cit> .

e-step for the first window
the e-step requires finding the expectation of the log-likelihood function in  over λ and s. closed form expressions are not available, while the numerical monte-carlo methods are computationally prohibitive in practice. as an alternative, we rely on bustard’s approach to call sequences in the training set and use the resulting s^k, 1≤k≤r, to approximate the expectation with respect to s. in particular, we approximate the objective of maximization  by 

  o=−∑k=1reλk∑i=w+1w12lλk,Ŝik,Θl, 

where 

  l=logdetλk2∥xi∥2Σi)+yik¯−λk∏j=2ikixiktΣi−1kixik))2∥xi∥ <dig>  

and y¯i=yi for i= <dig> and y¯i=yi−αyi− <dig> for i> <dig> i<=n. the superscript k is an index of a read in the training set and ranges from  <dig> to r. then the expectation over λk in  is evaluated numerically via importance sampling, leading to an approximation of the objective function 

  o≈−12∑k=1r∑i=w+1w∑j=1niswj,kl, 

where wj,k denote normalized weights of nis= <dig> samples λjk generated for each read in the training set from the gaussian distribution n . the mean of the sampling distribution for each read in the training set, λ^k, is obtained by maximizing the log-likelihood function  given the current estimates of the parameters Θ and base calls s^k.

m-step for first window
the objective function in  is separately differentiable and convex over each of the parameters in Θ except β. to optimize it, we rely on a cyclic co-ordinate descent scheme which rotates among the components of Θ. to find β, we employ a grid search. the co-ordinate descent is terminated when the ratio of the change in the value of the objective function to the value of the objective function in a previous iteration is less than ε = <dig> . we use a similar stopping criterion for termination of the expectation-maximization algorithm.

e-step for subsequent windows
due to phasing effects and other imperfections affecting generated and measured signal, using bustard’s calls to approximate expectation of the log-likelihood function as in  fails to provide reliable parameter estimates in subsequent windows. on the other hand, numerical evaluation of the objective function in , e|Θn− <dig>  as we already argued in this section, is computationally prohibitive in practice. to facilitate practically feasible evaluation of the e-step for windows l> <dig>  for each read in the training set we approximate the transduction coefficient λk by its mean, λ^k, and replace the objective function by 

  ∑k=1r∑i=1mplog), 

where λ^k is obtained by maximizing the log-likelihood function  given the parameters inferred in the st window. posteriori probabilities p needed to evaluate expression  can be found from the state posteriori probabilities. for instance, posteriori probability that the ith base is a is 

  p=∑j=14p, 

where tj∈{aa,ac,ag,at,ca,…,tt}, 1≤j≤ <dig>  clearly, we need to find p. for this, we turn to dynamic programming ideas – in particular, the forward-backward and soft-output viterbi algorithms.

forward-backward algorithm
denote the transition probability from state k in the ith stage to state l in the th stage of the trellis by akl=p. if no prior information about transition probabilities is available, we will assume that the valid transitions are equally likely. moreover, note that the state priors may be computed from the symbol priors, if those are available. for instance, prior for the state ti=ac can be found as the product of the priors for sit= <cit>  and si+1t= <cit> . let fl=p denote the so-called forward probabilities, and bl=p denote the backward probabilities. moreover, let el=p denote emission probabilities. then the recursion that computes forward probabilities can be stated as 

 fli+1=elyi+1∑k=116fkiakl, 

 while the backward recursion is given by 

 bki=∑l=116elyi+1aklbli+ <dig>  

 the recursions are initialized by setting f0= <dig> and bk=ak,e, where ak,e denotes the probabilities of the terminating state as computed by the forward algorithm. finally, the posterior probability is obtained as 

 pti=tk|y,λ,Θ=fkbk∑j=116fjbj, 

 for all 1≤k≤ <dig>  1≤i≤m. in order to ensure that the finite size of the trellis does not adversely effect reliability of the computed probabilities, we add an extra  <dig> cycles in the calculations .

soft output viterbi algorithm
the forward-backward algorithm computes exact posteriori probabilities of the bases in a sequence. on the other hand, one can rely on various heuristics to obtain reasonably good approximations of posteriori probabilities while suffering only minor degradation in accuracy. such heuristics include the soft-output viterbi algorithm , a modification of the viterbi algorithm implemented on the same trellis we described in previous sections.

let vk denote the probability of the most likely state sequence which ends at ti=tk, i.e., 

 vki=maxt <dig> …,ti−1py <dig> …,yi,t <dig> …,ti− <dig> ti=k. 

 retaining the notation introduced for the description of the forward-backward algorithm, we can recursively compute vk as 

 vl=elmaxkaklvk, 

 where the recursion is initialized by setting v0= <dig>  vk= <dig> for all k> <dig>  this recursion is at the core of the viterbi algorithm, which then proceeds by backtracking through the optimal trellis path to determine the most likely sequence of states. the viterbi algorithm, however, provides only the most likely sequence of states and does not find posteriori probabilities of the symbols. to this end, a soft-output variant of the viterbi algorithm was proposed in  <cit> . sova traces back optimal path through the trellis and for each symbol  explores alternative paths that could have changed the decision of the viterbi algorithm for that symbol. cost metrics of the alternative paths are then used to approximate posteriori probabilities for the base under consideration. to allow computationally efficient procedure, we limit the length of deviation of the alternative paths from the optimal one to  <dig> edges. note that it is necessary to normalize the posterior probabilities obtained in the described fashion. as we will demonstrate in the subsequent sections, the forward-backward algorithm achieves better base calling error rates than sova, but it does so at the cost of having reduced speed.

m-step for subsequent windows
the m-step for subsequent windows is very similar to the m-step for the first window. the only difference is that the objective function being maximized is now 

  −12∑k=1r∑i=w+1w∑j=14pl. 

the optimization follows the same procedure as described for the first window.

updatingλ^k - after each step of the em algorithm used for estimating parameters in a given window, we make calls for sik . the calls and the most recent parameters are then employed to update λ^k by maximizing the log-likelihood function . the updated value of λ^k is used by the em algorithm in the next window.

base calling
given Θ inferred by the em algorithm and y <dig> y <dig> …,yn, the goal of base calling is to determine s <dig> s <dig> …,sl, i.e., to find 

  Ŝi=argmaxsjp, 

where sj can take values of unit vectors comprising three zeros and one non-zero entry equal to  <dig>  and 1≤j≤ <dig>  base probabilities p can be calculated from the state probabilities of the trellis that we defined in the parameter estimation section, e.g., 

  p=∑j=14p, 

and so on. note that these probabilities are also the ‘quality score’ assigned to the given basecall . clearly, we need to find posteriori probabilities p. for this, we again turn to the soft-output viterbi and forward-backward algorithms that we described in the previous section.

note that the value of λ^ used for base calling in window l is approximated by the value of λ which maximizes the log-likelihood function formed using Θ and Ŝi from the previous window, l− <dig> . it is straightforward to show that this maximization entails solving the quadratic equation in λ

  ∑i=w+1lw4λ2+tΣi−1)x^i2λ−yi¯Σi−1yi¯)2x^i2= <dig>  

and choosing the positive solution as the value of λ^.

quality scores
performance of various base calling algorithms can be compared by evaluating error rates that they achieve when applied to determining the order of nucleotides in a known sequence. in practical applications, where the sequence being analyzed is not known, we need to assess the confidence of a base calling procedure. to this end, quality scores provide information as to how reliable the corresponding base calls are. the quality scores that we assign to base calls are the posterior probabilities of the bases computed by the forward-backward/sova schemes. in particular, we use the posteriori probabilities of the bases computed according to  as the quality scores. in order to assess the ‘goodness’ of quality scores, we consider their discrimination ability  <cit> . the discrimination ability for a given error rate is obtained by sorting all bases according to their quality scores in descending order and finding the number of bases called before the error rate exceeds the predefined threshold.

RESULTS
gaii
performance of the forward-backward algorithm and sova is verified on a full lane data obtained by sequencing phix <dig>  bacteriophage using illumina’s genome analyzer ii which generates reads of length  <dig>  after basecalling the lane by bustard, naivebayescall, rolexa, ibis, forward-backward and sova, the calls were mapped onto the known reference sequence comprising  <dig> bases. the optimal alignment is found using a hamming distance metric. reads that map with less than 30% errors are retained while reads having more errors are removed to ensure that there is no ambiguity in the alignment. this results in approximately  <dig> million reads and  <dig> million bases which are used to compare the performance of the considered basecalling schemes. average error rates computed over the entire lane are compared in table  <dig>  figure  <dig> shows the by tile error rates, by cycle error rates and the discrimination abilities of the different basecallers. forward-backward algorithm and sova outperform all other schemes in terms of error rates and discrimination abilities.

a comparison of error rates and running times  for different base callers .

hiseq
performance of the forward-backward algorithm and sova is verified on reads from e.coli  using illumina’s hiseq <dig> comprising of  <dig> cycle paired end data. the error rates for both pairs of reads are shown as a function of cycle number in figure  <dig>  average error rates are compared in table  <dig> for both sova and fb schemes. as can be seen, we improve on bustard’s calls by  <dig>  and  <dig> % for the first and second pair respectively.

a comparison of error rates for different base callers for hiseq.

discussion
computational complexity
for each read, the most computationally expensive bustard’s step is its correction of phasing effects. for both forward-backward algorithm and sova, we need to evaluate  <dig> objective functions for the states at each stage of the trellis. in order to avoid finite window effects, for each window of length  <dig> additional  <dig> cycles are included in the computations. therefore, for a  <dig> cycle read, we need to evaluate 131× <dig> state values. additional overhead due to combining these values requires mostly additions . naivebayescall on the other hand, performs matrix inversion of the same complexity as those performed by bustard, followed by evaluation of 4×76× <dig> terms. the factor  <dig> arises due to the fact that naivebayescall needs to solve a quartic equation using a golden section search that requires  <dig> evaluations per base. thus, forward backward and sova are ≈ <dig> times faster than naivebayescall.

implementation and running times
we implemented our codes on an intel i <dig> machine @ <dig> ghz using only a single core. with our codes written in c, it takes approximately  <dig> seconds to read in an intensity file, perform the parameter estimation step on  <dig> reads, call bases for the whole tile and write it in fastq format for the forward backward scheme and  <dig> seconds for sova. processing an entire lane requires about  <dig> minutes and  <dig> minutes for fb and sova, respectively. naivebayescall, on the other hand, requires  <dig> hours just for its parameter estimation step while its basecalling takes  <dig> hours. thus, our fb and sova implementations are  <dig> and  <dig> times faster than naivebayescall. note that the run times of naivebayescall are reported for an implementation on a processor with  <dig> cores; it is expected that a parallel implementation of our algorithm would reduce the total running time by roughly  <dig> times. in addition, our proposed schemes would be able to almost instantaneously provide very high quality base calls to the end user since they are online  in nature. a comparison of the running times for processing an entire lane between the forward-backward algorithm and sova and the other basecallers is shown in table  <dig> 

improving error rates using supervised parameter estimation
although the described parameter estimation procedure assumes no supervision, the proposed forward-backward and sova schemes allow incorporation of non-uniform priors that may improve accuracy of the inferred parameters and hence the overall base calling performance. illumina platforms typically have a dedicated control lane comprising reads from a known reference. in such a case, it is possible to obtain priors by aligning the reads onto the reference and using them to improve the accuracy of the estimated parameters.

to this end, we utilize the calls from bustard and align the reads onto the reference phix <dig> genome using the same mapping criteria as described in the results section. a basecall that is perfectly mapped to the reference is assigned a prior probability of  <dig>  while in case of a mismatch the prior probabilities are split between the base suggested by the reference and the base called. if the reference is not very trustworthy, lower prior can be assigned to the base implicated by the reference. the described change requires very minor modification of the parameter estimation step. table  <dig> shows the improvement obtained using the supervised scheme. both forward-backward and sova schemes benefit marginally if the parameters are estimated in the supervised setting.

a comparison of error rates for supervised and unsupervised schemes on a single tile for gaii.

CONCLUSIONS
we presented a formulation of the base calling problem on illumina platforms that is amenable to being solved by dynamic programming methods, and proposed forward-backward and soft-output viterbi algorithms for solving it. base calling error rate performance of the proposed algorithms was demonstrated on experimental data to be superior to illumina’s bustard and several other publicly available base callers. the developed base callers are tested on data obtained by genome analyzer ii and hiseq <dig> but the model, concepts, and algorithms should apply to other illumina’s platforms as well. the developed schemes are online , scalable, and much faster than competing model-based base callers. in addition, they are capable of accounting for soft inputs  and generating soft outputs  – a feature we exploited to devise a supervised scheme for learning parameters of the sequencing model, and may further be useful in applications where prior knowledge about reads is available.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
algorithms and experiments were designed by s das  and haris vikalo . algorithm code was implemented and tested by sd. the manuscript was written by sd and hv. both authors read and approved the final manuscript.

