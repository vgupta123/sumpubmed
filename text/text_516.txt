BACKGROUND
rna viruses such as foot-and-mouth disease virus  evolve rapidly due to their large population size, high replication rate and the poor proof-reading ability of their rna-dependent rna polymerase. mutation rates of rna viruses are cited to be between 10− <dig> and 10− <dig> mutations per nucleotide per transcription cycle  <cit> , therefore mutations can potentially be introduced every time a viral genome is replicated. as a result, these viruses exist within their hosts as large, complex and heterogeneous populations, comprising a spectrum of related but non-identical genome sequences  <cit> . the capacity of rna viruses to evolve rapidly is a key driver of viral virulence, vaccine resistance and host-jumping . therefore, an understanding of the mutational dynamics of rna viruses is essential for our understanding of viral disease progression, transmission and the development of antiviral therapeutics  <cit> .

next generation sequencing  techniques provide the means for rapid and cost-effective dissection of viral evolutionary dynamics at an unprecedented level of detail  <cit> . the massively parallel and high-throughput nature of ngs platforms, combined with the relatively short genome of an rna virus, enables the analysis of viral samples  to a very high depth. such ultra-deep coverage of the genome enables the diversity of the whole viral population to be examined and subsequently compared between samples to investigate evolutionary events such as selection and bottlenecks. furthermore, high depth enables the identification of important variants present within the viral population at low frequencies, such as those that increase pathogenicity or convey drug resistance  <cit>  which would pose a risk if they become dominant in later populations due to selection. however, a current problem with the application of ngs platforms to viral population analysis is that true low frequency viral variants cannot be effectively distinguished from variants caused by errors during sample preparation or sequencing.

viral samples typically have to undergo substantial sample processing before sequencing which introduces errors in the form of artefactual mutations into viral genomes . this problem is most evident for rna viruses that must first be reverse transcribed  to cdna, which is then typically amplified by polymerase chain reaction  to produce sufficient quantities of dna for sequencing on ngs platforms. the rt and pcr processes are error prone, with the error rate dependent on the fidelity of the enzymes used. rt is a non-expansive process, whilst pcr utilises multiple cycles to amplify the dna in an exponential manner. in terms of error, the pcr process is cumulative, as any mutation introduced into a copied viral genome in one cycle will be passed on to all the progeny of that genome in later cycles in addition to new mutations being introduced. therefore, there is an increase in the number of errors introduced into the viral sequences through the pcr process as the cycle number increases, elevating the error rate of the pcr process with each cycle.figure  <dig> 
rna virus sequencing and error introduction. the typical steps involved in processing an rna virus sample for sequencing. at each step, errors are introduced in the form of artefactual mutations that will be present in the resultant ngs reads. artefactual mismatches to the reference genome are practically indistinguishable from low-frequency real biological variation within the reads.



focussing on the illumina platform, after rt and pcr, viral samples undergo fragmentation via sonication  to generate dna fragments of various sizes. typically 300-400 bp fragments are then selected by gel extraction. the process of sonication results in approximately half of the sample dna being destroyed via evaporation, and only ~10% of the remaining dna is located in the 300-400 bp range  <cit> . this results in a total sample loss of ~95% and therefore represents a sampling bottleneck of 5%. however, the latest illumina nextera dna sample preparation kit simultaneously fragments and tags sample dna with adapters in a single step  and results in significantly less sample loss  <cit>  and hence requires substantially less starting dna. illumina library preparation involves the ligation of illumina adaptors and unique sequencing tags onto the ends of dna fragments followed by 10 cycles of pcr to amplify the library ready for sequencing on an illumina platform.

base calling on the illumina platform is complicated by factors including similar emission spectra of the a/c and g/t fluorophores as well as phasing and pre-phasing issues  <cit> . in terms of error rate, the illumina platform is considered to be better than its rivals  <cit>  and every sequenced base in every read is assigned a quality score that is a measure of the uncertainty of the base call using the phred scale. at one of the highest quality scores q <dig>  there is a  <dig> in  <dig>  probability of a base being called incorrectly. although this is a very low probability, the ultra-deep coverage of viral samples can result in a coverage of 10 k-100 k at each position in the genome. this implies that even if every base were of the highest quality, we would still expect errors to be introduced via base miscalls from the illumina machine. furthermore, it has been reported that illumina machines have a sequence specific error profile whereby errors occur more frequently around certain motifs such as ggc and ggx  <cit> . such systematic errors are poorly understood given that motifs such as ggc  occur very frequently in dna, and yet it is only a small number of these motifs in a genome that suffer such errors, suggesting that there are other issues involved such as the dna sequence further upstream of the motif.

the introduction of erroneous mutations during the sample preparation process and miscalls during base calling confounds the identification of true low frequency viral variants. consequently, true low-frequency variants will be practically indistinguishable from process error, whilst high frequency viral variants will be easily identified, as they are observable at levels much higher than can be attributed to error. however, there have been few experimental analyses to determine thresholds above which a variant is highly likely to be real. furthermore, there are very few computational methods available to distinguish true viral variants from erroneous mutations, with the majority of methods tailored to the  <dig> platform such as ampliconnoise  <cit>  and readclean <dig>  <cit> . other tools aimed at detecting low frequency variants include segminator ii  <cit>  which is specialised towards temporally sampled data, v-phaser which utilises information on the co-location of variants on reads  <cit> , lo-freq which utilises quality scores to model base miscalls  <cit> , and an approach which incorporates re-sequencing with reads distribution and strand bias  <cit> . however, these tools do not consider the error effects of the rt or pcr processes; although flux simulator  <cit>  does consider these processes it is specialised towards rnaseq datasets.

here we focus on fmdv which has a genome of ~ <dig>  kb and is a non-enveloped, positive-sense, single-stranded rna virus in the aphthovirus genus of the family picornaviridae. fmdv is the causative agent of the highly contagious and economically serious foot-and-mouth disease , a vesicular disease of cloven-hoofed animals, which can spread extremely rapidly and has the potential to cause enormous economic losses. in this study, we have created a set of novel control samples in the laboratory that are all derived from a dna plasmid containing a full-length fmdv cdna with extremely limited diversity in the starting population. one sample was ultra-deep sequenced on the illumina platform without any rt-pcr sample processing, whilst the other samples were either pcr or rt-pcr amplified prior to sequencing. this enabled the level of error introduced by the rt and pcr processes to be individually assessed and also enabled minimum frequency thresholds to be set for true viral variant identification. we combined these experimental data sets with a genome wide computational model of the sample processing and ngs calling process to gain a detailed understanding of the error rates and thresholds at each step. furthermore, we demonstrate how the model can also used to investigate a specific site of interest in any ngs data set, by utilising the site’s coverage, quality scores, and the fitted rt-pcr error distributions, one can investigate if the number of mutations observed are more than would be expected from processing errors alone. combined, these data sets and the computational model provide an effective means of separating true viral mutations from those erroneously introduced during sample processing and sequencing.

methods
control sample and data preparation
control sample preparation was performed in the high containment laboratory of the pirbright institute. the starting template used for all samples was an  <dig>  bp pt7s <dig> plasmid containing a full-length fmdv b <dig> strain o1kaufbeuren cdna . the dna sequence of this plasmid clone had been determined previously by the sanger method. this plasmid was clonally amplified in e. coli to create the necessary quantity of dna for direct sequencing and as the starting template for all samples. given the proof reading capabilities of bacterial dna polymerases , the clonally amplified plasmid represents a relatively clonal starting population with a extremely limited diversity. linearised pt7s3-o1k b <dig> plasmid  was generated using the restriction enzyme hpai which in turn was used to generate four different control samples spanning three areas of error introduction :figure  <dig> 
control sample processing and genome targets.  is a schematic of the four different control samples and the processing that each one has undergone. the numbers in brackets represent the number of pcr cycles used during pcr steps, the  label on reverse transcription denotes that there was a preceding transcription step.  is a schematic of the dna plasmid used, the genomic regions each of the control samples covers, and the regions that are directly comparable between all controls and used for later analyses.

sequence: a  <dig>  bp dna fragment containing 80% of the fmdv genome was cut from the undiluted linearised pt7s3-o1k b <dig> plasmid using restriction enzyme digestion with psti and then gel extracted. no rt or pcr was required to amplify this clonal sample.

pcr-low: undiluted and linearised pt7s3-o1k b <dig> plasmid was subjected to two independent pcr amplifications, 19 cycles each, to generate  <dig> independent but overlapping pcr products - pcr <dig> and pcr <dig> - which are 4065 bp and 4033 bp in length, respectively. the dna polymerase  used in this pcr process was platinum taq high fidelity .

pcr-high: linearised pt7s3-o1k b <dig> plasmid was diluted to  <dig> plasmid dna copies per μl , and then subjected to two independent pcr amplifications of 39 cycles each, to generate pcr products pcr <dig> and pcr <dig>  pcr efficiency was measured with efficiency measured at 80% during the initial stages of pcr . the  <dig> starting copy number and 39 cycles of pcr are the optimised values used to process fmdv samples for ngs sequencing determined from our previous studies  <cit> , where further details of the laboratory protocol can be found.

rt-pcr: linearised pt7s3-o1k b <dig> plasmid was diluted  <dig> fold to  <dig> plasmid dna copies per μl, was first transcribed from dna to rna using the megascript t <dig> kit , and then reverse transcribed using superscript iii  to generate cdna followed by amplification via 39 cycles of pcr to generate pcr products pcr <dig> and pcr <dig>  as above.



the sequence control sample covers a slightly different portion of the plasmid genome than the remaining three pcr amplified samples due to the different location of the psti restriction site to the previously optimised pcr primer sites ; restriction site selection was influenced by biosecurity measures whereby no more than 80% of the fmdv genome could be transported out of the high security area for sequencing. to enable direct comparisons between all the samples, we focus our analyses on the portions of the fmdv genome that are shared between all of the samples . the ends of the dna fragments are ignored due to large spikes in coverage at these positions, due to sonication bias.

control samples were sequenced at the polyomics facility, university of glasgow, on an illumina genome analyzer iix. briefly, dna was fragmented using sonication and the resultant fragment distribution assessed on an agilent bioanalyzer  <dig> platform. after size selection of fragments of between  <dig> and 400 bp, a library of purified genomic dnas was prepared by ligating adapters to the fragment ends to generate flow-cell-suitable templates. a unique 6-nt sequence tag for multiplexing was added to each control sample followed by pcr of 10 cycles using phusion oy  dnapol. the amplified and tagged dna samples were then pooled and attached to the flow cell by complementary surface-bound primers, isothermal bridging amplification formed dna clusters for reversible terminator sequencing, yielding reads of  <dig> nucleotides.

reads were processed using a previously developed pipeline  <cit> ; however, other tools such as bwa  <cit>  and bowtie  <cit>  could be used as alternatives. reads were first trimmed from 73 nt to 70 nt due to the very poor quality of the last  <dig> bases and then filtered, with reads discarded if they had an average probability of error per nt greater than  <dig> %. the trimmed and filtered reads were then aligned to the plasmid reference genome with a simple, custom-made scoring algorithm, previously described in  <cit> . we further trimmed the first and last  <dig> nts of each aligned read, as they showed a higher number of mismatches to the reference sequence due to insertions or deletions close to the edges of the reads  <cit> . information on the aligned reads is then piled-up to give the reference base, coverage and the number of as, cs, gs and ts at each genome position along with an average probability of a sequencing error derived from the quality scores. the fastq files from all four samples have been uploaded to the ebi sra repository  with the accession numbers err <dig>  err <dig>  err <dig>  and err <dig> for the rt-pcr, pcr-high, pcr-low and sequence controls respectively.

computational model
the computational model operates at the population level. instead of modelling  <dig> or  <dig> individual genomes  through the exponential pcr process, which would be computationally intensive, we view the population as an array or alignment of  <dig> initial genomes and operate on the numbers of as, cs, gs, and ts present at each genome position. this population view is based on the assumption that the copying of each individual base in each genome during pcr  is independent and is therefore a single binomial trial with a probability of success  equal to the dna polymerase error rate. therefore, with  <dig> genomes being duplicated in the first pcr cycle, each individual genome position is independently copied  <dig> times with a mutation potentially introduced each time. an example of the alignment view and error accumulation through pcr cycles is presented in additional file  <dig> 

after simulation, the model gives a polymorphic/mismatch frequency  for each genome position, which we present in the form of a mutation spectrum. the spectrum is generated by grouping genome positions into discrete bins based on their observed mismatch frequencies and then plotting the proportion of nucleotide sites in each mismatch bin  against the mismatch frequency of the bin . this spectrum provides a richer view of the diversity within a viral population, and enables easy comparison between populations. the mutation spectrum outputted from the model is then compared to the mutation spectrum from the corresponding experimental data set to give a sum of squares score  representing how well the model recreates the experimental data:figure  <dig> 
experimental data mutation spectra. the mutation spectrum of each of the experimental samples: seq , pcr-low , pcr-high  and rt-pcr  along with a real fmdv sample  from an infected cow . each genome position considered is placed into a discrete bin based on its mismatch  frequency. the x-axis represents the mismatch frequency of the bin whilst the y-axis represents the number of genome positions that are in that bin. both axes are presented on a log <dig> scale.

 s=∑1bei−mi <dig> 

where b is the number of polymorphic bins, and ei and mi are the number of nucleotide sites  in the ith polymorphic bin, for the experimental  and model  mutation spectrums, respectively. for model fitting and parameter estimation, we use a sequential monte carlo  algorithm  <cit> , a form of approximate bayesian computation . we use the smc algorithm proposed by  <cit> , further details of which are presented in the additional file  <dig>  but briefly a parameter  θ is sampled from a prior distribution π and propagated through a sequence of intermediate distributions π ≤ εi) until it represents a sample from the target distribution π ≤ εt). d is the distance between the model simulated dataset  and the laboratory data set  calculated by the sum of squares score above. the tolerances εi are minimum distance scores and are chosen such that εi > … > εt ≥  <dig>  thus the distributions gradually evolve towards the target posterior. for the model fitting, we used uniform prior distributions and a dynamic tolerance schedule that progressively decreases the tolerance at each step. the smc process essentially fits the model parameters to the given data set, and thus provides estimates of the error rates of the enzymes used during rt and pcr, as well as the underlying distributions involved.

the model is composed of a number of sample processing steps enabling various combinations to be selected for a simulation :set initial diversity: the first step is to set the initial diversity of the viral population. this can be set to clonal , fixed  where αs, βs are the two shape parameters of a beta distribution; αs, βs can either be defined or added to the parameters to be estimated. for this study, we use the clonal  setting.

determine rt enzyme error profile: assuming the rt enzyme is more likely to make errors  at some genome positions than others  the rt enzyme error rate for each genome position is randomly selected from a beta distribution . alternatively, one can simply use a single rt enzyme error rate for all genome positions.

transcription and reverse transcription: as our starting clonal sample is dna, we first transcribe our dna to rna so that it can be reverse transcribed back to cdna, enabling us to evaluate errors that impact upon the rna template. we therefore represent this process as two distinct steps but where transcription uses the same parameters as reverse transcription, as we are unable to disentangle these two processes from each other using the data sets available. transcription and reverse transcription are linear copying processes, with the original template removed after completion. each genome position is considered in turn, where the population at each of the  <dig> bases  at that position is also considered in turn. error introduction is modelled by a binomial distribution , where pri is the rt enzyme error rate at genome position i and nij is the number of the jth base currently at genome position i. a random draw from this distribution gives the number of errors made, each error is randomly assigned to one of the other  <dig> bases at that genome position whose population increases by one, whilst that of the transcribed base decreases by one.

determine dnapol error profile: assuming the dnapol enzyme is more likely to make mutations at some positions than others, the dnapol error rate for each genome position is also randomly selected from a beta distribution . alternatively, one can simply set the dnapol error rate to be the same for all genome positions.

sample preparation pcr: each genome position is considered in turn, where the population at each of the  <dig> bases  at that position is also considered in turn. first, if pcr efficiency is not 100% the number of bases to be copied is determined from a binomial distribution , where pc is the pcr efficiency for pcr cycle c and nij is the number of base j at genome position i; the population of base j is then increased by the number of bases to be copied. second, the number of mutations that were made during copying is determined from another binomial distribution  where pmi is the dnapol error rate at genome position i and nij is the number of the jth base copied at genome position i ; poisson and then normal approximations to the binomial are used when the population of nij is high . third, the population of the copied base is decreased by the number of mutations to be made, and for each mutation the resulting mutated base is randomly selected to be one of the  <dig> other bases whose population increases by  <dig> 

sonication: the 5% sampling bottleneck caused by sonication is represented by a hyper-geometric distribution . here we use the poisson approximation of the hyper-geometric for speed, and the corresponding normal approximation when the base population  is large. a poisson distribution  is used if the base population is < <dig> where λij is the mean and equal to ; for base populations > <dig> we use the normal approximation of the hyper-geometric.

illumina library pcr: the 10 cycles of pcr performed during the illumina library preparation proceeds in the same manner to the sample preparation pcr above, except that the dnapol error rate is fixed for each genome position to the published error rate of phusion oy .

illumina sampling: the reads obtained from an illumina sequencing run represent a small sample of the amplified illumina library. to represent this bottleneck, a sample of bases is randomly selected stochastically from the population present at each genome position, with the number of bases to be sampled given by the read coverage at that genome position from the corresponding experimental data set.

illumina error: every base in every read is assigned a quality score  by the illumina machine, which can be converted into a probability of error  with the formula p = 10-q/ <dig>  for each genome position, a mean probability of error can then be calculated from all the quality score p values aligned at that position. the number of illumina errors at each position is then modelled by randomly drawing from a binomial distribution  where pei and ni are the average probability of a sequencing error and the coverage at genome position i, respectively.



RESULTS
data analysis
after read filtering and alignment, coverage was high and similar in all four samples with an average coverage of  <dig>  in the seq control sample . although coverage is variable across the genome, this variability is reproducible as all four samples display the same peaks and troughs in coverage at the same positions suggesting coverage is related to the underlying genome sequence. this variation correlates well with gc content  and has previously been reported to influence coverage due to amplification bias during the illumina library preparation pcr  <cit> . the very high peaks in coverage observed in figure  <dig> occur at the end of the dna fragments  and are over-represented in the data set, presumably as a result of sonication bias at the end of dna fragments.figure  <dig> 
genome coverage. the read coverage for each of the samples across the  <dig>  bp plasmid genome: seq , pcr-low , pcr-high  and rt-pcr . the peaks correspond to the ends of dna fragments, which are over-represented in the data set, presumably due to sonication bias; as can be seen the seq sample operates on a slightly different section of the genome  but with substantial overlap with the pcr amplified samples. the two regions of the genome that are used for direct comparison between all samples are highlighted with dashed black boxes, which avoid the regions with large and potentially biased coverage spikes at the ends of dna fragments and primer regions. the seq control  suffers a drop in coverage at around position  <dig>  due to a large poly c tract that is found in all fmdv genomes, and which is problematic for both pcr and sequencing.



figure  <dig> displays the mutation spectra of the four experimental samples and represents a novel data set that measures the amount of error introduced during each stage of processing an rna viral sample for sequencing, with the mutation spectrum shifting progressively towards higher error frequencies the more processing the viral sample undergoes. for the seq control sample, the only source of error is from the illumina machine and this error is still clearly observable despite read and alignment filtering; error from the illumina library pcr step is predicted to be negligible due to the high fidelity of the dnapol used . the addition of pcr to the sample preparation shifts the mutation spectrum substantially to higher frequencies, and the addition of rt shifts the spectrum further still. the median mismatch frequencies of each spectra are presented in table  <dig>  along with the frequency at which 75%, 95% and 100% of genome positions are below. these frequencies can be used to set minimum thresholds above which one can be confident that observed mutations are real. for example, the maximum mismatch frequency observed in the full rt-pcr sample is  <dig> %, whilst 95% of genome positions are below a frequency of  <dig> %. the mutation spectrum from a real fmdv sample obtained from a foot lesion of an infected cow is also included in figure  <dig>  this sample underwent the same processing as the rt-pcr sample . the spectrum of the real fmdv sample is similar to the clonal rt-pcr sample at low frequencies which represents the error introduced from sample processing and base calling, but it clearly diverges away from the clonal samples at a frequency of  <dig> % bin onwards which therefore represent real biological variants in the viral population. there is undoubtedly real low frequency viral variation below  <dig> % in the sample due to nature of viral replication, however this is practically indistinguishable from process error at this level.table  <dig> 
variant frequency thresholds


25%
50%
75%
95%
100%

seq

pcr-low

pcr-high

rt-pcr

real
this table contains the 25th, median 50th, 75th, 95th and the maximum 100th percentiles of genome position mismatch frequencies in each of the four samples.



table  <dig> shows that the maximum observed mismatch frequency in the rt-pcr sample was  <dig> %; in the mutation spectrum this genome position is placed in the bin that has a midpoint of  <dig> % . we can be highly confident that any mutation observed above this frequency is true, for samples processed with this protocol. however, there are likely to be a number of true viral variants present below this threshold. figure  <dig> shows that the real fmdv sample begins to deviate from the rt-pcr sample at the  <dig> % threshold, with more variants being observed in the real sample from this point on. a total of  <dig> variants are observed above this threshold in the rt-pcr control sample, whilst  <dig> variants are observed in the real fmdv sample,  <dig> of which are therefore likely to be true. a threshold of  <dig> % would correctly identify all the errors , but would only identify  <dig> of the  <dig> likely variants in the real fmdv sample as true. lowering the threshold by half to  <dig> % results in a doubling of the number of likely true variants identified to  <dig> at a relatively small cost with specificity only dropping to  <dig> %. whilst at a threshold of  <dig> %, specificity drops to 96% with  <dig> errors falsely identified as true, but all  <dig> are the likely true variants also identified. it is important to note that a threshold will be dictated by the actual sample processing used and must also be taken in context. the fidelity of the enzymes used and the number of the pcr cycles will highly influence threshold setting, whilst the coverage of a genome position and the quality of it’s aligned reads will influence a threshold in a site specific manner.

next we characterised all the base substitutions that occurred in each of the clonal samples. in the seq control sample, by far the most frequent base miscall was g to t  which represented approximately 23% of all illumina base miscalls, followed by t to c, c to t, g to a and a to g which varied between approximately 12% and 15% . over 40% of all illumina errors occurred at g positions, even though there were less g positions in the genome sequence considered than a or c positions; the remaining 60% of errors were distributed equally between the a, c and t positions. this suggests that the illumina machine is more prone to error at g positions and could reflect the known issues with ggc and ggx motifs discussed previously. for the pcr samples, the most common base substitutions are those representing transitions between the purines  and pyrimidines , all of which are dominant within the dataset; although the a to g and t to c transitions are more abundant. although substantially lower than these transitions, the g to t is by far the highest transversion, highlighting the contribution of the above illumina error into the overall mutation spectrum. the rt sample shows the same characteristics as the pcr sample.

overall, the mutation spectra suggest that illumina error is clearly observable and should therefore be considered when examining viral populations at ultra-deep coverage. the main bulk of the error appears to come from the pcr process rather than the rt step, but rt does substantially affect the high frequency end of the spectrum. although rt enzymes have higher error rates than their pcr counterparts, pcr utilises numerous amplification cycles and as errors are cumulative the amount of pcr error in the sample increases with each cycle.

computational model
we next developed a computational model of the sample preparation and sequencing process to parameterise each process and the errors that are introduced. in the model, the seq control sample starts with an initial population containing no diversity, this is then sonicated , has library prep pcr amplification of 10 cycles, and is sampled at each position by the corresponding coverage in the real data set. illumina base miscalls are represented via random draws from a binomial distribution based on the coverage and probability of sequencing error  at each genome position. this relatively simple approach to address illumina error has been used successfully on fmdv  <cit>  and other systems  <cit> . figure 5a shows that there is ample agreement between the simulated and experimental data sets for the seq control sample but the model slightly overestimates the amount of error introduced via illumina miscalls. one possible explanation is that the probability of error associated with each quality score should in fact be lower  than the published value. this was also suggested in  <cit> , who observed significantly lower mismatch rates than predicted from illumina, quality scores when using overlapping paired-ends.figure  <dig> 
model simulated data mutation spectrums. the mutation spectra of simulated data sets compared to its corresponding experimental data set:  seq experimental  and simulated  and  pcr-high experimental , simulated fixed dnapol error , and simulated distributed dnapol error ;  rt-pcr experimental  and simulated distributed rt error .



for the pcr-high model, there is poor agreement between the simulated and experimental data sets when one assumes that the dnapol error rate is the same for all genome positions . when genome positions have the same mutation rate, there is not enough diversity in the predicted mutation spectrum. however, when one assumes that the dnapol enzyme is more likely to make mutations at some positions than others, through a beta distribution, there is good agreement to the experimental data . this could be due to the sequence composition of the surrounding bases influencing the likelihood of the dnapol making an error at certain positions. the model predicts that the median error rate of the dnapol used  to be  <dig>  × 10− <dig> substitutions per nt copied from the fitted beta distribution , which is only slightly higher than the manufacturers published error rate of  <dig>  × 10− <dig> . this beta distribution results in variation between genome positions, with some positions slightly more or less prone to error during each pcr cycle than others , which will be magnified the more pcr cycles are used.figure  <dig> 
beta distributions representing enzyme error rates. the model derived beta distributions representing the error rates for the dna polymerase  and reverse transcriptase  enzymes.  <dig>  samples were randomly drawn from each beta distribution. all error rates are logged  and the x-axis is truncated at − <dig>  to aid viewing and focus on the differences between the two distributions.



similarly, for the rt-pcr model, there is again good agreement between the simulated and experimental data sets when one assumes the rt enzyme is more likely to make mutations at some positions than others . the model predicts a highly skewed beta distribution , with a low median error rate of  <dig>  × 10− <dig> but with a mean an order of magnitude higher at  <dig>  × 10− <dig> substitutions per nt copied , which is within the range of published error rates for superscript iii  which again varies depending on the assay used. the 95th percentile of this distribution has an error rate of  <dig>  × 10− <dig> suggesting some genome positions are highly prone to error during the reverse transcriptase process. this could be due to sequence specific effects or perhaps the 2d/3d structure of the rna influencing where the reverse transcriptase makes an error.

through model fitting, we now have a good understanding of each of the sample processing stages along with their enzyme error rates and underlying distributions involved. as sequencing is always performed with dna, reverse transcription of rna viruses must always be performed. however, future technological advancements could make pcr redundant  or make future sequencing machines error-proof. therefore, the model was used to examine the impact of such developments. figure  <dig> shows that when the sample preparation pcr is removed, the peak of the mutation spectrum shifts substantially to lower frequencies. however, the high frequency end of the spectrum is still visible demonstrating that the rt process alone can generate observable high frequency mutations due to the skewed nature of its error distribution. when illumina error is removed as well, the mutation spectrum changes drastically, with most genome positions  having no observable error in the ngs reads. however, high frequency mutations up to the  <dig> % threshold are again still observable again due to highly skewed error distribution of the rt process. overall, this suggests that although pcr and illumina machine error do contribute substantially to the mutation spectrum, removing them does not substantially affect the high frequency tail end of the spectrum or the minimum frequency thresholds. however, an error-proof sequencer capable of sequencing tiny amounts of dna without pcr amplification would obviously be a major technological advancement. although some sites would still have “high” frequency variants , the majority of the genome will have no observable error, whilst only 5% of sites would have an observable error above  <dig> %. this demonstrates that it is important to consider the overall error distribution of processes such as rt and pcr when selecting an appropriate frequency threshold and deciding upon the amount of error one is willing to tolerate. we investigated the effect of coverage and quality scores on the frequency threshold , and found the  <dig> % threshold to be stable at coverage’s of  <dig>  and above, but the threshold does increase below this coverage. in addition, lowering the quality of the reads increases the threshold across all coverage levels, again highlighting that a threshold should be considered in context with all available information taken into account.figure  <dig> 
predicted mutation spectrums with higher fidelity polymerase and the true diversity of a real sample. the predicted mutation spectrum of the full rt-pcr sample , rt with no sample preparation pcr , and rt with no sample preparation pcr or illumina error .



a secondary use of the model is to investigate whether the number of observed mutations at a given genome position of interest in a real viral data set is greater than would be expected from processing errors alone. the model is first provided with the genome position’s coverage and quality scores, along with the biological sample processing information. the model then utilises the previously determined rt and pcr enzyme error distributions to simulate the number of mutations that would be observed from processing errors alone at this position. the simulation is essentially replicated  <dig>  times as each position in the model’s genome is set to have the given coverage and quality scores, but each position receives a random draw from the rt/pcr error distributions. the resulting histogram then displays the range of error mismatches that could be observed at this position given the variations due to sampling bottlenecks, stochasticity, and the enzyme error rates . this histogram can then simply be compared to observed number of mutations to determine how likely it is that they are real.figure  <dig> 
simulated number of mismatches from sequence and process error alone at a specified site. for a given site, the model simulated  <dig>  replicates, with each replicate having the same coverage  and average sequence error  of the site, but receiving a random draw from the enzyme error distributions. the simulated number of mismatches for all replicates is presented as a histogram  that can then be compared to the real number of mismatches observed at the specified site. if one had observed  <dig> mismatches  in the real biological data set, it is unlikely that any of these are real given the simulated distribution. however, if one observed  <dig> mismatches  it is much more likely that there is real biological variation at this site.



figure  <dig> is an example of setting a frequency threshold in the context of a genome position of interest. in such a case, one already knows the coverage and quality scores, both of which can greatly affect a frequency threshold. coverage directly affects frequency , whilst a site with poor quality could similarly lead to a high error frequency from machine miscalls alone. therefore, the model can be used to investigate the possible affect the rt and pcr processes are having at a specific site given what is already know from the sequence data. we envisage that the model would be used in this case for a select number of genome positions, such as though at the higher scale of the mutation spectrum, or specific sites of interest such as those that convey drug resistance, pathogenicity or become fixed later on in a transmission chain, due to the computational cost of running the model across the whole genome.

discussion
ngs technology provides the means to go beyond consensus sequences and investigate the population structure of viral samples. the introduction of erroneous mutations during the sample preparation process and miscalls during sequencing confounds the identification of true low frequency viral variants. identifying such variants is important when tracing the source and development of specific mutations of interest, or when comparing the total diversity within viral populations, such as those before and after a bottleneck. in this study, we generated and analysed a collection of novel ngs data sets that reveal the amount of error introduced during each viral sample processing step and used them to set a minimum frequency threshold of ~ <dig> % to separate real mutations from process error. however, this threshold could be lowered at the cost of specificity, and will naturally vary depending on the specifics of the sample processing steps used such as enzyme fidelity and number of pcr cycles. we complemented this with a genome-scale model to gain a deeper understanding of each processing step.

the data and model show that despite high quality read filtering, illumina error is still observable, although typically at relatively low frequencies. the introduction of pcr resulted in a substantial increase in the observable error to higher mismatch frequencies. however, using the highest fidelity dnapol enzymes and minimising the number of pcr cycles will logically reduce the amount of pcr error introduced. the model predicted the mean error rate of the dnapol used  to be  <dig>  × 10− <dig> substitutions per nt copied, which is only slightly higher than the manufacturers published error rate of  <dig>  × 10− <dig> ; a further study using novel beaming technology estimated a lower bound error rate of  <dig>  × 10− <dig>  <cit> . however, the measured error rate is extremely dependent on the assay used, with estimates for taq itself range from  <dig>  × 10− <dig>  <cit>  to between  <dig>  × 10− <dig>  <cit>  and  <dig>  × 10− <dig>  <cit> . in this report, we have used a novel approach using ngs reads to estimate the error rate, and propose that the error rate is in fact best represented with a distribution rather than a single value. the model predicted a mean error rate of the rt enzyme used  to be  <dig>  × 10− <dig> substitutions per nt copied, which is within the range of published error rates for superscript iii  which again varies depending on the assay used. our rt-pcr sample was first transcribed into rna using t <dig> polymerase  before reverse transcription back into dna. therefore, the rt error rate estimated by the model is applied to both the transcription and reverse transcription steps as these two steps cannot be fitted separately with the available data. if the transcription step is less error prone, then the rt step will in fact have a higher error rate than we have reported here to compensate, and vice versa.

the model predicts that not all genome positions are equal and that some sites are much more prone to pcr error than others. this novel model prediction could be due to the sequence composition of surrounding bases influencing the likelihood of dnapol making an error, which has previously been reported for human polymerase v  <cit> . the model also makes a similar prediction for the rt process, which resulted in a highly skewed beta distribution with some genome positions highly susceptible to error. interestingly, this does not appear to be specific to the rt-pcr enzymes  or viral sequence  that we used in this study. similar findings have been reported with hepatitis c virus  <cit> , using different rt-pcr enzymes combined with a consensus sequencing strategy of numerous molecular clones. they reported that rt-pcr errors were not evenly distributed, but were concentrated in specific hotspot regions, one of which coincided with a known region of hyper-variability in the viral genome. this suggests that any specific sequence context or secondary structure that negatively impacts on the rt-pcr enzymes may also affect the viral rna polymerase itself. therefore, hyper-variable regions in viral genomes may be, in part, the result of natural polymerase mutational hotspots. overall, identification of the sequence and structural signatures of mutational hotspot regions, and a comparison of these between different viruses would make an interesting study and could lead to novel insights into what drives the mutational dynamics of viruses.

from the data itself we identified a minimum frequency of ~ <dig> % above which one can be reasonably confident that an observed mutation is real. however, a minimum mismatch frequency threshold should be interpreted with some caution and must always be considered in context, as any mismatch frequency is highly dependent on the coverage at a genome position. for example,  <dig> observed mismatch at a genome position with a coverage of  <dig> leads to a mismatch frequency of 1% which would appear relatively high. however, a single mutation is not reliable given the error prone nature of the processes, and a coverage of  <dig> would be considered low compared to the average of  <dig>  observed in our seq control data set, suggesting an underlying problem with that genome position that has resulted in limited coverage. additional filters should therefore be applied when validating the mismatches at a particular genome position, such as minimum coverage cut-offs, and validation against the number of mismatches expected given the sample processing procedure.

to address this, our model can also be used to investigate if the mutations observed at a given genome position are real, by simulating the number of mutations that would be expected from sequencing and sample processing error alone, and comparing this distribution to the observed number. this could be applied to any viral illumnia data set, and adapted to represent alternative rt-pcr enzymes if estimates of their error rates are known; alternatively, one could simply run the model multiple times scanning through a range of enzyme error rates. although we used 39 cycles of pcr in our optimised protocol, alternate virus/system protocols may well utilise fewer cycles. as shown in figure  <dig>  when the number of pcr cycles is halved, the mutation spectrum shifts substantially to lower error frequencies. therefore, one should logically limit the number of pcr cycles used if possible as pcr errors accumulate with each cycle. alternatively, there are high-fidelity polymerases available that have published error rates orders of magnitude above the platinum taq polymerase used in our protocol, which will also greatly reduce the observed errors. however, in both cases the rt step remains which alone can still result in erroneous high frequency variants, although only at relatively few genome positions. in addition, as illumina machines share the same sequence by synthesis chemistry, and as we utilise the quality scores outputted by the sequencer itself, our results are applicable to more recent machines.

recent techniques have been developed  <cit>  that utilise the fragmentation followed by circularisation of viral rna. each small rna circle is then reverse transcribed multiple times to create a single cdna strand with multiple copies of the original rna sequence. the cdna can then be directly sequenced  and as each read will contain multiple copies  of the same viral rna sequence, reverse transcriptase errors and sequencer base miscalls can be readily identified; a similar strategy to that of utilising overlapping read pairs  <cit> . these approaches are very promising, however, they are currently limited to in-vitro samples due to the substantially large amount of initial viral rna required, as they do not utilise pcr at any step. therefore, our strategy still provides a valuable means for identifying and quantifying errors introduced during the processing of biological samples, such as field isolated during an epidemic, which typically have to amplified to achieve the required amount of rna as standard. furthermore, our results that reverse transcriptase is highly prone to error at some sites, may have implications for such circular re-sequencing techniques. an alternative approach to error handling is the incorporation of unique barcodes into sample dna which have been successfully applied to viral population and fitness analyses  <cit> .

our work here has been focussed on the illumina platform, but application of the same experimental techniques would be useful to assess and compare the error profiles, in terms of viral populations, of different platforms. it would also be possible to apply the models to other platforms, although this would likely involve substantial more work. although a large proportion of the models, such as the rt-pcr component, could be directly applied to other sequencing platforms, the models would then need to be extended to represent each platform’s specific sample preparation protocol and sequencing error profile.

CONCLUSIONS
we have created a novel set of laboratory control samples that enabled the level of error introduced by the rt and pcr processes to be assessed and minimum frequency thresholds to be set for true viral variant identification. we combined this with a genome-scale computational model of the sample processing and ngs calling process to gain a detailed understanding of the errors at each step, which predicted that rt and pcr errors are more likely to occur at some genomic sites than others. the model can also be used to investigate whether the number of observed mutations at a given site of interest is greater than would be expected from processing errors alone in any ngs data set. these data sets and models provide an effective means of separating true viral mutations from those erroneously introduced during sample processing and sequencing. furthermore, the data sets themselves provide an ideal test set for the evaluation of viral variant calling tools to assess their ability to distinguish real viral variants from rt-pcr and sequencer errors.

availability of supporting data
the raw fastq files from all four control samples will be deposited and made publicly available from the sequence read archive  at the european nucleotide archive .

additional files
additional file 1: 
additional description of the model fitting method and parameter estimation.




competing interests

the authors declare that they have no competing interests.

authors’ contributions

rjo, cfw, mjm, djp, dpk and dth conceived and designed the study. cfw carried out all laboratory work, except the pcr efficiency experiments carried out by djk. rjo carried out all modelling and analysis work. mjm wrote the initial bioinformatics pipeline. all authors were involved in writing the final manuscript. all authors read and approved the final manuscript.

