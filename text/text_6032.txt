BACKGROUND
for models built from high-dimensional data, e.g. arising from microarray technology, often survival time is the response of interest. what is wanted then, is a risk prediction model that predicts individual survival probabilities based on the covariates available. because of the typically large number of covariates, techniques have been developed that result in sparse models, i.e., models where only a small number of covariates is used. in modern approaches, such as boosting  <cit>  and the lasso-like path algorithms  <cit> , it is avoided to discard covariates before model fitting, and parameter estimation and selection of covariates is performed simultaneously. this is implemented by  putting a penalty on the model parameters for estimation. the structure of this penalty is chosen such that most of the estimated parameters will be equal to zero, i.e., the value of the corresponding covariates does not influence predictions obtained from the fitted model.

often there are clinical covariates, such as a prognostic index, available in addition to microarray features. the former could be incorporated into the model just like an additional microarray feature, but due to the large number of microarray features compared to the typically small number of clinical covariates there is the danger, that the clinical covariates might be dominated, even when they carry important information. therefore mandatory inclusion for such covariates is needed. when it is also of interest whether use of microarray features can improve over models based solely on the clinical covariates, i.e., the latter are not only included for increasing prediction performance, the parameters of the clinical covariates have to be estimated unpenalized. only then the resulting model can be fully compared to models based only on clinical covariates, where typically unpenalized estimates are used.

to our knowledge, existing techniques for estimating sparse high-dimensional survival models do not naturally allow for unpenalized mandatory covariates. in contrast, for the generalized linear model class there is a recent approach that fits this need  <cit> . we therefore extend this one to survival models. as will be shown, this new approach is closely related to the existing high-dimensional survival modeling techniques when no mandatory covariates are present. therefore, we first review some of the latter, before developing the extension.

given observations , i =  <dig>  ..., n, where ti is the observed time to the event of interest for individual i, δi takes the value  <dig> if an event occurred at that time and  <dig> if the observation has been censored, and xi = ' is a vector of covariates obtained at time zero, many approaches for high-dimensional survival data are based on the cox proportional hazards model for the hazard

  λ = λ0exp), 

where λ <dig> is the baseline hazard and f  is a function of the covariates, depending on a parameter vector β. when a linear predictor of the form f = x'β is used, each element of the parameter vector β = ' specifies the influence of a single covariate. for estimation, the baseline hazard λ <dig> is left unspecified and an estimate β^ is obtained by maximizing the partial log-likelihood

  l=∑i=1nδi−log⁡exp⁡))), 

where i() is an indicator function taking value  <dig> if its argument is true, i.e., if individual j is still under risk just before time ti, and value  <dig> otherwise.

when the number of covariates is large, maximization of  can no longer be carried out by standard techniques. in lasso-like approaches   <cit>  a penalty term λ∑j|βj| is added to the partial log-likelihood . the resulting penalized partial log-likelihood then is maximized by quadratic programming techniques or by the more efficient path algorithms  <cit> . the penalty parameter λ can be determined e.g. by cross-validation. due to penalizing the absolute value, many elements of the resulting estimate β^ will be equal to zero, i.e., the solution will be sparse, larger values of λ leading to more sparseness. lasso-like approaches have in addition been developed for additive risk models  <cit>  and accelerated failure time models  <cit> .

an alternative approach for fitting of sparse high-dimensional models is provided by gradient boosting techniques  <cit> . the underlying principle is that of stepwise optimization of a function f in function space by minimizing a loss function. for fitting a cox model, the negative partial log-likelihood is used as a loss function  <cit> . in each step k =  <dig>  ..., m the negative gradient of the loss function, evaluated for the current estimate fk- <dig> at the observations, is fitted e.g. by standard least squares techniques. the resulting fit fk, which depends on some parameter vector γk, then is used to updated the overall fit via fk = fk- <dig> + εf^k, where ε is some small positive value.

in componentwise boosting a linear predictor of the form fk = x'β^k is used and only one element of β^k is updated in each boosting step  <cit> . the parameter to be updated in step k is determined by evaluating fits to the gradient f^kj = γ^jxij, j =  <dig>  ..., p, where γ^j is determined by least-squares, and selecting that one that improves the overall fit the most. this results in sparse fits similar to lasso-like approaches, with many of the estimated coefficients being equal to zero.

for linear models with squared-error loss function, gradient boosting is equivalent to iterative fitting of residuals. this idea has been adapted to the generalized linear model setting as an alternative to the gradient approach  <cit> . in each boosting step, estimation is performed by a standard newton-raphson step, based on a penalized likelihood, where previous boosting steps are incorporated as an offset. an advantage of this offset-based boosting approach is that it allows for very flexible penalty structure, including unpenalized mandatory covariates. adapting it for survival models would help to resolve the highlighted issues arising when clinical covariates should be included in high-dimensional survival models.

one could also try to adapt existing gradient boosting techniques to allow for unrestricted mandatory components, but we think the offset-based approach is a more natural starting point. alternatively, approaches such as the grouped lasso  <cit> , which allow for groups of covariates with varying penalization, could potentially be adapted by introducing groups with no penalization. as this has not yet been considered by their authors, and also the group lasso approach for the cox model  <cit>  no longer uses simultaneous estimation of all parameters, we do not follow this route here.

in the following will therefore adapt the offset-based boosting approach from  <cit>  for estimating cox proportional hazards models. the resulting advantage of allowing for unpenalized mandatory components for clinical covariates will be illustrated with data from patients with diffuse large b-cell lymphoma.

RESULTS
algorithm
the aim of the new coxboost approach is to estimate the parameter vector β for a linear predictor f = x'β in the cox proportional hazards model . typical gradient boosting approaches either use all covariates for the fitting of the gradient in each step, e.g. based on regression trees, or, in componentwise boosting, update only one element of the estimate of β, corresponding to only one covariate. the flexibility of the offset-based approach in  <cit>  partly is due to considering a flexible set of candidate sets, i.e., a set of sets of covariates, for updating in a specific boosting step. this is adapted for the coxboost approach. in boosting step k =  <dig>  ..., m there are qk predetermined candidates sets of covariates with indices ℐkl ⊆ { <dig>  ..., p}, l =  <dig>  ..., qk. for each of these qk sets a simultaneous update of the parameters for the corresponding covariates is evaluated. the candidate set that improves the overall fit the most will then be selected for the update.

with β^k−1=′ being the actual estimate of the overall parameter vector β after step k -  <dig> of the algorithm, and η^i,k−1=x′iβ^k− <dig> being the corresponding linear predictors, potential updates for the elements of β^k- <dig> corresponding to ℐkl are obtained by maximizing the penalized partial log-likelihood

  lpen=∑i=1nδiexp⁡)−λγ′klpklγkl 

with respect to the parameter vector γkl of size |ℐkl|, where xi,ℐklkl is the covariate vector for subject i containing only those covariates with indices in ℐkl. the penalty parameter λ which has to be selected, results in a cautious update, if it is large enough. the penalty matrices pkl can be specified separately for each boosting step and each candidate set, which provides considerable flexibility of the coxboost approach. typically these will be diagonal matrices, for penalizing each covariate separately, but by varying the size of the diagonal elements, differential penalization is introduced. in contrast, for gradient boosting approaches the fitting in each step is performed unpenalized and only afterwards the update is multiplied by a small shrinkage factor ε, thus applying equal penalization to all covariates. for the present application of the coxboost approach we will use only diagonal elements  <dig> and  <dig>  for "penalization" and "no penalization".

the parameter estimates γ^kl for evaluating the candidate sets are obtained by penalized partial likelihood techniques  <cit> . using the starting value γ^kl0= <dig>  the first newton-raphson step is

  γ^kl=ipen−1u, 

where u =  is the score function and ipen =  + λpkl is the information matrix, obtained from the first and second derivatives of the unpenalized partial log-likelihood l, i.e.,  without the penalty term. as further updates can take place in later boosting steps, only one newton-raphson step is performed.

given the sets of sets of indices ℐk={ℐk <dig> ...,ℐqk}, corresponding penalty matrices pkl, k =  <dig>  ..., m, and the penalty parameter λ, the general coxboost algorithm is as following:

 <dig>  initialize η^i,  <dig> =  <dig>  i =  <dig>  ..., n, and β^ <dig> = '.

 <dig>  repeat for k =  <dig>  ..., m

 obtain potential updates γ^kl for the candidate sets ℐkl, l =  <dig>  ..., qk, via .

 determine the best update l* which maximizes the penalized partial log-likelihood .

 obtain the updated parameter vector β^k vector via

 β^kj={β^k− <dig> j+γ^klβ^k− <dig> jj∈ℐkl∗j∉ℐkl∗j= <dig> ...,p, 

where γ^kl is that element of γ^kl that corresponds to β^k, j, and update η^k,i=x′iβ^k, i =  <dig> ..., n.

note that the step size for the updates in part 2c) of the algorithm is  <dig>  this is in contrast to gradient boosting algorithms, where the fits f^k to the gradient are multiplied by some small positive value ε before updating. in the coxboost algorithm the role of ε is taken by the penalty parameter λ during estimation. in the following, for unpenalized mandatory components the corresponding elements of the penalty matrix pkl are taken to be zero, resulting in fast building up of coefficient estimates.

componentwise coxboost with mandatory covariates
componentwise coxboost, similar to componentwise ridge boosting  <cit> , is obtained when in each boosting step only one element of the overall parameter vector is updated, i.e., ℐk = {{1}, ..., {p}}, k =  <dig>  ..., m . in this setup coxboost is very similar to the idea of stagewise regression described in  <cit> . based on the results given there and in  <cit>  we expected the resulting coefficient paths, i.e., the estimated parameters in the course of the boosting steps, to be very similar to lasso-like approaches. for strong correlations between covariates, again due to its similarity to stagewise regression, it is expected that the coefficient paths of componentwise coxboost are even more stable, i.e., more monotone, than that of lasso-like approaches  <cit> .

there are two approaches for incorporating mandatory covariates into the coxboost algorithm. given the indices of the mandatory covariates ℐmand, the indices from componentwise coxboost can be augmented via ℐk = {{1} ∪ ℐmand, ..., {p} ∪ ℐmand}, omitting components {j} ∪ ℐmand where j ∈ ℐmand. this allows for simultaneous estimation of the parameters of mandatory and optional covariates. when the diagonal elements of the penalty matrices pkl corresponding to ℐmand are set to zero, while the others still have a value larger than zero, this furthermore leads to unpenalized estimation of the parameters of the mandatory covariates. when one wants to evaluate whether the optional covariates provide additional predictive power compared to the mandatory covariates, this is the appropriate penalty structure. alternatively, mandatory covariates can be introduced by updating their parameters before each step of componentwise coxboost. this corresponds to ℐ2k- <dig> = {ℐmand}, ℐ2k = {{1}, ..., {p}} , k =  <dig>  ..., m. again, for evaluating the additional predictive performance obtained from the optional covariates we suggest to use penalty equal to zero for the mandatory covariates.

implementation
there are several implementation decisions to be made for the coxboost algorithm. at the lowest level, a criterion for selecting the best update l* in each step has to be chosen. ideally, the penalized partial log-likelihood  or some variant of it that incorporates model complexity  would be used. while for a small number of covariates, say p <  <dig>  this is computationally unproblematic, for large p it is no longer feasible to evaluate this criterion for each candidate set in each step. as an approximation, we therefore propose to employ a penalized version of the score statistic

 u′ipen−1u 

evaluated at γ^kl <dig> this is based on a low-order taylor expansion of the penalized partial log-likelihood  and requires no extra computation. in our experiments, selecting boosting step updates by the largest value of this score statistic was very close to selecting by the penalized partial log-likelihood itself, but considerably reduced computation time.

for including mandatory covariates, computational considerations led us to use the coxboost variant with separate updating of the mandatory parameters. this avoids frequent inversion of ipen, because in the componentwise updating step of this variant for the optional covariates this reduces to a simple division. the coxboost algorithm has two tuning parameters, the penalty parameter λ and number of boosting steps m. while selection of the latter is critical to avoid overfitting, the penalty parameter is of minor importance, as long as it is large enough. we therefore suggest to select only the number of boosting steps by a procedure such as cross-validation. the penalty parameter λ is selected only very coarsely such that the corresponding selected number of boosting steps m is larger than  <dig>  this approach was seen to work well for offset-based boosting for generalized linear models  <cit> .

the algorithm has been implemented in the statistical environment r  <cit>  in the package "coxboost", which is available from the authors.

example
we illustrate the coxboost algorithm with the diffuse large b-cell lymphoma data from the study in  <cit> . a review of attempts to build predictive survival models from such data is found in  <cit> . there is a potentially censored survival time response for  <dig> patients with a median follow up of  <dig>  years, where 57% of the patients died during that time. for prediction there are  <dig> microarray features available. in addition, the international prognostic index , a well-established prognostic score derived from five clinical covariates  <cit> , is available for  <dig> patients. as we want to investigate whether the microarray features increase predictive performance compared to a purely clinical model based on the ipi, analyses are restricted to this smaller set of patients. missing values for the microarray features were imputed as described in  <cit> .

in  <cit>  the data is split into a training set where the parameters are estimated, and a test set where prediction performance is evaluated. the disadvantage of this is that not all data is available for model building and parameter estimation. we employ an alternative approach  <cit> , based on bootstrap samples, which allows to use all observations for model fitting, but nevertheless results in accurate prediction error estimates. for evaluation of prediction performance the brier score is used, i.e., the  squared difference between predicted survival probability at a time t and the true state . this can be be plotted as a function of time, resulting in prediction error curves. for estimation of the latter, prediction error estimates obtained from single bootstrap samples are aggregated into a .632+ estimate. an additional summary measure is obtained when for every single bootstrap sample a .632+ prediction error curve is calculated and integrated . see the methods section for more details.

as a conservative reference for performance comparison the kaplan-meier prediction is used, a non-parametric estimate of the survival probability over time. that way it can be checked whether procedures potentially perform worse than a prediction that does not use any covariate information at all. the performance of componentwise coxboost is furthermore compared to that of gradient boosting for the cox model  <cit>   and that of coxpath, a lasso-like path algorithm for fitting the cox model  <cit>  . for fitting models with these procedures only the microarray features  are used. in addition, componentwise coxboost with the ipi as an additional optional and as an unpenalized mandatory covariate is compared to a simple cox model that has the ipi as its only covariate. the tuning parameters, i.e., the number of boosting steps and the number of path algorithm steps, are chosen by 5-fold cross-validation with respect to the partial log-likelihood. all other settings are at the default values of the respective implementations.

before looking at prediction performance, we investigate the influence of unpenalized mandatory covariates on the coefficient paths, i.e., the parameter estimates for the individual covariates plotted against the norm of the parameter vector . figure  <dig> shows the coefficient paths for coxpath, gradient boosting, componentwise coxboost, and componentwise coxboost with the ipi as a mandatory covariate . the estimates corresponding to the number of coxpath steps and the number of boosting steps selected by cross-validation are indicated by vertical lines. covariates that receive non-zero parameter estimates by all four approaches in that cross-validation solutions are indicated by solid curves, the others by dashed curves. for the former, and other microarray features with corresponding parameter estimates that are large in absolute value, the uniqids are given in the right margins of the plots.

it is seen that the coefficients paths for componentwise coxboost, gradient boosting and coxpath are very similar. for the latter they are a bit more unstable, in the sense that they are not monotone, which is to be expected based on the results in  <cit> . nevertheless, the six microarray features with the largest absolute value of the parameter estimates are the same for all three approaches.

the coefficient paths of coxboost with the ipi as a mandatory covariate are different, with only a small number of distinct covariates receiving large parameter estimates. the reason for this might be that the mandatory covariate already explains much of the variation in the response and there is less incentive to boost a large number of parameters to fit the remaining variability. the number of boosting steps selected by cross-validation  also supports this, as it is smaller compared to simple componentwise coxboost when ipi is present as a mandatory covariate. in this example, including an unpenalized mandatory covariate also changes the ranking of the microarray features with respect to the absolute values of the parameter estimates. after inclusion of the ipi the microarray feature with uniqid  <dig> is associated with a strong protective effect, while it seemed to be of minor importance judged by the other fits. in contrast, the feature with uniqid  <dig> is deemed to be less important when the ipi is included as an unpenalized mandatory covariate. so the latter clearly changes the interpretation of the fitted models.

the left panel of figure  <dig> shows the .632+ prediction error estimates for all models that incorporate only microarray features, i.e., coxpath , gradient boosting , and componentwise coxboost . it is seen that all three perform very similar. the prediction error is well below the kaplan-meier benchmark , which does not employ any covariate information. this is not self-evident, as for example in the evaluation in  <cit>  some other procedures failed with respect to this criterion. so the offset-based boosting approach does not seem to result in a loss of prediction performance and it therefore is a reasonable basis for an approach incorporating unpenalized mandatory covariates. while according to the prediction error curve estimates there seems to be no disadvantage for coxpath, the out-of-bag partial log-likelihood, i.e., the mean partial log-likelihood evaluated for the observations not in the respective bootstrap samples, is the smallest for this procedure . for gradient boosting and componentwise coxboost it is - <dig>  , i.e., also with respect to this error measure there seems to be no disadvantage of using the coxboost approach. a similar pattern is seen for models that incorporate the ipi as an optional covariate in addition to microarray features . there is a general improvement over models that did not include the ipi, with all procedures again performing very similar. according to the prediction error curve estimates there may be a slight advantage for coxpath, which seems to gain the most prediction performance. however, the out-of-bag partial log-likelihood is again the smallest for this procedure , while for gradient boosting it is - <dig> , and for coxboost it is even - <dig> .

the effect of various ways for dealing with clinical covariates is illustrated in the right panel of figure  <dig>  there the .632+ prediction error estimate for componentwise coxboost is given, together with prediction error curves for coxboost approaches that incorporate the ipi, either as an optional  or as a mandatory covariate . in addition, the estimated prediction error curve for a standard cox model that incorporates only the ipi is given . the performance of a microarray-only coxboost fit  is roughly similar to the cox model that includes only the clinical information from the ipi , with an advantage for the latter for early prediction times. so both types of model might contain the same amount of information. the question whether the microarray features contain information that is different from that of clinical covariates is therefore still unanswered. when, as a first step, the ipi is included as an additional optional covariate for componentwise coxboost, as already noted, there is a distinct increase in prediction performance compared to componentwise coxboost based only on microarray features. so the two types of covariates seem to contain  different information. when the ipi is included as an unpenalized mandatory covariate the performance increases even more . this shows that it is really necessary to assign the ipi this special role, as otherwise it cannot exert its full predictive potential. here the flexibility of the coxboost approach allows to incorporate subject matter knowledge, i.e., knowing that the ipi is a good predictor, to increase predictive performance. coxboost with the ipi as a mandatory covariate also allows for a valid comparison to the cox model that contains ipi as its only covariate. as in both models the parameters for the ipi are estimated unpenalized, the exact additional value of the microarray features in terms of predictive performance can be seen from the difference between the two curves.

CONCLUSIONS
modern techniques for the fitting of predictive survival models, such as lasso-like approaches and boosting, are capable of handling the large number of covariates often arising in bioinformatics applications, e.g. from microarrays. what has been missing is an approach for incorporating mandatory covariates into such models. we therefore adapted an offset-based boosting approach, which allows for flexible penalization of covariates, for the estimation of cox proportional hazard models.

the flexible penalty structure of the new approach allows for unrestricted estimation of the parameters for mandatory covariates. as seen in an example application, this also influences the coefficient paths for the optional covariates, in this case resulting in a more transparent structure. the main benefit, on the one hand, was increased prediction performance by combining clinical and microarray information. on the other hand, the increase of prediction performance over a microarray-only model and a purely clinical predictive model helped to answer the question about the additional benefit arising from microarray technology for predicting survival. in the example, including a mandatory covariate also affected the ranking of microarray features with respect to absolute value of the parameter estimates and therefore potentially changed the clinical implications of the result.

componentwise gradient boosting approaches could potentially also be adapted for incorporating unpenalized mandatory covariates. however, simply augmenting the componentwise base learners by mandatory components would not be sufficient, as in gradient boosting the base learner fits are multiplied by some small constant ε before adding them to the overall fit. therefore the building up of the coefficient estimates for mandatory covariates would still be rather slow. introducing intermediate steps with ε =  <dig>  where only mandatory covariates are updated, could address this. however, the offset-based boosting approach, which we used as a basis for the coxboost algorithm, more naturally allows for unpenalized mandatory components.

incorporating unpenalized mandatory covariates is only one of the many possible ways of leveraging clinical information and subject matter knowledge using the proposed boosting approach. for example, information from clustering of the microarray features could be incorporated, by distributing boosting steps over a set of clusters. further refinements of the boosting scheme and the penalization structure could be devised, for further increasing prediction performance and to more generally increase the usefulness of the resulting predictive model.

