BACKGROUND
developing computational models of biochemical networks is complicated by the complexity of their interaction mechanisms  <cit> . typically, hypotheses on how the system operates are formalized in the form of computational models  <cit> . these models are subsequently calibrated to experimental data using inferential techniques  <cit> . despite the steady increase in data availability originating from new quantitative experimental techniques, the modeler is often faced with the problem that several different model topologies can describe the measured data to an acceptable degree  <cit> . the uncertainty associated with the predictions hinders the investigator when trying to make a clear distinction between competing models. in such cases, additional data is required. optimal experiment design  methods can be used to determine which experiments would be most useful  <cit> . these methods typically involve specifying an optimality criterion or design aim and finding the experiment that most effectively attains this goal while considering the current parameter uncertainty. existing methods of oed for model selection are usually based on assuming an uncertainty distribution around best parameter estimates  <cit>  or model linearization  <cit> . due to the non-linearity of the model and the non-gaussian shape of the parameter distribution, these methods are rarely appropriate for systems biology models  <cit>  . in this work, we employ a bayesian approach using the posterior predictive distribution  which directly reflects the prediction uncertainty and accounts for both model non-linearity and non-gaussianity of the parameter distribution. ppds are defined as distributions of new observations conditioned on the observed data. samples from the ppd can be obtained by drawing from the posterior parameter probability distribution and simulating predictions for each parameter set. by simulating a sample from the ppds for all experimentally accessible moieties and fluxes, differences between models can be explored  <cit> .

previously, predictive distributions have been used to perform experiment design targeted at reducing the uncertainty of specific predictions  <cit> . in the field of machine learning, optimal experiment design based on information-theoretic considerations is typically referred to as active learning  <cit> . in the neurosciences, the bayesian inversion and selection of nonlinear states space models is known as dynamic causal modelling . although dcm is dominated by variational  bayesian model inversion - the basic problems and ensuing model selection issues are identical to the issues considered in this work. in dcm, the issue of optimising experimental design focuses on the laplace-chernoff risk for model selection and its relationship with classical design optimality criteria. daunizeau et al.  consider the problem of detecting feedback connections in neuronal networks and how this depends upon the duration of design stimulation  <cit> . we will consider a similar problem in biochemical networks - in terms of identifying molecular interactions and when to sample data. we present a method to use samples from simulated predictive distributions for selecting experiments useful for model selection. considering the increased use of bayesian inference in the field  <cit> , this approach is particularly timely since it enables investigators to extract additional information from their inferences.

in a bayesian setting, model selection is typically based on the bayes factor, which measures the amount of evidence the data provides for one model over another  <cit> . for every pair of models, a bayes factor can be computed, defined as the ratio of their integrated likelihoods. one advantage of the bayes factor is that it automatically penalizes unnecessary model complexity in light of the experimental data. it therefore reduces the risk of unwarranted model rejections. this penalization occurs because more parameters or unnecessarily wide priors lead to a lower weighting of the high likelihood region. this is illustrated in figure  <dig> 

what the bayesian selection methodology does not provide, however, is a means to determine which experiment would optimally increase the separation between models. determining which measurements to perform in order to optimally increase the bayes factor in favor of the correct model is a difficult task. we propose a method which allows ranking combinations of new experiments according to their efficacy at increasing the bayes factors which point to the correct model. predictions whose distributions do not overlap between competing models are good measurement candidates  <cit> . often distributions for a single prediction show a large degree of overlap, hampering a decisive outcome. fortunately, ppds also contain information on how model predictions are related to each other. the relations between the different prediction uncertainties depend on both the data and the model. differences in these inter-prediction relations between competing models can be probed and used . we quantify these differences in predictive distributions by means of the jensen shannon divergence   <cit> .

there are many design parameters that one could optimize. in this paper, we focus on a simple example: namely, which system variable should be measured and at which time point. we argue that by measuring those time points at which the models show the largest difference in their predictive distributions, large improvements in the bayes factors can be obtained. by applying the methodology on an analytically tractable model, we show that the jsd is nearly monotonically related to the predicted change in bayes factor. subsequently, the jensen shannon divergence is computed between predictions of a non-linear biochemical network. since each model implies different relations between the predictive distributions, certain combinations of predictions lead to more discriminability than others. the method serves as a good predictor for effective experiments when compared to the obtained bayes factors after the measurements have been performed. the approach can be used to design multiple experiments simultaneously, revealing benefits that arise from combinations of experiments.

methods
consider biochemical networks that can be modeled using a system of ordinary differential equations. these models comprise of equations f,u→,p→) which contain parameters p→ , inputs u→ and state variables x→. given a set of parameters, inputs, and initial conditions x→, these equations can be simulated. measurements y→=g,q→,r→) are performed on a subset and/or a combination of the total number of state variables in the model. measurements are hampered by measurement noise ε→, while many techniques used in biology necessitate the use of scaling and offset parameters r→ <cit> . the vector θ→, defined as θ→={p→,r→,x→0}, lists all the required quantities to simulate the model. the parameters q→ determine the experimental design and could include differences in when various responses are measured or the mapping from hidden model states x→ to observed responses y→. we will refer to these as ‘design parameters’ that are, crucially, distinguished from model parameters θ→. design parameters are under experimental control and determine the experimental design. in what follows, we try to optimise the discriminability of models in terms of bayesian model comparison by optimizing an objective function with respect to q→. in the examples, we will consider q→ as the timing of extra observations.

to perform inference and experiment design, an error model is required. considering r time series of length n <dig>  n <dig> … n
r
, hampered by independent noise, one obtains the equation:

  pyd|θ→,mi=∏k=1r∏j=1nkpykdtj,θ→,mi 

where m
i
 indicates a model and y
d
 the observed data. the parameters are given by θ→, while ykd indicates the value of a data point of state variable k at time j, respectively.

predictive distributions
posterior predictive distributions  are defined as distributions of new observations conditioned on the observed data. they correspond to the predicted distribution of future experiments, considering the current model assumptions and uncertainty. to obtain a sample from these predictive distributions, we propagate the uncertainty from the data to the predictions. by specifying prior distributions on the parameters and applying bayes rule, it is possible to define a posterior distribution over the parameters. the posterior parameter probability distribution p is given by normalizing the likelihood multiplied with the prior to a unit area:

  pθ→|yd=pyd|θ→pθ→pyd=pyd|θ→pθ→∫pyd|θ→pθ→dθ→ 

where p is the distribution of the observed data given parameters θ→. the parameter prior distributions p typically reflect the prior uncertainty associated with the parameters. to sample from the posterior parameter distribution, one needs to verify that the posterior distribution is proper. this can be checked by profiling the different parameters and determining whether the likelihood times the prior does not flatten out  <cit> . after checking whether the posterior distribution of parameters is proper, a sample from this distribution can be obtained using markov chain monte carlo   <cit> . mcmc can generate samples from probability distributions whose probability densities are known up to a normalizing factor  <cit>  . a sample of the posterior parameter distribution reflects the uncertainty associated with the parameter values and can subsequently be used to simulate different predictions. the predictive distribution can now be sampled by simulating the model for each of the samples in the posterior parameter distribution and adding noise generated by the associated error model. the latter is required as future observations will also be affected by noise.

model selection
in a bayesian setting, model selection is often performed using the bayes factor  <cit> . this pivotal quantity in bayesian model selection expresses the change of relative belief in both models after observing experimental data. by applying bayes rule to the problem of assigning model probabilities, one obtains:

  pm|yd=pyd|mppyd 

where p represents the probability of model m given observed data y
d
, while p and p are the prior probabilities of the model and data, respectively. rather than explicitly computing the model probability, one usually considers ratios of model probabilities, allowing direct comparison between different models. as the prior model probability can be specified a priori , the only quantity that still requires evaluation is p, which can be obtained by integrating the likelihood function over the parameters:

  pyd|m=∫pyd|m,θ→mpθ→m|mdθ→m. 

the bayes factor is actually the ratio of these integrated  likelihoods and is defined as:

  b12=pyd|m1pyd|m2=∫pyd|m <dig> θ→m1pθ→m1|m1dθ→m1∫pyd|m <dig> θ→m2pθ→m2|m2dθ→m <dig> 

where m <dig> and m <dig> refer to the different models under consideration. unnecessarily complex models are implicitly penalized due to the fact that these result in a lower weighting of the high likelihood region, which results in a lower value for the integrated likelihood. this is illustrated in figure  <dig>  this means that maximizing the model evidence corresponds to maintaining an accurate explanation for the data while minimizing complexity  <cit> .

bounds can be defined where the bayes factor value becomes decisive for one model over the other. typically, a ratio of 100: <dig> is considered decisive in terms of model selection  <cit> . in dynamic causal modelling, variational methods predominate, usually under the laplace assumption. this assumes that the posterior density has a gaussian shape, which greatly simplifies both the integration problems and numerics. note that assuming a gaussian posterior over the parameters does not necessarily mean that the posterior predictive distribution over the data is gaussian . computing the required marginal likelihoods is challenging for non-linear problems where such asymptotic approximations to the posterior distribution are not appropriate. here one must resort to more advanced methods such as thermodynamic integration   <cit>  or annealed importance sampling  <cit> . though the bayes factor is a useful method of model selection, determining what to measure in order to improve the bayes factor in favor of the correct model is a non-trivial problem. as such, it provides a means to perform model selection, but not the optimal selection of data features.

experiment design
the approach is based on selecting measurements which provide the largest discriminatory power between competing models in terms of their predictive distributions. the design parameter in the proposed methodology corresponds to the choice and timing of a new observation. in other words, we want to determine which observable should be measured when in order to maximize the divergence between the posterior predictive densities, thereby maximally informing our model comparison. this divergence is quantified by means of the jensen shannon divergence  as it provides a measure of the dissimilarity between the probability density functions of competing models. the jsd is defined as the averaged kullback leibler divergence d
kl
 between probability distributions and their mixture:

  djs=∑i=1kpmidklpy|mi,∑i=1kpmipy|mi. 

here k represents the number of probability densities, p the  probability of model m
i
 and p the posterior predictive distribution. additionally, this metric is monotonically related to an upper and lower bound of the classification error rate in clustering problems  <cit>  and is bounded between  <dig> and  <dig>  in the case where the model that generated the data is in the set of competing models, it is analogous to the mutual information between a new measurement  coming from a mixture of the candidate models and a model classifier . in this study, we opted for comparing models in a pairwise fashion . this allows us to determine which models are distinguished by a new experiment. mutual information has been considered before in the context of experimental design for constraining predictions or parameters of interest  <cit> , but not in the setting of model selection. though appealing for its properties, estimating the jensen shannon divergence for one or more experiments requires integration over the predictive densities, since:

  dkl=∫plog2pqdx. 

here p and q are random variables with p and q their associated densities. considering that only a sample of the ppds is available, it is required to obtain a density estimate suitable for integration. density estimation can be approached in two ways: by kernel density estimation , or by k-nearest neighbor  density estimation. in kernel density estimation , an estimate of the density is made by centering normalized kernels on each sample and computing weighted averages. this results in a density estimate with which computations can be performed. the kernels typically contain a bandwidth parameter which is estimated by means of cross validation  <cit> .

for well behaved low dimensional distributions, kde often performs well. considering the strongly non-linear nature of both the parameter and predictive distributions, a gaussian kernel with constant covariance is not appropriate. as the dimensionality and non-uniformity of the problem increases, more and more weights in the kde become small and estimation accuracy is negatively affected  <cit> . additionally, choosing an appropriate bandwidth by means of cross-validation is a computationally expensive procedure to perform for each experimental candidate.

with k-nearest neighbor  density estimation, density is estimated by computing the volume required to include the k nearest neighbors of the current sample  <cit> :

  pθ→=1nkρkθ→dvd 

in this equation ρk represents the distance to the k
t
h
 nearest neighbor, d the number of dimensions and v
d
 the volume of the unit ball in rd. furthermore, n denotes the number of included samples and v
d
 is given by:

  vd=πd/2Γ 

where Γ corresponds to the gamma function. the advantage of using the knn estimate is that this estimator adapts to the local sampling density, adjusting its volume where sampling is sparse. note, however, that, similar to the kde, this estimator also suffers from a loss of accuracy when estimating high dimensional densities. fortunately, the number of experiments designed simultaneously, and therefore the dimensionality of the density, is typically low. consider y→jmi, a vector of predictions simulated with model m
i
 and parameter set θ→j, where each element of the vector corresponds to a different model prediction. a model prediction is defined as a quantity which can be computed by supplying model m
i
 with parameter set θ→j . for oed purposes, these should be quantities that could potentially be measured. the set of these predicted values coming from model m
i
 shall be referred to as Ωmi. inserting these quantities, the knn estimate of the jsd becomes:

  djs=12nm1∑i=1nm1log2q <dig> +12nm2∑i=1nm2log2q <dig>  

with q
a,b
 given by

  qa,b=2nmbrky→ima,Ωmbdnmbrky→ima,Ωmbd+rky→ima,Ωma∖y→imad 

here d denotes the number of elements in y→jmi , and rkxi,Ωmj corresponds to the euclidean distance to the k
t
h
 nearest neighbor of x
i
 in Ωmj. note that the backslash indicates excluding an element from the set. using this equation, the jsd can straightforwardly be computed for all possible combinations of experiments and used to rank according to how well these experiments would discriminate between the models. a larger value for the jsd indicates an informative experiment. the last step involves sampling several combinations of measurements and determining the set of experiments which have the greatest jsd. the proposed methodology is depicted in figure  <dig> 

testing the method: numerical experiments
to demonstrate the method, a series of simulation studies are performed. since we know which model generated the data, it is possible to compare to the bayes factor pointing to the true model. after generating an initial data set using the true model, ppds are sampled for each of the competing models. as the design variable, we consider the timing of a new measurement. hence, we look at differences between the predictive distributions belonging to the different models at different timepoints. we use a sample of simulated observables at specific timepoints to compute jsd estimates between the different models. we thereby test whether the jsd estimate can be used to compare different potential experiments. the new experimental data is subsequently included and the jsd compared to the change in bayes factor in favor of the correct model. note that this new bayes factor depends on the experimental outcome and that this approach results in a distribution of predicted bayes factors. a large change in bayes factor indicates a useful experiment.

analytic models
the method is applied to a number of linear regression models. linear regression models are models of the form:

  y=∑i=1lθibi+ε 

where θ→ represents a parameter vector and b constitutes a design matrix with basis functions b
i
. given that σ, the standard deviation of the gaussian observation error ε, is known and the prior distribution over the parameters is a gaussian with standard deviation ξ, the mean and covariance matrix of the posterior distribution can be computed analytically . using linear models avoids the difficult numerical integration commonly required to compute the bayes factor and makes it possible to perform overarching monte carlo studies on how these bayes factors adjust upon including new experimental data. the analytical expressions make it possible to compare the jsd to distributions of the actual bayes factors for model selection.

non-linear biochemical networks
to further test the methodology, a series of artificial models based on motifs often observed in signaling systems  <cit>  were implemented . artificial data was simulated for m <dig>  subsequently, inference was performed for all four competing topologies. the difference between each of the models was the origin and point of action of the feedback mechanism .

each of the artificial models was able to describe the measured data to an acceptable degree. we used a gamma distribution with α= <dig> and β= <dig> for the prior distributions of the parameters. this prior is relatively non-informative , while not being so vague that the simplest model is always preferred . data was obtained using m <dig>  observables were bp, of which three replicates were measured, and dp, of which two replicates were measured. these were measured at t=. all replicates were simulated by adding gaussian white noise with a standard deviation of  <dig> . the parameter values corresponding to the true system were obtained by running monte carlo simulations until a visible overshoot above the noise level was observed. parameter inference was performed using population mcmc with the noise σ as an inferred parameter. as design variables we consider both the choice of which observable to measure and the time point of the measurement.

computational implementation
all algorithms were implemented in matlab . numerical integration of the differential equations was performed with compiled mex files using numerical integrators from the sundials cvode package . absolute and relative tolerances were set to 10− <dig> and 10− <dig> respectively. mcmc was performed using a population mcmc approach using n
t
= <dig> chains with a temperature schedule given by tn=ntn <dig> <cit> . this also permitted computation of the bayes factors between the non-linear models by means of thermodynamic integration. the gaussian proposal distribution for the mcmc was based on an approximation to the hessian computed using a jacobian obtained by simulating the sensitivity equations. after convergence, the chain was thinned to  <dig> samples. since the number of experiments designed simultaneously  was reasonably small , the knn search was performed using k-d trees  <cit> . the figures in this paper were determined using k= <dig> 

RESULTS
analytic models
a series of experiments were performed using linear regression models. to demonstrate the method, consider the following four competing models, where m <dig> is used to generate the data:

  ym1=θ1t 

  ym2=θ1t+θ2t <dig> 

  ym3=θ1t+θ2t2+θ3sin15t <dig> 

  ym4=θ1t+θ2t2+θ3sin15t3+θ4sin2tt 

the presence of sine waves in m <dig> and m <dig> elicits particularly noticeable patterns in the optimal experiment design matrices. d equidistantly sampled time points were generated as data . to make sure that the model selection was unsuccessful a priori, these were sampled in a region where the models roughly predict the same behavior. initially, the bayes factors were log10= <dig>  , log10=− <dig>   and log10= <dig>  . ppds were generated for each of the models and used to compute credible predictive intervals that enclose 95% of the predictive density. bayes factors were computed for each of the models. since the aim of the design is to successfully select between the models after performing new experiments, the change in bayes factor in favor of the true underlying model was computed. since the experimental outcome is not known a priori, a distribution of bayes factors is predicted:

  Δ:=elog10pp−log10pp. 

the expectation is taken with respect to new realizations of the data ynd. note that such an overarching estimation would be computationally intractable for a non-linear model. new experiments can be simulated in two ways. either by using the correct model with the true parameter values and adding measurement noise or by taking samples from the posterior predictive distribution of the correct model . in practice these ‘true’ parameter values are not known and the predictive distribution of the measurement based on the posterior samples provides the best obtainable estimate given the current parameter uncertainty. the change in bayes factor  was compared to the jensen shannon divergence between the competing models. large predicted changes indicate that the experiment would result in a successful selection. as for the jsd, a large value indicates a large distance between the joint predictive distributions, marking the measurement as useful. see figure  <dig> for an example of the analysis results. as shown in the different panels, different models parameterized on the same data, result in different posterior predictive distributions . when comparing model  <dig> and  <dig> , we can see that differences in predictions occur mostly beyond the time range previously measured. whereas model  <dig> predicts a straight line, the true underlying system deviates from a single line. consider two new measurements. from the differences in ppds, it is clear that measuring beyond the region where data is available would lead to a decisive choice against model  <dig> as corroborated by the large bayes factor updates shown in the left plot on the middle row . it can also be seen that the ppds differ more for negative time than positive time. therefore the area which is decisive is larger for negative time. the jsds follow this same pattern. the ppd for model  <dig> is less different from the ppd the true model would have generated. for the simulations coming from model  <dig>  we can see that the value for positive and negative time is correlated. for model  <dig>  these prediction values are negatively correlated. consequently, performing one measurement for negative time and one for positive time would lead to the largest discriminatory power.

the jsd agrees well with the actual bayes factor updates as shown in the third row of figure  <dig>  interestingly, all the designs based on the jsd result in designs that would effectively discriminate between the true model and its competitors without having to specify a true model a priori. subsequently, a monte carlo study was performed where a large number of random models were generated and compared. plotting the relationship between the updated bayes factors upon a new experiment and the corresponding jsd typically reveals a monotonic relationship that underlines its usefulness as a design criterion . these analyses were performed for a large number of randomly generated linear models. the spearman correlation coefficient between the jsd and the expected bayes factor averaged at  <dig>  for the experiments we performed .

nonlinear models
in figure  <dig>  we show the different predictions after performing model inference. two sets of ppds were simulated for two experimental conditions. these sets mimic two different concentrations of a signaling molecule, and have been implemented by setting the stimulus u to either  <dig> or  <dig>  to test the effect of measuring multiple predictions, divergence estimates were computed for a large number of different combinations of two measurements. these results are shown in figure  <dig>  each subplot corresponds to a different model comparison. the axis of each subplot is divided into ten sections corresponding to different predictions. within each section, the axis represents time. the color value indicates the jsd, where a large value indicates a lot of separation and therefore a good measurement. note the bright squares corresponding to the concentration of bpcp in each of the models. these high efficacies are not surprising considering that the ppds show large differences between the models for these concentrations . also noticeable is that many of the experiments on the same predictions reveal dark diagonals within each tile. measuring the same concentration twice typically adds fewer predictive constraints than measuring at two different time points, unless the second measurement is performed using a different concentration of signaling molecule . interestingly, measuring certain states during the overshoot is highly effective , while the overshoot is less informative for others . all in all, the information contained in such a matrix is very valuable when it comes to selecting from a small list of experiments. for example, we can also see that considering the current predictive distributions, model  <dig> and  <dig> can barely be distinguished. this implies that in order to actually distinguish between these two, a different experiment is required. such a new experiment could, again, be evaluated by generating a new competing set of ppds.

to test the results, in silico experiments have been performed by simulating new data from the true model and determining the bayes factor change upon including this data. bayes factors were estimated using thermodynamic integration . the calculation of each set of four marginal likelihoods took about  <dig> days of wall-clock time on an intel i <dig> cpu  with matlab r2010a. to validate the method, experiments are selected where differences between models are expected. the following experiments were performed: 

●  <dig>  steady state cp and bpcp concentration

●  <dig>  bp and dp during the peak in the second condition 

●  <dig>  steady state cp

experiment  <dig> should differentiate between m <dig> and m <dig> , but not between m <dig> and m <dig> or m <dig> and m <dig>  experiment  <dig> should give discriminatory power for all models . experiment  <dig> should not provide any additional discriminatory power at all. the results of these analyses are shown in table  <dig>  as predicted, experiment  <dig> leads only to an increase in discriminatory power between model m <dig> and m <dig>  experiment  <dig> improves the discriminatory power between all the models, while experiment  <dig> even reveals a decrease in discriminatory power for model  <dig> and  <dig>  noteworthy is also the large variance observed for experiment  <dig>  which is likely related to the large variance in the steady state predictions of cp. again, the predictions based on the jsd are well in line with the bayes factors obtained.


d

12
Δ

b

12
d

13
Δ

b

13
d

14
Δ

b

14
jsd and change in bayes factors denoted as mean ± standard deviation for each of the reported experiments .

CONCLUSIONS
this paper describes a method applicable to performing experiment design with the aim of differentiating between various hypotheses. we show by means of a simulation study on analytically tractable models that the jsd is approximately monotonically related to the expected change in bayes factor in favor of the model that generated the data . this monotonic relation is useful, because it implies that the jsd can be used as a predictor of the change in bayes factor. the applicability to non-linear models of biochemical reaction networks was demonstrated by applying it to models based on motifs previously observed in signaling networks  <cit> . experiments were designed for distinguishing between different feedback mechanisms.

though forecasting a predictive distribution of bayes factors has been suggested  <cit> , the implicit penalization of model complexity could have adverse consequences. the experiment design could suggest a measurement where the probability densities of two models overlap. when this happens, both models can describe the data equally well, which leads to an implicit penalization of the more complex model . this penalization can then be followed by subsequent selection . though a decisive selection occurs, such an experiment would not provide additional insight however. in  <cit> , this is mitigated by determining the evidence in favor of a more complex model. moreover, computing the predictive distributions of bayes factors required for this approach is computationally intractable for non-linear models that are not nested. by focusing on differences in predictive distributions, both these problems are mitigated, making it is possible to pinpoint where the different models predict different behavior. aside from their usefulness in model selection, such predictive differences could also be attributed to the different mechanisms present in the different models. this allows for follow-up studies to investigate whether these are either artificial or true system behavior.

a complicating factor in this method is the computational burden. the largest challenge to overcome is to obtain a sample from the posterior parameter distribution. running mcmc on high dimensional problems can be difficult. fortunately, recent advances in both mcmc  <cit>  as well as approximate sampling techniques  <cit>  allow sampling parameter distributions of increasingly complex models  <cit> . the bottleneck in computing the jsd resides in searching for the k
th
 nearest neighbor. a subproblem which occurs in many different problems and for which computationally faster solutions exist  <cit> . an attractive aspect of this methodology is that it is possible to design multiple experiments at once. however, the density estimates typically become less accurate as the number of designed experiments increases . therefore, we recommend starting with a low number of experiments  and gradually adding experiments while the jsd is low. density estimation can also be problematic when the predictions vary greatly in their dispersion. when considering non-negative quantities such as concentrations, log-transforming the predictions may alleviate problems. finally, the number of potential combinations of experiments increases exponentially with the number of experiments designed. it is clear that this rapidly becomes infeasible for large numbers of experiments. however, it is not necessary to fill the entire experimental matrix and techniques such as sequential monte carlo sampling could be considered as an alternative to more effectively probe this space. we revert the reader to additional file 1: section s <dig> for a proof of principle implementation of such a sampler.

one additional point of debate is the weighting of each of the models in the mixture distribution used to compute the jsd. it could be argued that it would be more sensible to weight models according to their model probabilities by determining the integrated likelihoods of the data that is already available. the reason for not doing this is two-fold. firstly, the computational burden this adds to the experimental design procedure is significant. more importantly however, the implicit weighting in favor of parsimony could strongly affect the design by removing models which are considered unnecessarily complex at this stage of the analysis. when designing new experiments, the aim is to obtain measurements that allow for optimal discrimination between the predictive distributions under the different hypotheses. optimal discrimination makes it sensible to consider the models equally probable a priori.

the method has several advantages that are particularly useful for modeling biochemical networks. because the method is based on sampling from the posterior parameter probability distribution, it is particularly suitable when insufficient data is available to consider gaussian parameter probability distributions or model linearisations. additionally, it allows incorporation of prior knowledge in the form of prior parameter probability distributions. this is useful when the available data contains insufficient constraints to result in a well defined posterior parameter distribution. because the design criterion is based on predictive distributions and such distributions can be computed for a wide range of model quantities, the approach is very flexible. in biochemical research, in vivo measurements are often difficult to perform and practical limitations of the various measurement technologies play an important role. in many cases measurements on separate components cannot be performed and measurements result in indirect derived quantities. fortunately, in the current framework such measurements can be used directly since distributions of such experiments can be predicted.

moreover, the impact of specific combinations of experiments can be assessed by including them in the design simultaneously which reveals specific combination of measurements that are particularly useful. this way, informative experiments can be distinguished from non-informative ones and the experimental efforts can be targeted to discriminate between competing hypotheses.

availability
source code is available at: http://cbio.bmt.tue.nl/sysbio/software/pua.html.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
jv developed the method, performed the analyses, and wrote the paper. ct, ph and nr contributed to the computational analysis, interpretation of the results and revised the paper. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplementary information.s <dig>  additional information regarding the mcmc sampler. s <dig>  thermodynamic integration. s <dig>  mutual information. s <dig>  analytical expressions for the linear models. s <dig>  nonlinear model equations. s <dig>  spearman correlation jsd and bayes factor. s <dig>  sampling bigger design matrices. s <dig>  choosing the size parameter for the density estimation.

click here for file

 acknowledgements
funding
this work was funded by the netherlands consortium for systems biology . the authors would like to thank h. w. h. van roekel, r. m. foster and c. Çölmekçi Öncü for helpful discussions.
