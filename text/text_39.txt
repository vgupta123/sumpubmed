BACKGROUND
complex human genetic disease
molecular biologists and geneticists alike now acknowledge that the most common human diseases with a genetic component are likely to have very complex etiologies, involving such complicating factors as locus heterogeneity, trait heterogeneity, and gene-gene interactions . however, only a small fraction of the human genetics literature specifically reports on investigations of such complexity. statistical geneticists continue primarily using traditional methodologies, such as linkage and association, which often detect but fail to replicate findings of main effect genes. while undoubtedly many of the original positive results are false-positives, true effects may not be replicated for many reasons, including population stratification and true differences in genetic etiology between study populations  <cit> .

current statistical approaches to detecting heterogeneity, such as the admixture test  <cit> , are neither sensitive nor powerful and can merely account for, not resolve, any underlying heterogeneity. in addition, while a small number of supervised computational methods exist for discovering gene-gene interactions, the power of these methods drops dramatically when locus or trait heterogeneity is present  <cit> . phenotypic data can be utilized to improve the performance of these methods in the face of locus or trait heterogeneity by facilitating heuristic stratification of data. however, for many diseases, little detailed phenotypic data has been collected consistently in combination with genotypic data. it is for these reasons that an unsupervised method, which does not rely on phenotypic data, is needed to mine potentially heterogeneous genotypic data as a means of data stratification and hypothesis generation.

for genetic factors involving heterogeneity, there are multiple independent  variables or else multiple dependent  variables that complicate the analysis by creating a heterogeneous model landscape. in the case of locus heterogeneity, multiple predictor variables  are present, some of which may be unmeasured or unobserved and, therefore, unavailable for inclusion in the disease model. in the case of trait heterogeneity, multiple outcome variables are present, which cannot or have not been distinguished based on the available phenotypic information. gene-gene interactions create a rugged model landscape for statistical analysis. there is clear and convincing evidence that gene-gene interactions, whether synergistic or antagonistic, are not only possible but probably ubiquitous  <cit> . thus, it is critical that complex genetic data sets be properly interrogated for possible underlying interactions.

statistical analysis
no one analytic method is superior in all respects for the range of complicating factors that might be present in a specific data set. given the relative shortcomings of our current analyses in complex diseases, we need to greatly extend the range of available analytical tools. there is a critical need for extensive reevaluation of existing methodologies for complex diseases, as well as for massive efforts in new method development. it is important that empirical studies be conducted to compare and contrast the relative strengths and weaknesses of methods on specific types of problems. for example, while cluster analysis has shown promise in numerous scientific and mathematical fields, including genetics  <cit> , its use with discrete genotypic data has not been adequately explored. similarly, artificial neural networks modified with evolutionary computation have great potential for discovering nonlinear interactions among genes and environmental factors  <cit> . however, work is still ongoing to evaluate their limitations with regard to the heritability and effect sizes that can be detected.

ultimately, though, the real power of existing and yet-to-be-developed methods lies in our ability to marry them into a comprehensive approach to genetic analysis, so that their relative strengths and weaknesses can be balanced and few alternative hypotheses are left uninvestigated. because no single method adequately investigates heterogeneity and interaction issues simultaneously, we propose routinely taking a two-step approach to analysis. for example, clustering or ordered subset analysis  <cit>  can be used first to uncover genotypic and/or phenotypic heterogeneity and to subdivide the data into more homogeneous groups. then in a second step, specific tests of interactions, such as the s sum statistic approach  <cit>  or the multifactor dimensionality reduction method  <cit>  could be used to investigate gene-gene or gene-environment interactions within each of the homogenized subgroups. while this is not a perfect approach, it is an important improvement over the more common alternative of a single-pronged approach to analysis.

cluster analysis
for over  <dig> years, cluster analysis has been used as a method of data exploration  <cit> . clustering is an unsupervised classification methodology, which attempts to uncover 'natural' clusters or partitions of data. it involves data encoding and choosing a similarity metric, which will be used in determining the relative 'goodness' of a clustering of data. no one clustering method has been shown universally effective when applied to the wide variety of structures present in multidimensional data sets. instead, the choice of suitable methods is dependent on the type of target data to be analyzed. clustering has been utilized widely for the analysis of gene expression  data; however, its application to genotypic data has been limited  <cit> .

most traditional clustering algorithms use a similarity metric based on distance that may be inappropriate for categorical data such as genotypes. newer methods have been developed with categorical data in mind and include extensions of traditional methods and application of probabilistic theory. three such methods were chosen for comparison in the task of discovering trait heterogeneity using multilocus genotypes – bayesian classification  <cit> , hypergraph-based clustering  <cit> , and fuzzy k-modes clustering  <cit>  – all of which are appropriate for categorical data.

RESULTS
descriptive statistics
the hubert-arabie adjusted rand index  was chosen as the standard for measuring clustering result fitness  <cit> . an ariha score of  <dig>  indicates excellent cluster recovery,  <dig>  good recovery, and  <dig>  moderate recovery. mean ariha values for bayesian classification, hypergraph clustering and fuzzy k-modes clustering were  <dig> ,  <dig>  and  <dig> , respectively. confidence intervals around the means were also produced to demonstrate the preciseness of the ariha measurements. the results for each method across all datasets are presented in table  <dig>  mean ariha values differed by genetic model type , with higher scores achieved on trait heterogeneity only  datasets for the bayesian classification and hypergraph clustering methods .

comparison of clustering methods
three categorical variables were constructed that could be tested using the nonparametric chi-square test of independence. the three variables were calculated as the number of clustering results achieving each of the three ariha cutoff values of  <dig>  ,  <dig>   and  <dig>  . results are displayed as percentages by clustering method  and by clustering method and genetic model . a chi-square test of independence was performed testing the null hypothesis that the number of clusterings achieving the specified ariha cutoff value was independent of the clustering method. the three methods performed significantly differently on each of the ariha cutoff statistics . bayesian classification outperformed the other two methods. however, across all the dataset parameters, bayesian classification achieved moderate or better recovery on only 48% of the datasets .

the performance of the three clustering methods across different dataset parameters was evaluated in an attempt to find particular conditions under which one method consistently achieved good or excellent recovery . for those datasets simulated under the tho model, bayesian classification performed well, with over  <dig> percent of its resulting clusterings achieving an ariha value of  <dig>  or greater, indicating excellent recovery . for this subset of the datasets, bayesian classification outperformed the other two methods, and again there was a significant difference in performance across the three methods, as measured by a chi-square test of independence on each of the three new ariha cutoff statistics . analysis of the other simulation parameters failed to show as great a difference among methods where the 'winning' method performed as well as the bayesian classification performed in the tho datasets . thus, this subset of data was chosen for further investigation into the efficacy of using the bayesian classification method to uncover trait heterogeneity in real data.

applicability to real data
to evaluate the validity of using the bayesian classification internal clustering metrics – class strength and cross-class entropy – as a proxy for the ariha , permutation testing was performed. resulting p-values for ariha, average log of class strength and average cross class entropy were used to calculate false positive and false negative rates at three significance levels of  <dig> ,  <dig>  and  <dig> . a clustering result was considered a false positive if it was considered significant according to either average log of class strength or average cross class entropy but was not considered significant according to our ariha standard. a clustering result was considered a false negative if it was called not-significant according to both average log of class strength and average cross class entropy but was considered significant according to ariha. figures  <dig> and  <dig> show the false positive and false negative rates, respectively, by alpha level.

the false positive, or type i, error rate was controlled very well at three percent or less for all three significance levels. the false negative, or type ii, error rate was not controlled as well, however. at the least stringent significance level , the type ii error rate was  <dig> percent, and at the most stringent level , the rate was  <dig> percent. other simulation parameters were examined for their impact on the false negative rate, and figures  <dig> and  <dig> show the false negative rate by alpha level paneled by number of nonfunctional loci and number of affecteds , respectively. as might be expected, the lowest false negative rates were achieved for datasets with the lowest number of nonfunctional loci  and the greatest sample size .

discussion
data simulation
the new data simulation algorithm produced complex genotypic datasets that included trait heterogeneity, locus heterogeneity and gene-gene interactions. most existing simulation software that attempt to simulate heterogeneity do so by allowing the user to specify what portion of the dataset is to be simulated under one model versus another, and the resulting individuals are simply combined into one dataset. in the new algorithm, however, the disease penetrance models, which were used to simulate the data, were constructed so that overall prevalence levels were controlled, allowing naturally occurring overlaps, in which some individuals would have both traits  by chance. this novel data simulation algorithm should prove very useful for future studies of other proposed genetic analysis methods for complex diseases.

comparison of clustering methods
the bayesian classification method outperformed the other two methods across most dataset parameter combinations, with the exception of the most complex model  on which fuzzy k-modes clustering performed best. when the results were further examined to find a set of parameters for which one or more methods performed well, bayesian classification was found to have achieved excellent recovery for 73% of the datasets with the tho model and achieved moderate recovery for 56% of datasets with  <dig> or more affecteds and for 86% of datasets with  <dig> or fewer nonfunctional loci . neither hypergraph clustering nor fuzzy k-modes clustering achieved good or excellent cluster recovery even under a restricted set of conditions.

bayesian classification was obtained as closed-source software, for which there were numerous parameters, which could have been optimized. initial parameter settings were chosen as recommended by the authors based on the type of data being analyzed. however, it is possible that alternative settings may have yielded better results. for example, for datasets with the more complex genetic models, greater numbers of nonfunctional loci and smaller sample sizes, the maximum number of classification trials and/or the maximum number of classification cycles per trial may need to be longer, and those parameters concerned with convergence rate and stopping criteria may need to be changed to delay convergence. if improvements in performance could be achieved with reasonable time and resource tradeoffs, such changes would certainly be desirable. the results of this simulation study are encouraging enough to warrant further investigation of this matter.

it was disappointing that hypergraph clustering did not perform very well under most conditions, despite its intuitive appeal as a method that would find frequently-occurring multilocus genotypic patterns. the hypergraph clustering method has been reported to work well with very large variable sets , which have complex patterns for which large numbers of clusters  were relevant  <cit> . however, there has been no examination of the method's performance on smaller variable sets. thus, it is possible that the restricted patterns present in our multilocus genotypic data were too simple and sparse and that the method is simply tuned to search for more complex patterns. also, we were required to devise a translation of the resulting partitioning of genotypes into a clustering of individuals. we tested several such translations and implemented the best process out of several tested. oftentimes, even when the method correctly chose the functional genotypes to be in different partitions, too many other nonfunctional genotypes were also chosen, which meant that the difference between an individual's likelihood of belonging to one cluster versus another was too small, making the choice of cluster assignment almost arbitrary.

the fuzzy k-modes clustering method performed comparably to bayesian classification for the more complex datasets and was much less computationally intensive. it has been widely reported that the performance of k-means algorithms is highly variable depending on the method of seeding the initial cluster centroids  <cit> . while we used the recommended method of selecting individuals from the dataset to serve as the initial cluster modes, we perhaps could have achieved better results if we implemented an additional step to ensure that the initial centroids were substantially dissimilar to each other. this is supported by evidence that when the fuzzy k-modes clustering resulted in only one cluster , the initial centroids were very similar and the method had converged early so that individuals had equal probability of belonging to any of the clusters. in such cases, the individual was arbitrarily assigned to the first cluster, thereby leading to all other clusters being empty.

as expected, the simpler the model, the better the performance of the three clustering algorithms, with the exception that the hypergraph clustering and fuzzy k-modes clustering methods performed somewhat better on the thb  datasets than they did on the thl  and thg  datasets. likewise, in general, the fewer the nonfunctional loci and the larger the sample size, the better the performance was.

applicability to real data
to determine the efficacy of using the bayesian classification method on real data, the reliability of its internal clustering metrics at finding good clusterings was evaluated. using the combination of the average log of class strength and the average cross class entropy to determine significance, the false positive rate was controlled very well, at three percent or less for all three significance levels. the false negative rate was acceptably low  for the less stringent significance levels of  <dig> . however, it was high  for the most stringent significance level of  <dig> . thus, if a clustering of data were called significant according to permutation testing using either the average log of class strength or the average cross class entropy, we can be quite confident that the result were real. typically geneticists prefer to accept a higher false positive rate to increase power; however, there is indeed a trade-off between these two types of error. valuable time and resources can be spent on follow-up studies, and it can be very detrimental to pursue leads that do not have a good chance of yielding new information about the disease under study.

CONCLUSIONS
the efficacy of three clustering methodologies at uncovering trait heterogeneity in genotypic data was investigated. one method, bayesian classification, was found to perform very well under certain conditions  and to outperform the other methods. permutation testing confirmed that the method could be used on real data with excellent type i error control and acceptable type ii error control. by controlling the false positive rate so well, bayesian classification offers a comfortable degree of certainty with regard to the hypotheses that it generates. this is true at least when the underlying data structure is similar to that simulated under the tho model. further investigation of how different parameter settings may improve the performance of bayesian classification is planned.

