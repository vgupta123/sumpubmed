BACKGROUND
the molecular phenotype of a coding non synonymous snp or disease associated mutation describes the functional and structural properties of a protein that are affected by a single amino acid substitution  <cit> . in this study we want to address whether the concept of the in silico determined molecular phenotype can be employed for large-scale classification of snps and disease mutations. the attempt to classify a large set of mutations based on an incomplete molecular phenotype may seem naive at first glance, had it not been suggested that individual properties such as protein stability, the accessibility of the amino acid substitution site, and the location of variants in surface pockets are predictive determinants of the phenotypic effect of a variation  <cit> . a comparative study of protein stability predictors by blundell and co-workers demonstrated that although protein stability changes caused by mutation can be relatively accurately estimated in silico, these predictions by themselves do not yield accuracy on large-scale classification between benign and disruptive mutations  <cit> .

furthermore, computational analyses rely heavily on the quality of the data under scrutiny and the computational methods used to evaluate these data. before investigating  <dig> structural properties of proteins and amino acid substitutions for their predictive power regarding snp classification, we have investigated what major liabilities are encountered when implementing an structural approach to snp annotation and classification. the results are compared with those achieved by the best performers among the state-of-the-art tools.

RESULTS
in this study we have identified the common issues that are encountered when performing large-scale analyses of structural properties of human coding variation. the first issue concerns the availability of structural data for nssnps and disease mutations, while the second involves the availability of computational tools to predict structural properties. the last issue concerns the quality of classification: are the training and evaluation data sets used in the analyses sufficient to extrapolate results for larger studies, and do the properties used have sufficient predictive power to separate the two data sets?

structural coverage of human genetic variation
despite structural genomics projects, the gap between sequence and structural information is still wide, and the coverage of variation data with structural data is estimated to be as low as 14%  <cit> . we have investigated the boundaries of structural coverage by varying the quality requirements on the structural model , the sequence identity between query sequence and modelled structure , the percentage of the wild type sequence covered by the structural model , and the length of the alignment between query and target . circa 12% of all nssnps present in the ensembl variation database  can be mapped on a structural model, in accordance with the estimate cited previously. however, this percentage is valid only when no restrictions regarding sequence identity, sequence coverage or structure quality are applied.

in figure s1a we see that the number of snps covered by structural data drops after 40% sequence identity. requirements on sequence identity sufficient for prediction are different for various methods. yue & moult  <cit>  found a sequence identity of 40% sufficient for accurate prediction, while chasman & adams  <cit>  obtained the best results with identities higher than 60%. however, these methods do not use full atomic detail to assess the structural properties of an amino acid substitution, and thus to do not require high sequence identity to be able to model the substitution. we use the foldx force field to model amino acid variation on structural models, which uses an all-atom representation of the structure. although this introduces high accuracy of stability estimation  <cit> , it also requires high quality structural models. our standard restrictions on building high-confidence structural models using the foldx force field are x-ray structures with a resolution lower than  <dig>  Ã… and sequence identity higher than 80%. applying these restrictions to the ensembl data results in a data set of  <dig> nssnps .

other factors in fluencing the structural coverage of snps is the length of the alignment and the percentage of coverage between the query sequence and the structural model. a realistic criterion for human proteins to apply would be to request the structural alignment to be about  <dig> amino acids long, or, for proteins shorter than  <dig> residues, to cover more than 80% of the query sequence. when this criteria are combined with the need for high quality structural data, we find that  <dig> nssnps remain in the data set. a summary of the number of snps covered by high quality structural data, in combination with criteria regarding the reliability of the nssnp data, is shown in table  <dig>  in this table we see that the application of stringent criteria will result in the structural mapping of very few nssnp data.

several criteria resulting from the above analyses are applied to assess the structural coverage and reliability of that coverage of human snps in the ensembl database, as well as the overlap of the structural coverage with quality parameters for the validation and frequency status of the polymorphism data.

predictability of structural properties
the second issue for a large-scale structural bioinformatics approach is the structural properties that are predictable with state of the art tools: how well can we describe the structural behaviour of a protein and its mutants? previous structural studies have identified protein stability, aggregation and misfolding as determinants of correct functioning on the single protein level  <cit> . mutations affecting the functional sites of a protein, such as dna, ligand and protein interaction sites, are not considered within this scope, but the investigation of these sites will most certainly be of great importance to assess the impact of amino acid substitutions.

tools have been developed that describe the structure and dynamics of a protein: stability, aggregation, amyloidosis, and folding. we have used computational methods that are capable of assessing the effects of a mutation on protein stability , aggregation  and amyloidosis . although algorithms exist that can predict folding of small single domain proteins , to date no computational method exists that can predict folding events on large multi-domain proteins, or that is applicable in genome wide studies.

although we have limited ourselves to the analysis of structural features of single protein molecules, and have not investigated protein-protein interactions in this study, we have included an analysis of the binding of proteins to molecular chaperones, as it is directly related to correct folding of the protein. the high abundance of chaperones in the cell emphasises their crucial role in the protein quality control system  <cit> , but this is not reflected in the availability of computational tools for chaperone binding. we have used the only available tool, the hsp <dig> binding predictor limbo  <cit> , to assess chaperone binding variation caused by amino acid alteration.

the predictive power of structural properties
following the recommendations of care et al  <cit> , we have used the swissprot annotated disease and polymorphism data  as the evaluation data for our analyses. mapping of these variants on high quality structural models  yielded a data set of  <dig> positive  mutations and  <dig> negative variations  in  <dig> proteins. to ensure that the analyses are comparable, we applied the sequence based predictors to the same small data set as the predictors that use 3d structures or structural models.

before we evaluated the discriminative power of the individual structural parameters, we wanted to assess whether our data showed distinguishable patterns for three important parameters. the first two criteria, stability difference and the degree of burial of the mutation site, have previously been identified as providing information about the severity of a mutation  <cit> . the third criterion is difference in aggregation propensity, which has been cited as likely to be an important factor in disease susceptibility  <cit>  but thus far has not been applied in a proteome wide mutation analysis.

in a first series of properties to test as classifiers, we have investigated  <dig> properties of the amino acid substitution site that contribute to the assessment of the effect of the mutation using the foldx algorithm . cut off values were generated that varied between the minimal and maximal values measure for the specific property, and the true and false positive rate, and the matthews correlation coefficient  were calculated for each cut-off value. table  <dig> lists the data for both the best mcc and the mcc <dig>  i.e. the coefficient that is measured at high specificity . the corresponding roc curves for these analyses can be found in supplementary figure s <dig> in additional file  <dig> 

the false positive rate  and the true positive rate  for the threshold on the specific property that gave the best matthews correlation coefficient  are shown. mcc <dig> is the matthews correlation coefficient for a specificity of 90% . the roc curves corresponding with the evaluation of all properties can be found in supplementary figure s <dig> in additional file  <dig>  foldx was used to evaluate both the overall stability contribution of the amino acid substitution site in the modeled structure and the various factors involved in this stability. the entropy of the variant amino acid was calculated using a sampling strategy to assess the possible side chain conformations allowed at the substitution site. both stability and entropy were calculated for all mutations and for a subset of buried mutations  and surface mutations . corresponding roc curves are shown in supplementary figure s <dig> in additional file 

the same strategy was then applied to predicted values of structural differences between mutant and wild type proteins . statistics were calculated for stability and entropy parameters, as well as for differences concerning protein aggregation, amyloidosis and chaperone binding .

foldx was used to evaluate both the overall stability difference between wild type and variant structure, and the constituting contributions leading to this stability difference. the entropy difference caused by the amino acid substitution was calculated using a sampling strategy to assess the possible side chain conformations allowed at the substitution site. both stability and entropy difference were calculated for all mutations and for a subset of buried mutations  and surface mutations . corresponding roc curves are shown in supplementary figure s <dig> in additional file  <dig> 

the results obtained from these detailed analyses are unanimous: none of the parameters evaluated can be used to separate the data. all mcc values are close to zero, and thus the predictions are no better than a random predictor would perform on the data. the high accuracy of foldx for quantitative stability prediction has been proven in various studies  <cit> , so we have high confidence in our stability estimations. in accordance with the analyses of worth and co-workers  <cit> , we find that high stability differences alone are no sufficient criterion to distinguish deleterious mutations and neutral variation. these results show that the dominant effect of for instance stability that was proposed in earlier large-scale studies  <cit>  can not always be generalised for other data.

the fact that none of the properties representing conformational differences between wild type and variant protein contain enough information to distinguish neutral and deleterious variation implies that large-scale classification based on singular structural properties is not feasible and requires a better understanding of how the complex interplay between biophysical and biochemical properties of a protein conspire to different tolerance for mutations in different proteins. although we can predict the structural and functional impact of a mutation of a protein, it is not always feasible to translate this into a prediction of the overall phenotypic effect, i.e. will the mutation result in a disease phenotype or not.

recent studies that combine structural and evolutionary information using machine learning techniques are able to classify relatively large data sets obtained for the swissprot database successfully . although the combination of these two types of information improves classification of disease mutations greatly, the incorporation of sequence conservation measures may obscure the mechanism underlying disease. low frequency substitutions at conserved positions suggest that the mutation will not be tolerated, but will not teach us what the underlying reason of disease is. although knowing that an amino acid is critical for correct function is of course useful, in a structural bioinformatics approach the focus is more on the molecular mechanism underlying disease.

a simple combination of the snpeffect structural bioinformatics toolsuite on our evaluation data set showed that in our case, at least a linear combination of these methods is not sufficient to classify the data . although we have not fully explored the predictive power of the properties in a more sophisticated approach, such as machine learning techniques that use non-linear combinations , the results obtained in the previous analyses have highlighted a major issue in disease mutation classification. the failure of the classification is mainly due to false positives, i.e. neutral mutations that are predicted to be deleterious. to assess the "predictiveness" of our data set, we applied the well-established evolutionary method sift  <cit>  to our data and found that sift was also not able to classify effectively the data. in fact the results were even worse than our naive classifier .

as an illustration of the influence of the data set used for evaluation on the performance of a predictor, we list the results for the variation in performance of snp classification of sift, that uses evolutionary information to label snps . the matthews correlation coefficient varies between - <dig>  on our data set over  <dig>  on human mutagenesis data, up to  <dig>  on the hiv- <dig> protease mutagenesis set in the original sift paper  <cit> . although the methodology and underlying data  is certainly sound, and there is no question that sift in most cases can be trusted to evaluate whether or not an amino acid change is tolerated in an evolutionary sense, this variability in classification success illustrates the importance of the choice of training and test data to build and evaluate predictors. sift was trained to classify mutations that disrupt the function of a protein, and may suffer from the same limitations as our structural approach. the ability to predict which mutations will affect function does not imply the ability to predict which mutations cause disease.

to date, we have not explored a machine learning technique that incorporates the functional effects predicted by our tools. some of the problems above may be improved by using non-linear combinations of the structural properties. since we pose great value in the interpretation of the classification, rule based techniques such as decision trees are our prime choice of machine learning technique. studies using random forests to classify snps show results similar to other state-of-the-art classification techniques as support vector machines  <cit> , and we plan to implement such techniques using the structural parameters described in this manuscript in the near future.

CONCLUSIONS
the concept of using the molecular phenotypic effect of a nssnp to assess its effect on the structure and function of the protein it alters was first introduced by bork and co-workers  <cit> . the question has been raised to how much of this molecular phenotype is necessary to evaluate the contribution of a snp to a disease phenotype: are there singular dominant properties that determine the impairment of structure and function, or do we need to consider the full ensemble of molecular properties to interpret the impact of the snp? other research groups have proposed that single properties such as stability  <cit>  and solvent accessibility  <cit>  can be used to classify snps. we have examined all the individual structural bioinformatics tools that were proposed in the snpeffect toolsuite  <cit>  for their ability to act as a binary classifier for deleterious and neutral snps. neither of the individual properties that were examined could serve this purpose. because several approaches were able to classify similar data sets as the one we have used, we applied the most used evolutionary method, sift  <cit> , to our data set. as it was not able to classify our data set accurately, we argued that generalisation of the results presented by the state of the art classifiers might be an important issue. we illustrated this problem with the variability of performance of sift on  <dig> different data sets used in various analyses.

from these analyses we concluded that strict classification of snps is not feasible at the time, both because there are still many technical difficulties to overcome, and because the biological interpretation of the molecular phenotype in relation to a disease phenotype is a complex matter. even at the single molecule level, we cannot assess how tolerant a specific protein is to structural variation. the inherent rigidity of a protein might influence the change in stability that is allowed before severe conformational changes are introduced. furthermore, on the cellular level biological interpretation is even harder: we can not predict the role of the protein quality control system plays in this tolerance level, not all interactions are described at the molecular level, and much more. even if we can predict the molecular effect accurately, this might not necessarily result in a disease phenotype because of functional redundancy of the protein.

however, not being able to classify human variation into disease mutations and neutral or beneficial variation does not mean that this approach or the methods developed are useless. by using high quality bioinformatics tools, we can select from a large pool of variations the candidates that are interesting for detailed investigation. this in itself is a valuable contribution, because the amount of variation data available is too massive to be investigated experimentally. in silico analyses can and will be used successfully as an addition to in vitro and in vivo studies.

