BACKGROUND
the number of proteins requiring functional characterization in the uniprot knowledgebase  <cit>  is still growing. although proteins can be electronically annotated using existing resources  <cit> , the most reliable and detailed annotation is still manually extracted from the literature by a team of experts. the problem with knowledge archived in the literature is that it is represented in scientific natural language where a variety of text phrases can be used to describe the same concept. traditionally, this information could be deciphered by humans but was not easy to interpret computationally. furthermore, the number of biological databases has also increased so that up-to-date annotation relies on the ability to integrate information from multiple sources.

currently, one of the most important advances in database annotation, querying and interoperability is the development and use of structured vocabularies. in this regard, one of the most successful is the 'gene ontology'   <cit> . since  <dig>  the goa database  <cit>  at the ebi has used go to provide consistent descriptors for proteins in its uniprot knowledgebase in the categories of molecular function, biological process and cellular component.

with the success of go's integration into the analyses of microarray  <cit>  and mass spectrometry data  <cit> , academic and pharmaceutical institutions are keen to fast-track the assignment of go terms to large datasets. consequently, a new generation of tools have been developed which aim to predict go annotations using interacting networks  <cit> , existing protein features  <cit> , sequence  <cit>  and semantic similarities  <cit> . numerous text mining systems  <cit>  have also attempted this task or reported results on aspects of this task.

while some of these tools are useful, others demonstrate a lack of understanding of how go is used and queried by a biologist. for example, the go term 'cell adhesion'  has been experimentally verified as a process involving the protein icam <dig> but to assign that go term automatically to every paper that mentions the protein icam <dig> is simply incorrect. every article that mentions icam <dig> will not experimentally verify that process within its text; instead, it might simply describe the sequence. annotating go terms to biomedical literature in this way is not useful to curators, as the go term is often not attached to a 'relevant' paper. for developers of automatic information extraction and retrieval techniques, however, this strategy might form part of a useful intermediate step to limit the number of go terms to be searched in a given piece of text.

so what do go curators really need? a useful tool would allow curators to retrieve all 'relevant' papers which report on the distinct features of a given protein and species and then to locate within the text the experimental evidence to support a go term assignment. given that go is not designed for text mining, it is of no surprise that exact text strings of many of the  <dig>  go terms will not be found verbatim in the literature. despite these difficulties, goa is often asked to evaluate various automatic go retrieval and extraction systems. to encourage their comparison and development and to save time in individually evaluating the different strategies, the goa team was delighted to take part in task  <dig> of the biocreative  challenge.

biocreative task  <dig> was designed to assess if automatically derived classification using information retrieval and extraction could assist biologists in the annotation of the go terminology to proteins in uniprot. for the training set, participants were provided with papers linked to go annotations from human proteins already publicly available  <cit> . for the test set, we annotated  <dig> blind  human proteins with go terms using the full text of  <dig> journal of biological chemistry articles. we manually evaluated  <dig>  segments of text, which were provided to support the correct go term and protein predictions. in this paper, we give a biological perspective on the evaluation, explain how we manually annotate go using literature and offer some suggestions to improve the precision of future text retrieval and extraction techniques. finally, we provide the results of the first inter-annotator agreement study for manual go curation, as well as results assessing our current electronic go annotation strategies, to help to establish a threshold for the text mining technology.

methods, 
RESULTS
current electronic and manual go annotation strategies at the ebi
one of the distinguishing features of the uniprot knowledgebase is the high level of annotation and database cross-references that are integrated with each entry. it therefore makes sense that the large-scale assignment of go terms to the proteins in uniprot should exploit the existing knowledge stored in these entries  <cit> . enzyme commission  numbers and swiss-prot keywords have been manually curated into uniprot entries for many years. a manual mapping of go terms to these existing vocabularies allows go terms to be retrofitted to appropriate uniprot records. similarly, uniprot records contain cross-references to the interpro and hamap  databases  <cit> . this is because the associated sequence contains features  which provide evidence for their membership in a particular protein family. based on a review of the literature of the well-annotated family members, go terms are manually mapped to interpro and hamap records. these mappings are released monthly with every goa release and provide a useful first pass electronic go annotation in the goa database. as of march  <dig>  this strategy provided 69% go coverage of uniprot records for over  <dig>  species  <cit> . surprisingly, biocreative participants did not appear to exploit these released go annotations to help limit the go lineages that might be found in the test set papers. later in this paper, we will explore the accuracy of these electronic go annotations and compare the results to the text mining systems used in biocreative.

electronic techniques are efficient in associating high-level go terms to large datasets. on the other hand, manual curation provides more reliable and detailed go annotation but is slower and more labour-intensive. it is clear that the manual curation process requires automatic assistance. however, before attempting to develop strategies to help curators make more rapid go assignments, it is important to first understand current manual approaches.

each go consortium member uses slightly different techniques in locating papers suitable for manual go annotation  <cit> . the following describes the approach of the goa curators. first we have to decide which human proteins to prioritize for go annotation. we concentrate on  <dig> categories,  those which have no go annotation,  those which have disease relevance and  those which are important for microarray analyses. having chosen the protein accession to annotate, we now need to find relevant scientific papers. the first step is to decide if the papers already linked within the uniprot entry are relevant for go annotation. the decision on whether to read the full text of a paper is based on the curator's interpretation of the text used in the paper title or abstract. the journals cited in uniprot/trembl records are inherited from embl/ddbj/genbank databases  <cit>  and so may describe the sequence rather than go function, process or component. papers that reference the sequence are accompanied by a remark located in the reference position  line, which says 'sequence from n.a.' . on the other hand, uniprot/swiss-prot records are manually supplemented with documents to support the annotation stored in the comment  lines. in these cases, the remark in the rp line might also indicate the type of information extracted from a paper e.g. 'subcellular location', 'function', 'interaction'. it should be noted, however, that the use of the word function in swiss-prot is not the same as 'molecular function' usage in go. frequently, go process terms can be extracted from function cc lines.

in addition to the papers archived in the uniprot records, the ncbi pubmed advanced search  <cit>  is queried to find papers that support supplementary go annotation. various combinations of the gene and protein, full and abbreviated names are searched. initially, searches are limited to 'title' or 'title/abstract' and to 'human entries only'. electronic go annotation and information in uniprot/swiss-prot cc lines often provide curators with an insight into the types of functions that could be extracted from the literature. with this information to hand, curators are able to refine their search options to find more than enough relevant papers for go annotation. in goa, our current aim is to find the most recent papers which provide experimental evidence for the unique features of a given protein. our approach is protein-centric rather than paper-centric as it is not necessary to read all the relevant papers that might be used to assign the same go term. in the future, however, adding more papers to experimentally verify a given function will provide greater confidence to the go annotations. a good source of a complete set of functional annotations is often retrieved from recent review articles. these reports often have links to relevant papers with experimental verification. any papers that report new data are fed back to the uniprot curators to add to the original entry.

most go consortium members would agree that the most difficult task in searching the literature is finding papers that have experimental information for a given species. often, the species 'name'  is not mentioned in the 'title' or 'abstract' and occasionally, not directly mentioned in the full text. on these occasions, the method section of the paper has to be read and perhaps the taxonomic origin of a cell line identified before any attempt at go curation. filtering 'human entries only' via pubmed is not always accurate. in addition, authors do not always cite the most up-to-date gene nomenclature e.g. use of upper case letters for human gene symbols  <cit> . this is likely to affect the precision of automatic 'gene product' entity extraction techniques.

finding functional annotation and choosing the correct go term
once a relevant paper is found, the full text is read to identify the unique features of a given protein. the majority of papers will mention more than one protein; however, a curator will concentrate on capturing the information pertinent to the main protein chosen for annotation. most curators still prefer to print out papers rather than view papers online. this is simply to limit computer eye strain and because a curator can quickly scan and select the most relevant parts of the document for curation. words or short phrases which can be converted to go terms are highlighted by hand and the correct go term identifier  is documented in the paper margins for review.

go terms are chosen by querying the go files with the quickgo web browser  <cit>  or with a local copy of dag-edit  <cit> . before assigning a go term, the definition must be read to check its suitability. obsolete go terms , obsolete cellular component , obsolete biological process ) are not used in annotation. when electronic or manual go annotations become obsolete, they are manually replaced with an appropriate term  <cit> . the reason for the obsoletion and suggestions for replacement go terms are documented in go comment lines. if a useful term is missing from the ontology, an existing go term is in the incorrect hierarchical position or a definition needs to be refined, a curator request is sent to the go editorial office using sourceforge  <cit> .

the go consortium avoid using species-specific definitions for go nodes; however some function, processes and component are not common to all organisms. inappropriate species-specific go terms  should not be manually annotated to mammalian proteins. sometimes these inappropriate terms can be distinguished by the sensu  designation . curators are cautious when manually assigning these terms. to avoid generating inappropriate go term assignments, the text mining community should read the go consortium documentation on the subject  <cit> .

if a curator is unsure of which process term should accompany a function term, they can consult the 'often annotated with' section of the quickgo browser. here, go terms that are assigned in tandem are displayed. these are also referred to as common concurrent assignments and are calculated on our existing manual and electronic go annotations  <cit> .

it is important to note that go terms are often extracted from particular regions of a paper. furthermore, according to go consortium rules, each go annotation must be accompanied by a pubmed identifier and one of  <dig> manual go evidence codes  <cit> . table  <dig> shows the important regions of the paper for go annotation. the 'materials and methods' section of a paper is only used to identify the species of protein used in the research and to determine which go evidence code should be used. it is not used to extract functional annotation. furthermore, curators often piece together information from different parts of a document to reinforce a decision to annotate. go annotation is not associated with uniprot entries until the entire article is read.

if no functional annotation can be found for a given protein after an exhaustive literature search, the go terms molecular_function unknown , biological_process unknown  or cellular_component unknown  can be assigned with go evidence code nd .

biocreative task  <dig> training set
it is clear from the above that the manual go annotation effort has many steps, which could be assisted by automatic information extraction techniques. for these reasons, biocreative organizers designed a biologically motivated task which asked systems to identify the proteins in the text, to check if any functional annotation was present and to return the go term id representing this information and the evidence text that supported the annotation.

to train systems to perform this task accurately, thousands of manual go annotation examples were required. the training data provided to participants is documented online  <cit> . essentially, the training set was extracted from the publicly available non-redundant human go annotation dataset   <cit> . it consisted of approximately  <dig> manual go associations linked to uniprot accessions, pubmed ids and go evidence codes. it was advised that go annotations with go evidence codes 'inferred from sequence/structural similarity' , 'inferred by curator'  judgment and 'no data'  should be ignored.

it is important to note that historically, most of the human go annotations in the goa database were generated before  <dig>  approximately  <dig> manual annotations were integrated from the former proteome inc. , which may or may not have been extracted from full text, while an additional  <dig> proteins were annotated by uniprot curators from abstracts only, as part of a fast-tracking strategy. these annotations can be identified in the goa database with go evidence codes nas or tas  <cit> . since  <dig>  full text articles are always read but the annotation and thus the creation of a large and useful training set is slow. these data can be identified in goa by extracting terms with the go evidence code, ida, iep, imp, igi or ipi  <cit> . as a result, the number of useful training data will be relatively small and will represent relatively few go terms. furthermore, the relevant passages of the text used in curation were not marked in the training set. as such, the training data was not equivalent to the task allocated . this was unavoidable given current annotation approaches and may have affected the precision and recall abilities of some systems in the first biocreative challenge. however a positive outcome of the biocreative evaluation is that marked passages useful in go curation have been manually verified and made available for future training.

biocreative task  <dig> test set
to create the biocreative test set, goa was asked to associate  <dig> papers with human proteins and go terms. the journal of biological chemistry'   was chosen by the organizers because of an arrangement to use the full text openly and freely. we chose a set of jbc articles already associated with human proteins within the uniprot flat files. this set was then filtered for proteins that had no previous manual go annotation. these criteria ensured that the annotations created for the test set would be new to both the goa database and the participants. in total, a list of  <dig> uniprot accessions together with the pubmed id of the article was distributed to  <dig> curators. a new go annotation tool was created to collect the go associations and to ensure that they would not be released or touched by other uniprot curators not involved in the biocreative challenge. the test set took the curators one month to complete . during this period,  <dig> distinct go terms were extracted from text within the papers. the evidence text was highlighted on paper and therefore not in a format for machine processing. on average, each protein had  <dig> go annotations. during the curation process, these go annotations were associated with the proteins from  <dig> other mammalian species  based on their sequence similarity to the human proteins. to prevent participants from back-extrapolating the test set annotations, associations with the evidence code 'iss' were also suppressed from goa releases. in table  <dig> an example is provided which shows that the human protein for 'estrogen receptor beta' has been annotated with the go term 'estrogen receptor activity' using pubmed id:  <dig>  because this protein has high-level sequence identity with the mouse ortholog, the original go annotation has been transferred from the human entry to the mouse 'estrogen receptor beta' protein  with the go evidence code 'iss'. the 'with' column indicates the source  of the go annotation. it would be easy to extrapolate back the original go annotation that was assigned to the human protein.

one difficulty in creating the test set was that curators were often restricted to a single article per protein. normally, a curator would seek verification of author statements from more than one paper. as a result, some articles were slightly over annotated compared to the normal curation process.

the test set was released to the biocreative organizers on  <dig> november  <dig>  it was advised that participants should not use versions of go archived in the cvs repository beyond this date. this was to ensure that the same go ontology files were available to both the annotators and participants. the test set was suppressed from the monthly goa release until january  <dig> 

evaluation tool and criteria
biocreative organizers created an online evaluation tool for task  <dig>  for subtask  <dig> , the tool displayed the uniprot accession in the test set, along with associated 'known' go terms and documents. participants were expected to return a segment of text  from the document that supported the annotation of the 'known' go term. the provision of evidence text was critical for the evaluators as it provided a basis for rejecting or accepting that finding. evidence text was visible to evaluators by means of a red text highlight. the full text surrounding the evidence text was also visible in black or blue font. the evaluation tool was easy to use and was designed with the evaluators to closely resemble a curation aid that might develop from this technology. two goa curators evaluated subtask  <dig> . there were  <dig> distinct users for this task but  <dig> separate runs were submitted for evaluation.

in the second subtask , participants were given the document and the associated uniprot accession and asked to return evidence text to support their system's go predictions for that protein . participants were aware of how many go process, function and component terms the curator had manually extracted from the document. this task was understandably more difficult for the participants and was evaluated by a single curator. there were  <dig> participants for this subtask but  <dig> separate runs were submitted for evaluation. in total,  <dig>  individual results were submitted for review. because of the lack of time and the expense of a  <dig> month evaluation, only  <dig>  text highlights were assessed. in these cases, entire proteins were skipped so that all participants were affected in the same way. in both subtasks, numbers 1– <dig> anonymously represented the participants. there was a problem visualizing the text highlights in the tool for some users. without the supporting text, go and protein predictions could not be evaluated. towards the end of the evaluation, some of the highlighting problems were fixed and  <dig> proteins with go associations were re-evaluated for user numbers  <dig> and  <dig> 

the curators made two separate evaluations of the evidence text: did it support the correct go term? did it support the correct protein association? to ensure the consistency of evaluations, criteria were agreed amongst the  <dig> evaluators and biocreative organizers .

it was clear from the evaluation and biocreative workshop  that not all participants understood the content of go or how it is used during annotation. the common mistakes collected by curators are presented in table  <dig> together with some suggestions for improvements. we hope this will be helpful in future biocreative challenges. the major problem that slowed down the evaluation considerably was that that the vast majority of text highlights were too long. even though the precision in go predictions was usually low, the evaluators had to re-read the passages for each go term, participant and run. these results, however, reflect the difficulty systems had in finding a piece of text that contained reference to both the query protein and the functional annotation. it may be more useful to curators and future evaluations if these entities are highlighted separately within the full text.

inter-annotator agreement
after the biocreative evaluation, goa was asked to perform an inter-annotator evaluation to measure how consistently the curators could precisely recall go annotation from the literature. this was important to judge the ceiling on performance that can be expected from the text mining systems for the same task. to speed up the study, each of the  <dig> evaluators randomly chose  <dig> papers that they had already curated during the creation of the biocreative test set and passed them to another curator. the second curator extracted go terms from the text blindly. in total,  <dig> papers were co-curated. the  <dig> sets of go terms extracted from the text were divided into  <dig> categories  exact term match ,  same lineage ,  new/different lineage . at the end of the study, the  <dig> curators evaluated together the go terms extracted in the  <dig> categories. results indicate that there is 39% chance of curators exactly interpreting the text and selecting the same go term, a 43% chance that they will extract a term from new/different lineage, and a 19% chance that they will annotate a term from the same go lineage . see additional file  <dig> for the original data used to perform this analysis. this variation is not surprising since curators are taught to annotate according to their individual level of confidence. this will vary according to how well the topic covered by the article matches the curators' biological background. it was also clear that  <dig> curators together would create a more accurate and complete go annotation of a protein than an individual. this phenomenon was light-heartedly referred to as the 'superhuman complex' at the biocreative workshop. variation is acceptable between curators but inaccuracy is not. fortunately, the inter-annotator agreement showed that 94% of the time curators were precisely extracting go annotation from the literature. 72% of the time curators recalled all possible valid go terms from the text. this creates a particularly high threshold for text mining systems which, in task  <dig> , precisely predicted go terms only 10–20% of the time.

evaluation of in-house electronic go annotation techniques with manual annotations created for the task  <dig> test set
as described earlier, the large-scale go annotation of uniprot entries involves electronic techniques based on transitive mappings . it was of interest to goa to also evaluate the precision of these annotation strategies. taking the manual go annotation created for the biocreative test set, we again compared the number of times the different electronic techniques predicted go terms exactly, with the same lineage and less granularity , same lineage and greater granularity  or new lineage. it should be noted that electronic predictions that exactly matched or represented a parent term of a manually annotated term were assumed to be correct. electronic go predictions that represented a new lineage or a child term to those chosen manually could be potentially correct or incorrect. this is because the go annotations represented in the biocreative test set were based on the curation of just a single article and therefore not fully curated. in agreement with goa release statistics, interpro2go  provided the most go coverage of the test set followed by spkw2go  and ec2go , data not shown. because the go function terms predicted by the ec2go mappings were quite deep/final node go terms, it was not surprising that 67% of the time they exactly matched the manual go annotation . the interpro2go  and spkw2go  mappings, however, were more likely to predict a higher level/ less granular term than those chosen manually. given that this was an automatic evaluation, the precision of electronic go term predictions was calculated based on new or more granular go terms being either correct or incorrect. as a result, a precision range is presented for each electronic strategy. in the worst case scenario, interpro2go, spkw2go and ec2go precisely predict the correct go term  <dig> to 70% of the time. on the other hand, all strategies were capable of up to 100% precision. the reason for this level of accuracy is because these electronic strategies rely on a manual mapping step based on quite high level go terms. as stated earlier, it was noticed by curators that the biocreative systems evaluated tended to over-predict go terms. it is more important for database curation to be accurate than to have complete coverage.

to further evaluate how precise our electronic strategies were, we manually evaluated a random set of  <dig> proteins that had both electronic and manual go annotation. this time, we verified whether the go predictions were correct or incorrect. there was little difference in the precision of each strategy and our electronic annotation was between 91–100% precise . these results suggest that, at the moment, our current large-scale go annotation protocol is more accurate than text mining technologies presented during the first biocreative challenge.

summary
the goa database currently provides 69% go coverage of the uniprot knowledgebase using in-house electronic and manual annotations as well as annotations integrated from go consortium members including mgi  <cit> , sgd  <cit>  and flybase  <cit> . the analyses presented in this paper indicate that these techniques have high precision  but every year, the number of new proteins requiring go annotation increases. as such we need to develop new techniques to increase go coverage without compromising on high quality annotation. we used the biocreative functional annotation challenge as an opportunity to help the research community, which might in turn ultimately help us to speed up our curation progress. the results of biocreative task  <dig> indicate that the prediction of go terms and provision of supporting text evidence is a difficult task. however, given the simple mistakes that were made and the creation of relevant training data during the evaluation  <cit> , the improved performance of text mining systems in the next biocreative challenge is inevitable. to supplement this training data and limit the expense of future evaluations, a tool that would allow curators to highlight and even link several important sentences that support a go annotation might be useful. ultimately, future functional annotation challenges could be evaluated semi-automatically by matching the highlighted regions of text.

although go was not designed with text mining in mind, it does try to create a vocabulary for biological research that could be deciphered by both humans and machine processing. the complications in matching exact go terms in the literature might be resolved when the go consortium implement their plans to decompose the go phrases into individual words or concepts and properties and by the mapping of more synonyms to go terms  <cit> .

CONCLUSIONS
improvements in the performance and accuracy of text mining should be expected in the next biocreative challenge. in the future we hope it will offer a useful supplement to the manual and electronic techniques already employed by goa.

authors' contributions
ra heads the uniprot knowledgebase and organized the collaboration with the biolink group. ebc coordinates the manual curation of goa database, drafted the manuscript and performed the statistical analysis . dgb coordinates the automatic classification of go terms in the goa database and worked closely with jm and db to create a go annotation tool used to build the biocreative task  <dig> test set. dgb also generated the data for table  <dig>  ebc, ecd, and vl helped to create the training and task  <dig> test set as well as evaluating the task  <dig> subtasks and the reliability of in-house electronic techniques. mm coordinates the manual curation of uniprot at ebi and was involved in creating the training set and in the design of the biocreative test set. all authors read and approved of the final manuscript.

supplementary material
additional file 1
this shows further details of the inter-annotator agreement. it contains individual counts for each uniprot accession and pubmed identifier that was co-curated.

click here for file

 acknowledgements
we would like to thank alfonso valencia, lynette hirschman, christian blaschke, alexander yeh and mark colismo for organizing the biocreative challenge and would also like to praise the community effort of the go consortium.

the goa project is supported by grants qrlt-2001- <dig> and qlri-2000- <dig> of the european commission and a supplementary grant, hg-o <dig> from the national institute of health .

figures and tables
where precision is the fraction of manual go term annotations that are correct . recall is defined as the fraction of correct go term annotations that were successfully retrieved during manual annotation . new lineage annotations minus incorrect annotations represent total number of the go terms that the curators should have correctly retrieved from the paper. f-measure =  =  <dig> × p × r/.

where the go evidence code iea is 'inferred from electronic annotation'  <cit> . 'same lineage > granularity' means where the electronic mapping  predicted a go term that was in the same lineage/branch as the manually curated go term but represented a more granular/parent term. 'total potential incorrect' annotations = 'same lineage >granularity' + 'new lineage'. 'total minimal correct' annotations = 'exact term' + 'same lineage < granularity'. percentage calculations are represented in parentheses.

percentage calculations are represented in parentheses.
