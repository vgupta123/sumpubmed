BACKGROUND
proteins carry out the functions of cells; so information on the proteome, i.e., the complement of proteins and the changing levels of individual proteins, can serve as the basis for understanding natural and disease-related cellular processes. tandem mass spectrometry  has become a staple approach for defining the proteome. after cleaving proteins to be identified into peptides, the resulting sets of peptides can be analyzed quickly and relatively inexpensively in "shotgun" fashion; large numbers of peptides are separated by their mass to charge ratio  and fragmented sequentially, producing mass spectra that can be analyzed. the identity of the protein is inferred from the identification of the peptide which in turn is based on the experimental determination of the masses of the whole peptide and its fragments in the mass spectrometer. one at a time, peptide ions having the same m/z are sequestered, fragmented, and the m/z of the fragment ions determined. ideally, the fragmentation occurs randomly at one of the peptide bonds, generating a series of b and y ions .

working backwards from the mass spectra to decipher the nature of the parent peptide, and by inference the protein, can be challenging, and this has been the focus of many research groups. one common approach is database matching: the spectra obtained experimentally are compared against virtual mass lists generated from known protein sequences. gaining in popularity is de novo sequencing, where algorithmic methods are used to determine the amino acid sequence of the parent peptide from the spacing and positions of spectra peaks, which correspond to the m/z of the peptide fragments. when examining spectra, it is common to look for the peaks with intensities that are much higher compared to the background  as such peaks are more likely to correspond to the b and y ions <cit> . this correlation does not always hold, however, so that simply eliminating peaks based on a ranking by intensity may cull data that is crucial for identification. in a characteristic collisional-induced dissociation  spectrum, the fifth most abundant fragment has intensity ten times lower than that of the most abundant fragment <cit> .

fragmentation can also occur at other points on the peptide. a common fragment generated by collisional-induced dissociation, for instance, is the a ion, which has a mass lower by  <dig> amu compared to the nearest b ion as it has also lost the c = o. fragments that have amino acid residues with a hydroxyl group may lose h2o , those that have amino groups may lose nh <dig> . the recognition of these additional fragments can help to distinguish one sequence over another. when peaks corresponding to these fragments are of low intensity, however, it can be difficult to distinguish between valuable information and background noise.

no matter what method of peptide identification is used, there is room for improvement. when four database search methods were compared, there was good agreement on half of the proteins identified in total  <cit> . in another report,  <dig>  tandem mass spectra were obtained for a mixture of known proteins. of these, only  <dig>  were able to be identified by the database matching program sequest <cit> . in a review of  <dig> de novo sequencing methods, allmer <cit>  points out that with current instrumentation and algorithms, an all-or-nothing score, ie. correct or wrong sequence, may be too harsh because very few sequences would be 100% correct. accuracy of identification is low because noise in the experimental data is a significant challenge. this is exacerbated when fragment ions that should theoretically exist are not detected or are of such low intensity that they cannot be distinguished from noise. choo and tham <cit>  illustrate the impact of background noise when they calculated a quality score  from self-convolution of the mass spectra for a theoretical 14-residue peptide subjected to different degradation processes. they showed that removal of  <dig> y ion peaks  reduced the qs by only 43% whereas increasing white gaussian noise from  <dig> to 30%  resulted in an 80% drop in their calculated qs. clearly, background noise poses a significant challenge to recognition especially of low-intensity peaks.

when thousands of ms/ms spectra are generated in a single experiment, removal of poor quality spectra from analysis can be time-saving. this was the approach taken by flikka et al. <cit>  who used machine learning to distinguish between good and bad spectra by using a number of spectral features. many such methods for selecting spectra to analyze such as the one used by wong et al <cit>  are partially based on the signal to noise ratio. simple methods for eliminating noise, such as removing all peaks below a certain threshold or selecting only a fixed number of the highest peaks have been explored. purvine et al <cit>  proposed a method to distinguish signal from noise. they sorted intensity values of the peaks in descending order, and defined the median value of the lower half of the intensity values to be the noise level. also with the goal of recognizing poor quality spectra, xu and freitas <cit>  first sort peaks according to intensity, from lowest to highest. the lowest intensity is considered to be noise. the algorithm starts by calculating the signal to noise ratio  for the second peak, comparing that to the snr threshold set by the user. if the snr value for a given peak is higher than that set by the user, the noise level is set to that peak. using database searching techniques, this algorithm was demonstrated to work better than other filtering approaches. a large percentage  of ms/ms spectra that did not yield a peptide match were removed, while only  <dig> - <dig> % of those that yielded a peptide match were removed. in these examples, the intensity of background noise is determined and applied uniformly across the spectrum. as noise levels vary across a spectrum, these methods can be too aggressive in some areas, while not being aggressive enough in others.

ding et al <cit>  takes into consideration changes in snr in different regions of the spectrum. intensities of peaks whose m/z values indicate relationships such as complementary b and y ions, fragments with and without loss of water or ammonia are adjusted upward, after which peaks that represent maxima in local regions of the spectrum are extracted. all other peaks that are presumed to be noise are eliminated. the de-noising method removes about 69% of peaks in a spectrum, and increased the number of spectra that could be identified by 14-31%, depending upon the dataset used. using such a linear combination approach and applying different weights to the intensity adjustment of the peaks, lin et al <cit>  were able to increase the success of identification by a further 14-23%. we propose an alternative approach that allows for noise levels to be automatically adjusted across an entire spectrum in a continuous manner.

the focus of this paper is on a new method to filter noise across an entire spectrum by modeling peak intensity using orthogonal polynomials. we designate different regions of each spectrum, giving the user a choice of how many such regions  to set. we filter out noise through construction of orthogonal polynomials to fit a curve to the noise level in separate bins. we demonstrate the effectiveness of the approach using spectra generated from peptides of proteins with known sequences. we track the number of significant peaks  while still noting but preserving the non-signficant peaks during filtering. this new filtering method is compared to the filtering methods of purvine et al. <cit>  and of xu and freitas <cit>  by application to the large dataset generated from a mixture of purified proteins <cit>  and an original dataset for one protein that was generated in-house. using a de novo sequencing method that we are developing, we assess the impact of our filtering method with the published methods on the accuracy of the amino acid sequences ascribed to each peptide.

a new filtering method
if a mass spectrum were to contain only a full complement of b or y ions, peptide identification would be trivial; one could simply calculate the difference between peaks. in practice, spectra are cluttered with an abundance of peaks representing a wide range of possible fragmentations or modifications.

one might note, however that the minimum mass difference between a pair of b ions, or a pair of y ions, is equal to the mass of the smallest amino acid - and in general, the difference will be larger than this minimum. the sequences of b and y ions are overlaid, but one can be assured that these primary ions will not be clustered together, and that typically, they will dominate their local neighborhoods.

noise levels can vary across a spectra; to separate significant ions from noise, the degree of filtering should also vary. the method pursued by our group can be seen as an extension of a recent method for filtering of data proposed by xu <cit> . the peaks of the spectrum are sorted from smallest to largest based on intensity, i^k where k =  <dig>   <dig>   <dig>  . . . , n. these peaks are referred to as an, and we use the naming scheme used by xu.

the xu algorithm finds noise levels by using the peaks in the data set combined with linear regression to calculate the snr. this process starts by examining the second smallest peak and calculating the snr ratio. because there is only one peak available, a user defined value referred to as œÅ is used to calculate the snr.

 i^2=*i <dig> 

 snr=iki^k 

if the snr ratio is greater than a user defined snrmin, then the process is halted, and the noise level for that bin is set at i <dig>  however, if the snr ratio is not greater than snrmin, peaks  <dig> to n are examined using a linear regression model described by xu <cit> . each peak is examined in order until the calculated snr is greater then the snrmin. the equations for this are as follows:

 ¬†¬†i^k=Œ±*k+Œ≤=‚àí1ati‚Äâ‚Äâ 

 a=1121‚ãÆ‚ãÆk-11andi=i1i2‚ãÆik- <dig> 

where Œ± and Œ≤ are linear regression parameters.

orthogonal polynomials
the algorithm by xu proved to be effective for many of the spectra we examined, but for others, it was easy to identify cases where the noise levels were either too high or too low. in practice, the level of noise differs across spectra, and also the masses at which noise levels change with a spectrum vary. this observation indicated that using a single noise level over a spectra was not ideal; we first adapted the xu approach to use a set of equally spaced bins, with each bin being assigned an individual noise level.

with multiple distinct noise levels, we saw a number of problematic situations. there were discontinuities between adjacent bins, where noise levels could vary considerably. to address these shortcomings, we pursued an approach for fitting a curve to the calculated noise levels; this allowed for smooth changes along the x-axis.

one well known numerical technique to fit a curve to a set of data points is to use a kth order orthogonal polynomial. in order to do this, a model for our problem was defined as

 y=‚àëj=0kŒ≤jœàj 

with k being the polynomial degree. the least squares estimate of parameters, under an orthogonal polynomial, defined as:

 Œ≤^j=‚àëi=0nyiœàj‚àëi=0nœàj <dig> 

because our data is equally spaced, and the mass values can be transformed by the equation

 half¬†-¬†range¬†t=1n+12¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†xi=i=xi-xtdifference¬†between¬†points 

the system of orthogonal polynomials ascribed to chebyshev can be used  <cit> :

 œà0=1œà1=Œª1xœà2=Œª2)œà3=Œª3x)œà4=Œª4x+3560) 

the values for Œªr are chosen such that the tabulated values for all œàj's are positive or negative integers, while n is the number of data points used. these polynomials have been extensively calculated by pearson and hartley with their calculations being used in our experiments <cit> . the method, and notation, are typical for this type of numerical analysis.

once Œ≤^j's have been tabulated, the filtering level at each of the recorded masses is then calculated. this is done by first transforming each of the masses defined by equation  <dig> above. these transformed masses are then placed into the orthogonal polynomial equation with the result being the noise level at that mass. an example of a noise level created in this manner can be seen in figure  <dig> 

experimental 
RESULTS
in this section, we demonstrate the impact of our new filtering approach. we have performed a series of experiments using spectrum data obtained from our research lab , and also samples obtained from the peaks research group <cit> , the pepnovo research group <cit> , and two data sets provided by keller et al <cit> .

our first set of experiments tracks the percentage of primary peaks and secondary peaks compared to noisy peaks. primary peaks are associated with any b or y ion, while secondary peaks are any a ions, or primary peaks coupled with a loss of water or loss of ammonia. noisy peaks are any peaks that are not classified as either primary or secondary. some data sets have been centroided . both centroided and non-controided data sets were used to illustrate that our method can be used to filter a variety of different data. for data sets that were not centroided, all peaks within  <dig>  amu were removed from the number of noisy peaks since they could be considered to be a primary or secondary peaks themselves. we use this metric, as it is similar to the work performed by bern <cit> .

in a second set of experiments, we use our de novo sequencing approach <cit> , with the spectra pre-processed by the variety of filtering methods. the results are reported with tables that show the number of identified amino acids over the whole data set.

data sets considered
to evaluate the different filtering methods, experiments were first run against a total of  <dig> spectra from the previously mentioned sources and  <dig> spectra generated in-house. each set of spectra contained a different amount of initial noise as well as primary and secondary ions. table  <dig> illustrates the initial nature of the data sets. for the data sets generated in-house, the primary and secondary peaks constitute  <dig> % of all observed peaks. by eliminating the lowest 10% from consideration, the percentage rises to  <dig> %. if one keeps only the highest 10% of peaks, the percentage of primary peaks rises to  <dig> %. it should also be noted that the keller and pepnovo data have been centroided. this has significantly decreased the number of peaks in each of their data sets, resulting in higher percentages table  <dig> 

there are a few important points to highlight with regards to the data. first, for any m/z value, there can only be an integer number of ions observed; a spectrum is in essence a discrete histogram. as one adjusts the "noise threshold," there can be large jumps in the number of eliminated peaks. second, as the filtering becomes more aggressive, and peaks are eliminated, the percentage of primary and secondary peaks increases - but it is frequently the case some of these valuable peaks are lost. to carry this to an extreme: if one were to filter, and retain only a single y ion - 100% of the remaining peaks would be primary in nature, but peptide identification would be nearly impossible.

comparison of filtering methods
for comparison, we first evaluated a filtering technique developed by purvine et al <cit> , known as spequal. this uses a single set of filtering parameters. the spequal method eliminated  <dig> % of the primary ion peaks and  <dig> % of the noisy peaks; it does not support snr adjustment.

both the xu algorithm, and the approach presented in this paper were applied with snr ratios of  <dig>   <dig>   <dig>   <dig>   <dig> and  <dig>  this range of snr ratios was chosen, as they vary from being very gentle to very aggressive, giving a good overview of the trends.

for our approach, bin sizes of  <dig>   <dig>   <dig>   <dig> and  <dig> were used. the use of three bins results in broad, smooth curves. as most peptides encountered had between five and twelve amino acids, there is little to be gained by having a higher number of bins - the curve fitting approach works best if there are primary ions in each bin. to adjust the flexibility of orthogonal polynomial curve fitting, polynomials with degree  <dig>   <dig>   <dig>   <dig> and  <dig> were used for each of the respective bin sizes.

there are constraints on the polynomial degrees - for a case with only three bins, one can have at most a degree two polynomial. as the number of bins increases, more complex polynomials become possible; at most, the degree can be one less than the number of bins. as the number of bins increases, however, we observed diminishing impact of higher degree polynomials - beyond degree  <dig>  we saw no significant change in the shape of the curves.

the results of these experiments can be found in tables  <dig>   <dig>   <dig>   <dig> and  <dig>  in comparing our approach against the xu and purvine methods, our approach was able to produce a better peak to noise percentage in every data set except for the peaks data set. for the peaks data set, the xu method produced the best results. on average, our method was able to retain twice as many primary ions than the xu method.


snrmin

snrmin

snrmin

snrmin

snrmin
protein and peptide identification
the end goal for mass spectrometry is the identification of peptides and proteins; the filtering of data is a means to that end. in this section, we detail experiments performed by our de novo approach, to show the impact of the filtering methods.

in order to make the nature of our experiments clear, we will be precise on the metrics used to evaluate an identification.

any sequence compared against a known reference can be evaluated in terms of amino acids that match in both sequences, and at the appropriate mass position. to be specific, we give the following example.

suppose that the original peptide was the sequence "abc," while the sequence suggested by the de novo algorithm was "wc," with the mass of ab being equal to w. the two sequences match on one amino acid, "c." the proposed sequence would be scored as being 33% correct. an alternative proposed sequence "cw" would not match on any amino acids, as the common c is not at the same position in terms of peptide mass. similarly, a proposed sequece of "cab" would also have no matching amino acids.

note that if the correct sequence was "wc", while the proposed sequence was "abc," the match would be 50% correct, with the correct sequence containing two amino acids. for consistency, we use the number of amino acids in the known sequence as the denominator, while the number of correctly matching amino acids is the numerator.

in all cases, the mass of a sequence suggested by an identification method should match that of the reference sequence .

the mass  present in the reference sequence has a significant impact on identification accuracy; the following tables illustrate this. our de novo sequencing approach produces a set of possible identifications; we report the best match from the top  <dig> scored sequences. in practice, de novo predictions are compared to databases of known peptides - it is generally not necessary to get an exact match, but rather get "close enough" for complete identification. with close matches, a database method could be used to help further the identification process.

the first set of results can be seen in tables  <dig> and  <dig>  these tables show the average percent of the amino acid chain identified over the entire data set. in each of these tables, our approach was able to identify, on average, more of the amino acid chain than the previous methods. however, what is interesting to note is which of the configurations produced the best results. for the peaks data, it came from using a setup of  <dig> bins with an orthogonal polynomial of degree  <dig> and a snr ratio of  <dig>  by contrast, the best results from pepnovo data were obtained using a set up of  <dig> bins with an orthogonal polynomial of degree  <dig> and a snr of  <dig>  these two experiments illustrate the merit of being able to tune a filtering method to data; given the diversity of spectrometry equipment, it is unlikely that there is any single "best" approach.


snrmin

snrmin
in our final set of experiments, we performed the same sequencing tests on the data provided to us from the keller group. these results can be found in tables  <dig> and  <dig>  again, it can be seen that the filtering method provided in this paper allowed for more accurate peptide identification. we note, however, that the identification accuracy is fairly low. after examining the spectra and the identifications, it appears that the data sets have a rather high error tolerance for both spectrum peaks as well as precursor mass. our de novo sequencing method is relatively sensitive to this; we are working on methods to make our approach more robust.


snrmin

snrmin
though our method produced the best result in tables  <dig> through  <dig>  the difference between our method compared to the other methods was not always significant. this can be easily seen in table  <dig> where the difference between the best percentage from our method and the percentage from the purvine method is  <dig> %. even small improvements are beneficial, however: when using de novo sequencing in combination with a database search tool such as blast, better initial sequencing results lead to more accurate blast results <cit> .

summary and 
CONCLUSIONS
the work presented in this paper is an expansion upon prior work described in an extended abstract <cit> . additional data sets have been considered, the description of our orthogonal polynomial approach has been expanded, and comparisons are made with a number of other methods.

in this paper, we have focused on filtering methods for ms/ms spectra. higher peaks are indicative of "primary" ions, but peak levels can vary considerably across a spectrum. simple fixed value cuts can remove important peaks, while step functions can introduce abrupt discontinuities.

the approach we have pursued here constructs an orthogonal polynomial function, which gently adjusts to track average peak intensities across a spectrum. using this method, we are able to filter by intensity at a "local" level - this helps preserve peaks at higher mass values, which can be critical for correct identifications.

on experiments with data obtained by our research group, and from the group which developed the peaks research group, we find that our method preserves a greater percentage of b and y ions compared to the prior method by xu. by adjusting the number of bins, and the desired snr ratio, the aggressiveness of our approach can be tuned easily.

in experiments with our de novo identification approach, the filtering improves results. across a wide range of spectra, top scoring sequences produced algorithmically matched the known peptide sequences.

experiments with a number of different approaches, described in tables  <dig> through  <dig>  allow for a few observations. first, we observe that the purvine method works well in many cases, and is consistent; a shortcoming is relatively little ability to adjust to incoming data. the xu method is more flexible, and can adjust noise levels, but performance can vary considerably, as evident in tables  <dig> and  <dig>  the spectra contained in the pepnovo data set have relatively uniform peak heights, with little variation between noise and signal; this proved challenging for the xu method. the approach we describe improves on this work, providing effective filtering across a wide range of spectra.

