BACKGROUND
next generation sequencing has revolutionised many areas of biology by providing a cheaper and faster alternative to sanger sequencing. one technology that is finding many applications, for example in de novo genome sequencing, or diversity studies of regions of dna that have been amplified by pcr, is  <dig> pyrosequencing  <cit> . it is this latter application of  <dig> to the sequencing of pcr products or amplicons that we will focus on here.  <dig> pyrosequencing generates large numbers of reads,  <dig>  in the original gs flx implementation increasing to  <dig>  with titanium reagents, which are long compared to other pyrosequencing platforms,  <dig> bp for gs flx increasing to around  <dig> bp for titanium. this makes it ideal for high resolution studies of the sequences and their relative frequencies in relatively long pcr products. the method is to simply sequence the diverse amplicon sample without cloning individual sequences. this has many applications for instance in viral population dynamics  <cit> , or characterising microbial communities through amplification of 16s rrna genes  <cit> .

per base error rates from  <dig> pyrosequencing are comparable to those from sanger sequencing  <cit>  but without cloning resequencing is impossible. in addition, the large read numbers obtainable mean that the absolute number of noisy reads is substantial. consequently, it is critical to distinguish true diversity in the sample from noise introduced by the experimental procedure. this is particularly true if we want to calculate the absolute number of different sequences, or clusters of sequences, present. this is effectively the problem in microbial diversity estimation, where sequences are clustered into operational taxonomic units  that proxy for traditional taxa and we are interested in estimating the number of such otus in a community. it has already been noted that noise in  <dig> amplicon sequencing leads to inflated estimates of otu number  <cit> . this is important because surprisingly large otu numbers together with a bias towards rare taxa were observed in the first studies of pyrosequenced 16s rrna genes  <cit> . this preponderance of rare taxa has been termed the 'rare biosphere'. the spurious otus generated by noise generally have low frequencies consequently noise may explain both the high otu numbers and the bias towards low abundances reported. development of effective noise removal algorithms is therefore a matter of urgency in the exploration of microbial diversity.

pyronoise is a relatively sophisticated algorithm that reconstructs the true sequences and frequencies in the sample prior to otu construction using a mixture model  <cit> . it is based on clustering flowgrams rather than sequences which allows  <dig> errors to be modelled naturally. using this approach it is possible to account for two facts: firstly that sequences with errors are likely to be rare and secondly that they should be similiar to a true abundant sequence. the mixture model approach allows this to be done in a very natural way by fitting noise distributions around each proposed true sequence. the drawback is that an iterative and hence computationally costly algorithm must be used.

two sources of error need to be considered in pyrosequenced amplicons. those from the pyrosequencing itself and those introduced by the pcr amplification. the original implementation of pyronoise simultaneously removed both sources of errors. consequently it was necessary to align flowgrams. here we present a new approach, ampliconnoise, which couples a fast flowgram clustering step without alignment, still called pyronoise, to a sequence based clustering, seqnoise, which does perform alignments. the latter explicitly accounts for the differential rates of nucleotide errors in the pcr process, and uses sequence frequencies to inform the clustering process. the result is a more sensitive program than the original pyronoise achieved at lower computational cost because the fast alignment free flowgram clustering reduces the data set size for the slower sequence clustering. ampliconnoise has already been used to determine gut microbial diversities  <cit>  and for viral population genetics  <cit> .

recently another flowgram based denoising algorithm, denoiser, has been developed  <cit> . this was motivated by the original pyronoise and uses the same flowgram alignments but incorporates several modifications to increase speed. it begins by finding unique sequences, orders them by frequency, and then starting with the most abundant maps the other reads onto these 'centroids' if their distance to the centroid is smaller than some threshold. the distance used is the same flowgram based measure as in the original pyronoise. it is therefore a greedy agglomerative clustering rather than iterative. this reduces the computational costs of the algorithm but misassignment of reads when the true sequences are similar, may result in a loss of ability to accurately reconstruct otus. an even faster approach is to perform the same centroid based clustering using sequence rather than flowgram based distances, this is referred to as single-linkage preclustering  by huse et al.  <cit> , and a similar strategy is adopted in the pyrotagger program  <cit> . in this paper we will describe ampliconnoise fully for the first time and compare to the original pyronoise algorithm, denoiser, and slp in terms of ability to remove noise and allow accurate otu construction. we will use  <dig> pyrosequencing data from known sequences for these evaluations including both standard gs flx and newer titanium data.

seqnoise accounts for pcr single base errors but the pcr process can also introduce sequences that are composed of two or more true sequences, so called 'chimeras'. these generate sequences that are quite different from either parent and so can not be removed by clustering. chimeras are generated when incomplete extension occurs during the pcr process and the resulting fragment effectively acts as the primer in the next round of pcr. existing algorithms for removing chimeras were developed for full length clone sequences and lack the sensitivity for short pyrosequencing reads  <cit> . the program chimeraslayer is to our knowledge the only current chimera checker capable of handling  <dig> reads effectively. chimeraslayer requires a reference data set of sequences that are known to be non-chimeric  <cit> . we introduce a new algorithm, perseus, based on two novel principles for chimera removal: firstly because the parents of any chimera must experience at least one more round of pcr than the chimera then we can search amongst all those sequences of equal or greater abundance than the chimera for possible parents; secondly that chimera removal should, with suitable training data sets, be treated as a problem in classification or 'supervised learning'. adopting these principles perseus has the sensitivity to remove chimeras from  <dig> bp gs flx reads with the advantage of not requiring a set of good reference sequences.

we will now demonstrate that ampliconnoise followed by perseus is capable of removing the vast majority of erroneous reads from  <dig> pyrosequencing data, reducing overall error rates, and thereby allowing accurate otu construction and microbial diversity estimation.

methods
test data sets
to test the noise removal algorithms we used eight previously published test data sets and one hitherto unpublished. these were all generated by preparing mixtures of known dna sequences in known concentrations and amplifying and pyrosequencing. for the published data sets the standard gs flx protocol was used but specifically for this study we generated a further data set with the most recent titanium reagents. three different 16s rrna regions were amplified in all cases with a standard taq polymerase, the v <dig> region  <cit> , the v <dig> region  <cit> , and for the titanium data v4-v <dig>  the mixtures consisted either of 16s rrna clones in the case of the v <dig> and v4-v <dig> data sets or dna extracted from  <dig> separate isolated organisms for v <dig>  two v <dig> data sets were prepared: one 'divergent' data set comprising  <dig> clones that differed at a least 7% of nucleotide positions mixed in equal proportions, facilitating the unambiguous mapping of each read to a known clone, and one 'artificial community' where some clones differed by just a single nucleotide and concentrations varied by two orders of magnitude mimicking a natural community. the v <dig> 'mock communities' were similarly split between three replicates where the extracted dna was mixed in equal proportions  and three where it was mixed unevenly . full details of these test data sets are available from the original publications  <cit> .

the titanium data was generated by pyrosequencing a mixture of  <dig> full length 16s rrna clones obtained from an arctic soil sample. these clones were independently sanger sequenced although only  <dig> sequences could actually be recovered. consequently the results presented here will be a lower bound on accuracy with a few sequences falsely categorised as errors that should be in the sample. since this will apply equally to all the tested algorithms our ability to compare between them is not affected. the mixture contained each clone in equal abundance. this dna mixture was then pyrosequenced following amplification with 16s rrna primers that also had a tag  and the standard titanium a and b adaptors attached. the primers used were both degenerate, f <dig>  and r <dig> . sequencing was forward from f <dig> so as to capture the v <dig> and most of the v <dig> region with a 400- <dig> bp titanium read.

the 16s rrna region amplified, and whether references were mixed in equal, 'even', or varying, 'uneven' proportions are summarised together with the read number, following filtering and after the qiime filtering used by the denoiser algorithm. all data sets were generated by gs flx except that denoted titanium.

origins of pyrosequencing noise
pyrosequencing is a sequencing by synthesis method. single molecules of dna are attached to beads and subjected to emulsion pcr to generate multiple identical copies. the beads are then localised into separate wells on plates that contain hundreds of thousands of such wells. these wells contain the enzymes and substrates necessary for dna synthesis such that if synthesis occurs light is emitted. each base in turn  is then washed across the plate. if a well contains a dna molecule where the next unpaired base is complimentary then a signal is observed. if a homopolymer is present then further synthesis will occur and the signal is increased. consequently a well should emit at least one signal in each frame of four bases. the pattern of light intensities, or flowgram, emitted by each well can then be used to determine the dna sequence. the standard base-calling procedure is to round the continuous intensities to integers. errors occur because the observed light intensities do not perfectly match the homopolymer lengths. instead, a distribution of light intensities is associated with each length. we will denote this distribution by p  where the observed signal intensity is f and the homopolymer length that generates it n. these distributions calculated for the three 'even' v <dig> 'mock communities' are shown in figure  <dig>  the variance of these distributions increases with length consequently the probability of a miss-call where a signal is rounded to the wrong integer does too. these appear as either insertions or deletions.

filtering noisy reads
it has previously been shown that some features correlate with noise in reads from  <dig> data  <cit> . consequently filtering for those reads can reduce the overall level of noise in the data set. however, substantial noise remains after this process  <cit> . the purpose of this study is not to evaluate different filtering methods for reads, but rather to address the problem of how to account for this remaining noise. consequently we adopted a rather strict filtering procedure: first we checked for an exact match to the primer and tag if present, we then used the observation that signal intensities between  <dig> - <dig>  are associated with noisy reads  <cit> . we therefore truncated all reads at the first such signal, or any sequence of the four nucleotide flows that failed to give a signal ≥  <dig> , any read where this occurred before flow  <dig> of the  <dig> flows in a gs flx run we removed. for the titanium reads which have  <dig> flows we used the same procedure keeping only reads where the first noisy flow occurred on or after  <dig>  in addition, the level of noise in reads increases towards the ends of reads  <cit> , to account for this we removed the last 10% of flows truncating all gs flx reads at flow  <dig> and titanium at  <dig>  the numbers of reads after filtering for each data set are given in table  <dig> 

removing pyrosequencing noise
our original algorithm for noise removal  <cit> , began with the observation that to describe pyrosequencing noise it is natural to use distances defined not with sequences but with the flowgrams. to define these distances we begin by calculating the probability that a given flowgram f¯= of length m will be generated by a sequence of nucleotides s¯ that maps to a perfect flowgram, i.e. one generated without noise, Ū . assuming that each signal is independent then this is simply the product of the individual probabilities of each signal. we then take the negative logarithm of the probability and normalise by the flowgram length m :

 d′=−log)/m=∑i=1m−log/m=∑i=1md/m, 

to generate a total distance which is simply the sum of the distances for each signal. we used the v <dig> distributions, figure  <dig>  to calculate flowgram distances for all the gs flx data sets but calculated new distributions for the titanium data.

pyrosequencing noise removal can be posed as the problem of inferring the true sequences and their abundances in the sample given the observed reads. we begin by defining the likelihood of the observed data. this has a natural interpretation as a mixture model where each component of the mixture is a different true sequence. we assume that each of the n flowgrams indexed i are distributed as exponentials about the l true sequences indexed j with a characteristic cluster size σp:

  ℱ=expσp 

the likelihood of the complete data set d is then the product of the probabilities that each read was generated from the mixture of sequences with relative frequencies τj:

  ℒ=∏i=1n. 

to infer the true sequences and their frequencies we maximise this likelihood using an expectation-maximization  algorithm. em algorithms apply very naturally to complex clustering problems. intuitively they exploit the fact that if the properties of the cluster centres are known, then the probability that a given data point was generated by a given cluster centre is easy to calculate, similarly if those probabilities are known then maximum likelihood solutions for the parameters of the cluster centres can be calculated. what is hard to do are these two steps simultaneously. in an em algorithm we avoid that by iterating the two steps separately until the parameters converge at a local maximum of the likelihood  <cit> . the algorithm used here differed in two ways from that proposed previously  <cit> . firstly, we did not align flowgrams to our denoised sequences before calculating the distances. that was necessary to allow for pcr errors that cause changes resulting in insertions and deletions at the flowgram level. flowgram gaps do occur very rarely as a result of pyrosequencing noise, occasionally no signal ≥  <dig>  is observed in a frame of four nucleotides, but as described above we truncated our flowgrams when this was observed. consequently by not performing alignments we ensured that only pyrosequencing noise would be filtered at this step. secondly, we did not construct the maximum likelihood sequences each flow at a time, instead we only allowed sequences that were observed in the data. this allows the final denoised reads to be mapped to the originals.

we will now explain the em algorithm in detail. we represent the mapping of data points to clusters, or in our case flowgrams to sequences, by a matrix z where each row i corresponds to a flowgram and contains only zeros and a single one at the column j indexing the sequence that generated it. this can be expressed using kronecker deltas zi, j = δi, m where m gives the sequence that generated flowgram i. we now define a complete data likelihood that includes both the observed data and this matrix of unobserved mappings:

  ℒc=∏i=1n∏j=1lzi,j, 

assuming that each row of z, the vector z¯i, is i.i.d according to a multinomial over l categories with probabilities τ <dig> ...,τl. we then define the quantity z^i,j as the conditional expectation of zi, j given the model parameters, i.e. the sequences and their abundances, under the complete data likelihood. these are the conditional probabilities that sequence j generated flowgram i. the em algorithm iterates between an e step, where the z^i,j are computed given the model parameters and an m step, where the model parameters are calculated so as to maximize equation  <dig> with the zi, j replaced by their estimates z^i,j. this process will, under quite general conditions, converge to a local maximum of equation  <dig>  <cit> . our em algorithm is:

 <dig>  m step: given the z^i,j set each sequence s¯j to the sequence corresponding to the perfect flowgram Ūj that maximises equation  <dig>  restricted to the set of p unique perfect flowgrams q¯k obtained by rounding the observed flowgrams to integers. this corresponds to finding the perfect flowgram with the smallest total distance to all the reads weighted by the conditional probabilities that each flowgram was generated by that denoised sequence:

  u¯j=q¯l where l=mink 

define new relative frequencies as τj=∑i=1nz^i,j /n. this generates sequences and their frequencies which maximize the complete data likelihood of equation  <dig> given d and z^.

 <dig>  calculate new distances d′.

 <dig>  e step: calculate new z^i,j as:

  z^i,j=τjexpσp)∑k=1lτkexpσp). 

 <dig>  repeat until convergence

expectation-maximization algorithms because they only find local optimum are sensitive to initial conditions. to initialise the em algorithm we performed a complete linkage hierarchical clustering based on flowgram distances and formed clusters at a given cut-off, cp. this also defines the number of denoised sequences l, although the number with non-zero weight τj usually decreases during the iteration. the pyrosequencing noise removal therefore has two parameters σp and cp, for all the results presented here these were set at the values 1/ <dig> and  <dig>  respectively.

removing pcr noise
the advantage of splitting the pyrosequencing and pcr noise removal steps is that it allows a more appropriate model to be used for the removal of the pcr single base errors. we used the same ideas as above to develop a procedure for removing pcr errors. we define a distance that reflects the probability that a given read r¯ could have been generated from a true sequence s¯, given pcr error. this probability is simply the sum of the necessary nucleotide transitions i.e. the probability that a nucleotide m is observed when the true nucleotide is n, p . the total probability of the read will be the product of these and we take the negative logarithm to generate a sequence error corrected distance between zero and infinity. we also normalise by the alignment length a:

  e=−log=−log/a=∑l=1a−log/a 

this requires alignment of the read to the sequence. alignment was performed with a specially modified version of the needleman-wunsch algorithm with a reduced gap cost for homopolymer insertion and deletions. this accounted for the possibility of pyrosequencing noise on low frequency reads which may not have been removed in the flowgram clustering. gap penalties were included in the distance measure.

the nucleotide transition probabilities were calculated by comparing all reads with pyrosequencing noise removed from the three 'even' v <dig> data sets with the known control sequences. these are shown in table  <dig>  it is interesting to note the higher frequencies of transitions  versus transversions. this was also found for the v <dig> and titanium data. indeed the relative magnitudes of these probabilities were similar for all the data sets, perhaps because standard taq polymerases were used throughout, and the per cycle error rates were the same order of magnitude as has been observed by other methods for taq polymerases  <cit> . we therefore used the transition probabilities in table  <dig> for calculating sequence distances in all the data sets.

the rows give the true nucleotides and the columns the observed as a result of errors. estimates of the individual probabilities are given as frequencies. the total error probability f for each nucleotide is also quoted in the penultimate column, followed by the per cycle error rate p calculated as p = 2f /n, where n =  <dig> is the cycle number  <cit> .

we used a mixture model to cluster the sequences, just as for pyrosequencing noise removal, where each component of the mixture corresponds to a true sequence about which observed noisy reads are distributed. the relative weights of each component are the true relative frequencies of the sequences. the reads are assumed to be distributed as exponentially decaying functions of their sequence error corrected distance from these true sequences. the magnitude of the sequence noise is described by the characteristic length of these exponentials, σs . a maximum likelihood fit of the mixture model can be obtained using an expectation-maximization algorithm initialised using the clusters formed from a hierarchical clustering of sequences at a given distance cut-off, cs . in this study we used parameter values of σs =  <dig>  and cs =  <dig> , parameters that experience has taught us work well for gs flx data. for the titanium data we compared two different values for σs,  <dig>  and  <dig> , whilst keeping cs =  <dig> . a standard gap was given a penalty of  <dig>  and a homopolymer gap,  <dig> . prior to our sequence clustering step we truncated at  <dig> bp for gs flx and  <dig> bp for titanium because of the increase in error rates at the ends of reads.

chimera identification
chimeras are generated when incomplete extension occurs in one round of pcr and then the resulting sequence fragment acts as a primer for a different sequence in the next round. consequently chimeras are composed of two  true sequences with a discrete break point where the transition from one sequence to another occurs. for our nine test data sets we were therefore able to determine which sequences after denoising were likely chimeras by aligning each sequence against the known reference sequences and finding the putative parents and break point which gave the closest match to the query sequence. if the closest match to a chimera of two sequences was at least three nucleotides or better than that to a single reference sequence then the query was considered as a possible two sequence chimera or 'bimera'. if it was not then it was considered a 'good' sequence. similarly if the match was further improved by three nucleotides when two break points were allowed then it was classified as possibly comprised of three sequences a 'trimera', and again for the transition to a composite of four sequences or 'quadramera'. however, the sequence was only classified to these putative definitions if the absolute match was sufficiently good as measured in terms of the sequence error corrected distance . otherwise the sequence was denoted as 'unclassified'. these could include contaminants, real unidentified 16s rrna operons, gross pyrosequencing or pcr errors, or most likely a chimera that failed to fall under our rather strict definition.

perseus: chimera removal as a problem in 'supervised learning'
for real pyrosequencing data we will not know a priori what sequences should be present and therefore chimera identification algorithms are necessary. given the mechanics of pcr amplification, any chimeras generated will experience at least one less pcr cycle than either parent, consequently both parents of a chimera will be present in the data set and with a frequency at least equal to the chimera. this ignores the possibility of the chimera experiencing preferential pcr bias over its parents, but it will be true in the vast majority of cases. to exploit this observation we developed an algorithm 'perseus' that considers each sequence in turn and performs exact pairwise alignments to all sequences with equal or greater abundance, the set of possible parents. the two parents and break point that give the closest match to the query are then identified and a three way alignment of these sequences is generated using the mafft-linsi program  <cit> . we calculate two quantities from this alignment - the first is the pcr error corrected distance from the query to the optimum chimera. for a sequence to be classified as chimeric this distance has to be absolutely smaller than  <dig>  and smaller than or equal to the distance to the closest sequence amongst the best possible parents. this simply ensures that the hypothesis that the sequence is a chimera is possible. however, we still have to account for the possibility that the chimeric pattern could have evolved. we do this by calculating a second quantity using the alignment the 'chimera index' i .

denoting the query sequence c¯, the closest matching parent a¯, and the more distant parent b¯, we calculate using parsimony the sequence ancestral to all three. we find the number of base pair changes along the three branches to a¯, b¯, c¯ and denote these x, y, and z respectively. we resolve changes to the two parts of alignment, either the part of the chimera matching parent a¯ or parent b¯, and denote these xa and xb , ya and yb , and za and zb . for a given chimera to be observed, two independent events must occur, changes to the distant parent b¯ must occur on that part of the alignment matching a¯. assuming all base changes are equally likely then the distribution of changes across the two parts will be binomially distributed with probability proportional to the size of each part. therefore, we can calculate the probability of the changes being as biased or more so than were observed. the same arguments apply for the changes to the closer parent, they should all lie in the part matching the more distant and we can calculate that probability. we then multiply the two probabilities together and take the negative log to obtain an index that will increase the less likely a chimeric pattern is to have evolved. this index is defined for two parent chimeras, our so called 'bimeras', it could be extended to higher order chimeras but we did not do this, finding that it sufficed for identifying these anyway.

having defined this index the problem of identifying chimeras becomes an example of supervised learning with our labeled test data sets as training and validation data. we used the test data sets with equal abundances, the v <dig> 'divergent' data set, and the three v <dig> 'even' mock communities for training. we calculated i for each sequence that satisfied the chimera matching criteria and then used the known classifications to either good or chimeric , determined through comparison with the reference sequences to train a one dimensional logistic regression on i separately for the v <dig> and v <dig> sequences  <cit> . this procedure allows us to account for our assumption that all bases are equally likely to evolve. a logistic regression assumes that the probability of a sequence being chimeric can be expressed as:

  p=11+exp. 

we also added to our training data set the result of taking the reference and calculating their indices without regard to sequence frequency: i.e., comparing all sequences to all others. we then used the test data sets with uneven abundances, the v <dig> 'artificial community' and the three v <dig> 'uneven' data sets, for validation. running them through our algorithm and then using the logistic regressions to generate probabilities of each sequence being chimeric. those sequences that do not have a good chimeric match have this probability set to zero. we then defined all sequences with a probability of greater than 50% of being chimeric as chimeras. this will minimise total misclassifications  <cit> . we also trained the classifier with the titanium v4-v <dig> data and associated reference sequences but in this case we lacked a separate data set for validation.

RESULTS
in addition to running the ampliconnoise pipeline we also denoised the data sets with the denoiser algorithm  <cit>  and using single-linkage preclustering  at the recommended 2% sequence difference as well as at 1% for comparative purposes  <cit> . we truncated the reads at  <dig> bp and  <dig> bp for gs flx and titanium respectively before calculating exact pairwise sequence distances for the slp algorithm. for slp we used the same filtered reads as for ampliconnoise but this was not possible for the denoiser since there filtering is through the qiime pipeline  <cit> . the qiime filtering is slightly less stringent than the procedure described above for gs flx data but more so for titanium where a quality window is recommended. the read numbers following qiime filtering are also given in table  <dig> 

per base error rates following noise removal
the first comparison we will make between the different noise removal algorithms is to calculate average per base error rates. to determine if the algorithms really do reduce noise we will also compare to the raw reads. to calculate error rates in the raw reads we simply compared each read to its closest match amongst the reference sequences, aligned, calculated the number of differences, and the alignment length. the per base error rate was then estimated as the sum of the differences divided by the sum of the lengths for the whole data set. the results are given in the first column of table  <dig>  the raw error rates vary slightly across data sets, the v <dig> - gs flx and v4-v <dig> error rates are similar at around  <dig> % slightly higher than the  <dig> % reported previously  <cit>  but this is substantially increased to more than twice that value in the v <dig> data sets.

noise removal by all the algorithms can be considered a form of mapping. we map a noisy read onto another that we believe really generated that read. to calculate per base error rates after noise removal we must account for the possibility that the mapping may be to the wrong read. to allow for this we estimated the denoised error rates by, for each read, calculating the number of differences between the denoised read it maps to and the closest matching reference of the original undenoised read. the total of the differences across the data set was then normalised by total alignment length to estimate the per base error rate. the results for the four algorithms are given in table  <dig>  for the denoiser results we only used those reads that were included in the ampliconnoise and slp data sets ensuring a fair comparison despite the slight differences in filtering. what is immediately apparent is that slp at both cut-offs does not actually remove errors instead it inflates them. this is due to the high rate of misassignment where a read is mapped not onto the reference that generated it but to a similar but incorrect sequence. the denoiser algorithm does better reducing per base error rates in most cases but it is substantially out-performed by ampliconnoise which is capable of reducing noise by one-third to a half in all data sets. given that some residual error will always remain because the sequencing of the references may not be entirely accurate and because of pcr chimeras and contaminants then this is impressive.

relative importance of pyrosequencing and pcr noise
the effect of these errors will be to inflate estimates of otu number. in order to quantify the relative importance of pyrosequencing and pcr noise to the excess of otus observed at different levels of sequence difference we calculated otu number following complete linkage clustering as a function of percent sequence difference for the 'artificial community' data set. we used the exact pairwise needleman-wunsch algorithm to calculate distances between sequences prior to otu formation. this removed the potential complicating factor of incorrect multiple alignments  <cit> . we used a complete linkage hierarchical clustering and took clusters at increments of  <dig> % nucleotide difference to form otus. complete linkage is more sensitive to noise than the alternative average linkage algorithm  <cit> , but it gives otus with a closer correspondence to taxonomic classifications  <cit> . the otu numbers are shown in figure  <dig>  this graph is logarithmically scaled. we generated otus for the filtered sequences , this gives an indication of the total effect of errors, the sequences after pyrosequencing noise removal by the first pyronoise stage of ampliconnoise , and following removal of pcr point mutations by the second seqnoise stage . the clustering of the reference sequences is shown by the black line, this indicates the true otu diversity in the sample. pyrosequencing errors account for roughly half of the extra diversity , the majority of the rest derive from pcr point mutations. however, even after applying both noise removal steps there is still an excess of otus, these we hypothesized were due to the formation of pcr chimeras.

chimera frequencies
for each of the data sets following noise removal by the different algorithms, the number of denoised sequences classified as either 'good', one of our three classes of chimeric sequences 'bimera', 'trimera', 'quadramera', or 'unclassified' are given in table  <dig>  just focusing on the ampliconnoise denoised samples then these results illustrate the potential importance of chimeric reads in pyrosequenced amplicons. what is most striking is the difference between the v <dig> and v <dig> sequences. in the former the total number of unique sequences classified as chimeric was about 25%, with slightly more chimeric sequences in the 'divergent' as opposed to the 'artificial community'. for the latter it was still true that the data sets with evenly mixed references had a higher proportion of chimeric sequences but the number of chimeric sequences was far higher. in fact only some 5-10% of sequences could be mapped back to the reference sequences. this provides ideal data sets for training and testing chimera identification algorithms. similar patterns were seen for the other algorithms although exact numbers and frequencies vary. the denoiser algorithm achieves a higher frequency of good sequences on the v <dig> mock communities but as we discuss below this is due to over-clustering.

the number of unique denoised sequences classified into the five categories, 'good', 'bimera', 'trimera', 'quadramera' and 'unclassified', as defined in the text are given together with percentages of the total in parentheses.

accuracy of otu construction following noise removal
following noise removal with ampliconnoise, we removed those sequences classified as chimeric from the 'artificial community' data and rebuilt otus. the results are shown as a magenta line in figure  <dig>  it can be seen that the otu diversity now closely tracks that of the reference sequences. this illustrates that assuming we can remove chimeras then we are getting the right number of otus following noise removal above a sequence difference of just over 1%. this is a significant improvement over the original pyronoise program as can be seen from figure  <dig> where we repeat the ampliconnoise results together with the otu numbers from our original one-stage clustering but with a linearly scaled y-axis. ampliconnoise is also considerably faster than the original pyronoise. in table  <dig> we give the time in seconds taken to run the 'artificial community' data using  <dig> cores of a linux cluster for the two algorithms. the times are resolved across the individual steps. this includes an initial calculation of pairwise sequence distances and an average linkage clustering which is used to split up the data set which is common to both. both wall times i.e. time from start to end of the program and the cpu times actually spent on calculations are given. the latter is summed over all cores and is therefore considerably larger. the total wall time for ampliconnoise is around seventy seven minutes compared to over ten times longer for the original pyronoise.

the results are for a xeon linux cluster running programs across a maximum of  <dig> cores. two times in seconds are quoted for each step, wall  is the true time elapsed between the start and end of a process and cpu  is the total time over all cores spent on actual calculations.

in figure  <dig> we also show the effect of applying single-linkage preclustering at 1% and the recommended 2% level, and the denoiser algorithm prior to otu construction. single-linkage preclustering at 1% is very poor greatly over-estimating otu numbers, 2% slp and the denoiser perform similarly above a 3% cut-off predicting slightly more otus than the original pyronoise program, both were worse than ampliconnoise. below this 2% slp predicts a constant otu number suggesting it is aggregating otus that should be separated. however, both slp and denoiser were orders of magnitude faster than ampliconnoise, single-linkage preclustering does not require a cluster and for this data set it ran in just a few minutes on a standard computer. the denoiser took less than an hour on a single core.

to investigate whether we are not only getting the right numbers of otus but also the right otus, we built otus using both the v <dig> reference sequences and the denoised sequences following removal of those classified as chimeras. having done this those otus that contain both reference and denoised sequences indicate true diversity that we have observed we classified these as 'good', those otus that only contain denoised sequences can be considered 'noise', and those otus only containing reference sequences indicate diversity that we have 'missed'. the numbers of each as a function of percent sequence difference for ampliconnoise applied to the 'artificial community' following noise removal are shown in figure 4a. this indicates that above about  <dig> % we are getting all the otus that should be there with only one or two noisy otus. there is a substantial improvement over the original one-step version of pyronoise in terms of reduction in otus attributable to noise  and a very slight improvement in terms of capturing the diversity that is there. it is also substantially better than the performance of 2% single-linkage preclustering . using slp noise reduction is not nearly as good, at 3% there are  <dig> otus attributable to noise as opposed to  <dig> for the original pyronoise and just one for ampliconnoise. more disturbingly, we now fail to capture all the otus that should be present, this is obviously to be expected at cut-offs below 2% but we still miss two otus at the 3% level. for this data set the denoiser does better than 2% slp getting all diversity above 2% cut-off with noise comparable to the original pyronoise.

in order to determine if these results generalise we built 3% otus for each set of denoised sequences together with the corresponding reference sequences for all the data sets. we then classified the 3% otus as above. the results are given in table  <dig>  from this table we see that ampliconnoise is the only algorithm that consistently obtains all the true diversity at 3%. only in the v <dig> data set 'uneven3' is a single otu missed. for the v <dig> and titanium data sets it simultaneously removes a large proportion of the noise. for the v <dig> data a significant number of noisy otus remain. however, this reflects the unusual frequency of chimeras in this data, a large proportion of those noisy otus likely being chimeras that differed by only one or two nucleotides from their closest parent and consequently not classified as chimeric under the rather strict definition given above. the two agglomerative algorithms perform substantially worse. across all the data sets slp at 1% is incapable of reducing noise to reasonable levels, whilst slp at 2% has missing 3% otus in all the data sets, and for the v <dig> data does extremely poorly in this respect, at the same time it fails to reduce the number of noisy otus below that of ampliconnoise for the higher quality v <dig> and titanium data sets. the denoiser performs adequately for the v <dig> data and for the v <dig> gs flx 'mock communities' the number of noisy otus is actually lower for the denoiser than ampliconnoise but just as for 2% slp this comes at a cost of a substantial number of missed otus, around 10% of the true diversity. a situation that gets dramatically worse for the titanium data set where over 75% of otus are missing. it is likely that this is due to high frequency chimeras being identified as cluster centers and then good sequences of lower frequency are being mapped onto them.

reads classified as chimeric by comparison with the references were removed prior to complete linkage otu construction at 3% sequence difference with the corresponding reference sequences. each otu was then classified as 'good' if it comprised reference sequences and denoised pyrosequenced reads, 'missed' otus are formed only from reference sequences, and those otus containing only pyrosequenced reads are designated 'noise'. the numbers of classified otus observed for each data set and noise removal algorithm are shown below.

we investigate the titanium data more fully in figure  <dig>  where as in figure  <dig>  we show the number of good, missing and noisy otus as a function of per cent sequence cut-off. this shows that ampliconnoise with σs =  <dig>   is capable of removing most noise and retaining all diversity above about 1%, with the smaller cluster size σs =  <dig>  it is possible to obtain all the otus that should be present down to just  <dig> % but at the cost of increased noise . single-linkage preclustering at 2% once again fails to get all the otus that should be there even at 3%  and the denoiser does very badly missing a large portion of the true diversity .

chimera classification accuracy
the results of the training on the v <dig> 'divergent' denoised sequences from ampliconnoise are shown in figure  <dig>  in this case the logistic regression on i can exactly separate the good and reference sequences from the chimeras. a consequence of this is that the algorithm fails to converge predicting a decision line, a p <dig> value corresponding to a 50% probability of a sequence being chimeric, but being unable to predict uncertainty about that line essentially because there is none. the results of applying this classification rule to the denoised v <dig> 'artificial community' are shown in figure  <dig>  two chimeras fall below the p <dig> line and hence with 50% probability cut-off would be falsely classified as good. there were no false positives. when we applied chimeraslayer to these sequences we missed 13/ <dig> chimeras at the suggested 90% bootstrap, at 50% which perhaps better reflects our classification rule it misses 7/ <dig> but at the cost of 11/ <dig> false positives. we also performed a further logistic regression on all the data, both the 'divergent' and 'artificial community' data sets and the reference sequences. this highly significant fit  gave an intercept of α = - <dig>  and coefficient of β =  <dig> . we would recommend these choices for the de novo classification of v <dig> sequences.

the results of the training on the three v <dig> 'even' sets of denoised sequences are shown in figure  <dig>  in this case with much more training data the logistic regression converges, generating both a p <dig> decision line and well defined uncertainties about that prediction given as the p <dig> and p <dig> lines, 25% and 75% probability of being chimeric respectively. details of the regression are given in the caption of the figure. the results of applying this logistic regression to the three 'uneven' validation data sets are given in table  <dig>  here for each category of sequence, determined by direct comparison with the references, we give the number classified good or chimeric by the logistic regression with the p <dig> classification rule. for all data sets a very high sensitivity is achieved with around 99% of chimeras, both bimeras and trimeras, being removed. this is at a cost in false positive rates that varies from 10% to 15% of good sequences. we also note from table  <dig> that two thirds of 'unclassified' sequences are flagged as chimeras. chimeraslayer can not achieve such high sensitivities, it misses some 15% of bimeras, and more of the trimeras, but the false positive rate is lower, at around 5% of good sequences . we also trained the classifier on the σs =  <dig>  titanium data in the same way obtaining α = - <dig>  and β =  <dig> . in this case we lacked a separate training and validation set but for the training data we identified 167/ <dig> =  <dig> % of the chimeras successfully at a cost of obtained 2/ <dig> =  <dig> % false positives.

each row gives a separate category of denoised sequence according to its true classification as 'good', 'bimera', 'trimera', 'quadramera' and 'unclassified'. the columns are then split across data sets and give the number flagged as good or chimeric by classification with a logistic regression given a 50% probability cut-off.

each row gives a separate category of denoised sequence according to its true classification as 'good', 'bimera', 'trimera', 'quadramera' and 'unclassified'. the columns are then split across data sets and give the number flagged as good or chimeric by chimeraslayer at 50% bootstrap. occasionally a sequence remained unclassified probably because there was no good nast alignment. consequently rows do not alway sum to 100%. the broad institute 'gold' 16s rrna sequences were used as references.

in practice, dedicated training data sets for each study may not be possible, although we would recommend it. if data is gs flx v <dig> or v4-v <dig> titanium then the two pairs of α and β values given above should work well for their corresponding data types. for v <dig> data, we would not recommend the values in the caption of figure  <dig> because there is an implicit assumption of very high prior chimera probability generated from training on this atypical data set, reflected in the high value of α = - <dig> . the β values on all data sets are in the range of β =  <dig>  and we have found that this value, paired with α = - <dig> , performs well across a wide range of data sets.

CONCLUSIONS
we have demonstrated that ampliconnoise followed by perseus has the sensitivity to remove the majority of errors from gs flx and titanium pyrosequenced amplicons and allow accurate estimates of otu number. ampliconnoise outperforms both agglomerative clusterers, slp and the denoiser, both in terms of per base error rates and otu construction but at a cost of increased computational complexity and no doubt in some cases, where some noise can be tolerated, these simpler heuristic approaches may be the best option. however, the results here suggest that both agglomerative approaches must be treated with caution, by not simply looking at otu numbers as in previous evaluations  <cit> , but rather their identity we have established that these are prone to over-clustering, removing a substantial fraction of the true diversity even at the 3% level.

consequently, we believe that the ampliconnoise-perseus pipeline which is freely available as open source software  <cit>  with all data on a dedicated website  <cit> , will find a wide range of applications in microbial diversity estimation  <cit> , and population genetics  <cit> . they could be critical to the success of large-scale publicly funded efforts to explore microbial diversity, such as the sequencing of human associated microbes being conducted in the human microbiome project  <cit> . to facilitate their use we include programs for integrating their output into the qiime 16s rrna analysis pipeline  <cit> .

we have shown the importance of considering both the effects of pcr and sequencing errors in studies of diversity based on 16s rrna amplicons. this suggests the use of high fidelity polymerases to reduce per base pcr error rates. however, one of our most striking observations was just how variable chimera frequencies were in the test data sets. this must be due to pcr conditions, principally, cycle number, extension time, primer and template concentrations and polymerase type  <cit> . therefore optimising the whole pcr process to minimise all types of errors is probably a better strategy than just focussing on the enzyme.

in addition, the principles outlined here: the rigorous validation of noise removal algorithms with test data; the use of em algorithms to generate effective consensus sequences and remove noise from pyrosequenced amplicons; using sequence abundances in the classification of pcr chimeras; and the treatment of the latter as a supervised learning problem; will provide the basis for further algorithm development in this field and contribute to the maturation of next generation sequencing as a quantitative technique for the analysis of pcr amplicon diversity. these principles will be equally applicable to other pyrosequencing platforms, for example the illumina hiseq  <dig> which is capable of generating orders of magnitude more reads per run  <cit> . they should even hold for the third generation of sequencing technologies that will target individual molecules  <cit> . as ever larger amounts of sequence data are generated the question of how to distinguish true diversity from noise will become ever more important.

the titanium full length clones sequences have been submitted to genbank with accession numbers hq462473- <dig>  the titanium pyrosequencing reads have been submitted to the short read archive with accession srp <dig> 

authors' contributions
cq designed the algorithms, wrote software, performed analyses and wrote the paper. al contributed to the analyses and writing of the paper. rjd performed experiments and helped write the paper. pt designed and performed the experiments and helped write the paper.

