BACKGROUND
in medical informatics, the patient’s clinical data records, such as heart rate, are collected over time and therefore represent a time series. if the data is collected from two groups of patients , the task of multivariate time series  classification is to learn temporal patterns to determine whether the patient belongs to the group of symptomatic patients.

time series have been extensively analyzed in various fields, such as statistics, signal processing, and control theory. the focus of the research in these fields is on gaining a better understanding of the data-generating mechanism, the prediction of future values, or the optimal control of a system. from a statistical viewpoint, time series analysis is comprised of methods for analyzing time series data in order to extract meaningful statistics from the data. as a part of time series analysis, time series forecasting is aimed to use a model, e.g. autoregressive moving average , to predict future values based on previously observed values  <cit> . the ultimate objective of the signal processing community is the characterization of the time series in such a manner as to allow for transformation of the time series, with a method like fast fourier transformation , to extract useful information from the time series  <cit> . researchers and practitioners in control theory strive to calculate solutions for proper corrective action from the controller  that result in system stability. a set of past inputs and outputs is observed, and new inputs are set in such a way as to try to achieve a desired output  <cit> .

although all of the aforementioned methods could be helpful in our study, and the experience of researchers and practitioners from other fields are extremely valuable, the focus of our research is to classify a new time series as early as possible by looking at and extracting patterns from past observations rather than predicting future values or analyzing a single time series’ pattern.

in the data mining community, the time series classification problem has been studied in some detail as well. the predictive patterns framework has been introduced to directly mine a compact set of highly predictive patterns  <cit> . instead of adopting a two-phase approach by generating all frequent patterns in the first phase and selecting the discriminative patterns in the second phase, this approach integrates pattern mining and feature pruning into the same phase to filter out non-informative and redundant patterns while they are being generated. a temporal rule-based classification method for temporal pattern representation was recently proposed to address the deficiencies of existing methods  <cit> .

a method that extracts all meta-features from a multivariate time series was proposed by kadous et al.  <cit> . the types of meta-features are defined by the user, but are extracted automatically and are used to construct propositional attributes  for another high-level classifier, like a decision tree, that learns a non-linear hypothesis to distinguish among classes.

in the context of classification of unknown time series , models utilize the whole time series with the unknown label to predict it based on the information learned from training data. in an early classification context, the objective is to provide patient-specific classification of unknown time series as early as possible. therefore, instead of utilizing the whole time series, our msd method looks into a portion  of the unknown time series and determines whether it is able to predict the label of the whole time series without looking at the rest of the time series. if msd is able to predict at the time point which is at the end of the current stream, the label is predicted. otherwise, msd requires more data for the unknown time series and looks at a larger segment, and does so until it is able to predict the label of the time series.

for early classification, a new method called early classification on time series  has been proposed  <cit> . the idea behind the method is to explore the stability of the nearest neighbor relationship in the full space and in the subspaces formed by prefixes of the training examples. the disadvantage of ects is that it only provides classification results, without extracting and summarizing patterns from training data; thus, users may not be able to gain deep insights from the classification results. this drawback of ects has been resolved by extracting local shapelets which distinctly manifest the target class locally, and are effective for early classification  <cit> . however, the method is applicable only to one-dimensional time series.

in this study, we generalize the definition of local shapelets to a multivariate context and accordingly propose a method for early classification of multivariate time series. the proposed method is called multivariate shapelets detection . a multivariate shapelet consists of multiple segments, where each segment is extracted from exactly one dimension. the test time series is then classified based on the multivariate shapelets that best match the test time series.

in particular, we propose the following extensions to the existing univariate shapelet method: 

• extending the concept of univariate shapelets to multivariate shapelets, which are multidimensional subsequences with a distance threshold along each dimension.

• proposing use of information gain-based distance threshold.

• proposing use of weighted information-gain based utility score of a shapelet. a theorem is provided to show that the weighted information gain incorporates the earliness and assigns high utility score to the shapelet that appears earlier given the same accuracy performance.

the mathematical definition of the problem is presented in the definitions section. the method for multivariate time series classification is described in the methods section. datasets are described in the dataset and data processing section. in the results and discussion section, the experimental results are presented. finally, future work and concluding remarks are discussed in the conclusion section.

definitions
a time series t = {t <dig>  t <dig>  …, tl} of length l, len = l, is defined as a sequence of real values sampled at l time stamps. each time series is associated with a class label c ∈ cwhere c is a finite set of class labels. a dataset d is a collection of m pairs { : i = 1…m} where ti is the time series number i and ci = class is its class. given a time series t = {t <dig>  t <dig>  …, tl}, a subsequence s = {ti, ti +  <dig>  …, ti + l− 1}, s⊂t, is a sampling of contiguous positions of t of length l <l. given two subsequences s and h where len = len = l, the euclidean distance between s and h is defined as: 

  dist=∑k=1l <dig> 

for a given time series t of length l and a subsequence s of length l, the distance between s and t is defined as the minimum distance between s and all subsequences of t of length l. therefore, we slide a window of length l over the time series t to extract all subsequences {h <dig>  h <dig>  … hl − l + 1} of length l. as shown in figure  <dig>  the distance between s and t is computed as: 

  dist=min∀i∈{ <dig> ,…,l−l+1}dist 

a shapelet is defined as f =  where s is a time series subsequence of length l. the class label cf of the shapelet is called the target class. the other classes are called the non-target classes, and are referred to as c¯f. we call a time series ti a target time series if the class of the time series is cf. the distance threshold δis computed as follows: 

• the distance dibetween s and every time series ti  in the dataset is computed using equation  <dig>  the distance di  is represented as a point in the order line as shown in figure  <dig>  if class = cf, then di is represented as blue point. if class ≠ cf, then di is represented as red square.

• the distance threshold δis computed  to separate the two groups .

in another way, the distance threshold δ is computed such that the distance between any target time series ti and s is less than the threshold δ: 

  ∀∈d⇒dist≤δ 

 the distance between a shapelet f  and time series t is defined as dist : = dist.

an n-dimensional  time series of length l is defined as t =  where tj is the jth dimension of t and tj  is the value of the jth dimension of t at time stamp k. hereafter, we use the terms ‘multidimensional’ and ‘multivariate’ interchangeably.

an n-dimensional shapelet  of length l is defined as f = . the vector s =  where sj is the jth dimension of the shapelet. figure  <dig> shows an example of a 3-dimensional time series of length  <dig>  it shows an example of an extracted 3-dimensional shapelet of length  <dig>  the shapelet is extracted from the time series from position  <dig> to position  <dig> 

the distance between an n-shapelet f and n-dimensional time series t is a vector of n euclidean distances and is defined as: 

  dist=dists <dig> t <dig> dists <dig> t <dig> …,distsn,tn 

where dist is defined as in equation  <dig>  simply, the distance between two multivariate time series is a vector of distances where each component in the distance vector is the distance between the corresponding dimensions of the two multivariate time series. the distance between a shapelet f and a time series tis defined as dist := dist.

the distance threshold Δ =  where δj is computed  so that: 

  ∀∈d⇒dist≤δj∀j=1…n 

methods
in this section we first describe a recently proposed method for early classification of univariate time series  <cit>  together with our suggested modifications. then, we propose a new method for early classification of multivariate time series.

modifications of univariate shapelet for early time series classification
an early distinctive shapelet classification  method, which is proposed at  <cit>  and described in algorithm  <dig>  is aimed to extract a small set of shapelets from univariate time series for early classification.

algorithm 1: univariateshapeletsdetection
input: a training dataset d of m univariate time series; minl; maxl

output: a list of univariate shapelets 

 <dig>  foreach time series t ∈ d do {t is of length l}

 <dig>  forl ← minltomaxldo {for each shapelet length}

 <dig>  fork ←  <dig> tol – l +  <dig> do {for each starting position}

 <dig>  shapeletdist

 <dig>  computethreshold 

 <dig>  computeutilityscore 

 <dig>  add

 <dig>  pruneshapelets

 <dig>  returnshapeletlist

the method iterates over the time series in the dataset d . for each time series t, all shapelets of length l between minl and maxl  are extracted from t. for each shapelet flk  the method calls the function shapeletdist  that computes the distances between flk and all time series in d using equation  <dig>  then, the method computes the distance threshold  for the candidate shapelet flk using chebyshev’s inequality. then, it assigns flk a utility score  using a weighted f <dig> score measure. in line  <dig>  the method ranks all extracted shapelets using their utility scores and selects a subset of the highest ranked shapelets as the pruned set of shapelets which can exhaustively classify time series.

the functions that compute the distance threshold and utility score are explained in the following sections. we describe how to prune the shapelets and use them for early classification in the shapelet pruning and classification sections, respectively.

distance threshold method
the chebyshev’s inequality method is proposed for computing the distance threshold  <cit> . it guarantees that for any distribution, no more than 1/b <dig> of the distribution’s values are more than b standard deviations away from its mean  <cit> . the chebyshev’s inequality is applied to the non-target time series distances to compute the range where the non-target distance has a low probability of appearing. the method refers to a one-sided test, and is not able to find the distance threshold that can discriminate among the classes well. here we proposed information gain  <cit>  to find a discriminant distance threshold. in additional file 1: table s. <dig> of the supplementary document, we showed that using information gain as a method to compute the distance threshold outperformed the chebyshev’s inequality method.

information gain-based distance threshold for univariate shapelets

the basic idea is to find the shapelet’s distance threshold that maximizes the information gain and divides the dataset into two groups, target and non-target time series  <cit> .

first, the entropy of the dataset is computed as 

  entropy=−∑c∈cmcmlogmcm 

where mc is the number of time series of class c and m is the number of all time series. to compute the distance threshold, the method sorts the distances between the shapelet and all time series. then, it finds the mid point between two consecutive distances as a candidate for the threshold. the dataset is then divided into two datasets dl and dr as illustrated in figure  <dig>  the dataset dl contains all time series such that the distance between the shapelet and time series is less than or equal to the candidate threshold. the dataset dr contains the rest of the time series. then the entropies el and er of the datasets dl and dr are computed, respectively. by comparing the entropy before and after the split, we obtain a measure of information gain which is computed as 

  ig=entropy−mlmel−mrmer 

where ml and mr are the number of time series in dl and dr. therefore, we choose the distance threshold that maximizes the information gain for the shapelet. the algorithm is described in details in additional file 1: algorithm s. <dig> 

utility score method
the set of shapelets extracted from the dataset might be exceedingly large. therefore, it is important to rank the shapelets in order to select a small subset of the shapelets for classification. for this reason, each shapelet has to be assigned a score that takes into consideration earliness as well as discrimination among classes.

the weighted f <dig> score method is proposed to rank shapelets  <cit> . in our study, we introduce the weighted information gain as a new utility score method. in the supplementary document  we showed that our proposed method outperformed the weighted f <dig> method.

weighted information gain

the utility score of a shapelet should incorporate the earliness and the distinctiveness properties. first, we define the earliness  <cit>  between a shapelet f =  and a time series t as 

  eml=min∀i∈{ <dig> ,…,l−l+1}dist≤δ 

eml measures how early the shapelet f  has classified the time series t. the weighted information gain of the shapelet is computed as follows: 

 <dig>  compute the distance between the shapelet f =  and every time series tiin the dataset.

 <dig>  split the dataset d into two datasets dl and dr such that dl contains all time series where dist ≤ δ and dr contains all time series where dist >δ.

 <dig>  for each time series t in the dataset dl, if class = cf, then t is weighted by eml. otherwise, the time series is weighted by  <dig> 

 <dig>  compute ml as the weighted count of the number of time series in the dataset dl and mr is the size of the dataset dr.

 <dig>  compute the weighted information gain using equation  <dig> 

the following theorem proves that the weighted information gain incorporates the earliness and assigns high utility score to the shapelet that has better earliness given the same accuracy performance.

 theorem: if f <dig> and f <dig> are two shapelets that have the same distance threshold , the same class, and different earliness , then f <dig> has better weighted information gain than f <dig>  proof: suppose that the number of target time series in dl is nt and the number of non-target time series in dl is nnt. without loss of generality, since f <dig> has better earliness than f <dig>  suppose that for every target time series t in dl, eml = p <dig> and eml = p <dig> such that p <dig> <p <dig>  the weighted count ml <dig> and ml <dig> of the time series in dl for f <dig> and f <dig> is p1nt + nnt and p2nt + nnt, respectively. since p <dig> <p <dig>  then ml <dig> <ml <dig>  hence the weighted information gain of f <dig> is greater than the weighted information gain of f <dig> 

therefore, the weighted information gain gives high scores to the shapelets that come early in the time series.

shapelet pruning
to select a subset of the shapelets for classification, the shapelets are sorted in descending order using their utility scores. in this manuscript, two methods have been used to select a subset of the shapelets.

the first method iterates over the shapelets starting from the highest ranked shapelet. we select the shapelet and remove all training examples that are covered by that shapelet. the shapelet f  covers a training time series t if dist ≤ δ and class = cf. we use the next highest ranked shapelet to see if it covers any of the remaining training time series. if it covers some of them, then we select the shapelet and remove all time series that are covered. otherwise, we discard it and proceed to the next one. this process continues until all training time series are covered.

the second method simply involves keeping the top x shapelets from each class where x is a user-defined parameter. in our experiments, we used the top  <dig>   <dig>   <dig> and  <dig> shapelets from each class.

classification
if the length of the shortest shapelets extracted by algorithm  <dig> is l, then we can not classify any time series before observing l time points. hence, the classification method  initially reads l time stamps from the test time series. it then gets the highest-ranked shapelet. if the shapelet covers the current stream of the test time series then the time series is classified as the class of the shapelet and the prediction is done. otherwise, it gets the next shapelet from the ranked list and repeats the process. if none of the shapelets cover the current stream of the test time series the method reads one more time stamp and continues classifying the time series. therefore, the test time series could be classified after reading number of time points greater than the shapelet’s length. if the method reaches the end of the time series and none of the shapelets covers it, the method marks the time series as a not-classified example. in the results section, we report the relative accuracy as well as the percentage of the covered test time series.

multivariate shapelets detection for ecmts
in a dataset of n-dimensional time series, the method extracts all n-dimensional shapelets f = . the method assumes that all subsequences sj are extracted from the same starting position. hence, we slide a window of length l over the time series. at each time stamp p, a subsequence sj of length l starting from time point p is extracted from the jth dimension to construct s = . an example of a 3-dimensional shapelet is shown in figure  <dig> 

we follow the same procedures as in the univariate case. namely, for each n-shapelet f, we compute the minimum distance between f and every time series t in the dataset. the distance between f and t is a vector of distances  and is computed as in equation  <dig>  to compute the distance threshold of a shapelet, we need to provide a way to compare two multi-dimensional distances. therefore, two multidimensional distances d1= and d2= are defined to be ordered according to the following criterion: 

  d1<d2⇔d1j<d2j∀j=1…n 

equation  <dig> requires all n dimensions of d <dig> to be less than all corresponding n dimensions of d <dig>  therefore, we would require all n dimensions to be less than the shapelet’s threshold. this way, the method would try to find a pattern very similar to the shapelet at hand, which could lead to overfitting. in order to prevent overfitting, equation  <dig> is relaxed and redefined to be partially ordered according to the following criteria: 

  d1<percd2⇔d1qj<d2qj∀j=1…perc×n 

where perc ∈ ] <dig> ].

the algorithm for extracting the multivariate shapelets from a dataset is similar to algorithm  <dig>  the algorithm iterates over each time series and extracts all multivariate shapelets. for each candidate multivariate shapelet, it computes the distances with every time series. note that each distance is a vector of length n. hence, the distances between a multivariate shapelet and all time series is a matrix with dimensions n × m where m is the number of time series. then, the method computes the distance threshold and utility score for each candidate multivariate shapelet as explained in the following section. finally, it prunes the shapelets using the same procedure as mentioned in the univariate case.

distance threshold method
multivariate information gain-based distance threshold for multivariate shapelets

the multivariate information gain  is computed in a similar way to the one that computes the information gain in the univariate case. it takes as input an n-shapelet f; a matrix dist, that stores the multivariate distances between the shapelet and all m time series in the dataset; and perc, which determines the percentage of dimensions used to compute equation  <dig>  it sorts the matrix dist, and then the multivariate candidate threshold is computed as the mid-point between two successive distances . using the candidate threshold, the information gain is computed. finally, the algorithm returns the multivariate threshold Δ =  that has maximal information gain.

utility score method
the steps to adapt the utility scores defined on univariate time series are similar to the steps we have followed to adapt the distance threshold method.

after computing the score for each shapelet, the method sorts them in descending order according to their utility scores and then selects a subset of shapelets as explained in the shapelet pruning section. the classification process is similar to the process described in the classification section, taking equation  <dig> into consideration when computing the distance between the shapelet and the current stream of the query time series.

dataset and data processing
viral challenge datasets
we used two datasets for blood gene expression from human viral studies with influenza a  and live rhinovirus  to distinguish individuals with symptomatic acute respiratory infections from uninfected individuals  <cit> .

h3n <dig> dataset: a healthy volunteer intranasal challenge with h3n <dig> was performed in  <dig> subjects. of those subjects,  <dig> became symptomatic and  <dig> remained asymptomatic. blood samples were taken from each subject at  <dig> time points. some subjects have missed certain measurements at time points  <dig> , <dig> and/or  <dig>  hence, the gene expression values were measured on average 14- <dig> times for each subject.  <dig> genes were identified, in ranked order, as contributing to respiratory infection  <cit> . we used  <dig> unique genes from that list that were found in the available dataset.

hrv dataset: a healthy volunteer intranasal challenge with hrv was performed in  <dig> subjects. of those subjects,  <dig> became symptomatic and  <dig> remained asymptomatic. blood samples were taken from each subject at  <dig> time points. we ignored time stamps 8- <dig> because the majority of the subjects missed the measurements at those time points. thus, the gene expression values were measured on average 6- <dig> times for each subject.  <dig> genes were identified, in ranked order, as contributing to respiratory infection  <cit> . we used  <dig> unique genes from that list that were found in the available dataset.

drug response dataset
another clinical dataset was generated for studying the changes in cellular functions in multiple sclerosis  patients in response to drug therapy with ifn β <cit> . the dataset contains time series gene expression for  <dig> patients. the patients were classified as good responders  or bad responders  to the drug. the blood samples were taken every  <dig> months in the first year and every  <dig> months in the second year. some patients missed certain measurements, especially at the 7th time point. thus, the gene expression values were measured on average 5- <dig> times for each subject. the list of the genes used in our experiments is provided .

identification of triplets of genes for a bayes classifier of time series expression data of multiple sclerosis patients’ response to the drug has been performed  <cit> . previous research identified  <dig> genes in terms of triplets. hence, we generated four datasets: baranzini3a and baranzini3b, consisting of one triplet of the best two triplets of genes, respectively; baranzini <dig> has the top two triplets; and baranzinin <dig> has all  <dig> genes identified by all triplets.

a discriminative hidden markov model has been developed and applied to the ms dataset to reveal the genes that are associated with the good or bad responders to the therapy  <cit> . a total of  <dig> genes were found that are associated with the therapy. hence, we constructed a dataset, called lin <dig>  consisting of those  <dig> genes.

a mixture of hidden markov models has been developed to identify the genes that are associated with the patient response to the treatment  <cit> . a total of  <dig> relevant genes were found. therefore, we constructed a dataset called costa <dig> that contains data for these  <dig> genes.

environment setup and evaluation measure
in all experiments we set minl =  <dig> and maxl to be 60% of the time series’ length. since the number of subjects was small, bootstrapping was used for estimating the generalization error  <cit> . we sample with replacement a subset  from the original dataset. we train our model on the sample data and then test it on the subjects that are not used in the training data. this process is repeated  <dig> times and the final reported statistics  is the median of the statistics over all bootstrap samples. we report the median instead of the average since the distribution of the statistics is skewed and not symmetric.

in the results, we report the median of the accuracy, the coverage , and the earliness . note that the earliness varied from test example to another. in other words, each test example could be classified at different time point, so that our method is patient-specific and there is no fixed length of the time series used for classification.

because there is an imbalance in the drug response dataset, the accuracy  is calculated as the average between sensitivity and specificity: 

  sensitivity=tptp+fn,specificity=tntn+fp,acc=sensitivity+specificity <dig> 

where tp is the number of true positives, tn is the number of true negatives, fp is the number of false positives, and fn is the number of false negatives.

since the objective of the paper is to provide a method for early classification, we propose an evaluation measure that incorporates both the earliness  and the accuracy . we use fβ-measure as the weighted average between acc and ear. fβ-measure is defined as: 

  fβ=1+β2acc. β2+acc 

 where smaller values of β put more weight on the earliness and larger values of β put more weight on the accuracy. note that we use  because we want to penalize larger values of ear. in our experiments, we used the balanced f1-score, which gives both the accuracy and the earliness the same weight. f1-score reaches its best value at  <dig> and worst score at  <dig> 

RESULTS
evaluation of msd method
first, we show the effectiveness of the msd method on a single patient from the h3n <dig> dataset. in figure  <dig>  the top panel shows genes rsad <dig> and ifi44l observed at  <dig> time steps for an asymptomatic test subject from h3n <dig> data that is correctly and early classified by msd at the 5th time point. the msd method used a shapelet of length  <dig> to classify the test subject. in the bottom panel, msd used a shapelet of length  <dig> that was extracted from the time series of a symptomatic subject, so it correctly classified the symptomatic test subject at the 8th time point .

next, the msd method was evaluated on the viral and drug response datasets using all genes defined by the dataset. in table  <dig>  we report the median of the coverage, the relative accuracy, and the earliness. the list of the parameters that have been used for each method is provided in additional file 1: table s. <dig> 

the performance of the msd method on  <dig> datasets is shown in the table. the msd method achieved good accuracy on most of the datasets using a small fraction of the time series. the distribution of the statistics were skewed and not symmetric, so we report the median of the statistic.

from table  <dig>  it is clear that the msd method achieved high accuracy using a small fraction of the time series. for example, msd on the h3n <dig> dataset covered approximately 100% of the dataset, and out of the covered time series it achieved  <dig> % accuracy using 62% of the time series’ length. on another benchmark dataset called lin <dig>  the method developed in  <cit>  achieved 85% accuracy using the full time series  while our msd method achieved approximately 68% accuracy using less than half of the time series’ length on average .

for the viral infection dataset, a list of  <dig> genes associated with the viral infection sorted by their relevance to the infection diagnosis is provided in a recently published study  <cit> . starting from this list, we searched for a subset of genes that could be used to achieve more accurate results. we ran msd using different numbers of top genes provided by the ranked list. the coverage, the relative accuracy, and the accuracy of msd on h3n <dig> are shown in figure  <dig>  it is clear that the method becomes more accurate when using  <dig> genes instead of using  <dig> genes.

for the drug response dataset, no ranked list of genes is provided in previous publications. in  <dig> out of the  <dig> drug response datasets the number of the genes is small, therefore, on these datasets, we ran our msd method on all combinations of genes. the number of genes used for each dataset to achieve the highest accuracy is provided in table  <dig>  the accuracy of the msd method on those datasets is improved by using less number of genes. for example, the accuracy of msd on the lin <dig> dataset using only two genes is significantly improved .

the msd method has been evaluated on all combinations of the genes on  <dig> datasets. the accuracy of the classifier is improved than using all genes. for example, the performance of msd method on the lin <dig> dataset is improved significantly from 68% to 82% when using only  <dig> genes instead of  <dig> genes.

since our method achieved high accuracy using a small number of genes , we ran the univariate method  <cit>   on each gene in the dataset and report the best accuracy achieved. as shown in table  <dig>  our methods significantly outperformed the univariate method on all datasets except the h3n <dig> dataset, where they have similar accuracy but the univariate method is much earlier. the reason of achieving less accurate results using msd method as compared to the univariate method may be due to the non-robustness of the msd method to noisy variables so that msd does not extract meaningful features from the multivariate data in an automated fashion. therefore, equation  <dig> is affected by the noise in the variables which may lead to poor discrimination among the classes. in future work, we will investigate more resilient multivariate shapelet detection techniques that effectively utilize a subset of the variables providing maximum discrimination power as compared to using all the variables.

the univariate method  has been evaluated on each gene on all datasets. the best accuracy is reported.

baseline classifier for early classification
we compared the msd method with a random classifier to evaluate msd by comparison. the results of the random classifier are shown in table  <dig>  it is clear that the msd method is much accurate than the random classifier.

in addition, we compared msd to the baseline classical classifier, which uses shorter time series. recent research strongly suggested that the 1-nearest neighbor  method with dynamic time warping  is exceptionally difficult to beat  <cit> . therefore, we compared msd to the 1nn classifier using dtw. we compared  1nn using euclidean distance to 1nn using dtw and we found that 1nn with dtw is more accurate than 1nn with euclidean distance.

we constructed  <dig> datasets out of h3n <dig>  which we call 1nn and 1nn. we also constructed  <dig> datasets out of the hrv dataset, which we call 1nn and 1nn. the 1nn dataset was constructed from the prefixes of the original dataset such that all its time series are of fraction k of the original time series. for each dataset, 1nn was applied using all genes. the results are shown in figure  <dig> 

on the hrv dataset , the accuracy of 1nn using 50% of the time series’ length  is worse than our early classification method msd , and msd used a smaller fraction of time series on average. for instance, 1nn achieved 55% accuracy on 1nn dataset  while msd was more accurate using on average 40% of time series’ length . the results were consistent with the h3n <dig> dataset.

therefore, for the early classification task, using conventional classification methods on shorter time series is not as accurate as using methods specialized for early classification, such as our proposed method.

run-time analysis
in table  <dig>  we show the run time of the msd method on viral infection and drug response datasets. all experiments were conducted on a pc intel core i <dig>  <dig>  ghz with 8gb ram. it is evident that the run time grows exponentially with the number of examples and the time series length.

the run time of the msd method is reported for all datasets. the number of genes, number of examples, the time series length, and the run time in seconds are reported in the table.

CONCLUSIONS
for the early classification task, we proposed a method called multivariate shapelets detection . it extracts patterns from all dimensions of the time series. in addition, we proposed using of information gain-based distance threshold and weighted information-gain based utility score of a shapelet. the weighted information gain incorporates the earliness and assigns high utility score to the shapelet that appears earlier. in order to adhere to the limitations of clinical settings , datasets comprised of fairly short time series were used in reported experiments. however, our method is applicable to any domain. we showed that msd can classify the time series early by using as little as 40%-64% of the time series’ length. we compared msd to a baseline classifier and showed that using the method proposed for early classification is more accurate than using conventional methods.

the run time of the msd method grows exponentially with the number of examples and the length of the time series which limits the applicability of the proposed approach to datasets with smaller number of data instances and/or temporal observations. in practice, this is not a limitation for early classification in many health informatics applications  since decisions typically have to be made very early by learning from a small number of patients. however, in future work, we will speed up the run time of the method by incorporating parallelism in the algorithm.

we are working to improve msd by allowing the components of the multivariate time series shapelet to have different starting positions. since the number of candidate shapelets grows exponentially, the concept of closed shapelets, and maximal closed shapelets can be introduced to pruning redundant shapelets that are supersets of smaller shapelets. another extension to our work is to let the horizon between the time stamps in the subjects vary.

competing interests
both authors declare that they have no competing interests.

author’s contributions
mg designed the algorithms, implemented software, carried out the analysis, and drafted the manuscript. zo inspired the overall work, provided advice, and revised the final manuscript. both authors read and approved the final manuscript.

supplementary material
additional file 1
supplementary document. the supplementary document  contains additional analysis of the obtained results. these details are omitted for lack of space but are consistent with the findings reported here.

click here for file

 acknowledgements
we thank everyone in prof. obradovic’s laboratory for valuable discussions. special thanks to the reviewers for their valuable suggestions that helped improving presentation and characterizing the proposed method, and to dušan ramljak for reviewing the initial draft of the paper.

this work was funded, in part, by darpa grant  negotiated by ssc pacific grant; the us national foundation of science ; and the egyptian ministry of higher education.
