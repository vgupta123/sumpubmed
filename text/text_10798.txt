BACKGROUND
next generation sequencing technology has enabled characterization of metagenomics through massively parallel genomic dna sequencing. the complexity and diversity of environmental samples such as the human gut microflora, combined with the sustained exponential growth in sequencing capacity, has led to the challenge of identifying microbial organisms by dna sequence  <cit> . the library of sequenced dna fragments mapped to an identified taxonomy species has been growing in parallel; the latest release of ncbi genbank  has catalogued  <dig>  ×  <dig> basepairs of cdna and genomic dna from  <dig>  ×  <dig> records  <cit>  . the computational challenge has been to rapidly and accurately identify species level dna sequences from next generation metagenomic shotgun sequencing data.

currently the most widely used classification algorithm, blast  <cit> , relies on indexing unique fragments of dna that narrow the search space. while blast works well for small numbers of sequences, the algorithm scales poorly to the large number of reads generated by next generation sequencing files  <cit> . other sequence alignment software has been created specifically adapted to next generation sequencing output such as bowtie <dig>  <cit> , burrows-wheeler aligner  <cit> , and short oligonucleotide analysis package  <cit> . these alignment software work well for the precise alignment of a large number of next generation sequencing reads against single organism genomes but scale poorly when attempting to align reads against all known dna sequences. megan and metaphyler have been developed to work with blast specifically for the use of metagenomic sequencing classification  <cit> . however even though these probabilistic approaches have high accuracy  <cit> , they remain limited by the computational expensive nature of blast. in addition, empirical approaches have also used machine learning algorithms with both supervised  and unsupervised methods .

recently kraken was developed to specifically address the problem of classifying next generation sequencing output from metagenomics projects  <cit> . briefly, kraken works by creating a k-mer database mapped to the lowest common ancestor, reducing the search space significantly. by doing so, kraken performs exact k-mer matching and maps reads against its database with high speed and throughput. validation of kraken suggested processing of  <dig> million reads per minute at a rate over  <dig> times faster than megablast  <cit> . the limitations of kraken includes the long execution time and memory consumption during the database construction as well as the current databases being limited to bacterial, archaeal, and viral genomes, necessitating the elimination of host genomic dna prior to classification using kraken.

in addition to kraken, a number of other approaches have been published. in particular clark  <cit>  and lmat  <cit>  have been shown to have similar if not higher accuracy while maintaining the impressive throughput of kraken. lmat, similar to kraken, attempts to utilize taxonomy information to reduce the database of k-mers, but current implementations are limited to microbial genomes and do not include mammalian sequences. clark attempts to decrease the k-mer search space by only indexing keys that uniquely identify a given taxonomy level and offers several modes of execution, including a version called clark-l that is optimized for limited ram environments by subsampling the database to smaller fraction. all three techniques, kraken, lmat, and clark, attempt to limit the k-mer search space by either finding the least common ancestor  k-mers or finding discriminatory k-mers that uniquely identify an organism at a given taxonomy level.

recently, the mapreduce programming model  <cit>  has caused a substantial shift in the way that large data sets may be distributed in parallel within a computing cluster. for example, google used the mapreduce  <cit>  framework to regenerate their index of the internet, and the mapreduce framework has become popularized as a generic framework to solve big data bioinformatics problems in many-core cluster systems . database sharding has been used in other fields to horizontally scale very large sets of data and can reduce the each subset of the database into a datastructure in memory limited environments  <cit> . unlike prior algorithms which limited the k-mer search space, we sought to leverage parallel computing and a mapreduce computational framework with a sharded database to create a scalable complete search heuristic for next generation sequencing files from metagenomics projects.

implementation
computational infrastructure
the university of washington provides a shared high-performance computing cluster known as hyak. currently uw hyak has  <dig>  intel xeon processing cores with  <dig> computational nodes. each node used to test computational scaling contained  <dig> cpu cores with 64 gb of memory.

construction of database
the v <dig> release of ncbi genbank was downloaded  and each genbank accession was linked using the ncbi taxonomy database to a single species and class. using parallelization across  <dig> cores and a mapreduce framework, the genomic dna was then virtually cut at every  <dig> basepairs, and each 30-mer was linked to the corresponding species and class and sorted. finally merge sort was used to combine all the sorted 30-mers for classification. the dataset was then split into shards based on the first four basepairs of each 30mer creating a  <dig> separate databases that could be deterministically searched. the databases were saved in a hashtable format that could be loaded at runtime into memory by each search program.

description of search heuristic
a total of  <dig> search programs are started asynchronously in parallel with each program assigned a  <dig> basepair shard as part of the mapping step. each search program then iterates through the list of sequences in fasta or fastq format and slides a  <dig> basepair window if the first  <dig> basepairs match the assigned shard definition of the executing program. the remaining  <dig> basepairs are then used to execute an in-memory hash-table lookup . the reverse complement is also checked for every read. each successful match to a species, genus, or class is kept and recorded. in addition, a 1-edit distance permutation algorithm was created to generate every possible one base-pair substitution permutation of the 30-mer search to account for sequencing errors and single nucleotide polymorphisms, without accounting for insertions or deletions. the results of each program are sequentially reduced to create the final classification results. matching is performed at the species level and multiple matches against different organisms are collected. if any match is mammalian then the read is classified as mammalian; the highest voted match at the species, genus, and class taxonomy levels are calculated for each read for the final classification. if the highest classification for a read is a tie, then the read is labeled as ambiguous for a given taxonomy level.fig.  <dig> sketch of search strategy in pseudocode



datasets tested
simulated datasets  were taken from the publicly available datasets that were used to evaluate kraken  <cit> .

in a previous clinical trial of acute conjunctivitis/epidemic keratoconjunctivitis , a total  <dig> patients with clinical signs and symptoms of epidemic keratoconjunctivitis were recruited worldwide. institutional review board approval was obtained through goodwyn irb  clinical research adhered to the tenets of the declaration of helsinki and was conducted in accordance with health insurance portability and accountability act regulations. written informed consent was obtained before participation for all participants in the study. conjunctival samples from the upper/lower tarsal conjunctiva and fornix were collected using sterile dry swabs . genomic dna was isolated from conjunctival swabs using qiagen blood & tissue dna kit  as per protocol. three samples were randomly selected for whole genome sequencing . one nanogram of genomic dna from each sample was used to prepare libraries for wgs according to the manufacturer’s instruction using illumina nextera xt sample prep kit . the dna libraries were sequenced using miseq system following the manufacturer’s standard protocols . three conjunctival samples were used from this clinical trial collected from patients on the day of enrollment prior to the initiation of either placebo or the investigative drug. the fastq files for these samples have been uploaded to the ncbi sra archive . flash was used to preprocess the paired end libraries and sickle was used for quality trimming  <cit> .

in addition, data from the human microbiome project  <cit>  was downloaded as an additional metagenomic dataset. specifically, three gut microbiome datasets  were downloaded from the ncbi sequence read archive, and sickle again was applied prior to analysis of the samples.

evaluation of accuracy and speed
to allow for direct comparison of performance statistics, the same definition of sensitivity and precision were used as described by wood et al.  <cit> . briefly sensitivity was defined as the number of correct classifications of reads divided by the total number in each dataset. precision was defined as the number of correct classifications divided by the total number of reads attempted to be classified.

comparison of smart to kraken and clark
in order to compare the accuracy and performance of the three tools, dna sequence files from all the bacterial, viral, and archaeal sections of refseq were downloaded. for kraken, clark, and smart, the same sequences were used to build a database in each tool respectively following the documentation provided. the simulated datasets were then analyzed by each tool on the same computational node  in the uw hyak with multithreading enabled to the maximum number of cpus. for kraken, the database was preloaded into memory for maximal performance as suggested by the creators of kraken for users with nfs filesystems. for clark, the standard mode  was used to analyze the simulated files as the program failed to start with other modes due to the ram limitation. in order to calculate throughput, each program was run sequentially three times and the lowest execution time was utilized to calculate throughput.

software and statistics
custom software was written in c++ and ruby. statistics were performed using r . conjunctival classification results from kraken were obtained using illumina basespace and ncbi blast was run with the database downloaded on november  <dig>  software depends on google sparsehash  and gnu parallel . software used to run smart, prebuilt libraries, and training of custom libraries is available at a public repository .

RESULTS
after transferring all genomic dna reads from the latest release of the ncbi genbank , a total of over  <dig> × 1011 bp of  <dig>  ×  <dig> sequences from  <dig>  ×  <dig> species of  <dig>  ×  <dig> classes were indexed. the number of sequences indexed and the total number of uniquely identifying sequences from the  <dig> most abundantly represented classes and species are shown in tables  <dig> and  <dig> respectively. over  <dig>  ×  <dig> sequences  and  <dig>  ×  <dig> sequences  were uniquely identifying of a single species and class respectively. with a  <dig> basepair shards,  <dig> separate hashtables were created and indexed using a quadratic probing hashtable structure. the uncompressed sharded files used 137 gb of hard disk space to store, with each shard on average consuming  <dig> gb of space. total database construction was completed within  <dig>  h and each thread consumed less than 1gb of memory.table  <dig> twenty most abundantly represented classes by  <dig> basepair fragments in genbank


homo sapiens

mus musculus

rattus norvegicus

bos taurus

sus scrofa

zea mays

danio rerio

hordeum vulgare

ovis canadensis

cyprinus carpio

solanum lycopersicum

apteryx australis

strongylocentrotus purpuratus

spirometra erinaceieuropaei

pan troglodytes

oryza sativa

nicotiana tabacum

solanum pennellii

echinostoma caproni

triticum aestivum


using the same simulated datasets that were used to evaluate kraken  <cit> , we measured the sensitivity and precision at the species, genus, and class taxonomy levels . on a single node with  <dig> search programs executing in parallel, each of the simulated datasets took a total of 30 min to finish. the maximum memory consumed by a single search program was  <dig>  gb with an average of  <dig>  gb used by each program. a 100 % utilization of each cpu core was noted during the execution of each search program. using multiple nodes to further parallelize the computation, we achieved linear scaling in throughput with inversely proportional decreases in total computational time . by increasing the number of nodes to  <dig>  we achieved a maximum throughput of over  <dig>  million reads per minute and the ability to classify each of the simulated datasets in under 5 min. performance of classifying a “real-world” human conjunctival derived metagenomic next generation sequencing result did not show any difference in computational scaling . when the cost of 1 bp permutations was measured, there was on average a  <dig>  times increase in execution time . however an average of  <dig>  ×  <dig> additional reads  was classified in the three simulated datasets.fig.  <dig> accuracy results of deep search on simulated datasets using the genbank library. a sensitivity with exact matching at species, genus, and class levels for simulated datasets . b sensitivity with  <dig> basepair permutations during search. c precision with exact matching. d precision with  <dig> basepair permutations during search. error bars represent 95 % confidence intervals

fig.  <dig> computational scalability of smart on a computing cluster using the genbank library. a, c, e, g overall execution time to complete processing of datasets with increasing number of computing nodes utilized. b, d, f, h throughput measured in reads per minute processed with increasing number of computing nodes utilized. i execution time of datasets with  <dig> nodes utilized and  <dig> basepair permutations during search. j throughput of datasets with  <dig> nodes utilized and  <dig> basepair permutations during search



because many metagenomic projects come from a single host organism, a major bioinformatic challenge is to effectively filter the host organism genomic dna from the dna of the microbial organisms. indexing the totality of known dna from the ncbi genbank and using the ncbi taxonomy classes allows for simultaneous classification of all reads to both mammalian genomes and non-mammalian genomes without a need for a pre-filtering stage. to prevent false positive match for microbial dna, a conservative approach was used in that if a read was classified even once as mammalian then it was considered to be mammalian in origin. of note, in genbank only  <dig>  % and  <dig>  % of all known 30-mers have perfect matches for bacterial and viral dna, respectively, at the class taxonomy level.

using this strategy, the whole genome sequencing results from three separate conjunctival samples and three gut microbiome samples from the human microbiome project  were analyzed with  <dig> basepair permutations . in the human gut samples, on average  <dig>  % of all reads were classified with  <dig>  % matching non-mammalian dna. on average, in the paucibacterial conjunctival samples  <dig>  % of all the reads were classified; of these,  <dig>  % matched non-mammalian dna. the total reads by classified genus were normalized by the depth of coverage of the human genome in each sample to account for sequencing depth variability. the top twenty organisms from each sample are shown in fig.  <dig> table  <dig> classification results of metagenomics samples using smart with the genbank library

n
n
n
n
n
fig.  <dig> twenty most common genera in metagenomics samples. a human conjunctival metagenomics whole genome sequencing samples and b human gut metagenomics whole genome sequencing samples with total reads normalized by coverage of human genome



to compare the three methods, one conjunctival sample was analyzed. human reads were filtered using illumina basespace, and was run through kraken and clark with libraries built using all the bacterial, viral, and archaeal sequences from refseq. kraken attempted to classify  <dig>  ×  <dig> non-human reads but 98 % were unable to be identified. comparison of the same read results with smart revealed that 83 % of unclassified reads by kraken were mammalian dna in origin. in addition,  <dig>  % of microbial classified reads by kraken also matched mammalian dna by smart. a comparison of the microbial matched reads by kraken against blast revealed a similar trend . in addition, a similar comparison was made with the results from clark; the majority of the reads classified by clark as microbial were identified by smart as having mammalian origin and this was confirmed independently using blast .table  <dig> comparison of kraken results to smart using the genbank library and blast for conjunctival sample 1

 
altermonas
 
propionibacterium
 
mycoplasma
 
pseudomonas
 
pandoravirus dulcis
 
pandoravirus salinus
 
staphylococcus
 
human endogenous

 
retrovirus k113
 
delftia
 
corynebacterium
 
alteromonas
 
mycoplasma
 
propionibacterium
 
pandoravirus dulcis
 
pandoravirus salinus
 
bracovirus
 
ichnovirus
 
yersinia
 
pseudomonas
 
hepacivirus


when comparing smart to kraken and clark directly, a separate database for smart was developed with all the bacterial, viral, and archaeal sequences from refseq. a total of  <dig>  sequences were indexed by each tool. during execution each tool utilized all  <dig> cpus for multithreading. sensitivity, precision, throughput, and memory utilization are shown in fig.  <dig>  smart utilized on average  <dig>  gb of ram per search program. disk space of databases for kraken, clark, and smart were 151 gb, 113 gb, and 29 gb respectively.fig.  <dig> comparison of accuracy, throughput, and memory utilization among kraken, clark, and smart built from the same refseq sequences. a, b, c sensitivity at the level of species, genus, and class for simulated datasets . d, e, f precision at the level of species, genus, and class. throughput  and memory utilization  of datasets with  <dig> parallel threads in the same computing environment



discussion
by indexing every 30-mer in the ncbi genbank with a multiplexed, parallel searching strategy, we were able to achieve an unsurpassed ability to classify reads against all currently catalogued dna simultaneously while maintaining similar throughput, sensitivity and precision to kraken and clark on simulated datasets. to the authors’ knowledge, this is the first metagenomic classification algorithm capable of efficiently matching against all the species and sequences present in the ncbi genbank, allowing for a single step classification of microorganisms as well as large plant, mammalian, or invertebrate genomes from which the metagenomic sample may have been derived and allows for identification of novel sequences without pre- or post- filtering steps.

kraken represented an improvement in throughput and accuracy in classification algorithms when released in  <dig>  <cit> . during the construction of the kraken-gb database, wood et al. noted that there were several draft genomes that had included mislabeled dna or included adapter sequences and cautioned against the interpretation of kraken’s matches  <cit> . our approach of searching the entire genbank genomic dna catalogue would protect against these false-positive matches as erroneous sequences would be present in multiple organisms and these results would label the read as ambiguous. however, this highlights the limitation and potential biases introduced by selective over-representation of certain species in the ncbi genbank. for example, many of the animal models used in the biomedical science are overrepresented in the genomic dna catalogued, as scientists are most interested in these organisms . hence false-positive matching may occur against these organisms if the true organism has not been sequenced yet. statistical modeling could be used to generate matching likelihoods to each organism based on relative database representation.

with integration into illumina basespace, kraken has rapidly become the bioinformatics pipeline used to analyze metagenomics next generation sequencing results. however, smart has a number of advantages over kraken. smart employs a scalable infrastructure that is not dependent on a common database and can distribute the workload across many computational nodes. in addition, many metagenomics samples come from host-rich environments and kraken suffers from false positive identification of microbial organisms. in our study, when comparing the human gut microbiome samples, kraken could not classify  <dig>  % of the reads compared to  <dig>  % with our search strategy. with the conjunctival samples, kraken identified numerous reads matching mycoplasma, pandoravirus dulcis, pandoravirus salinus, and human endogenous retrovirus k <dig>  by smart and blast, all of these reads were of mammalian origin .

in a direct comparison among kraken, clark, and smart using the same training refseq sequences and the same computing environment, clark and smart were noted to have higher sensitivity and precision compared to kraken at the species classification level. without  <dig> basepair permutations, smart was noted to have similar throughput to clark and with  <dig> basepair permutations, smart was noted to have similar throughput to kraken. smart was noted to use significantly lower ram compared to kraken and clark. the main advantage of smart appears to be utilizing a many-shard database approach to achieve horizontal scaling of a very large training set. while kraken and clark have similar throughput and accuracy, they are unable to index a large training set that includes many mammalian, plant, fungal, and other protozoan organisms, both in the database construction phase and in analysis due to limitations in ram. since the sharded database can be loaded asynchronously in pieces, smart can work in limited ram environments without any changes to the algorithm by lowering the number of threads.

the exact k-mer matching approach has been used in several prior classification algorithms. smart is similar to kraken, clark, and lmat in using exact k-mer matching for classification. however, smart utilizes substantially lower ram usage in the database construction phase by avoiding linking k-mers to a taxonomy tree and determining lca. unlike clark, smart keeps all k-mers in the database and does not limit the search space by only keeping discriminatory k-mers. by using a deterministic sharding scheme, smart is able to handle the expanded search space by asynchronously loading shards of database and allows for scalability. while the matching approach is similar to prior algorithms, smart scales efficiently in a many-cpu, many-node environment and allows for accessing the entire ncbi genbank in a single classification step.

despite filtering human sequences in the conjunctival sample using basespace prior to classification, kraken  and clark  had many reads classified as bacterial or viral which were classified as mammalian by smart. blast verified that the majority of these sequences were indeed mammalian. if an improved filtering step were implemented, or if kraken or clark included mammalian genomes in their databases, their performance in host-rich metagenomics samples would have likely been improved. unfortunately due to memory constraints on the database construction steps of both kraken and clark, it was not possible for us to construct a database to include mammalian genomes in the evaluation databases for kraken and clark. inclusion of human and mammalian sequence filtering as an intrinsic component of the smart protocol resulted in higher specificity of sequences assigned to non-host sources.

as the number of species sequenced grows, the ncbi genbank will continue to expand, and the database shards used in this approach will also grow and consume more memory. at a certain point in the future each shard may consume too much memory and the database may need to be split with larger barcodes. however, computational infrastructure have also been growing in accordance to moore’s law  <cit>  and with cheaper costs in computer memory, this tipping point may be further away.

while we only benchmarked this approach in a cluster-computing environment, this deep search technique could be easily translated to a cloud computing infrastructure  <cit> . these on-demand high-memory instances could be elastically created in parallel to handle each workload and destroyed after their use, allowing another layer of parallelization to occur. one limitation of the uw hyak computing cluster that we faced was the relatively slow input and output  performance of the network filesystem. in contrast, many of the cloud computing infrastructures are optimized for io performance and this approach may benefit from implementation and tuning in a cloud environment.

further improvements in this approach are possible to increase the throughput. in particular, the generation of  <dig> basepair permutations of the query may benefit from further optimization and from another mapreduce step. in addition, higher throughput would be achieved with the recruitment of more computational nodes in the cluster. this approach would also be applicable to rnaseq data in identifying gene transcripts and pathogen rna by using a similar approach to index all the cdna data in the ncbi genbank. in particular viral transcripts may be proportionally enriched both in the genbank catalogue as well as in the biological samples.

CONCLUSIONS
we present the first scalable complete search approach to effectively classify metagenomics sequencing data using both exact 30-mer matching and  <dig> basepair permutations using the entirety of the ncbi genbank. we anticipate this approach will be useful in identifying pathogens, characterizing complex microbiomes, and be extendable into labeling transcripts in rnaseq data.

this work was facilitated though the use of advanced computational, storage, and networking infrastructure provided by the hyak supercomputer system at the university of washington. conjunctival samples were provided by novabay pharmaceuticals, inc .

funding
supported in part by nih p <dig> ey <dig>  r <dig> ey <dig>  k <dig> ey <dig>  and an unrestricted grant from research to prevent blindness. none of the funding bodies had any part in the design of the study, data collection, analysis, and interpretation.

availability of data and materials
the described software is freely available for non-commercial use and posted on a public repository . the simulated datasets used in the manuscript are publicly available from the kraken website . the human gut microbiome  are publicly available from the ncbi sequence read archive . the conjunctival microbiome datasets used have been publicly deposited into the ncbi sra as srr <dig>  srr <dig>  and srr <dig> 

authors’ contributions
ayl designed the software. all authors was involved in writing the manuscript, data interpretation, study design, contributed to, read, and approved the final version of this manuscript.

competing interests
the authors declare that they have no competing interests.

consent for publication
not applicable.

ethics approval and consent to participate
the conjunctival samples were obtained through an irb approved protocol and with written consent. . the simulated datasets and human gut microbiome data were obtained as public data and required no consent nor irb approval to use.
