BACKGROUND
one-dimensional  structures of a protein are residue-wise quantities or symbols onto which some features of the native three-dimensional  structure are projected. 1d structures are of interest for several reasons. for example, predicted secondary structures, a kind of 1d structures, are often used to limit the conformational space to be searched in 3d structure prediction. furthermore, it has recently been shown that certain sets of the native  1d structures of a protein contain sufficient information to recover the native 3d structure  <cit> . these 1d structures are either the principal eigenvector of the contact map  <cit>  or a set of secondary structures , contact numbers  and residue-wise contact orders   <cit> . therefore, it is possible, at least in principle, to predict the native 3d structure by first predicting the 1d structures, and then by constructing the 3d structure from these 1d structures. 1d structures are not only useful for 3d structure predictions, but also helpful for intuitive understanding of the correspondence between the protein structure and its amino acid sequence due to the residue-wise characteristics of 1d structures. therefore, accurate prediction of 1d protein structures is of fundamental biological interest.

secondary structure prediction has a long history  <cit> . almost all the modern predictors are based on position-specific scoring matrices  and some kind of machine learning techniques such as neural networks or support vector machines. currently the best predictors achieve q <dig> of 77–79%  <cit> . the study of contact number prediction also started long time ago  <cit> , but further improvements were made only recently  <cit> . these recent methods are based on the ideas developed in ss predictions , and achieve a correlation coefficient of  <dig> – <dig> .

recently, we have developed a new method for accurately predicting ss, cn, and rwco based on a novel machine learning scheme, critical random networks   <cit> . in this paper, we briefly describe the formulation of the method, and recent improvements leading to even better predictions. the computer program for ss, cn, and rwco prediction named crnpred has been developed for the convenience of the general user, and a web interface and source code are made available online.

implementation
definition of 1d structures
secondary structures 
secondary structures were defined by the dssp program  <cit> . for three-state ss prediction, the simple encoding scheme  was employed  <cit> . that is, α helices , β strands , and other structures  defined by dssp were encoded as h, e, and c, respectively. note that we do not use the casp-style conversion scheme  in which dssp's h, g  and i  are encoded as h, and dssp's e and b  as e. we believe the ck mapping is more natural and useful for 3d structure predictions . for ss prediction, we introduce feature variables  to represent each type of secondary structures at the i-th residue position, so that h is represented as , e as , and c as .

contact numbers 
let ci,j represent the contact map of a protein. usually, the contact map is defined so that ci,j =  <dig> if the i-th and j-th residues are in contact by some definition, or ci,j =  <dig>  otherwise. as in our previous study, we slightly modify the definition using a sigmoid function. that is,

ci,j = 1/{ <dig> + exp }     

where ri,j is the distance between cβ  atoms of the i-th and j-th residues, d = 12Å is a cutoff distance, and w is a sharpness parameter of the sigmoid function which is set to  <dig>  <cit> . the rather generous cutoff length of 12Å was shown to optimize the prediction accuracy  <cit> . the use of the sigmoid function enables us to use the contact numbers in molecular dynamics simulations  <cit> . using the above definition of the contact map, the contact number of the i-th residue of a protein is defined as

ni=∑j:|i−j|>2ci,j.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgubgbdawgaawcbagaemyaakgabeaakiabg2da9maaqafabagaem4qam0aasbaasqaaiabdmgapjabcycasiabdqgaqbqabaaabagaemoaaomaeiooaozaaqwaceaacqwgpbqacqghsislcqwgqbgaaiaawea7cagliwoacqgh+agpcqaiyagmaeqaniabgghildgccqgguaglcawljagaaczcamaabmgabagaegomaidacagloagaayzkaaaaaa@475e@

the feature variable yi for cn is defined as yi = ni/log l where l is the sequence length of a target protein. the normalization factor log l is introduced because we have observed that the contact number averaged over a protein chain is roughly proportional to log l, and thus division by this value removes the size-dependence of predicted contact numbers.

residue-wise contact orders 
rwco was first introduced in  <cit> . this quantity measures the extent to which a residue makes long-range contacts in a native protein structure. using the same notation as contact numbers, the rwco of the i-th residue in a protein structure is defined by

oi=∑j:|i−j|>2|i−j|ci,j.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgvbwbdawgaawcbagaemyaakgabeaakiabg2da9maaqafabawaaqwaceaacqwgpbqacqghsislcqwgqbgaaiaawea7cagliwoaasqaaiabdqgaqjabcqda6maaemgabagaemyaakmaeyoei0iaemoaaogacaglhwuaayjcsdgaeyopa4jaegomaidabeqdcqghris5aogaem4qam0aasbaasqaaiabdmgapjabcycasiabdqgaqbqabagccqgguaglcawljagaaczcamaabmgabagaeg4mamdacagloagaayzkaaaaaa@4e40@

the feature variable yi for rwco is defined as yi = oi/l where l is the sequence length. due to the similar reason as cn, the normalization factor l was introduced to remove the size-dependence of the predicted rwcos .

critical random networks
here we briefly describe the critical random network  method introduced in  <cit>  which should be referred to for the details. unlike most conventional methods for 1d structure prediction , the crn method takes the whole amino acid sequence into account. in the crn method, an n-dimensional state vector xi is assigned to the i-th residue of the target sequence . neighboring state vectors along the sequence are connected via a random n × n orthogonal matrix w. this matrix is also block-diagonal with the size of blocks ranging uniformly randomly between  <dig> and  <dig>  the input to the crn is the position-specific scoring matrix , u =  of the target sequence obtained by psi-blast  <cit>  . we impose that the state vectors satisfy the following equation of state:

xi = tanh      

for i =  <dig>  ..., l where v is an n ×  <dig> random matrix , and β and α are scalar parameters. the fixed boundary condition is imposed . by setting β =  <dig> , the system of state vectors is made to be near a critical point in a certain sense, and thus the range of site-site correlation is expected to be long when α is sufficiently small but finite  <cit> . the value of α was chosen so that the resulting solution xi oscillates continuously with respect to the residue number i; that is, each component of xi having values from - <dig> to  <dig>  rather than being a discrete sequence of - <dig> or  <dig>  it can be shown that there exists a unique solution of eq.  <dig> for a given pssm u . the solution {xi} of eq.  <dig>  can be interpreted as some kind of patterns that reflect the complicated interactions among neighboring residues along the amino acid sequence. in this way, each state vector implicitly incorporates long-range correlations, and its components serve as additional independent variables to the linear predictor described in the following. the 1d structure of the i-th residue is predicted as a linear projection of a local window of the pssm and the state vector obtained by solving eq. 4:

yi=∑m=−mm∑a=121dm,aua,i+m+∑k=1nekxk,i     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwg5bqedawgaawcbagaemyaakgabeaakiabg2da9maaqahabawaaabcaeaacqwgebardawgaawcbagaemyba0maeiilawiaemyyaegabeaakiabdwha1naabaaaleaacqwghbqycqggsaalcqwgpbqacqghrawkcqwgtbqbaeqaaaqaaiabdggahjabg2da9iabigdaxaqaaiabikdayiabigdaxaqdcqghris5aawcbagaemyba0maeyypa0jaeyoei0iaemyta0eabagaemyta0eaniabgghildgccqghrawkdaaewbqaaiabdweafnaabaaaleaacqwgrbwaaeqaaogaemieag3aasbaasqaaiabdugarjabcycasiabdmgapbqabaaabagaem4aasmaeyypa0jaegymaedabagaemota4eaniabgghildgccawljagaaczcamaabmgabagaegynaudacagloagaayzkaaaaaa@5f8a@

where yi is the predicted quantity, and dm,a and ek are the regression parameters. in the first summation, each pssm column is extended to include the "terminal" residue. since eq.  <dig> is a simple linear equation once the equation of state  has been solved, learning the parameters dm,a and ek reduces to an ordinary linear regression problem. for ss prediction, the triple  is calculated simultaneously, and the ss class is predicted as arg maxs∈{h,e,c}yis
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwg5bqedaqhaawcbagaemyaakgabagaem4camhaaaaa@311e@. for the cn and rwco prediction, real values are predicted. 2-state prediction is also made for cn using the average cn for each residue type as the threshold for "exposed" or "buried" as in  <cit> . we have noted earlier  <cit>  that the apparent accuracy of 2-state cn prediction depend on the threshold. although we proposed that using the median instead of average cn should be more appropriate for the threshold, here we use the average in order to compare our results with others. the half window size m is set to  <dig> for ss and cn predictions, and to  <dig> for rwco. note that the solution of the equation of state  is determined solely by the pssm. therefore, obtaining the solution to eq.  can be regarded as a kind of unsupervised learning, and the method for solving the equation of state is irrelevant for learning the parameters.

ensemble prediction
since the crn-based prediction is parametrized by the random matrices w and v, slightly different predictions are obtained for different pairs of w and v. we can improve the prediction by taking the average over an ensemble of such different predictions.  <dig> crn-based predictors were constructed using  <dig> sets of different random matrices w and v. cn and rwco are predicted as uniform averages of these  <dig> predictions.

for ss prediction, we employ further training. let sit,n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgzbwcdaqhaawcbagaemyaakgabagaemidaqnaeiilawiaemoba4gaaaaa@3359@ be the prediction results of the n-th predictor for 1d structure t  of the i-th residue. the second stage ss prediction is made by the following linear scheme:

yiss=∑n=120∑t∑m=−33wn,t,msi+mt,n     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwg5bqedaqhaawcbagaemyaakgabagaem4camnaem4camhaaogaeyypa0zaaabcaeaadaaeqbqaamaaqahabagaem4dac3aasbaasqaaiabd6gaujabcycasiabdsha0jabcycasiabd2gatbqabagccqwgzbwcdaqhaawcbagaemyaakmaey4kasiaemyba0gabagaemidaqnaeiilawiaemoba4gaaaqaaiabd2gatjabg2da9iabgkhitiabiodazaqaaiabiodazaqdcqghris5aawcbagaemidaqhabeqdcqghris5aawcbagaemoba4maeyypa0jaegymaedabagaegomaijaegimaadaniabgghildgccawljagaaczcamaabmgabagaegonaydacagloagaayzkaaaaaa@5a8e@

where ss = h, e, c, and wn,t,m is the weight obtained from a training set. finally, the feature variable for each ss class of the i-th residue is obtained by / <dig>  this last procedure was found particularly effective for improving the segment overlap  measure.

additional input
another improvement is the addition of the amino acid composition of the target sequence to the predictor  <cit> : the term ∑a=120fafa
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaaewaqaaiabdaeagnaabaaaleaacqwghbqyaeqaaaqaaiabdggahjabg2da9iabigdaxaqaaiabikdayiabicdawaqdcqghris5aogaemozay2aasbaasqaaiabdggahbqabaaaaa@3926@ was added to eq.  <dig> where fa is a regression parameter, and fa is the fraction of the amino acid type a. from a preliminary work based on a linear predictor  <cit> , it was observed that this input slightly improved the accuracy by ~ <dig> %.

training and test data set
we carried out a 15-fold cross-validation test following exactly the same procedure and the same data set as the previous study  <cit> . in the data set, there are  <dig> protein domains, each of which represents a superfamily according to the scop database   <cit> . this data set was randomly divided so that  <dig> domains were used for training and the remaining  <dig> domains for testing, and the random division was repeated  <dig> times . no pair of these domains belong to the same superfamily, and hence they are not expected to be homologous. thus, the present benchmark is a very stringent one. for obtaining pssms by running psi-blast, we use the uniref <dig>  amino acid sequence database  <cit>  containing some  <dig> million entries. also the number of iterations in psi-blast homology searches was reduced to  <dig> times from  <dig> used in the previous study. this especially increased the accuracy of ss predictions. these results are consistent with the study of  <cit> .

numerics
one drawback of the crn method is the computational time required for numerically solving the equation of state . for that purpose, instead of the gauss-seidel-like method previously used, we implemented a successive over-relaxation method which was found to be much more efficient. let v denote the stage of iteration. we set the initial value of the state vectors  as

xi=tanh⁡.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaieqacqwf4baedaqhaawcbagaemyaakgabagaeiikagiaegimaajaeiykakcaaogaeyypa0jagiidaqnaeiyyaemaeioba4maeiiaag2aamwaceaaiigacqgfxoqycqwgwbgvcqwf1bqddawgaawcbagaemyaakgabeaaaogaay5waiaaw2faaiabc6cauiaaxmaacawljawaaewaceaacqai3awnaiaawicacaglpaaaaaa@4558@

then, for i =  <dig>  ..., l , we update the state vectors by

xi←xi+ω{tanh⁡−xi}.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqacegabaaabaacbegae8heag3aa0baasqaaiabdmgapbqaaiabcicaoiabikdayiabdaha2jabgucariabigdaxiabcmcapaaakiabgcziskab=hha4naadaaaleaacqwgpbqaaeaacqggoaakcqaiyagmcqwg2bgdcqggpaqkaagccqghrawkiigacqgfjpwdcqgg7bwecygg0badcqgghbqycqggubgbcqggobaacqggbbwwcqwgxbwvcqggoaakcqwf4baedaqhaawcbagaemyaakmaeyoei0iaegymaedabagaeiikagiaegomaijaemodaynaey4kasiaegymaejaeiykakcaaagcbagaey4kasiae8heag3aa0baasqaaiabdmgapjabgucariabigdaxaqaaiabcicaoiabikdayiabdaha2jabcmcapaaakiabgucariab+f7ahjabdafawjab=vha1naabaaaleaacqwgpbqaaeqaaogaeiyxa0laeyoei0iae8heag3aa0baasqaaiabdmgapbqaaiabcicaoiabikdayiabdaha2jabcmcapaaakiabc2ha9jabc6cauaaacawljagaaczcamaabmgabagaegioagdacagloagaayzkaaaaaa@768c@

next, we update them in the reverse order. that is, for i = l, ...,  <dig> ,

xi←xi+ω{tanh⁡−xi}.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqacegabaaabaacbegae8heag3aa0baasqaaiabdmgapbqaaiabcicaoiabikdayiabdaha2jabgucariabikdayiabcmcapaaakiabgcziskab=hha4naadaaaleaacqwgpbqaaeaacqggoaakcqaiyagmcqwg2bgdcqghrawkcqaixaqmcqggpaqkaagccqghrawkiigacqgfjpwdcqgg7bwecygg0badcqgghbqycqggubgbcqggobaacqggbbwwcqwgxbwvcqggoaakcqwf4baedaqhaawcbagaemyaakmaeyoei0iaegymaedabagaeiikagiaegomaijaemodaynaey4kasiaegymaejaeiykakcaaagcbagaey4kasiae8heag3aa0baasqaaiabdmgapjabgucariabigdaxaqaaiabcicaoiabikdayiabdaha2jabgucariabikdayiabcmcapaaakiabgucariab+f7ahjabdafawjab=vha1naabaaaleaacqwgpbqaaeqaaogaeiyxa0laeyoei0iae8heag3aa0baasqaaiabdmgapbqaaiabcicaoiabikdayiabdaha2jabgucariabigdaxiabcmcapaaakiabc2ha9jabc6cauaaacawljagaaczcamaabmgabagaegyoakdacagloagaayzkaaaaaa@7c08@

we then set v ← v +  <dig>  and iterate eqs.  and  until {xi} converges. the acceleration parameter of ω =  <dig>  was found effective. the convergence criterion is

∑i=1l||xi−xi||rn/nl<10−3     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaaewbqaaiabcyha8jabcyha8hqabiab=hha4naadaaaleaacqwgpbqaaeaacqggoaakcqaiyagmcqwg2bgdcqghrawkcqaiyagmcqggpaqkaagccqghsislcqwf4baedaqhaawcbagaemyaakgabagaeiikagiaegomaijaemodaynaey4kasiaegymaejaeiykakcaaogaeiifawnaeiifaw3aasbaasqaaiab=jfasnaacaaameqabagaemota4eaaawcbeaakiabc+cavmaakaaabagaemota4ealeqaaogaemitawkaeyipawjaegymaejaegimaazaawbaasqabeaacqghsislcqaizawmaaaabagaemyaakmaeyypa0jaegymaedabagaemitaweaniabgghildgccawljagaaczcamaabmgabagaegymaejaegimaadacagloagaayzkaaaaaa@5be7@

where ||·||rn denotes the euclidean norm. this criterion is much less stringent than previous study , but this does not affect the prediction accuracy significantly. convergence is typically achieved within  <dig> to  <dig> iterations for one protein.

it is noted that the algorithm and parameters presented in this subsection are determined only for efficiently solving the equation of state . as such, the choice of the parameters such as ω or the threshold of convergence has little, if any, impact on the prediction accuracy.

RESULTS
there are two main ingredients for the improved one-dimensional protein structure prediction in the present study. first is the use of large-scale critical random networks of  <dig> dimension and  <dig> ensemble predictors. second is the use of a large sequence database  for psi-blast searches. as demonstrated in table  <dig>  the crn method achieves remarkably accurate predictions. in comparison with the previous study  <cit>  based on 2000-dimensional crns , the q <dig> and sov measures in ss predictions improved from  <dig> % and  <dig> % to  <dig> % and  <dig> %, respectively. similarly, the average correlation coefficient improved from  <dig>  to  <dig>  for cn predictions, and from  <dig>  to  <dig>  for rwco predictions. the 2-state predictions for cn yield, on average, q <dig> =  <dig> % per chain and  <dig> % per residue, and matthews' correlation coefficient of  <dig> .

ss, secondary structure prediction: q <dig> is the percentage of correct prediction.; sov is the segment overlap measure  <cit> .

cn, contact number prediction: cor is the pearson's correlation coefficient between the predicted and native cns; deva is the rms error normalized by the standard deviation of the native cn  <cit> .

rwco, residue-wise contact order prediction: cor and deva are defined as for cn but calculated with predicted and native rwcos.

the dependence of the ss prediction accuracy on the dimension and ensemble size of crns shows clearly that larger scale crns lead to better predictions . the difference between "crn2000× <dig> " and "crn2000×10"  signifies the improvement due to the use of larger sequence database, which is in fact quite significant. on the other hand, the difference between crn3000× <dig> and crn3000× <dig> exemplifies the difference due to the the ensemble size . increasing the ensemble size does improve the accuracy, but the effect is relatively small. the accuracy steadily increases as we use  <dig> and  <dig> dimensional state vectors. finally, a small improvement is made by the use of the second stage filter. it may be possible to employ even larger state vectors to further improve the accuracy, but we did not try such possibility because of the hardware limitations.

a closer examination of the ss prediction results  reveals the drastic improvement of β strand prediction from qe =  <dig> % to  <dig> % . although the values of qc and qepre
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgrbqudaqhaawcbagaemyraueabagaemicaanaemocainaemyzaugaaaaa@3340@ are slightly lower than in the previous study by  <dig> – <dig> %, the accuracies of other classes have improved by  <dig> –4%. comparison of one prediction method with others is a difficult problem. different methods are based on different data sets for both training and testing as well as the definition of secondary structural categories. in addition, prediction accuracies usually depend on the number of homologs used , and the number of homologs, in turn, depends on the sequence database used. there are also ambiguities in what to compare: the best possible accuracies by any means or learning capacity of the method from a specified set of data, etc. with these cautions in mind, we present below the comparison between crnpred and other methods. the widely used psipred program  <cit>  which is based on conventional feed-forward neural networks achieves q <dig> of 78%. a more recently developed method, porter,  <cit>  which is based on bidirectional recurrent neural networks achieves q <dig> of 79%. an even more intricate method based on bidirectional segmented-memory recurrent neural networks  <cit>  shows an accuracy of q <dig> = 73% . to further examine the performance of crnpred in comparison with other methods, we extracted recently solved protein structures  from the eva server  <cit>  that do not include any of the proteins in the training set. for this purpose, we trained the parameters by using  <dig> scop  domain representatives. the application of crnpred to these proteins yielded the average q <dig> of  <dig> %, whereas the values for the same set of proteins obtained by psipred <cit>  and porter <cit>  were  <dig> % and  <dig> %, respectively. that is, the crnpred result was inferior to these methods by 1–3% on the basis of the eva data set. when the data set was limited to those  <dig> proteins with more than  <dig> homologs in the sequence database, the q <dig> obtained by crnpred, psipred, and porter were  <dig> %,  <dig> %, and  <dig> %, respectively. crnpred seems to be sensitive to the number of homologs used for constructing a pssm. regarding the contact number prediction, crnpred, achieving cor =  <dig> , is the most accurate method available today. the simple linear method  <cit>  with multiple sequence alignment derived from the hssp database  <cit>  showed a correlation coefficient of  <dig> . a more advanced method based on support vector machines  achieves a correlation of  <dig>  per chain <cit> .

qspre
qs: the number of correctly predicted residues of the ss class s = h, e, c divided by the number of residues in the class in native structures.

qspre
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgrbqudaqhaawcbagaem4camhabagaemicaanaemocainaemyzaugaaaaa@339c@: the number of correctly predicted residues of the ss class s = h, e, c divided by the number of residues predicted as the corresponding class.

mc: matthews' correlation coefficient.

it is known that the number of homologs found by the psi-blast searches significantly affects the prediction accuracies  <cit> . we have examined this effect by plotting the accuracy measures for a given minimum number of homologs found by psi-blast . for example, we see in fig.  <dig> that, for those proteins with more than  <dig> homologs, the average q <dig> for ss predictions is  <dig> %. the effect of the number of homologs significantly depends on the type of 1d structure. for ss prediction, q <dig> steadily increases as the number of homologs increases up to  <dig>  but it stays in the range between  <dig>  and  <dig>  until the minimum number of homologs reaches around  <dig>  and then it starts to decrease. for cn prediction, cor also increases steadily but more slowly, and it does not degrade when the minimum number of homologs reaches  <dig>  this tendency implies that cn is more conservative than ss during protein evolution, which is consistent with previous observations  <cit> . on the contrary, rwco exhibits a peculiar behavior. the value of cor reaches its peak at the minimum number of homologs of  <dig> beyond which the value rapidly decreases. this indicates that rwco is not evolutionarily well conserved. it was observed that the accuracies of ss and cn predictions constantly increased when the dimension of crns was increased from  <dig> to  <dig>  but such was not the case for rwco . rwco seems to be such delicate a quantity that it is very difficult to extract relevant information from the amino acid sequence.

finally, we note on practical applicability of predicted 1d structures. we do not believe, at present, that the construction of a 3d structure purely from the predicted 1d structures is practical, if possible at all, because of the limited accuracy of the rwco prediction. however, ss and cn predictions are very accurate for many proteins so that they may already serve as valuable restraints for 3d structure predictions. also, ss and cn predictions may be applied to domain identification often necessary for experimental determination of protein structures. crnpred has been proved useful for such a purpose  <cit> . although of the limited accuracy, predicted rwcos still exhibit significant correlations with the correct values. since rwcos reflect the extent to which a residue is involved in long-range contacts, predicted rwcos may be useful for enumerating potentially structurally important residues <cit> . an interesting alternative application of the crn framework is to regard the solution of the equation of state  as an extended sequence profile. by so doing, it is straightforward to apply the solution to the profile-profile comparison for fold recognition  <cit> . such an application may be also pursued in the future.

CONCLUSIONS
we have developed the crnpred program that predicts secondary structures , contact numbers , and residue-wise contact orders  of a protein given its amino acid sequence. the method is based on large-scale critical random networks. the achieved accuracies are at least as high as other predictors for ss and currently the best for cn and rwco, although the success for rwco prediction is still limited. crnpred will be a useful tool for computational as well as experimental biologists who need accurate one-dimensional protein structure predictions.

availability and requirements
project name: crnpred

project home page: 

operating system: unix-like os .

programming language: c.

other requirements: zsh, psi-blast , the uniref <dig> amino acid sequence database.

license: public domain.

any restrictions to use by non-academics: none.

abbreviations
crn, critical random network; ss, secondary structure; cn, contact number; rwco, residue-wise contact order; 1d, one-dimensional; 3d, three-dimensional.

authors' contributions
a. r. k. designed and implemented the method, carried out benchmarks, wrote the first draft of the manuscript. a. r. k. and k. n. analyzed the results and improved the manuscript.

supplementary material
additional file 1
list of the scop domains used for cross-validation.  archive of text files).

click here for file

 acknowledgements
we thank yasumasa shigemoto for helping construct the crnpred web interface. this work was supported in part by the mext, japan.
