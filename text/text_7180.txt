BACKGROUND
these last years have seen the emergence of a wealth of biological information. technical improvements in genotyping and sequencing technologies have facilitated the access to the genome sequence and to massive data on genes expression and on proteins. this large availability of molecular information has revolutionized the research in many fields of biology. in parallel to these technical developments, methodological advances are needed to address the various questions of scientific interest that have been targeted when developing these new molecular tools. for example, the identification of up to several millions genomic variations in many species and the development of chips allowing for an effective genotyping of snps panels in large cohorts have triggered the need for statistical models able to associate genotypes from individuals and interacting snps to phenotypic traits such as diseases, physiological and productions traits  <cit> . our paper is a contribution to this association problem.

the systematic exploration of the universe of variants spanning the entire genome through genome-wide association studies  has already allowed the identification of hundreds of genetic variants associated to complex diseases and traits, and provided valuable information into their genetic architecture  <cit>  while allowing to improve prediction of phenotypic outcomes  <cit> . nevertheless, most variants identified so far have been found to confer relatively small information about the relationship between changes at the genomic locations and phenotypes because of the lack of reproducibility of many of these findings, or because the identified variants most of the time explain only a small proportion of the underlying genetic variation  <cit> . this observation, quoted as the ‘missing heritability’ problem  <cit>  of course raises the following question: where does the unexplained genetic variation come from? several authors have postulated that many genes and mutations could be involved, with individual small effects, resulting into a low detection power in most of the performed studies, but with large collective effects  <cit> . another tentative explanation is that genes do not work in isolation, leading to the idea that sets of genes  could have a major effect on the tested traits while almost no marginal effect is detectable at individual locus level. note also that this gene network hypothesis is a potentially credible explanation to the lack of reproducibility of obtained positive results  <cit> , due to situations where different mutations or mutations combinations within the network  could lead to similar phenotypic effects  <cit> .

consequently, an important question still remains about the exact relationship between the genomic configuration, including the interactions between the involved genes, and the phenotypic expression. the major idea in this respect is to try to associate observed variations at the macroscopic level  to identified variations and their interactions at the molecular level.

this view introduces at least two challenges. first, the genetic mechanisms underlying most traits of interest are complex and probably involve most of the time many genes and many interactions between these genes, leading to a complex relationship between genomic variants and phenotypes. so, modeling and identification of every, and even of any, interaction is a potentially very challenging task  <cit> . second, from a more statistical point of view, fully modeling the interactions leads to models with large number of parameters to be estimated and large search space, leading to the well-known ‘curse of dimensionality’ problem  <cit> . furthermore, increasing the number of parameters to be estimated potentially makes the power issues mentioned above even more critical. nevertheless, introducing interactions into the model might lead to a more accurate model of the underlying genetics, which in turn might improve the detection power of effects of interest. so it is not obvious that interaction models will present poor power when compared to non-interaction ones, which should motivate more research on the subject.

in the literature, various statistical methods have been used to detect gene-gene or gene-environment interactions  <cit> . many of these statistical methods are parametric and rely on large samples properties  <cit> . on the other hand, nonparametric methods have generated intense interest because of their capacity to handle high-dimensional data  <cit> . in order to limit the size of the search space, many of the proposed approaches may have missed potential interactions by only considering variants that have a significant genetic marginal effect as, for example, in the logistic regression method proposed by  <cit> , where the model relates one or more independent variables  and their corresponding interaction terms  to a discrete dependent variable . because of issues linked to the dimensionality, models such as the logistic regression are limited in their ability to deal with interactions involving many factors  <cit> . in response to these limitations, novel methods for detecting interacting variants have been designed, such as neural networks  <cit> , random jungles  <cit> , random forests  <cit> , boost “boolean operation-based screening and testing”  <cit> , support vector machine  <cit> , megasnphunter  <cit> , antepiseeker  <cit>  or odds ratio  <cit> .

one of the most successfully used family of methods in the gene-interactions problems is multifactor dimensionality reduction   <cit> . the mdr method is nonparametric , model-free , and directly applicable to case-control and discordant-sib-pair studies  <cit> . the main idea in mdr is to reduce the dimensionality of multi-locus data to improve the ability to detect genetic combinations that confer disease risk  <cit> . mdr has been proposed to identify gene–gene or gene-environment interactions when marker and/or environment information is available  <cit> . an advantage of the mdr methods is, as pointed out in  <cit> , that, due to their nature, they theoretically allow to highlight gene–gene interactions of any order  <cit> .

refinements of the method have been proposed to deal with potential limitations. cattaert et al.  <cit>  has proposed a novel multifactor dimensionality reduction method for epistasis detection in small or extended pedigrees, fam-mdr. cattaert et al.  <cit>  and  <cit>  have also developed model-based multifactor dimensionality reduction , a mdr-based technique that is able to unify the best of both nonparametric and parametric worlds, allowing to include corrections for cofactors, as in parametric models, while using the flexible framework of non-parametric mdr analyses. another extension is generalized mdr , a version of the mdr method that permits adjustment for discrete and quantitative covariates and is applicable to both dichotomous and continuous phenotypes  <cit> .

although applied to numerous genetic studies  <cit> , mdr faces important challenges. first, mdr can be computationally intensive, especially when a large number of markers needs to be tested  <cit> . second, the interpretation of mdr results is difficult, for example in situations where a strong marginal effect makes the effects of the other polymorphisms in the interaction questionable  <cit> . third, the mdr method can fail in finding the correct models, because it assumes that there is no genetic heterogeneity, as in situations where a group of cases are explained by a combination of loci different from the one that explains another group of cases  <cit> . lastly, the number of possible combinations explodes exponentially with the number of interacting factors, which makes the approach impractical in terms of needed cohorts sizes and computing time in situations where large numbers of genetic and/or environmental determinants are involved, another instance of the ‘curse of dimensionality’ problem.

in this paper, we propose a novel mdr approach using k-nearest neighbors  methodology  for detecting gene-gene interaction as a possible alternative to current mdr methods in situations where the number of involved determinants is potentially high and the number of tested markers is large. after explaining the rationale of our method, we will provide results on the comparison of knn-mdr to a set of competitor methods on both simulated and real datasets.

methods
knn method
knn stands for “k nearest neighbors” and is one of the most popular algorithms for pattern recognition and classification. roughly, classification of an observation can be made using a majority vote within the k nearest neighbors of the observation  <cit> , where the neighborhood is based on a defined distance between observations. although simple, many researchers have found that the knn algorithm accomplishes very good performance in their experiments on different data sets  <cit> . also, knn is a multivariate method that retains the variable relationships seen in the data because the logical relationships among response variables will be maintained  <cit> , a feature of importance in our genetic context. the flexibility of knn is also a great advantage and this technique helps to alleviate the curse of dimensionality by shrinking the unimportant dimensions of the feature space, bringing more relevant neighbors close to the target point  <cit> .

mdr method
the method will be described for dichotomous traits for the sake of simplicity, but could be extended to other situations using the approach described for gmdr  <cit> . the multi-dimensional reduction  method is designed to replace large dimension problems with reduced dimension ones, allowing to make inferences based on a smaller set of variables. in the context of genomic studies, the idea in  <cit>  is to replace the high dimensional problem arising from considering several markers simultaneously, with one unique variable  that can take only  <dig> values . to illustrate, if a set of n snp markers is used in a case-control study to define the multi-locus genotype, 3n genotypes are possible. each of these genotypes can be mapped to a status with only  <dig> values  using a majority vote on the statuses of the training set individuals falling into that genotype. the classification performances of any set of markers used to define the genotypes can then be assessed, typically using a cross-validation procedure, where the performance is estimated on a test set for each partition trough a measure involving sensitivity and/or sensibility of the classifier, and averaged over all partitions. for all computations reported in this paper, we have used a 10-fold cross-validation procedure and assessed the performances using ‘balanced accuracy’, which is a simple average of the sensibility and the sensitivity of the classifier. repeating this procedure over all possible markers sets allows obtaining the best model, which is defined as the set of markers providing the best allocation performances. in practical situations, the potential number of tested markers sets might be huge: if an exhaustive search is to be performed on all p-markers interactions in a gwas with m markers, about m!/ ~ mp/p! combinations would need to be checked, a huge number with nowadays available markers panels. significance for the optimal model can be obtained through a permutations test, in which the potential links between the individuals’ genotypes and the phenotypes are disrupted by randomly shuffling the phenotypes. the p-values obtained using this test have then to be corrected for multiple testing, where multiple tests are due to the number of models that are successively tested.

knn-mdr method
although a widely used and well-established technique, mdr faces several problems, as detailed above. the computational load described in the previous section remains a major issue. although recent publications  <cit>  have provided some tools to achieve low order interactions screening in a gwas, the task will remain very challenging for larger order interactions and for larger markers sets, such as sequencing data, and alternative approaches reducing the computer burden remain desirable. another problem linked to the mdr methodology arises when a test set individual’s multi-locus genotype has not been observed in the training set, making it impossible to classify the newcomer. furthermore, in situations where very few training individuals share the same multi-locus genotype as the tested one, the accuracy of the assignment can also be questioned. since the number of multi-locus genotypes explodes exponentially when the number of markers in the markers sets increases, this problem becomes rapidly critical, and could finally render the approach inaccurate  or even unusable  in situations where more than 3– <dig> markers are to be used simultaneously and with classical cohorts’ sizes. another consequence of the limited number of markers that can be considered simultaneously in mdr is that the genomic regions involved in interactions will most of the time be represented through a single marker, although, due to linkage disequilibrium, considering several linked markers might increase the association signal intensity, and consequently improve the detection power.

our proposal is therefore to slightly modify mdr to allow facing some of the shortcomings of the method. the only modification is in the status allocation procedure: while mdr uses a majority vote among the  set of individuals sharing the same multilocus genotype as the tested individual, we propose to use a majority vote within a set of the k nearest neighbors of the tested individual. this procedure has the obvious advantage to eliminate the problem of potentially scarce or empty genotypic configurations mentioned above. on the other hand, this strategy introduces the need to define the neighborhood: a “distance” between individuals based on the genotypic configurations at the selected markers will be needed, and the size k of the neighborhood will have to be provided. these parameters of the method - the chosen distance, k - are further discussed in the discussion section. a second advantage of our approach is that more markers can be considered at once than in the classical mdr strategy. the idea, also detailed in the discussion section, is thus to replace the sets of single markers used in mdr by sets of windows spanning several markers: the m markers are split into w windows of contiguous markers, where the choice of the windows sizes and positions could use genetic criteria explained in the discussion section, and the distances used in knn-mdr are based on these windows. all the other steps are similar to the classical mdr steps . note that the number of windows w might be much smaller than the number of markers m, as explained below. consequently, the proposed approach might greatly reduce the needed amount of computations, and consequently make higher-order interactions more affordable. although alternatives are possible, we have used mahalanobis distances in our analyses because of its numerous advantages in our setting .

note that, in knn-mdr, the computer burden scales quadratically with the number of individuals since the distances between pairs of individuals are needed, but is less sensitive to the number of markers since markers are pooled into windows. so, the important parameter from a computing point of view is the number of windows w, which does not necessarily increase when the number of markers increases.

competitor methods
after designing our method, we needed to compare the performances of our approach to some of the other proposed algorithms. since many methods are available  <cit> , we decided to consider four of the most popular ones to be used in the comparison, namely: mdr, boost, megasnphunter and antepiseeker. the rationale for choosing this set of methods is the following:antepiseeker  <cit>  and boost  <cit>  have been recommended as efficient and effective methods in the comparative analysis of  <cit> ,

mdr  <cit>  is one of the most famous methodologies for detecting interactions  <cit> ,

megasnphunter  <cit>  is targeting high level interactions, one of the potential advantage of knn-mdr. also, a method for exploiting large genotypes sets is provided, which is another objective of our algorithm,

all these methods have been applied successfully to real datasets,

these methods have different search strategies: exhaustive search , stochastic search  and heuristic search ,

software implementing the methods is available.




simulation
in order to assess the performances of the proposed method, we have simulated various situations and ran mdr, boost, megasnphunter, antepiseeker and knn-mdr on the same datasets to compare the performances in terms of detection power and accuracy. the generation of the simulation datasets will be described in the following lines.

one of the aims of our study was to assess the performance of the methods to unravel gene-gene or gene-environment interactions in the absence of large marginal effects. the reason for that choice was that many methods are able to detect such large marginal effects and to infer interactions within a limited set of loci selected on that basis. accordingly, we wanted to devise an approach that is able to detect interactions even in the absence of marginal effects. for that reason, efforts have been devoted to generate datasets with interacting genes in the absence of significant marginal effects. furthermore, heterogeneity between samples has been shown to be a major source for the non-reproducibility of significant signals  <cit> . we have modeled heterogeneity by associating penetrances to the multi-locus genotypes underlying the simulated binary trait. the data generation algorithm proceeds along the following lines:to obtain a linkage disequilibrium  pattern similar to patterns that can be observed in humans, snps spanning the human chromosome  <dig>  have been obtained from a study on crohn disease in caucasians  <cit>  for  <dig> individuals. two thousand markers with minor allele frequencies  above  <dig> , and no missing genotype have been selected. hardy-weinberg equilibrium tests have been performed on the genotypes for these markers, and the high maf threshold has been chosen to select informative markers among the complete list of markers, to compensate for the information loss resulting from discarding the other available markers to decrease the computational load. nevertheless, since experimental data has been used, genotyping errors might be present. presence of ld in the data was checked using simple association tests between consecutive markers .

since many different individuals are needed in the simulations, we used a trick similar to  <cit>  to generate new individuals based on the few available genotypes: each individual genotype was chopped into  <dig> snp windows, leading to  <dig> windows with   <dig> different  <dig> loci genotypes. each simulated individual genotype was then built by randomly sampling a genotype for each window and concatenating the  <dig> genotypes into a new complete genotype with  <dig> markers. this technique allows for  <dig> potentially different individuals while conserving some ld.

g snp were then randomly chosen as having an effect on the simulated phenotype, where g =  <dig>   <dig>   <dig> or  <dig>  since snp selection is random, snp might be linked or not.

selected snp genotypes were then used to generate the binary phenotypes. more details of the algorithm are given in an appendix , but roughly:a penetrance is computed for each multi-locus  genotype in such a way that each of the g snp shows no marginal effect: pa|gi=0=pa|gi=1=pa|gi=2=p 


where gi denotes the genotype for locus i ,  <dig>   <dig>   <dig> are the number of instances of the minor allele in the snp genotype, a means affected, p is the penetrance for genotype gi, and p is the prevalence of the disease in the sample .

the multi-locus penetrances mp = p where k, m, … =  <dig>   <dig> or  <dig> are obtained to meet the requirement of no marginal effect . an algorithm to compute these penetrances is provided in an appendix .

the phenotypes  are then obtained by randomly sampling a uniform distribution between  <dig> and  <dig> and comparing the obtained deviate d to the multi-locus penetrance mp: if d <  mp, the individual is  affected.




one snp out of  <dig> consecutive snps was then randomly discarded, leaving  <dig> markers for the analyses. the rationale of this selection is that causative mutations might nowadays be present or not in the genotyped variants. this will also be the case in our simulations.

genotypes and corresponding phenotypes were generated for each simulation, and the obtained datasets were studied using all four methods. knn-mdr windows size was set to  <dig> markers, leading to  <dig> non-overlapping windows, and k value was set to  <dig>  the parameters for the other methods were chosen so that resolution was almost similar for all methods.

finally,  <dig> permutations of the phenotypes were performed for each simulation  and the resulting datasets were analyzed using the four methods in order to assess significance. although this number of permutations is too low for routine work, it was used to reduce the computing burden and help us to discriminate between results clearly non-significant  and those potentially significant . when a higher precision was needed for the p-values , an adaptative permutations scheme was used, in which windows not reaching a pre-determined p-value threshold are progressively abandoned in the permutations scheme since these windows are very unlikely to finally reach a significant result  <cit> .




real data
analyses using real data have also been performed. rheumatoid arthritis  genotype data on  <dig> cases and  <dig> controls have been obtained from wtccc  <cit> . genotypes from the affymetrix genechip 500 k mapping array set have been filtered using the usual quality controls tests on dna quality , markers quality , genotypes frequencies . missing genotypes for the genechip markers have been imputed using impute <dig> software  <cit> . this procedure led to  <dig> snp to be analyzed for the  <dig> cohorts. zhang et al.  <cit>  and  <cit>  also used this dataset to infer potential interactions. these studies will therefore serve as a comparison for the results obtained with our approach.

working on large datasets
when working on large sets of markers, such as for example those commonly met in gwas analyses, splitting the complete set into a reasonable set of windows could necessitate including large numbers of markers in each window, which would eventually swamp the signals of interest, as explained in the discussion section. an alternative is to pre-select a subset of markers  and to define a first set of windows based on these markers. this strategy would allow windows to cover potentially large regions while preserving some detection power. after a first run of knn-mdr using this subset, the detected regions  would be used for a second round of knn-mdr runs. in this new round, the markers hidden in the first round could be partially or totally recovered for each of the identified regions, and the same approach as in the first round could be used recursively on these new regions. the sequential detection of progressively denser regions could continue down to single markers. an example of this strategy in a gwas study is provided in the of “results on wtccc data” section.

RESULTS
results on simulated data
since performing classical mdr analyses on a large number of markers is not an obvious task, especially when the number of putative involved snps  is  <dig> or more, we restricted our analyses to g =  <dig> and g =  <dig> to make comparisons to other methods feasible. we have defined the “power” as the proportion of simulations where an association signal was detected , and the “corrected power” as the proportion of simulations where the association was detected and involved the causal snp . the comparison of the five tested methods is presented for situations where g =  <dig> in table  <dig> and for g =  <dig> in table  <dig> .table  <dig> simulation results when g =  <dig> and the number of cases and controls is 500


tables  <dig> and  <dig> shows the results of  <dig> simulations. for knn-mdr, the number of neighbors is set to  <dig>  and the  <dig> markers are split into  <dig> windows of  <dig> consecutive markers. all possible sets of up to  <dig> windows for table  <dig>  and up to  <dig> windows for table  <dig>  have been tested. parameters for the other methods were set to default values. due to the very large number of tests required when  <dig> markers are involved, mdr results have not been obtained in table  <dig>  data sets used to generate these  <dig> tables are provided as additional files  <dig> and 3





as can be seen from tables  <dig> and  <dig>  knn-mdr seems to show reasonable power when compared to its competitors. more importantly, corrected power of the method is significantly better than for the other tested methods .

a short literature survey  leads to the conclusions that many of the methods seem to be marred by high false positive rates. to test that, we have simulated situations where no snp was involved in the generation of the phenotypes, so that snp detection by the algorithms would correspond to false positives. table  <dig> shows the results of these simulations.table  <dig> simulation results when g =  <dig> and the number of cases and controls is 500

the detection threshold α is set to  <dig> . the data set used to generate this table is provided as additional file 5





we ran another set of simulations to assess the respective effects of the sizes of the windows and of the number k of neighbors on the  detection power. results of these simulations are reported in table  <dig> table  <dig> power  and corrected power  when the parameters k  and w  are varied in  <dig> simulations with  <dig> cases and  <dig> controls and g = 2

the data set used to generate this table is provided as additional file 6





results on wtccc data
since working on such a large dataset  is very demanding in terms of computing time, we proceeded as follows:20 k snp were first extracted from the data. although several selection procedures could be applied, we simply selected  <dig> snp every  <dig> snp.

we divided the data into  <dig> windows of  <dig> snp each.

we then tested each of the  <dig> possible pairs of windows  using knn-mdr.

we extracted the  <dig> sets for which the p-values were lower than  <dig> e- <dig> . to reach that significance level using a permutations procedure, we used the following adaptative scheme: after  <dig> permutations performed on the  <dig> possible pairs of windows, only those reaching the  <dig>  level were considered for the next round of permutations, assuming that those not reaching that level of significance were very unlikely to reach the desired significance at the end of the process. this left us with  <dig> combinations. in the next round,  <dig> more permutations were performed, and only the combinations reaching the  <dig>  level were kept . repeating this procedure for  <dig> e <dig>   <dig> e <dig>   <dig> e <dig> and  <dig> e <dig> permutations, and respective thresholds equal to  <dig> e- <dig>   <dig> e- <dig>   <dig> e- <dig> and  <dig> e- <dig>  we ended up with the  <dig> sets cited above.

the snp hidden in step  <dig> were then recovered, leading to  <dig> sets of  <dig> snp .

knn-mdr was applied on every set from step 5: the sets were divided into  <dig> windows of  <dig> snp and all  <dig> combinations of windows pairs in each set were considered by knn-mdr.

we kept the  <dig> sets of  <dig> snp with a p-value <  <dig> e- <dig> .

mdr was then used for the sets from the previous step, leading to examine  <dig> snp-snp interactions for each set.

the interactions with a p-value <  <dig> e- <dig>  were then considered as significant.




results from this analysis are presented in table  <dig>  the full version of table  <dig> is provided in a supplementary file. figure  <dig> provides a view of the significant results at the chromosome level for our study as well as for  <dig> other similar studies on this dataset .table  <dig> the  <dig> most significant results of the analysis on the ra dataset from wtccc

the first two columns provide the names and chromosomal positions of the snp found to be associated to the phenotype. positions are indicated by the chromosome and the snp physical position on the chromosome using the ncbi human build  <dig>  the third column contains the corresponding balanced accuracies and the last column reports the p-values computed using an adaptative permutation scheme. the complete table is provided as additional file 7



fig.  <dig> comparison of the inter-chromosomal interactions detected on the ra dataset by knn-mdr and other interaction methods using this same dataset as example 




discussion
this paper has introduced a new mdr approach to find markers interactions in genomic scans. it could also be used for other attributes than markers, such as environmental factors, leading to a gene-environment interaction search method. due to the proposed strategy relying on the mdr approach, and in parallel with a recent study  <cit>  using  mdr as a reference strategy, we have compared our proposed method’s performances to this reference and other reference methods , and tried to show that our method could have benefits compared to these methods. of course, other algorithms might have been tested, such as the recent bayesian high-order interaction toolkit  <cit>  which is proposing a mcmc approach to scan the very large search space of potential sets of markers . our point in this respect was not to be exhaustive, but simply to show that the approach we propose can bring some more information than other popular methods, and might be a useful addition to the arsenal developed to tackle genomic interaction problems.

the results obtained through the simulations demonstrate some of the features that potentially make knn-mdr helpful. more specifically, the simulations show the feasibility of scans using large number of markers, as opposed to mdr where the computer burden explodes with the number of markers . this might allow to highlight interactions between markers far apart on the genomic map , while some strategies proposed to restrict the scans to close-by markers  to reduce the amount of computations.

we now discuss some of the features of the method:

number of interacting loci
in this paper, although the algorithm given in the appendix can be used for g larger than  <dig>  only  <dig> markers have been used to generate the phenotypes. nevertheless, in practical applications, it is not unlikely that situations involving more than  <dig> loci might exist. these situations might increase the interest of using methods such as knn-mdr. indeed, when more regions are involved in the phenotype, this could decrease the distance measure between individuals sharing some or all of these regions and better cluster individuals sharing the same status. conversely, in mdr, discovering such complex patterns would likely necessitate to increase the number of loci scanned simultaneously, which would make computations even more difficult. also, increasing the number of loci increases the number of cells with no  observations, making status allocation potentially inaccurate or even impossible.

parameter settings
we mentioned earlier that parameters setting in knn-mdr mainly involves defining the sizes, positions and the number of windows, the number k of neighbors and the distance measure. all parameters are problem dependent, making it difficult to devise general rules. nevertheless, some guidelines might be given.

in all the analyses performed in this study, we have only used mahalanobis distances, as already mentioned. the reason was that this distance allows to take into account potential correlations between attributes  and because it makes it possible to weight the attributes in the sum . in our studies, only snps have been used, for which the distance proposed in the mahalanobis measure makes sense, with d = d =  <dig> * d, where aa, ab and bb are the three possible snp genotypes. this might be different and might need more investigations if other types of genetic variants are used. note also that, in most computations, to reduce the computational burden, the correlation between neighboring markers has not been estimated but set to  <dig> , which might potentially affect the power. although we did not explicitly test this, we expect that including the correlations would lead to better take into account the linkage disequilibrium, which should have a positive effect on the detection power. so, using this information might be favorable in terms of power, but at the cost of an increase in the computation time. note also that using this kind of distance makes less sense when working with markers with more than  <dig> alleles, unless it can be postulated that the distance between, for example, alleles  <dig> and  <dig> is roughly twice the distance between alleles  <dig> and  <dig>  an easy to compute and similar distance measure would then be to square the number of differing alleles  between two compared genotypes, to normalize as for the mahalanobis distance, to sum over all markers in the window and to take the square root of the product. this “binary” distance is implemented in our knn-mdr software.

for the windows dimensions, our idea is to use the assumption that individuals sharing mutations responsible for the trait should look more similar in the surroundings of these mutations than those not sharing these mutations. the resemblance should thus extend to neighboring markers, where the neighborhood size is a function of the linkage disequilibrium  in the region. in situations where ld increases , distance between individuals sharing genomic regions  should decrease and detection power should increase. note that this genomic feature is ignored in the other tested methods. accordingly, the windows sizes w should ideally be defined to capture the local linkage disequilibrium. since the measurable ld is dependent on the population history and on the markers density, assessment of this measure should first be made in order to have reference dimensions for the various windows to be used in knn. note that the extent of ld need not be the same across the whole genome: accordingly, the size of the windows might be varied along the genome to better reflect the underlying structure and better capture the relevant information.

to illustrate that expected behavior, we have performed the simulations leading to table  <dig>  as visible from that table, the powers decrease when the windows sizes increase. our interpretation of this result is that, due to the way the simulated data are generated, chunks of five linked  markers are used, which should restrict the signal caused by ld to five markers. adding more markers to the windows adds noise, and consequently reduces the resemblance between the composite pieces of chromosomes harboring the causative mutations, and thus the power.

next, the number k of neighbors should somehow reflect the number of animals sharing regions harboring causal mutations. this number is of course unknown and difficult to evaluate a priori because it is dependent on various population and trait parameters such as the history of the population or the genetic heterogeneity of the trait. furthermore, it might vary from region to region, making it difficult to devise general rules allowing to infer relevant values of k. possible “brute force” approaches would be to rerun the algorithm with varying number of neighbors  or to use bootstrap methods  <cit> . this strategy could allow to capture regions of interest while integrating potential sources of variations, at the cost of supplementary computer burden. another point of view is that the corrected powers do not significantly  disagree between the various k values for the tested windows sizes, which indicates that the results might not be very sensitive to this parameter, at least in our simulations. for this reason, we used k =  <dig> or k =  <dig> in our computations. note also that odd k values might facilitate the majority vote.

false positive rates
our simulations have shown that, as reported in other studies, results are often penalized by high false positive rates . one obvious reason is multiple testing: the large number of performed tests necessitates that the significance threshold be properly adapted, which is not always easy to do. another reason in our study is the way we have performed the simulations. indeed, we have managed to have epistatic interactions with little marginal effects in order to avoid the easier situations where individual loci can be identified in a first step, followed by the identification of interactions between these loci identified first in a second step. to obtain these situations, we have used multi-locus prevalences, which has led to some kind of genetic heterogeneity: a same multi-locus genotype could simultaneously be present in cases and in controls, making it harder to identify these loci. these complicating factors have been associated to higher false positive rates in other studies, along with other design factors such as the number of cross-validation subsets  <cit> . our model might be less sensitive to these factors: looking for neighbors might allow selecting the individuals sharing the relevant features in a heterogeneous set of individuals. also, decreasing the number of tests , might also lead to somehow relaxing the penalty arising from multiple testing.

power and corrected power
the reason for the drop in the power of the alternative methods when considering the accuracy is not completely clear, but we can suggest a tentative explanation.

as can be seen from table  <dig>  all methods show high rates of false positive results, while knn-mdr seems to behave reasonably well from that point of view. although this is no definite proof, this is an indication that the high power observed in the simulations for the alternative methods is probably due to false positive results. correcting for the accuracy  therefore eliminates most of these false positive results, so drastically reducing the observed power.

a potential criticism on our “accuracy measure” is that using windows sets makes it more likely to cover the culprit regions, and so this “accuracy” measure is biased in favor of knn-mdr. for that reason, and to make the comparison fair between the methods, we have chosen the parameters to end up with similar number of markers in the finally selected markers sets in each approach. note nevertheless that the resolution of knn-mdr could eventually be increased in these analyses, for example using the strategy described for large datasets in the methods section.

figure  <dig> shows that no combination at the chromosome level is consistent across our study and two other similar studies on the same dataset  while other significant results are specific to one or two methods. some results from knn-mdr are consistent with those obtained by shchetynsky, others are consistent with those of zhang while no corresponding results between zhang and shchetynsky studies could be found. power and false positive issues might potentially explain these discrepancies, although no definite proof can be put forward based on these preliminary analyses.

so, in our study as in the other ones, statistically significant snp interactions have been identified using knn-mdr and mdr in a genome-wide association study. their biological relevance is obviously not clear at this stage and needs more investigations in the future. we can nevertheless say that some of our results are consistent with other results in the domain of rheumatoid arthritis  and that, in addition, new candidates contributing to the etiology of this disease have potentially been identified. this result shows that, as suggested in the simulations, differences in the approaches and potential differences in the respective powers of the used methods might lead to new insights in the etiology of the disease. this observation should trigger more research on the use of composite methods, combining the qualities of several approaches.

computer resources
in our results, the comparisons between  and knn-mdr in terms of computer resources has not been fully addressed. nevertheless, it has been shown how and why knn-mdr decreases the computer load with respect to mdr, making it a potential candidate to analyze large datasets, as shown for the ra data. to be fair, it should be mentioned that computing nearest neighbors is more computer intensive than a majority vote in the subset sharing the same multi-locus genotype. nevertheless, as shown in the simulations, and as can be understood from the previous discussion, computations remain more affordable in knn-mdr than in mdr and the other methods for similar scans. furthermore, strategies could also be devised to make knn-mdr efficient, such as pre-computing distances for windows and using distance additivity properties to compute distance over several windows.

another point that might be worth adding is that, although knn is natively a classification method, we have used it here in a detection context. knn-mdr could nevertheless as well be used as a classification tool: to that end, the best model  could be used to compute the neighborhood of a new individual and classify the latter in one or the other category.

CONCLUSIONS
in summary, knn-mdr is an alternative to existing methods for detecting epistatic interactions, with interesting features. among these, we have demonstrated that knn-mdr is more computationally efficient than other exhaustive strategies, facilitating the analysis of large-scale data sets with potentially genome-wide snps. the method is also capable to detect high-order interactions and to take into account linkage disequilibrium . another advantage is that it is able to detect interactions between snps even in the absence of marginal effects. also, the method is non-parametric: no prior distribution is assumed, unlike many parametric-statistical methods. nevertheless, parameters  are available to allow some flexibility in the search strategies, which could help to render the method useful in other classification contexts.

although knn-mdr is potentially beneficial for epistasis detection, several aspects would nevertheless deserve more investigations. for example, the burden associated to the computation of the k nearest neighbors could become an issue when the dataset is very large. since the load increases quadratically with the number of individuals, and linearly with the number of markers, improving the computational performances of the method could necessitate some code optimization to make the program more efficient. another point necessitating more work is the tuning of the parameters allowing an optimal detection power. this includes the optimal sizes of the windows - which should be dependent on the studied population, the markers density, the ld pattern, the optimal size of the neighborhoods to be considered, the pre-selection of markers in the early phase of large dataset analyses, the distance measure or the adaptative selection scheme for the selection of markers in large studies, among others.

additional files

additional file 1: computing multi-locus penetrances. 

 
additional file 2: the data set supporting the results of table  <dig>  

 
additional file 3: the data set supporting the results of table  <dig>  

 
additional file 4: competitor methods. 

 
additional file 5: the data set supporting the results of table  <dig>  

 
additional file 6: the data set supporting the results of table  <dig>  

 
additional file 7: table  <dig> complete. 

 
additional file 8: knn mdr user’s guide. 

 


abbreviations
bhitbayesian high-order interaction toolkit

boostboolean operation-based screening

fam-mdrflexible family-based multifactor dimensionality reduction

gmdrgeneralized multifactor dimensionality reduction

gwasgenome-wide association study

hsa9human chromosome 9

knnk-nearest neighbors

ldlinkage disequilibrium

mafminor allele frequency

mb-mdrmodel-based multifactor dimensionality reduction

mdrmulti dimensional reduction

snpsingle-nucleotide polymorphism

wtcccwellcome trust case control consortium

