BACKGROUND
analysis of environmental samples, metagenomic analysis, is defined as the characterization of microbial genomes via the direct isolation of genomic sequences from the environment without prior cultivation  <cit> . environmental samples are sequenced using next-generation sequencing technologies which yield short reads lengths   <cit> . in traditional analysis, the whole genome of an organism is sequenced and assembled, then genes are predicted along this continuous sequence. in metagenomics, single genomes cannot be assembled and therefore, it is a challenge to predict genes within these short sequences  <cit> . accurate gene annotation for environmental samples is needed so that genes can be classified to their correct functions, and it paves the way for functional studies in metagenomics.

traditionally gene prediction programs can be categorized in two different groups. first, the ab initio programs, which train model parameters on known annotations in order to predict unknown annotations, are widely used in gene prediction  <cit> . there are a large number of ab-initio gene-finding programs, e.g.: genie  <cit> , genscan  <cit> , geneid  <cit> , glimmer  <cit> , and genemark  <cit> . the second group of gene prediction programs, known as homology-based programs, that align input sequences to the closest homologous sequence in the database to predict genes. some popular homology-based programs are genewise  <cit> , agenda  <cit> , and the well-known blast  <cit> . in addition, hybrid approaches that combine the gene prediction programs have been proposed for traditional gene annotation  <cit> . unfortunately, it is not possible to use traditional gene prediction methods in metagenomics. applying these conventional approaches to metagenomics is restricted by the identification of open reading frames , which begin with a start codon and end with an in-frame stop codon  <cit> . usually, genes in prokayrotes are 1000-bp in length on average  <cit> . due to the short sequence length of metagenomic reads , they contain incomplete orfs that lack start and/or stop codons, thus conventional ab-initio programs cannot be applied to metagenomics  <cit> . similarly, homology-based approaches for gene predictions rely on databases that only contain known, and thus a limited set, of genes. therefore, both of these categories do not work well for metagenomic fragments which are about  <dig> bp/less than  <dig> bp when produced by sanger and next generation sequencing, respectively  <cit> .

therefore, recent tools have emerged to address these problems for metagenomic reads. three programs are widely used for this purpose: orphelia  <cit> , metagene  <cit> /metageneannotator  <cit> , and genemark  <cit> . this paper will benchmark and compare these methods. then we demonstrate that we can boost specificity drastically by 10% by combining the programs' predictions and overall, improve accuracy by 1-4% while also improving annotation  by 1-8%.

overview of metagenomic gene prediction programs
genemark   <cit> 
like the previous genemark, genemark for metagenomics utilizes a heuristic approach that builds a set of markov models using a minimal amount of sequence information. the heuristic approach is used to find genes in small fragments of anonymous prokaryotic genomes and in genomes of organelles, viruses, phages and plasmids, as well as, in highly inhomogeneous genomes where adjustment of models to local dna composition is needed. it is proven that the heuristic built model is useful for dealing with prokaryotic species whose genomic sequence information is available in small amounts. procedures for building the heuristic models are the following:

 <dig>  obtain the relationships between positional nucleotide frequencies and the global nucleotide frequencies as well as relationships between the amino acid frequencies and the global gc% of the training sequences.

 <dig>  approximate the obtained relationships by the standard linear regression

 <dig>  obtain the initial values of frequency of occurrence of each of the  <dig> codons by calculating the products of the three positional nucleotide frequencies of corresponding nucleotides.

 <dig>  modify the initial value of codon frequency by the frequency of each amino acid determined by the gc content.

 <dig>  create a codon usage table for all  <dig> codons.

 <dig>  construct the 3-periodic zero order markov model of a protein coding region using the codon usage table.

the heuristic model was built using these procedures described by using a training data that consists of  <dig> bacteria and archaea species  <cit> . in order to build a mixture dual model, they are further divided into two sets:   <dig> archaea species and  <dig> bacteria species;   <dig> mesophilic species and  <dig> thermophilic species  <cit> .

mga  <cit> 
metagene annotator  is an upgrade version of another software package, called metagene  which is used in gene prediction in metagenomic sequence data. metagene predicts genes in two stages. first, all possible orfs are revealed from the input sequences. next, all orfs are scored by their base compositions and lengths using the log-odds scoring scheme. in the log-odds scoring, the frequency of an event observed in orfs is divided by the observed frequency in random orfs, and a base-two logarithm of the ratio is used as the score for the event  <cit> . second, an optimal  combination of orfs is calculated using the scores of orientations and distances of neighboring orfs in addition to the scores for the orfs themselves  <cit> . however, there are two major limitations that exist in  software program: the lack of ribosomal binding site  model, and a low sensitivity to atypical genes, whose codon usages are different from those of typical genes  <cit> . to overcome these limitations and to improve the usability of the program, a new version of the mg called the  was developed  <cit> . the mga has statistical models of prophage genes that enables it to detect lateral gene transfers or phage infections. the mga also has an adaptable rbs model based on complementary sequences of the  <dig> tail of  <dig> s ribosomal rna which helps it to precisely predict translation starts of genes even when input genomic sequences are short and anonymous sequences. since the mga is based on the algorithm of mg, it has logistic regression models of the gc content and the di-codon frequencies  of mg  <cit> . these features of the mga remarkably improve prediction accuracies of genes on a wide range of prokaryotic genomes.

orphelia   <cit> 
orphelia is a metagenomic orf finding program for the prediction of protein coding genes in short fragments of dna sequences with unknown phylogenetic origin. the orpehelia prediction engine performs gene-finding in two stages. in the first stage, features for monocodon usage, dicodon usage and translation initiation sites are extracted from the orf sequence using linear discriminants. in the second stage, an artificial neural network combines the sequence features with orf length and fragment gc-content, and computes a posterior probability of an orf to encode a protein. its neural network is trained on randomly excised dna fragments of a specified length from the genomes that were used for discriminant training.

RESULTS
to improve metagenomic gene prediction and annotation, we first analyze the three leading metagenomic gene prediction programs' sensitivity and specificity to predict whether a read contains a gene. then, we analyze the upper-bound prediction error of the algorithms, which quantifies the error when all prediction programs mark the read incorrectly. the upper-bound error is much lower than the individual methods, demonstrating that improvements can be made if we combine the predictors. finally, we analyze different ways of combining the prediction programs to improve prediction accuracy, sensitivity, specificity, and annotation accuracy.

benchmarking the three gene prediction programs
in this section, we aim to rigorously benchmark three different gene prediction programs for different read lengths and fragment types . in figures  <dig> and  <dig>  we show the sensitivities and specificities of the three algorithms. mga's sensitivities are higher than those of orphelia and genemark for 200- <dig> bp reads. however, its specificities are the lowest, shown in figure  <dig>  for short reads, genemark does not have the highest sensitivities, but its specificities are the highest. overall, no algorithm exceeds 80% specificity. the f-measure can indicate a combined performance of the algorithm that is not biased by the amount of training/testing data. in the supplementary material, figure additional file  <dig> we see that the genemark program has the best performances in terms of f-measure for most read lengths. in figures  <dig> and  <dig>  we average over the fragment types to plot sensitivity and specificity vs. read-length, but we also wish to analyze the performance for the different fragment types. in the supplementary material in fig. additional file  <dig> genemark's f-measure for types b and d decreases with length of the fragments, while f-measure of types a and c increases with length of the fragments. similarly, mga shows such a pattern too. however, orphelia's f-measure for types a and c has similar values to types b and d.

in conclusion, we note that mga has the best sensitivity but has the worst specificity for most read lengths. genemark has average sensitivity but outstanding specificity for most read lengths, which gives it a better overall f-measure for most read lengths.

we note that our sensitivity and specificity measures are lower than that reported in  <cit> ,  <cit> , and  <cit> . we have several reasons that this may occur. first, we have a more diverse dataset than previously studied, and we are testing with twice as many genomes as genemark used in their test set and eight times as many as orphelia and mga used. secondly, hoff et al.  <cit>  uses the positive predictive value  ppv=tptp+fp, to benchmark their performance which measures the gene-prediction specificity of correctly predicting just the gene regions and not non-coding regions. in fact, we show that using the ppv, seen in figure  <dig> and fig. additional file  <dig> instead of the traditional definition of specificity, results in high rates like previous papers. finally, unlike genemark that discards hypothetical genes, we also used genbank's hypothetical gene annotations to be as complete as possible. also, previous methods do not describe the size of the training set , so our sampling of ~  <dig> reads per genome per read-length is quite reasonable. also as mentioned, we take  <dig>  reads of different read lengths more than any previous study has endeavored. we would like to point out to the reader that the trends are the highlight of our analysis and that the specific numbers should not be the focal point.

upper-bound analysis
previously, we have analyzed the upper-bound prediction error of the three algorithms  <cit> . in this analysis, we aim to demonstrate that the three prediction programs complement each other, and we aim to show that using combinations of the three may reduce prediction error. in the upper-bound analysis for coding regions, we choose the best prediction out of the three. in other words, if all three programs incorrectly predict the read, this would contribute to the upper-bound prediction error, otherwise if one of the predictors is correct, we mark the read as correct. through this analysis, we can also calculate the upper-bound prediction accuracy, the accuracy if at least one of the predictors was correct. in figure  <dig>  we see that the upper-bound prediction error is 5-25% lower than any single method. in fact, at  <dig> bp, the upper-bound prediction error seems to stabilize at a constant level . therefore, we aim to combine predictors to significantly improve gene prediction performance.

combining the classifiers to improve prediction
first, we aim to analyze the different types of reads, which are fully-coding, partially coding and partially non-coding, and finally non-coding. we can average over the read-lengths to see a trend in the prediction programs for each different fragment type. in figure  <dig>  the single-methods  predict type b  fragments better than the gene edges . also, the fully noncoding fragments , perform the worst. mga performs the best on type b. genemark performs the best on type d, with orphelia slightly behind and mga lagging. for the combined methods, we can see that gm&orph greatly improve the performance on type d fragments, while gm|mga|orph significantly enhances prediction of fragments with gene edges . lastly, the consenus method marginally enhances prediction of all fragment types.

we also investigated how the sensitivities and specificities vary for different fragment types. all programs have relatively good sensitivities for type b fragments. we compare the three programs in the supplementary material, in figures additional file  <dig> additional file  <dig> and additional file  <dig> and show that mga has the highest sensitivities while orphelia has the lowest sensitivities for types a, b and c. on the other hand, genemark has the best specificities among the three programs for all fragment types and read-lengths, while mga has the lowest specificities, shown in the supplementary material in figure additional file  <dig>  in order to mitigate weaknesses of these programs, we implement the boolean logical combinations of them to combine the sensitivity vs. specificity trade-off and the figures also show that the logical combination of gm & orph has the best specificities. the logical combination of these classifiers shows promising results which we further investigate to find the best performance.

we tested all the logical combinations, and we plot the best three for sensitivity, specificity, and accuracy in figures  <dig>   <dig> and  <dig>  figure  <dig> shows that gm|mga|orph boosts sensitivity for the gene prediction. however, this combination has the lowest specificity while gm & orph has the highest specificity at the sacrifice of lowest sensitivity. so, the question remains - what measure should we optimize for? usually, there are more coding regions than non-coding regions, and our dataset reflects this, with 3/ <dig> of the reads containing at least part of a gene fragment. therefore, the accuracy measure takes this into account, and the accuracies for the combined boolean logicals are shown in figure  <dig>  this plot gives us insight that different boolean combinations have different accuracies for the various read lengths. for 100- and 200-bp reads, gm|mga|orph perform the best while the consensus measure performs the best for 300- and 400-bp, and finally, gm & orph performs the best for 500- <dig> bp. we therefore propose that the different combinations should be used for different read lengths.

we have shown that different logical combinations have better sensitivity or specificity, and this provides an advantage for some logical operations to obtain higher accuracy for longer read lengths while other combinations are better for shorter read lengths. to assess which method is best independent of read length, we varied the read length and evaluated the receiver-operating characteristic  curves for each method. we summarize the results of the roc analysis by providing the area-under-the-curve in table  <dig> . out of the three single methods, genemark has the best performance. although, by combining gm & orphelia, this logical combination improves performance over genemark by 8% auc, while providing the best accuracy on longer reads in general.

the area under the curve for each method for the receiver-operatoring characteristic  curves, constructed by varying the read-length. genemark has the best auc for a single-method while gm & orphelia has the best performance for the combination methods, with the consensus combination trailing by a few percentage.

combining the classifiers to improve gene annotation
while we have previously addressed whether a read contains a gene or partial gene, we now assess the boolean logical combinations to annotate the start and stop of the genes using the annotation error metric . if annotated inaccurately, secondary structure will most likely be incorrectly predicted, thus accurate annotation is essential. in figure  <dig>  we find that using the consensus of any two programs to predict gene annotation produces the lowest annotation error relative to each single program. for the single methods, genemark has the best annotation accuracy for short reads and orphelia may be better for long reads. as a sidenote, we found that mga has a tendency of predicting two or more genes on a fragment that consists of one gene.

in figure  <dig>  the consensus combination is further compared to the logical combinations, where the best combinations for annotation error are the intersection of the annotations: mga & genemark, genemark & orphelia, orphelia & mga, and & of all three; the annotation error for the unions, or ors, are shown in the supplementary material, figure additional file  <dig>  we can see that the intersection of all three programs has the best annotation error for mostly  <dig> bp and longer reads. but we note that the intersection of the three programs has a trade-off between good annotation error and poor prediction accuracy . gm&orphelia is the best for  <dig> bp and above reads since its prediction accuracy is the best while maintaining relatively low annotation error. we provide a table for each suggested method and read length in table  <dig>  all data used for the prediction accuracies and annotation errors are provided in additional file  <dig> 

the best method  vs. read length. the method with prediction accuracy/annotation accuracy are in the parentheses. as shown, the consensus gives the best overall performance for  <dig> bp- <dig> bp reads. for single methods, mga has the best performance for  <dig> bp reads while gm is the best single-method for 200- <dig> bp length reads. for longer reads , orphelia and gm&orphelia have the best performance.

discussion
demonstration on a real dataset, the human twin lean gut data
to correctly annotate the start and stop of the genes, we previously found that a combination of all the predictors performs best for  <dig> bp read lengths boosting annotation accuracy by 4%. therefore, we demonstrate the algorithm on first  <dig>  illumina reads, with average read length of  <dig> bp, from the distal gut from a lean human twin  <cit>  seen in figure  <dig> 

for this analysis, we chose to compare against the best classifier combinations to predict coding regions in  <dig> bp reads - the consensus combination and the gm|mga|orph. we can see that the gm|mga|orph combination method produces the highest gene/non-gene ratio . secondly, the consensus method which was shown to have the best annotation accuracy, predicts 79% of the sequences as genes, and orphelia falls between these two predictions with an 84% gene percentage. while the gm|mga|orph finds a similar coding/non-coding ratio found in the typical microbial genome  <cit> , there is not sufficient evidence to show that a typical metagenome will represent this ratio. in the future, we plan to examine the coding/non-coding content of metagenomic samples in varying environments.

to explore the distribution of types found in the twin gut microbiome population, we see figure  <dig>  we see that the gm|mga|orph predicts high amounts of types a, b, and c while predicting a low amount of type d. while we cannot verify these results from real data, we believe that by combining gm|mga|orph, we can predict more of the reads as type b instead of type d, which results in a coding/non-coding ratio that more resembles reality. also, we have previously shown that the consensus combination has the best annotation accuracy for the reads predicted to have coding regions, and this is reflected in figure  <dig>  where the amounts of type a's and c's are almost equivalent using the consensus method .

while this is beyond the scope of this paper, the next step would be to characterize and validate the extra genes discovered using the classifier combination. we propose that these coding regions may have characteristics which make them difficult to identify and may be of potential interest.

CONCLUSIONS
we show that performances of programs, genemark, metageneannotator, and orphelia, vary for different read lengths and fragment types. the different algorithms result in a trade-off of sensitivity vs. specificity and a gradual decline in these rates for shorter reads. genemark's sensitivity and prediction accuracy are lower than those of orphelia and mga, while its average specificities are the highest for most read lengths. this is due to genemark's ability to correctly predict type d fragments as non-coding. also, genemark has the lowest annotation error, meaning it is the best in predicting the start and stops of genes, for short read lengths while orphelia has the lowest annotation error for longer read lengths.

we show that we can improve on these trade-offs by combining the methods' predictions and annotations. in general, the intersection of the methods improves annotation accuracies but at the cost of poor prediction accuracies, while the union of the methods improves predictions accuracies at the cost of poor annotation. we validate the genemark, mga, orphelia, and the best combinations on a human gut sample sequenced by illumina technology, and find that gm|mga|orph and orphelia produce the highest coding/non-coding ratios, though more investigations are needed to determine the gene content of metagenomes. in conclusion, the consensus logical combination, or majority vote, has the best performance  for 100- <dig> bp while gm&orphelia has the best performance for  <dig> bp and longer.

