BACKGROUND
predicting the 3d structure of a protein from its sequence continues to be one of the most challenging, unsolved problems in biology. however, thanks to the development of programs such as rosetta
 <cit>  and i-tasser
 <cit> , along with the exponential growth in computational power, it is now possible to predict the structures of small proteins with a high degree of accuracy
 <cit> . to be maximally effective, most modern ab initio structure prediction programs must generate tens of thousands of candidate structures  and then use specially developed heuristic “energy” functions or structure clustering techniques to evaluate and select the top scoring candidates. as a general rule, the performance of an ab initio structure prediction program critically depends on: 1) the number of candidate structures generated; 2) the variety of candidate structures generated; 3) the quality of candidate structures generated and 4) the performance of the scoring function and candidate selection criteria. our focus here is on improving the performance of the latter.

many studies
 <cit>  indicate that structural clustering can improve the selection of correct or near-correct folds over a simple heuristic energy function. shortle et al. <cit>  were probably the first to study the performance of candidate fold detection using structural clustering. they hypothesized that when a reasonably good energy function is used or when a reasonably good structure generation tool is available, most candidate structures generated by a protein structure prediction program will tend to cluster near  the correct fold. as a result, cluster centers tend to be closer to the correct fold than other decoys within the cluster. because of the significant performance enhancements seen with structural clustering, some of the most successful ab initio structure prediction programs, such as i-tasser and rosetta, now incorporate structural clustering as part of their candidate selection process .

while clustering can be quite useful in both candidate fold selection and correct fold detection, its speed deteriorates rapidly as the size of the decoy set increases
 <cit> . many decoy clustering algorithms, such as spicker , which is used by i-tasser, as well as the clustering algorithm implemented in rosetta, use a similar clustering approach. in particular, they select the decoy with the largest number of neighbors falling within a certain structural similarity threshold that is either given or detected automatically. the top decoy and its neighbors are selected to form the first cluster. afterwards the structures in the first cluster are removed from the pool of decoys and the process is repeated until a sufficient number of clusters are identified. other clustering strategies use hierarchical clustering
 <cit>  or a travelling salesman approach
 <cit>  to aid in structure clustering and candidate selection. all of the above-mentioned strategies require repeated pairwise structure comparisons. therefore, if the size of a decoy set increases  <dig> fold, then the number of required structure comparisons increases roughly  <dig> fold. such a trend can lead to prohibitively poor runtime performance – especially with large decoy sets.

to accelerate the clustering and selection steps on large decoy sets, several methods have been developed in recent years. these include scud
 <cit> , calibur
 <cit>  and durandal
 <cit> . scud
 <cit>  accelerates the clustering process by speeding up the underlying structure comparisons. rather than performing a time-consuming superposition operation in every pair-wise structure comparison, as is done in traditional decoy clustering algorithms, scud superposes every decoy to a reference structure. as a result, scud uses the superposition to the reference structure when comparing two structures. the difference between structures, which is called the rrmsd, is essentially an upper bound for the true rmsd . results showed that while the quality of structures selected by scud did not improve, the clustering speed improved by a factor of  <dig> on a decoy set of  <dig>  structures.

in contrast to scud, calibur
 <cit>  accelerates clustering without changing the structure comparison method. instead it uses a variety of filtering strategies to limit the number of pairwise comparisons. in the first step, calibur removes structural outliers that are not similar to a set of randomly sampled decoys by a certain distance threshold. then, lower and upper bounds derived from the alpha carbon  rmsd properties and the triangle inequalities in metric distance are used to reduce the number of structure comparisons. extensive testing has shown that calibur is faster than both the rosetta clustering program and spicker . this is especially true when the decoy set is larger than  <dig>  structures. the quality of the candidates selected by calibur has been found to be similar to that of spicker.

durandal
 <cit>  combines the lower and upper bounding technique used in scud and calibur with an information entropy technique. for a decoy set of size n, durandal generates a pair-wise distance matrix of size n*/ <dig> and measures the information entropy by counting the number of “undecided” distances in the matrix. a distance in the matrix is “decided“ if it is clearly below or above a given threshold. once the matrix is set up, durandal randomly chooses a set of decoys, computes distances between the decoys, and uses the lower and upper bounds to determine the “undecided” distances so that the entropy in the distance matrix decreases. the algorithm keeps using the lower and upper bounds to decide distances as long as it reduces the entropy in the matrix more efficiently than computing distances. tests show that durandal is faster than both scud and calibur, by a factor of up to  <dig>  on a decoy set of  <dig>  structures. not only is durandal faster, it was also shown to improve upon the quality of folds that could be selected. in particular, on  <dig> large i-tasser decoy sets, durandal selected a more correct fold than the i-tasser energy function  in  <dig> out of  <dig> cases versus just  <dig> for spicker. however, as noted by its authors, durandal is not very efficient with memory usage due to the quadratic size of the distance matrix
 <cit> . a recently updated version of durandal applies a quaternion-based characteristic polynomial method
 <cit>  to accelerate its rmsd calculations. this change has improved durandal’s overall runtime by up to 25%.

with continuous algorithmic improvements over the last decade, many ab initio structure prediction methods are now running significantly faster than earlier methods. for example, i-tasser was reported to take just  <dig> hours to model the structures of certain smaller proteins on a single cpu
 <cit> . presumably this involved the generation of thousands of decoy structures. running multiple copies of i-tasser on multiple cpus  would no doubt reduce the sampling time significantly . however, clustering large decoy sets with existing methods such as durandal can still take at least half an hour -- as shown in later in this paper. furthermore, to the best of our knowledge, currently there is no parallelized version of durandal  available. this suggests that structural clustering is becoming a significant time bottleneck in ab initio structure prediction. clearly the improvement of clustering speed would benefit a number of state-of-the-art structure prediction methods such as i-tasser.

in this work we describe a faster, more efficient and more accurate approach to detect correct protein folds using a technique called partial clustering. given that the ultimate goal of clustering in decoy selection is to return representatives of the decoy clusters, rather the clusters themselves, then we would argue that partial clustering should be sufficient to solve the problem at hand. in particular, we have designed a partial clustering algorithm that does not need to define cluster membership for every decoy but is able to extract key representatives quickly. our method also combines the speed of this fast clustering approach with a more intelligent approach to scoring or ranking candidate folds. in contrast to other quality assessment methods, our method uses the scoring energy to rank a small set of cluster centers instead of the whole decoy set. we have found that our clustering strategy can be applied to any scoring function to enhance the rate of correct fold detection. tests conducted on decoy sets generated by rosetta and i-tasser show that our method is able to select a higher proportion of correct folds than using energy functions alone or using other fast  structure clustering algorithms. speed and efficiency testing also shows that our method is up to  <dig> times faster than durandal , and that it is also significantly more memory-efficient.

methods
our method, called hs-forest, is based on the concept of partial clustering and then “intelligently” combining this partial clustering with energy function evaluation to detect the most correct fold from a given decoy set. the first step in the program involves using a novel structural clustering scheme to detect structurally “dense” regions in the given decoy set. these regions are thought to be local minima in the protein folding energy landscape according to the hypothesis that most candidate structures generated by a protein structure prediction program will tend to cluster near the correct fold if the structure generation program is reasonably good
 <cit> . in the clustering stage, we extract a representative for each cluster without assigning every object to a cluster. we then compute the distances between the extracted representatives and a small number of lowest energy decoys. unless otherwise specified, in this paper we measure the distance between two decoys using the cα rmsd distance . however, it is important to note that our method is applicable to both metric and non-metric distance functions. once this extracted representative distance calculation is done, we then select a representative with the smallest total distance to the lowest energy decoys as a candidate for the best decoy. in the clustering stage we introduce a random factor, described later, so that each clustering process will typically generate a different optimal candidate. we run the clustering process multiple times to generate a set of candidates. from this set of candidates, we return a consensus decoy that has the smallest total distance to all other candidates. the algorithm is outlined step-by-step below:

 <dig>  select a given number of pivots randomly;

 <dig>  for each pivot create a hashing function;

 <dig>  build the root node of a tree, which is the first leaf of that tree;

 <dig>  randomly select a hashing function to split the leaves of the tree;

 <dig>  go to step  <dig> until the tree reaches a certain height as described below;

 <dig>  determine the cluster nodes;

 <dig>  for each of the largest cluster nodes, select a representative;

 <dig>  rank the representatives by their total distances to the top energy decoys. the top one is the candidate of the tree;

 <dig>  go to step  <dig> until a given number of trees are constructed;

 <dig>  return the consensus of the candidates as the best decoy.

to efficiently detect locally “dense” regions in a given decoy set we decided to use a recently developed database searching technique called local sensitive hashing <cit> , or lsh. the idea of lsh is to design a set of hashing functions so that similar objects have a probability  of being hashed to the same bin by each hashing function. even though every individual hashing function is not perfect and can hash dissimilar objects to the same bin, by combining multiple hashing functions we can still manage to group similar objects together.

our design of the hashing function is based on a well-known metric property
 <cit>  that two similar decoys usually have similar distances  to a third decoy. first, our algorithm randomly selects a small set of decoys from the given decoy set as pivots and computes the distances between these pivots and all decoys in the decoy set. then, for each pivot our algorithm divides all the decoys into two bins, depending on whether the distance between the pivot and the decoy is less than the median of all distances to the pivot. similar decoys are likely to have similar distances to the pivot and thus have a higher probability to be placed in the same bin as opposed to different bins. from each pivot, our algorithm constructs a hashing function. we use the median distance to divide the decoys because it makes the bin size of the hashing function more balanced.

to organize the hashing functions, our algorithm constructs a tree, denoted as an hs-tree. “hs” in hs-tree and hs-forest stands for “height and size”. these two letters come from the two factors behind our clustering algorithm, both of which are used to define the clusters. starting from the whole decoy set, our algorithm splits the decoys into two nodes by applying a randomly chosen hashing function, with each bin of the hashing function corresponding to a node. each of the two nodes is further split by another randomly chosen hashing function, i.e., each layer of the tree is split by a randomly chosen hashing function. this process is repeated until a certain height to the tree is reached. at the beginning of the process, the nodes in the tree can contain very dissimilar decoys, but as the nodes are split further down the tree, dissimilar decoys are more likely to be separated into different nodes while similar decoys are more likely to remain together thereby representing a “denser” region.

the next step is to decide which nodes contain a dense region, otherwise known as a cluster. ideally, a cluster is a region balanced with density and size . since the density of a node is indicated by its height in an hs-tree, the ideal clusters are in nodes balanced by both height and size. we define such a node as a cluster node as given below.

definition  <dig> . in an hs-tree t of height h  and size z , let p be a path in t from the root to a leaf node, a cluster node in p is a node v of height h and size z so that

  {hvh≥logzvlogzv=argminv∈phv 

intuitively, as shown in figure
 <dig>  a cluster node is a node near the line y=x in a 2d plot with y= log)/log, and x=h/h. we apply a log scaling function to normalize the node size since the node size in an hs-tree usually decreases in an exponential manner when traveling from the root to the leaves. each path in an hs-tree from the root to a leaf can contain at most one cluster node. note that in our algorithm, not every decoy is necessarily assigned to a cluster node. on the other hand, each hs-tree must contain at least one cluster node, as guaranteed by the lemma below.

lemma  <dig>  in an hs-tree t of height h  and size z , let c be the number of cluster nodes in t, then c ≥  <dig> 

proof synopsis: since the first hashing function splits the decoy set in half by the median distance with each half close to z/ <dig> and no larger than z*2/ <dig> , for any node of height h=h and size z, log/log ≤ log/log. since log/log = 1-log/log, log/log < 1– <dig> /log <  <dig>  given that h/h = h/h =  <dig>  log/log <h/h, and equation  is satisfied. therefore, c ≠  <dig> 

having extracted the clusters from the non-redundant set of cluster nodes, the next step is to extract a representative from each cluster. at this stage, the challenge is two-fold:  even though each cluster is likely to consist of decoys structurally similar to each other, it is possible that some dissimilar decoys are mixed into the cluster; and  the size of clusters can vary significantly so that the traditional approach of computing a medoid with the minimal total distance to the decoys within a cluster can involve a quadratic number of distance computations. in our algorithm, we select a representative of a cluster by choosing the decoy with the smallest total distance to the set of pivots we picked when constructing the hashing functions. no additional distance computation is needed.

to select the best decoy from the pool of cluster representatives, we borrow an idea used by
 <cit> , which ranks every decoy in a decoy set by its total distance to the lowest energy decoys. in our algorithm, for given numbers s  and e , we select the s largest clusters and rank their representatives by the total distance to the e lowest energy decoys . the additional distance computation is minimal when s and e are small. the highest ranked representative is a candidate for the best decoy in the decoy set.

due to the random factor we introduced when constructing our hs-tree, different hs-trees can return different candidates for the best decoy. to compute a consensus, our algorithm generates multiple hs-trees. first a set of p pivots are randomly selected and p corresponding hashing functions are created. then, hmax hashing functions are randomly chosen to build an hs-tree, so that the maximal height of the tree is hmax. this process is repeated until a given number  of hs-trees are constructed. finally, our algorithm ranks the candidates from all the trees by the total distance to all the candidates, and returns the highest ranked candidate.

in our implementation, when constructing hs-trees and extracting cluster nodes, we use hmax to replace the actual tree height h. in large decoy sets, these two values are usually the same. the advantage of using this approximation is that we can construct only a portion of the whole tree and stop splitting whenever the inequality  in definition  <dig> is satisfied. this significantly reduces the runtime. the disadvantage is that, theoretically, we can no longer guarantee that the returned result is not empty. however, this appears to be a rare event as an empty result was never returned in our experiments with  <dig> different decoy sets consisting of between  <dig>  to  <dig>  structures. in the worst case, if an empty result is returned, the user can simply reduce hmax and re-run the program.

RESULTS
we tested our method, hs-forest, on a large collection of rosetta and i-tasser decoys. the rosetta decoys consist of  <dig> decoy sets for different protein targets with a diverse size range from  <dig>  to  <dig>  structures. the i-tasser decoys consist of  <dig> large decoy sets
 <cit>  with varied size between  <dig>  and  <dig>  structures, and were used to test durandal
 <cit> . both the rosetta and i-tasser decoys were generated by a combination of ab initio and fragment assembly methods, and each decoy set contained at least one decoy having a cα rmsd below  <dig> angstroms relative to the native  structure. the exact sizes of the rosetta and i-tasser decoy sets are shown in additional files
 <dig> and
 <dig>  respectively.

speed and efficiency
to evaluate the speed and efficiency of hs-forest, we tested it on both the rosetta and i-tasser decoy sets. given that the results presented by
 <cit>  clearly showed that durandal was faster than scud and calibur, in our experiments we only compared hs-forest with durandal  and calibur-lite
 <cit> . calibur-lite is a simpler, faster program than calibur that only outputs the best decoy. for our tests, durandal was also set to output the best cluster only, using a value of  <dig>  for the semiautomatic threshold detection as suggested by its authors
 <cit> . all reported runtime values are measured by the linux program “time” and averaged over  <dig> runs on a dual quad-core amd opteron  <dig> linux sever with  <dig> gb of ram using only a single core.

the first test set we investigated was the 1shfa set from the i-tasser decoys. we varied the decoy set size from  <dig>  to  <dig>  by randomly sampling a portion of the entire decoy set. the runtimes for the three methods are shown in figure
 <dig>  as seen in this figure, hs-forest when using the i-tasser energy function significantly outperforms both durandal and calibur-lite. furthermore the speed gap increases as the size of the testing set increases. using the full set of  <dig>  decoys, hs-forest achieves a speedup of  <dig> over both durandal and calibur-lite.

the second test set we studied was the 1bm <dig> set from the rosetta decoys. this decoy set contains  <dig>  decoys and poses a significant challenge to most clustering algorithms. similar to the first experiment, we varied the decoy set size from  <dig>  to  <dig> . the runtime for all three methods are shown in figure
 <dig>  durandal runs out of memory once the number of decoys exceeds  <dig> , while calibur-lite takes more than  <dig> hours to finish a run as soon as the size exceeds  <dig>  decoys. hs-forest using the rosetta energy function finishes all calculations and outperforms durandal and calibur-lite consistently, requiring just  <dig> seconds to analyze  <dig>  decoys. the speed-up factors for hs-forest over durandal and calibur-lite are  <dig> and  <dig> respectively.

figures
 <dig> and
 <dig> also show that the performance of a given clustering method can be quite different for different decoy sets. for instance, we can see that calibur-lite has a much slower runtime than durandal in figure
 <dig> but its runtime is about the same as durandal in figure
 <dig>  to compare the performance on diverse decoy sets of varying size, we compared the runtime for all three methods on an independent collection of  <dig> rosetta decoy sets . these decoy sets are smaller than the 1shfa and 1bm <dig> sets, but have a size range from  <dig>  to  <dig>  decoys. hs-forest had the shortest runtime among the three methods for all  <dig> sets, achieving a speedup factor of  <dig>  to  <dig>  over durandal, and  <dig>  to  <dig>  over calibur-lite. the detailed results can be found in additional file
 <dig> 

hs-forest is also very memory efficient. while durandal ran out of memory when testing the 1bm <dig> set from the rosetta decoy set on our testing machine with  <dig> gb ram, hs-forest was able to finish analyzing all the decoys in this set with the maximal memory usage of around  <dig> gb.

the computational time complexity for hs-forest can be calculated as follows: let the number of decoys be n, then the computational time complexity to compute the p hashing tables is o. the complexity to construct an hs-tree is o. therefore the total time complexity of hs-forest is onlogn) where t is the number of trees.

performance for correct fold detection
in this section, we will show that hs-forest is able to improve the performance of energy functions in correct fold detection. we will also compare hs-forest with two state-of-the-art decoy clustering programs: durandal and calibur-lite. since both rosetta and i-tasser have their own clustering programs and since these clustering programs might have an advantage when tested on their own decoy sets, we also included these two clustering programs in our comparison. the results show that hs-forest exhibits better performance than all the above-mentioned methods.

measuring the performance for correct fold detection is a challenging task because the same method usually performs differently on different decoy sets. for example, a method can outperform a standard energy function significantly in one decoy set but consistently fail to outperform the same energy function in other decoy sets. to measure performance we adopted two different criteria. for the first criterion, denoted as criterion- <dig>  the percentage of decoys that the top-scoring decoy outperformed is averaged over all decoy sets is used as the metric. criterion- <dig> measures, on average, how close the selected decoy is to the native  structure compared with the other decoys in the decoy set. the second criterion, denoted as criterion- <dig>  was adopted from
 <cit> . it essentially measures how frequently a given method can outperform the energy function in different decoy sets. specifically it measures the frequency with which the top decoy selected by a given method has a lower cα rmsd than the top decoy selected by the energy function alone. since both hs-forest and durandal contain random seeding functions, unless otherwise specified, we averaged their results over  <dig> runs.

for our experiments on the rosetta decoys, we applied hs-forest to enhance the decoy selection using different energy functions, including the rosetta energy values , the gafolder pseudo-energy values
 <cit> , and a consensus score  that ranks decoys by the total cα rmsd distance to a number of reference structures. to calculate the consensus score, we chose the same number of reference structures as the number of pivots used in hs-forest. when hs-forest was combined with an energy function, we use that function to select the  <dig> lowest energy decoys in hs-forest. the results are shown in table
 <dig>  under the criterion- <dig> column, we can see that hs-forest is able to enhance the decoy selection performance for all three energy functions with a percentage difference of  <dig> %  for the rosetta energy, and a percentage difference of  <dig> %  for the gafolder energy. since hs-forest already used the consensus method to select a representative for each cluster, we would expect the hs-forest results would be similar to that of the consensus method alone. nonetheless, hs-forest still outperformed the consensus result by a percentage difference of  <dig> % . under the criterion- <dig> column, we compare how frequently different methods can perform against the baseline rosetta energy function. on average, when the rosetta energy is used in hs-forest it outperforms the rosetta energy, alone, in  <dig>  out of the  <dig> decoy sets . the gafolder and consensus energy functions outperformed the rosetta energy in just  <dig> and  <dig> out of  <dig> cases respectively. however, after applying the hs-forest algorithm to the same energy functions, the results improved to  <dig> and  <dig> out of  <dig> cases, respectively.

criterion- <dig> and criterion- <dig> are defined in the manuscript. criterion- <dig> measures the percentage of decoys that the top-scoring decoy outperformed; criterion- <dig> measures the frequency with which the top decoy, selected by a given method, has a lower cα rmsd than the one selected by the energy alone. the values behind ± are the standard deviations among different runs.

on the rosetta decoys, we also compared hs-forest with durandal, calibur-lite, and the clustering program used in the recently released rosetta  <dig> . when testing on the largest decoy set, 1bm <dig>  durandal ran out of memory and calibur-lite took more than  <dig> hours to finish. consequently, we used the largest sample that durandal and calibur-lite could generate on the 1bm <dig> set to calculate its performance for this particular decoy set . the average result for durandal using criterion- <dig> is  <dig> %, which is better than all of the energy-only methods but is still  <dig> % less than what rosetta energy function combined with hs-forest achieved. for criterion- <dig>  the average result for durandal was  <dig>  out of  <dig> decoy sets  versus  <dig>   for rosetta with hs-forest. this result for durandal is somewhat worse than what hs-forest achieved even when it used the gafolder and consensus energy functions. similar to durandal, the performance of calibur-lite and the rosetta clustering program in table
 <dig> are worse than what was achieved when the rosetta energy function was combined with hs-forest.

the results on the i-tasser decoy sets are shown in table
 <dig>  the energy functions we tested include the i-tasser values that come with the decoys, the gafolder energy and the consensus score. using criterion- <dig>  similar to the results on the rosetta decoys, hs-forest improves the decoy selection for all three energy functions. under criterion- <dig>  we used the i-tasser energy alone as the baseline method. as seen in table
 <dig>  hs-forest methods consistently outperform the energy-only methods, with an improvement of  <dig>  cases for gafolder and  <dig>  cases for the consensus method. on this data set we also compared hs-forest with durandal, calibur-lite, and spicker  <dig>  . the performance of durandal for criterion- <dig> and criterion- <dig> is  <dig> % and  <dig> / <dig> respectively, which is worse than what was obtained using gafolder with hs-forest . the performance of calibur-lite for criterion- <dig> and criterion- <dig> is  <dig> % and  <dig> / <dig> respectively, which is slightly worse than those of gafolder with hs-forest. for spicker  <dig> , we used the clustering results downloaded from its website to calculate the scores. more specifically, we used the so-called “closc” decoys that come with the decoy sets to calculate the criterion- <dig> and criterion- <dig> scores. a “closc” decoy is the decoy closest to the first spicker cluster centroid. it has been shown that these “closc” decoys have a lower rmsd to the native structure than the spicker average structure after removing most steric clashes
 <cit> . the results shown in table
 <dig> also indicate that spicker  <dig>  exhibits a poorer performance than gafolder with hs-forest using both criteria.

criterion- <dig> measures the percentage of decoys that the top scoring decoy outperformed; criterion- <dig> measures the frequency with which the top decoy selected by a given method has a lower cα rmsd than the one selected based on energy alone. the values behind ± are the standard deviations among different runs.

because the performance of hs-forest on the i-tasser data appears to be modestly better than the performance of durandal and calibur-lite, we performed an unpaired student’s t-test to assess the statistical significance
 <cit>  of this performance difference. the choice of a student’s t-test was based on the fact that the criterion- <dig> and criterion- <dig> scores exhibited a normal  distribution over the  <dig> sample runs performed with each program  and the fact we were interested in determining whether the average criterion scores were statistically different. for durandal the t-test p-values are  <dig> x10- <dig> and  <dig> x10- <dig> for criterion- <dig> and criterion- <dig> respectively; for calibur-lite the p-values are  <dig> x10- <dig> and  <dig> x10- <dig> for criterion- <dig> and criterion- <dig> respectively. so using the standard p< <dig>  criterion for statistical significance these differences are statistically significant. we also calculated the standard deviation on the cα rmsd values, relative to the native structure, among different runs for each decoy set. these are found in additional files
 <dig> and
 <dig>  the average standard deviation is  <dig>  and  <dig>  angstroms for the rosetta and i-tasser data sets, respectively.

we also tested hs-forest using another distance metric known as the gdt-ts distance
 <cit>  as implemented in tmscore
 <cit> . since there is no trivial way to run durandal and calibur-lite using a distance metric other than cα rmsd, we calculated the gdt-ts scores using tmscore for the top decoys selected by durandal and calibur-lite. the following results were averaged over  <dig> runs. on the rosetta decoys, the average performance of rosetta energy with hs-forest for criterion- <dig> and criterion- <dig> was  <dig> % and  <dig> / <dig> respectively. these values are better than the performance achieved with durandal  and calibur-lite . on the i-tasser decoys, the average performance using the gafolder energy with hsforest for criterion- <dig> and criterion- <dig> were  <dig> % and  <dig> / <dig> respectively, which is also better than that found for durandal  and calibur-lite .

discussion
parameter settings
hs-forest contains several parameters that can be adjusted, including the number of pivots p, the maximal tree height hmax, the number of trees t, the number of largest clusters to consider s, and the number of lowest energy decoys e. in our experiments, to reduce the number of parameters while at the same time maintaining randomness among the trees, we always set hmax = p/ <dig>  this is done so that different trees have partially different sets of hashing functions to generate different candidates for the final consensus stage. while increasing p, t, s or e generally increases the runtime, how each affects the program’s performance is not particularly obvious. to study the impact of these parameters, we tried different values for the  <dig> parameters on the i-tasser decoys using the i-tasser energy function with hs-forest. unless otherwise specified, for all the experiments we set p =  <dig>  t =  <dig>  s =  <dig>  and e =  <dig>  overall, the results show that the performance of hs-forest is not particularly sensitive to these parameters.

the value of p is probably the most important parameter in hs-forest. figure
 <dig> shows the criterion- <dig> performance  when p is varied from  <dig> to  <dig>  at the lowest p of  <dig>  hs-forest is least accurate. as p increases, the performance also increases and reaches a maximal value when p =  <dig>  afterwards, the performance declines slowly. we conjecture that the reason for the slow decline is that when p  values are too large, it makes the inequality  in definition  <dig> less optimal to extract clusters. based on these data we recommend a p value between  <dig> and  <dig> 

figure
 <dig> shows the criterion- <dig> performance  with the number of trees  varied from  <dig> to  <dig>  this graph shows that when t is smaller than  <dig>  the performance is suboptimal, but for t ≥  <dig>  the result is stable. this result also indicates that the final step of hs-forest, which involves computing the consensus of the results from multiple trees, does improve the performance. in other words, the consensus of multiple trees  gives a better performance than a single tree. figures
 <dig> and
 <dig> show how the criterion- <dig> performance  varies with respect to the numbers of largest clusters s and lowest energy decoys e. for s >  <dig>  the performance levels off; and e does not have a significant impact on the performance.

for all the experiments presented in the results section, we set p =  <dig>  t =  <dig>  s =  <dig>  and e =  <dig> on both the rosetta and i-tasser data. the parameters were selected based on “training” with the i-tasser data using the cα rmsd distance, and then tested on the leave-out rosetta data set. as seen in the last section the performance of hs-forest remained superior even on the leave-out data. to further validate this result we also tested the same parameters on i-tasser and rosetta data using the gdt-ts distance, which served as another leave-out testing set. again, our results show that hs-forest’s performance was not compromised. the superior performance of hs-forest on all leave-out sets, as well as the data shown in figures
 <dig> 
 <dig> 
 <dig> 
 <dig> shows that these parameter settings are quite robust and certainly general enough to be applied on other decoy sets using different similarity metrics.

the role of clustering
our method combines the power of partial clustering with the use of energy functions to help detect correct folds. it is interesting to see how clustering contributes to the improved performance of hs-forest. to explore this further, we tested the rosetta decoys by simply ranking the decoys by the total cα rmsd distance to the  <dig> lowest energy decoys. when using the  <dig> lowest rosetta energy decoys, the performance is  <dig> % for criterion- <dig>  compared with the results in table
 <dig>  this criterion- <dig> performance is only  <dig> % better than what is obtained when using the rosetta energy only, while it is  <dig> % lower than that of rosetta with hs-forest. a similar result is obtained when using the  <dig> lowest gafolder energy decoys, with a criterion- <dig> performance just  <dig> % better than that obtained using the gafolder energy alone and  <dig> % lower than that of gafolder with hs-forest. these results show that without partial clustering, the idea of using the  <dig> lowest energy decoys as reference structures to rank decoys is only slightly better than using the energy functions alone.

essentially, the clustering step in hs-forest narrows the collection of decoy candidates to smaller set that are close to the cluster centers. as hypothesized by
 <cit> , these structures are more likely to be closer to the correct fold. while the energy/consensus ranking in hs-forest is not perfect, we conjecture that the initial focus on selecting good candidates helps improve the criterion- <dig> performance mentioned above. another factor that may contribute to the performance improvement seen in hs-forest is the use of a consensus of multiple random trees, as shown in figure
 <dig> 

on the other hand, clustering on its own may also have problems discerning the correct fold in certain situations, particularly when there are multiple clusters of nearly equal size. in these cases, choosing the wrong cluster could lead to a very poor result. this is where other information, such as the energy score ranking, can help. it is for these reasons that hs-forest combines both structure clustering and energy score information to detect correct folds.

improvements to energy functions
our results on the rosetta and i-tasser decoys show that hs-forest is more effective at detecting correct folds for certain energy functions than for others. as seen in table
 <dig>  gafolder with hs-forest has the largest performance enhancement over its energy-only method. a similar trend is observed in the i-tasser results presented in table
 <dig>  when using energy-only methods, the criterion- <dig> performance of gafolder is somewhat worse than those of the i-tasser energy and consensus score on the i-tasser decoys. however, after applying hs-forest, the criterion- <dig> performance of gafolder is improved by  <dig> % and it actually performs the best among all methods. this improvement can be explained by inspecting figure
 <dig>  as seen here, the energy-only method of gafolder chooses particularly poor quality decoys for two decoy sets: 1hbka and 2cr7a. these mistakes were corrected after applying hs-forest so that the cα rmsd to the native structure drops from  <dig>  to  <dig>  Å and from  <dig>  to  <dig>  Å respectively.

the significant improvements seen with gafolder indicates that our hs-forest concept may open the door for computational biologists to reconsider using some previously discarded heuristic energy functions in structure prediction and refinement. in some cases the inferior overall performance for some energy functions may be due to a very poor performance on a small subset of proteins. such weaknesses could be corrected by using hs-forest as part of the energy function or as part of the evaluation criteria.

this result also illustrates the advantage of hs-forest over traditional clustering algorithms in that it is able to combine different energy functions and adapt to different types of decoy sets. decoys generated by different structure prediction programs can often have very different properties. therefore it is important to be able to use different energy functions to detect the most correct folds. while traditional clustering algorithms only rely on structure comparisons within a given decoy set to select correct folds, hs-forest combines efficient structure clustering with different energy functions to generate better decoy selections.

performance on small decoy sets
to test the performance of hs-forest on small decoy sets, we downloaded just such a set located on the i-tasser website
 <cit> . this is a non-redundant subset of the original i-tasser decoys that were structurally refined via gromacs  <dig> . these very small decoy sets vary in size from just  <dig> to  <dig> structures. we ran hs-forest on this set and compared the results with the spicker results that come with the data set. in order to be able to compare the scores on the whole i-tasser data set as well, we used the values of the whole set as a reference when calculating the criterion- <dig> and criterion- <dig> scores. it turns out that the spicker results on this new data set are actually worse than the spicker results obtained on the whole set in table
 <dig>  with a criterion- <dig> score of  <dig> % and a criterion- <dig> score of 20/ <dig>  this indicates that clustering over a reduced and/or refined subset might not necessarily have an advantage over clustering on the whole decoy set. as might be expected, hs-forest shows no advantage over spicker on these very small decoy sets. this is because hs-forest is designed to run on much larger decoy sets. nonetheless, hs-forest still enhances the performance of the gafolder energy on this new data set. the criterion- <dig> and criterion- <dig> scores for hs-forest using the gafolder energy are  <dig> % and  <dig> / <dig> respectively, which are better than those of using the gafolder energy only . similar to the case with spicker, the scores of hs-forest and gafolder energy  are also worse than those of the whole data set.

drawbacks of hs-forest and decoy clustering programs
hs-forest is not without some limitations. first of all, hs-forest may not work particularly well with poor quality decoy sets. following the previous work of durandal, we limited our testing data to those decoy sets with at least one decoy being less than  <dig> Å cα rmsd away from the native structure. to see how well hs-forest would perform on decoy sets that do not satisfy this condition, we tested hs-forest on  <dig> i-tasser decoy sets from
 <cit> . these decoy sets were left out from our experiments in the results section because they do not satisfy the above-mentioned rmsd condition. the criterion- <dig> score of hs-forest with the gafolder energy on this data set is  <dig> %, which is somewhat higher than those of gafolder energy alone  and i-tasser energy alone . for all the three methods, their scores were found to be much worse than the results in table
 <dig> for the  <dig> i-tasser decoy sets. the significantly lower criterion- <dig> performance for all three methods on the  <dig> poor quality decoy sets indicates that not just the poor quality sets will have overall poorer quality decoys, but the decoy selection methods are also less effective on such decoy sets , making it harder to return useful models from these decoy sets than from good quality decoy sets.

a second shortcoming with hs-forest is that the use of a random factor leads to a non-deterministic output. the speed gains with hs-forest rely primarily on randomized hashing and the fact that it does not perform a full clustering. these changes make its clustering results a little less stable than other clustering methods. to reduce this effect, hs-forest creates multiple trees and computes the consensus. as we can see in tables
 <dig> and
 <dig>  its standard deviations on criterion- <dig> score are close to those of durandal.

CONCLUSIONS
in this work, we have proposed a novel partial clustering scheme for decoy selection in protein structure prediction. this method, called hs-forest, avoids the computationally expensive task of clustering every decoy, yet still allows superior correct-fold selection. the basic idea behind hs-forest is to take advantage of local sensitive hashing and the generation of multiple, independent trees to create a consensus result. our method is able to adapt to different decoy sets by utilizing decoy-specific energy functions to detect correct protein folds. extensive tests on both rosetta and i-tasser decoy sets show that our method is up to  <dig> times faster than two recently published clustering/decoy selection methods durandal and calibur-lite. our method also achieves better accuracy using both cα rmsd and gdt-ts distance metrics for two different decoy sets.

while no clustering method or scoring function has yet been developed that can consistently identify the most correct structure among large decoy sets, we believe hs-forest is a step in the right direction. we hope this idea can inspire the development of even better methods for correct fold detection, and that this concept of partial clustering may be seen to have applications to other scientific fields facing similar clustering challenges.

availability and requirements
project name: hs-forest

project homepage:
http://webdocs.cs.ualberta.ca/~jianjun/hsforest/

operating system: tested on linux.

programming language: c++.

other requirements: none.

license: gnu general public license

competing interests
the authors declare that they have no competing interests.

authors’ contributions
all authors jointly developed the methods and wrote the article. they read and approved the final manuscript.

supplementary material
additional file 1
details of the rosetta decoy sets. the size of each decoy set is larger than  <dig>  and each decoy has the same length as the native structure in pdb.

click here for file

 additional file 2
details of the i-tasser decoy sets.

click here for file

 acknowledgements
we are very grateful to dr. david baker and his group for providing the rosetta decoy sets and for their helpful suggestions. hs-forest uses portions of the cα rmsd and gdt-ts code implemented in durandal, calibur and tmscore.

funding
this work is supported by the alberta prion research institute  and alberta innovates biosolutions.
