BACKGROUND
next-generation high-throughput sequencing  holds the promise of revolutionizing the field of biological research  <cit> . by producing millions of short reads  per run at a moderate cost, these new sequencing platforms move whole genome sequencing from large centers to individual scientists. to name a few, the list of applications includes gene expression analysis, mutation mapping, non-coding rna discovery, metagenomics, and protein binding site identification  <cit> . from bioinformatics point of view, there are essentially two types of problems: short read alignment or mapping and de novo assembly . de novo assembly of short reads into larger dna contigs/scaffolds has proven a bioinformatics challenge both in terms of algorithmic and computational power  <cit> .

over the past few years, several algorithms have been developed for assembly of short reads. these algorithms can be divided into two broad categories. some methods, based on 3'kmer extension, use particular data structures to efficiently search for short reads extending a seed sequence  <cit> . in contrast, the graph-based methods pose the sequence assembly as a problem of finding paths on a graph that encodes the short read overlap information  <cit> .

mate pair and paired-end sequencing represent key innovations in short read sequencing that enabled assembly of short contigs into larger scaffolds. mate pair sequencing was a key innovation that allowed shotgun sequencing of large complex genomes such as humans and drosophila  <cit> . mate pair libraries are generated by enzymatically isolating the ends of a long  dna molecule. these ends are sequenced in the same direction. in contrast, paired-end sequencing involves sequencing the ends of a smaller  dna fragment from both ends in the opposite direction. in this paper, unless we are explicitly contrasting the two methods, we will use the term mate pair to refer to both these technologies.

the current version of some of the above-mentioned short read assemblers can handle mate pair information. however, the use of this information was not central to the concepts that led to the design of most of these algorithms. the sole exception is the allpaths assembler  <cit> , where the use of mate pairs is essential. from a practical point of view, one drawback of allpaths is that it requires at least two paired libraries, with very different insert sizes. also, the performance of this assembler degrades rapidly as the coefficient of variation of insert size in a library increases past a few percent  <cit> . this sensitivity is a problem for assembly of real sequence data, as we will see. in the context of previous generations of sequencing technologies with longer reads, the incorporation of mate pair information has also been addressed, either in conjunction with contig assembly  <cit>  or as a scaffolding module  <cit> .

generally speaking, current scaffolding algorithms fall into two categories. prominent de bruijn graph based contig building algorithms  utilize mate pairs to improve the path/walk in the same de bruijn graph. the other category of scaffolding algorithms  <cit> , formulate the problem in terms of graph theoretic constructs in which vertices of the graph are associated to contigs and edges encode mate pair information. although our approach to the scaffolding problem has partial similarity to this last category, our solution strategy is different, as we will explain. our algorithm could be implemented, in principle, for any kind of mate pairs, from sanger reads to the hts data. however, the special challenges inherent in scaffolding with short read data necessitate an approach that is more sophisticated than those developed so far. that is why we implemented and tested sopra in the context of short reads from next-generation technologies.

existence of repetitive regions in dna, errors in the sequencing process and mis-assembly of short reads into contigs are all factors which contribute to the complexity of scaffold building using mate pair information. this complexity arises in the form of apparent inconsistency among the set of constraints laid by the mate pairs. detecting and eliminating the sources of these inconsistencies is essential for the success of any algorithm dealing with mate pair data. this issue is especially important in the context of short read data, since, we expect a higher number of problematic mate pair constraints in the process of scaffold building.

existing scaffolding algorithms follow a greedy approach, starting with certain schemes of ordering the contigs and pairing information. the mate pairs are then iteratively incorporated as long as the new information does not conflict with the previously assembled scaffolds. in other words, at each step, only a subset of contigs and links in between are considered to improve the assembly. given the nature of short read data, solution strategies employed in previous studies face difficulties for such data  <cit> .

in this paper, we present sopra , a new tool for de novo assembly of short reads produced by new sequencing platforms. the design of sopra is especially targeted to exploit the mate pair information in the process of scaffold assembly. in other words, sopra is a module that can be combined with any of the available algorithms for contig assembly. such a modular design allows greater flexibility and control over the scaffold building process, as has been noted before  <cit> . sopra proceeds in an iterative fashion where at each step problematic mate pair constraints are detected and removed. at each step, one finds a solution consistent with most of the constraints by statistically optimizing over a cost function. then, one relaxes the most violated constraints. this alternation between removing suspicious data and optimization continues, till we get scaffolds consistent with the remaining trusted constraints.

among the available de novo assemblers, as far as we are aware, velvet  <cit>  is the only one that can handle color-space data. adapting available assemblers for color-space data is not a trivial task, since, naive translation from color-space to base-space leads to serious error amplification  <cit> . particular attention was paid so that sopra could handle data from the solid platform. the final output, given in base-space, is constructed from the color-space assembly, as well as from additional information obtained by translating only the first color call of all the reads. this method will prevent the propagation of the error that can happen in the naive translation. sopra is available freely, under the gnu public license, at http://www.physics.rutgers.edu/~anirvans/sopra/

RESULTS
the flow chart of the assembly process is shown in figure  <dig>  below, we will explain each section in more details.

contig assembly preliminaries
as we mentioned, sopra is focused on scaffold assembly. the information sopra needs from a contig assembler is the computed positions of reads in each contig. sopra reconstructs the contigs based on this information. note that, in the case where these reads do not show perfect overlap, reconstruction of the contigs by sopra may not agree with the output of the original contig assembler.

in this paper, we present the performance of sopra integrated with two particular contig assembly algorithms, namely, ssake  <cit>  and velvet  <cit> . we will refer to these two versions as s-sopra and v-sopra, respectively. this integration is relatively straightforward and described in the methods section. however, for color-space data, there is one additional step of translating the assembled contigs to base-space.

robust translation of contigs assembled in color-space
solid  is a novel hts platform. it uses four fluorescent color probes  for reading dinucleotides, namely, two neighboring bases at a time. the sixteen possible dinucleotide combinations are divided into groups of four, each of which is assigned a unique color . however, the groups are designed in such a way that, every combination of the first base and the color call uniquely determines the second base. in other words, each color encodes a transition matrix in the base-space.

each solid read starts with a reference base, the last base in the primer , followed by a certain number of color calls e.g. g <dig> .. <dig>  using the reference base and the first color call, we can find the first letter base, which in turn can be combined with the second color call to obtain the second letter base. continuing so forth, we can translate the whole sequence from color-space to the conventional base-space. the issue is if one of the color calls is wrong , the whole translation from that point on will be wrong. in other words, one error in the color-space will propagate into many errors in base-space. it is because of this error rate magnification that we do not simply translate the solid output directly to the base-space. instead, sopra translates the resulting color-space assembly using a dynamic programming method that avoids such error propagation, as we will explain below.

we only translate the first color call  to the base-space but keep the rest of the sequence in color-space. this means a library of sequences, each of which consists of a reference base and l color calls, will become a library of sequences that start with a dna base followed by l -  <dig> color calls. if we ignore for a moment the first dna base, we can use the l -  <dig> base long sequences for contig assembly in the same way as in regular base-space data. of course, the result of this assembly will be contigs in the color-space. although we do not use the first letter base of the sequences in the assembly process, once a sequence is used in building a contig, we record where on the contig the first letter base of the corresponding sequence lies . notice that the first letter base lies between two color calls and serves as a suggestion for what the dna base at that position should be. on the other hand, each color call is located between two neighboring dna bases and provides information about the corresponding dinucleotide.

at this point, the assembly result is a sequence in color-space, c, plus some letter base suggestions at certain locations of each contig, f. in figure  <dig>  the color-space contig is represented using blue numbers 0- <dig>  whereas, base-space suggestions are shown in magenta. now, we pose the following question: given a color-space sequence plus its letter base suggestions, what is the most likely dna sequence which gave rise to this data? we will set up a model that allows for mistakes in the base suggestions as well as in the assembled color-space contigs. to each arbitrary base-space sequence, the model assigns a probability for that sequence to be the real dna sequence. the final translation output would be the base-space sequence that maximizes this probability.

the reason why this method prevents propagation of error can be intuitively understood as follows. if the presence of a color call error is ignored, the naïve translation will disagree with most of the base-space suggestions. if this disagreement goes on for a long stretch, from the perspective of the probability function, it is better to declare that particular position to be a color call error and replace it with another color such that the translation becomes consistent with the stretch of base-space suggestions. the ability to alter a color call to enhance the consistency with base suggestions in long stretches helps not only with substitution errors, but also helps to compensate for inconsistency arising from indels. the details of the model are explained in the methods section.

contig self-consistency check
we implemented the self-consistency checks described below only in s-sopra. the reason for these checks is that the programs, like ssake, which use a greedy algorithm for contig assembly, are particularly vulnerable to generating chimeric contigs. if two legs of a mate pair are located on the same contig, then their relative orientation and position in the contig should match the ones suggested from the mate pair link. if we observe more than certain number of times  cases where the orientation disagrees or the separation between reads is more than one standard deviation different from the insert size, we discard that contig. this method, however, does not necessarily detect chimeric contigs where two or more regions from different parts of the genome have been mis-assembled into one contig. mate pair information can be used to detect such mis-assemblies, as explained below.

if a contig is genuine, there should be several mate pairs connecting different locations on the same contig . however, if it is the case that the contig is composed of two or more sequences coming from different parts of the genome, there should not be as many mate pair links connecting those sequences together. for each point on a contig, we count how many mate pair links connect the right side of that point to the left side. if this number is particularly low for some region, we cut the contig into two at that position.

estimation of insert size
in the case where there are enough long contigs, the typical value of the insert size can be estimated from the mate pairs located on the same contig. to do so, we first remove the outliers for which the separation between the pair is different from the suggested insert size by more than the value of the suggested insert size . the empirical insert size is equal to the mean value of the separation for the remained pairs. the user needs to know only an approximate value for the insert size based on the library preparation protocol. prior knowledge of the typical insert size needs to be accurate only when almost all contigs are smaller than the typical inserts.

in case the insert size targeted by the library preparation methods is not available to the user, he/she could take advantage of the empirical distribution of insert sizes output by sopra and determine the typical insert size by inspection. in any case, it is a good idea to inspect this distribution, to ascertain the quality of the mate pair library.

removal of reads in high coverage regions from scaffolding process
a contig containing repetitive regions can provide conflicting mate pair constraints and cause mis-assembly in the scaffolding process. although, one could take up the problem of resolving the repeat structures, our approach currently is to identify and remove such contigs from the scaffolding process. one way of detecting repeats is by looking for high coverage regions in each contigs. if a contig has high mean coverage  we remove such a contig from scaffold assembly before starting the process. some contigs have high coverage locally without having high mean coverage. we exclude mate pairs with reads falling in such local high coverage regions for the scaffolding considerations as well .

scaffold assembly
if two legs of a mate pair are incorporated into two separate contigs, we can infer the relative orientation and relative position of those two contigs on the genome. however, such ordering of contigs is not an easy task, since, the constraints imposed by mate pairs are often not self-consistent. the best one can do is to assign the orientations and positions so that as many constraints as possible are satisfied. in addition, there can be misleading or incorrect information. these dubious constraints arise not only from issues like erroneous contig assembly, but also from innate problems in mate pair data itself.

to elucidate this point, let us examine the two real libraries discussed below in the performance comparison section. in figure  <dig>  we plot the histogram of separation between the two reads belonging to a mate pair, obtained by matching the reads to the reference genome. as we can see, the distribution of separation could be thought of as a combination of a sharp peak and a broad background that spans over the entire length of the genome. even if we limit ourselves to the sharp peak , the standard deviation is around 20% of the mean value. the variability in separation is much larger than values used for generating simulated data in some studies  <cit> . the algorithm for position assignment has to be robust to such large degree of uncertainties. as will be discussed in the coming sections, in our approach, this goal is achieved by identifying and removing those mate pairs that belong to the broad background as well as from averaging effect of imposing all the remaining constraints together.

for contig building, it is often convenient to represent the sequence overlap information using graph theoretic constructs, e.g. in terms of an overlap graph or a de bruijn graph. similarly, it is useful to encode the constraints given by mate pair information into a graphical model. in this model, the underlying undirected graph has vertices corresponding to each contig. any two contigs connected through mate pairs have an edge in between. we call this graph the contig connectivity graph. this graph is similar to the contig-mate-pair graph introduced in  <cit> , except that here each contig is represented by a single vertex as opposed to two. this kind of graph structure has been used in other studies as well  <cit> . the structure of the contig connectivity graph, at different stages of the assembly, can be visualized with the help of programs such as graphviz package  <cit> .

in our formulation, orientations and positions for each contig are variables living on the vertices of this graph. if we introduce the mate pair information as probabilistic constraints on relative orientations and positions of neighboring vertices on the graph, this graphical model has the structure of a markov random field model  <cit> . markov random field models were originally inspired by problems in statistical physics. there are relatively obvious connections between finding the ground state  of certain statistical physics models and well-known graph optimization problems as was pointed out by several researchers in the eighties  <cit> . such analogies also led to the simulated annealing  <cit>  as a heuristic method for solving hard combinatorial optimization problems . we will explain our procedure by invoking the physical analogies, but one could often describe the same procedure using a language familiar to computer scientists.

we perform the scaffolding in two steps. we first assign the orientation of contigs, without considering their positions. once the orientation is determined, in the second step, we calculate the position of contigs. in this second step, we only use those mate pair links which are consistent with the orientation assigned in the first step. in principle, one could have optimized for orientation and position together, however, our two steps process simplifies the algorithm.

one additional constraint is that distinct contigs cannot be assigned to the same or overlapping positions. this should be true for every possible pair of vertices. this means that if we want to impose this condition in the contig connectivity graph, every possible pair of vertices will be connected by an edge representing this non-overlapping condition. in other words, every vertex will be directly connected to all other vertices. in this sense, the markov random field structure on the contig connectivity graph is violated. we first solve for orientations and positions ignoring the non-overlapping constraints. the resulting solution typically includes some scaffolds for which the non-overlap condition is not satisfied. we segment these scaffolds into smaller scaffolds satisfying the non-overlap condition using another markov random field model living on a new graph obtained by augmenting the contig connectivity graph with additional edges between apparently overlapping contigs.

determining the relative orientation
we indicate the two possibilities for the orientation of contig i by si =  <dig> and si = -  <dig>  if two contigs i and j are connected through mate pair links, we associate a number to it, denoted by jij. the sign of jij is positive if the links suggest that two contigs have the same orientation, otherwise it is negative. the absolute value of jij is equal to the number of links that connect the two contigs. if all the mate pairs connecting two contigs do not agree with each other, we require that at least a significant majority do. to be a significant majority, we require the percentage of the mate pairs in the dominant group to be higher than a certain threshold, which is a parameter in the software. otherwise, all the links between those contigs are ignored.

the reason for rejecting all these links is as follow. for two close-by genuine contigs, not belonging to repeats, the source of orientational conflicts is the presence of spurious mate pairs. usually, these inconsistent spurious links form a small minority. however, when a part of a contig belongs to repetitive regions or one of the contig is chimeric, the nature of the orientational conflicts is different. for example, it is likely that part of the mate pair information suggests the contig belongs to one strand while some other part of the information suggest it belongs to the other strand. in such cases, the majority group and the minority group can have comparable number of links. if a significant majority of links do agree, the minority links are ignored suspecting that they are spurious. if the numbers are comparable, then all links are ignored for the reason mentioned above.

for each configuration of orientations, s = , n being the number of contigs, we define the following cost function:   

this quantity, a measure of how many of the mate pair links are satisfied, could be thought of as the energy of an ising spin system with interactions jij. if it were possible to find a configuration to satisfy all the constraints, we would have: sign  = sign , ∀ i, j. the energy of this configuration would be: . as we mentioned before, it is often the case that such a configuration does not exist. therefore, our goal is to find the best configuration in which as many mate pair links as possible are satisfied. effectively, we want to find the orientation assignment that minimizes the energy function in equation  . this minimization is equivalent to the maximum weight cut problem, which appeared in the context of shotgun sequencing  <cit>  and of scaffold assembly  <cit> . given that this problem is np-complete  <cit> , it is natural to search for heuristic methods. the approach of these earlier studies is to resolve the constraints in the scaffold assembly problem through particular greedy algorithms that depend upon ad hoc schemes of ordering the contigs. the contrast between such approaches and ours will become clear, as we will explain our algorithm in the methods section.

determining the relative position
for determining the relative positions of contigs, we only use the mate pair links that are orientation-wise consistent with the optimal configuration found in the previous section. consider a set of contigs connected through mate pair links. let x = , denotes the positions of the start points of these contigs. by putting x <dig> =  <dig>  we have chosen a particular system of coordinates. each mate, r, connecting contigs i and j, provides us with some information about xi - xj, encoded in the probability distribution pr. this distribution is picked around certain value, , which can be determined from the location of the two reads in the corresponding contigs and the insert size of the mate pairs .

had we not assigned the orientations, one could still define , with the orientations only affecting the sign of the quantity. note that  is the suggested distance between the corresponding contigs, whereas, the sign determines the ordering . in figure 4a, next to each edge, we just show jij's. however, each edge also carries the additional information on the relative position of the corresponding contigs . before assigning the orientation, the contig connectivity graph does not fully capture the ordering of contigs, since, as we explained,  is determined up to a sign. after the orientation assignment, the full information about relative position of contigs is captured by this graph.

the overall information provided by all the mate pairs linking contigs i and j is given by . note that | jij | is the number of mate pairs bridging between contigs i and j. we do not know the exact form of pr ; however, if we take it to be a gaussian centered around , we will have:   

where σ corresponds to the variance in the insert size of mate pairs. our approach is to determine the relative position of contigs by maximizing the joint probability distribution:   

where  is the average suggested distance between the start points of contigs i and j. equivalently, one could minimize the function:  

this function has an alternative interpretation as the energy of a coupled system. in this analogy, the collection of mate pairs between two contigs i and j is replaced by a spring connecting the start points of those contigs. the spring constant is equal to | jij |, and the relaxed length of the spring is given by . in this way, the original system of contigs connected through a network of mate pairs is modeled as a system of objects connected through a network of springs . the solution maximizing the probability given in equation  corresponds to the equilibrium position  of the objects in the spring system. these positions could be calculated by solving a set of linear equations corresponding to the force on each object being zero.

in the equilibrium position, if the distance between two contigs is equal to the distance suggested by the mate pairs connecting them, then the corresponding spring is relaxed; otherwise, the spring is either stretched or compressed. in other words, we could define  as a measure of the degree to which the mate pair constraints are violated. if all the suggested distances were self-consistent, all Δij's would be nearly zero . in real data, it is possible that some sequences match in several locations on the genome, and therefore, mate pair information would not uniquely determine the position of contigs. in our model, the sign of this non-uniqueness is that in the equilibrium solution, x*, some of the springs will be stretched or compressed. the same situation can arise because of contig mis-assembly where two separate regions of the genome are joined into one contig.

when there is a stretched or compressed spring, we remove the contigs attached to the end of that spring from the system and restart the scaffold assembly on the remaining contigs. in other words, we go back to the orientation assignment step . the cycle stops when in the equilibrium position, all the springs are close to their relaxed state, namely, all Δij's are below a certain threshold. note that x* is the positions of the start points of contig. if the orientation of contig i is positive, it means that it covers the interval  on the scaffold. if i has negative orientation, we assign the reverse complement of i to the interval .

the greedy algorithms, previously applied to the combinatorially difficult problem of assigning relative positions, consider contigs in a certain order; an order that depends on the number of links associated with each contig  <cit> . potentially, such methods could be prone to incorporating repeats/chimeric contigs which could have significant number of links associated with it. in contrast, our method has the advantage of providing an unambiguous means for flagging misleading distance constraints with having to commit to any such order.

detecting tangled scaffolds by the contig density profile
we calculated the position of the contigs in a scaffold from a set of linear equations based on the assumption in equation . of course, position intervals corresponding to distinct contigs should be non-overlapping. however, the solution of these linear equations is not guaranteed to satisfy this non-overlap condition. in fact, such overlapping configurations do arise in practice. below, we explain some of the causes leading to this problem.

consider the scenario described in figure 5a. there are two sets of contigs, shown in green and magenta, belonging to distinct regions of the genome. contigs within each set are self-consistently connected through mate pairs. assume during contig assembly, contig  <dig> from the first set and contig  <dig> from the second set get mis-assembled into one contig. in this case, we obtain a scaffold that contains all the contigs and yet, does not have any stretched or compressed spring.

in addition to contig mis-assembly, existence of repetitive regions in the genome is another factor that can cause improper joining of multiple scaffolds. in that case, contigs  <dig> and  <dig> in figure 5a are seen as one contig in the assembly, whereas they are really copies of the same sequence that matches on multiple places on the genome. each copy can cause the mis-incorporation of a new set of contigs from its neighbors.

in order to detect this type of complication, we define the 'density profile', a quantity that represents how many contigs cover each region of a scaffold. in the final assembly output, this density should be near one for all regions of each scaffold . for a configuration like in figure 5a most of the points in the problematic region are covered by two contigs, leading to a higher density. therefore, by inspecting the density profile, we expect to detect these cases where two or more scaffolds are mis-assembled into one scaffold. figure 5b shows the density profile obtained in the assembly process of a real dataset from e. coli genome . notice that there are two regions with density above the background density of one, and that those high densities are in fact very close to integers . the nearly integral values indicate how many potentially distinct scaffolds have been joined together.

scaffold segmentation
after detecting high-density regions, we need a procedure to identify and remove the problematic contigs that lead to the merger of disjoint scaffolds. we will call these contigs "junctures" for future references. we wish to assign the rest of the contigs into distinct scaffolds in such a way that each scaffold has an acceptable density profile. with that goal in mind, we provide each contig i with a variable σi. one could think of σi's as a putative scaffold label. from the density profile, we can determine q, the total number of distinct labels  that we need. for example, the profile in figure 5b implies q =  <dig> 

we want to assign the labels according to two criteria. on one hand, we want the contigs that are directly connected by mate pairs to have the same label. on the other hand, we want the contigs that lie over each other to have different labels. to present these criteria mathematically, we define two matrices d and o. if contigs i and j are directly connected by mate pairs, the matrix element dij is one; otherwise, it is zero. the matrix element oij is a positive number monotonically increasing with the length of the region that contigs i and j cover simultaneously. we want to find the label assignment that minimizes the following cost function:   

here,  is the kronecker delta; it is one if σi and σj are equal and zero otherwise. this cost function is exactly the energy of a q-state potts model with both ferromagnetic and antiferromagnetic interactions. we use a simulated annealing method  <cit>  to find a configuration of label assignment that minimizes the above energy .

in the minimum energy configuration, neighboring contigs belonging to the same scaffold prefer to have the same label while contigs belonging to different scaffolds, juxtaposed in position space, prefer to have different labels. this is a direct consequence of the two criteria with which we began. however, these two criteria cannot be satisfied everywhere at the same time. around the junctures, namely, contigs joining such juxtaposed scaffolds, the two criteria are at conflict with each other. the result of this conflict is the formation of domain boundaries  in the neighborhood of the junctures. to get a better sense of this phenomenon, let us revisit the example in figure 5a. the result of label assignment by our algorithm could give rise to any of the three configurations in figure 5c . note that the juncture is always located at the boundary where different labels meet.

motivated by this discussion, we form an initial list of suspected junctures from the contigs located at label boundaries, namely, contigs having at least one neighbor with a different label. this list often has much fewer members than the original set that we started with. ideally, one would like to consider the result of removing all the different combinations of suspected contigs from the original set to check if it resolves the problems in density profile. an exhaustive search over all combinations becomes possible when the list is small. otherwise, one has to limit the list to members located at the densest part of the scaffold. if the list is still too large, we have to proceed with a randomly chosen subset.

after removing any subset of these suspected junctures from the original set of contigs, the remaining set of contigs will form one or more connected components. we score each subset by combining two numbers, one penalizing the formation of too many small components and the other penalizing the presence of high-density regions. we choose the best scoring subset to be removed and focus on the resulting connected components.

for each connected component, we check whether the corresponding density profile is free of high-density regions. all connected components with satisfactory density profiles are declared to be new scaffolds. for the rest, we restart the labeling process individually for each component, and continue this process until all the components have satisfactory density profiles. the removed contigs, either in the potts model or in the spring model, are reported as single contigs at the end of the assembly.

the potts model based approach is different from the formulation in terms of non-self-overlapping path introduced in pop et al.  <cit> . the method of arbitrarily picking the longest non-self-overlapping path  <cit>  through the tangle might end up joining two scaffolds wrongly. in our method, we remove the problematic contigs, even if, in some cases, it could lead to some good scaffold breaking up. if there are mate pairs overarching the removed contigs, it is possible for scaffolds to have the correct continuation. this is the case for the example in figure 5a, since contigs  <dig> and  <dig> are connected by a mate pair overarching contig  <dig> 

contig joining and gap estimation
in the last stage of scaffold assembly, we decide whether neighboring contigs in a scaffold are to be joined or be separated by a gap. notice that according to the computed positions, the end of two neighboring contigs might still have a small positional overlap ; otherwise, they will be separated by a gap. in either the case of positional overlap or the case where the estimated gap is smaller than certain value , if the ends of neighboring contigs are similar, we join those two contigs. for the rest of the cases, we insert a sequence composed of letter 'n' between the contigs. the length of each sequence is decided by rounding the length of the corresponding gap to the closest multiple of  <dig>  in the special case where there is no sequence similarity, despite the positions indicating a small overlap, we separate the contigs by a  <dig> base long sequence of 'n'.

assembly performance on real data
metrics of assembly quality
before we discuss our results, we need to define how we assess the quality of a de novo assembly. the first obvious measure of performance is the typical size of assembled contigs and scaffolds. this quantity is often reported in terms of an n <dig> value. roughly speaking, half of the bases are covered by contigs/scaffolds that are longer than the n <dig> value. however, n <dig> provides no indication of the accuracy of the assembled contigs/scaffolds. in order to evaluate the quality of the assembly, it is common to study the performance of the algorithm on data from known genomes. while comparing the assembled components to the reference genome, we need to pay attention to different kinds of errors that could arise and define the metrics of performance accordingly.

to define such metrics, let us bear the following question in mind: in order to map a contig to the reference genome, what type of different operations do we need to do? for example, it might be possible for an entire contig to be matched to a continuous part of the genome with a few mismatches and indels. however, it could also be the case that the contig cannot be matched to a continuous region of the genome; instead, different parts of the contig might match to different regions of the reference genome. of course, for some contigs, one might not find any significant match at all. in addition to errors in the contigs, there would also be errors in the assignment of relative positions and orientations of contigs in a scaffold.

it is common in the sequence assembly literature to single out mismatch rates and combine some of the other kinds of errors in the 'no-match' category. the emphasis of our algorithm is on using the mate pair information for orienting, positioning and joining contigs. improper execution of these tasks leads to the formation of chimeric contigs, dislocation and inversion of contigs in a scaffold, as well as merger of distinct scaffolds. metrics for quality assembly corresponding to these categories of errors are essential for fair comparison among different algorithms. in general, for each algorithm, there is a trade-off between building large scaffolds and making small number of mistakes. for example, a cautious algorithm might produce smaller scaffolds rather than keep on joining suspicious fragments together.

following the spirit of the above discussion, we will define four categories of errors in order to assess the quality of the assembly. we used megablast  <cit>  with a minimum identity threshold of 92% to align the sequences against the reference genome . the sum of the length of all the contigs for which no blast hit is found, expressed as a percentage of total assembled bases, is reported as the no-match error rate, εno_m. each blast hit for a contig comes with a number of mismatches and short indels. mismatch error rate, εmis_m, reports the total number of mismatches and indels as a percentage of total assembled bases. in addition, if only some parts of a contig do not match to the reference genome, the total length of those parts contributes to mismatch counts as well.

as we discussed above, there are other types of error that lead to large-scale 'rearrangements' of genomic sequence. the use of the term 'rearrangement error' is inspired by the analogy with the process of genome evolution. just as local errors in assembly have similarity to mutations and indels, the large scale errors in assembly, have their evolutionary analogues: inversion, translocations etc.

these rearrangement errors, measured in the unit of number of events per mbp of assembly, are divided into the following categories. the error rate εch is associated with chimeric mis-assemblies, namely, the cases where two distinct parts of the genome have been joined into one contig. for chimeric contigs, we would like to differentiate between the cases where the real gap between mis-assembled parts is in the order of few hundred bases and the cases where this gap is in the order of, for example, a few megabases. therefore, overall error rate εch is broken down to two parts,  and , accounting for chimeric contigs involving gaps smaller or larger than  <dig> bases, respectively.

apart from the issue of chimeric contigs, we also have erroneous assignment of orientations and positions of contigs in a scaffold. each time the relative orientation of two neighboring contigs on a scaffold disagrees with that in the reference genome, we have an event contributing to the error rate . in addition, for any two consecutive contigs in a scaffold, we have an estimated separation, which decides the number of 'n' bases we insert in between those contigs in the final output. for consecutive contigs with verified relative orientations, we compare the estimated separation with the real separation on the reference genome. the last category of rearrangement error rate, , is associated with the cases where the difference between those values is greater than  <dig> bases. the two categories of error, presented in this paragraph, keep track of events where two contigs from different strands or from far apart regions have been brought together.

description of the libraries
we present the assembly result for two real datasets, one being a mate pair library from solid, while the other is of the paired-end kind from illumina. in paired-end technology, mainly used by illumina, two reads in a pair come from the opposite strands. in mate pair technology, both reads in a pair are from the same strand. the insert size is also typically larger for the mate pair libraries, which is beneficial for many applications. at the same time, owing to the particular enzymatic steps required to make the mate pairs, there is a higher rate of production of molecules which do not represent true ends of the large dna molecule. the sequence information from these molecules has to be properly identified and handled so as not to lead to inconsistent scaffolds.

the first dataset is a  <dig> bp mate pair dataset, generated by solid platform, for the  <dig>  mb genome of escherichia coli dh10b http://solidsoftwaretools.com/gf/project/ecoli2x50/. after we used an in-house filter  <cit>  to remove polyclonal and error-laden reads, we were left with  <dig>  million pairs of  <dig> bp long sequences. for this mate pair library, we used the insert size of  <dig> bp . assembly of these reads resulted in very poor quality output. therefore, we decided to trim down the reads to  <dig> bp, expecting most of the sequencing errors are concentrated towards the end of the reads  <cit> . even after filtering and trimming, the remaining reads provided 100× coverage, and produced better assembly than the raw data set .

the other dataset contains  <dig>  million pairs of  <dig> bp long reads from the illumina platform, providing 40× coverage of the  <dig>  mb genome of pseudomonas syringae pv. syringae b728a  <cit> . for this paired-end library, we used the insert size of  <dig> bp .

performance comparison
we compare the performance of our algorithm to that of velvet  <cit> . one reason for selecting velvet is that several studies found that the performance of velvet was either better or at least competitive with other available programs  <cit> . the other reason is that we wanted to study the platform dependence of the performance of sopra. velvet is the only program among the popular assemblers that handles color-space data. for p. syringae dataset from the illumina platform, the original study  <cit>  from which we obtained the library has compared performance of several assemblers. the authors attempted assembly using euler-sr  <cit>  and sharcgs  <cit> , but they ran out of random access memory . it also turned out that velvet outperforms ssake  <cit> , vcake  <cit>  and edena  <cit> . these last two assemblers do not incorporate mate pair information and were run only in unpaired mode. allpaths  <cit>  requires multiple paired libraries with different insert sizes. given the above issues, we decided to proceed with comparison velvet.

in many areas, including biological data mining, a common exercise for assessing the performance of a binary classifier is to consider the det or roc curve  <cit> . as one reduces the stringency of the classifier, false negative rate decreases at the cost of increasing the false positive rate. det/roc curves provide a quantitative representation of this trade-off and are essential for finding optimal operating point that balances the conflicting goals of keeping both of these error rates down. as we mentioned before, in the context of de novo assembly, there is a similar trade-off between n <dig> and the assembly quality  <cit> . in this analogy, smaller n <dig> corresponds to having a high false negative rate, while low quality of the assembly plays the role of high false positive rate.

the comparative assembly performance, in the form of n <dig> versus error rate, is shown in figures  <dig> and  <dig>  ideally, one would like to be on the top left corners of these graphs, which corresponds to large sizes and low error rates. we present the performance of the algorithms both for contig assembly  and scaffold assembly .

in the case of e. coli data produced by solid platform, for contig assembly, the mismatch rate for v-sopra is lower than that for velvet . this is partly because of error correcting feature of our algorithm for translating color-space data. in contrast, s-sopra produces much shorter contigs compared to the other two. running velvet with the paired option did not particularly improve the n <dig>  but it increased the mismatch rate significantly. in comparison to velvet, both v-sopra and s-sopra perform better in term of scaffold size and error rate, with v-sopra outperforming s-sopra.

in contrast to the case of the e. coli mate pair dataset from solid, pairing information helps velvet generate much larger scaffolds from the p. syringae paired-end illumina dataset. figure 6b shows the results of running velvet, with 'paired' option, on the p. syringae reads, for two different parameter sets. note that the two-fold increase in n <dig> comes at the cost of increasing the error rate by more than one order of magnitude. this trade-off pattern is consistent with a study comparing, among other things, the performance of velvet for many combinations of parameters  <cit> . v-sopra produces comparable n <dig> at a much lower mismatch rate. for this particular dataset, the contig building performance of v-sopra and velvet is nearly identical. like in the e. coli dataset, the performance of s-sopra is worse than v-sopra.

more or less the same pattern continues with the large-scale rearrangement error rates. in figure  <dig> we report n <dig> versus the combined rearrangement error rates. in the case of illumina dataset, v-sopra did not produce any errors in certain categories .

the error rate εno_m represents the sum of length of the contigs/scaffolds with no blast hit as a percentage of total assembled bases. mismatch error rate εmis_m reports the total number of mismatches and indels as a percentage of total assembled bases. the error rates  and  are associated with chimeric mis-assemblies, involving gaps smaller or larger than  <dig> bases, respectively. the error rate  accounts for the number of cases where the relative orientation of two neighboring contigs disagrees with that in the reference genome. the cases where the estimated separation between two consecutive contigs on a scaffold differs from the real separation in the reference genome by more than  <dig> bases are associated with . these last four categories of errors are measure as the number of erroneous events per megabases of assembly.

in general, for both datasets and all categories of error, our algorithm utilized the mate pair information to enhance n <dig> by one or two orders of magnitude without significantly increasing the error rates . the n <dig> gain from contigs to scaffolds, for the solid dataset is remarkable for sopra when compared to the corresponding gain for velvet. we believe, based on our simulations , that our gain for the illumina dataset would have been much larger if, instead of being around  <dig> bases, the insert size of this library were close to a kilobase. another reassuring aspect of sopra as compared to velvet is that for solid dataset, the algorithm managed to keep the mismatch error rate low, partly thanks to the robust handling of the color-space translation.

for the definition of different error rates, see the caption for table  <dig> 

we also used megablast to analyze the contigs which sopra removed from the scaffolding process during the assembly. the result is presented in table  <dig>  for the p. syringae dataset from illumina platform, most of the removed sequences were either chimeric or belonged to repeats . for the e. coli dataset from solid sequencer, slightly more than half of removed sequences were determined to be problematic. in both cases, the total length of removed sequences remains a small fraction of the total assembly. it should be noted that for a removed contig which was not determined to be problematic, there is a possibility that it contains a short stretch of sequence belonging to repeats which was not identified by megablast.

problematic contigs refer to contigs which are either chimeric, belong to repeats, or do not match to the reference genome. genomic length means that for repeats, the length is multiplied by the corresponding copy number.

CONCLUSIONS
the goal of scaffold assembly is to arrange contigs such that most of the mate pair constraints are satisfied. given the inconsistencies in the constraints, any solution strategy inevitably has to decide upon a subset of constraints to be ignored. in our algorithm, this choice is made iteratively, going back and forth between the optimization step and removal of offending constraints. for example, in the process of assigning the optimal orientations, we also detect the links that are not satisfied and are to be removed. the same was true for the next step, where, by modeling the links as springs, we both assign the positions and remove the constraints that cause stretch/compression in this solution.

taking the entire set of remaining mate pair constraints into account simultaneously at each round of optimization is critical to the success of our approach. some algorithms, at each step, consider only a small subset of contigs and links in between to improve the assembly in a particular region  <cit> . this manner of local processing of mate pair information stands in stark contrast to our global approach.

in a sequencing project, the issue of large variability in separation of mate pairs  has an important implication for the choice of the insert size in the library preparation. the insert size should preferably be large enough to bridge over most of the small repeats or the shallowly sequenced regions. however, as the typical insert size increases, so does the standard deviation of the separation for individual mate pairs. the averaging effect from having multiple mate pairs between two contigs depends upon the number of such pairs, which, in turn, is limited by the size of the corresponding contigs. therefore, beyond a certain point, larger insert size might result in higher uncertainty in contig positioning. we expect the optimal insert size to be dependent upon the typical size of the contigs, the depth of coverage, and most importantly, the ability to restrict size variation in the library preparation. in our simulations for assembly of some bacterial genomes, the optimal insert size is typically around  <dig> kb, if we were to choose only one insert size . however, if the contig assembly mostly produces small fragments, namely, the contig n <dig> is much less than  <dig> kb, the quality of scaffold assembly suffers significantly.

in our study, we emphasized the possible conflict between getting larger scaffolds and avoiding mis-assembly. we showed that the n50/error rate trade-off characteristics for v-sopra is excellent. in a practical de novo assembly project, mis-assembly rates are hard to estimate. as a result, one may be tempted to increase the n <dig> without consideration of accumulating inaccuracies <cit> . therefore, it is important for such projects to develop a set of independent benchmarks to assess the accuracy of assembly. the n50/error rate trade-off curve, based on such benchmarks, can be used to set the optimal parameters for the assembler.

currently, sopra is quite conservative and it errs on the side of breaking up things whenever there is any confusion. as we have seen, this tendency has not resulted in smaller n50s compared to other algorithms. however, it is possible that a more sophisticated algorithm could partially reconstruct the structure of repeat regions while solving the orientation and positions of different contigs. one may also be able to breakup some chimeric contigs at the right place rather than remove the whole contig. we hope to include these features in the future versions of the algorithm.

the current hts platforms not only read sequence fragments but also generate additional information regarding relative position and orientation of pairs of reads. our methodology is particularly adept at exploiting this extra information. the approach developed here could be easily adapted to any new technology that provides additional positional and orientational constraints on multiple reads. combination of efficient algorithms for utilization of such constraints and improvements in accuracy of reads leading to better quality contig building will bring us closer to the goal of assembling genomes from the next generation of hts data.

