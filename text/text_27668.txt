BACKGROUND
selecting the most native-like model from a set of possible models is a crucial task in protein structure prediction. a variety of model quality assessment programs  have been developed that assign numeric scores to models in a set, and then use the scores to rank the models and ultimately select a single model. mqap methods can be divided roughly into three categories based on the type of information they use: evolutionary methods use sequence or profile similarity between target sequence and template, consensus methods use similarity between models, and structure-based methods use model coordinates  <cit> . each category of methods has inherent strengths and weaknesses.

evolutionary methods can provide quality scores that have been shown to correlate with structural similarity to native  <cit> . however, for lower confidence alignments the scores do not correlate well with structural similarity. furthermore, identification of the best template and specific alignment can be difficult. in addition, models built from multiple templates or template-free methods cannot be scored appropriately by evolutionary methods alone.

consensus methods take advantage of the observation that similar models produced by different predictors tend to be more accurate than those that are structural outliers. in practice, consensus methods outperform the methods they draw from, and they rarely pick a very poor model. the disadvantage, however, is that when the best model is a structural outlier it will be overlooked for lack of popularity  <cit> . also, consensus methods are not appropriate for selecting from small sets of structurally diverse models, especially in the extreme case of a two-model set.

while consensus methods depend on similarity between models, structure-based methods calculate scores on each model independently. for this reason, structure-based methods can be applied to model sets of any size and diversity, and will produce the same score for a model regardless of the other models in the set. structure-based methods can also be used for template-free modeling  <cit>  and model refinement procedures  <cit> . one weakness of high resolution structure-based methods, including protein free energy approximation functions  <cit>  and physics based approaches  <cit> , is their sensitivity to local structural irregularities such as steric clashes and chain breaks, which can significantly bias scores on otherwise accurate models. even slight differences in model backbones can produce significantly different scores  <cit> . lower resolution structure-based methods, such as statistical potentials  <cit> , are more robust to backbone variation, but are sensitive to extended low contact-order regions in the models.

here we describe selectpro, a novel structure-based mqap that combines high and low resolution energy terms into a model selection method that is effective on model sets of variable size, diversity, and target difficulty. most of our assessment is calculated from the casp <dig> model quality assessment category  results published online  <cit> . the qa category provides a framework for the unbiased evaluation of mqaps on ensembles of models produced by diverse automated prediction methods.

RESULTS
we analyze the casp <dig> quality assessment category predictions with a focus on the quality of the model ranked first by each predictor and the recovery of the most native-like model in the set. only setall is used in the assessment of the quality of the model ranked first by each group . the results are very similar when using setcomplete  because qa groups rarely rank an incomplete model first.

a the number of targets where the qa group made a valid prediction  with the number of domains of these targets  in parentheses.

* selectpro  results appear in bold face and all results that are better than selectpro are underlined. statistically significant p-values  are also in bold.

the assessment of the recovery of the most native-like model, is performed on both setall and setcomplete  because the few cases where an incomplete model is the most native-like have a significant effect on the average recovery metrics of all qa groups. incomplete and irregular models are especially challenging for structure-based methods. a comparison of the average pearson correlation on setall and setcomplete, highlights these issues . the frequency of recovering the most native-like model is calculated on setcomplete .

a the number of targets where the qa predictor scored mmax  with the number of domains of these targets  in parentheses.

b in the casp <dig> submission selectpro did not have a score for mmax of target t <dig> due to a processing error. we added in the score for this analysis in order to make complete common subset comparisons.

* selectpro  results appear in bold face and all results that are better than selectpro are underlined. statistically significant p-values  are also in bold.

a consensus method.

b structure based method.

c qa scores calculated as gdt-ts similarity to human predictor of lee.

the utility of selectpro for selecting the best model from a small set is demonstrated by selecting from the five models submitted for each target by the top automated predictors. these small set selection results are calculated using setall . selectpro is also evaluated on a recent benchmark set of  <dig> small proteins with large decoy sets of  <dig> to  <dig> models for each protein and compared to i-tasser .

to make fair comparisons to groups participating on only a subset of targets, common subset comparisons between selectpro and each of these groups are included in tables  <dig> and  <dig>  only groups participating on at least half of the targets are included, and for groups with multiple submissions only the best one is shown. in the results tables any value that is better than selectpro is underlined.

for multiple domain targets, the sum of gdt-ts over all domains is used as the gdt-ts of the model. since the qa predictions correspond to the entire structures, it is impossible to fairly assess the domains independently.

to assess the significance of the summary statistics compared in table  <dig>  table  <dig>  and figure  <dig>  we performed paired t-tests between selectpro each other group on common subsets of targets . all p-values from the tests appear in the tables and figure, but only statistically significant p-values  are shown in bold.

the following notations are used throughout the results section:

• mmax: the model with the highest gdt-ts among all server models.

• mqa1: the model with the highest qa score.

• nt: the number of targets a group made valid predictions on.

• nd: the number of domains a group made valid predictions on.

the recovery of mmax by a qa predictor can only be evaluated if mmaxwas scored by the predictor. in most cases qa predictors did not provide scores for all available server models, and frequently there is no score for mmax. for example, predictor 016_ <dig>  made submissions on  <dig> targets, but mmax is only scored for  <dig> of these targets – so only these targets  can be evaluated for this predictor.

quality of model ranked first  relative to most native-like model 
in this section on the assessment of the model ranked first, and the corresponding table  <dig>  we use the following three metrics:

• Δgdtqa <dig> = gdt-ts - gdt-ts : the gdt-ts difference between mmax and mqa <dig> measures how much is lost by selecting mqa <dig> rather than mmax for a single target.

• Δgdtqa1¯ = ΣΔgdtqa1/nd : the average Δgdtqa <dig> is a simple way of assessing the quality of mqa <dig> over all targets.

• Δgdtqa1% = Δgdtqa1/gdt-ts : the gdt-ts difference percentage allows for comparison across targets with different numbers of domains and difficulty levels.

the columns of table  <dig> are:  group number;  number of targets the group made predictions on;  number of targets such that Δgdtqa <dig> = 0;  number of targets such that Δgdtqa1% < 1%;  number of targets such that Δgdtqa1% < 10%; and  Δgdtqa1¯. the common subset results section has an additional column for the p-value of the paired t-test using Δgdtqa <dig>  the rows are sorted first by the number of targets and then by Δgdtqa1¯. of the groups participating on all  <dig> targets, selectpro has the lowest average Δgdtqa <dig>  with a value of  <dig> , followed closely by group 713_ <dig> , with a value of  <dig> . predictor 038_ <dig>  has an average Δgdtqa <dig> of  <dig> , with predictions on  <dig> targets. in common subset comparisons with these two groups selectpro is not significantly better, with p-values of . <dig> and . <dig> respectively. in common subset comparisons with all remaining groups selectpro is significantly better.

another way to assess the quality of mqa <dig> over many targets is to count the number of targets such that mqa <dig> is the best model, or nearly the best, in the set. a method that performs very well on most targets, but very poorly on a few, would still be recognized by this criteria. selectpro recovers the best model for  <dig> targets, selects a model with Δgdtqa1% < 1% for  <dig> targets, and selects a model with Δgdtqa1% < 10% for  <dig> targets. group 091_ <dig>  also performs well, with  <dig>   <dig>  and  <dig> targets in the respective categories. only the  <dig> targets with Δgdtqa1% < 10% of predictor 038_ <dig>  on its  <dig> target subset are better than selectpro in common subset comparison .

the blunder measure recovery of mmax
how well does a qa predictor recover mmax? the traditional metric to assess mmax recovery is the rank of mmax, and the average rank over many targets . while rank captures some important information, it ignores the redundancy of models and the quality of models ranked better than mmax. consider the following hypothetical situation: group a ranks mmax 10th and all nine models ranked above it are redundant with Δgdt of ~ <dig> , group b ranks mmax 5th and the four models ranked above it are diverse with a Δgdt between  <dig>  and  <dig> . which group has done a better job of recovering mmax? in this example, the rank metric favors group b, although group a ranks only a single redundant model above mmax. in addition, the models ranked better than mmax by group a have only slightly lower gdt-ts than mmax, while the models ranked better than mmax by group b are significantly worse than mmax. to address these weaknesses of the rank metric, we introduce the blunder metric, which focuses on the worst model ranked better than mmax . this measure is not affected by model redundancy and measures the quality of models ranked above mmax. the blunder metric is defined using the following notation, and used in the assessment of the recovery of mmax and the corresponding table  <dig> and figure 1:

• mblunder: the model with the minimum gdt-ts among models ranked better than mmax.

• Δgdtblunder = gdt-ts - gdt-ts : the gdt-ts difference between mmax and mblunder measures the size of the worst blunder.

• Δgdtblunder¯ = ΣΔgdtblunder/nd : the average Δgdtblunder measures how well a method robustly recovers mmax over many targets.

• Δgdtblunder% = Δgdtblunder/gdt-ts : the Δgdtblunder percentage allows for comparison across targets with different numbers of domains and difficulty levels.

pearson correlation for individual proteins
the assessor evaluation of the quality assessment category  <cit>  focused on the pearson correlation between the qa scores and gdt-ts. here we use the pearson correlation only to highlight some of the difficulties for structure-based methods in dealing with incomplete models, as well as basic non-protein like structural features. approximately half of the models in setall are incomplete, with backbone coordinates missing for one or more residues.

incomplete models present a challenge to selectpro and other structure-based methods because the scores for each model are only comparable when calculated on coordinates for the same set of residues. another issue is that some complete models have severe chain-breaks, severe steric clashes, or significant portions modeled only as extended chains. these local problems can overwhelm the energy of what may otherwise be a good model. consensus methods do not suffer from these local structure problems. given this rationale, one would expect structure-based methods to see the most improvement in terms of average pearson correlation on setcomplete relative to setall. table  <dig> shows the average pearson correlation of five selected groups. predictors 713_ <dig> , 633_ <dig> , and selectpro are structure-based mqaps, while 634_ <dig>  is a consensus method and 556_ <dig>  scored structures based on the gdt-ts similarity to their human model  <dig> casp <dig> prediction  <cit> . as expected, the structure-based mqaps improve more than the structural similarity-based methods. the even greater increase in pearson correlation for selectpro can be accounted for by the failure to generate appropriate complete models for some of the incomplete models resulting in qa scores calculated on extended chains.

reranking top server group models
predictors in casp may submit up to five models, but casp evaluation focuses on the model designated as model  <dig>  clearly, the selection of model  <dig> is critical in the casp setting and for protein structure prediction in general. figure  <dig> contains the results when selectpro is used to rerank the five models submitted by each of the top ten servers from casp <dig>  compared to each server's results. in the following assessment mmax-g is the model with the highest gdt-ts of the five models submitted by a server. figure  <dig>  shows that selectpro recovers mmax-g more frequently than  <dig> of the top  <dig> server groups; in addition, when selectpro is used to select model  <dig> the average gdt-ts increases for  <dig> of  <dig> sever groups; however, the increase is only statistically significant for  <dig> groups. selectpro improves using both criteria for the top  <dig> server groups . these results highlight the utility of selectpro for the task of model selection. the comparisons made here are fair because structure-based methods can be applied in the server setting to any number of models.

large decoy set model selection
here we analyze selectpro's model selection capability on the large decoy sets for  <dig> small proteins from a recent i-tasser benchmark set  <cit> . the i-tasser prediction method generates  <dig> to  <dig> different backbone conformations. the complete decoy sets can be downloaded from  <cit> . the consensus method spicker  <cit>  is used to cluster the models and a centroid model is built from the first cluster. a second round of simulation resolves the steric clashes in the centroid model and results in the final predicted model. the centroid model and final model are not part of the decoy set. in order to make a fair model selection comparison the decoy model closest to the centroid is used as i-tasser's mqa <dig> 

on the benchmark set selectpro has an average gdt-ts of  <dig> , while i-tasser has an average gdt-ts of  <dig> . selectpro's average Δgdtqa <dig> is  <dig>  and i-tasser's Δgdtqa <dig> is  <dig> . figure  <dig> displays the gdt-ts results for the individual proteins in the benchmark set. different symbols are used to indicate the gdt-ts of mmax , the gdt-ts of selectpro's mqa <dig> , and the gdt-ts of i-tasser's mqa <dig>  for each protein. a paired t-test of the hypothesis that selectpro and i-tasser's mean performance are equal produces a p-value of . <dig>  which is not statistically significant, but does give some evidence that selectpro can select a very good model from a large set of decoys at least well as an established method that utilizes consensus methods.

CONCLUSIONS
a mqap that can select the most native-like model from a set of possibilities has a variety of applications in protein structure prediction. the new quality assessment category introduced in casp <dig> allows for the unbiased assessment of mqaps on the models produced by automated predictors. this category allows researchers to focus on the model scoring aspect of protein structure prediction.

the results presented in this work demonstrate that selectpro, a structure-based model selection method, consistently selects one of the best models from the large diverse sets of models produced by automated predictors, across all levels of target difficulty. on these large diverse sets of models, selectpro also recovers the single most native-like model well compared to other methods. on the small sets of five models submitted for each target by the top automated predictors, in most cases selectpro selects better models than the predictors themselves.

since selectpro and other structure-based methods score models independently, they can be incorporated into the model selection pipelines of individual protein structure prediction servers. for this reason, it may help predictors if the casp organizers distinguished methods that score models independently from those that do not.

consensus and structure-based methods can be combined to achieve improved results. for example, the meta-server method pmodeller  <cit>  combines consensus  and structure-based methods  to predict protein structures more accurately than either method in isolation. the assessment of the qa category by casp assessors recognized the consensus method pcons  for the high pearson correlation between their scores and model gdt-ts on most targets  <cit> . in their own assessment the authors of pcons recognized that while consensus methods perform well in most cases, "when most of the models are incorrect and the few correct models are outliers a consensus based approach cannot be expected to make an optimal choice."  <cit>  for instance, they identified three particular targets in casp <dig> where their consensus method failed: t <dig>  t <dig>  and t <dig>  <cit> . the pcons average Δgdtqa <dig> on these three targets is  <dig> . the same research group's structure-based method proq  has an average Δgdtqa <dig> of  <dig> . in contrast, on these three targets selectpro has an average Δgdtqa <dig> of only  <dig> . this example highlights the potential of combining selectpro with existing model selection methods.

selectpro has been made publicly available as a server, where users may submit from  <dig> to  <dig> models for evaluation. in addition to the global confidence scores, the scores of individual energy terms are also returned to the user by email for each model submitted. selectpro is one of several protein structure tools in the scratch suite of predictors  <cit> , and is available through: .

