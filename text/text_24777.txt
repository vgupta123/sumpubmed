BACKGROUND
dna microarray technology provides researchers a high-throughput means to measure expression levels for thousands of genes in an experiment. careful analyses of microarray gene expression data can help better understand human health and disease and have very important implications in basic sciences as well as pharmaceutical and clinical research. some existing methodologies for microarray gene expression data analysis, such as introduced in  <cit>  and  <cit> , have demonstrated their usefulness for a variety of class discovery or class prediction problems in biomedical applications. in a microarray study, we typically face a problem of analyzing thousands of genes from a relatively small number of available samples. this nature gives rise to a very high likelihood of finding lots of "false positives" with conventional statistical methods. therefore, properly selecting the group of genes that are significantly related to a target disease has created one of the key challenges in microarray data analysis.

gene selection problem basically can be viewed as a variable selection problem associated with linear regression models. an incomplete list of those classical variable selection methods/criteria includes the ratio of error sum of squares for the model with p variables to the error mean square of the full model and adjusted with a penalty for the number of variables or the cp criterion  <cit> , the akaike information criterion or aic  <cit> , and the bayesian information criterion or bic  <cit> . george and foster  <cit>  later suggested that these criteria corresponded to a hierarchical bayesian variable selection procedure under a particular class of priors. following the similar setting with a slightly different prior specification, yuan and lin  <cit>  provided another approach to solve this problem and they showed that their algorithm was significantly faster and could be potentially used even when the predictor dimension is larger than the training sample size. although both of these algorithms have been shown to favorably enhance the selection performance comparing to the classical methods such as cp, aic or bic, they share a common disadvantage. that is, even after the hyperparameters are estimated, the variable selection criteria need to be evaluated on each candidate variable for optimality. usually, the number of candidate models grows in an exponential rate with the increase of the number of variables, whereas the typical number of the investigated genes in a microarray data analysis problem is in thousands. this motivates the development of the class of the markov chain monte carlo  algorithms under a bayesian framework to attack the problem. one of the most widely used mcmc algorithms is the gibbs sampler. for the microarray analysis problem, lee et al.  <cit>  suggested a bayesian model based on a linear probit regression setting and proposed a gibbs sampler to solve it. an extension to this method based on a multinomial probit regression setting has also been proposed  <cit> . similarly, zhou et al.  developed another bayesian approach built upon a linear logistic regression model to the gene selection problem.

the linear model based methods mentioned above have been shown with various levels of effectiveness in finding the set of significant genes in a wide range of real microarray experiments. however, they all share some common limitations: the first also the most important one is that, a linear model is not necessarily always a good approximation for the underlying physical model; second, linear model based methods are more likely to bring in false positives; third, the computations of these linear model based algorithms usually involve calculating the inverse of a matrix that is possibly singular when the number of potentially important genes is relatively large. to overcome these disadvantages, zhou et al.  <cit>  introduced a non-linear term into the basic linear probit regression model and applied a bootstrapping procedure to enlarge the sample size. a technique called sequential monte carlo was adopted in the numerical bayesian computation in their work. some other models were also developed for tumor classification problems with gene expression profiling. for instance, based on the simple nearest centroid classifier and via a shrinking strategy, tibshirani et al.  <cit>  offered the so-called "nearest shrunken centroids"  algorithm. by combining two ensemble schemes, i.e. bagging and boosting, dettling  <cit>  introduced the method "bagboosting" as an enhanced version of the regular boosting algorithm. both of these methods were shown being very effective when applied to a few published datasets.

the kernel-induced machine learning is one of the most promising approaches for exploring the potential non-linearity for a given classification or regression problem through the feature space concept. for example, kernel-induced support vector machines  have been successfully applied to a number of learning tasks and are generally accepted as one of the state-of-the-art learning methods. theoretically, lin et al.  proved that a svm with an appropriately chosen kernel and model parameters can approach the bayesian bound of a given problem when the training sample size is large enough. for the gene-selection problem, guyon et al.  <cit>  proposed the method "recursive feature elimination"  to rank the genes with respect to a provided svm, thus the svm can be utilized for microarray data analysis. rfe was shown to be very effective with a linear kernel. however, when the number of genes is large , rfe doesn't function well with a non-linear kernel. this limits the applications of svms to the analysis of microarray data. zhu and hastie  later proposed a framework called kernel logistic regression and suggested a method called "import vector machine" to solve it. however, they also chose the rfe as the strategy to select the significant genes.

as bayesian probability theory can help construct a unified framework for modeling data and facilitate tuning of the involved parameter and/or hyperparameter, developing a proper bayesian probabilistic model is usually beneficial for a machine learning method. mackay  <cit>  introduced a probabilistic evidence framework as a bayesian learning paradigm for neural networks. with the close relationship between neural network methods and kernel-induced learning methods, kwok  <cit>  and gestel et al.  <cit>  developed a bayesian framework for svms and least square support vector machines  respectively, with guidance of the principle of the evidence framework. neal  <cit>  also showed that, as the number of hidden units increases in a bayesian neural network, the prior over the network output converges to a gaussian process  if independent gaussian distributions are used as the priors for network weights and bias. lssvms conceptually are close to svms, except that they use equality constraints instead of inequality constraints and they use a squared error penalty function. getting solution of an lssvm therefore only involves solving a set of linear equations, which though loses the sparseness featured in an svm, it makes an lssvm much easier for an on-line implementation. if we consider the characteristic similarity between the mapping from input nodes/data to hidden units in a neural network and the mapping from input data to a feature space conceptually embedded in an lssvm, it's not surprising that under the gaussian noise assumption, the mean of the posterior prediction made by a gp coincides with the optimum decision function made by an lssvm, whereas a gp offers a more approachable probabilistic model. this fact motivated us to develop a new bayesian learning method named kernel-imbedded gaussian process  for microarray gene expression data analysis based on the gaussian process theory.

the general framework of the kigp method is sketched in fig.  <dig>  where the box bounded by the dotted lines represents the proposed learning component of the method. conceptually, via a gene-selection procedure, a small group of the gene data is selected. through a feature mapping function Ψ, the selected gene data are mapped into a feature space where the optimal classification procedure is processed. with the theory of kernel-induced feature space  <cit> , we do not really do the feature mapping computationally. instead, we train the data via a kernel-imbedded gaussian process by using a kernel function. in the output end, there are basically three consecutive phases, the "kernel parameter fitting phase", the "gene selection phase", and the "prediction phase". given a kernel type, the kigp algorithm finds the fitted kernel parameter in the "kernel parameter fitting phase". after fixing the kernel parameter at the fitted value, it continues with the "gene selection phase" and yields a group of significant genes under some given confidence level. based on the fitted kernel parameter and the selected significant gene data, the algorithm makes a probabilistic prediction for each testing sample in the "prediction phase". the details of the algorithm are discussed in the "methods" section.

the rest of this paper is organized as follows: we show the results from applying the proposed kigp method to simulated datasets as well as real published microarray datasets in the "results" section. the conclusions and the further research discussions are summarized in the "discussions and conclusions" section. in the "methods" section, we provide the mathematical content of the methodology followed by a detailed description of the algorithm.

RESULTS
some terms and acronyms defined in the "methods" section are used in this section. they include "gene-selection vector ", "linear kernel ", "polynomial kernel ", "gaussian kernel ", "normalized log-frequency ", "false discovery rate ", "kernel parameter fitting phase", "gene selection phase", "prediction phase", "misclassification rate ", "average predictive probability ", "leave-one-out cross-validation " and "3-fold cross-validation ". one can refer to the "methods" section for the details.

simulation studies
example 1
this example was designed to illustrate all the key concepts, elements and procedures of the kigp framework introduced in the "methods" section. it consists of two cases. in the first case, the bayesian classifier of the underlying generative model is linear; while in the second case, the bayesian classifier takes a very non-linear form. we set the number of the significant/explanatory genes as two, so we can better graphically display the bayesian classifier and the relative performance of the kigp method. in both of these cases, the number of training samples is twenty. ten training samples were generated from the class "1" and the other ten samples were generated from the class "-1". the number of testing samples is  <dig>  for each sample, the number of investigated genes is 200; the indices of the two underlying explanatory genes were preset as  <cit> . for each case, we independently generated  <dig> sets of training samples from the generative model and ran the simulation on each of them.

 case with a linear bayesian classifier
in this linear case, the two preset significant genes were generated from the bivariate gaussian distribution n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgobgtcqggoaakdawadaqaauaabeqaceaaaeaacqaixaqmaeaacqaixaqmaaaacaglbbgaayzxaagaeiilawyaamwaaeaafaqabegacaaabagaegymaedabagaeyoei0iaegymaedabagaeyoei0iaegymaedabagaegomaidaaagaay5waiaaw2faaiabcmcapaaa@3be0@ for the class "1" and from the bivariate gaussian distribution n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgobgtcqggoaakdawadaqaauaabeqaceaaaeaacqghsislcqaixaqmaeaacqghsislcqaixaqmaaaacaglbbgaayzxaagaeiilawyaamwaaeaafaqabegacaaabagaegymaedabagaeyoei0iaegymaedabagaeyoei0iaegymaedabagaegomaidaaagaay5waiaaw2faaiabcmcapaaa@3dba@ for the class "-1". for those insignificant genes, each of them was independently generated from the standard normal distribution n. the probabilities for the class "1" and the class "-1" were equal. with this generative model, the bayesian classifier for the two classes is a mathematical linear combination of the two prescribed significant genes.

the kigp method with an pk, or with an gk, or with an lk was applied to each of the  <dig> training sets respectively. the prior probability for γj =  <dig> for all j in the gibbs sampling simulations was set at  <dig> . for all the gibbs sampling simulations in this example, we ran  <dig> iterations in both the "kernel parameter fitting phase" and the "gene selection phase" and treated the first  <dig> iterations as the burn-in period. in the "prediction phase", we ran  <dig> iterations and treated the first  <dig> iterations as the burn-in period. the threshold for "fdr" in the "gene selection phase" was set at  <dig> .

for all of the  <dig> simulated training sets, when an pk was the kernel type for the kigp method, the algorithm chose the pk after the "kernel parameter fitting phase" and found both the prescribed significant genes at the end of the "gene selection phase" . however, kigp with an pk resulted with one "false positive" gene in  <dig> of the  <dig> sets. in the prediction phase, the average testing mr for the  <dig> sets correctly found the  <dig> preset significant genes with no "false positive" was  <dig> . it was very close to the bayesian bound . however, the average testing mr for the  <dig> sets with one "false positive" was significantly worse. it was only  <dig> . the average testing mr for all  <dig> sets was  <dig> .

the results of the simulation studies with an lk were very similar to that of the simulations with the pk. in all the simulations, the kigp found the  <dig> preset significant genes , but in  <dig> of the  <dig> sets, the algorithm resulted with one "false positive" as well. this result was exactly same as that from the simulations with the pk. the average testing mr for the  <dig> sets with an lk was  <dig> , almost the same as to that with an pk.

for the results of the stimulation studies with an gk, the algorithm perfectly found the only  <dig> prescribed significant genes in  <dig> of the  <dig> sets . in other  <dig> sets, the kigp identified the  <dig> prescribed significant genes as well as one "false positive". in one other set, the kigp resulted only one of the two prescribed genes  and one "false positive". the mean and the standard deviation for the fitted width of an gk for these  <dig> simulations were  <dig>  and  <dig>  respectively. the average testing mr for the  <dig> simulations with an gk was only  <dig> . based on the testing mr measure, we should use the kigp with either a polynomial kernel or a linear kernel to make any further analysis for this problem.

as an illustration, we specifically display the results from applying the kigp to one of the training sets, in which both an pk and an gk worked very well. for the simulation with the gk, the posterior probability density function  of the width parameter "r" is plotted in fig. 2a, in which its mode was found at around  <dig> . after the "kernel parameter fitting phase", the kernel was fixed as the gk. with the posterior samples obtained in the "gene selection phase", the nlf for each gene was calculated . following the procedure described in the "gene selection phase" subsection, the local fdr with respect to the nlf value was estimated . with the threshold for fdr set at  <dig> , the cutoff value for nlf was  <dig>  and we found that only the two prescribed genes  were found significant. the contours of the posterior predictive probabilities for the class "1" are plotted in fig. 3d, where the x-axis is the value of the gene  <dig> and the y-axis represents the value of the gene  <dig>  in fig. 3d, the numbers associated with the contour curves are probabilities; the asterisks denote the positive training samples and the circles present the negative training samples; the dotted line shows the bayesian classifier. the mr of the independent testing set for this simulation was  <dig> . for the simulation with an pk, after the "kernel parameter fitting phase", the estimated posterior probability masses for the discrete degree parameter "d" were prob =  <dig>  and prob =  <dig>  respectively. with the highest estimated posterior mass at d =  <dig>  we accordingly fixed the kernel as the pk. with the same gene-selection procedure described in the simulation with the gk, the two prescribed genes again were found as the only two significant genes . the contour plot of the posterior predictive probability for the class "1" is drawn in fig. 3f. the testing mr was  <dig>  for this simulation. the performance of the kigp with the pk was very similar to that of the kigp with an lk . both of them behaved like the linear bayesian classifier. as a benchmark comparison, we further applied a regular svm/rfe  to each of the  <dig> simulated training sets. in fact, rather than using a cross-validation procedure, there is no effective way for a svm/rfe to set the model parameter  and to select the number of significant genes. technically, it is also important to mention that the svm/rfe is not proper for microarray data analysis with a kernel type having variable parameter such as a gaussian kernel. nevertheless, for this linear example, we applied a svm/rfe with an lk to the datasets and preset the box constraint as  <dig>  the obtained results were similar to those of the kigp with an lk case. in  <dig> out of the  <dig> sets, the gene  <dig> and the gene  <dig> were ranked as the top  <dig> genes in the significance gene list. however, in the remaining  <dig> of the  <dig> sets, the gene  <dig> was ranked as the top significant gene but the gene  <dig> was ranked in the 3rd place and in the 5th place respectively. for the prediction with rfe, we used the top genes including the gene  <dig> and the gene  <dig>  the resulted average testing mr for all  <dig> sets was  <dig> . even in this linear case, the kigp with an lk or the pk outperformed the svm/rfe with an lk in an automatic fashion. more importantly, the svm/rfe only made a binary prediction of the class for each testing sample, while the kigp gave a probabilistic prediction on the certainty of the decision. furthermore, the proposed kigp framework offered the posterior distribution for each model parameter as well as a universal significance measure  for each investigated gene at the end.

in the majority of the simulations, the kigps found the two preset significant genes in this linear case. they all performed very close to the bayesian bound when the two preset genes were perfectly found. since the kigp with the pk gave the best average testing mr, we should use it for any further analysis.

 case with a non-linear bayesian classifier
in this non-linear case, the two preset significant genes were generated from a mixture gaussian distribution with equal probability on n and n for the class "1" and from an independent normal distribution n for the class "-1".  <dig> and i <dig> denote the one-vector and the identity matrix respectively  of the "methods" section). for those insignificant genes, each of them was independently drawn from the standard normal distribution n. the probabilities for the two classes were equal. the bayesian classifier given the two significant genes looks like two parallel lines  and the bayesian bound for the mr is  <dig> . we applied both the linear probit regression method proposed by lee et al.  <cit>  and an kigp with an lk  to the  <dig> training sets. unsurprisingly, both of them failed badly in terms of finding the correct significant genes and making optimal class predictions for this non-linear case.

for the  <dig> simulated training sets, when an pk was the kernel type for the kigp method, the algorithm chose the pk for  <dig> sets and the pk for the other  <dig> sets after the "kernel parameter fitting phase". only in  <dig> of the  <dig> sets, the kigp with the pk perfectly found the two prescribed genes as the only significant genes. the average testing mr for these  <dig> sets was horrendous. however, for those two sets correctly found the two preset significant genes, the testing mrs were both fairly close to the bayesian bound.

the results of the simulations with an gk were much better. for all of the  <dig> sets, the kigp successfully found the  <dig> preset significant genes . the kigp also resulted with one "false positive" for  <dig> sets as well. the mean and the standard deviation of the fitted width of an gk for these  <dig> sets were  <dig>  and  <dig>  respectively. in the "prediction phase", the average testing mr was  <dig>  for the  <dig> sets correctly found the  <dig> preset significant genes. it was very close to the bayesian bound . the average testing mr was  <dig>  for the  <dig> sets with one "false positive". the average testing mr for all  <dig> sets was  <dig> .

as an illustration, we depict the results from applying the kigp to one of the training sets, in which both an pk and an gk worked well. the procedure and all settings of the simulations and the legends of the figures were same as described in the linear case. we first applied an kigp with an gk to the training set. the mode of the posterior pdf of the width parameter was found at around  <dig>  after the "kernel parameter fitting phase" . with the gk, the cutoff value of  <dig>  for nlf was obtained at the end of the "gene selection phase". based on the nlf statistic, the two prescribed genes were successfully retrieved  and the kigp performed well with mr =  <dig>  . it was very close to the bayesian bound. for the simulation with an pk, the posterior probability masses of the degree parameter were prob =  <dig>  and prob =  <dig>  respectively. the nlf plot for each gene and the relative cutoff line for the nlf are both displayed in fig. 4e. the two prescribed genes were discovered. the performance of the kigp with the pk was very well with mr =  <dig>  . it was very close to the bayesian bound too.

we tried to apply the regular svm with rfe to this example as we did in the linear case, but svm/rfe failed to work with an lk, nor an gk , nor an pk. the key problem might be due to the large dimension  of this example. comparing the kigp method to the svm/rfe in this non-linear case, besides those beneficial properties of the kigp that we already observed in the linear case, the kigp method particularly shows its better adaptability for non-linear problems. in summary, owing to the non-linear setting of this case, all linear methods were not applicable. the regular svm/rfe approach also did not work. on the contrary, in terms of the testing mr measure, the kigp with an gk provided a performance very close to the bayesian bound. comparatively, the kigp with an pk seems to be less robust and consistent than the kigp with an gk for a non-linear problem in general.

as a side note, it's worth pointing out that the posterior pdf of the width parameter seems to disclose some special nature of a dataset for a classification problem when one applies the kigp with an gk. for instance, we observed that if the underlying bayesian classifier can be well approximated by a linear function, the mode  of the pdf of the width parameter significantly moves to the right side of the value  <dig> ; whereas if the bayesian classifier is very non-linear, it moves to the left side of the value  <dig> .

example 2
we further designed this example to demonstrate the effectiveness of the proposed kigp method when the number of investigated genes is large, especially for a problem with a very non-linear bayesian classifier. a total of  <dig> genes in a simulated microarray experiment and  <dig> of them were preset as the significant genes with indices  <cit> . these  <dig> significant genes were generated from the mixture gaussian distribution with equal probability on n and n for the class "1" and from the gaussian distribution n for the class "-1", where  <dig> denotes a vector with  <dig> "0" elements. the probabilities for the two classes were equal. the rest of other insignificant genes were independently generated from the standard normal distribution n. similar to the first example, the number of training samples is  <dig>   <dig> of which were generated from the class "1" and the other  <dig> samples were generated from the class "-1"; the number of testing samples is 5000; we independently generated  <dig> sets of training samples from the model and ran the simulation on each of them.

the procedure for this example is same as in the non-linear case of the first example. the prior probability for γj =  <dig> was set at  <dig> . for both the "kernel parameter fitting phase" and the "gene selection phase", we ran  <dig> iterations and treated the first  <dig> as the burn-in period, and for the "prediction phase", we ran  <dig> iterations and treated the first  <dig> as the burn-in period.

for the  <dig> simulated training sets, when an pk was the kernel type for the kigp method, the algorithm chose the pk in  <dig> out of  <dig> sets. only in  <dig> of these  <dig> sets with the pk, the algorithm found all  <dig> significant genes. however, for the  <dig> sets with an gk, the  <dig> prescribed genes were all found in each of the  <dig> sets. there was one "false positive" being brought into the significant group in one set. there was almost no error for the testing samples and extremely close to the bayesian bound.

in fig.  <dig>  we show the simulation results from applying the kigp method to one of the training sets. fig. 5a and 5b are for the simulation with an pk, whereas fig. 5c and 5d are for the simulation with an gk. based on fig. 5a, the pk was chosen after the "kernel parameter fitting phase". after the "gene selection phase", with the yielded cutoff line for the nlf, the kigp found all  <dig> prescribed significant genes and one "false positive" . the mr of the testing set was  <dig> . in the simulation with an gk, the mode of the posterior pdf for the width was found at around  <dig>  . with the gk, after the "gene selection phase", all  <dig> prescribed genes were correctly found with no "false positive". with the found significant genes, we did not find any testing error in the "prediction phase". based on the testing mr, we should choose the gk for further analysis. this example not only illustrates the usefulness of the proposed algorithm for problems with very large number of investigated genes, but also reinforces all the arguments we have made for the bayesian kigp framework in the last example.

real data studies
following the similar procedure executed in the simulated studies, the kigp was applied to four published microarray gene expression datasets. a brief summary of these datasets is provided in table  <dig> and the experimental details are extracted and described below.

acute leukemia data
the leukemia dataset was originally published by golub et al.  <cit> , in which the bone marrow or peripheral blood samples were taken from  <dig> patients with either acute myeloid leukemia  or acute lymphoblastic leukemia . the data was divided into two independent sets: a training set and a testing set. the training set consists of  <dig> samples, of which  <dig> are all and  <dig> are aml. the testing set consists of  <dig> samples, of which  <dig> are all and  <dig> are aml. this dataset contains expression levels for  <dig> human genes produced by affymetrix high-density oligonucleotide micorarrays. the scores in the dataset represent the intensity of gene expression after being rescaled. by using a weighted voting scheme, golub et al. made predictions for all the  <dig> testing samples and  <dig> of them were reported being misclassified.

the kigp with an gk, an pk, and an lk was applied to the training dataset  respectively. the prior parameter πj for all j was uniformly set at  <dig> . in both the "kernel parameter fitting phase" and the "gene selection phase", we ran  <dig> iterations and treated the first  <dig> iterations as the burn-in period; and in the "prediction phase", we ran  <dig> iterations and treated the first  <dig> iterations as the burn-in period.

for the simulation with an pk, the resulted posterior probability masses of the degree parameter d are prob =  <dig>  and prob =  <dig> . with the pk,  <dig> genes were identified as "significant" at  <dig>  significance level . using the pk and the found significant genes, we made predictions for the  <dig> testing samples. we then ran a leave-one-out cross-validation  for the  <dig> training samples. this "loose" loocv procedure was however only involved in the "prediction phase". since the fitted kernel parameter and the significant genes chosen from the first two phases had already contained the most information of the whole training dataset, it was not a proper validation measure for kernel type competition. more properly, we further did a rigorous 3-fold cross-validation  that included all  <dig> phases of the proposed algorithm . this whole procedure was then repeated for the simulation with an gk and with an lk respectively. all the results are summarized in table  <dig> 

the columns labeled by "test" are for the independent tests.

the columns labeled by "cv " are for the rigorous 3-fold cvs .

the columns labeled by "loocv " are for the loose loocvs .

*: index of the genes not reported in  <cit> .

in table  <dig>  the kigp with an lk gave the best testing performance: only  <dig> error was found. we found that many publications  reported the same testing error for this dataset as well. only zhou et al.  <cit>  reported  <dig> testing error. however, based on the results of  <cit> , the testing app was only  <dig> , which is much worse than that of the kigp with an lk . we suspect that this misclassified testing sample by kigp/lk may be phenotyped incorrectly.

the significant genes found by the kigp with an lk are reported in table  <dig> and the nlf plot is plotted in fig. 8a. in table  <dig>  the genes with asterisks  are those not reported by the original paper  <cit> . the heat map of the found significant genes for all the samples  exhibits a very good consistency between the training set and the testing set . we realize that the posterior pdf of the width parameter of an gk can disclose some special nature of the feature space for a given dataset and problem. fig. 9a illustrates the dominant linearity of this case. another issue that needs to be addressed is that, if the number of the available samples is small , the measure of "the number of testing errors" may have noticeable bias. instead using "the number of testing errors", the measure of app is more reliable under this scenario. in this case, it's easy to see in table  <dig> that, the app of the rigorous 3-fold cv is very consistent to that of the independent testing, whereas the "loose" loocv is not. this gives a good example on how a "loose" loocv brings in the so-called "gene-selection bias".

small round blue-cell tumor  data
the srbct data was originally published by khan et al.  <cit> . the tumor types include ewing family of tumors , rhabdomyosarcoma , neuroblastoma  and non-hodgkin lymphoma . the dataset of the four tumor types is composed of  <dig> genes and  <dig> samples, while  <dig> blinded testing samples are available. in this study, we only focused on two classes, ews and nb. thus, there are only  <dig> training sample  and  <dig> testing samples .

we applied the same procedure as we did in the leukemia data case to this dataset. the computational settings were also almost the same except that πj for all j was set at  <dig> . the overall performance report is given in table  <dig>  the kigp with the pk performed best with respect to both the independent testing and the rigorous 3-fold cv. the  <dig> significant genes found by the kigp with the pk are listed in table  <dig>  the nlf plot is shown in fig. 8b. the heat map of the significant genes for all samples is drawn in fig.  <dig>  the posterior pdf of the width parameter of the gk is depicted in fig. 9b. in table  <dig>  the genes with asterisks  are those not reported by the original paper  <cit> . based on the heat map plot , except the gene  <dig>  the other  <dig> genes  are consistent through the training samples to the testing samples.

all the captions are same as in table  <dig> 

"ann" stands for the "artificial neural network" method used by the paper  <cit> 

*: index of the genes not reported in <cit> .

similar to the leukemia data case, the app of the rigorous 3-fold cv is very consistent to that of the independent testing while the "loose" loocv is rather biased. we also found that the kigp with the pk outperformed the artificial neural network  method in terms of app .

breast cancer data
the hereditary breast cancer data used in this example was published by hedenfalk et al.  <cit> , in which cdna microarrays were used in conjunction with classification algorithms to show the feasibility of using the differences in global gene expression profiles to separate brca <dig> and brca2/sporadic.  <dig> breast cancer tumors were examined:  <dig> with brca <dig>   <dig> with brca <dig> and  <dig> considered sporadic.  <dig> genes were investigated for each sample. we labeled the samples with brca <dig> as the class "1" and others as the class "-1".

the computational procedure and settings of this example are same as those in the srbct case except that there is no independent testing. in order to highlight the "gene-selection bias" problem, besides running a rigorous 3-fold cv procedure to measure the performance of a kernel type, we further added a "loose" 3-fold cv procedure . the overall performance report is provided in table  <dig>  based on the rigorous 3-fold cv, we selected the gk as the fitted kernel for this dataset. the posterior pdf of the width parameter is shown in fig. 9c. we list the  <dig> significant genes found by the gk in table  <dig>  there are two genes  that were not reported by the original paper  <cit> . the nlf plot is shown in fig. 8c.

all the captions are same as in table  <dig>  except that there is no independent testing.

the columns labeled by "cv " are for the loose 3-fold cvs .

*: index of the genes not reported in <cit> .

it's not surprising to find that the general performance of the kigp with an lk or the pk was not good since we notice that there is an unusual local peak on the left side of the posterior pdf of the width parameter r . this local peak usually implies the existence of non-linearity in the data for the given problem. a fairly logical reason for this phenomena can be found in  <cit> , in which efron showed that the empirical null of this dataset was significantly different from its theoretical null based on a large-scale simultaneous 2-sample t-test and he argued that this was probably due to the fact that the experimental methodology used in the original paper had induced substantial correlations among the various microarrays.

this example is also a good case to show the "gene-selection bias" problem. in table  <dig>  with the selected significant genes found by training all the available samples, the performance of the kigp with an lk from the "loose" 3-fold cv was much better than that of the kigp with a gk. however, from the results of the rigorous 3-fold cv, the kigp with an lk gave very poor predictive performance, while the kigp with an gk still worked reasonably well.

colon data
this dataset was originally published by alon et al.  <cit>  and we noticed that dettling  <cit>  has reported the performances of many state-of-the-art learning methods that had been applied to this dataset. we applied the kigp method to this dataset so as to have a more side-by-side performance comparison with other methods.  <cit>  did a pre-filtering of genes based on the wilcoxon test statistic and only ran all the simulations within a 200-gene pool. however, based on the reported procedure, it should not bring in much gene-selection bias. therefore, it forms a good dataset for comparing different microarray data analysis methods.

we applied the proposed kigp to the whole dataset without any pre-filtering to preclude any possible gene-selection bias. the computational procedure and settings of this example are very similar to those in the srbct case except that there is no independent testing. as for the cross validation procedure, we ran  <dig> independent simulations and reported the average of the results to decrease the possible data split bias. the procedure for the data splitting is described in the "kernel type competition" subsection. we did this with each kernel type: pk, gk and lk, respectively. the resulted performance and the performances of other methods  are summarized in table  <dig>  we found that the mr of the kigp with an gk is very close to that of the best classifier  shown in the list. it is worth mentioning that pam's performances were ranked as average to significantly worse than  <dig> other methods, especially comparing to kernel-induced methods such as the svm for other published real datasets . the kigp method with an appropriate kernel is at least not worse than the svm.

for the kigp with each of the three different kernel types , we took  <dig> independent rigorous 3-fold cvs to  <dig> samples  and reported the average mr. for the  <dig> referred classifiers, the results and the experimental details were originally reported by <cit> .

rf: "random forests"

pam: "nearest shrunken centroids"

dlda: "diagonal linear discriminant analysis"

based on the mr of the rigorous 3-fold cv, we selected the gk as the winning kernel type. we then ran kigp with a gk to all the available samples. after the "kernel parameter fitting phase", with the posterior pdf of the width parameter , we fixed the kernel as the gk. the resulted nlf plot with the gk after the "gene selection phase" is depicted in fig. 8d. the indices of the  <dig> identified significant genes are provided in table  <dig> 

another interesting finding of this experiment is that, based on the results of the "loose" cv, the kigp/lk performed better than the kigp/gk for this dataset. however, with a multiple rigorous 3-fold cv, it turned out that kigp/gk was the more reliable kernel type for this problem. when we checked the heat map of the significant gene set identified by the kigp/gk , we found that a few samples, particularly including the sample # <dig>  # <dig>  # <dig>  # <dig> and # <dig>  are significantly different from other samples in their labeled class. however, they are very consistent to those samples in their opposite class. in fact, these samples were also almost always misclassified by the kigp in the multiple rigorous 3-fold cv tests. we therefore suspect that these samples are mistakenly phenotyped. we think that this is probably the reason why all other learning methods referred in table  <dig> do not perform well for this colon dataset. this also supports the nature of a kigp/gk being less sensitive to the mislabeled training samples than a kigp/lk.

discussions and 
CONCLUSIONS
this work was motivated by the data analysis challenges posed by microarray gene expression experiments and the mathematical beauty of the kernel-imbedding approach in their ability to solve a non-linear classification problem in the feature space rather than in the observation space. we have presented a unified supervised learning model named kernel-imbedded gaussian process  under a hierarchical bayesian framework. this model was specifically designed for automatic learning and profiling of microarray gene expression patterns. in the simulated examples, without knowing anything of the underlying generative model, the proposed kigp method has been shown to perform very close to the bayesian bound not only in the linear case, but also in the non-linear case.

with a probit regression setting and the introduction of latent variables, the kigp model was set for a binary disease classification problem. an algorithm with a cascading structure was proposed to solve this problem and a gibbs sampler was built as the mechanical core to do the bayesian inferences. given a kernel type such as a gaussian kernel or a polynomial kernel, with the training data as input, the fitted parameter of the kernel type and a set of significant genes will be the output of the algorithm. the algorithm also offers a probabilistic class prediction for each sample. the proposed kigp can explore not only the linear but also the potential non-linear relationship between the target disease and its associated explanatory genes. comparing to the regular svm , the proposed kigp has two advantages. first, the probabilistic class prediction from the kigp could be insightful for borderline cases in real-world applications. second, the kigp method has implemented specific procedure for tuning the kernel parameter  and the model parameters . tuning parameters has always been one of the key issues for non-linear parametric learning methods. the results of the simulated examples show that the kigp significantly outperformed the regular svm method with rfe as a gene selection strategy in a non-linear case and it provided more useful information, such as the posterior pdf of the parameters, for further prediction and analysis as well. computationally, kigp is also proven to be robust, therefore it's very amenable to be adopted to a gibbs sampling system. both the simulated examples and the real data studies have demonstrated the effectiveness of the proposed method.

there are still a few interesting problems left for future research. for example, although the kigp in this study is developed to only solve a binary classification problem, it can easily be extended to a multi-class classification problem based on a multinomial probit regression setting. on the other hand, some other problems are not only challenging but also critical. first, the kernel type competing problem is still a tough issue. the use of the predictive fit measure method discussed in the "methods" section is simple to formulate, but it may be problematic when the independent testing set is not available and/or there are many candidate kernel types. we are currently working on addressing this issue by implementing a reversible jump markov chain monte carlo  algorithm as a simultaneous integrative approach for kernel type selection within the kigp framework. another important problem is the independent prior assumption on elements of the gene-selection vector γ and the "component-wise drawing" strategy to sample it. although this will eventually lead to convergence based on the mcmc theory, it may take a very long time if the true underlying explanatory genes are highly correlated with each other. therefore, a proper kernel-induced clustering algorithm under some proper generative model will definitely be helpful on this regard. furthermore, if a more appropriate prior for γ can be found, the dependency between genes can be simply taken into account to the whole framework by sampling γ not in a component-wise fashion but in a block-wise fashion instead. this will then dramatically increase the speed for reaching convergence.

interestingly, building a kernel based on the feature of the given data and the classification problem is the ideal way to take full advantage of the kernel-induced learning algorithm. for example, if an appropriate generative model is available for the given dataset, a class of kernels named "natural kernels" is applicable in this context. this problem and the pre-clustering problem mentioned above seemingly share many fundamental elements. however, the further investigation of this is beyond the scope of this paper.

