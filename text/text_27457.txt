BACKGROUND
with the increased availability of assembled genomes, methods that can analyse whole genome data and reconstruct phylogenetic trees based on large sctructural variations become increasingly relevant. a problem of great interest is the reconstruction of ancestral genomes based on gene order data. this is a classical problem in the field of genome rearrangements, where a large amount of research has been devoted, and still poses many challenges. in this problem, we are given a phylogenetic tree with extant genomes at its leaves, and need to reconstruct the gene orders at the internal nodes of the tree, corresponding to ancestral genomes.

we can broadly divide approaches of solving this problem in two categories. the first is a parsimonious approach, called event- or distance-based, were a rearrangement distance is given and the aim is to find ancestral genomes that minimize the length of the tree, defined as the total number of rearrangement events on all edges of the tree. since bpanalysis  <cit> , the first proposed method, which was based the breakpoint distance, many other distance-based methods were developed, with different distances, such as the reversal distance , the double cut and join  distance  <cit>  , and the single cut or join  distance  <cit>  , just to cite a few examples.

another category can be called homology-based, where methods usually do not apply rearrangement models directly, but instead treat conserved structures between the input genomes, such as conserved adjacencies or gene clusters, as binary characters . these characters can also have weights that represent a confidence or probability measure, and ancestral genomes are found by optimizing an objective function that might combine factors such as maximization of weights or probabilities, and minimizing character changes in the tree. notable examples include the pioneer infercars  <cit> , as well as gapadj  <cit> , anges  <cit> , pmag +  <cit> , procars  <cit>  and physca  <cit> .

in our recent contribution to this field, we proposed a method that combines ideas from homology-based methods, namely adjacency weights, with the dcj rearrangement model, by defining intermediate genomes, genomes that arise in optimal dcj scenarios. we obtained promising results with this aproach, both in terms of running time and quality of the ancestral reconstruction  <cit> .

our previous approach, as well as most of the aforementioned methods , assume that all the input genomes have the same gene content, with just one copy of each gene, which is of course not a very realistic assumption, but it does make the problem much less complicated. in recent years, the focus has been shifted to include also gene content operations, such as gene insertion and deletions. mgra and pmag +, for instance, are updates of previous methods that dealt only with same gene content genomes.

in this direction, in this paper we extend the intermediate genome definition to unequal gene content genomes, by using the dcj indel model  <cit> . using this model, we study theoretical in “preliminaries”, “intermediate genomes” and “ancestral reconstruction” sections and practical aspects in “ancestral reconstruction algorithms” and “results” sections. the complexity of the problem is unknown but we show that, depending on certain features of breakpoint graph we know how to solve the problem in polynomial time and in all other cases we have a ftp algorithms when we parameterize by the number c of the chromosomes. the ideas from this studying are partially used inspiring a description of a heuristic that has shown very good results regarding quality and time. in the last “discussion” and “conclusion” sections we discuss obtained results.

preliminaries
genes and genomes
a gene
g is a sequence of two elements g
t
g
h or g
h
g
t. so, g
t
g
h and g
h
g
t represent the same gene g with different orientation. we call g
h and g
t
extremities, g
t is a tail and g
h is a head of g. two different genes don’t share extremities. if g is a set of genes, denote g±=∪g∈g{gt,gh}. so, if |g|=n, then |g±|=2n.

a chromosome
c is a sequence of genes that can be linear or circular. denote by v
c the set of genes in c. if c is linear we represent it by adding a telomere, represented by the symbol ∘, at its endpoints. an adjacency in c is a pair x
y≡y
x such that x and y are in vc±∪{∘}, implying that two genes are consecutive in c. if x or y is a telomere, this represents an extremity of a linear chromosome, and this type of adjacency is called a telomeric adjacency.

a genome is a set of chromosome and it is represented by the union of adjacency sets of their chromosomes. a genome is circular  if all its chromosomes are circular . for two genomes a and b, if v
a=v
b, we say that they have the same gene content. conversely, if v
a≠v
b, they have unequal gene content.

dcj operation and the breakpoint graph
let a be a genome, and x
y≠v
w two adjacencies in a. a double cut and join operation   <cit>  on genome a is an operation that cuts two adjacencies of a and joins the free extremities in a different way. many common rearrangement operations, like reversals and translocations, can be represented by a dcj. formally, a dcj transforms a into genome a−{x
y,v
w}∪{v
y,x
w}. there is also the special case of a−{x
y}∪{∘x,∘y} and the reverse case a−{∘x,∘y}∪{x
y}, for x,y≠∘. for two genomes a and b with same gene content, the dcj distance between a and b is the minimum number d
dcj of dcj operations that transforms a into b. the distance d
dcj can be found with the breakpoint graph of a and b, denoted by bp, which is an edge-colored graph g=, that is, the vertices are the gene extremities, and edges the adjacencies of both genomes . edges from a have one color and edges from b have a different color. by definition, the breakpoint graph is collection of color alternating cycles and paths. figure  <dig> shows and example of a breakpoint graph.
fig.  <dig> breakpoint graph bp of genomes a={∘1t,1h2t,2h3t,3h4t,4h∘,∘5t,5h6t,6h7t,7h∘} and b={1h2h,2t3h,3t4t,4h1t,∘6t,6h5t,5h7h,7t∘}. edges of a are green, of b are blue





the dcj distance is given by 
  <dig> ddcj=n−c+peven <dig>  


where n=|g| is the number of genes, c and p
even are the number of cycles and the number of paths with even number of edges in bp respectively, which can be found in linear time  <cit> .

for genomes a and b with unequal gene content , extra operations are required for inserting and deleting genes in a in order to transform a into b. genes in v
b−v
a are called unique genes of b, and conversely v
a−v
b is the set of unique genes of a. an insertion in a consists in inserting a contiguous sequence of genes of v
b−v
a in a, and a deletion in a is the inverse operation, i.e, removing a contiguous sequence of genes of v
a−v
b from a. an indel is a general expression meaning an insertion or a deletion. the dcj-indel distance between a and b is the minimum number of dcjs and indels required to transform a into b, and it is denoted as d
dcj
ind. this distance can also be found in polynomial time, using two different approaches . here, we use compeau’s approach, which is based creating prosthetic chromosomes  <cit>  in each genome, formed by the unique genes of the other, creating two new genomes with the same gene content.

dcj distance for unequal content genomes
for genomes a and b with unequal gene content, let g=va∪vb be the set of genes from both genomes. the breakpoint graph has a similar definition as before, changing only the vertex set, that is, bp=, which means that new types of vertices and paths will be present.

a vertex a in bp is a-open if a∉va±, it is b-open if a∉vb± and it is not-open otherwise. as well as telomeres, a missing gene in a or b appears as a endpoint of a path as we can see in fig.  <dig>  for a path p in bp, we say that p is even if the number of edges of p is even and it is odd otherwise; p is not-open if its endpoints are both not-open; p is an aa-path  if its endpoints are both a-open ; p is an ab-path if it has one a-open and one b-open endpoint; p is an a-path  if it has one a-open  and one not-open endpoint. define p
ab as the number of ab-path and pao as the number of odd a-paths. other notation for the number of odd/even-length paths  are defined analogously. when comparing two genomes a and b, a singleton is a circular chromosome c composed only by unique genes from one of the genomes, that is, v
a∩v
c=∅ or v
b∩v
c=∅. the number of singletons for a and b is denoted by s
i
n
g. clearly, we can obtain s
i
n
g in polynomial time.
fig.  <dig> breakpoint graph bp of circular genomes g
a= and g
b=, with adjacency sets a={1h3h,3t5t,1t5h} and b={1h2t,2h3t,3h4t,4h1t}. edges of a are green, and of b are blue. there is one aa-path and two ab-paths




a completion for a and b is a pair of genomes a
′ and b
′ obtained from a and b by adding artificial singletons  in a and b in such way the va′=vb′=g.

compeau  <cit>  showed that the dcj-indel distance is given by 
  <dig> ddcjind=mina′,b′ddcj+sing. 


a completion a
′ and b
′ for a and b such that minimize d
dcj is called optimal.

in order to find optimal completions, consider the following definitions. for a set a, a matching
m is a collection of disjoint subsets of a. m is a perfect matching of a or simply a perfect matching if the union of all sets in m is a. m is a k-matching if every set in m has k elements. a completion can then be seen as a perfect 2-matching of a-open vertices joined with a perfect 2-matching of b-open vertices in bp. in fig.  <dig>  we have an example of a breakpoint graph and a completion.
fig.  <dig> the unique optimal completion c of bp from fig.  <dig>  where a-open  vertices are joined by green  double edges, closing the aa-path and linking both ab-paths, which makes d
dcj
ind=n−c= <dig>  the orange edges m
′={1h2t,2h4h, 4t3h,3t1t,5t5h} form a set of non-crossing chords covering all vertices of c. by claim  <dig>  m
′ leads to an intermediate genome. notice that s={5t5h} is an artificial singleton, that is, a circular chromosome with only unique genes of a. therefore, m=m
′−s={1h2t,2h4h,4t3h,3t1t}, representing the circular chromosome , is an intermediate genome. m is present in the optimal scenario s={m0=a,m1=, m
2=,m
3=b}, composed by one deletion, one insertion, and one reversal 




let c be the set of all completions for a and b. if n
a=|v
b−v
a| and n
b=|v
a−v
b| are the number of unique genes in both genomes, then bp has 2n
a
a-open vertices and 2n
b
b-open vertices. since there are !! different 2-matchings for the a-open vertices and !! different 2-matchings for the b-open vertices, we have that 
  <dig> |c|=!!·!!, 


which is exponential on the number of unique genes of a and b. however, an optimal completion can be found in polynomial time, which implies, since we can obtain s
i
n
g in polynomial time, that  can also be computed in polynomial time  <cit> .

enumerating all optimal completions
the intuition behind finding an optimal completion is that eq.  is minimized when the number of cycles and even paths of bp is maximized. this guides the linking of components with a- and b-open vertices into creating as many cycles and even paths as possible. therefore, aa-paths and bb-paths are always closed directly by linking their own a- or b-open vertices, since each becomes a cycle. ab-paths are usually linked in pairs, creating one cycle per pair. a-paths are also paired, ideally two paths with opposing parity, since this creates an even pair, and similarly for the b-paths. in many cases, this simple strategy is already enough to find optimal completions. unfortunately, this can get more complicated when in some cases a triplet of components, specifically one a-path, one ab-path and one b-path can be linked in an optimal completion. in the following, we enumerate the space of all optimal completions, summarizing the results introduced by compeau  <cit> .

let c∗ be the space of all optimal completions for a and b. using results from  <cit>  we define a hypergraph ℋ representing c∗. the vertices represent components of the breakpoint graph, and hyperedges of ℋ represent linked components that form a new component in a completion. in any completion, components without open vertices are not linked with other components. also, aa-paths  become cycles by adding an edge between the two a-open  vertices in any optimal completion. therefore, these components are not in ℋ.

in the following definitions, we use the notation of cartesian product, but exclude pairs of identical elements, since a component can not be linked to itself. let v be the set of vertices of ℋ. v is the union of the following sets, representing components of the bp: Λ
o, Λ
e, Υ, Γ
o and Γ
e, the set of odd a-paths, even a-paths, ab paths, odd b-paths and even b-paths respectively. consider the set of hyperedges of ℋ that is the union of sets t
1=Λ
o×Λ
e; t
2=Γ
o×Γ
e; t
3=Υ×Υ; t
4=Λ
o×Λ
o; t
5=Λ
e×Λ
e; t
6=Γ
o×Γ
o; t
7=Γ
e×Γ
e; t
8=Λ
o×Υ×Γ
o; t
9=Λ
o×Υ×Γ
e; t
10=Λ
e×Υ×Γ
o; t
11=Λ
e×Υ×Γ
e. 
if p
ab is even, pao≤pae and pbo≥pbe, an optimal completion is any perfect matching using hyperedges in t
1∪t
2∪t
3∪t
5∪t
 <dig> 

if p
ab is even, and pao≥pae and pbo≤pbe, an optimal completion is any perfect matching using hyperedges in t
1∪t
2∪t
3∪t
4∪t
 <dig> 

if p
ab is odd, and pao≤pae and pbo≥pbe, an optimal completion is any perfect matching using only one hyperedge in t
 <dig> and hyperedges t
1∪t
2∪t
3∪t
5∪t
 <dig> 

if p
ab is odd and pao≥pae and pbo≤pbe, an optimal completion is any perfect matching using only one hyperedge in t
 <dig> and hyperedges in t
1∪t
2∪t
3∪t
4∪t
 <dig> 

if pao<pae and pbo<pbe, an optimal completion is any perfect matching using hyperedges in t
1∪t
2∪t
3∪t
5∪t
7∪t
 <dig> 

if pao>pae and pbo>pbe, an optimal completion is any perfect matching using hyperedges in t
1∪t
2∪t
3∪t
4∪t
6∪t
 <dig> 




claim 1
let n=|g| and c the sum of the number of chromosomes in a and b. then, there are at most !)2·o different ways to choose a 3-matching in an optimal solution in ℋ.

proof
each set with three components represents one a-path, one ab-path and one b-path. since each a-path and b-path has one telomere each and we have c chromosomes, there are i≤c triples in a solution. considering that i= <dig> …,c, there are at most 
 n0+n1+…+nc=o  different ways to choose a set of ab-path to obtain triples in a optimal completion.

once chosen a set of ab-path and we have to choose no more than 2c
a-path and no more than 2c
b-path. so, we have a total of no more than !)2·o different ways to choose a 3-matching in an optimal solution in ℋ. □

methods
in our previous approach, we used the concept of intermediate genomes to propose a new ancestral reconstruction method, in the context of genomes with same gene content  <cit> . we extend this approach here to genomes with unequal gene content, by dealing with gene insertion and deletion events.

in the following sections, every key aspect of the proposed method will be explained. basic properties of intermediate genomes are described, based on existing results, and new properties for the case of genomes with unequal gene content are shown. then, we show how the classic problems of small phylogeny and genome median can be reformulated adding intermediate genome constraints, also proposing a new problem, the maximum weight intermediate genome, that is at the core of our method.

practical aspects such as estimating tree branch lengths and finding adjacency weights at each internal node of the tree are described. finally, we describe the main algorithm, that iteratively reconstructs ancestors at internal nodes in a bottom-up approach, by using intermediate genome properties and adjacency weights.

intermediate genomes
in this section, we review some key combinatorial properties of intermediate genomes and extend the definition for genomes with unequal gene content, assuming that gene deletions and duplications have occurred.

basic properties of intermediate genomes
an optimal dcj scenario between two genomes a and b is an ordered list of genomes s= where k=d
dcj, a=m
 <dig>  m
k=b and m
i can be obtained from m
i− <dig> by applying a dcj operation, for i= <dig> …,k. any genome mi∈s is called an intermediate genome of a and b.

optimal dcj scenarios can be found by dealing with each component in the breakpoint graph independently. a scenario that follows this strategy will be called independent component scenario. there are also optimal scenarios where a dcj operations may act on two different components, specifically two even paths, but these are very rare  <cit> . currently, we ignore recombination of even paths, in order to simplify the combinatorial analysis. in other context, a method was proposed to include this type of events  <cit> , and we plan to add a similar extension to our framework as well.

given breakpoint graph bp, a circular breakpoint graph can be obtained by transforming the paths into cycles as follows: i) to for each even path, add a new vertex ∘ and connect both extremities of the path to this new vertex; ii) for each odd path, add two new vertices ∘ <dig> and ∘ <dig> with and edge connecting both, and connect each extremity of the path to a different new vertex. this circular version of the breakpoint graph is composed only of cycles and it preserves the dcj distance equation given by eq. , adjusting n to n+k/ <dig> to account for the extra number of k artificial vertices added  <cit> .

the main property of intermediate genomes on independent component scenarios is given by the following theorem:

theorem  <dig> 
given genomes a and b with the same set of genes, a genome m is an intermediate genome of a and b in an independent component scenario if and only if the edges of m are non-crossing chords in the cycles of the circular bp, and m covers all vertices of bp.

in practice this makes it very easy to verify if a given genome is an intermediate genome, or even to create one given a choice of possible adjacencies, a key aspect of our ancestral reconstruction algorithm.

intermediate genomes for dcj indel scenarios
the definition of intermediate genomes for genomes with unequal content is the same as the original one, just considering optimal dcj-indel scenarios, instead of dcj only scenarios.

it is somewhat straightforward to extend the definition of intermediate genomes, using the dcj-indel model of compeau  <cit>  and the concept of optimal completions. given an optimal completion c of a breakpoint graph bp, we can create a circular completion by applying the operation of transforming all paths into cycles, similarly as done above to a breakpoint graph for genomes with the same gene content. after a circular completion is found, the resulting breakpoint graph is essentially the same as a breakpoint graph for genomes with same gene content. therefore, we extend the results of theorem  <dig> in the following claim.

claim 2
given genomes a and b, a circular optimal completion c of bp, and a set m
′ of non-crossing chords in the cycles of c, covering all vertices of c, the genome m=m
′−s, where s is the set of the adjacencies of all singletons of m
′ in respect to a and b, is an intermediate genome of a and b. conversely, if m is an intermediate genome of a and b, there exists a circular optimal completion c of bp and a set of adjacencies s, where m
′=m∪s is a set of non-crossing chords in the cycles of c, covering all of its vertices, and s forms the set of adjacencies of singletons of m
′ in respect to a and b.

note that this result is general, also applicable for the same gene content genomes, since in this case we can consider that the breakpoint graph is directly an unique and optimal completion, and the set of singletons is always an empty set. figures  <dig> and  <dig> show an example of an optimal completion and an intermediate genome.

ancestral reconstruction
in this section we explore how the concept of intermediate genomes can be used for ancestral reconstruction of gene orders.

in the context of rearrangement distance models, the ancestral reconstruction problem can be stated as: considering a measure distance d between genomes a and b, given a tree t with n extant genomes at the leaves, find a labeling of the internal nodes corresponding to ancestral genomes, such that the total length of the tree, defined as the sum of all distances d on the edges, is minimized. this is usually called the small phylogeny problem.

the simplest instance of this problem happens when only three genomes a, b and c are given, and we want to find a genome m minimizing d+d+d, the genome median problem. despite being np-hard for dcj and many other models, it is well studied and many exact and heuristic methods have been proposed  <cit> ,

here we investigate new definitions of both the median problem and the small phylogeny problem that include intermediate genomes, motivated by the fact that some studies show that purely minimizing the tree length  might not be the best option for ancestral reconstruction  <cit> .

let ig represent the set of intermediate genomes between a and b. for the median problem, we can use the fact that d+d=d if m is in ig to give the following definition.

problem 1
given two genomes a and b, and an outgroup genome c, find an m∈ig minimizing d.

problem 2
given a rooted binary tree t with n extant genomes at the leaves, find a labeling of the internal nodes such that the tree length is minimized, and each genome on an internal node is an intermediate genome of its children.

theorem 2
the dcj intermediate genome median is np-hard.

proof
a balanced bicoloured graph
g is a graph where each vertex has the same number of red and blue incident edges, all vertices have degree two or four, and there is no cycle formed by edges of the same colour. an alternating cycle in g is a cycle where red and blue edges are alternating. the breakpoint graph decomposition problem  is to find a maximum number of edge-disjoint alternating cycles of g. this problem is np-hard  <cit> .

a proof for this theorem can be derived directly from the original proof of np-hardness of the dcj median problem, where a reduction from bgd is performed  <cit> . in that proof, from an instance of the bgd with g=, where v is a set of vertices and ℬ and r are sets of blue and red edges, the genomes a, b and c on g are constructed. the set g contains one gene x for each degree  <dig> vertex and two genes x and x¯ for each degree  <dig> vertex x of g. the set of adjacencies of a is xhxt:x∈g. the set of adjacencies of b is xhx¯t,xtx¯h:x∈vand degree ofxis 4∪xhxt:x∈vand degree ofxis  <dig>  the set of adjacencies of c is defined adding to c an adjacency in xhyh,xhy¯h,x¯hyh,x¯hy¯h for each xy∈ℬ, and an adjacency in xtyt,xty¯t,x¯tyt,x¯ty¯t for each xy∈r. figure  <dig> shows an example of the construction of genomes from a balanced bicoloured graph.
fig.  <dig> given a balanced bicoloured graph g , a breakpoint graph is constructed , with genomes a=1t1h,2t2h,2¯t2¯h,3t3h,3¯t3¯h,4t4h , b=1t1h,2t2¯h,2¯t2h,3t3¯h,3¯t3h,4t4h  and c=1t2t,2¯t3t,3¯t4t,1h3h,2h3¯h,2¯h4h . in this example, m=b⊆a∪b is a median 




defining a,b,c this way, there is a median m⊆a ∪ b that indicates the number of alternating cycles we have in a maximum edge-disjoint alternating cycle of g  <cit> .

as a consequence of m⊆a∪b, we have that m∈ig  <cit> . so, m∈ig and minimizes d
dcj+d
dcj+d
dcj solving both the dcj median for this specific instance and the bgd for the general case. it follows, since we can construct genomes a,b,c in polynomial time and bgd is np-hard, that dcj intermediate genome median is also np-hard. □

since the median and consequently the small phylogeny problem are np-hard also in their intermediate genomes formulation, we propose an approach that combines adjacency weighting methods that are common in adjacency-based algorithms, with the dcj rearrangement model in the form of intermediate genomes, but without the need to explicitly consider searching for rearrangement events and/or scenarios, which makes the problem much more tractable.

maximum weight intermediate genome
problem 3
given genomes a and b on set of genes gand a set of adjacency weights w=wij|ij∈g±×g±, find a genome m such that 
 m=argmaxm∈ig∑δij·wij  where δ
ij= <dig> if i
j∈m,  <dig> otherwise.

if the genomes a and b have the same genes, this problem can be solved in polynomial time, since finding a maximum weight set of non-crossing chords in a cycle is equivalent to finding a maximum weight independent set on a circle graph   <cit> . therefore, it is possible to find an optimal m∈ig by solving a mwis for each component of bp.

if a and b have different gene sets, the problem becomes much harder, since each completion of bp will give rise to different components and therefore different solutions for the individual mwis. the naive method of finding the maximum weight ig for all completions is impractical, since, according eq. , there is an exponential number of completions.

a strategy to solve problem  <dig> is to search a perfect matching in the graph ℋ that represents all possible optimal completions in c
∗, where the weight of each hyperedge is the weight obtained by solving the mwis for the correspondent component.

edmonds  <cit>  shows that the maximum weighted perfect 2-matching problem can be solved in polynomial time. it follows directly from the ℋ representation that

claim 3
suppose that p
ab is even, and pao≤pae and pbo≥pbe or pao≥pae and pbo≤pbe. then, the maximum weight intermediate genome problem can be solved polynomially.

moreover, we have that

claim 4
suppose that p
ab is odd, and pao≤pae and pbo≥pbe or pao≥pae and pbo≤pbe. then, the maximum weight intermediate genome problem can also be solved polynomially.

proof
since p
ab is odd, pao≤pae and pbo≥pbe or pao≥pae and pbo≤pbe, there is exactly one hyperedge with  <dig> elements. the number of hyperedges with  <dig> elements in ℋ is limited by n3=o. once one hyperedge with  <dig> elements is removed, according to claim  <dig>  finding a perfect 2-matching in the remaining vertices of the graph is polynomial. therefore, an optimal solution is found in polynomial time by repeating this for all o hyperedges with three elements and choosing the solution with maximum weight. □

unfortunately, the cases where pao<pae and pbo<pbe, or pao>pae and pbo>pbe are most likely np-hard, due to the presence of up to c  triple-matchings in optimal completions, as opposed to just one. this means that the complexity of the maximum weight intermediate genome problem is still open for the general case. however, considering that the number of chromosomes is constant, we have the following interesting result from the theoretical point of view.

theorem 3
there is a polynomial time fpt algorithm for the maximum weight intermediate genome problem when it is parameterized by the number c of chromosomes.

proof
claim  <dig> and  <dig> guarantee that there is a polynomial time algorithm if pao≤pae and pbo≥pbe or pao≥pae and pbo≤pbe. if pao<pae and pbo<pbe, or pao>pae and pbo>pbe, using a polynomial algorithm for maximum weighted perfect 2-matching and claim  <dig>  we have a ftp algorithm with parameter c. □

ancestral reconstruction algorithms
in this section we describe the practical algorithms that were used for our proposed ancestral reconstruction method. first, we discuss how adjacency weights can be obtained. then, how these weights are used by a heuristic to find candidate intermediate genomes for the ancestral nodes of the input tree.

finding adjacency weights
adjacency weights were obtained using two methods. first, using the software declone  <cit> , that randomly samples evolutionary scenarios and assign weights based on how often an adjacency is present on those scenarios. the parsimony score of a given scenario is determined by the number of gains/losses of adjacencies along the branches of the tree. declone samples scenarios depending on a parameter kt. when kt is close to zero, only optimal scenarios  are sampled, and as kt increases, sub-optimal scenarios have a higher chance of being sampled. the weights for each adjacency at each internal node depend on how often this adjacency is observed at this internal node. typical values include k
t= <dig>  for sampling optimal scenarios almost exclusively, and k
t= <dig> for a more balanced distribution including non-optimal scenarios  <cit> .

we also propose a second way of deriving adjacency weights, inspired by the weighting scheme used in infercars  <cit> . given a rooted phylogenetic tree t, let w
α denote the weight of adjacency ij at a node α. weights in all nodes are recursively defined by 
  <dig> wα=dl·wr+dr·wldl+dr 


where d
l  is the distance to the left  child of α, and w
l ) is the weight of ij at the left  child of α. for leaf nodes, w
α= <dig> if the adjacency is present and w
α= <dig> otherwise.

to define the weights in our approach, we proceed as follows: for every internal node α, let γ be the the parent node of α, and create a new tree t
′ by removing from t the subtree defined by the node α. then, remove the original root and reroot t
′ at the node γ and use the recurrence equation above to find w
γ for all adjacencies ij. the adjacency weights for α are then w
α=w
γ for each ij. an example is shown on fig.  <dig> 
fig.  <dig> to find adjacency weights for node α on a tree t, a new tree t
′ is created where α and its subtree t <dig> are removed, and t
′ is rerooted at γ, the parent node of α. then, eq.  is applied to find weights for γ, which are then assigned to node α on the original tree t





the motivation for using this weighting algorithm is that, while reconstructing a particular node α, the information from the leaves is given in the form of the breakpoint graph, while the weights that will guide the reconstruction of the intermediate genome should reflect information from the “other side” of the tree. the experimental results show, somewhat surprisingly, that this simple weighting scheme not only is faster than declone, but also increases the quality of the reconstruction.

estimating branch lengths
for the infercars weight algorithm, branch lenghts are needed. since branch lengths are not always available, we tested how different estimation methods might impact the adjacency weights and consequentely the ancestral reconstruction. for this, we implemented two classic methods of branch length estimation, minimum evolution  <cit>  and fitch-margoliash least squares  <cit> , briefly described in the following.

let t be an unrooted tree with k leafs and n=2k− <dig> edges, with edge lengths denoted by the vector w=. let m be a m×n matrix, where m=k <dig>  each column of m represents a branch length, and each row a pairwise comparison between two leafs of t. an element m
ij of m is  <dig> if the edge j is present in the tree path from the two leafs being compared, and m
ij= <dig> otherwise. let d= be a vector where each element d
i stores the dcj-indel distance of the two genomes being compared on this row i. therefore, for k> <dig> leafs, we have m>n and m
w=d is an over-determined equation system. then, as proposed by fitch and margoliash  <cit> , a good candidate for the edge weights is the vector w
∗ that minimizes the least squares error, that is,





another idea is to assume that the pairwise distances in d are a lower-bound for the tree traversal distances, and find edge lengths that satisfy this restriction and have minimum total sum. this method, called minimum evolution by waterman et al.  <cit> , is based on solving the following linear programming formulation: 
 minimize∑i=1nwisubject tomw≥dwi≥ <dig> i= <dig> …,n 


an algorithm for the ig-indel small parsimony problem
given a rooted phylogenetic tree with genomes at the leaves and a set of adjacency weights, our method works in a bottom-up fashion, by choosing two leaves with the same parent, reconstructing the ancestor at this parent node, and labeling this current node as a leaf, until the root of the tree is reconstructed.

at each node being reconstructed, given the two children genomes and a set of adjacency weights, a heuristic for the maximum weight intermediate genome  problem is called, which tries to quickly find an optimal completion with high adjacency weight.

to do that, we build the hypergraph ℋ representing all optimal completions c∗, but ignore triple matchings, focusing only on 2-matchings present in optimal completions, as given by the sets t
i, i= <dig> …, <dig>  the weight of an edge in ℋ is given by the solution of a mwis on the component correspoding to the given edge. if p
ab is even, there is a perfect matching in ℋ corresponding to an optimal completion. we find a maximum weight perfect matching using blossomv  <cit> . then, from each mwis solution for the matched components, we get adjacencies to build a genome g that is a high weight solution for the mwig. if p
ab is odd, we could use claim  <dig> strategy of removing every possible triplet of ℋ and solving the even case as described, picking then the combination with highest weight. since the number of triplets can be very high, we chose to solve this in a faster way by adding three dummy nodes v
a, v
b, and v
ab to ℋ, connected with zero weights to all vertices corresponding to a-, b- and ab-paths, respectively, artificially transforming ℋ in a even p
ab case, and then finding a maximum weight perfect matching on ℋ. the three components that are matched to the dummy nodes are then combined, and a mwis is solved for this triplet.

a pseudocode of the proposed method, which we call ig_small_phylogeny, is given at algorithm  <dig> 





RESULTS
we implemented our algorithms in a software called ringo , available at https://github.com/pedrofeijao/ringo. we created several simulated datasets to test our proposed algorithms and compare with other existing approaches. ringo was ran with declone weights for k
t= <dig> , k
t= <dig>  and k
t= <dig> , and also our custom weight algorithm. for the custom weights, we used the branch lengths given from the simulations, and also tested with branch length estimates given by minimum evolution and least squares.

we compared ringo with two other methods for ancestral reconstruction of unequal content genes, mgra  <cit>  and pmag +  <cit> , implemented in the tool mlgo  <cit> .

simulated datasets
the simulated datasets were created using a similar procedure as in  <cit> , with a few extra parameters to include indel events. a birth-death model with a birth rate of  <dig>  and a death rate of  <dig> generates an ultrametric tree with n= <dig> leaves, and the branch lengths are disturbed by multiplying by e
d, where d is a real number uniformly chosen from the interval . the branch lengths are then rescaled so the tree has a diameter d∈{ <dig> n,1n, <dig> n,2n, <dig> n}, where n= <dig> is the number of genes, and the diameter is the maximum distance between two leaves.

the root node is labeled with an unichromosomal genome with  <dig> genes, and evolution is simulated along the edges by performing a number of random events defined by the edge length. events are chosen randomly between reversals, deletions and insertions, with probability 1−p, p/ <dig> and p/ <dig> respectively, with p∈{ <dig> . <dig> . <dig> .6}. the length of an indel is sampled uniformly from , with i∈{ <dig> }. although the expected size of the leaf genomes is  <dig>  there is not guarantee that genomes will have the same size. for each combination of d, p and i, we generated  <dig> datasets.

discussion
all algorithms were compared in terms of quality of the reconstruction, dcj distance to the correct ancestral genomes, and running time.

the quality results of all simulations are summarized on fig.  <dig>  each column represents the average results of ringo, mlgo and mgra on each dataset, showing the average number of true positives and false positives, when comparing the adjacencies of the simulated and the reconstructed genomes, in all internal nodes of a given tree. more detailed results are given on table  <dig>  that also shows all variations of the ringo algorithms.
fig.  <dig> quality of the adjacency reconstruction for each dataset, with single gene indels  and with indel size ∈ <cit>  . each column group represents the average results of ringo , mlgo and mgra on each dataset, with the percentage of true positives and false positives, when comparing the simulated adjacencies and the reconstructed adjacencies in all internal nodes of the simulated trees



diameter 

adjacency results 

unitary indels




ringo – declone weights, k

ringo – declone weights, k

ringo – declone weights, k

mlgo

mgra





ringo – declone weights, k

ringo – declone weights, k

ringo – declone weights, k

mlgo

mgra
ringo algorithm was tested with the infercars adjacency weights, using the simulated tree branch lengths and with branch lenghts estimated with minimum evolution or least squares methods. ringo was also ran with declone weights, with varying kt values




in datasets with small amount of evolution , specially with unitary indels , mgra has a slightly better quality than the two others. but, as soon as the rearrangement rate increases, mgra quality decreases rapidly, while ringo and mlgo quality seems to decrease in a slower, somewhat linear rate.

at higher rates , mlgo has a slightly higher number of true positives, but at the cost of a much higher number of false positives. ringo is a more conservative method, with the smallest number of false positives in all datasets.

when comparing the datasets with i= <dig> versus i= <dig>  we notice a decrease in quality for all algorithms for the larger indels, but mgra has a slightly larger loss of quality, specially at higher rates of evolution. in fact, in most datasets with i= <dig>  increasing the indel probability p also increases the quality of mgra, while the opposite happens for i= <dig>  we believe that this might be a consequence of the way that mgra models the prosthetic chromosomes by adding an edge v
t,v
h for each missing gene, which implicitly assumes that this gene is part of an unitary indel. in that respect, using a dcj indel model such as the one in ringo, that allows for block indels, will give better results when block indels do occur, which we believe is the more realistic case.

using the dcj-indel distance  <cit> , we also measured how far the reconstructed genomes are from the simulated genomes in average, and the results are shown on fig.  <dig>  as the quality results indicate, at lower rate, specially with unitary indels, mgra has the smallest distances, but they increase rapidly for higher rates. comparing mlgo and ringo for the higher rates, even though mlgo has a higher percentage of false positives, it has the smallest distances to the ancestral genomes. we believe that this is caused by the fact that the dcj distance strongly penalizes fragmentation. therefore, comparing conservative methods like ringo, that have a lower percentage of false positives, with more aggressive methods like mlgo, with more true positives at the cost of higher false positive percentage, the latter methods will have smaller distances. for instance, consider an ancestral unichromosomal genome a=, where the letters represent four blocks. if a method correctly reconstructs all four blocks, but not the connection between them, that is, a fragmented genome b= with four chromosomes, then we have that the dcj distance is d= <dig>  now, consider another method that also reconstructs the four blocks correctly, but gives a wrong ordering, such as c=. surprisingly, we have that d= <dig>  even though this reconstruction has the same number of correct adjacencies as the previous one, but more false positives. indeed, for the general case of an ancestral genome a= and a fragmented reconstruction b=…, we have d=n− <dig>  which is in fact the dcj diameter for n blocks, that is, the maximum possible dcj distance. any ordering of the blocks a
 <dig> …,a
n, even completely random, will have an equal or smaller distance to genome a. therefore, aggressive methods that try to minimize fragmentation by adding adjacencies, even with small support, will have a smaller distance to the correct ancestral genome, but we argue that this is no indication of a better reconstruction.
fig.  <dig> dcj-indel distance between the simulated and reconstructed genomes, with only single gene indels  and with indel size ∈ <cit>  . each column group represents the average distances of ringo, mlgo and mgra on each dataset




while comparing running times, for ringo and mgra the determining parameter is the rate of evolution, controlled by the parameter d. for mlgo, the running times stayed around one minute regardless of the rate. on table  <dig>  we show the average running times for all repetitions and indel rates i∈{ <dig> . <dig> . <dig> .6}, in each of the different diameters, for the two datasets i= <dig> and i= <dig>  the running time of ringo is smaller than mlgo and increases in a much slower rate than mgra, which increases exponentially for larger rates of evolution.

dataset

diameter 

ringo

mlgo

mgra



in summary, these results show that algorithms based on intermediate genomes can perform at quality levels equal or higher than current approaches for ancestral reconstruction, while also being much faster.

CONCLUSIONS
in this paper we proposed a new method for ancestral reconstruction of gene orders for genomes with unequal gene content by expanding a previous approach for genomes with same gene content. the ig algorithm is faster and in many datasets has a better reconstruction quality than mgra and mlgo, specially for higher rates of evolution. we believe that one of the strongest points of our approach is the use of extra information, in the form of intermediate genomes, and not simply relying on the parsimonious idea of minimizing tree distances. with that, not only the quality of the reconstructed genomes is improved but also the search space is drastically reduced, resulting in faster algorithms. we also think that a combined approach with ideas from both worlds could deliver very good results. as an example, we could think of combining the space reduction power of intermediate genomes with the strong space search techniques of mgra.

there are many ways that we can improve the ideas presented in this paper. for one, instead of using a heuristic for solving the maximum weight intermediate genome, we will test how solving this problem exactly changes the results, whether by using a fpt such as the one described, or resorting to an integer linear programming for the more complex cases. we also plan to extend the current framework to allow the presence of duplicated genes.

the proposed algorithms were implemented as python  <dig>  scripts in a software called ringo, that can be downloaded at https://github.com/pedrofeijao/ringo. also included are scripts to generate simulations and parse the reconstruction results on the simulated datasets, comparing ringo with other algorithms.

