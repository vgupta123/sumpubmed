BACKGROUND
the combination of sequencing and post sequencing approaches together with annotations efforts and in silico analysis have produced a tremendous amount of available biological data and knowledge. as technologies evolve, the production of raw data is now becoming daily routine. while transcriptomics produce lists of differentially expressed or co-regulated genes, proteomics produce lists of proteins that are differentially expressed, that carry unusual post-translational modifications or that interact to form a complex. the characterization of those sets of genes or proteins in the light of all available knowledge is therefore a crucial task for the biological researchers and the computational biologists.

to characterize sets of genes or proteins, many tools and methods have been developed  and their main principle is to look for over-represented or enriched features.

undoubtedly, the key to the success of this technique is its ability to confront heterogeneous data: the set of genes of interest can be compared to the sets of genes i) having the same annotation , ii) involved in the same pathway , iii) co-cited in the literature, iv) co-localized on the chromosome, and so on.

a typical analysis is illustrated in figure  <dig> through a synthetic example where a query set of genes of interest  is searched for enrichment in annotations . to process the query, the search engine converts the annotations  into target sets. for example, the term 'rna splicing' will be converted to the target set including the gene d which is directly annotated with this term and also the genes that are annotated with the more specialized terms i.e. the genes b and c annotated with 'regulation of rna splicing' and the genes c and e annotated with 'nuclear mrna splicing' resulting in the target set {b, c, d, e}. during the search, the query set is compared to the target sets by the means of a similarity measure  and the system returns the similar target sets  with their annotations ordered by decreasing similarity up to a certain threshold. from the enriched features, our query set can be characterized by the rna splicing process.

despite the variety of methods proposed to perform such an analysis, very little has been done in terms of formalization, and this has unfortunate consequences. first computationally, the lack of formalism offers very few possibilities for reusable optimizations causing a waste of resources . considering the growing rate of data, this might soon become an issue. second and more importantly for the users, current methods generally ignore the structure of the confronted data which leaves the user with numerous enriched features of varying relevance to manually filter and synthesize.

in this article, we first propose a formalism to represent the feature data. central to all enrichment search methods is the concept of neighborhood proposed by danchin in  <cit> : instead of considering genes and proteins as individual entities, the principle is to focus on the relationships between these biological objects . the generalization of this concept leads to build target sets of entities  sharing a particular relationship . depending on the relationship, larger target sets can include smaller ones and this information can be represented by directed acyclic graphs  <cit> . such directed acyclic graphs  are equivalent to mathematical objects: partially ordered sets  which make them perfectly suited for abstraction, formal representation and manipulation. in this paper, we define a neighborhood as the feature data represented by a partially ordered set of target sets.

based on this formalism, we are able to face the problem of the relevance of the enriched features and the structure of the data, for which we introduce the concept of the pertinence of target sets in the context of a given query set of genes. in our synthetic example of figure  <dig>  we observe a redundancy in the hits composing the results: rna splicing, rna processing, mrna metabolic process, etc, are all reported and it is noticeable that rna splicing matches exactly the content of our query, and therefore only this particular hit should be presented to the user. the other hits are due to the hierarchical structure of the annotations and should be omitted: rna processing  appears in the results because it includes rna splicing and another gene  not present in the query which should make this target set not pertinent. similarly, regulation of rna splicing  appears because it is included in rna splicing but contains fewer genes of our query, and this, again, should make this target set not pertinent. these observations allow us to formally define the pertinence of a target set in the context of partially ordered sets. for simplicity here, the target set matches exactly the query, but it is rarely the case, and more than one target set can be pertinent as we explain later. interestingly, this definition of pertinence holds for the various dissimilarity indices used by current methods . having a formal pertinence definition, we solve a classical query optimization problem involving a time-space trade off and an early pattern evaluation: instead of storing a very large number of target sets , we need to generate only the interesting ones on the fly from a less explicit representation. in this paper, we present algorithms working on compact representations of the data for the generation and evaluation of pertinent target sets. compact representations exploit the structure of the data  and algorithms efficiently use rules derived from the pertinence definition.

RESULTS
in this section, we first provide formal definitions of neighborhoods  and target sets pertinence. then, we introduce an algorithm for the identification and the comparison of pertinent target sets in a dag when all the set compositions are directly available . next, we propose a generic compact representation of neighborhoods and detail the adaptation of the previous algorithm in this context. the rest of this section focuses on specific representations and algorithms relying on the dag properties that lead to further time and space optimizations.

definitions
uniform representation of data: dags defining sets partially ordered by the inclusion relation
we will denote s the set of objects considered in the remaining of this paper. for example, s can be the set of proteins of an organism. we consider that a neighborhood is a set n  partially ordered by the inclusion relation ≺. partially ordered sets  are generally represented by hasse diagrams in which there is an edge from y to x if and only if y covers x . this means that x ≺ y and there is no other element z such that x ≺ z ≺ y .

in the following, we consider that a neighborhood is a hasse diagram  that defines a poset .

target sets pertinence
several methods and tools use a similarity or dissimilarity index to compare sets, we can cite amongst others: funspec  <cit> , blastsets  <cit> , gostat  <cit> , ease  <cit> , pandora  <cit> , abandapart  <cit> , gocluster  <cit> , see  <cit>  for a review. even though, these methods are using various similarity indices to compare the query and target sets , they all have in common that they consider only counts of elements such as the sizes of the query and target sets, or the number of common and differing elements. thus, when comparing two sets, the bigger the number of common elements and the smaller the number of differing elements, the more similar they are considered.

formally, this corresponds to any similarity index f between a query set q and a target set t, such that f increases with |t ∩ q| and decreases with |t - q|. then, given such a similarity index for the comparison of a given query set to a neighborhood, it is not necessary to perform the comparisons with all the target sets in the neighborhood. we introduce the notion of pertinence of a target set for its comparison to a given query set, which allows to consider target sets that are likely to have good similarity values  and to ignore target sets that will give redundant results .

our main observation is that redundancy is caused by two target sets when one includes the other and when they have either the same common elements or the same differing elements with the query. for example in figure  <dig>  the target sets t <dig> and t <dig> are both redundant with t <dig> for the query set q. t <dig> is redundant because it includes t <dig> and has the same common elements  so it is less similar and it does not bring more information than t <dig> alone. similarly, t <dig> is redundant because it is included in t <dig> and has the same differing elements . the fact that one target set includes the other is important for the biological meaning of the results. let us consider t <dig> and t <dig> of figure  <dig>  in this case, one should be tempted to decide that only t <dig> is pertinent . actually, t <dig> is also pertinent because the differing elements do not include those of t <dig> and thus t <dig> may be associated to a pertinent nonredundant biological meaning.

mathematically, the observation above is written simply as follows:

definition. a target set t in a neighborhood n is pertinent for its comparison to a given query set q if and only if:

 t ∩ q ≠ ∅ 

 ∄t' ∈ n such that t' ⊂ t and t' ∩ q = t ∩ q 

 ∄t' ∈ n such that t' ⊂ t and t' - q = t - q 

this mathematical definition suggests that one must test all possible t' to decide if a target set t is pertinent. however, it is easy to deduce that only the parent and child nodes of t in the hasse diagram representing the neighborhood n must be checked. let us suppose that such a t' exists for  ). then, due to the inclusion relation between t and t', all the sets in n on the path linking t and t' also satisfy  ), and then especially a child node  of t.

as only the parent and child nodes of t need to be considered, the test of pertinence can be performed on the number of common and differing elements. this is because if these numbers are equal then we are in presence of the same elements .

as a result, the mathematical definition can be simplified into the following  <dig> rules  that are more suitable for the design of an efficient algorithm:

rule 1: |t ∩ q| ≠ 0

rule 2: ∄t' such that t' ≺ t and |t ∩ q| = |t' ∩ q|

rule 3: ∄t' such that t ≺ t' and |t - q| = |t' - q|

structures and algorithms
algorithm for the identification and the comparison of pertinent target sets in the explicit representation
the pertinence rules allow us to define an algorithm  for the identification of target sets that are pertinent for their comparison to a given query set. its principle is to search the dag of the neighborhood, starting from the leaves corresponding to query elements  and exploring their ancestors to identify nodes satisfying the pertinence definition. this corresponds to a multiple sources breadth-first search, in which the queue is initialized with the nodes corresponding to the query elements. each time a node is processed it is tested for pertinence. the search can stop at nodes including the query  or when the target set size is too big to give a significant similarity value. in the latter case, a test on the target set size is performed if an upper bound max_target_size can be computed theoretically . in this algorithm, we suppose that the sets corresponding to the nodes of the dag are available . the worst-case time complexity of a breadth-first search is o where v is the number of vertices of the dag and e is the number of edges. to test the pertinence of a node, we need  to compute the number of common and differing elements of the sets corresponding to the nodes, and  to compare these values to the parent and child nodes to test if neither rule  <dig> nor rule  <dig> are violated. the computation of the number of common and differing elements for a node can be done in o, the maximum size of a set. the test of pertinence done in pertinent necessitates an access to all the parent and child nodes, which adds up to o supplementary tests. thus, the worst-case time complexity of algorithm  <dig> is o = o. the worst-case time complexity occurs when all the nodes except the root include some but not all of the query elements. let us consider the average case, in which we expect the query set to be small compared to the total number of elements |s|. the target sets sharing elements with the query represent only a subgraph of the dag , and, the pertinent target sets should have sizes that are commensurate with the query size, which implies that they are deep in the dag . then, the number of nodes processed is typically very small compared to v. moreover, the average target set size is small compared to |s|. thus, the added |s| factor to the complexity may be considered as a constant and be negligible in the average case.

algorithm  <dig> assumes that the set compositions are available. in the following, we introduce compact representations of neighborhoods and algorithms efficiently working on such representations.

generic compact representation of neighborhoods
it is generally inefficient to explicitly store the composition of all the sets of a neighborhood. a compact representation is needed. such a representation should permit one both to identify and to generate pertinent target sets efficiently, in a way that avoids the generation of all the sets and the traversal of the entire graph. indeed, the uniform representation of neighborhoods by dags is adequate for compactness: we can store only the dag defining the poset and reconstruct sets corresponding to nodes on the fly. in this compact representation, leaf nodes  correspond to singleton sets  and all the other nodes correspond to sets that can be built by searching the labels of reachable leaf nodes. for efficiency reasons, internal nodes are labeled with the size of the sets they represent as we will explain later. figure  <dig> illustrates the compact representation corresponding to the dag of figure 1b.

algorithm for the identification of pertinent target sets in the generic compact representation
like in algorithm  <dig>  the principle is to start from the leaves representing query elements and traverse the dag to search for pertinent target sets among the sets sharing elements with the query. the difficulty we have to solve is that the set compositions are not available. we thus  store the size of the set corresponding to a node, as illustrated in figure  <dig>   order the nodes in the queue by their size and  propagate the common elements during the search.

in order to test rule  <dig>  we need the number of common elements of the node processed and its child nodes. with the breadth-first order, the node 'mrna metabolic process' of size  <dig> of figure  <dig> would have been processed before the node of size  <dig> 'rna splicing' , and thus the common element b would not have been propagated yet. the solution is to maintain the queue ordered by the set sizes to ensure that sets smaller than the node processed  have already been processed. this way, all the common elements have been propagated and rule  <dig> can be tested at the level of the node processed.

in order to test rule  <dig>  we need the number of differing elements of the node processed and its parent nodes. unfortunately, this number is not available at this time for the parent nodes because all the common elements may not have been propagated to the parent nodes yet. for example, the node 'rna processing'  in figure  <dig> is processed before the element g has been propagated to the node 'rna metabolic process' of size  <dig>  the solution is to consider the node processed as the parent node and test if rule  <dig> is not violated for its child nodes that is, we test the pertinence of its child nodes.

as a result, the pertinence decision is divided in two steps:

step 1: rule  <dig> is tested at the level of the node processed.

step 2: rule  <dig> is tested at the level of the child nodes of the node processed.

the efficiency of the resulting algorithm , compared to algorithm  <dig>  is only affected by the extraction of the next element of the queue, which must be ordered by the set sizes. as we can have at most |s| different sizes of set, the worst-case time complexity is affected by a factor of log |s| by using an adequate data structure. as for the previous algorithm, the average target set size is expected to be small compared to |s|, so the log |s| factor may be considered as a constant and be negligible.

specific representations of neighborhoods
the previous compact representation is general and can be used for any neighborhood. nonetheless, we identified cases where further time and space optimizations can be envisaged. it arises when:

• the dag is actually a tree . in this case, the tree can be stored as a parenthesized expression without the need to store the size of the sets for each node. typical examples of this situation correspond to the gene expression profiles hierarchically clustered, or the iubmb enzyme nomenclature  <cit>  that is, sets of genes/proteins annotated with the same ec number.

• the dag obtained after building the neighborhood is implicit and thus, does not need to be stored. it corresponds for example to a correspondence analysis or a principal component analysis of the codon usage  or the sets of genes that are adjacent on the chromosome. in the latter case, we only need to know the order of the genes: any pair of genes defines an interval which defines a set of adjacent genes.

in the following, we present efficient algorithms for the identification of pertinent target sets in these specific representations.

algorithm for the identification and the comparison of pertinent target sets in the tree compact representation
the main advantage to searching pertinent target sets in a tree is that for a given node, the child nodes define a non overlapping partition of the set their parent node represents, and thus, only the number of common and differing elements need to be propagated.

the principle is to recursively compute a triplet of values  for each node by using a stack of stacks to parse the parenthesized expression. the idea is to push an empty stack when an opening parenthesis is encountered, or a triplet of values when an element is encountered. when a closing parenthesis is read, the computation can occur and consists in the following:

 compute the number of common and differing elements corresponding to this node by summing up the values of the triplets contained in the top stack.

 test rule  <dig> for current node: if the number of common elements is bigger than all of the triplets contained in the top stack then the tag is set to potentially pertinent .

 test rule  <dig> for child nodes: if child nodes tagged potentially pertinent have less differing elements than the current node, then they are pertinent and the comparison is performed.

 replace the top stack by the triplet of computed values.

 stop if all the query elements are included or if the target set size exceeds max_target_size.

compared to the previous algorithm, this one avoids  the merging of the common elements for each node and  the extraction of the next element of the queue. the tree is composed of |s| leaves and at most |s| -  <dig> nodes, thus, the worst-case time complexity of this algorithm is o.

algorithm for the identification and the comparison of pertinent target sets in implicit compact representations
an implicit representation requires us to provide a specific algorithm for each different implied dag. however, this loss in genericity allows considerable time saving in the search for pertinent target sets and considerable space savings for storing the neighborhoods. we have chosen to present the sets of adjacent genes on the chromosome because it can be described very simply and briefly, leads to a straightforward algorithm, and was often encountered in our experiments.

for our illustration, we only need to store the genes in the order they appear on the chromosome. thus, the space requirement is θ, instead of θ needed for the dag representation.

to identify pertinent target sets, we only need to know the position on the chromosome of each of the query elements. then, each pair of positions defines a lower and an upper bound of an interval that, in turns, defines a set. for such a set to be pertinent, the bounds of the interval must be such that the position just before  the lower  bound must not be an element of the query since it would violates rule  <dig>  rule  <dig> holds because the bounds correspond to query elements. the worst-case time complexity of the resulting algorithm is o, q being the query set. compared to algorithm  <dig> working on the generic compact representation, this algorithm spares  the merging of common elements and  the extraction of the next element of the queue.

testing and validation
in this section, we illustrate the gain in storage space, number of comparisons performed and quality of the results  through a typical search of gene ontology  <cit>  annotations enrichment in sets of proteins corresponding to multi-protein complexes, and compare the results obtained with and without considering the pertinence of target sets. for convenience, we used the blastsets system  <cit>  to obtain these results because it allows to use all the sets of a neighborhood  as query sets to be searched for feature enrichment , but any of the previously cited methods and tools may be used instead.

data sets
query sets
the query sets of proteins correspond to protein complexes of the yeast saccharomyces cerevisiae referenced at the mips  <cit>  . the motivation for this choice is that once the proteins involved in a complex are identified, the next step is often to search for a molecular function or a biological process with which to annotate the newly grouped set of proteins. moreover, the yeast proteome is well annotated with  <dig>  <dig>   <dig>  <dig> and  <dig>  <dig> annotated gene products  respectively for the molecular function, biological process and cellular component branch of the gene ontology . we extracted  <dig> query sets of proteins, one for each protein complex.

target sets/neighborhoods
the gene ontology is organized in a dag hierarchical structure. the pertinence definition is thus perfectly suited to this case as go terms allow very specific as well as very general annotations, which may often lead to results of varying relevance. we constructed three gene ontology neighborhoods , one for each gene ontology branch i.e., cellular components , molecular functions  and biological processes , by performing the following:

 the dag of the gene ontology is the generic compact representation of the neighborhood,

 to the previous dag, we add  nodes corresponding to proteins, and connect them as child nodes for each go term they are annotated with,

 we recursively traverse the dag in a bottom-up fashion to compute the size of the set corresponding to each node.

this construction implies that when a protein is annotated with a go term, all go terms on the paths from this term to the root  are also annotating this protein.

overall validation performances
each of the  <dig>  <dig> protein complexes served as a query set of proteins, and each was searched for similar sets in the three gene ontology branches constructed neighborhoods. the threshold for set similarity significance was set to  <dig> . this corresponds to the probability of obtaining a similarity value  at least as good by submitting a random set of the same size, see  <cit>  for more details.

space requirements
we compared the storage space needed by the generic compact representation  and by the explicit storage of sets compositions . we assume that the set sizes, nodes and elements identifiers are represented by the same memory unit , say  <dig> bit integers. for the explicit representation, the sets are represented as lists associated with nodes identifiers. for the generic representation, the size of the sets are stored directly in adjacency lists. the sizes obtained are listed in table  <dig>  as a result, the space needed to store the generic representation requires only 27% to 49% of the space needed for the explicit representation. note that the dags have small sizes . thus, in this situation, an explicit representation is not prohibitive. however, it is noticeable that even for such small sizes, the compact representation requires at most half of the storage space needed by the explicit representation. as the sizes of these dags will grow  the generic representation performances will increase accordingly.

summary of the results obtained for the  <dig> gene ontology branches extracted neighborhoods  in terms of space requirements in memory units  for the explicit  versus the generic representation of neighborhoods.

efficiency
summary of the results obtained for the  <dig> gene ontology branches extracted neighborhoods in terms of efficiency that is, the number of sets comparisons performed.

redundancy reduction
in table  <dig>  we give, for each go branch, the number of sets found significantly similar  to protein complexes with and without applying the test of target set pertinence. these results show that pertinent target sets similar to the query represent only 50%, 48% and 36% of the hits returned by the system. thus, with at least half less results to examine, the analysis and interpretation made by biologists is simplified and more effective.

summary of the results obtained for the  <dig> gene ontology branches extracted neighborhoods in terms of redundancy reduction that is, the number of target sets found similar  to the  <dig> mips protein complexes in the gene ontology obtained with and without applying the test of pertinence.

typical outcome for a protein complex
to illustrate further the gain in results pertinence, we focus on one particular mips complex: ' <dig> . <dig> mrna splicing' composed of  <dig> proteins. the results obtained for this complex emphasize well the redundancy issue and the need of pertinence testing. the list of similar sets found  in the bp branch is given in table  <dig>  the relationships between go terms are given in figure  <dig> 

sets found similar to the mips ' <dig> . <dig> mrna splicing' protein complex  in the neighborhood constructed on the biological processes branch of the gene ontology. results are sorted to better illustrate pertinence definition application . pertinent target sets are in bold shape. non pertinent target sets that violate rule  <dig> are in normal shape . non pertinent target sets that violate only rule  <dig> are in italic.

among the  <dig> hits, only  <dig> are actually pertinent,  <dig> hits violate at least rule  <dig>  the test of rule  <dig> not being performed, and  <dig> hits violate rule  <dig>  the target sets are sorted in order to better understand the violations of pertinence: pertinent target sets are listed first in bold, followed by target sets that are not pertinent because of the previous pertinent set. the first hit, go: <dig>  is pertinent and the following  <dig> hits are not as they have the same number of common elements but correspond to less specific go terms, which violates rule  <dig>  the same scenario occurs for the next  <dig> pertinent target sets, go: <dig> and go: <dig> with respectively  <dig> and  <dig> following hits that violate rule  <dig>  then, go <dig> is pertinent, and the following, go: <dig>  is not because it has the same differing elements  and has less common elements i.e. it is less general, which violates rule  <dig> 

only  <dig> of the  <dig> query elements are found together in a hit. in the general case, this may  highlight bad annotations or  provide a hint or indications on the role of the missing elements. here, the missing elements are the products of the genes ygl128c and ykl078w. ygl128c is annotated as 'component of a complex containing cef1p, putatively involved in pre-mrna splicing'. it is currently annotated with 'biological process unknown' which explains why it is not found in the results. interestingly, it is associated with 'spliceosome complex' in the cellular components branch which complies with its supposed involvement in pre-mrna splicing. moreover, its association to a complex containing cef1p strongly suggests that ygl128c should be annotated with go: <dig>  ykl078w is annotated as 'predominantly nucleolar deah-box rna helicase, required for 18s rrna synthesis'. it is annotated as go: <dig> ribosome biogenesis of the biological process branch of the go. this term corresponds to a set of size  <dig> that is not part of the results that is, it is not significantly similar to  <dig> . <dig>  in  <cit> , the authors state that ykl078w is not required in pre-mrna splicing, but it is required for pre-rrna cleavage , and thus its go annotation is consistent.

comparison to other methods
since  <dig>  several methods  have been successfully applied to search for over-represented features based on a dissimilarity index to compare a query set to target sets . more recently, alternative or complementary approaches have been developed, mainly to find more relevant features or to combine multiple features. hereafter, we discuss and compare our methods to the major trends in the field.

frequent itemset mining
the problem of identifying pertinent target sets resembles the frequent or closed itemset mining problem  in many aspects. unfortunately, the methods developed for frequent itemset mining cannot be applied to our context. indeed, these methods rely on the anti-monotonic property of the support function . in our situation, the pertinence test is not anti-monotonic: a non pertinent target set can have ancestors that are pertinent. as a result, the pertinence definition cannot be used in the same way to prune the search. moreover, closed itemsets permit the generation of all frequent itemsets contrary to pertinent target sets .

dissimilarity index based over-representation methods and tools
the methods we describe in this article improve the global quality of the results found by using a statistical test to decide the over-representation of a given feature in a given set . depending on the test performed  and the correction for multiple testing , the set of over-represented features will vary in size but the top features  will remain essentially the same. among those features, we clearly showed mathematically and also with biological results that a lot of them are actually redundant and non-informative. as it is based on the same principles , our method can only perform at least as good as those others. for example, we submitted the query set of proteins of the complex  <dig> . <dig> to gostats  <cit>  and we obtained  <dig> significantly enriched go terms in the biological processes branch among which  <dig> are not in table  <dig>  the observed differences  are due to different versions of the data and to the multiple testing adjustment method . the additional hits are related to the pertinent hits found in table  <dig> and do not bring much additional insight to our query set biological function.

an original approach exploiting the go structure was proposed in  <cit> . like us, they consider the go as a partially ordered set and work on the dag. their method and ours diverge due to the dissimilarity index. to score target sets, they define a pseudo distance which can be stated roughly as the average distance between the genes of the query and the target go term. while this approach is formal and applicable to ontologies in general, it suffers some significant limitations. first computationally, for each query set they need to score all the go terms. second statistically, because of the use of a distance, they are only able to rank the go terms and cannot assess the significance of the results. and finally, they also encounter the problem of redundancy and pertinence of the results. they partially address it by finding in the top ranked go terms, the ones that are not comparable .

information content based methods
an interesting approach has been proposed by  <cit>  to take into account the go hierarchy. the difficulty to address when dealing with the go hierarchy is that the level of a go term in the hierarchy does not reflect the degree of specificity of this term. as a result, the degree of specificity  at which to look for enrichment should not be specified in the query because it can yield to misleading results or missed discoveries. in  <cit> , the authors propose an information theoretic approach that allows to specify the degree of specificity desired for the enriched features. this is done by splitting the graph of the gene ontology into subgraphs. the split is such that the resulting subgraphs  contain comparable information content, i.e. they concern the same number of genes. to illustrate their approach, they analyze the 'map <dig> oxidative phosphorylation' set of proteins corresponding to genmapp proteins involved in oxidative phosphorylation. for this analysis, the gene ontology was split into  <dig> partitions among which a clear enrichment in 'transport' is visualized. in contrast, a corresponding go biological process levelwise analysis performed at depth  <dig> exhibits visual enrichments in 'cellular process' and 'physiological process' which is misleading.

results obtained with our method on the same query set, map <dig> oxidative phosphorylation, are presented in table  <dig> for enrichments  in the go biological process branch. in this table, proton, cation and electron transport appear at the 1st, 2nd and 3rd position respectively. it is worth noting that there exists other significant enrichments not reported in  <cit>  . this is due to the pre-specified level of specificity.

with our method, it is not possible to specify a given degree of specificity as with their tool gopad  <cit> . however, similar results can be obtained by constructing other neighborhoods for the gene ontology that would correspond to different level of specificity or information content. an alternative solution can also be to use go slim  instead of the whole gene ontology. more generally, by looking at all the levels of the go hierarchy, our method successfully identifies pertinent target sets, which automatically selects the most relevant levels to look at. besides, our method is more generic in the sense that it can be applied to any hierarchically defined sets. for example, it can be applied to the hierarchical clustering of gene expression profiles which results in a dendogram  or the gene localization on chromosomes . in those cases, an information content method is of no help because the degree of specificity needs to be specified a priori and this is typically not known.

integration of multiple data sources
another trend in the field is the search for enrichment in combination of features by the use of multiple data sources. the direct approach consists in intersecting target sets as proposed in  <cit>  where target sets of genes with composite go annotations are obtained. this allows to find enrichments that are significant for the composite annotation  while not being enriched in the original annotations . a similar approach has been proposed in  <cit>  where frequent co-annotations  are mined. the principle is to search for frequent itemsets in the features of the query genes , and then to look at the significance of the enrichment in the combined features. alternatively, the converse approach consists in the addition of go term relationships such as is-involved-in as proposed in  <cit> . the principle is to augment the gene ontology to connect terms from different branches  to reflect the fact that a molecular function is involved in a biological process which takes place in a cellular component.

although, the enrichment of combination of features is not addressed in this paper, similar results can be obtained by manipulating the neighborhoods. for example, it is possible to combine the go biological process and the go molecular function neighborhoods by adding nodes corresponding to the set intersections  to the hasse diagram representing the neighborhood. similarly, the augmentation of neighborhoods such as the additional gene ontology layer proposed in  <cit>  can be achieved by adding the corresponding edges between go terms and by propagating the gene products through the newly created paths. this could prove useful as we have seen in the results obtained for complex  <dig> . <dig> with the gene ygl128c that the go annotations are sometimes missing in a particular branch whereas present in another one.

numerical features
numerical features can be very interesting to consider for feature enrichment. for example, it can be used to discover that some genes of a query set are surprisingly close to each other on a chromosome, or that all the molecular weights of the query proteins fall within a surprisingly small range. to our knowledge, our approach is the sole capable of searching for enrichments in numerical features such as the gene localization on chromosomes . this might be because  it is inefficient and sometimes unfeasible to store and compare all the sets corresponding to adjacent genes and  because the redundancy in the results  makes them unexploitable.

CONCLUSIONS
in this article, we addressed the problem of the characterization of a set of genes or proteins by finding pertinent over-represented features. the key advances presented here are a formalism for representing and manipulating the data to be searched, and the introduction of the concept of target set pertinence and its formal definition. the choice of partially ordered sets as a formal representation was naturally driven by the generalization of the concept of neighborhood between genes or proteins: biological relationships  group genes or proteins into sets of neighbors, which can be nested. these foundations exhibit their strength in many aspects. first, they make it possible to take into account the structure of the data and get rid of the non informative results. second, their generic and universal aspect make them directly usable by most of the current methods and tools . third, they provide a solid basis on which to develop optimized structures and algorithms such as those presented in this article: a generic compact representation applicable to any neighborhood, a specific compact representation for trees , and an example of an implicit compact representation for gene location on chromosomes. the validation was performed by searching enriched go annotations in  <dig> protein complexes. the performances observed clearly show the usefulness of our approach: in terms of resources, we were able to save up to 73% storage for the data and to avoid up to 98% of the comparisons performed between sets during the search. more importantly, we observed up to 64% of statistically significant enriched features that were actually not pertinent and that should be discarded. this means that the biological researchers and the computational biologists will be presented far less results to interpret, making the characterization of gene sets faster, safer and easier.

in this article, we illustrated our methods with sets of genes and proteins examples but they can be applied to other data as well: formally, our approach is already general because it only considers elements of a finite set. good candidate data sets should exhibit a finite set s of elements and various neighborhood relationships. these relationships can be inferred for example from a many to many relation between elements of s and elements of another set, or, a hierarchical structure and a relation associating elements of s to nodes of the hierarchy. for example with the growing number of complete genomes available, it should be interesting to build sets of genomes based on various neighborhood relationships and test what features result in similar groupings.

the methods presented in this paper naturally lead to a new challenge: the identification of similar sets between two neighborhoods. such a task is of utmost importance as it would allow to analyze nearly automatically large amounts of data. for example, for gene expression data , the clusters of co-expressed genes would be matched to pertinent gene ontology terms. a naive solution would generate all the sets of one neighborhood and submit them as independent query sets to identify similar sets in the other neighborhood. this approach has two significant drawbacks. first, it implies the generation of all the sets of a neighborhood, which is exactly what we sought to avoid, and more importantly, the query set inclusions will cause redundancy both in the computations and in the results. second, the pertinence definition is not symmetric, that is, given two neighborhoods n <dig> and n <dig>  the results obtained will differ depending on which neighborhood  will serve as the query and the target neighborhood. this is because all the query sets are assumed pertinent, which is typically not the case. in the example of the hierarchical clustering of gene expression profiles and the gene ontology, a "two-sided" pertinence definition would allow to identify the pertinent clusters to be compared to the pertinent go terms. thus, the pertinence definition should be reviewed in this context to ideally permit the design of algorithms that search both neighborhoods simultaneously.

authors' contributions
rb wrote the computer code, carried out the experiments, and drafted the manuscript. id drafted and refined the manuscript. djs and id both provided substantial comments. all authors read and approved the final manuscript.

