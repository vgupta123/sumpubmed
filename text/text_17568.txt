BACKGROUND
medical imaging techniques have dramatically improved our ability to explore the neural processes relevant to psychiatric disorders. these techniques can be group into two classes based on type of measurements: direct and indirect. electroencephalography  and magnetoencephalography  are non-invasive modalities and directly measure electric changes associated with neural activity in the brain. a major limitation within these eeg and meg are that they can only sense the electrical activity and magnetic fields oriented perpendicular to the surface of the brain and face the challenge of identifying the source of the underlying signal. while they have superb temporal resolution, their spatial resolution is limited.

magnetic resonance imaging , functional magnetic resonance imaging , positron emission tomography , and single-photon emission computed tomography  are the major approaches utilized in neuroimaging studies and indirectly measure neural activity. mri/fmri is the most widely used method in the brain imaging because of its low risk for subjects, better temporal and spatial resolution relative to other indirect neuroimaging methods. pet measures blood flow in the brain by injecting small amounts of radioactive tracer. then, the accumulation of the tracer is scanned. similar to pet, the modality of spect uses radioactive tracers and a gamma camera to construct two- or three-dimensional images with the computer support. spect scanners are more affordable that pet scanner. both pet and spect can also be used to assess specific neurotransmitter receptor binding potential and functioning. many studies have exploited these modalities in brain research and addiction  <cit> .

however, these discoveries have not been either specific or sensitive enough to assist in the diagnosis or treatment of psychiatric disorders. thus, the identification of persons either at risk of or suffering from most psychiatric disorders, including substance use, schizophrenic, affective, and anxiety disorders, remains dependent upon descriptive signs and symptoms. brain imaging obtained from healthy and non-healthy groups can be analyzed via data-driven machine learning and data mining algorithms to elicit the key difference between subject groups. the findings may pave the path for identifying new neural mechanisms underlying these disorders as well as detecting those at risk or responsive to specific treatment approaches.

support vector machines  are relative new multivariate machine learning / pattern classification algorithms which have been intensively studied and benchmarked against a variety of techniques  <cit> . an svm  classifier seeks maximum margin separation in multidimensional  feature space in order to separate two classes with minimum error and has generalization power and feature mapping advantages over other classifiers such as bayesian, neural networks, and decision trees. the paramount advantage of svm classifiers over linear methods  is the use of a function to map original data to another multidimensional space in which linear separation yields more accuracy  <cit> . svms also offer a great deal of flexibility in that they can learn from multivariate subject data  such as demographic or clinical measures, gene expressions, or cognitive measures.

this intelligent software has been used to detect brain diseases, such as schizophrenia , alzheimer’s disease  , huntington’s disease  <cit> , attention deficit/hyperactivity disorder  , parkinson disease  <cit> , and social anxiety disorder  <cit> . classification accuracies of these studies vary between  <dig> and 100 % for two-category classification of healthy control vs diseased. one of most successful classification studies  <cit>  used linear svms to classify patients with ad from four different groups  via t1-weighted anatomic mri scans. in addition to the successful classification of ad and control participants, this technique was able to distinguish patients with mild ad from control subjects, and subjects with ad from those with frontotemporal lobar degeneration. the subjects were correctly assigned to the appropriate diagnostic category in 95 % of trials with 95 % sensitivity and 95 % specificity within loo accuracy assessment method. as for substance use disorders, only alcohol-addicted subjects have been studied with similar data mining and machine learning algorithms  so far. alcohol-dose effects on brain activation were explored using independent component analysis to isolate systematically non-overlapping networks and their time courses  <cit> . to our knowledge, there are no published studies presenting classification of cocaine dependence using spect data.

the primary impetus for the present study was to develop a clinically applicable framework to identify cocaine-dependent patients via brain imaging, using study participants assessed with single photon emission computerized tomography   <cit> . the main aim of this study was to determine the brain regions to optimally classify cocaine dependents versus healthy controls using measures of regional cerebral blood flow . we also wanted to explore whether the brain regions that classified cocaine-dependent vs. healthy controls would be related to cortico-striatal-limbic systems relevant to the addictive process . on the other hand, the framework that was developed in this work does not depend on any particular experimental task, which means that the framework can be applied to and tested on spect data from studies which study other types of brain disorders.

materials and methods
participants and data acquisition
ninety three two- to four-week abstinent cocaine-dependent and  <dig> healthy control participants,  <dig> to 48 years old, were studied . all participants underwent a medical history and physical examination, structured clinical interview for diagnostic and statistical manual of mental disorders-fourth edition , clinical laboratory tests and urine drug screen. t1-weighted mri scans were obtained from all but the first  <dig> subjects  to enhance spect registration and rule out anatomic abnormalities. financial compensation was provided to the participants for their involvement. approval for the study was obtained from the institutional review boards of the university of texas southwestern medical center at dallas and the va north texas health care system.table  <dig> demographics of participants




cocaine-dependent subjects were recruited from patients obtaining residential treatment for cocaine dependence at the va north texas health care system in dallas, homeward bound, inc. and the nexus recovery center. all cocaine-dependent participants endorsed cocaine as their primary drug of choice. cocaine-dependent participants were hospitalized as soon as possible after their last reported use of cocaine and remained in a structured, residential unit until the initial scan was completed. participants were excluded from participation if they took any central nervous system active medications  or had any major medical or neurological disorders, active affective, anxiety or psychotic disorders , axis i disorders, or organic brain syndrome. women were all premenopausal. a negative pregnancy test was obtained on all female subjects prior to spect scanning.

healthy controls were recruited through local ads in newspapers, the internet and notices on bulletin boards. exclusion criteria for healthy controls included the criteria as noted for the cocaine-dependent subjects, as well as a lifetime history of substance use or other axis i disorder . healthy controls with a first-degree relative or two or more second-degree relatives with a substance-use disorder were also excluded.

study sessions took place in the afternoon at the nuclear medicine center or the clinical trials office at the university of texas southwestern medical center at dallas. participants from three studies were included: study i) subjects  participated in two study sessions to assess limbic sensitivity to the local anesthetic procaine  <cit> . saline was administered in the first session. subjects were blinded to condition. study ii) subjects  participated in four sessions to assess cholinergic and 5ht <dig> receptor systems. saline was administered in one of the four sessions; study order was double-blind and randomized  <cit> . study iii) subjects  were assessed at rest  <cit> .

spect images were acquired on a prism 3000s three-headed spect camera  using low energy ultra high-resolution fan-beam collimators  in a 128 ×  <dig> matrix in three-degree increments. for each scan, 20 mci of 99mtc hmpao was administered, and total scan duration was 23 min. image reconstruction was performed in the transverse domain using back-projection with a ramp filter. the voxel size in the reconstructed images were  <dig>  mm <dig>  reconstructed images were smoothed with a fourth-order butterworth 3-d filter, attenuation corrected using a chang first-order method with ellipse size adjusted for each slice.

to register spect images more accurately, a rigid-body co-registration of the spect scan to a skull-stripped t1-weighted high-resolution  structural mri scan of the same subject transformed the spect image into the same space as the mri. spatial transformation parameters were then calculated using the statistical parametric mapping  to warp the mri into standard mni  <cit>  space. the same transformation was then applied to the co-registered spect image and output images were resliced to 2 mm3 voxels. all images were smoothed to a final resolution of 10 mm and the voxel signal values normalized to whole brain counts . all scans were combined and mapped into 2d matrix where each column was a subject and each row was a feature . in this representation all non-mask voxels were eliminated, reducing the feature space twofold. all statistical analyses were carried on a 64-bit  <dig>  ghz pc using matlab scientific programming language  <cit> . in the reporting of brain regions, we used automated anatomical labeling  <cit>   atlas with the dimension of 79 × 95 × 69 voxels.

the framework
we designed the framework in which the input is the normalized spect data of participants from both groups, cocaine-dependent and healthy control. the framework here is not task-dependent, which means that the classification framework is also applicable to other similar neuroimaging studies. since all spect images were normalized to aal mask, it is proper to consider a voxel  as a feature at the very low level, representing the subject. therefore, an array of voxels from the same spatial location in 3d for each participant represents one-dimension of the multidimensional classification space. through this study, the term of feature is used for the voxels in the cerebrum.

referring to fig.  <dig>  we first eliminated all non-aal mask voxels from the data set of  <dig> participants . after the elimination, the imaging data set was represented as a 2d matrix, where one dimension is used for voxels , and the other dimension denotes the participants . numerous non-informative voxels were eliminated using information gain method  in order to reduce the feature space. after this step, individual voxels which are not a member of a connected cloud of voxels  were iteratively removed . the classification accuracy was assessed with an svm classifier ; and the least significant r voxels  were removed, where r was set to  <dig> empirically. the loop of sub-sections fig. 1c–e continues to run until there is no voxel to be used in classification . further details of the framework are described in the following subsections.fig.  <dig> feature selection, parameter selection, classification, and reproducibility framework. to find the best classification model, the framework is started with single photon emission computerized tomography  scans  . information gain algorithm  removes non-informative voxels. a loop of parameter selection and support vector machines -based feature selection then takes place. only voxel clusters with size ≥ <dig> are kept in the dataset  with dbscan, a density-based clustering method. at each iteration, the dataset is trained and tested ; svm’s feature elimination  refines voxels before next dbscan run. when there was no more than 100 voxels, parameter search was ended. at the next steps, model, 10-fold cross validation , and leave-one-out  classifications were carried out and accuracies and set of selected voxels were identified 




we considered principal component analysis  as a feature selection method before the other information-thoretic approach. however, the issue of computational complexity of pca made us search for another method. note that the voxel size of mask is n =  <dig> , and we had m =  <dig> participants. so, the covariance matrix c ∈ ℝnxn, which has time complexity of o. furthermore and addition computation with o required for singular value decomposition applying to resulting matrix of first step. a pca is also requires a great amount of the memory for the matrix calculations. we did experiments to find principle component and received this error: matrix of  <dig> x 203633 =  <dig> elements is too large to be allocated using a single java array. note that 203633 = 203632 + class variables with 4-byte unit size requires around 155 gb memory to calculate covariance matrix and following eigenvalue calculations, which is impractical with a mediocre computer.

information gain
the whole unprocessed dataset consisted of a concatenated 162 ×  <dig>  matrix from all of the brain scans. a spect image of participant in the dimension of 79 × 95 × 69 =  <dig>  voxels, which leaves  <dig>  voxels after brain extraction and thresholding using the aal brain atlas. this meant that we should device a classifier to deal with all  <dig>  features at-large, which is practically infeasible. as done in many classification frameworks , we reduced the number of voxels by selecting only the most informative ones. to find out the optimal analysis for initial voxel selection, the gaussian distribution of each voxel over all sample was investigated first. since we found out that only  <dig>  % of  <dig>  voxels were normally distributed with the method proposed by lilliefors et al.  <cit>  in both groups, we opted not to use the traditional statistical methods to reduce the size of features. information gain  <cit> , an information theoretic-based feature reduction algorithm, was employed in this step. as a result,  <dig>  of the  <dig>  brain voxels were identified as significantly informative in the classification of the two groups of subjects. information gain, also known as kullback–leibler divergence, is a non-parametric method used to select a feature that reflects minimum randomness in class distributions. more formally for a two-class problem, it is given as


ig = − p
1 log2
p
1 − p
2 log2
p
 <dig> 

where, p
 <dig> and p
 <dig> are the probabilities that the voxel v belongs to class  <dig> and  <dig>  respectively. this first step of entropy-based voxel selection served as a blind dimension selection and discarded all but voxels with ig >  <dig> regardless of the spatial or informational correlation between pairs of voxels .

clustering voxels in 3d
following the removal of many features with information gain, we are left with  <dig>  voxels that are from different locations from the aal bring regions. before the classification step, we removed individual voxels which are spatial proximity of a group of other selected voxels. we wanted to determine the minimum cluster size  which would provide an overall false discovery rate of  <dig>  and a voxel-level false discovery rate of  <dig> . we used the alphasim utility of afni software which runs monte-carlo simulations, and we determined cluster size to be  <dig>  <cit> .

a spatial case of a density-based algorithm for discovering clusters in large spatial databases with noise   <cit>  with ϵ= <dig> minpts= <dig> was used. the ϵ and minpts are two parameters for dbscan clustering algorithm to fine-tune how far a boundary of a cluster can go and how dense at least each cluster can be, respectively. the conditions of ϵ= <dig> minpts= <dig> requires that the minimum size cluster be two and these two voxels should be next to each other sharing a common edge ϵ= <dig> in 3d space .fig.  <dig> during the expansion of clusters  in 3d, a cluster can grow via those voxels that are sharing a common edge with the one of existing cluster’s voxels. this property is regulated by parameters of ϵ= <dig> minpts= <dig> . in this 3d figure, each cube represents a voxel. the centered  voxel  and only five  of its twelve common-edge neighbors are depicted for the sake of simplicity. note that point-based neighborhood  between two voxels does not satisfy the condition of cluster expansion




let k be set of voxels resulted from either first feature selection, fig. 1b or svm feature elimination step, fig. 1e. the pseudocode of a special case of dbscan is presented in fig.  <dig>  it performs only one pass in the set of voxels, k, and finds all clusters under a given parameter conditions above. at the beginning all voxels are labelled as unclustered. for each voxel that is not yet clustered, dbscan checks whether this voxel, v, is a core . this is simply to check if v has at least one common-edge neighbor. if the voxel is a core, a new cluster is expanded starting with this voxel . otherwise, the voxel is labelled as a non-member . to expand an existing cluster, dbscan begins by inserting all common-edge neighbors of the initial voxels into a queue . for each voxel, y, in the queue, the algorithm finds all common-edge neighbors of y and inserts only voxels that are unclustered yet and not the member of queue into the queue. this is repeated until the queue is empty. since each voxel of a cluster is labelled with a clusterid, they are not process again in the later stage of the algorithm. the dbscan algorithm labels each voxel either a member of a cluster or a non-member. at the end, all non-members voxels and members of a cluster with less than 20 voxels are removed from the data set. for instance, in the first run of dbscan algorithm,  <dig> out of  <dig>  voxels are removed data set either because of they are not a member of any cluster or they could not form big enough clusters .fig.  <dig> the pseudocode of the modified dbscan algorithm to find group of voxels through the processes of feature selection




a statistical classifier: support vector machine
from the machine learning perspective, classification is the process of mapping a new data sample  to one of known labels where rules or functions are induced from a training population. in this study, the size of training data was  <dig> – round =  <dig> participants in the case of 10xcv, and  <dig> – 1 =  <dig> in loo. training and test datasets were normalized between − <dig> and  <dig>  identification of cocaine-dependent participants in a cohort with healthy controls is a binary classification and it was carried out via the svm statistical classification algorithm, svm. note that in this study svm was used as both classification tool and feature selection method. in the classification phase , all voxels from the dense cluster found with dbscan clustering algorithm were fed to svm as a feature set.

in many svm classification problems the resulting classifier cannot be visualized because of high dimensionality. for instance, given the fact that we worked on this study with thousands of voxels, it would be impossible to present the classifier for human perception. hence, a toy example with only two features and  <dig> samples  are depicted in fig.  <dig>  where the resulting hyperlines of linear and polynomial kernels of svms are shown to visualize kernel effect in classification. furthermore, two svm models classifying all of the participants with only two features  are visualized in fig.  <dig>  in each sub-figure, the resulting separating line  with corresponding training kernel is shown. the accuracy of obtained models were not same. in fig. 5-upper, the problem space was divided into two sub-regions and yielded an accuracy of  <dig>  . however, in fig. 5-lower, hyperplane is polynomial to include more patients in correct regions. for instance, the subject with the left superior parietal  expression around  <dig> and right superior temporal pole  around  <dig> was misclassified with linear kernel. however, the same subject was correctly labeled as cocaine-dependent once svm classifier was trained with a polynomial kernel. particular to this comparison, the mapping of data with a polynomial kernel increased the model accuracy of the system from  <dig>  to  <dig> , meaning that three more participates are correctly classified. an svm classifier labels the group/category membership  by defining a hyperplane in multi-dimensional space, separating group-specific features . however, the large number of voxels in spect images in our case creates unmanageably high dimensionality, requiring that only a subset of selected features is used in the classification algorithm. therefore, the framework introduced in section  <dig>  includes two levels of feature selection schema: 1) dimension reduction with information gain reduced the number of voxels to a manageable set, i.e., from  <dig>  to approximately  <dig>  voxels ; 2) svm-based feature selection reduced size of voxels from 1000s to the order of 100 s , iteratively.fig.  <dig> in this toy illustration, the hyperlines  of support vector machine  separate cocaine-addicted  from healthy control  participants via two features f <dig> and f <dig>  left panel: the kernel function, which maps a data point to another dimension, is in the form of Φ. Φ, which produces a linear decision boundary. right panel: the separating line is non-linear, since a polynomial kernel, . Φ) <dig>  is used to map the data. in this case, the decision boundary is non-linear, placing more cocaine-addicted participants in the correct regions. for instance, two of the cocaine-addicted participants and one healthy participant pointed with green arrows are misclassified with a linear kernel. once trained with a polynomial kernel, the decision boundary is more flexible resulting in fewer misclassified participants. the use of the polynomial kernel increases the accuracy


fig.  <dig> the hyperplanes  of svm model separating patient groups via two voxels from right superior temporal pole  and left superior parietal  with different kernel functions.  the kernel function was in the form of Φ. Φ, which produces a linear decision boundary having  <dig> out of  <dig> subjects were correctly classified.  the separating line is non-linear since a polynomial kernel . Φ) <dig> was used to map the data.  <dig> out of  <dig> subjects were correctly classified




through these iteratively refined steps, the most significant voxels remained in the dataset by removing the less significant ones. at each step with the refined voxel set, a new svm classifier was trained to separate controls from the cocaine-dependent participants. because of the nature of svm’s heavy dependence on parameter selection, in each iteration, training and classification were done using various parameters and kernels. the list of kernels and parameters are given in table  <dig>  the classification was performed using all of the parameter combinations listed in table  <dig>  and the parameter combinations which yielded the best classification accuracy result are reported in the results section below.table  <dig> list of various parameters used with svm on the dataset


c



since one of the aims is to elicit brain regions that significantly contribute to the svms classifier, we kept all  <dig> subjects in the main loop of the framework. once we determined the best-possible parameter and voxel sets, their classification power was evaluated with loo and 10xcv methods.

support vector machine based feature selection
in a classification framework, features  are information carrying representatives of samples . in this context, feature selection involves removal of insignificant features aiming for a better classification accuracy with the remaining features. for the feature selection sub-section in the loop of framework, an svm-based approach was adapted since svm is being used in the classification of controls and cocaine-dependent participants in the previous step. guyon et al.  <cit> , for example, showed that their recursive feature elimination technique utilizing svm yielded better accuracy than correlation-based methods in a dna microarray dataset, which is similar to this study in terms of high dimensionality of voxel data set and the machine learning task of classification of control and diseased subjects.

the feature elimination framework started with the set of possible significant features to be used in the classification algorithm . this set of features was then refined through the elimination of non-significant ones from the initial set. features to be removed were selected utilizing weight vector w ∈ ℝn of linear svms. since each component of w corresponds to a feature in the classification problem, guyon et al.  <cit>  showed that the larger |w
j|, j ≤ n , the more contribution to decision in the classification. in the previous notation, n is number of voxels and |w
j| is the length of j
th component of the weight vector w. after each classification attempt in the framework, a linear svm classifier was run and  <dig> least significant features were removed from the data set. when comparing to the information gain feature selection method which was used in the first step to remove vast number of voxels, the one proposed by guyon et al. is more refined, i.e. more successful in sorting out the voxels which contribute most to the classification.

classification software and accuracy
libsvm  <cit>  was used for the training of svm models and classifications of participants. the accuracies obtained with 10xcv and loo assessment methods along with sensitivity and specificity are reported. in 10xcv, the dataset  was divided into  <dig> non-overlapping quasi-equal class distribution partitions. in each of the  <dig> folds, one partition  was held as test data, s, while a model was built with the remaining nine partitions . s is also called the validation set. through this method, every participant is entered in the test set one time, and in the training set nine times. finally, the average of accuracies from each fold was reported.

the loo is the exhaustive version of k-fold cross-validation with k = n =  <dig>  and it simply avoids combination-driven calculation problem of k-fold cross-validation. in loo, we exclude only one test subject, s =  <dig>  from the group of n =  <dig> and classify whether s is dependent or healthy using the model built based on the remaining n – s =  <dig> subjects, which constitute the training group. in turn, each subject is considered as s once, and this classification process is repeated n =  <dig> times for each of the subjects. the accuracy of model is reported in each case , and an average of all  <dig> accuracies is reported. note that svm classification models obtained for each training dataset result in different but similar classification model  even if the svm is trained with exactly same parameters and constraints. in general, a dataset with significant informative features would be more robust to removal of a particular s assuming that the other subjects who are of the same class as s will cover the missing information excluded by the removal of s.

f-measure was the criterion to choose the best classification model. f-measure, extensively used in information retrieval domain, is the harmonic mean of precision and recall. recall is the percentage of positive labeled instances that were predicted as positive and found by true positive/. precision is defined as the percentage of positive predictions  that are correct, and calculated as true positive/. based on given ratios, the f-measure was calculated as 2*precision*recallprecision+recall. if the same f-measure is obtained from several svms, the one with highest recall is selected.

RESULTS
feature selection and classification accuracy
since only  <dig>  % of the  <dig>  voxels were normally distributed, information gain returned only  <dig>  voxels as a starting set . while the voxels are iteratively refined in the loop of the framework, it was found that polynomial kernel, f =  <dig> . Φ + 10) <dig> with coefficient r =  <dig>  and penalty parameter c =  <dig> within a c ‐ svms yielded the best average classification  accuracy using 1500 voxels. figure  <dig> shows how average accuracy changed over number of selected voxels.fig.  <dig> 10-fold cross-validation accuracies . these result are obtained with support vector machines , polynomial kernel, f =  <dig> . Φ + 10) <dig>  coefficient r =  <dig> , and penalty parameter c =  <dig>  the 10-fold cross validation accuracy peaks with 1500 voxels in  <dig> clusters




to select the parameter set for svm and voxel clusters, three different types of schemas, focusing 1500 voxels in  <dig> clusters with at least  <dig> spatially connected voxels in each, were explored. model accuracy for all assessments  was  <dig> , meaning that both groups were perfectly separable in a higher dimensional space, which had all  <dig> participants mapped by a degree-four polynomial kernel. the f-measure of loo and 10xcv were  <dig>  and  <dig> , respectively. sensitivity and specificity were  <dig>  and  <dig>  for loo;  <dig>  and  <dig>  for 10xcv, respectively. similar results for loo and 10xcv indicated that the classification model build using  <dig> clustered voxels appeared robust to the exclusion of either one or  <dig> subjects.  <dig> of  <dig> clusters showed significant features having p-value less than or equal to  <dig> . all identified clusters and corresponding regions are detailed in the fig.  <dig> fig.  <dig> regions that is used to classify cocaine-dependent and controls participants with the best accuracy. 1500 voxels in  <dig> clusters were identified. figure shows sagittal sections of region-of-interests  where 100 % model, 89 % loo, and 88 % 10xcv accuracies were obtained. red identifies clusters of increased regional cerebral blood flow  in cocaine-dependent participants relative to controls. blue identifies clusters of decreased  in cocaine-dependent participants relative to controls. slice numbers are in mni coordinates. mni coordinates of each cluster and images in axial planes are provided in the additional file 1





identified regions of interest
of the  <dig> clusters used to successfully classify cocaine-dependent and control participants,  <dig> showed relative rcbf increases in cocaine-dependent compared to control participants and three showed rcbf decreases in cocaine-dependent compared to control participants . a large cluster in the left superior parietal gyrus, encompassing almost 20 % of the voxels used in classification, showed higher rcbf in the cocaine-dependent participants relative to controls. other clusters of increased rcbf in the cocaine-dependent participants included the right and left pre- and post-central gyrus and cerebellum, the left transverse temporal gyrus, inferior parietal lobule, thalamus, parahippocampus, posterior cingulate, and cuneus, and right middle temporal gyrus, lingual gyrus and precuneus. clusters with decreased rcbf in the cocaine-dependent participants, relative to controls, were restricted to the left lateral ofc and bilateral superior temporal cortex.

discussion
it is shown that a machine learning framework based on svm-based classifier and feature selection method and primarily supported with a density-based clustering tool successfully classified cocaine-dependent from healthy controls individuals with  <dig>  loo and  <dig>  10xcv accuracies. sensitivity, the ability to correctly identify those having the disorder, was  <dig> . given these high classification rates, determined by cross-validation, our final svm model may offer insights into the pathogenesis of cocaine addiction.

several clusters successfully classifying cocaine-dependent participants and healthy controls are highly relevant to the addictive process, including regions relevant to cognitive control   <cit> , default mode network related self-referential thought   <cit> , behavioral inhibition   <cit> , and contextual memories   <cit> . perhaps of equal note are some regions intimately associated with the addictive process that were not identified in the classification process . similarly, hyperactivity of prefrontal cortex in addiction subjects was reported in  <cit> . in our attempt to limit false positives, at least  <dig> spatially connected voxels were required during feature selection. thus, smaller–but physiologically relevant–clusters may have been missed. conversely, a number of clusters important to our classification did not encompass regions typically associated with addictive processes, highlighting the potential importance of a theoretical statistical approaches for identifying relevant–but unexpected–brain regions. our findings, therefore, highlight the importance of utilizing whole brain analyses to identify regions useful in discriminating persons with addictive disorders from healthy controls. svm classification of resting state functional connectivity has also been used to successfully classify heroin-dependent subjects and healthy controls, although the study population was limited to  <dig> participants  <cit> .

although classification was conducted in a binary fashion, i.e. positive  or negative , brain alterations may occur over the course of an addiction and may differ depending upon disease severity. thus, an extension of the svm approach could consider probabilistic classifiers in the future, allowing the identification of specific subgroups of addicted patients . pariyadath et al., for example, has recently identified resting state neural networks predictive of nicotine dependence using an svm-based classification approach  <cit> .

strengths of our approach included a relatively large sample of cocaine-dependent participants at least 2– <dig> weeks abstinent, precluding the acute and withdrawal effects of cocaine that confound imaging studies conducted during the first several days of abstinence. participants were without other active dsm-iv substance dependent  or psychiatric disorders and were not taking psychotropic medications. the spatial resolution  of the spect, 4 × 4 ×  <dig> cubic mm voxel size, and 20 voxel cluster restriction provided a minimum cluster size well within the resolution of our device. potential limitations included the use of highly selective populations dissimilar from typical clinical populations that may limit the generalizability of our findings. also, the use of both saline and resting scans offers a possible confound, although we have previously reported similarities in rcbf during both scan in cohort i participants  <cit> . another limitation is that the features selection algorithms, both of information gain and svm-based feature selection, have not been tested on a completely new sample of test subjects . note that this problem only regards feature selection steps, not the classification algorithm. the loo and 10xcv approaches remedy this problem and classification algorithm run on an entirely different test subject set. the two groups differed in gender, age, and race, although consideration of these potential confounds may minimally affect the model. since other demographic variables were not consistently obtained over the span of time used to collect the three cohorts, other potentially relevant confounds  were not available for inclusion.

our findings support the use of machine learning statistical approaches in the classification of patients with substance use disorders. coupled with structural and functional neuroimaging, this approach offers a powerful technique for distinguishing neural signatures of relapse, classifying features overlapping with and/or dissimilar from other psychiatric disorders, and potentially identifying neuroplastic alterations underlying these disorders.

CONCLUSIONS
in this study it is presented that a generalizable machine learning framework can successfully classify cocaine dependent subjects using spect images. the brain regions associated with the best classification accuracy mainly point to some of the addiction related brain regions. in the future, disease state of cocaine dependency can be determined with a similar framework since the distance of each subject from a subject to hyperline, which is boundary to separate controls and dependent participates in multidimensional space, implies probability of being positive or negative in the classification. in a screening study, detecting those who are at risk or moving toward to decision boundary could benefit individually before they are acutely dependent.

this study was conducted with svm classification and svm-based feature selection algorithms. although svm is known as one of the most successful classifier for multidimensional dataset, in the future a methodological comparison study involving other classifiers  and feature selection algorithms will be conducted.

additional file

additional file 1: transverse images and mni coordinates for identified brain regions. 




abbreviations
10xcv10-fold cross validation

adalzheimer’s disease

adhdattention deficit/hyperactivity disorder

dbscandensity-based algorithm for discovering clusters in databases with noise

dsmdiagnostic and statistical manual of mental disorders

eegelectroencephalography

looleave-one-out

megmagnetoencephalography

mnimontreal neurological institute

mrmagnetic resonance

mrimagnetic resonance imaging

ofcorbitofrontal cortex

pcaprincipal component analysis

petpositron emission tomography

rcbfregional cerebral blood flow

rocreceiving operating characteristic

spectsingle photon emission computerized tomography

spmstatistical parametric mapping

svmssupport vectors machines

vaveteran administration

declarations
this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2016: proceedings of the 13th annual mcbios conference. the full contents of the supplement are available online at http://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-17-supplement- <dig> 

funding
this study was supported by nida da <dig>  da <dig>  da <dig>  da <dig>  texas a&m university-commerce graduate school research grant, and the ut southwestern center for translational medicine ul1tr <dig>  the authors thank the staff of the substance abuse team at the va north texas health care system, homeward bound, inc., and the nexus recovery center for their support in the screening and recruitment of study subjects. ceretec  was generously supplied by ge healthcare. experiments and computational results described in this study were performed using resources from the texas a&m university-commerce high performance cluster computing lab. hpc lab resources were obtained through grants and funds from doe  and l <dig> communications. funding of this publication is provided by texas a&m university-commerce and university of texas southwestern medical center.

availability of data and materials
not applicable.

authors’ contributions
mm and ba initiated the project. the data was collected and analyzed by ba and md. tsh did normalization and statistical analysis. mm designed the framework. us analyzed the voxel clusters and provided a secondary analysis in afni. mm develop the software for the feature selection, clustering, and classification experiment. ba, md, and js provided mentoring for the study and significantly contributed to the discussion section. all of the authors contributed, read, and approved the manuscript.

competing interests
the authors declare that they have no competing interests.

consent for publication
not applicable.

ethics approval and consent to participate
this study is a secondary analysis of data obtained from the study nida-funded study, “impulsivity, neural deficits and cocaine addiction”  and “impulsivity, neural deficits and cocaine addiction” . human subjects were studied under the auspices of the ut southwestern medical center  and va north texas health care system . informed consent was obtained from all participants.
