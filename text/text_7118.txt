BACKGROUND
orthogonal projections to latent structures   <cit>  is a linear regression method that has been employed successfully for prediction modelling in various biological and biochemical applications  <cit> . among the benefits provided by the opls method is its innate ability to model data with both noisy as well as multi-collinear variables, such as spectral data from metabolic profiling and other omics platforms  <cit> . the opls method employs the descriptor matrix x , where n denotes the number of samples and k the number of variables in x, to predict the response matrix y , where m denotes the number of variables in y. the unique property of the opls method compared to other linear regression methods is its ability to separate the modelling of co-varying variation from structured noise, defined as systematic y-orthogonal variation, while simultaneously maximising the covariance between x and y.

the opls algorithm models the variation in the data matrix x by means of two sets of latent variables  <cit>   tp and to; see equation  <dig>  here, tp  denotes the y-predictive score matrix for x, ppt  denotes the y-predictive loading matrix for x, to  denotes the corresponding y-orthogonal score matrix, pot  denotes the loading matrix of y-orthogonal components and e denotes the residual matrix of x. both the y-predictive and y-orthogonal score matrices describe properties of the modelled observations that are useful for identifying expected and unexpected trends, clusters or outlying samples in data. the relationship between opls and other linear regression methods is discussed explicitly elsewhere  <cit> .

  x = tpppt + topot + e  

kernel-based pattern recognition methods  <cit>  such as support vector machines   <cit> , kernel-pca   <cit>  and kernel-pls   <cit>  have previously been applied in a multitude of contexts for exploratory analysis and classification, including biological applications  <cit> . common among these kernel-based methods is their application of the 'kernel trick'  <cit> ; allowing the kernel matrix to be treated as dot products in a high-dimensional feature space. specifically, this is achieved by adopting a linear method to so-called dual form, so that all instances of the descriptor matrix x are expressed in terms of dot products, e.g. xxt. subsequently, xxt is substituted for the kernel gram matrix k with entries ki,j = k, where xi and xj corresponds to the ith and jth row-vector in the descriptor matrix x, respectively, and k represents the kernel function. hence, one can avoid explicitly mapping x to higher-dimensional spaces as well as computing dot products in the feature space, which is computationally beneficial. the transformation to higher-dimensional spaces is performed implicitly by the kernel function k; where common kernel functions include polynomial or gaussian functions .

  k = p  

  k = exp 

the kernel functions in equations 2– <dig> depend on the choice of the parameters p and σ, respectively, which typically influences the predictive ability of the kernel-based method. the traditional approach to kernel parameter selection is to pre-define parameter limits and subsequently perform an exhaustive grid search over the entire parameter space. at each setting, the generalisation properties of the model are evaluated using e.g. cross-validation  <cit>  to identify the parameter setting yielding the lowest possible generalisation error. unfortunately, even moderately short step sizes can result in a large number of evaluations and unacceptable run times. the alternative in such cases is to utilise stochastic methods, such as simulated annealing  <cit> , which may identify reasonable approximations of the global generalisation error minimum using less evaluations.

the kernel-opls method  <cit>  is a recent reformulation of the original opls method to its kernel equivalent. k-opls has been developed with the aim of combining the strengths of kernel-based methods to model non-linear structures in the data while maintaining the ability of the opls method to model structured noise. the k-opls algorithm allows estimation of an opls model in the feature space, thus combining these features. in analogy with the conventional opls model, the k-opls model contains a set of predictive components tp and a set of y-orthogonal components to. this separate modelling of y-predictive and y-orthogonal components does not affect the predictive power of the method, which is comparable to kpls and least-squares svms  <cit> . however, the explicit modelling of structured noise in the feature space can be a valuable tool to detect unexpected anomalies in the data, such as instrumental drift, batch differences or unanticipated biological variation and is not performed by any other kernel-based method to the knowledge of the authors. pseudo-code for the k-opls method is available in table  <dig>  for further details regarding the k-opls method, see rantalainen et al.  <cit> .

implementations of various kernel-based methods are available in the literature for the r and matlab environments. among the r packages available on cran  <cit> , a few relevant examples include kernlab , e <dig>  and pls . kernlab provides a number of kernel-based methods for regression and classification, including svms and least-squares svms, with functionality for n-fold cross-validation. the e <dig> package contains functions for training and prediction using svms, including  n-fold cross-validation. the pls package includes an implementation of both linear pls as well as a linear kernel-based pls version. this enables more efficient computations in situations where the number of observations is very large in relation to the number of features. the pls package also provides a flexible cross-validation functionality.

matlab toolboxes implementing kernel-based methods include e.g. the svm and kernel methods matlab toolbox  <cit> , least squares – support vector machines matlab/c toolbox  <cit>  and libsvm  <cit> . the latter contains a general collection of svm related algorithms implemented in c++ and java, including interfaces for matlab, python and a number of other environments. all of these packages provide implementations of various kernel-based methods as well as cross-validation functionality and basic plot functions. additional kernel-based software packages can be found at kernel-machines.org  <cit> .

an implementation of the original linear opls method  <cit>  is available in the windows-based software simca-p+  <dig>  . simca-p includes a vast number of visualisation features as well as n-fold cross-validation functionality to estimate the number of y-predictive and y-orthogonal components.

here, we describe an implementation of the k-opls algorithm for r  <cit>  and matlab  licensed under the gnu gpl. to the best knowledge of the authors, there are no other software packages currently available that implement the k-opls method. the package includes fundamental functionality for model training, prediction of unknown samples and evaluation by means of cross-validation. included is also a set of diagnostic tools and plot functions to simplify the visualisation of data, e.g. for detecting trends or for identification of outlying samples.

the k-opls method can be used for both regression as well as classification tasks and has optimal performance in cases where the number of variables is much higher than the number of observations. typical application areas are non-linear regression and classification problems using omics data sets. properties of the k-opls method make it particularly helpful in cases where detecting and interpreting patterns in the data is of interest. this may e.g. involve instrumental drift over time in metabolic profiling applications using e.g. lc-ms or when there is a risk of dissimilarities between different experimental batches collected at different days. in addition, structured noise  may also be present as a result of the biological system itself and can therefore be applied for the explicit detection and modelling of such variation. this is accomplished by interpretation of the y-predictive and the y-orthogonal score components in the k-opls model. the separation of y-predictive and y-orthogonal variation in the feature space is unique to the k-opls method and is not present in any other kernel-based method.

the utility of the k-opls software package is demonstrated by means of a metabolic profiling data set from a biological study of hybrid aspen, where the k-opls method is compared in parallel to the similar kpls method.

implementation
the k-opls algorithm has been implemented as an open-source and platform-independent software package for matlab and r, in accordance with  <cit> . the k-opls package provides functionality for model training, prediction and evaluation using cross-validation. additionally, model diagnostics and plot functions have been implemented to facilitate and further emphasise the interpretational strengths of the k-opls method compared to other related methods.

the following features are available for both matlab and r:

 estimation  of k-opls models
an implementation of the pseudo-code in table  <dig> for modelling the relation between a kernel matrix k  and a response matrix y using a predictive and ao y-orthogonal score vectors.

 prediction of new data using the estimated k-opls model in step 
an implementation of the prediction of yhat  given a test kernel ktest .

 cross-validation functionality to estimate the generalisation error of a k-opls model
this is intended to guide the selection of the number of y-predictive components a and the number of y-orthogonal components ao. the supported implementations are:

• n-fold cross-validation. data is split into n separate groups and models are sequentially built from n- <dig> groups while the nth group is predicted and used to measure the generalisation error.

• monte carlo cross-validation   <cit> . data is randomly split into cross-validation training and test sets. a model is built from the cross-validation training set while the test set is predicted and used to measure the generalisation error. the procedure is repeated n times to achieve a distribution of prediction errors.

• monte carlo class-balanced cross-validation . same as regular mccv except that the split into cross-validation training and test sets is balanced with respect to the existing class labels.

 kernel functions
including the polynomial  and gaussian  kernel functions.

 model statistics
• the explained variation of x .

• the explained variation of y .

• prediction statistics over cross-validation for regression tasks .

• prediction statistics over cross-validation for classification tasks .

 plot functions for visualisation
• scatter plot matrices for model score components.

• model statistics and diagnostics plots.

code examples for the functionality described above is available in additional file  <dig> for both matlab and r. the k-opls package, including source code and documentation, is available for different operating systems in additional files  <dig>   <dig>   <dig> or for download on the project home page .

RESULTS
the utility of the method has previously been demonstrated using simulated data and for applications in analytical chemistry  <cit> . here, we describe a biological data set originating from a study measuring differences in biochemical composition across two genotypes of hybrid aspen. the genotypes will be denoted mutant and wild-type  throughout. samples have been taken from three biological replicates of each genotype at eight different positions of the tree , constituting  <dig> different observations, of which  <dig> are included in the analysis . the internode gradient denotes an approximate growth gradient of the tree. metabolic profiling data has been collected by means of high-resolution magic angle spinning proton nuclear magnetic resonance  spectroscopy. data pre-treatment, including bucketing and removal of residual water, is described in the original study  <cit> .

the modelled descriptor matrix x  contains the nmr data and the response matrix y  contains the genotypes labelled as - <dig> and + <dig>  the aim in this study is to predict an unknown sample into the correct category  based on the metabolic profile. an additional  <dig> samples were used as an independent test set to further estimate the generalisation error. both data sets were column-wise mean-centred prior to modelling.

a k-opls model was fitted using the gaussian kernel function with σ =  <dig> , one predictive component  and nine y-orthogonal components  as recommended by seven-fold cross-validation. the model statistics r2x =  <dig> %, r2y = 100% and q2y =  <dig> %  suggests a highly predictive and general model. the predictive score vector t1p is plotted against the first y-orthogonal score vector t1o in figure 1a. the discriminatory direction is described by t1p, showing that the classes are evidently well separated. from the external test set, which has been predicted into the model as shown in figure 1a, all class labels of the test samples are correctly estimated. the y-orthogonal components characterise variation that is systematic but linearly independent of the class labels. the variation in the first y-orthogonal score vector t1o describes an internode  gradient for the mutant samples but not for the wt samples, which is captured in t2o . this implies that i) the internode gradients are systematic and independent of the direction separating the different genotypes; and ii) that the internode gradients are independent across the different genotypes. from a biological perspective, this is obviously an interesting effect induced in the mutant.

from figure 1b one can also note that there is a joint internode gradient, formed by a linear combination of t1o and t2o. furthermore, figure 1b reveals a somewhat bimodal behaviour of the mutant internode gradient. in figure 1c the joint internode gradient is shown only for the mutant samples, colour-coded by biological replicate. biological replicate a displays a deviant behaviour, which is an intermediate between the profiles of biological replicates b and c and the wt samples  and explains the bimodal behaviour. also from the original study one can superficially see  that biological replicate a is an approximate intermediate of the stronger mutants b and c and the wt samples. a plausible explanation for this behaviour is that the anti-sense construct used to create the modified samples is not as strongly active in biological replicate a; either due to the process involved in generating the mutant or slight differences in growth conditions.

for comparison, a kpls model was fitted in parallel using the gaussian kernel function with σ =  <dig>  and  <dig> y-orthogonal components as recommended by seven-fold cross-validation. the first latent variable t <dig> is plotted against the second t <dig> in figure 1d. one can note that the discriminatory direction is now a linear combination of both of the latent variables . the different internode gradients are distinctly seen also in the kpls model, although the internode gradient of the wt samples is correlating perfectly with the discriminatory direction, implying that this direction is related to the class separation. in relation to the k-opls model, one can clearly see that this is not the case from figure 1a–b and previous discussions, which highlights the advantages of the k-opls method. furthermore, it is not possible in the kpls model to quantify the amount of variance related to class discrimination  in relation to the variance related to the internode gradient .

practical code examples of the functionality of the package are available in additional file  <dig>  describing both matlab and r code including illustrations from an additional demonstration data set. this demonstration data set also is available with the supplied package .

CONCLUSIONS
kernel methods have previously been applied successfully in many different pattern recognition applications due to the strong predictive abilities and availability of the methods. the k-opls method is well suited for analysis of biological data, foremost through its innate capability to separately model predictive variation and structured noise. this property of the k-opls method has the potential to improve the interpretation of biological data, as was demonstrated by a plant nmr data set where interpretation is enhanced compared to the related method kpls. in conjunction with the availability of the outlined open-source package, k-opls provides a comprehensive solution for kernel-based analysis in bioinformatics applications.

availability and requirements
• project name: kopls

• project home page: 

• operating systems: os portable .

• programming languages: matlab and r

• other requirements: matlab version  <dig>  or newer, r version  <dig>  or newer.

• license: gnu gpl version  <dig> 

abbreviations
opls, orthogonal projections to latent structures; k-opls, kernel-based orthogonal projections to latent structures; svm, support vector machine; kpca, kernel principal component analysis; kpls, kernel partial least squares; nmr, nuclear magnetic resonance; 1h hr/mas nmr, high-resolution magic angle spinning proton nmr; lc-ms, liquid chromatography-mass spectrometry

authors' contributions
mb and mr jointly implemented all provided source code, analysed the populus data set and drafted the manuscript. jkn, eh and jt supervised the project. all authors read and approved the final manuscript.

supplementary material
additional file 1
code examples for r and matlab. provides code examples  for running typical tasks using the k-opls package for both r and matlab

click here for file

 additional file 2
k-opls package version  <dig> . <dig> for r . provides the k-opls package version  <dig> . <dig> for r, built for unix-like systems 

click here for file

 additional file 3
k-opls package version  <dig> . <dig> for r . provides the k-opls package version  <dig> . <dig> for r, built for windows

click here for file

 additional file 4
k-opls package version  <dig> . <dig> for matlab. provides the k-opls version  <dig> . <dig> source code and documentation for matlab

click here for file

 acknowledgements
the authors are grateful to andreas sjödin at the umeå plant science centre, umeå, sweden, for useful comments. this work was supported by grants from the metagrad project funded by astrazeneca and unilever plc. , the swedish foundation for strategic research  and the swedish research council .
