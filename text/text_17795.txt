BACKGROUND
introduction
since the demonstration in  <dig> that different brain tumour types displayed distinct spectral patterns  <cit> , it became apparent that in order to determine whether in-vivo 1h-mrs had any clinical diagnostic value it was necessary first to gather a sufficiently large database of brain tumour 1h-mrs data and second, to perform statistical analysis of these multiple spectral features  <cit> , which is frequently known as pattern recognition analysis  or classification.

it was shown later on that it was possible to carry out a successful pr of the four most common brain tumour types, on a multicentre database of in-vivo single-voxel  1h-mrs data acquired at  <dig> t  <cit> . the study was subsequently refined during the interpret project  <cit> , which successfully developed a pr-based decision-support system to assist radiologists in diagnosing and grading brain tumours using sv mrs data. however, the need for tools that allowed a rapid development of multiple classifiers for the already existing databases available  <cit>  remained. this should also allow to rapidly test hypothesis that may surface during the lengthy process of data collection, especially in prospective studies  <cit> . this is especially relevant in the case of studies on human subjects  <cit> , for instance with multi-voxel  tumour data  <cit> .

moreover, the ever-increasing amount of biological data generated by metabolomics techniques also requires a tool allowing quick hypothesis testing on data that are difficult and expensive to gather  <cit> . in this sense, the pr analysis becomes just one stage in the iterative process of data-driven biological knowledge discovery.

however, 1h-mrs data are commonly analysed with either commercial , non commercial  or home-made programs running over statistical packages of matlab  <cit> , and usually require a certain degree of mathematical expertise for testing each individual hypothesis  <cit> . some other packages for pr and classifier development  are less complex tools, but commercial. furthermore, pirouette is platform-limited, because it is designed specifically for the windows operating system.

therefore, in order to facilitate the development of mrs-based classifiers, we developed spectraclassifier , a java software solution to design and implement classifiers based on mrs data. the main goal of sc is to allow a user with minimum background knowledge of multivariate statistics to perform a fully automated pr analysis, from the feature extraction and/or selection stage to the evaluation of the developed classifier.

the purpose of this report is to describe sc, from the algorithms implemented to its main functionalities, with a focus on the different mrs data types it is able to work with. in addition, a standard format for exchanging either sv, mv or high-resolution mrs data for pattern recognition studies will also be described.

pattern recognition techniques
pr techniques aim to recognize and classify data  into different categories using the observed features. to do this, one of the possibilities is to base the development on a machine learning  approach, in which a dataset is used to fit an adaptive model to solve the problem. ml provides the mathematical and computational mechanisms to infer knowledge in a formal model from specific data of a given domain  <cit> .

the life cycle of a pr problem based on ml is composed of two main phases: the training phase and the recognition phase. during the training phase, a set of signals from the problem domain is used to adapt a mathematical function to the output values, e.g. diagnosis, treatment, doses or risk. in this phase, the pre-processing and the features obtained from the signals are established, and the adaptive model is fitted, selected and evaluated in order to obtain the best generalization for solving new cases. once the model is ready, it can be used for the recognition of new cases. figure  <dig> shows a diagram of components of a typical pattern recognition system  <cit> , and which of these components are covered by sc: feature selection and/or extraction , classification  and evaluation .

selecting and/or extracting features
several feature selection  or feature extraction  methods based on pattern recognition have been applied to the significant part of the spectra , looking for a subset of relevant peak heights of typical resonances  or a reduced representation set of combinations of them. by removing most irrelevant and redundant information from the data, the valuable selected features help to improve the performance of learning models  <cit> .

fe works by combining the existing data features into new ones that best describe the whole dataset according to a given criterion. it is, therefore, mostly a dimensionality reduction approach. fs, on the other hand, provides a selected subset of frequencies sufficient to classify tumour cases with reasonable accuracy.

sc implements two sequential fs methods based on a "hill climbing" search , either forward or backward, and evaluates the selected features with a cfs  evaluator  <cit> . this java class evaluates the worth of a subset of attributes by considering the individual predictive ability of each feature along with the degree of redundancy between them. subsets of features that are highly correlated with the class while having low intercorrelation are preferred.

in addition to these fs methods, a fe method is also implemented: pca . pca performs a principal components analysis and transformation of the data, used in conjunction with a ranker search . dimensionality reduction is accomplished by choosing enough eigenvectors to account for a predefined percentage of the variance in the original data . attribute noise can be filtered by transforming to the principal component space, eliminating some of the worst eigenvectors, and then transforming back into the original space.

creating a classifier
the purpose of creating a classifier is to separate data vectors into one of two or more classes based on a set of features that better describe the data . in general, we assign a data vector to one of a number of classes based on observations made on the data. these classes are already known or predetermined.

at the moment, sc uses fisher linear discriminant analysis  as the technique of choice for distinguishing cases between two, three or four classes. fisher lda is a fundamental and widely used technique, that provides a reasonable way of reducing the dimensionality of the problem  <cit> . with the software, the user can assign each class to a different tumour type, or to a super-class generated by grouping tumour types  <cit> .

as the original version of fisher lda does not assume any probability distribution to define the model, the limitation of fisher lda for estimating the probability of a case of belonging to a class, has been overcome by approximating the resulting projections through spherical gaussian distributions, one for each class. the centre of each distribution has been assumed as the class mean estimated from data and the standard deviation common to all. therefore, the probability of membership of every case to each class is estimated applying the bayes' theorem over these distributions  <cit> .

evaluating models
an essential part of the life cycle of a classifier is its validation. to do that, there are evaluation methods to estimate how well the model will work with new but similar data in the future. sc implements some of the most commonly used methods for evaluating mrs-based classifiers. those are briefly described below:

- confusion matrix: each row of the matrix represents the members in a predicted class, while each column represents the actual value of members in the original class. it is a visualisation tool mainly used in supervised learning.

- cross-validation: one round of cross-validation involves partitioning a dataset into complementary subsets, performing the training on one subset, and validating the model on the other subset. in k-fold cross-validation, the original dataset is partitioned into k subsamples. of the k subsamples, a single subsample is retained as testing data for testing the model, and the remaining k- <dig> subsamples are used as training data. the cross-validation process is then repeated k times , with each of the k subsamples used exactly once as testing data. the k results from the folds can then be averaged to produce a single estimation of prediction accuracy  <cit> . in sc, the k value can be set by the user. it is typically used in scenarios where the goal is prediction, and it is desirable to estimate how accurately a predictive model will perform in practice.

- leave-one-out : is a special case of a k-fold cross-validation. it uses a single case from the original dataset as testing data, and the remaining cases as training data. this is repeated such that each case in the dataset is used once as testing data. this is the same as a k-fold cross-validation with k being equal to the number of cases in the original dataset.

- bootstrapping: it is implemented by constructing a number n of bootstrap cases of the observed dataset , each of which is obtained by random sampling with replacement from the original dataset . the n results from the bootstrap samples can then be averaged to produce a single estimation  <cit> . in sc, we set n equal to  <dig> by default, but this value can be modified by the user. bootstrapping could be better at estimating error rates in a linear discriminant problem, outperforming simple cross-validation  <cit> .

- receiver operating characteristic  curve: is a graphical plot of the sensitivity  vs. 1-specificity  for a binary classifier system as its discrimination threshold is varied  <cit> .

implementation
sc is aimed at being an intuitive and user-friendly software tool. it has been built using the java programming language , ensuring the platform independence of sc. jfc/swing classes were used to provide a graphical user interface  for the program. java runtime environment  <dig>  or later version is required to run the program.

in addition to the implementations of the methods described above, sc uses some open-source and well-known libraries, such as: weka  <cit> , a collection of machine learning algorithms, used in sc for selecting and extracting features; javastat  <cit> , developed for performing basic statistics, used in sc for the implementation of the classification method ; and king   <cit> , which is an interactive system for three-dimensional vector graphics, and is used in sc to visualise the canonical variables or the components projection.

for exchanging data between applications, the development of a standard format capable of storing all the information needed for a dataset in a readable way was required. the selected language to create this format was xml   <cit> , the meta-markup language developed by the world wide web consortium  which provides a general method of representing structured documents and data in the form of lexical trees.

RESULTS
main capabilities of sc
sc is composed by the following modules or tabs:  classifier design,  data exploration,  data visualisation,  classifier evaluation,  reports, and  classifier history. figure  <dig> is a flow chart that contains the main activities and transitions involved in the construction and validations of a classifier using sc.

in this section, the standard format definition for exchanging data and the main capabilities of sc will also be described. the development of a classifier will also be illustrated through computer screenshots. at the end of this section, some annotations about the validation with real data and computational consumption of the sc will be provided. for more detailed technical information about sc, please see the help and manual of the software in the additional file  <dig> 

standard format definition for data exchange
classifier design
when sc is launched, classifier design  is the first tab that can be seen. this tab allows the user to tune the desired input parameters for designing a classifier, such as the training datasets, the definition of classes and the selection or extraction of relevant features, which will be used as classifier inputs.

the development of a three-class classifier using sc is demonstrated throughout this report, using for this example a short te sv training dataset of mr brain tumour data from interpret  <cit> . in the example, "class 1" is named low-grade m and contains  <dig> cases of the meningiomas  type; "class 2" is named aggressive and contains  <dig> cases of the glioblastoma multiforme  type and  <dig> cases of the metastases  type; and "class 3" is named low-grade g and contains  <dig> cases of the low grade astrocytomas  type,  <dig> cases of the oligoastrocytomas  type and  <dig> of the oligodendrogliomas  type.

importing and exporting datasets
there are two ways to import data, depending on whether the user wants to work with one spectrum per case or if he/she wants to make a combination of two spectra by case and merge them for analysis. when working with one spectrum per case, a matrix will be created with each row corresponding to the spectrum of the case and using only the selected range of interest of the spectrum. in cases of two spectra by case, a matrix will be created in a similar way as when there is one spectrum per case, but in each row, the first spectrum will be followed by the second one only in the range of interest that has been previously selected .

for importing dataset files, the preferable format is xml with the structure described before. it can be used for the three types of mrs data allowed by sc. other formats can also be used to import dataset files, according to the type of mrs data:

 <dig>  in-vivo sv data, usually with a low number of points per spectrum :

 <dig> . file with extension .txt or .art in the interpret  <cit>  canonical format, with  <dig> points in the  ppm range, which only contains the information of one spectrum in one row. similarly, files with extension .dat, exported with spss or similar, and composed by rows of  <dig> tokens, where the first row is columns labels , and the rest of rows correspond to cases , having the following information each: identifier of the class, identifier of the case, and  <dig> points of the spectrum.

 <dig> . file with extension .txt, processed and exported using the magnetic resonance user interface package   <cit> . it is composed by a header and a four-column matrix of data. the header is partially used by sc, because it contains the number of points of the spectrum , and the information that allows inferring the spectral range . from the data matrix, only the third column ) is read by sc.

 <dig>  in-vivo mv data, also with a low number of points per spectrum , but with a large number of spectra per acquisition . sc treats each acquisition as one dataset:

 <dig> . file with extension .bsp, that corresponds to data pre-processed with 3d interactive chemical shift imaging v <dig> . <dig>   <cit> , and exporting the data in ascii format  <cit> . it has the following structure: first row for the name of the set , line-break, number of voxels: , line-break, number of points per voxel: , line-break, voxel index: , line-break, and then two columns with real and imaginary data.

 <dig>  high resolution data, usually with a large number of points per spectrum .

 <dig> . file with extension .txt, for hrmas. the original file having been processed with topspin  <cit>  or similar and exported as text file. the number of points accepted is variable; the most commonly used are from  <dig> to  <dig>  with a  ppm range. each file only contains the information of one spectrum in one column.

the pre-processing tasks needed for imported files are out of the scope of sc, therefore they have to be carried out before using sc, including adjustments of sweep width and number of points if spectra from different manufacturers are to be used. on the other hand, all imported datasets, training and testing sets, regardless of its original format, can be exported in the xml file format described before. figure  <dig> shows the format in which the information of the classifier will be exported.

data exploration
data exploration is the second tab of the application. it can be used to plot the spectrum of individual cases, to plot the mean and the standard deviation of a set of cases, and to display the features obtained in the fs process .

data visualisation
data visualisation is the third tab of the application . this tab can be used to visualise the position in the projection space of each case from the training and test sets after pca  or fisher lda  in two or three dimensions.

the implementation of this visualisation uses the king library, which is called from sc to load a preformatted kinemage made automatically with the information of the data visualisation. in cases of one or two dimensions , the boundaries of the classes will be calculated and displayed .

classifier evaluation and reports
the classifier evaluation tab in sc  has been developed with the purpose of identifying how well the classifier developed by the user performs and how robust it will be. current evaluation methods implemented in spectraclassifier just take into account the classifier, not involving other stages such as the feature selection or extraction . this approach can be slightly optimistic, so it is recommended to perform several tests with different numbers of features or to use an independent test set before deciding on a final classifier.

the application also generates reports  with the results obtained after creating a classifier, for training and testing data, and allows the user to export them as text files .

the classifier history tab can store the main description of the classifiers chosen by the user. it can be used to compare these classifiers, checking variations of results obtained when developing classifiers with different parameters.

validation with real data
as it can be seen in the figures throughout the text, each functionality of sc has been specifically validated with real data with the purpose of bug-testing and methods validation. data from the interpret project  <cit>  at short  and long te  were used. table  <dig> and  <dig> compile representative results of experiments conducted to validate the correct implementation and performance of the software. what was seen was that the results obtained with sc compare well with previous non-automated analyses of the same dataset  <cit> . sample brain tumour data from real patients are distributed with sc for testing purposes.

in this example, multiple binary classifiers were developed for long and short te of sv mrs data from interpret  <cit> . in  <cit>  the pca covered the 75% of the variance of the dataset. for sc, a variance of 80% has been covered. as performance measure, the auc and its standard error  were used. the number between brackets refers to the number of principal components used. the classes are: 1) glioblastomas, 2) meningiomas, 3) metastases and 4) astrocytomas grade ii.

in this example, a classifier for low grade meningioma, aggressive  and low grade glioma was developed for short, long and the combination by concatenation  <cit>  of long + short te of sv mrs data from interpret  <cit> . in  <cit> , k-random sampling train-test  with stratified test sets with  <dig> repetitions was the evaluation procedure used. in sc, a bootstrapping method with  <dig> repetitions was the one used. although both methods used to evaluate the classifiers are not exactly the same, both are equivalent sampling methods, therefore their results can be compared. as performance measure, the accuracy and the standard deviation were used.

computational consumption
the computing time needed to develop a classifier depends on the dataset size. for example, in a  <dig> ghz cpu and  <dig> gb ram personal computer, the typical performance values for interpret 512-point files  in a classification problem with  <dig> cases and three classes are  <dig> seconds for feature selection with the sequential forward method and  <dig> seconds for fisher lda. for the same problem with concatenated spectra , times are  <dig> and  <dig> seconds, respectively. for hrmas spectra of  <dig> points , feature selection takes  <dig> min with the same conditions. the computing time should be quite reduced using a high performance server.

discussion and 
CONCLUSIONS
spectraclassifier is a user-friendly software for performing pr of mrs data, which has been designed to fulfil the needs of potential users in the mrs community. it works with all types of mrs, i.e. sv, mv and high-resolution data . in addition, it also supports two concatenated spectra of the same resolution and number of points, since it had been previously shown that combination of data from two different tes can provide useful additional information for classification  <cit> .

sc allows easy data exploration, with four different spectra visualisers through which individual cases, class mean, standard deviation, as well as the selected classification features in each experiment can be explored. classification results are shown both visually and numerically. the data visualisation tab allows feedback on classification errors through potential outlier analysis by using the four spectra visualisers.

the software is limited in two aspects: first, only very basic pr techniques have been implemented yet and second, at the moment its data reading capabilities span a few formats .

with respect to the first limitation, it has been shown  <cit>  for in-vivo sv 1h-mrs data, in a multicentre multiproject evaluation of classification methods for brain tumours that in fact most methods give comparable results. this has been as well shown in other pr challenges  <cit> . for this reason, we consider that the low number of methods implemented should not be considered as a drawback of sc.

other widely used softwares  <cit>  have also this limitation, such as simca-p+, amix, and pirouette, offering methods such as pca, partial least square , soft independent modelling of class analogy , principal component regression , and classical least square .

with respect to the second limitation, i.e. format reading, in fact the lack of a common exchange format affects all areas of work of the mrs community, especially for clinical scanners. although a standard dicom had been defined  <cit> , its implementation in output formats from in-vivo human scanners at  <dig>  t  is still far from being general. for this reason, we decided to leave the pre-processing step to the user  in order to minimise the number of different formats that have to be understood by sc. the program has been made compatible with the interpret dss software for in-vivo mrs data, which is accessible at no cost, upon signature of a disclaimer form  <cit> . since interpret developed a canonical format for in-vivo mrs data at  <dig> t, it would therefore be possible for users with their own databases of  <dig> t data to process data from different manufacturers and sweep widths and number of points with the dss itself  <cit> , and to export those into sc. at the same time, it is also possible to enter jmrui-processed data into sc. jmrui is able to read most existing clinical scanner formats. the future version of jmrui , which will be able to accept plug-ins, should allow jmrui and sc to connect with one another, adding the pre-processing step to the pipeline for mrs data analysis through the developed xml format. in this way, sc relies on existing processing software for format conversion and pre-processing and concentrates on the pr process.

future sc developments include testing the software performance with other pr problems and with different data types .

in conclusion, sc is a software that accepts all kinds of pre-processed mrs data types and classifies them semi-automatically, allowing spectroscopists to concentrate on interpretation of results with the use of its visualisation tools. the classifiers created can be exported as xml files for their easy implementation into decision-support systems , such as the interpret dss  <cit> .

availability and requirements
project name: spectraclassifier 

project home page: http://gabrmn.uab.es/sc

operating system: platform independent

programming language: java

other requirements: java virtual machine  <dig> . <dig> or higher

license: available free of charge

any restriction to use by non-academics: subject to the signature of a disclaimer and user agreement text available at the project homepage.

abbreviations
auc: area under the curve; cfs: correlation-based feature subset; cls: classical least square; dicom: digital imaging and communication in medicine; dss: decision support system; fe: feature extraction; fpr: false positive rate; fs: feature selection; jfc: java foundation classes; lda: linear discriminant analysis; loo: leave-one-out; ml: machine learning; mrs: magnetic resonance spectroscopy; mv: multi-voxel; pca: principal component analysis; pcr: principal component regression; pls: partial least square; pr: pattern recognition; roc: receiver operating characteristic; sc: spectraclassifier; simca: soft independent modelling of class analogy; sv: single-voxel; te: echo time; tpr: true positive rate; xml: extensible markup language.

authors' contributions
som, io and mjs participated in the design of the application. som implemented the software. io contributed with the know-how in pattern recognition techniques. mjs and ca carried out major test of the software, and contributed with data interpretation and software validation. ca coordinated the work. all authors helped to draft the manuscript and approved the final version of it.

supplementary material
additional file 1
help and manual of spectraclassifier  <dig> . the "help and manual of spectraclassifier  <dig> " provides more detailed technical information about the software.

click here for file

 acknowledgements
this work was funded by ciber-bbn  and projects  from ministerio de educación y ciencia; and  from ministerio de ciencia e innovación in spain. partial funding also contributed by the european commission: etumour , and healthagents .

the authors also thank former interpret partners: carles majós , Àngel moreno-torres  john griffiths and franklyn howe , arend heerschap , witold gajewicz  and jorge calvar  for access to data.
