BACKGROUND
in many modern data analysis applications, the user is faced with data matrices with a huge number of columns and/or rows. such matrices arise in disciplines ranging from astronomy through genomics and social sciences to zoology. as a specific example, let us consider gene expression microarray data. in a typical study, hundreds of thousands of probe expressions are measured for a large number of samples. this methodology has had a significant impact on gene expression research, but the publication of studies with dissimilar or contradictory results has raised concerns about the reliability of this technology, especially when all the individual values of gene expressions are requested. on the other hand, when the goal is more modest, e.g., just classifying the samples into few categories, there is typically ample information available in the data, and one can hope that the huge redundancy in the data compensates for the possible errors of the technology.

in such cases, it is common to employ one of several dimensionality reduction methods in order to identify low-dimensional features for use by a downstream analyst. many popular methods, e.g., principal component analysis , multidimensional scaling, recently-popular nonlinear-dimensionality reduction methods, etc., boil down to the singular value decomposition . the singular vectors, or principal components, associated with the largest singular values have strong optimality properties, and they can often be quite useful as a tool to summarize and identify major patterns of the data.  nevertheless, it is typically quite hard for a geneticist or downstream data analyst to interpret those vectors in terms of the application domain from which the data are drawn. the reason for this is that the singular vectors are mathematical abstractions defined for any matrix, and they are typically linear combinations of all of the input data. this has been noted most explicitly by kuruvilla et al. <cit> . after describing the many uses of the vectors provided by the svd and pca in dna microarray analysis, they bluntly conclude that "while very efficient basis vectors, the  vectors themselves are completely artificial and do not correspond to actual  profiles. .. thus, it would be interesting to try to find basis vectors for all experiment vectors, using actual experiment vectors and not artificial bases that offer little insight."

to address these and other issues, mahoney and drineas
 <cit>  proposed the cur matrix decomposition method. cur decompositions are low-rank matrix decompositions that are explicitly expressed in terms of a small number of actual columns and/or actual rows of the data matrix: 

  a≈cur 

where a is the original data matrix, c consists of a small number of actual columns of a, r consists of a small number of actual rows of a, and u is a small carefully constructed matrix that guarantees that the product cur is close to a. since they are constructed from actual data elements, cur decompositions are interpretable by practitioners of the field from which the data are drawn . for example, cur decompositions have been used for interpretable data analysis of dna single-nucleotide polymorphism data
 <cit> . the theory of cur matrix decompositions works as follows
 <cit> . to determine which columns to include in c , one computes an "importance score" for each column of a and then randomly samples a small number of columns from a using that score as an "importance sampling" probability distribution. this importance score depends on the matrix a and an input rank parameter k. if
vjξ is the j-th element of the ξ-th right singular vector of a, then the normalized statistical leverage scores equal 

  Πj=1k∑ξ=1k <dig>  

for all j =  <dig> …,n. these quantities, up to scaling, equal to the diagonal elements of the so-called "hat matrix," i.e., the projection matrix onto the span of the top k right singular vectors of a <cit> . as such, they have a natural statistical interpretation as a "leverage score" or "influence score" associated with each of the data points; and they have been widely-used for outlier identification in diagnostic regression analysis.

the basic algorithm for choosing columns from a matrix—call it columnselect—takes as input any m × n matrix a, a rank parameter k, and an error parameter ε, and then performs the following. 

 <dig>  first, compute v <dig> …,vk and the normalized statistical leverage scores of equation .

 <dig>  second, keep the j-th column of a with probability
pj=min{ <dig> cΠj}, for all j ∈ { <dig> …,n}, where
c=o.

 <dig>  third, return the matrix c consisting of the selected columns of a.

in some applications, this restricted cur decomposition, a ≈ pca = cx, where x = c + a, is of interest and where c + denotes a moore-penrose generalized inverse of the matrix c.a

in other applications, one wants a cur matrix decomposition in terms of columns and rows simultaneously. the basic algorithm for this performs the following. 

 <dig>  run columnselect on a with
c=o to choose columns of a and construct the matrix c.

 <dig>  run columnselect on at with
r=o to choose rows of a  and construct the matrix r.

 <dig>  define the matrix u as u = c + ar + .

thus, in contrast to pca and the svd, where the low-dimensional basis consists of singular vectors that are linear combinations of all the data vectors, here the matrices c and r consists a small number of actual columns and rows of a, respectively. the details of this procedure, including the use of randomness, are important for the strong underlying theory
 <cit> ; but in practice several variations that exploit the structure identified by the statistical leverage scores perform very well. these practical design decisions we made for our implementation will be described in the next section.

in this paper, we describe the rcur package, which is a freely available, open source r implementation of the cur matrix decomposition method. we will summarize functionality and features of the package that allow the user to obtain the statistical leverage scores and the matrices c, u, and r by simple s <dig> classes and methods. in certain cases, we have found that the statistical leverage scores themselves are useful directly, and thus we also describe variations to select the columns or rows that deviate from the theory described above. we will then demonstrate the strength of the technique on a microarray study. in particular, we will show that even for a very large set of heterogeneous samples with various experiments, rcur is able to select a few percent of the probes that have the same classification capacity as the original full set.

finally, it should be emphasized that this cur approach is very different the classical statistical perspective, where statistical leverage scores have been used in diagnostic regression analysis to identify outliers and errors
 <cit> ; and that bien et al. <cit>  have described the connections between cur matrix decompositions and sparse optimization methods. see also
 <cit>  as a previous example in the genomics literature for gene selection via outliers.

implementation
the rcur package was developed to allow users to easily perform cur matrix decompositions. for this purpose, an easy to use primary function, called cur, was implemented. the input of the function cur is a two dimensional matrix with column and row names. if any of the column or row names is missing then the index of the dimension is assigned automatically. from the matrix a the function cur calculates the statistical leverage scores and the matrices c, u, and r. importantly, the rank parameter  has a fundamental influence on the resulting leverage scores , since only the top k singular vectors of a are used in their calculation. thus, it should be chosen carefully, based on domain-specific considerations. if no value is supplied for k, then k is arbitrarily set such that the sum of the top k singular values is more then 80% of the sum of all singular values. in our implementation the size of the resulting c, u and r matrices is not determined dynamically based on the error parameter ε. rather the number of columns  and rows  to be selected are input parameters and the actual error of the approximation is returned if requested. the outputs of function cur are stored as slots in a s <dig> class, called curobj. in addition, in certain cases we have found that the leverage scores themselves can be used directly, and thus we also provide an implementation that selects the columns or rows without involving any randomness. in particular, this involves using the statistical leverage scores as a "ranking function" rather than as an "importance sampling" distribution, and then deterministically choosing k or slightly more than k of the the highest leverage columns/rows. selecting data points or features  in this way makes the analysis more reproducible and interpretable, although we have found that in some cases the inclusion of random additional columns indeed slightly improves the precision of the approximation.

several other column selection methods are also implemented in rcur. these can be selected by the parameter "method".

random the original method described in
 <cit>  that was outlined above; this is the default.

exact.num.random like the default method, but it is guaranteed, that exactly as many rows and columns are selected as requested. 

top.scores the rows and columns with the highest leverage scores are returned deterministically.

ortho.top.scores columns and rows are selected in an iteration based on a factor that combines not just the leverage score but also the orthogonality of the next vector to the already selected subspace.

highest.ranks rows and columns with the highest rank of leverage score for some rank parameter are selected. every possible value is tried up to the value of k.

these methods are considered experimental and they provide roughly the same precision as the default method. for certain problems with highly correlated columns/rows one method  seems to be very promising. in this way the selection of multiple similar columns/rows, which does not contain new information is avoided, hence the necessary number of columns/rows can be reduced.

to extract the matrices c, u, r and the statistical leverage scores from the object curobj function, the functions getc, getu, getr and leverage, respectively, may be used. with the function top.leverage, one can get the indexes of the rows or columns with highest leverage scores as the most influential features . using these indexes one can get subset of the matrix a for further analysis.

to improve efficiency the computation of components that are not used can be switched off. in particular, if the restricted cur decomposition is required, the parameter r can be set to the actual number of rows of a. in this case row selection is skipped and x can be recovered as ua. 

in addition, with the function plot.leverage, one can plot the statistical leverage scores themselves, highlighting the largest values and indicating the uniform level directly from curobj.

for users who would like to test the functionalities of the package on published, real world data sets we incorporated the data used by paper
 <cit>  presenting cur decomposition. this is a subset of a soft tissue tumor data set
 <cit> . the  <dig> samples of dataset belong to three phenotypes gastrointestinal stromal tumor , leiomyosarcoma  and synovial sarcoma . for each sample,  <dig> gene expression values are stored in matrix sttm and annotation information in data frame stta. the original, full dataset
 <cit>  is downloadable from the gene expression omnibus database  and from stanford microarray database .

RESULTS
we illustrate the benefits of cur matrix decompositions and dimension reduction with the rcur package by comparing it with two different previously-published case studies. in the first, we show that feature selection based on leverage scores can differentiate classes with a performance similar to that of the entire gene set of a microarray. in the second, we show that cur performs well not only in the separation of classes, but in addition we can get comparable results in trend analysis with a fraction of full feature set. we provide all the code that is neccessary to reproduce these results as additional file1and additional file2

case study 1: soft tissue tumor discrimination
here our goal is to check if it is possible to separate groups with genes filtered by cur and obtain a performance similar to that with the total gene set. in this example we use a soft tissue tumor dataset, which is incorporated in the package as mentioned above . by using the rcur package, we repeated the analysis that was performed in the paper publishing the cur method
 <cit> . after running the function cur the genes with highest leverage scores are selected as the most influential features . then, using the  <dig> genes with the highest normalized leverage scores, we performed a pca, just as with the total  <dig> genes of dataset. biplots were created from the two first components from both pcas .

according to biplots, one can conclude visually that using cur as feature selection method we can discriminate the classes with many fewer variables , obtaining performance comparable to the full set.

case study 2: discrimination and trends
one of the major problems of microarray studies is that the individual probe values are not always well correlated with the expression of the corresponding gene. on the other hand, it has been shown
 <cit>  that even for a very large and inhomogeneous set of microarray samples it is possible to extract reliable information and classify the samples. in that study, the authors collected  <dig>  human samples representing  <dig> different cell and tissue types, disease states, and cell lines. the samples were part of a public international archive, which means that not only the sample parameters but the research groups doing the experiments, the goal of their studies, the sample handling, etc. were very heterogeneous. we have reconstructed their main finding in figure
 <dig>  note that, despite the mixed origin of the samples, one can clearly identify distinct classes like hematopoietic or malignant samples. after reducing the number of probes from  <dig>  to  <dig> with cur, we obtain the very same trends in the first few components. since we have several classes and many dimensions, figure
 <dig> provides just a visual demonstration of the classification.
 <cit>  showed that the first two principal components can be interpreted as "hematopoietic" and "malignancy" axes. in their study they used all  <dig>  gene expression values in the principal component analysis. the two plots on the left side are reproductions of their results. classifying the tissue of samples due to a "hematopoietic" direction  a trend can be found along the horizontal axis . using another classification of samples  a "malignancy" trend can be recognized vertically . dots represent the samples colored according to classes determined by lukk et al.
 <cit> . using the  <dig> most influential features filtered by leverage scores from cur decomposition  very similar pattern was plotted .

to make the goodness of the classification more quantitative, we apply the following metrics to measure the separation of the classes. for all group pairs all point pair euclidean distance was calculated. we measure the separation of two groups by the median of these distances for that group pair. for all group pairs these medians were summarized as a total separation measure. in figure
 <dig>  we plot the value of the measure of separation against the reduced number of genes at different k values. we can see that for most of the parameter values the cur compression not just reproduces the results of the original pca  but gives somewhat better values. it is interesting that the critical value where the separation performance of the compressed representation jumps to similar values as the full pca is k =  <dig> which is the number of the different classes. the optimal separation performance is around  <dig> genes and k =  <dig>  using less than 1% of the original  <dig>  genes.

CONCLUSIONS
the package rcur provides functions for the users to perform cur matrix decompositions in the r environment. in gene expression studies, it may give an additional way of analysis of differential expression and discriminant gene selection based on the use of statistical leverage scores. the approach proposed
 <cit>  is quite novel in the sense of interpretable dimension-reduced matrices and in handling "outliers" as the most important data points. we have also demonstrated that by using rcur it is possible to significantly reduce the number of necessary probes in classification studies. this may open the way towards much cheaper diagnostic chips.

availability and requirements
project name: rcur

project home page:http://cran.r-project.org/web/packages/rcur/index.html

operating system: platform independent

programming language: r other requirements: package mass, methods, matrix

license: gnu gpl any restrictions to use by non-academics: none

end notes
aif
c=ucΣcvct is the svd of c, then
c+=vcΣc−1uct.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
ab and ns implemented the functions of rcur. ns constructed the package, examples and performed all the analysis. ics, mm and ns wrote the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
rcur package: the r package rcur  with functions for cur decomposition.

click here for file

 additional file 2
rcur_case_studies.r: r-script file containing all the sources necessary to reproduce the results presented in the paper.cdqwqc.

click here for file

 acknowledgments
we want to thank gábor tusnády who suggested the usage of method for our work. the function cur in package uses the function ginv from package mass <cit> .

this work was supported by the national office for research and technology, hungary  and the project tÁmop  <dig> . <dig> b-11/2/kmr-2011- <dig> 
