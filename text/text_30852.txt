BACKGROUND
we have witnessed a revolution in sequencing technologies in last decade. the biological sciences are undergoing an explosion in the amount of genome sequences. there are increasing interests about using computational methods to identify the biological functions of the protein sequences  <cit> , as experimentally determining protein functions is time-consuming and it cannot catch up with the fast growth of newly found proteins  <cit> .

various studies have applied machine learning methods to protein data from biological experiments to predict the functions for unknown proteins. . classical computational approaches for protein function prediction represent each protein as a set of features, and employ machine learning algorithms to automatically predict the protein function based on these features. the most well-established methods  <cit>  are the blast  <cit>  approach based on sequence, prosite  <cit>  based on sequence motifs, and pfam  <cit>  based on profile methods.

in recent years, the development of experimental methods for genome scale analysis of molecular interaction networks offers new ways to infer protein function in the context of protein-protein interaction  network, wherein proteins and detected ppis are represented by nodes and edges, respectively. the basic idea is that the direct interaction partners of a protein are likely to share similar biological functions  <cit> . assignment of protein functions using ppi data has also been extensively studied, such as neighborhood counting based method  <cit> , graph theoretic methods  <cit> , hierarchical clustering-based methods  <cit>  and graph clustering methods  <cit> . although many efforts have been made in protein function prediction, most of them were based on either sequence similarity that ignores the protein interactions, or ppi information without using attributes derived from the content of protein sequence. the former method often fails to work if a query protein has no or very little sequence similarity to any proteins of known labels, the latter method has similar problem if there are insufficient relevant ppi information.

to explicitly use the information of the content of the data and the links information of the ppi network to improve the prediction performance, collective classification  is proposed. it received considerable attentions in the last decade. various cc algorithms has been proposed in the literature  <cit> , such as the iterative classification algorithm   <cit> , gibbs sampling   <cit> , and variants of the weighted-vote relational neighbor algorithm   <cit> . here, we focus on ica-type approaches, which consist of a local classifier, such as knn, to infer the class labels of related instances. the key idea is to construct new relational feature vectors by summarizing the label information from neighborhood nodes, and then use the relational features together with the attribute features derived from the content of data to learn local classifiers for prediction.

this paper describes an effective markov chain based cc algorithm  to tackle the label deficiency problem in cc for protein function prediction from ppi networks. our idea is to model the classifier mar via the markov chain with restart. the markov chain model computes the ranks of labels to indicate the importance of a set of labels to an instance by propagating the label information in a graph constructed from labeled and unlabeled data. the icam algorithm further refines the markov chain model using an ica framework to generate the possible labels for a given instance. by these techniques, mar can be learned more effectively. experiments on the realworld yeast ppi datasets have demonstrated that our proposed icam method improves the classification performance when compared with the ica-type cc methods. the main contributions of this paper are as follows.

• we study the label deficiency problem of collective classification  and show that the protein function prediction problem from ppi networks can be formulated as a cc task.

• we extend the ica-type cc algorithm and propose the icam algorithm to leverage the unlabeled portion of the data to improve the classification performance of cc via the markov chain with restart.

• we demonstrate the effectiveness of our proposed icam algorithm using the yeast benchmark datasets. we find that icam leads to significant accuracy gains compared to other ica-type methods when there are limited numbers of labeled data available.

methods
preliminaries
assume that the ppi network data are represented as a graph g, where v is a set of nodes, e is a set of edges representing the interactions between the instances. each instance/node vi ∈ v is described by an attribute vector xi ∈ xa. each yi ∈ y is a set of labels for vi, and c is the number of possible labels. assume that we have a set of labeled nodes vk ⊂ v with known labels yk = {yi|vi ∈ vk}, and the task is to predict the labels yu for unlabeled nodes vu = v - vk. in this paper, we are primarily interested in generating a ranking of possible labels for a given protein such that its correct functions receive higher ranking than the less unlikely one.

the icam algorithm
inspired by the ica approach, we introduce the icam algorithm for collective classification. the algorithm is summarized in algorithm  <dig>  similar to the ica framework, the icam algorithm has two parts as follows: bootstrap and iterative inference. the bootstrap part learns an attribute-only classifier ma from the known nodes, and uses ma to predict labels for the unknown nodes vu . in the iterative inference part, the relational features xr are updated based on the estimated class labels of data . specifically, xr of the -th iteration is based on the known and predicted labels from the i-th iteration. next, the algorithm trains a collective classifier mar using both attribute features xa and relational features xr to compute the labels for unlabeled data. the iterative process stops when the predictions of mar are stabilized or a fixed number of iteration is reached.

an important component of the ica algorithm is to build the relational features that summarizes the relational information, and to construct new feature vectors to train the classifier mar. for instance, neville et al.  <cit>  summarize the labels of neighboring nodes as relational features as illustrated in figure  <dig>  where node "b" has two positive neighboring nodes and two negative neighboring nodes. here, the relational features is "¡ <dig>  2¿", and then "¡ <dig>  2¿" is appended onto the original feature vector, <xi, <dig>  xi, <dig>  ⋯>, as new features, " <xi, <dig>  xi, <dig>  ⋯,  <dig>   <dig> >". ica-type cc methods usually increase accuracy for network data using a large amount of labeled data to train mar. in this scenario, the supervision knowledge can be effectively propagated in the network and improve the learning accuracy  <cit> . however, the labeled data are time-consuming to obtain and the number of labeled data is very limited. most of the nodes may not link to the labeled nodes, as illustrated in figure  <dig>  as a result, the prediction accuracy of the collective classifier mar will be decreased greatly.

algorithm  <dig> icam 

input:

v = nodes, e = edges, xa = attribute feature vectors, yk = labels of known nodes, n = # of iterations,

procedure:

1: ma = learnclassifer;

2: yu=predict;

3: for t =  <dig> to n do

4:    xr = aggregation;

5:    re-train mar = learnclassifer;

6:    yu=predict;

7: end for

8: return yu

in our icam algorithm, we assume that the attribute features xi and the relational features ri are conditionally independent given the class label yi  <cit> . we then use two distinct classifiers to make two separate predictions for attribute features xa and relational features xr. the prediction is given as follows:

 p=pppp=pppppppp=γppp 

where γ is a constant independent of yi. the attribute classifier to estimate p is referred to as ma, and the relational classifier to estimate p is referred to as mr.

there are two main advantages of this prediction method. first, this method allows us to train classifiers ma and mr for attribute features xa and relational features xr in parallel. second, in the collective inference process, the classifier mr can be re-trained in each iteration based on the re-constructed relational features xr to improve the prediction accuracy of the collective classifier mar.

various traditional supervised learning methods can be used to train ma and mr where the classifier, such as knn, naive bayes and logistic regression  <cit> , is learned from a separate training data with a large amount of labeled data. however, when dealing with label deficiency problem in ppi networks, we propose to use transductive learning method for acquiring additional information from unlabeled data to improve the classification performance. specifically, we set up markov chain based learning models to estimate p and p.

markov chain based learning
the markov chain based learning model is based on the idea of random walk with restart. we note that there are many learning tasks using random walk techniques such as protein network cluster discovery  <cit> , community discovery  <cit> , multi-instance multi-label learning  <cit> , and transfer learning  <cit> . the idea of random walk with restart is to consider a random walker that starts from labeled nodes, and iteratively transmits to its neighborhood with probabilities proportional to their edge weights. at each step, it has a probability α to return back to labeled node. the steady-state probability that the walker will finally stay at node j is the relevance score of node j with respect to the labeled nodes  <cit> :

 u=pu+αq 

where u =  is the steady-state probability of relevance scores of different nodes, p is the affinity matrix associated with the instances in markov chain transition probability graph, and q is the label distribution vector containing the elements of labeled instances being  <dig> and  <dig> for others. here, the steady-state probability  captures the global structure of the graph and relationship between the nodes. the advantage of this random walk procedure is that it converges to a unique solution for any initial u. the process converges fast, needing just a few iterations. the random walk and related methods have been shown to have good performance on the learning tasks mentioned above. in the following, we introduce the learning of ma via the markov chain with restart using all the instances . the process of learning mr is similar.

given the constructed attribute feature vector xi ∈ xa for a node vi ∈ v, pairwise affinity a∈ℝm×m between the nodes based on relational information are computed using the gaussian kernel function as follows

  ai,j=exp-||xi-xj||22σ <dig>  

where ||xi - xj|| is the euclidean distance between the i-th feature vector and the j-th feature vector in xa. the parameter σ is a positive number to control the linkage in the manifold  <cit> . the m-by-m matrix a, with its -th entry given by ai,j, is always nonnegative. similar to , using the gaussian kernel to ri ∈ xr leads to the affinity matrix r for relational features. we then set up markov chain models for classifiers ma and mr based on a and r, respectively.

for the classifier ma, we construct an m-by-m markov transition probability matrix p by normalizing the entries of a with respect to each column, i.e., each column sum of p is equal to one, ∑ii,j= <dig>  for such p, we model the probabilities of visiting the other instances from the current instance in a markov chain transition probability graph. we construct a transition probability graph, all the labeled and unlabeled instances are linked together. intuitively, a random walker starts from nodes with known label to propagate labels among labeled instances to the other unlabeled instances. the walker iteratively visits its neighborhood of nodes with the transition probability graph based on a.

next we use the idea in topic-sensitive pagerank  <cit>  as a markov chain with restart  <cit>  to solve the learning problem. the random walker has a probability of α to return to labeled instances at each step. it can be interpreted that during each iteration each instance receives the label information from its neighbors via the random walk, and also retains its initial label information. the parameter αspecifies the relative amount of the information from its neighbors and its initial label information. using this approach, we compute the steady state probabilities that the random walker finally stay at different instances. these steady state probabilities give ranking of labels to indicate the importance of a set of labels to an unlabeled instance.

more formally, we adopt the following equation:

  u=pu+αq, 

to compute the steady probabilities u =   according to p and q =   which is the assigned probability distribution vector of the class labels that are constructed from the labeled data. the restart parameter  <dig> < α < <dig> controls the importance of the assigned probability distributions in the labeled data to the resulting label ranking scores of instances. given the training data, one simple way to construct qd is using a uniform distribution on the instances with the label class d. the summations of the entries of qd is equal to  <dig>  more precisely,

  qdi=1/ld,ifd∈yi <dig> otherwise. 

where ld is the number of instances with the label class d in the training data.

the steady probability distribution vector u is solved by the iteration method with an initial matrix u <dig> where each column is a probability distribution vector. the overall algorithm is summarized in algorithm  <dig> 

algorithm  <dig> markov chain based classifer

input: p, q and u <dig>  α, and the tolerance ϵ

output:the steady probability distribution matrix u

procedure:

1: set t = 1;

2: compute ut = put- <dig> + αq;

3: if ||ut - ut-1|| < ϵ, then stop, set u = ut; otherwise set t = t +  <dig> and goto step  <dig> 

experimental 
RESULTS
in this section, we compare the performance of icam algorithm with other ica-type collective classification algorithms: ica, gibbs and icml. we show that the proposed algorithm outperforms these algorithms given limited number of labeled training data.

kdd cup  <dig> data and baselines
the first experiment is conducted for yeast gene function prediction from kdd cup  <dig>  <cit> . the dataset includes  <dig>  genes and  <dig>  interactions among the pair of genes encoding the proteins physically interact with one another. these interaction relationships are symmetric. the protein functions are autocorrelated in this dataset and a subset of the data have been withheld for testing. the task is to predict the functions of the proteins encoded by the genes. there are  <dig> functions and a protein can have one  function.

we compare our proposed method with the following three baseline learners:

 <dig>  ica. the iterative classification algorithm  algorithm proposed by neville et al.  <cit>  is one of the simplest and most popular cc methods that is frequently used as baseline for cc evaluation in previous studies. for multi-label problem, we transform it into multiple single-label prediction problems using one-against-all strategy and employ ica to make prediction for each single-label problem.

 <dig>  gibbs. this baseline is another ica-type cc algorithm using the ica iterative classification framework. in each iteration, gibbs re-samples the label of each node based on the estimated label distribution  <cit> . we also use one against-all strategy to convert the multi-label problem into multiple single-label problems for the gibbs algorithm.

 <dig>  icml. this baseline is a multi-label cc algorithm proposed by kongetal.  <cit> . icml extends the ica algorithm to multi-label problems by considering dependencies among the label set in the iteration process.

in the experiments, we use knn as node classifier for ica, gibbs and icml. the parameter k was automatically selected in the range of  <dig> to  <dig> at an increment of  <dig> using 3-fold cross validation on the training set. for the proposed icam method, we learn the classifiers ma and mr using markov chain based models to perform separate predictions. we set the value of α in the markov chain model to be  <dig>  as suggested in  <cit> .

evaluation criteria
we evaluate the performance of our proposed method with four multi-label evaluation measures: average precision, coverage, ranking loss, and one-error. they are commonly used for multi-label learning algorithm evaluation.

given a multi-label dataset d = {| <dig> ≤ i ≤ m}, where xi∈x is an instance and yi⊆y is the true labels of xi, and yi =  ∈ { <dig>  1}c. here xi belongs to the j-th label when yij =  <dig>  otherwise yij =  <dig>  and c is the number of possible labels. the evaluation measures are defined using the following two outputs provided by the learning algorithms: s returns a real-value that indicates the confidence for the class label l to be a proper label of xi; ranks  returns the ranks of class label l derived from s.

coverage  <cit>  evaluates how far we need, on the average, to go down on the list of labels in order to cover all the true labels of an instance:

 coverage = 1m ∑i=1mmaxl∈yiranks- <dig>  

ranking loss  <cit>  evaluates the average fraction of label pairs that are reverse ordered for the instance:

 rloss = 1m ∑i=1m1|yi||Ȳi|.|ri|, 

where ri={|h≤h,∈yi×Ȳi|}.

one-error  <cit>  evaluates how many times the top-ranked label is not in the set of true labels of the instance. define a classifier h that assigns a single label to an instance xi by h=argmaxl∈yhxi,l, then the one-error is

 one-error=1m ∑i=1mh∉yi. 

average precision  <cit>  evaluates the average fraction of labels ranked above a particular label l ∈ y in y:

 avgprec=1m ∑i=1m1|yi|∑y∈yi|pi|ranks, 

where pi={l′∈yi|ranks≤ranks}.

the smaller the value of coverage ranking loss and one-error, the better the performance. as for average precision, the bigger the value the better the performance, we report the results of 1-average precision. thus, for all evaluation metrics, the smaller the value the better the performance.

results on kdd cup  <dig> data
in this experiment, we test the performance of our proposed icam algorithm on the kdd cup  <dig> dataset. we randomly select 50% of data as training set, and use the remaining 50% of data as test set. the experiment is conducted  <dig> times by randomly selected training/test split , and we report the results of mean as well as standard deviation of each compared algorithm. the mean as well as standard deviation of each compared method over the same  <dig> trails are reported.

we also test the performance of different comparable algorithms with different number of labeled instances ranging from  <dig> to  <dig> with an increment of  <dig>  for example, we randomly pick up  <dig> instances as training data. the remaining data is used for testing. the experiment is conducted  <dig> times by randomly selecting training/test split. we report the results of mean as well as standard deviation of each compared algorithms. figure  <dig> shows the performance of icam and other learning methods with respect to different number of labeled instances.

we can see from the figure that icam  has the best performance in general. icam outperforms other algorithms using different number of training data, especially when the size of training data is small. specifically, icam achieves coverage improvement of  <dig>  over the second best method gibbs  and achieves  <dig>  improvement on ranking loss  when the number of training instance is  <dig>  as the size of training data increases, icam consistently achieves better performance than other learning algorithms across all evaluation criteria.

we find that icam outperforms the other ica-type methods substantially in terms of coverage. on the other hand, icam only slightly outperforms other methods in terms of one-error. we note that one-error and coverage are two different quantitative measures. one-error evaluates how many times the top-ranked label is not in the set of possible labels. thus, if the goal of a prediction system is to assign a single function to a protein , the one-error is identical to test error. whilst coverage measures how far we need, on the average, to go down on the list of the labels in order to cover all the possible labels assigned to a protein. coverage is loosely related to precision at the level of perfect recall  <cit> . the experimental results indicate that the top-rank label predictions from other ica-type methods are as accurate as those from icam, but the predictions from icam are more complete than other ica-type methods. a reasonable explanation for this finding is that the ica-type methods focused on the single-label setting. in this case, the multi-label problem is first transformed into multiple single-label prediction problems, and then the ica-type methods use independent classifiers induced from labeled training data for each problem. nevertheless, ica-type approaches ignore the effect of unlabeled data and the interdependence of the protein functions. on the other hand, our proposed icam approach is based on markov chain based transductive learning method that uses both label and unlabeled data for label propagation. the markov chain based method takes the correlation of the classes into account to effectively compute ranking of labels to an instance. therefore, icam provides an opportunity to leverage the individual ica-type classifiers to achieve higher coverage of predictions.

results on kdd cup  <dig> data
to validate the effectiveness of the proposed method when there are only a limited number of positive labeled training data, we conduct additional experiments on a relatively large scale yeast dataset from kdd cup  <dig>  it consists of  <dig> instances  from experiments with a set of cerevisiae  strains. each instance is described by various types of information that characterize the gene associated with the instance. the data sources for describing the instances include abstracts from the scientific literature , gene localization and functions. we represent each instance by a feature vector with  <dig> dimensions. the pairs of genes whose encoded proteins physically interact with one another. such protein-protein interaction network consists of  <dig> links.

each instance is labeled with one of three class labels "nc", "control" and "change". the "change" label indicates instances in which the activity of the hidden system was significantly changed, but the activity of the control system was not significantly changed. the goal of the kdd cup  <dig> task is to learn a model that can accurately predict the genes that affect the hidden system but not the control system. in this case, the positive class consists of those genes with "change" labels and the negative class consists of those genes with either "nc" or the "control" label. this partition is highly imbalanced. the rate of positive instances is only  <dig> %. therefore, we base our evaluation analysis on receiver operating characteristic  curves, which reflect the true positive rate of a classifier as a function of its false positive rate. roc curves are commonly used for evaluating highly skewed binary classification problems. recent study has shown that roc curves have a deep connection to the precision-recall  curves  <cit> .

to evaluate the performance of our icam algorithm, we compare it with the linear kernel svm method that implemented by libsvm  <cit> . figure  <dig> shows the results of roc curves on the kdd cup  <dig> task for icam and svm. the x-axis and y-axis of the figure refer to the false positive rate and true positive rate respectively. we see from the figure that our icam , outperforms the svm method  in general. icam achieves improvement of  <dig> %  on area under the roc curves. the experimental results imply that the proposed icam method is able to deliver better performance in the situation of small positive labeled data size.

experiments on collaboration networks
in this section, we compare the performance of the proposed icam algorithm with other collective classification algorithms on  <dig> multi-label collaboration networks datasets to validate the effectiveness of the proposed method more thoroughly. these collaboration networks datasets are collected from the dblp computer science bibliography website, and used in prior work to study the multi-label collective classification problems  <cit> . their characteristics are listed in table  <dig>  specifically, we extract dblp coauthorship networks that contain authors who have published papers during the years 2000- <dig> as the nodes of the networks, and link any two authors who have collaborated with each other. at each node, we extract a bag-of-words representation of all the paper titles published by the author, and used it as the attributes of the node. each author has one  research topic of interests from  <dig> research areas. the representative conferences from each area are selected as class labels. if an author has published papers in any of these conferences, we assume the author is interested in the corresponding research class. the task is to classify each author with a set of multiple research classes of interest. the conferences corresponding to the class labels of two datasets  are given as follows.

the classes of dblp-a are as follows:

 <dig> database: icde, vldb, sigmod, pods, edbt

 <dig> data mining: kdd, icdm, sdm, pkdd, pakdd

 <dig> artificial intelligence: ijcai, aaai

 <dig> information retrieval: sigir, ecir

 <dig> computer vision: cvpr

 <dig> machine learning: icml, ecml

the classes of dblp-b are as follows:

 <dig> algorithms & theory: stoc, focs, soda, colt

 <dig> natural language processing: acl, anlp, coling

 <dig> bioinformatics: ismb, recomb

 <dig> networking: sigcomm, mobicom, infocom

 <dig> operating systems: sosp, osdi

 <dig> parallel computing: pod, ics

we test icam and other ica-type algorithms with different number of labeled instances from  <dig> to  <dig> with an increment of  <dig>  the average results as well as standard deviation of a 10-time data split are given in figure  <dig>  the experimental results are in concordant with our previous study. we observe that icam consistently outperforms the other ica-type methods on these two datasets, especially when there are only limited number of labeled instances, i.e. larger improvement is obtained with less labeled data.

CONCLUSIONS
in this paper, we studied the label deficiency problem in collective classification . we showed the protein function prediction problem from ppi networks can be formulated as a problem, and proposed an effective and novel markov chain based cc learning algorithm, namely icam. it focuses on how to use labeled and unlabeled data to enhance the classification performance of ppi network data. experimental results on two real-world yeast ppi network datasets and two collaboration network datasets showed that our proposed icam method is effective in learning cc tasks in the paucity of labeled data. in future, we will consider other semi-supervised learning techniques for collective classification in ppi network data and we will also research on other complex biological networks, such as heterogeneous network classification.

competing interests
the authors declare that they have no competing interests.

authors' contributions
q. wu, m.k. ng. and y. ye participated in designing the algorithm and drafted the manuscript. q. wu and r. shi performed the implementations. s.s. ho revised and finalized the paper. all authors read and approved the final manuscript.

