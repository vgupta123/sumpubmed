BACKGROUND
major histocompatibility complex  molecules belong to a large family of proteins used by the immune system to recognize foreign antigens such as pathogens. in humans, mhcs are called human leukocyte antigens . attached to the cell-surface, mhc molecules loaded with peptide fragments of intra- or extra-cellular origin are presented to t-cells for recognition, after which cell-killing or downstream signaling events are triggered
 <cit> . hence, binding of peptides to mhc molecules is a requirement for t-cell recognition
 <cit> . accordingly, accurate peptide:mhc binding predictions are useful for the development of reagents, therapeutics and diagnostics for infectious and autoimmune diseases, allergy and cancer.

because of the importance of peptide:mhc binding in determining t-cell epitopes, much effort has been expended to collect experimentally measured binding affinity data and make them available to the scientific community
. accompanying the growth of the binding data, many mhc class i peptide binding predictors have been reported to date. to compare their predictive performances, a number of large-scale benchmarking studies have been carried out. in the case of mhc-i predictors, high predictive performances with average areas under receiver operating characteristic curves  of ~ <dig>  from cross-validations have been reported
 <cit> , suggesting that the predictive methods have matured.

despite much progress in the development of predictive methods for binding of peptides to mhc class i molecules, a number of important questions remain. first, given that cross-validated predictive performances are estimates of ‘true’ performances in real world applications, how accurate are these estimates? second, what is the role of sequence similarity in influencing the accuracies of these estimates? that is, does presence of similar peptides between testing and training sets lead to inflated predictive performances? third, are there additional factors that lead to deviations between blind and cross-validated performances?

to address these questions, we tested existing predictive methods against a large set of blind data sets and measured deviations of cross-validated performances with respect to those on a blind dataset. we introduced a cross-validation strategy where sequence similarity between testing and training data sets is dramatically reduced. furthermore, we examined various characteristics of cross-validation and blind data sets to better understand how they influence estimates of blind predictive performances.

RESULTS
assembling a comprehensive set of mhc class i binding data for cross-validations
to better understand the factors that contribute to accuracy of predictive performances estimated by cross-validation, we prepared the three binding data sets shown in table 
 <dig>  the binding data came from the immune epitope database 
 <cit> , as well as some data from submissions currently in process from the buus and sette labs. bd <dig> and bd <dig> are data sets prepared in years  <dig> and  <dig>  respectively, for re-training of the predictive tools hosted on the immune epitope database analysis resource 
 <cit> . cross-validated predictive performances were generated against bd <dig>  compared to bd <dig>  bd <dig> contained about 30% more data points. the bd <dig> data set covered  <dig> species ,  <dig> mhc-i alleles,  <dig>  data sets with  <dig> affinity measurements on average, and a total of  <dig>  measurements. bd <dig> is the largest binding affinity measurement data set assembled to date for mhc class i peptide binding.table  <dig> 
binding data statistics


+
++

alleles

data sets

data set size

total data points

+all cross-validations were carried out using bd <dig> 


++
blind was generated by subtracting bd <dig> from bd <dig>  against blind, all blind predictions were made using the predictors trained on bd <dig> 

each  combination is associated with a data set.



to prepare independent data sets against which to estimate ‘true’ predictive accuracy, we compiled a blind data set . against this data set, all blind predictions used in this study were generated with the predictors trained on bd <dig>  the blind set was prepared by subtracting bd <dig> from bd <dig> and removing ‘similar’ peptides with respect to bd <dig>  two peptides were considered ‘similar’ if they shared at least 80% sequence identity and were of same lengths. in the table, only those data sets for which  combinations were shared between bd <dig> and blind and that had at least  <dig> data points are reported. the blind data set contained binding data associated with  <dig> alleles,  <dig>  data sets, and  <dig>  measurements. all of the benchmark data sets mentioned are available at the iedb-ar benchmark datasets website
 <cit> .

cross-validations tend to over-estimate blind predictive performances
in a typical run of cross-validation, a data set is randomly partitioned into n subsets, and one subset is held out for making predictions using a predictor trained on the remaining n -  <dig> subsets. this type of cross-validation has been used widely for benchmarking peptide:mhc binding predictors
 <cit> . to distinguish it from other types of cross-validations that will be introduced later, we will call it cv_rnd from here on, indicating a random partitioning for cross-validation.

in terms of absolute predictive performances against the cv_rnd cross-validation data sets, netmhcpan performed better than either smmpmbec or netmhc, while smmpmbec and netmhc performed similarly . against the blind data sets, however, netmhcpan and netmhc performed similarly while smmpmbec performed worse . this is in agreement with previous performance benchmarks
 <cit> .

in figure 
 <dig>  standard errors of means of differences in predictive performances estimated using cv_rnd and performances measured on the blind data sets are shown for the three predictive methods. the means of all three distributions of prediction differences are above zero, indicating that the cv_rnd cross-validation strategy over-estimates blind predictive performances for all three methods. for both smmpmbec and netmhcpan, but not netmhc, the over-estimation is statistically significant , as shown in column ‘p-values: one sample’ in  for the cv_rnd cross-validation strategy.figure  <dig> 
cross-validations tend to over-estimate predictive performances against blind data sets. for each predictive method, a mean of its distribution of differences in performances between cv_rnd and blind  and a standard error of the mean are shown. hence, a positive mean indicates over-estimation in performance by cv_rnd. predictive performances are reported as aroc values . scores of  <dig>  and  <dig>  indicate a random pattern and perfect prediction, respectively.



reducing sequence similarity in cross-validation data sets does not mitigate over-estimation of blind predictive performances
the observed over-estimations had a number of possible explanations. one previously provided explanation
 <cit>  was that sequence similarity shared between training and testing data sets during cross-validation inflated predictive performances, because similar peptides are easier to predict than completely novel ones. to test this hypothesis, we utilized two cross-validation strategies that reduce sequence similarity: cv_sr and cv_gs. the cv_sr  strategy reduced sequence similarity by removing peptides so that there were no similar peptides in the data sets using a sequence threshold of 80%. once similar peptides were removed, random partitioning was done as for cv_rnd. in the case of the cv_gs strategy , rather than removing all similar peptides, we only required that there were no similar peptides between the paired training and testing sets in cross-validation partitions. hence, in comparison to cv_sr, cv_gs kept many more peptides, but distributed them differently in the cross-validation partitions. details of these implementations are provided in the methods section.

as shown in additional file
1: table s <dig>  differences in arocs between cross-validated and blind predictive performances for the cross-validation strategies cv_sr and cv_gs show means that are closer to zero than cv_rnd for both of these cross-validation setups. as shown in column ‘p-values: two sample’, this shift to smaller means is statistically significant for smmpmbec and netmhcpan, but not netmhc. however, as shown in column ‘p-values: two-sample, absolute value’ for all three methods, using either cv_sr or cv_gs strategy did not lead to significantly more accurate estimates of blind predictive performances than cv_rnd . scatter plots of deviations shown in figure 
 <dig> confirm that cv_rnd and cv_gs perform similarly. for the remaining sections, the cross-validation strategy cv_gs will be used throughout.figure  <dig> 
comparison of cross-validation strategies in terms of their differences in predictive performances with respect to those of blind. for each strategy, its blinded performances were subtracted from those of cross-validated: cv - blind. hence, positive values indicate over-estimation of blind predictive performances.



data set size, evenness of peptide sequence space coverage, and range of predicted affinities can explain over-estimation of cross-validated predictive performance
to better understand why predictive performances estimated using cross-validation deviated from those of blind, we defined two classes of deviations for each method, as shown in figure 
 <dig>  in the figure, a band around the diagonal differentiates cross-validated predictions with small deviations  from those with large deviations , using an arbitrary threshold defined by the mean of deviations  for smmpmbec. using the same threshold for smmpmbec, netmhc, and netmhcpan,  <dig>   <dig> and  <dig> data points were considered ‘large’ deviations, respectively.figure  <dig> 
two classes of deviations of predictive performances estimated with cross-validation with respect to those of blind. predictive performances are in arocs for the three predictive methods. the width of the band is set at values +/- mean of absolute differences between blind and cv_gs of smmpmbec: mean). data points classified as ‘large deviations’ are highlighted in red.



we also looked at scatter plots of predicted versus measured binding affinities for data sets with large deviations. the scatter plots revealed that large deviations were associated with blind data sets where the ranges of predicted affinities were narrow with respect to those of cross-validated ones, or measured affinities for peptides were concentrated in the region bordering a cutoff value for binders. additional file
1: figure s <dig> is an example of the latter case for h- <dig> db, where most of the peptides had measured affinities between  <dig> and  <dig> nm, while those from the cross-validation data set were more broadly sampled .

motivated by these observations, we characterized cross-validation and blind data sets using a number of features. briefly, the features captured data set sizes, evenness of peptide sequence space coverage, range of predicted/measured affinities, and overlap of these ranges between cross-validation and blind data sets. the features used are listed in table 
 <dig>  details of their calculations are provided in the methods section.table  <dig> 
statistical significances of features of cross-validation and blind data sets in discriminating large deviations from small


pmbec
 <dig> e-07
 <dig> e-04
 <dig> e-02
 <dig> e-05
 <dig> e-03
 <dig> e-02
 <dig> e-04
 <dig> e-03
 <dig> e-02
 <dig> e-05
 <dig> e-04
 <dig> e-03
 <dig> e-03
 <dig> e-02
 <dig> e-03
 <dig> e-02
 <dig> e-02
here, deviation = |cv_gs - blind|, where blind and cv_gs correspond to predictive performances in arocs. significant features  are italicized. see methods for definitions of the features.



table 
 <dig> lists features, and their statistical significances, used for discriminating cross-validated predictive performances with large deviations from those with small deviations, using the threshold introduced above. as shown, the features that showed the most significant discrimination were largely the same for all three methods. the size of the cross-validation and blind data sets  were among the strongest, and they inversely correlated with deviations. the next strongest features were evenness of sequence space coverage of blind and cross-validation data sets , and they also inversely correlated with deviations. the next strongest feature was the ‘spread’ of predicted affinities for the blind data sets, which also inversely correlated with deviations. for netmhcpan, entss_bl was the strongest feature in discriminating the two classes of deviations, instead of data size. this difference was probably due to the fact that this pan method used data from different mhc alleles at the same time and therefore was less impacted by a low number of data points in the specific mhc allele for which predictions were made. scatter plots of deviations versus log_size_cv and entss_bl are shown in .

logistic regression models of observed deviations for cross-validated predictive performances improve accuracy of predicting biased benchmark data sets
the results shown above suggested that certain features of the training and blind datasets could be used to identify when it is likely to observe a large difference between cross-validated and blind prediction performances. we therefore set out,to build models to quantify this likelihood using logistic regression
 <cit> , given a specific data set. logistic regression was chosen because we wanted to model probabilities of the two classes of deviations defined earlier  as a function of the features considered here. for each  combination, a logistic regression model returned a probability of large deviation based on the features for the data set, and its reference class label was based on the deviation threshold used in figure 
 <dig>  predictive performances of the logistic regression models in arocs were measured using leave one out cross-validations , where ‘testing’ set has a size of  <dig> while ‘training’ set the size of the remaining data.

we systematically tested how well combinations of two features of the training or blind datasets could predict the likelihood of having a large deviation between cross-validated and predicted performances . the two features of the training set with the highest predictive power were size of the dataset  and entropy of its measured binding affinities . the top row of figure 
 <dig> shows the predictive power of a model using these features of the training set alone for smmpmbec, netmhc and netmhcpan methods which achieved aroc values of  <dig> ,  <dig>  and  <dig>  respectively. the predictive power of the model for netmhcpan was likely lower as additional training data was used beyond the data available for the particular allele. after repeating the same analysis for features of the blind set, we found that a model using sequence space coverage  and entropy of the predicted binding affinities  had the highest discriminatory power, achieving aroc values of  <dig> ,  <dig>  and  <dig>  for the three methods. this demonstrates that features of the blind set and features of the training set independently impacted the likelihood of having mismatching performance estimates between cross-validation and blind prediction performances.finally, we built another model combining the two models described above. in this combined model, a probability of large deviation was calculated by taking a higher probability returned by the two models. the bottom row of figure 
 <dig> shows results of loocv with aroc values of  <dig> ,  <dig>  and  <dig> , respectively. the combined model showed a much higher average aroc of  <dig>  than the model using features of the training set  or blind set  alone, further illustrating that both training set and blind dataset need to be of adequate size and representative of the problem space in order to give consistent results in benchmarking performance.figure  <dig> 
distributions of observed deviations for two predicted classes of deviations: small vs. large. leave one out cross-validation  results were separated into two groups , based on a probability cutoff of  <dig> . first and second rows used logistic regression models combining features indicated as row labels. third row uses the ‘max’ approach to combine models used in the top two rows. overlaid on top of each distribution, lower, middle, and upper line segments represent 25th, median, and 75th quartiles, respectively.



discussion
to better understand how well the accuracy of peptide:mhc class i binding predictive methods can be estimated for practical applications, we utilized a large blind data set to measure the extent of deviation of cross-validated predicted performance with respect to those generated from blinded approaches. we found that cross-validations tend to over-estimate blinded prediction performance. reducing sequence similarity between training and testing sets during cross-validation had only a marginal role in mitigating the over-estimations. instead, a multitude of factors contributed to deviations . namely, large deviations are due to small training data sets, unevenness of sequence coverage of either training or blind data sets, and narrowly ranged predicted affinities for blind data sets.

results from the logistic regression modeling showed that features of both cross-validation and blind data sets contributed to the deviations. our results suggest that predictors trained with sufficiently large data sets that evenly cover sequence and affinity space are ‘good’, in the sense that it is less likely that they have ‘blind spots’ and will perform great on one test set but poorly on another. based on these results, we incorporated the accuracy assessment given by the training set classifier into our benchmarking results to indicate those datasets for which the cross-validated performance measure may not be an accurate estimate .

our results indicate that, at least for mhc-i predictors, cross-validations do give accurate estimates of ‘true’ predictive performances, if sizes of training data are sufficiently large and peptide sequence space has been sampled evenly. there are additional reasons to prefer cross-validation over blind sets: for one, it takes a long time until a sufficiently large blind dataset can be assembled to perform reliable prediction assessments. moreover, there is a fundamental problem with blind sets being generated at least as they are submitted to the iedb: given the increased use of mhc binding predictions in practical applications, binding data is increasingly generated on such pre-selected peptides that will have a decreased coverage of the sequence and affinity space. as we have shown here, such blind sets are intrinsically harder to predict. thus, the seeming drop in performance from the cross-validated predictions on the iedb  <dig> dataset compared to predictions of the data newly added into the iedb since then is likely a reflection of the more difficult nature of these datasets.

our study could be further improved in several ways. first, by repeating cross-validation runs with different data partitions and averaging the performance over different runs, a more solid estimate of performance could have been gathered. we did not do that in order to be directly comparable by previous benchmarks published from our group. in addition, the aroc as a performance measure is prone to generate less robust results for data sets with few numbers of binders, which is the case for several of the datasets in this study. so some of the variability between cross-validated and blind performance that we observed for datasets with low numbers of binders might be less pronounced if a different performance metric was used. however, the number of binders per dataset was one metric that was evaluated for its ability to identify datasets with highly divergent blind vs. cross-validated prediction performance, and the total number of peptides in each set in general performed better, so this issue does not seem to be a dominant concern. with these considerations, we stuck to the aroc measure to evaluate performances, also to be directly comparable to our previous benchmarks.

others have also looked into estimating reliability of given predictions. recently, confidence intervals of individual predictions were estimated for peptide-mhc binding based on training data
 <cit> . we expect that such confidence interval estimations will be complementary to our findings.

CONCLUSIONS
using the largest ever assembled set of training and blind data sets for peptide:mhc class i binding, we determined the extent to which cross-validated predictive performances deviate from those based on blind validation. removing sequences that shared 80% or greater sequence identity between training and testing sets during cross-validations had a marginal role in influencing the extent of deviations. instead, we found that data set size and the evenness of coverage of the sequence space can explain most of the deviations observed for both the cross-validation and blind data set approaches. our results identify quantitative features that will facilitate more accurate assessment of the performance of peptide-mhc binding predictors.

