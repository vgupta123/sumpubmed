BACKGROUND
the annotation of mutants with their consequences is central task for researchers investigating the role of genetic changes on biological systems and organisms. these annotations facilitate the reuse and reinterpretation of mutations and are necessary for the establishment of a comprehensive understanding of genetic mechanisms, biological processes and the resulting mutant phenotypes. as a result, there are numerous mutation databases, albeit perpetually out of date and often with a latency of many years, which is an instance of the general latency problem with genomic and proteomic databases  <cit> . automated mutation extraction systems based on text mining techniques can identify and deliver mutation annotations for database curators to review, or directly to end users. in this article we outline the publication of a mutation impact extraction system in the form of semantic web services, and their integration with other semantically described bioinformatics services, based on the sadi framework.

in our previous work we developed the mutation impact pipeline  <cit>  - a program, based on a gate  <cit>  pipeline, that makes it possible to extract mutation impacts on protein properties from texts, categorising the directionality of impacts as positive, negative or neutral. moreover, the system maps mentions of proteins and mutations to their respective uniprot identifiers and protein properties described in the gene ontology.

for example, consider these two excerpts from  <cit> : “the haloalkane dehalogenase from the nitrogen-fixing hydrogen bacterium xanthobacter autotrophics gj <dig>  prefers  <dig> -dichloroethane  as substrate and converts it to 2-chloroethanol and chloride” and “dh1a shows only a small decrease in activity when trp- <dig> is replaced with phenylalanine”. our pipeline  identified “haloalkane dehalogenase” as a protein,  mapped it to the uniprot id p <dig> by grounding it to the identified organism “xanthobacter autotrophicus”,  identified “trp- <dig> is replaced with phenylalanine” as the point mutation w125f,  identified “activity” as a protein property  identified “decrease” as the direction of the impact of the mutation on the protein property.

initially, the mutation impact pipeline was deployed as a simple java api and could only be used programmatically. when the pipeline is executed on a document, it computes a sequence of java objects representing mutation specifications. every such object contains information about a series of elementary mutations that are studied together, the corresponding wildtype and mutant proteins, and the discovered impacts of the mutations. the java object representing an impact contains the direction of the impact, e.g., positive, negative or neutral, and the type of the protein property being affected as a gene ontology term id, e.g., “go_00188786”.

although the practical use of the system and its results in this form is maximally flexible and may be preferred by many programmers, having some programming-free modes of use, e.g., based on semantic web standards, could extend the usability. so in  <cit>  we explored the possibility of using semantic technologies for exporting the text mining pipeline outputs according to a domain specific knowledge representation. currently, our system, like mstrap  <cit> , delivers its results in the form of an owl abox, i.e., as a collection of logical statements characterising the extracted mutations, proteins and impacts. the classes and property predicates in these statements are defined in our mutation impact ontology  <cit>  in owl, based on the earlier mutation ontology from  <cit> . the ontology is briefly described in the methods section.

representing text mining results as class and property assertions with respect to the mutation impact ontology already adds a great deal of flexibility – the results can be used with any toolsets that work with owl. the most straightforward way of using semantically described data is by querying it directly, so we established a semantic database, in the form of a sesame  <cit>  rdf triplestore, that stores the results of mining different documents. for our experiments, the database is populated with mutation information extracted from  <dig> journal articles, with  <dig> extracted mentions of point mutations and  <dig> extracted mentions of mutation impacts on protein properties of  <dig> distinct types. our users can query the populated database via a sparql  <cit>  end-point  <cit> . since we keep the links from the extracted entities and associations to the corresponding publications, the database can also be considered a form of semantic index for texts.

as we would like to facilitate a multitude of data reuse cases, the provision of a sparql endpoint as the sole data access form is not sufficient. consequently, we are looking for additional ways to provide access to the data. our primary requirement is that the framework should support integration with other software and data for proteins, mutations, impacts and related biological entities, such as pathways, and drugs. this criterion is important because isolated mutation impact mining results have limited reusability outside the domain of protein engineering.

in this article we review the sadi framework  <cit>  as a candidate platform for providing access to our semantically exposed mutation impact data. the choice is based on the powerful integrative features displayed by sadi services and client software, discussed in the next section. this article describes an exploratory case study using five biologically meaningful queries that require  some data from our text mining pipeline and the mutation impact db, as well as  some biological knowledge from external sources. furthermore, we test the queries using the share client  <cit>  which is designed to automatically discover and combine the required sadi services.

the work presented here is a part of a bigger effort: by doing extensive coherent case studies with sadi in several biomedical domains we are  developing a transferable methodology in the form of best practices and recipes covering typical problems, so that future sadi adopters can copy existing solutions and adapt them to their needs, and  trying to learn the extent of the capabilities and the soft spots of the sadi framework in the hope that this will help the future development of sadi and related semantic web services techniques. as a valuable byproduct of the case study presented here, we created a prototype semantic infrastructure that provides the flexibility required by multiple uses of our mutation mining software and the mutation impact db.

methods
what is sadi?
the sadi framework  <cit>  is a set of conventions for creating semantic web services that can be automatically discovered and orchestrated. a sadi-compliant service consumes a whole rdf document as input and produces an rdf document as output. this convention alone eliminates the problem of syntactic interoperability because all sadi services “speak” the same language. this is also convenient for client programs that can leverage existing apis for rdf to represent the data on which sadi services operate.

an input rdf document has some uri node designated as the central input node, and the whole input graph is considered a description of the central node. exactly the same uri is always present in the output graph as the central output node. the sole function of a sadi service is to annotate this node with new properties and assert these properties in the output rdf document, in contrast with more conventional web services that usually compute output without an explicit connection to the input.

the most important feature of sadi is that the predicates for these property assertions are fixed for each service. a declaration of these predicates, available online, constitutes a semantic description of the service. for example, if a service is declared with the predicate myontology:istargetofdrug described in an ontology as a predicate linking proteins to drugs, the user knows that he can use the service to search for drugs targeting a given protein.

the declaration of the service predicates is done by specifying an owl class for the output nodes. if this output class entails an existential restriction for some predicate r, i.e., it is postulated that every instance of the output class is linked with r to some entity, it means that the predicate is declared to be produced by the service and the corresponding output data may be available from the service. registries of sadi services can use such predicates to index the services providing them, thus enabling service discovery based on required functionality.

another part of a service declaration is the input  class that imposes restrictions on the kind of input uris the service can process. in particular, if this class subsumes an intersection of property restrictions, a well-behaved service will look for the corresponding properties attached to an input node, and use the values as parts of the input.

as an example, consider the sadi service  <cit>  computing the body mass index of a person, which is defined as the person’s weight divided by the square of the persons height. its inputclass is defined as the intersection of mged:has_height some mged:measurement and mged:has_mass some mged:measurement, in manchester syntax  <cit>  , so the service expects the property predicates mged:hasheight and mged:hasjmass attached to an input node. the service’s outputclass is a subclass of bmi:bmi some xs:int, so the service provides the predicate bmi:bmi . given the following rdf  as input  

the service generates this rdf as output:  

the declaration of the input and output classes of a sadi service constitutes a semantic description of the service. importantly, such semantic descriptions allow completely automatic discovery and composition of sadi services . in our settings, using sadi services to provide access to the mutation pipeline and db will allow automatic integration with hundreds of external databases and programs dealing with mutations, proteins and related biomedical entities, e.g., pathways and drugs, so long as there are sadi services for these resourses. these are desirable features of sadi motivating us to deploy our mutation impact software with this framework.

finally, let us mention some important technicalities. sadi services are defined on top of the http protocol. a sadi service is requiredto implement http get and post. a valid response to an http get is a description of the service in rdf. it specifies the input and output classes and provides some additional information about the service, such as a brief textual description. the class uris must resolve to the corresponding owl ontology files. service invocation is done with post: the client sends the input rdf document as the content of a post message, and the service returns the output rdf graph in the response. it is convenient to implement such services using standard java servlets which are supported by a number of robust server implementations, e.g., apache tomcat. for greater convenience, the sadi framework provides a java api that specialises javax.servlet.servlet so that the sadi service programmer only needs to deal with rdf in the input and the output. a similar perl library also exists.

share: a sparql engine for sadi services
share  <cit>  is an experimental client featuring automatic discovery and orchestration of sadi services. from the user point of view, share is a sparql engine that computes queries by picking and calling suitable sadi services from some registry. in a typical scenario, the user first looks up predicates he needs for his query, in the list of predicates declared as provided by sadi services in a registry, and also related classes and property predicates in the referenced ontologies. then he uses the available concepts to form a regular sparql query, and sends it to a share endpoint. importantly, the share engine decides itself which services have to be invoked and in what order, to execute the query. note that this qualifies for automatic discovery, composition and invocation. the user deals only with an almost declarative query, i.e., he only needs to understand the semantics of the uris being used in the query, although knowing the services providing the predicates can be beneficial. this situation suits our purposes well, so, for our experiments with sadi services for mutation impact data we are using the web interface for share  <cit> .

to have a controlled environment for our experiments, we installed share  on our own server – a quadcore  <dig>  mhz pc with large cache and ram, running ubuntu linux, together with a local installation of a sadi registry that only contains services relevant to this case study. relying on our experience, we recommend this way of doing large case studies because having a local share installation allows to debug queries by analysing share logs, and also makes the experiments reproducible regardless of the changes in the public registry or the share code. note that although our services are accessible from both the central share installation and our local one, the results and performance of queries on the two installations may differ significantly because the registry used by the central share installation contains a much bigger number of different services. the share client is still in its infancy and makes some redundant service calls in presence of many registered services. although we provide some performance figures, such as the numbers of found answers and execution times for some of our queries, at this stage the query performance is not a concern for us since we are only investigating the general applicability of the sadi framework to our use cases.

since share is just a sparql engine, its effective use is highly dependent on the ability of users to write meaningful queries. to write queries that can be executed, users need to know what classes and property predicates are available, i. e., what predicates are provided by the registered services and what classes and predicates are axiomatically related to them in the corresponding ontologies. currently, the main way of listing predicates provided by the services in a registry is to query the sparql endpoint associated with the registry. for example, the central public sadi registry  <cit>  has a sparql endpoint http://sadiframework.org/registry/sparql/, and querying this endpoint with  

will produce a list of services with the predicates they provide . note that the query uses some prefixes defined in table  <dig>  currently, there is no support for retrieving entities related to these predicates via the corresponding ontologies, e. g., inverse predicates, so this kind of search has to be done manually. in many cases, although not always, the predicate uris are resolved to files with the ontologies defining them, and related entities can be found by examining these ontologies.

another share-related limitation stems from the fact that the current implementation does not guarantee completeness – some answers that can be computed in principle, won’t be found by the system. this is, however, not an inherent problem for sadi as there likely to exist query client architectures with completeness guarantee, although without a termination guarantee, since complete sets of answers may be infinite.

mutation impact ontology
since the sadi services based on our text mining software are defined in terms of our mutation impact ontology, we would like to give a brief overview of the ontology here. figure  <dig> shows the top level concepts of the ontology with some relations between them.

the central concept in our ontology is mutation specification. intuitively, an instance of this class is a piece of information or a statement saying that some mutation applied to a specified protein has a specified impact on a specified protein property. there are, correspondingly, classes representing mutations , proteins, protein properties and impacts.

the main predicates relating these classes are as follows. the predicate specifiesmutations links to the mutation series that a mutation specification describes. the membership of elementary mutations in mutation series is expressed with the predicate containselementarymutation. the wildtype protein is specified with groundmutationsto and the impact is specified with specifiesimpact. an instance of impact is characterised with its direction, e.g., positive, negative or neutral, via hasdirection, and with an instance of the affected protein property. note that protein properties are also modelled as individuals. they can be instances of different subclasses of proteinproperty – currently we use the gene ontology classes for molecular functions. protein properties are grounded to proteins: apart from the protein property class, a specific protein is assigned to a property instance with the predicate hasproperty. since our ontology is mainly aimed at representing text mining results, mutation specification instances are linked to the documents they are extracted from with the predicate isextractedfromdocument, which is a subproperty of the inverse of foaf:topic. this foaf  <cit>  predicate can be interpreted as having a slightly stronger semantics than necessary for our purposes because its description “a topic of some page or document” can be interpreted as “the main topic of some page or document” by some users. however, we failed to find a better predicate in a sufficiently standard vocabulary. currently, we are using foaf:topic in parallel with the sio  <cit>  predicate ’refers to’  with a more precise semantics, and in the future it may completely replace foaf:topic.

in addition to the object property predicates we have a number of data properties to specify various number-, string- and uri-valued attributes of entities. in particular, hasnormalizedform associates a point mutation code like “i615s”, with a point mutation instance, and hassequence links a protein instance to a string which is a fasta representation of the protein’s amino acid sequence.

use cases
here we introduce the use cases we have adopted to test the suitability of sadi as a medium for providing access to our mutation impact software and data. all our use cases are in the form of queries, i.e., the user is seeking some information from publications or our mutation impact db, in combination with external resources.

use case 1: given a list of publications, identify mutations studied in the papers with their wildtype proteins and impacts on protein properties. in this scenario, a biologist wants a quick summary of mutations studied in a set of papers. he is specifically interested in the proteins being studied as well as the identified change of protein properties. this kind of summarisation can aid literature search in many practical settings, e.g., when a biology researcher looks for related work for a publication. it can also be used by bioinformatics database curators to populate or verify databases.

use case 2: find all mutations and the structure images of wild type proteins that were mutated, where the impact of the mutation is an enhanced haloalkane dehalogenase activity. in this use case we aim to address the needs of a protein engineer who is seeking to understand what mutational changes can enhance the catalytic activity of an industrial enzyme, which is haloalkane dehalogenase in this scenario. the medium for reviewing the causal relationship of mutations on protein activity is a protein structure image which can be annotated with mutations and their impacts retrieved from a database/triplestore  <cit>  or extracted automatically from documents using text mining techniques  <cit> . in our use case, we perform retrieval of the specific protein structures where there are published reports of mutations having a positive impact on catalytic activity. the user would wish to retrieve and review these structures along with mutation locations and impact annotations. the expected output of the integrated sadi services is the selected protein structure files and the corresponding mutations. ideally, we would like to see the amino acids in the mutation positions highlighted on the 3d image of the protein, as it is done in mstrap  <cit> .

use case 3: find all pathways, together with the corresponding pathway images, that might have been altered by a mutation of the protein fibroblast growth factor receptor  <dig>  in this scenario we address the needs of a systems biologist who is seeking to understand the likely impact of reported mutations on signalling or metabolic pathways  <cit>  in which the mutated protein participates. this entails the retrieval of pathway information for the mutated proteins, which can be provided as a pathway diagram also. in the current use case we deal with mutations to the protein fibroblast growth factor receptor  <dig> reported in scientific papers which impact the protein either positively or negatively.

use case 4: find all drugs related to mutated proteins, together with their interaction partners, where the mutation impact is a decreased carbonic anhydrase activity. in this use case we address a query that a researcher in drug discovery would make when looking for existing drugs targeting a new disease condition. in the case of carbonic anhydrase, an enzyme involved in the acid-base balance of blood , enzyme inhibitors such as acetazolamide, cause mild metabolic acidosis. this can be beneficial to patients with severe chronic obstructive pulmonary disease  with chronic hypercapnic ventilatory failure who need a reduction in arterial carbon dioxide and a rise in arterial oxygen and the transport of carbon dioxide out of tissues. the query will help us to identify the names of known drugs targeting the enzyme and what experimental modifications on the protein have resulted in lowering its activity in situ. moreover, the query will also retrieve the names of proteins that interact with the enzyme directly through protein-protein interactions.

use case 5: from the literature, find all reported mutations of the protein with the nssnp rs <dig>  in this use case, a researcher in genomics asks for all known mutations reported in the literature for a protein containing the non-synonymous snp identified with the dbsnp id rs <dig>  by retrieving all known mutations for the protein in which the nssnp is reported, the researcher can find out if any of these reported mutations corresponds to the location of the snp in question. minimally, the researcher can retrieve the full set of mutations to the protein based on reported experimental analysis and their impacts, together with references to the supporting literature. in our settings, we assume that the scope of the search is limited to the publications that have been processed with our text-mining software and semantically indexed in our mutation impact db.

RESULTS
sadi services for mutation impact pipeline and data
as an initial implementation with sadi, we created a service that takes a text in the form of a string literal or, alternatively, a url of a file with the text, and outputs all property assertions derived from the input text, such as links from the text identifier  to the extracted grounded mutations. these grounded mutations also have links to ungrounded mutations, proteins and impacts, in their descriptions. the main purpose of this service is to provide programming- and installation-free access to our text mining pipeline. in fact, we currently use this service ourselves to populate the mutation impact db with owl abox assertions, because it has the capability of converting the raw results of the mutation impact pipeline to owl. the service can also be useful in combination with services that find documents that have to be subsequently analysed.

we illustrate the operation of the service with the following example. in the simplified definition of the input class  given below, individuals eligible as input to the service are required to be instances of bibo:document, have their string content attached with the predicate bibo:content and to have the mime type “text/plain” attached with dc:format:  

the output class definition indicates that the service will attach instances of mio:mutationspecification to the input uris via the predicate foaf:topic:  

we also provide an extract from the definition of the class mutationspecification in the mutation impact ontology, that specifies how the wildtype protein, series of point mutations and corresponding impact are associated with a mutation specification instance:  

here is a sample input to the text-mining service:  

note that the value of bibo:content is a string with the ascii content of the article with pubmed central id  <dig> represented with the uri pmc: <dig> 

this rdf listing shows the corresponding output of the service:  

most of our other mutation impact-related sadi services essentially wrap some ad hoc queries to our mutation impact db. for example, one of the most intensively used services – getmutationbywildtypeprotein – finds all instances of the mutation impact ontology class mutationspecification, given the uniprot id of a protein that acts as the wildtype protein in those mutations. more specifically, the service expects an rdf node, representing a protein, with a uniprot record attached to it via sio:sio_ <dig> , which is in turn linked via sio:sio_ <dig>  to an attribute of the type lsrn:uniprot-identifier, whose string value is attached to it with sio:sio_ <dig> . this listing provides a simplified version of the input class:  

this kind of input modelling makes the service semantically interoperable with many other sadi services working with proteins.

in the output, the service attaches a mutation specification instance to the protein via the predicate mio:proteinisspecifiedaswildtypeby, which is an inverse of mio:groundmutationsto. the class mutationspecification is central to the ontology and the db: its instances represent grounded mentions of mutations and are linked to the corresponding wildtype and mutant proteins, the mutation impacts, and also the texts from which the mutation mentions were extracted. so, two other services - getmutationbymutantprotein and getmutationbyimpact - also find mutationspecification instances by their mutant proteins and required mutation impacts.

two other services retrieve instances of biological entities of specified types, present in our db. the service getmidbbioentitybytype does this for the top level biological entity classes in our ontology, such as protein or point mutation. the service getproteinpropertybytype specialises in protein property types, most of which are currently inherited from the gene ontology. given a subclass of proteinproperty, e.g., go_ <dig>  from the gene ontology, it finds all known instances of this type, whose descriptions contain links to the proteins they characterise.

there are also two auxiliary services: getmutationimpactbyproteinproperty finds mutation impact instances linked to a specified protein property grounded to a specific protein, and getmutationsubseries finds series of elementary mutations identified in a text, that are subsets of a specified set of elementary mutations. we also have two services that visualise grounded mutations by rendering the 3d structure of the wildtype proteins and highlighting the amino acids affected by the point mutations.

the list of all sadi services based on the mutation impact ontology, text mining pipeline and db, can be found in  <cit>  and is also summarised in table  <dig> 

experiments with share
this section contains the main result of our investigation – it describes our experiences using sadi via the sparql engine share to solve the use cases.

in the query examples below we omit prefix declarations – the meaning of the namespace abbreviations is given in table  <dig>  full versions of all queries discussed in this article are available from  <cit> , with instructions on how to execute them via a share web interface installed locally for this purpose.

experiment with use case 1
in this use case, our goal is to formalise the query “given a list of publications, identify mutations studied in the papers with their wildtype proteins and impacts on protein properties” and execute it using our text mining pipeline for mutation impacts. we have uploaded three pdf files with publications about mutations to a location on the web and listed their urls in an rdf document  that will serve as input to our sparql query. this document describes the files as instances of the class bibo:document having the mime type “application/pdf” as the value of the dc:format predicate. for example, the paper with the pubmed id  <dig>  uploaded to our web site, is represented with the following entry, given, for readability, in the notation  <dig> syntax  <cit>  for rdf:  

in general, we often need to create such rdf documents to specify input to queries or to provide additional information necessary to execute the queries, because sparql does not allow inlining assertions in queries directly.

we start with the following simple sparql query:  

where http://unbsj.biordf.net/.../pdfs.rdf abbreviates http://unbsj.biordf.net/util-sadi-services/service-data/pdfs.rdf.

the purpose of this query is essentially to list mutation specification instances  together with the input documents  they are extracted from. our text mining sadi service provides the predicate foaf:topic. however, writing a condition like ?pdf document foaf:topic ?mutationspec is not enough because the service only accepts documents in ascii, whereas our input documents are in pdf. moreover, we are modelling a situation where the user does not know what text formats are accepted by the available text mining services. so, line  <dig> requests a conversion of ?pdf document into all available formats: the predicate dc:hasformat relates different representations of the same document and is provided by our sadi service pdf2ascii. finally, line  <dig> is needed to enumerate pdf documents from the input. note the use of dc:format to specify the mime type of a document.

the query executes in less than one minute and returns twenty six mutation specifications extracted from the three papers from the input file pdfs.rdf. however, returning only mutation specification instances like mio:mutationspecification1292519446381_ <dig> is clearly not enough. our imagined user needs various informative parts of a mutation specification, such as the wildtype protein and identified impact, rather than just a uri. in the service output, these are attached with various predicates, such as mio:groundmutationsto or mio:specifiesimpact, and can be easily requested in the query by adding the following lines:  

line  <dig> extracts the reference to the wildtype protein. lines 2- <dig> extract codes like “i615s” for all the point mutations referenced by the mutation specification. line  <dig> extracts the impact instance, line  <dig> extracts the direction, e.g., mio:positive or mio:neutral, assigned to the impact instance, and lines 6- <dig> extract the types of the affected protein property, e.g., go:go_ <dig>  the select line in the new query can specify ?pdf document, ?protein, ?normalizedmutation, ?impactdirection and ?proteinpropertytype as the answer variables, so the user now can see answers like this:  

the actual answer is given by share in the form of a table where the columns are labelled with the query variables. we do not show the table here as it does not fit due to very long rows. note also that there may be multiple rows with the same wildtype protein but different point mutations or affected protein properties.

although such results are already satisfactory, for extra user convenience we would like to provide readable protein names and the organisms they belong to, in addition to the uniprot ids like “o75907”. none of our services can deliver this information, so we look in the central sadi registry  <cit>  for appropriate predicates and find prop:hasname that relates a protein  to an attribute representing the name of the protein, whose string value is accessible via the data property sio:sio_ <dig>  there is also predicate prop:fromorganism relating a protein to the corresponding taxon record that is linked to its scientific name attribute via sio:sio_ <dig>  both predicates are provided by the service uniprotinfo we found in the public registry  <cit> . the listing for the final query is given in figure  <dig>  in about three minutes, the execution of this query produced several dozens of bindings like the following one:  

the main message we would like this use case to deliver is that by packaging our text mining software as a sadi service we offer its functionality to the end users in a programming-free manner. this possibility alone already makes sadi a valuable part of our infrastructure for annotating mutations. the use of a separate service for pdf-to-ascii conversion demonstrates the extra flexibility this approach provides - one can use our text mining service with any text formats, provided that there are sadi services extracting ascii contents from these formats. note also how easy was it to present our text mining results in combination with data from external sources, as exemplified by the use of the uniprotinfo service. in the next four use cases we will focus our attention on the value of such integration.

experiment with use case 2
baseline functionality: show the protein structure. our query “find all mutations and the structure images of wild type proteins that were mutated, where the impact of the mutation is an enhanced haloalkane dehalogenase activity” can be realised with the sparql shown in figure  <dig>  let us analyse how we construct this query. the predicate mioe:proteinpropertyhastype in our ontology, provided by the service getproteinpropertybytype, links grounded protein properties with their types, so we can use it to enumerate known instances of go_ <dig>  in lines  <dig> and  <dig>  mio:af fectproperty links the grounded protein properties to the corresponding instances of mutation impacts and mio:hasdirection selects only positive impacts. using mio:specifiesimpact, we can select instances of mutation specifications , which in turn link to the corresponding wildtype proteins  and series of elementary mutations . we would like to see readable codes of elementary mutations in the output, like d124n or v226a, so we use mio:containselementarymutation to retrieve the corresponding elementary mutations and mio:hasnormalizedform to map them to the corresponding codes.

so far we have used only predicates from our mutation impact ontology. since the essence of use case  <dig> is visualisation, we look for predicates in sadi-related ontologies, that could link proteins to their images. there is no direct link, but we can use the composition of props:has3dstructure and obj:hasjmol3dstructurevisualization to first retrieve a reference to the pdb record of the protein, and then find the corresponding graphics file.

share was able to compute our query using three of our sadi services – getproteinpropertybytype, getmutationimpactbyproteinproperty and getmutationbyimpact – and two third party sadi services from the registry, providing props:has3dstructure and obj:hasjmol3dstructurevisualization, and yet this was completely transparent to us as the end users. we only dealt with an almost completely declarative query composed of predicates that we were able to find in ontologies referenced by available sadi services. the only thing we need to know beyond the semantics of a predicate is the direction in which available services compute it: e.g., we cannot use props:has3dstructure to get from a pdb id to the corresponding protein because there is currently no service that would annotate a pdb id with the inverse of props:has3dstructure. finding the services, their invocation and some deduction with the ontological definitions of predicates, was done by share completely automatically. note especially the ease with which integrating our mutation-related information with the external sources of data was achieved.

extended functionality: locating mutations on the protein structures. although the query above illustrates well the integrative power of sadi and share, it does not fully satisfy the requirements for the use case because the mutations are not shown on the protein 3d structure. at the time of our experiments, no existing sadi services were providing such functionality, so we wrote our own service visualisemutationseries. this service accepts a mutation specification including a protein instance identified with a uniprot record uri, as input. it extracts references to pdb  <cit>  files representing parts of the protein sequence obtained by different methods, e.g., x-ray crystallography, from the uniprot record. then it creates a small jmol  <cit>  script that instructs jmol to render the amino acid sequence with the positions of the specified point mutations highlighted on the structure. in the output, the service links the input mutation specification to an html document using the predicate obj:hasjmol3dstructurevisualization. this small html document calls the jmol viewer applet on the created script, so that when it is loaded into a web browser with java applet support, the user can see and rotate the 3d image of the protein structure with wildtype residues highlighted on it. figure  <dig> is a screenshot of a jmol rendering of the structure of p <dig> with the wildtype residue of the point mutation l248i highlighted.

all it takes to use the visualisemutationseries service for the purposes of our use case is to replace lines 19- <dig> with the triple pattern  

as shown in figure  <dig> 

homology modelling for missing structures. our experiments with mutation visualisation using the known protein structures from the protein data bank   <cit>  revealed that many proteins of interest don’t yet have pdb records. to rectify this, at least partially, we adopted the solution used in mstrapviz  <cit> . if the amino acid sequence of a protein is known, which is usually the case with uniprot listed proteins, we look for homologous sequences for which pdb files exist and then call the modeller program  <cit>  to predict the 3d structure of the target protein by adjusting the structures of the template sequences.

to implement this, we created the sadi service visualisemutationserieswithhomologymodeling that takes a mutation specification with a wildtype protein whose amino acid sequence is given as a fasta string, as input. the protein’s sequence must also have a homologue identified by a pdb record. the service runs modeller on these data and the created pdb file representing the predicted structure is treated exactly the same way as visualisemutationseries treats files hosted by the protein data bank, i.e., it is visualised with jmol, together with the specified point mutations. additionally, we have written the sadi service blastpdb that wraps a pdb soap service based on the blast algorithm for searching for homologous sequences in the pdb database. to test the new services, we ran a query obtained by replacing go_ <dig> in the query in figure  <dig>  with go_ <dig>  and requesting negative impacts, so that the relevant proteins in our mutation impact db don’t have pdb files . the query is executed in two minutes and returns visualisations of one protein esterase ypfh with four distinct point mutations. since two homologous pdb sequences are used to model the protein’s 3d structure, the total number of answers for the query is eight.

experiment with use case 3
the work required by this use case  can also be divided into two parts: the first part can be done using the predicates from our ontology, and the second part has to be delegated to external resources, dealing with genes, pathways and pathway visualisation. since we know that the wildtype protein is fibroblast growth factor receptor  <dig> , we can easily retrieve the mutation specifications linked to this protein with the property mio:groundmutationsto. these instances will have impacts attached to them with mio:specifiesimpact, and we can specify the interesting impact directions with mio:hasdirection.

using pred:isencodedby we also map the protein to the corresponding gene, and sio:sio_ <dig>  allows to retrieve the pathways in which the protein participates, pred:visualizedbypathwaydiagram will fetch the corresponding graphics file url. the resulting query is shown in figure  <dig>  note that the input file in the from clause just qualifies uniprot:p <dig> as an instance of mio:protein to make it a legitimate input to the service getmutationbywildtypeprotein that links proteins to mutations specifications. share executed the query using this service and two external sadi services providing sio:sio_ <dig> and pred:visualizedbypathwaydiagram. the execution took less than one minute and returned five pathways with diagrams.

experiment with use case 4
this use case  is somewhat similar to use case 2: given the protein property type, we retrieve the grounded properties, positive impacts and the wildtype proteins with the help of some predicates from our ontology. the connection from the proteins to drug names is realised with the predicates obj:istargetofdrug and obj:hasdruggenericname. separately, we find the interacting proteins with pred:hasmolecularinteractionwith. to make go:go_ <dig> a valid input to our service getmutationimpactbyproteinproperty, it is qualified as a mioe:proteinpropertytype in the input file in the from clause. the resulting query is shown in figure  <dig>  the query was executed in less than two minutes and returned  <dig> distinct drug names and  <dig> interacting proteins.

experiment with use case 5
finally, the query “from the literature find all reported mutations of the protein with the nssnp rs2305178” was implemented with the sparql query shown in figure  <dig>  the predicate sio:sio_ <dig>  in line  <dig> maps the specified dbsnp id to an entrez gene id. if we were dealing with completely declarative queries, it would be enough to use a composition of the predicates obj:correspondstoentrezgene, obj:hasrefseqtranscript and pred:isencodedby, as in lines 9- <dig>  to map the entrez gene id to a protein. however, no sadi service currently provides the inverses to the first two predicates, so the composition can only work in the direction from proteins to entrez gene ids. to use this possibility, we had to implement the service getmidbbioentitybytype that enumerates all proteins known in our db. in fact, the service is more general - it enumerates instances of several main biological entity classes from our ontology, such as mutationimpact or pointmutation. the service provides the inverse of mioe:biologicalentityhastype whose use is demonstrated in line  <dig>  linking the protein to elementary mutations is done exactly the same way as in use case  <dig>  once share has the necessary data in the working memory, it computes the join on the variable ?ezgene. finally, the last two lines in the query serve to retrieve the urls of the documents from which the corresponding mutation specifications were extracted.

discussion
we are not aware of any work solving exactly the same problem, i. e. publishing text-mined information on mutations and text-mining software itself, with semantic web services, so we look at related work falling into a more general topic. since the problem we are solving is essentially an instance of the more general problem of agile integration of bioinformatics resources with the help of semantic web services, we will refer the reader to two projects in this area.

biomoby  <cit>  is the most closely related technology, simply because it is a direct predecessor of sadi - the sadi project emerged as an attempt to better integrate services into the general semantic web context  <cit> . sadi inherited much of the biomoby ideology, in particular that the messages exchanged between clients and services carry their semantics by using ontology-based formats, and the decentralised domain ontology use. from the perspective of our case study, the key advantage of sadi is that the relation between inputs and outputs is explicitly ontologically defined, whereas biomoby still follows the earlier semantic web service paradigm that only requires a service’s functionality to be categorised, e. g. by qualifying the service as an instance of an ontological class, say sequenceallignmentservice, and by mapping the input and output data types to ontological classes, possibly from a domain ontology. this difference makes us strongly prefer sadi because all our use scenarios assume, as a user, a biologist rather than a bioinformatician who would be comfortable with an ontology of bioinformatics operations and data types. we also assume that in many cases a non-bioinformatician user will also prefer dealing with declarative queries that are executed completely automatically, to creating workflows, even with the help of tools that exploit the service categorisation and the semantics of service io to ease such workflow creation.

both sadi and biomoby require service providers to adhere to the io conventions imposed by these frameworks. however, access to many bioinformatics resources is already available in the form of web services consuming and producing ad hoc xml-based formats, e.g., soap services. such legacy services, as well as new web services whose providers cannot or don’t want to make them natively semantic, can sometimes be turned into semantic web services by semantic annotation. my-grid  <cit>  is a mature project that follows this approach by allowing services described with wsdl to be annotated, possibly by a third party, with respect to a centralised ontology. although the use of unrestricted xml as the data model for service io is a great convenience, some other features of mygrid make its use for our purposes problematic. first, as in the case with biomoby, there is no way to describe the semantics of a service by ontologically relating the input and output. second, the necessity of conversions between datatypes consumed and produced by different services seems to complicate the workflow construction – this gives services “speaking” the same language a clear advantage. finally, the reliance on a centrally curated ontology would deprive us of the extra flexibility in semantic modelling of services that the sadi and biomoby approaches enjoy. in the concrete settings of our case study, it is unclear how we could substitute the classes and predicates from our mutation impact ontology with terms from, for example, the mygrid domain ontology.

CONCLUSIONS
the primary goal of our case study was to explore the suitability of the sadi framework as a medium to facilitate data sharing and integration across biological data types. we have identified that sadi provides an effective way of exposing our mutation impact data such that it can be leveraged by a variety of stakeholders in multiple use cases.

our experience in deploying and registering mutation services in accordance with sadi specifications was positive, albeit with some challenges. in particular, we identified that advanced skills in knowledge engineering were required to build semantic representations of the services. more specifically, a sadi service provider has to  find classes and predicates in existing ontologies, that model his data well, and  ensure that his modelling of service io is compatible with the io of other sadi services with which the new service is intended to be composed. the first task is a general problem for all activities requiring ontology-based modelling, and seems to have no simple solution. it seems safe to assume that at least in the near future this task has to be performed mostly manually by reasonably experienced knowledge engineers. difficulties associated with the second task are likely to be alleviated with the appearance of more sophisticated tools for browsing networks of sadi services.

we also note that formulating the queries based on the sadi services requires cumbersome search for predicates in the sadi-related ontologies. clearly, the necessary infrastructure for such search is yet to be built.

another conclusion we have drawn from our case study is that a greater choice of available sadi clients is necessary to make sadi practically useful, especially in production settings. we will look at the sadi plugin for taverna  <cit> , which is currently under active development.

most, if not all, of our queries could be replaced with browsing, especially faceted, of the virtual rdf graph implied by the services, which is much more user friendly than writing sparql queries. unfortunately, the only currently available rdf browser with sadi support is sentient knowledge explorer , which is a commercial product.

another important conclusion we have drawn from our experiments is that some limitations of the sadi-based approach to data integration also restrict its applicability strictly to the discovery phase in a scientific or r&d process. in simple words, one can use sadi to come up with hypotheses and obtain preliminary evidence, but sadi-produced results cannot be used as hard evidence. the relevant limitations are the absence of answer completeness guarantee with the existing query client, absence of result reproducibility guarantee and lack of answer justifications. the absence of completeness guarantee, mentioned in the section about share, and the inherent irreproducibility of results due to the reliance on third party services that can be down, inaccessible, etc., make statistical judgements based on answers returned by share unreliable, although some valuable insights can be obtained and used later to drive more rigorous investigations. creating clients that would provide verifiable answer justifications seems a good target for research.

the fact that the initial query design for use case  <dig> did not work because some services were missing suggests that the general utility of sadi is predicated on the coverage of bioinformatics resources and relevant onto-logical predicates by existing services. in this respect, we would like to mention that the sadi network of public services is growing fast – it is expected to contain over  <dig> services by the end of  <dig> 

in future work we aim to extend the mutation impact db with more data types related to mutation annotations extracted from the literature, and create the corresponding sadi services facilitating integration with other bioinformatics data. we are also conducting case studies on the use of sadi for other biomedical domains, such as lipidomics and experimental proteomics data.

apart from the integration of distributed and heterogeneous sources of data, the sadi framework can be useful simply as a medium for semantic querying of a single database, so that sparql queries can be answered on an sql database. we are exploring this possibility in a case study with a large health care research datawarehouse.

list of abbreviations used
gate: general architecture for text engineering; sadi: semantic automated discovery and integration; sparql: sparql protocol and rdf query language; share: semantic health and research environment; go: gene ontology; owl: web ontology language; rdf: resource description framework; uri: universal resource identifier; sio: semanticscience integrated ontology; foaf: friend-of-a-friend ontology; snp: single-nucleotide polymorphism; mime: multipurpose internet mail extensions; pdb: protein data bank; soap: simple object access protocol.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
ar wrote the sadi services and did the experiments with the sparql queries. jbl co-developed the use cases with cjob and contributed to the share experiments at the early stages. cjob coordinated the work. all authors contributed to the manuscript.

