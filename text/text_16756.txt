BACKGROUND
pattern recognition methods allow the classification of objects or patterns in a number of classes  <cit> . in statistical pattern recognition, given a set of classes and an unknown object, a pattern recognition system associates the object to one of the classes, based on defined measures in a feature space. in many applications, specifically in bioinformatics, the feature space dimension tends to be very large, making difficult both classification tasks and network inference. in order to overcome this inconvenient situation, the study of dimensionality reduction problem in pattern recognition is very important.

the so called "curse of dimensionality"  <cit>  is a phenomenon in which the number of training samples required to a satisfactory classification performance or network inference is given by an exponential function of the feature space dimension. this is the main motivation by which performing dimensionality reduction is important in problems with large number of features and small number of training samples. many applications in bioinformatics belong to this context. data sets containing mrna transcription expressions from microarray or sage, for example, present thousands of genes  and only some dozens of samples that may come from cell states or types of tissues.

in this context, this work presents an open-source feature selection environment, which includes different search algorithms and criterion functions. due to the curse of dimensionality phenomenon, error estimation is a crucial issue. we have developed two ways to embed error estimation in the criterion functions, one based on classification error and another based on classification information. the main idea is based on the penalization of non-observed or rarely observed instances. after the feature selection, it is possible to apply classical error estimation methods like resubstitution and holdout cross-validation. illustrative results obtained on gene regulation networks inference and on classification of breast cancer cells using the proposed software are included in the present paper.

methods
data preprocessing
a critical issue in gene expression data analysis is the pre-processing of the data. depending on the criterion function used, a step of data quantization may be required if the original input data is not discrete. the quantization usually requires a normalization of the data. in the software, the normal transformation is implemented. it consists in subtracting the signal  by its mean and then dividing by its standard deviation. formally, for every signal g, its equation is given by η=g−eσ, where e and σ are, respectively, the expectation and standard deviation of g. there are two options:  <dig> – normalization by feature ;  <dig> – normalization by sample .

the quantization of a normalized signal η  is a mapping from the continuous expression into k qualitative expressions { <dig>   <dig>  ..., k - 1}. it is performed by a threshold mapping that divides the interval given by the minimum and the maximum values of g in k intervals of equal size. a given continuous value r of η is mapped to an integer i -  <dig>  where i is the i-th interval to which r belongs.

implemented feature selection algorithms
the simplest implemented feature selection algorithm is the exhaustive search. this algorithm searches the whole search space. as a result, the selected features are optimal according to the criterion function chosen to guide the algorithm. however, the computational cost often makes this approach inadequate, thus requiring alternative  algorithms.

in this work we have implemented two sub-optimal approaches with unique solution, being also known as wrappers  <cit> . in the first one, the selected subset starts empty and features are inserted by optimizing a criterion function until a stop condition is satisfied, which is often based on the subset size or a threshold. this method is known as sequential forward selection   <cit> . although simple and efficient, sfs presents an undesirable drawback known as nesting effect. this effect arises because the selected features are never discarded.

in order to circumvent the nesting effect, the sequential floating forward selection  is also available in the software. the sffs algorithm may be formalized as in  <cit> . a schematic flowchart of the sffs algorithm is presented in figure  <dig>  in this algorithm, the sfs and the sbs  are successively applied.

considering the adopted feature selection and criterion function, the intermediate results are stored in a list. the best sets of this list are displayed as the algorithm result.

other feature selection methods will be implemented, both wrappers  and filters , and embedded methods   <cit> . also, methods for feature extraction, such as pca and ica  <cit> , will be implemented. we hope that the availability of the software as open-source will help the inclusion of other functions by the international community.

implemented criterion functions
we implemented criterion functions based on classification information  and classification error , introducing some penalization on poorly or non-observed patterns. introducing penalization on rarely observed instances embeds the error estimation into the criterion functions. this approach has two main advantages. first, the selected features are obtained not only by their informative content about the classes, but also by the number of well observed instances they have . another advantage is that the right dimension of the feature subset is also estimated . in the software, the implemented feature selection algorithms  requires the dimension parameter just to limit the execution time, performing the procedures until the subset size reaches that parameter. according to the criterion function values, the resulting subsets can have smaller dimension than the specified by the parameter.

mean conditional entropy
information theory, originated by shannon's works  <cit> , can be employed on feature selection problems  <cit> . the mean conditional entropy of y given all the possible instances x ∈ x is given by:

  h=∑x∈xp h, 

where p is the conditional probability of y given the observation of the instance x, h = -Σy∈y p log p. shannon's entropy can be generalized by using tsallis entropy  <cit>  with a parameter q. we have that q =  <dig> is the shannon's entropy , q <  <dig> is the sub-extensive entropy and q >  <dig> is the super-extensive entropy. the tsallis entropy is given by h=1q−1)q) lower values of h yield better feature subspaces .

coefficient of determination
the coefficient of determination   <cit> , like the conditional entropy, is a non-linear criterion useful for feature selection problems  <cit> . the cod is given by:

  cody=εy−εyεy, 

where εy =  <dig> - maxy∈y p is the prior error, i.e., the error by predicting y in the absence of other observations, and εy = Σx∈x p ) is the average error by predicting y based on the observation of x. larger values of cod yield to better feature subspaces .

penalization of non-observed instances
in order to embed the error estimation, due to feature vectors with large dimensions and insufficient number of samples, we adopted a strategy to involve non-observed instances in the criterion value calculus  <cit> . a positive probability mass is attributed to the non-observed instances and the corresponding contribution is the same as observing only the y values with no other observations. in the case of mean conditional entropy, the non-observed instances receive the entropy equal to h whereas the cod receives the prior error εy value. the probability mass for the non-observed instances is parameterized by α. this parameter is added to the absolute frequency  of all possible instances. so, the mean conditional entropy with this type of penalization becomes:

  h=αhαm+s+∑i=1nhαm+s, 

where m is the number of possible instances of the feature vector x, n is the number of observed instances , fi is the absolute frequency  of xi and s is the number of samples.

the cod becomes:

  cody=1−ααm+s−∑i=1n)εy. 

penalization of rarely observed instances
in this penalization, the non-observed instances are not taken into account. this penalization consists in changing the conditional probability distribution of the instances that have just a unique observation  <cit> . it makes sense because if an instance x has only  <dig> observation, the value of y is fully determined  =  <dig> and cody = 1), but the confidence about the real distribution of p is very low. a parameter β gives a confidence value that y = y. the main idea is to equally distribute  <dig> - β over all p and to attribute β to p. in barrera et al  <cit> , the β value is 1|y| where |y| is the number of classes , i.e., the uniform distribution . by adapting this penalization to the equation , the mean conditional entropy becomes:

  h=m−nsh,...,f))+∑x∈x:p>1sp h, 

where f : { <dig>   <dig>  ..., |y| - 1} →  <cit>  is the probability distribution given by

 f={βfor i=y,1−β|y|−1for i≠y, 

and n is the number of instances x with p>1s .

since εy =  <dig> - β when p=1s, the cod with this penalization is given by:

  cody=1−sεy−∑x∈x:p>1sp)εy. 

classifier design and generalization
after the feature selection, the classifier is designed from the table of conditional probabilities of the classes  given the patterns . this table is used as a bayesian classifier where, for each given instance, the chosen label is the one with maximum conditional probability for the considered instance. in case of ties, i.e., instances that have two or more labels of maximum probability , generalization according to some criterion is carried out. a commonly used criterion is the nearest neighbors with some distance metric  <cit> . the nearest neighbors using euclidean distance has been implemented for generalization. in this implementation, the nearest neighbors with equal distances are taken and the occurrences of each label given these patterns are summed. if the tie persists, the next nearest neighbors are taken and the process is repeated until only one of such labels has the maximum number of occurrences . the label with the maximum number of occurrences at the end of this process is chosen as the class to which the considered instance belongs.

there are many classifiers described in the literature, such as parzen, fisher linear discriminant, binary decision tree, perceptron, support vector machines and so on  <cit> . the software has been designed so that such additional classifiers may be incorporated. one of the objectives of the software is to implement as many classifiers as possible. however, the primary focus is given on feature selection algorithms and criterion functions.

error estimation methods
in order to validate and compare feature selection algorithms, error estimation methods are required, especially for data with high dimensionality and small number of samples which is usually the case in bioinformatics problems.

the first method implemented in the software is the simple resubstitution in which the training set is also used as testing set. for many bioinformatics problems, the resubstitution error estimation works fine since usually there is not enough data samples to perform a good estimation, and partitioning the data leads to an even smaller training sample size .

the holdout cross validation is also available in the software. it consists in partitioning the data in two sets: training and test sets. the size of the training set can be defined in terms of percentage of total data samples. thus, the feature selection method is applied to the training set, the classifier is designed taking into account the resulting feature set and the conditional probability distributions given by the training set. finally, the designed classifier is applied to the test set and the classification error rate is obtained. alternative error estimation methods like k-fold cross-validation, leave-one-out  and bootstrap will be implemented in the software.

network inference
if one considers the training set as time series data, i.e., each sample representing a state of the system at time t, it is possible to generate networks of prediction based on the probabilistic genetic network model described in  <cit> . this model is a markov chain that mimics the properties of a gene as a non-linear stochastic gate. given a fixed gene y as target, the general idea of this model is to determine the subset of genes z ⊆ x that makes the best prediction of the y value in the next instant of time. in order to achieve this intent, the system is considered translation invariant, i.e., the transition function is the same for every time instant t. in this way, it is possible to apply a feature selection method for every target, since a sample is represented by the pair  and the training set is composed by a collection of these samples along the time domain . after the application of the feature selection process for every target, the best feature subsets obtained for each target gene define the network connections.

software description
the software is implemented in java in order to be executable in different platforms. it is open source and intended to be continuously developed in a world-wide collaboration. there are four main panels: the first one allows the user to load the data set . the second is optional for the user to define a quantization degree to the data set. the quantized data may be visualized . it is worth noting that the mean conditional entropy and the cod require data quantization to discrete values. this fact explains the quantization step available in the software. please refer to the section data preprocessing for further information on the normalization and quantization methods applied. the reason to keep the quantization optional is because other criterion functions, such as the distance-based criteria, do not require quantization and the software is intended to implement as many criterion functions as possible. the third panel is dedicated to perform single tests . with this panel, the user can execute feature selection methods  along with the criterion function . the user has two options: the first one is to apply the simple resubstitution method ; the second option consists in to apply the classifier on a different data set given by the user specified at input test set text box .

the fourth panel is very similar to the previous one, but allows the application of automatic cross-validation procedures  <cit>   . this panel presents only the holdout cross-validation with training and test sets partitioning performed randomly.

there are other implemented utilities, including the visualization of the results of the feature selection as scatterplots, parallel coordinates  <cit>  and graphs. every panel has its own help button that activates a popup window explaining the functionalities and options regarding the corresponding panel. also, description boxes are displayed when the mouse cursor is placed over some option in order to quickly recall the user about the functionalities of the system.

RESULTS
this section presents some illustrative results in two different typical situations in bioinformatics. all results shown here were obtained by using the default values in the software: sffs with mean conditional entropy and penalization of non-observed instances, α =  <dig>  tsallis parameter q =  <dig>  and maximum set size =  <dig> 

initially the software was applied for feature selection in a biological classification problem to classify breast cancer cells  <cit>  in two possible classes: benign and malignant. the training set is composed by  <dig> instances and  <dig> features. the sffs algorithm with the entropy criterion was explored. the results shown in figure  <dig> present very low variations and high accurate classification achieving  <dig> % of accuracy on average. the scatterplot taking the first two features of the feature set result obtained by sffs is shown in figure  <dig>  while the parallel coordinates of the resulting six features are shown in figure  <dig> 

the second addressed computational biology problem is genetic network identification. in this case we used an artificial genetic network generated by the approach presented in  <cit> . the adopted parameters were:  <dig> nodes, binary quantization,  <dig> transitions ,  <dig> average degree per node and random graphs of erdös-rényi. figure  <dig> presents the network identification, in which no false negatives occurred and just few false positives. the methodology of network generation is described in section network inference, the same as used to generate probabilistic genetic networks of the plasmodium falciparum from microarray data  <cit> , thus showing the possibility of exploring the software in such an important problem in bioinformatics.

CONCLUSIONS
the proposed feature selection environment allows data analysis using several algorithms, criterion functions and graphic visualization tools. our experiments have shown the software effectiveness in two distinct types of biological problems. besides, the environment can be used in different pattern recognition applications, although the main concern regards bioinformatics tasks, especially those involving high-dimensional data  with small number of samples. users without programming skills are allowed to manipulate the software in an easy way, just by clicking to select file inputs, quantization, algorithms, criterion functions, error estimation methods and visualization of the results, while an intuitive help system quickly presents the instructions of the present functionalities that the user may look for. on the other hand, the availability of the software as open-source allows programmers to explore the implemented methods as libraries in their own programs. the environment is implemented in "wizard style", i.e., with tabs delimiting each procedure. a complete technical report that complements the help information is available at the project web site. two video demos of the software in use are available at .

this software opens space for future work. the next steps consist in the implementation of other classical feature selection algorithms, including wrappers, filters and embedded methods, criterion functions ; classifier design methods like parzen, fisher linear discriminant, binary decision tree, perceptron and support vector machines; error estimation methods such as leave-one-out, k-fold cross-validation and bootstrap; and the inclusion of classical methods of feature extraction, such as pca and ica  <cit> .

availability and requirements
• project name: dimreduction

• project home pages:  and 

• operating system: platform independent

• programming language: java

• other requirements: java runtime environment   <dig>  or higher

• license: code available; gnu lesser general public license

• video demos: 

• complete technical report: 

authors' contributions
fml and dcm wrote the software code, analyzed the data and wrote the manuscript. rmcj participated in the design and coordination of the study. all authors contributed to, read and approved the final manuscript.

