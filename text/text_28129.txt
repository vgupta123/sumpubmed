BACKGROUND
feature weighting is an important step in the preprocessing of data, especially in gene selection for cancer classification. the growing abundance of genome-wide sequence data made possible by high-throughput technologies, has sparked widespread interest in linking sequence information to biological phenotypes. however, the expression data usually consist of vast numbers of genes , but with small sample size. therefore, feature selection is a necessary for solving such problems. reducing the dimensionality of the feature space and selecting the most informative genes for effective classification with new or existing classifiers are commonly adopted techniques in empirical studies.

in general, the feature weights are obtained by assigning a continuous relevance value to each feature via a learning algorithm by focusing on the context or domain knowledge. the feature weighting procedure is particularly useful for instances based on learning models, in which a distance metric is typically constructed using all features. moreover, feature weighting can reduce the risk of overfitting by removing noisy features, thereby improving the predictive accuracy. existing feature selection methods broadly fall into two categories: wrapper and filter methods. wrapper methods use the predictive accuracy of predetermined classification algorithms , such as the support vector machine , as the criterion for determining the goodness of a subset of features  <cit> . filter methods select features according to discriminant criteria based on the characteristics of the data, independent of any classification algorithms  <cit> . commonly used discriminant criteria include entropy measurements  <cit> , fisher ratio measurements  <cit> , mutual information measurements  <cit> , and relief-based measurements  <cit> .

as a result of emerging needs in the biomedical and bioinformatics fields, researchers are particularly interested in algorithms that can process data containing features with large  dimensions, for example, microarray data in cancer research. therefore, filter methods are widely used owing to their efficient computation. of the existing filter methods for feature weighting, the relief algorithm  <cit>  is considered to be one of the most successful owing to its simplicity and effectiveness. the main idea behind relief is to iteratively update feature weights iteratively using a distance margin to estimate the difference between neighboring patterns. the algorithm has been further generalized  to average multiple nearest neighbors, instead of just one, when computing sample margins, whose name is relief-f  <cit> . sun et al. showed that relief-f achieves significant improvement in performance over the original relief. sun also systematically proved that relief is indeed an online algorithm for a convex optimization problem  <cit> . by maximizing the averaged margin of the nearest patterns in the feature scaled space, relief can estimate the feature weights in a straightforward and efficient manner. based on the theoretical framework, i-relief, an outlier removal scheme, can be applied since the margin averaging is sensitive to large variations  <cit> .

to accomplish sparse feature weighting, the author incorporated a l <dig> penalty into the optimization by i-relief  <cit> .

in this paper, we propose a new feature weighting scheme within the relief framework. the main contribution of the proposed algorithm is that the feature weights are estimated from local patterns approximated by a locally linear hyperplane, and thus we call the proposed algorithm as lh-relief or , for short. it is shown that the proposed feature weighting scheme achieves good performance when combined with standard classification models, such as the support vector machine , naive bayes   <cit> , k-nearest neighbors , linear discriminant analysis   <cit>  and kierarchical k-nearest neighbor   <cit> . the superior performance with respect to classification accuracy and excellent robustness to data heavily contaminated by noises make the proposed method promising for using in bioinformatics, where data are severely degraded by background artefacts owing to sampling bias or the high degree of redundancy, such as in the simultaneous parallel sequencing of large/huge numbers of genes.

the advantages of our method are as follows:  the gene selection process considers the discriminative power of multiple similar genes that are conditional on their linear combinations. this allows joint interactions between genes to be fully incorporated to reflect the importance of similar genes;  lhr assigns weights to genes and thus allows the selection of important genes that can accurately classify samples;  using the genes selected by lhr, classic classifiers including nb, lda, svm, hknn and knn achieved comparable or even superior accuracy as reported in the literature. this confirms that incorporation of interactions among similar genes in feature weighting estimation under local linear assumptions not only conveys information of the underlying bio-molecular reaction mechanisms, but also provides high gene selection accuracy.

RESULTS
to evaluate the performance of the proposed lhr, we conducted extensive experiments on different datasets. first, we performed experiments on a synthetic data from the famous fermat’s spiral problem  <cit> . we then tested it on nine medium to large benchmark microarray datasets, which were all used to investigate the relationship between cancers and gene expression.

evaluation methods
in this study, we tested the performance of the proposed lhr by combining it with standard classifiers, including nb, knn, svm, and hknn  <cit> . we applied leave-one-out cross-validation  or 10-fold cross validation  to evaluate classification accuracy. loocv provides an unbiased estimate of the generalization error for stable classifiers such as knn. using loocv, each sample in the dataset was predicted by the model built from the rest of the samples and the accuracy for each predication was included in the final measurement. using the 10-fold cv scheme, the dataset was randomly divided into ten equal subsets. at each turn, nine subsets were used to construct the model while the remaining subset was used for prediction. the average accuracy for  <dig> iterations was recorded as the final measurement. for classifiers with tuning parameters , the optimal parameters were first estimated with 5-fold cv using the training data and then used in the modeling. to simplify the comparison, some of the accuracy results were taken from the literature.

parameter settings
lhr takes two parameters: the number of nearest neighbors  and the regularized constant . the choice of k depends on the sample size. for small samples, k should be small, such as  <dig> or  <dig>  whereas for large samples, k should be set to a larger value, such as  <dig> or  <dig>  performance generally improves as k increases, however, beyond a certain threshold, larger values of k may not lead to any further improvement  <cit> . a rule of thumb is to set k to be the odd number  <dig>  λ helps to stabilize the matrix inversion from singular and is generally a tiny constant. in our experiments, we set λ=10- <dig> 

synthetic experiments on fermat’s spiral problem
in the first experiment, we tested the performance of the proposed method on the well-known fermat’s spiral problem. the test dataset consists of two classes with  <dig> samples for each class. the labels of the spiral are completely determined by its first two features. the shape of the fermat’s spiral distribution is shown in figure  <dig>  heuristically, the label of a sample can easily be inferred from its local neighbors. therefore, classification based on local information thus gives a more accurate result than global measurement based prediction  since the latter is sensitive to noise degradation. to test the stability and robustness of lhr, irrelevant features following the standard normal distribution were added to the spiral for classification testing. the dimensions of the irrelevant features were set to { <dig> , <dig> , <dig> , <dig> , <dig> ,10000}. to compare the ability to recover informative features, both the i-relief and logo algorithms were also used because of its intrinsic closeness to lhr. the three feature weighting schemes were first applied to rank the importance of the features. only the top five ranked features were retained to test the robustness of feature selection schemes under noisy contamination. performance comparisons were conducted on the truncated dataset using five classic classifiers: svm, lda, nb, knn, and hknn. for each experiment, both 10-fold cv and loocv were used to evaluate the classification accuracy. to eliminate statistical variations, we repeated the experiments ten times on each dataset and recorded the average classification errors. the detailed numerical results are given in tables  <dig> and  <dig> for 10-fold cv and loocv, respectively. to visualize the results, we created a box plot of the distributions thereof for the experimental results after 10-fold cv and loocv in figure  <dig> and , respectively. each plot represents the classification accuracy for a single dataset. figure  <dig> shows the 10-fold cv accuracy for each of the five classifiers against the dimensions of the noisy features. figure  <dig> shows the loocv accuracy values against the dimensions of the noisy features. we use dark colors to denote the accuracy results achieved using i-relief and logo, while a light color is used for those by lhr. in most cases, the performance of lhr coupled with various classifiers is superior to that of both i-relief and logo, and thus the corresponding box plot lies above the ones for i-relief and logo.

when using 10-fold cv criteria, the lhr outperforms both logo and i-relief in terms of accuracy by classical classifier of svm, lda, nb, knn and hknn. the better averaged value after the three methods are highlighted in bold. with the increase of dimension of the irrelevant features, the performance of lhr keeps stable.

when using loocv criteria, the lhr outperforms both i-relief and logo in terms of accuracy after classical classifier of svm, lda, nb, knn and hknn. the better averaged value after the two methods are highlighted in bold. with the increase of dimension of the irrelevant features, the performance of both logo and i-relief are degraded while lhr keeps stable.

the line graph of the average performance confirms that the proposed method is more robust to noise than i-relief and logo. in both cv experiments, we observed that the performance of the three methods was very similar in case where the dimension of the irrelevant features was small. for example, with a zero dimension of irrelevant features, i.e, no noisy features, classification results by the five classifiers were very similar. the average accuracy is  <dig> % for lhr and  <dig> % for 10-fold cv,  <dig> % for lhr and  <dig> % for loocv. however, as the dimension of the irrelevant features increases, both the performance of i-relief and logo are severely degraded by the noisy features. in comparison, the performance of lhr is very stable and superior to that of the other combinations. in both experiments, the overall accuracy by lhr is better than that of i-relief and logo. we also observed that the accuracies after logo, when combining with the five classifiers, were in small variance. this nice property implies that the logo method could derive features that are less dependent on classification model, and thus are less redundant than lhr and i-relief do.

empirical large/huge microarray datasets
in the second experiment, we tested the performance of the proposed algorithm on nine binary microarray datasets. the benchmark datasets, which have been widely used to test a variety of algorithms, are all related to human cancers, including the central nervous system, colorectal, diffuse large b-cell lymphoma, leukemia, lung, and prostate tumors. characteristics of the datasets are summarized in table  <dig> 

we note that most of the test datasets have small sample sizes . this poses a difficulty in evaluating the performances of classifiers using the standard fold cv schemes. in this experiment, the loocv method was used instead to estimate the accuracy of the classifiers. each sample in the dataset was predicted by a classifier constructed using the rest of the samples. to assess the generality of the selected informative genes, classic classifiers including lda, knn, nb, hknn and svm were tested on the selected genes. the experimental results are summarized in table  <dig>  note that some of the results were taken directly from the literature.

ξclassification with our selected genes.

ηclassification with selected genes by  <cit> .

γthe value of  <dig>  in  <cit>  could have been rounded to  <dig>  and is suboptimal.

the optimal and suboptimal values on each tested data are highlighted in bold and italic, respectively. the averaged performance of the proposed method with hknn classifier is suboptimal to bmsm-svm by a neglectable difference. besides, the averaged performance of lhr, coupling with five classifiers show a dramatically smaller variance  than other bmsm does , thus implying a high capability of stability with respect to classification models.

for the individual dataset, lhr outperformed or achieved comparable performance to the best result reported in the literature. for the cns data, the lhr-svm, lhr-lda and lhr-hknn achieved superior performances with almost 100% accuracy, which is much higher than the second best performance by k-tsp  <cit> . for the colon data, although the accuracy of the lhr-based classifier is worse than that of bmsf-svm, ivga-svm and logo, the accuracy of all the five classifiers are similar. this implies that the selected genes are very robust to the choice of different classifiers. similar results are observed on the dlbcl, prostate <dig> and prostate <dig> datasets. for the gcm, leukemia, lung and prostate <dig> datasets, the lhr-based classifier was ranked either first or second. the selected genes tested by the five classifiers show similar performance on the leukemia, lung and prostate <dig> datasets. for the prostate <dig> data, bmsf-svm realized remarkably good accuracy, although the results using the other three classifiers with bmsf feature selection are less impressive. logo also performed nicely, yet the average is suboptimal to lhg. in comparison, the performance using lhr feature selection is fairly stable. for the prostate <dig> data, logo based classifiers performed very well, while the lhr based ones were slightly less accurate than the top ones. compared with logo in terms of the ability to select informational genes, the proposed algorithm achieved comparable performance by reaching the classification accuracy of  <dig> %, which is slightly less than logo of  <dig> %.

when considering the average accuracy for each algorithm across all cancers datasets, the top four methods with the highest average accuracy are logo-hknn, bmsf-svm, lhr-knn/logo-knn, lhr-svm and lhr-hknn. the proposed scheme has a slightly lower average accuracy than bmsf-svm and logo-hknn, but a higher accuracy than the others. however, the values for mean ± standarddeviation of the averaged accuracy are  <dig> ± <dig>  for lhr,  <dig> ± <dig>  for logo and  <dig> ± <dig>  for bmsf. this shows that the proposed lhr outperforms both logo and bmsf in terms of overall accuracy as well as confirming its excellent stability in terms of the choice of classification method.

comparison with standard feature selection methods
for comparison with other feature selection models, eleven standard techniques were tested as well as the proposed lhr. the selected techniques include t-statistic , twoing rule , information gain , gini index , max minority , sum minority , sum of variances , one-dimensional support vector machine , minimum redundancy maximum relevance   <cit>  and i-relief  <cit> . the code for the first eight schemes is available through rankgene at http://genomics <dig> bu.edu/yangsu/rankgene. the code for mrmr is available at http://penglab.janelia.org/proj/mrmr/, where two implementations of mrmr: namely, mid and miq, are provided. the i-relief package is available at http://plaza.ufl.edu/sunyijun/ <cit> .

it has been suggested by the author in  <cit>  that accurate discretization could improve the performance of mrmr. the author also reported consistent results when the expression values are transformed into  <dig> or  <dig> states using μ±kσ with k ranging from  <dig>  to  <dig>  and where μ and σ are gene specific mean and standard deviation, respectively . in our experiments, we followed the transformation rule suggested in  <cit>  to simplify the comparison. expression values greater than μ+σ were set to 1; values between μ-σ and μ+σ were set to 0; and values less than μ-σ were set to - <dig> 

in each experiment, a feature selection scheme was first used to select the informative genes, followed by classification tests on the truncated dataset. for subjective comparison, we set the number of informative genes for the selected feature selection scheme to be the same as that determined by lhr, which usually finds a relatively small number of genes . this allowed us to examine whether the limited number of informative genes generated by lhr had more discriminative power than those generated by the other methods.

the loocv accuracy for each of the five classification algorithms  is reported in table  <dig>  the number of genes selected by lhr is listed in the second column and the same number is used to create the truncated data for the other feature selection schemes. in most cases, the variables selected by lhr achieved the optimal or suboptimal loocv accuracy when coupled with the five classifiers. to investigate the extent of the information conveyed by the selected genes, we created a box plot of the loocv accuracy for the five classification algorithms  on each of the tested datasets in figure  <dig>  a remarkable characteristics of the proposed lhr is its low dependence on the classifiers, resulting in the corresponding box plot having a narrower bandwidth than that for the other methods, shown in figure  <dig>  this property implies that the genes selected by lhr are highly informative, and thus the discriminative performance is robust to the choice of different classifiers.

ξpreprocessing of the data via t-test with confidence leel of  <dig>  to reduce the computation burden on estimating of mutual information.

ηhyper-parameters are estimated via 5-fold cross validation.

the number of genes is determined by lhr and used for all other fsss. loocv criteria is used to evaluate the performance of the fsss, coupling with five classification models. the optimal and suboptimal accuracy  on each tested data are highlighted in bold and italic, respectively.

computation complexity
solving of the lhr algorithm involves in a quadratic minimization problem ) for each sample. therefore, it needs a much higher computational cost than linear method does, such as i-relief and logo. although the matrix of htwh in eq.  is positive-definite and in small size, the minimization problem of eq.  can be solved in polynomial time  for n nns of a sample). thus, the complexity in each iteration are approximately o times higher than i-relief does.

CONCLUSIONS
in this paper, we proposed a new feature weighting scheme to overcome the common drawbacks of the relief family. the nearest miss and hit subsets are approximated by constructing a local hyperplane. then feature weight updating is achieved by measuring the margin between the sample and its hyperplane in a general relief framework. the main contribution of the new variation is that the margin is more robust to the noise and outliers than those of earlier works. therefore, the feature weights can characterize the local structure more accurately. experimental results on both synthetic and real-world microarray datasets validated our findings when combining the proposed method with five classic classifiers. the performance of the proposed weighting scheme performed is superior in terms of classification error on most test datasets. extensive experiments demonstrated that the proposed scheme has three remarkable characteristics: 1) high accuracy in classification, 2) excellent robustness to noise and 3) good stability with respect to various classification algorithms.

