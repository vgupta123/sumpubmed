BACKGROUND
cancer is known as a category of disease causing abnormal cell growths or tumors that potentially invade or metastasize to other parts of human body  <cit> . it has long become one of the major lethal diseases which leads to about  <dig>  million, or  <dig> %, of all human deaths each year  <cit> .

to alleviate the impact of cancer to human health, considerable research endeavors have been devoted to the related diagnosis and therapy techniques, among which somatic point mutation based cancer classification  is an important perspective. the purpose of smcc is to detect the cancer types or subtypes based on somatic gene mutations from the patient, so that the cancer condition of the patient can be specified. due to the drop in the cost of dna sequencing in recent years, the availability of dna sequencing data has increased dramatically, which greatly promotes the developments of smcc  <cit> . compared with conventional cancer classification methods that are mostly based on morphological appearances or gene expressions of the tumor, smcc is particularly effective in differentiating tumors with similar histopathological appearances  <cit>  and is significantly more robust to environmental influences, thus is favorable in delivering more accurate classification results. other genetic aberrations such as copy number variance, translocation, and small insertion or deletion have also been shown to be associated with different cancers  <cit> , but due to the major causal role of somatic point mutations and potential application consideration, we only focus on this kind of genetic aberration in this study. moreover, the combinatorial point mutation patterns learned in predicting cancer types/subtypes can be used for developing diagnostic gene marker panels that are cost effective. this is particularly true , when compared to dna amplifications and rearrangements which usually require whole genome sequencing and is expensive for patients, especially regarding time series and whole genome sequencing used in tracing tumor linage evolution during cancer progression.

clinically, smcc may significantly facilitate cancer-related diagnoses and treatments, such as personalized tumor medicine  <cit> , targeted tumor therapy  <cit>  and compound medicine  <cit> . it can also aid cancer early diagnosis  in combination with the sampling and sequencing of circulating tumor cells  or circulating dna  . given the promising applications above, smcc is widely studied in recent researches .

in recent years, the drastic developments of machine learning methods have greatly facilitated the researches in bioinformatics, including smcc. in order to predict the cancer types/subtypes more effectively, many machine learning approaches have been applied in existing cancer type prediction works, which have shown promising results . currently, remarkable developments have been demonstrated in tumor cases of colorectal  <cit> , breast  <cit> , ovary  <cit> , brain  <cit> , and melanoma  <cit> . however, there are at least three major unresolved challenges:normal sequencing results involve extremely large number of genes, usually in tens of thousands, but only a small discriminatory subset of genes is related to the cancer classification task. the other genes are largely irrelevant genes whose existence will only obstruct the cancer classification. many recent works have been conducted in identifying the discriminatory subset of genes. for example, cho et al.  <cit>  apply the mean and standard deviation of the distances from each sample to the class center as criteria for classification; yang et al.  <cit>  improve the method in  <cit>  and bring inter-class variations into the algorithm; cai et al.  <cit>  propose the clustered gene selection, which groups the genes via k-means clustering and picks up the top genes in each group that are closest to the centroid locations. these methods are simple and effective in some cases, but their heuristics are designed for continuous gene expression data, and are not directly applicable to discrete, and especially binary point mutation data.

even within the discriminatory subset, the majority of genes are not guaranteed to contain informative point mutations and often remain normal   <cit> , which results in extremely sparse gene data  that is difficult to classify. yet, to the best of our knowledge, there has been no existing work specifically devised for reducing the data sparsity for smcc.

different genes related to specific types of cancer are generally correlated and have complex interactions which may impede the application of conventional simple linear classifiers such as linear kernel support vector machine   <cit> . therefore, an advanced classifier being capable of extracting the high level features within the discriminatory subset is desired. although there have been recent works utilizing sparse-coding  <cit>  or auto-encoder  <cit>  for gene annotation, no work has been devoted in applying high-level machine learning approaches to smcc.




in recent years, the developments of deep neural network   <cit>  have equipped bioinformaticians with powerful machine learning tools. dnn is a type of artificial neural network that aims to model abstracted high-level data features using multiple nonlinear and complex processing layers, and provides feedback via back-propagation  <cit> . first introduced in  <dig>  <cit> , dnn has garnered tremendous developments and is widely applied in image classification  <cit> , object localization  <cit> , facial recognition  <cit> , etc. dnn has the potential to introduce novel opportunities for smcc where it perfectly fits the need for large scale data processing and high level feature extraction. however, to the present, applying customized dnn on smcc is yet to be explored.

in this paper, we propose a novel smcc method, named deepgene, designed to simultaneously address the three identified issues. deepgene is a dnn-based classification model composed of three steps. it first conducts two pre-processing techniques, including the clustered gene filtering  based on mutation occurrence frequency, and the indexed sparsity reduction  based on indexes of non-zero elements; the gene data is then classified by a fully-connected dnn classifier into a specific cancer type. the proposed deepgene model has four distinct contributions:the proposed cgf procedure locates the discriminatory gene subset based on mutation occurrence frequency. cgf utilizes features from the whole dataset instead of the current sample alone , and thus more objectively reflects the correlations among the genes which can more effectively summarize the discriminatory subset. in addition, cgf does not require any prior knowledge from the original data and therefore functions well on both discrete and binary point mutation data.

the proposed isr procedure converts the sparse gene data into indexes of its non-zero elements. isr eliminates the vast majority of zero gene elements, and significantly reduces the complexity of the gene data during such process.

we establish a fully connected dnn classifier that uses the gene data after cgf and isr for cancer classification. with the capacity of high-level feature extraction, our classifier is able to effectively extract deep features from the complexly correlated gene data, and significantly improve the classification accuracy compared with conventional simple linear classifiers such as svm.

we compile and release the tcga-deepgene dataset, which is a reformulated subset of the widely applied tcga dataset  <cit>  in genome-related researches. tcga-deepgene selects  <dig>  genes of  <dig> types of cancer from  <dig> different samples, and regularizes the data in a unified format so that classification tasks can be readily performed.




the flowchart of deepgene is shown in fig.  <dig>  we conduct experiments on the proposed tcga-deepgene dataset, and deepgene is evaluated against three widely adopted classification methods for smcc. the results demonstrate that deepgene has generated significantly higher performance in terms of testing accuracy against the comparison methods.fig.  <dig> flowchart of the proposed deepgene method. the raw gene data is first pre-processed by the clustered gene filtering  and the indexed sparsity reduction , respectively, and then fed into the dnn classifier. the output label from the dnn indicates the cancer type of the input gene sample




methods
deepgene has three major steps, namely clustered gene filtering , indexed sparsity reduction , and dnn-based classification. the cgf and isr are two independent pre-processing modules, the results of which are then concatenated in the final dnn classifier.

clustered gene filtering
the cgf step is based on the mutation occurrence frequency of the gene data, and its workflow is summarized in table  <dig>  let a ∈ { <dig>  1}m × n be the matrix of raw data with binary value, where the n columns correspond to the n samples  in the dataset, and the m rows correspond to the m genes per sample. the binary value indicates whether a mutation is observed:table  <dig> workflow of clustered gene filtering 


input: gene data matrix a ∈ { <dig>  1}m × n, distance threshold d
cgf, group element threshold n
cgf.
1: sum a by row and sort the result in descent order, and then obtain the sorted index a
sum*;
2: initialize each element as ungrouped;
3: for each ungrouped element i in a
sum*:
  for each ungrouped element j in a
sum* other than i:
  i. calculate the similarity d, a);
  ii. if d, a) > d
cgf, assign j into the group of i;
4: set the output gene index set g
out = ∅;
5: for each group c of a after step 3:
  if group element number n
c ≥ n
cgf, select the top n
cgf genes with the highest mutation occurrence frequency as g
c;
  g
out = g
out ∪ g
c;
6: apply the index set g
out on a and get the filtered gene data a
cgf = a;

output: a

 aij=10ifmutationobserevedatgeneiofsamplejotherwise. 


we first sum a by row, and concatenate the result with the row indexes for later reference : asum=1⋮msuma,axis=row. 


since the genes with higher occurrence frequency are of more interest, the rows of a
sum are sorted in descending order by the second column as a
sum*. after that, we only keep its index column: asum*=asum*: <dig>  


the next step is to group a
sum* by inter-gene similarity . for two 1 × n gene samples p and q, we use the jaccard coefficient as their inter-sample similarity d: dpq=sump&qsump|q, 


where “&” and “|” stand for logical and and or.

starting from a
sum*, which stands for the index of the gene with the highest occurrence frequency, we calculate its similarity with each of the following genes. if their similarity is larger than a predefined threshold d
cgf, the latter gene is merged into the group of a
sum*. after the loop for a
sum*, we conduct the loop for the next ungrouped element in a
sum*, until all the genes are grouped with a unique group id.

the final step is to filter the elements from each group and form the discriminatory subset. we do this by selecting the top n
cgf genes in each group with the highest mutation occurrence frequency, where n
cgf is another predefined threshold. groups that have fewer than n
cgf elements are discarded. all of the selected genes are then united as the result of cgf .

indexed sparsity reduction
although the cgf can effectively locate the discriminatory gene subset and filter out the majority of irrelevant genes, it is still probable that the selected gene subset being highly sparse, i.e. most of the elements in a
cgf are zeros. the high sparsity is likely to obscure any distinguishable feature in the gene data and severely hinder the classification. hence, an effective process in reducing the gene data sparsity is highly desired.

to address the data sparsity issue, we propose the indexed sparsity reduction  procedure, which minifies the sparsity by converting the gene data into the indexes of its non-zero genes. for a 1 × n gene sample p ∈ { <dig>  1}1 × n, let the number of its non-zero element be n
nz. we set a pre-defined threshold n
isr. if n
nz ≥ n
isr, find the indexes of its top n
isr non-zero elements that have the highest occurrence frequency in a
sum* of the previous section, and these n
isr indexes are listed in ascending order as a vector p
isr, which is the output of isr; if n
nz < n
isr, we conduct zero-padding to the tail of p
isr to make it has the length of n
isr. the workflow of isr is illustrated in fig.  <dig> fig.  <dig> flowchart of the indexed sparsity reduction  step. after indexing of the non-zero elements, if n
nz ≥ n
isr, select the top n
isr non-zero elements that have the highest occurrence frequency; if n
nz < n
isr, we conduct zero-padding to the tail of the output data so that it has the length of n
isr





the significance of isr is apparent. for each gene sample p, isr filters out the majority of its zero elements and leaves most  or all  of its non-zero elements. since n
isr ≪ length, the percentage of zero elements will drop dramatically after isr, which means the impact of data sparsity will be significantly suppressed.

dnn-based classifier
as introduced in the previous two sections, both cgf and isr have their own advantages when conducted alone. however, the performance can be even higher if they are combined together . we thus combine both cgf and isr as the pre-processing for our dnn-based classifier.

as shown in fig.  <dig>  the raw gene data is processed by cgf and isr, separately, and then concatenated as the input of the dnn classifier. the concatenation is conducted by appending the output of isr to the tail of the output of cgf, by which the two outputs form a new and longer data vector. the classifier is a feed-forward artificial neural network with fixed input and output size, and multiple hidden layers for data processing. for a hidden layer l, its activation  is computed as: xl=fzl− <dig>  


where f is the activation function, z
l is the total weighted sum of the input: zl=wlxl+bl, 


where w
l and b
l are the weight matrix and bias vector of layer l . in our case, we adopt the relu  <cit>  function as f, and x
 <dig> is the input gene data after pre-processing. the size of the last layer l’s output x
l equals to the number of cancer types n
cancer . x
l is then processed by a softmax layer  <cit> , and the loss j is computed by the logarithm loss function: j=−∑i=1ncanceryilogpi, 


where y
i ∈ { <dig>  1} is the ground truth label of cancer type i, and pi=xli∑jexpxlj 


is the softmax probability of cancer type i.

in training, the loss j is transferred from the last layer to the former layers via back-propagation  <cit> , by which the parameters w and b of each layer are updated. the training then enters the next epoch, and the feed-forwarding and back-propagation are conducted again. the training stops when a pre-defined epoch number is reached. in testing, only the feed-forwarding is conducted  for a testing sample, and the type of cancer i corresponding to the largest softmax probability of p
i is adopted as the classification result. the workflow of the dnn classifier is summarized in table  <dig>  and the complete flowchart of deepgene is illustrated in fig.  <dig> table  <dig> workflow of dnn classifier


input: gene data matrix a ∈ { <dig>  1}m × n after cgf and isr, where rows and columns correspond to samples and genes, respectively; max training epoch e
max.
1: training: for each training epoch e ≤ e
max:
  for each sample a
i = a:
  i. conduct feed-forwarding and compute the loss j;
  ii. conduct back-propagation to update the w and b

2: testing: for each sample a
i = a:
  conduct feed-forwarding and get softmax probability p;
  adopt the cancer type correspond to max as the result of a
i.




RESULTS
experiment setup
dataset
our experiments are all conducted on the newly proposed tcga-deepgene dataset, which is a re-formulated subset of the cancer genome atlas  dataset  <cit>  that is widely applied in genomic researches.

the tcga-deepgene subset is formulated by assembling the genes that contain somatic point mutation on each of the  <dig> selected types of cancer. detailed sample and point mutation statistics for each cancer type can be found in table  <dig>  the data is collected from the tcga database with filter criteria illuminaga_dnaseq_curated updated before april,  <dig>  the mutation information for a gene is represented by a binary value according to one or more mutation  or without mutation  on that gene for a specific sample. we assemble a total of  <dig>  genes from the  <dig> samples, and generate a  <dig>  834 ×  <dig>   <dig> binary data matrix . this data matrix is the product of our proposed tcga-deepgene subset, where each sample  is assigned one of the labels { <dig>   <dig>  …, 12} meaning the  <dig> types of cancer above.table  <dig> sample and mutation statistics of the tcga-deepgene dataset on  <dig> cancer types




to facilitate the 10-fold cross validation in the following experiments, we randomly divide the samples in each of the  <dig> cancer categories into  <dig> subgroups, and each time we union one subgroup from each cancer category as the validation set, while all the other subgroups are combined as the training set. this formulates  <dig> training/validation configurations with fair distributions of the  <dig> types of cancer, and will be used for the 10-fold cross validation in our following experiments.

constant parameters
for the proposed dnn classifier, the output size is set to  <dig> ; the total training epoch e
max is set to 50; the learning rate is set to 50-point logarithm space between 10−  <dig> and 10− 4; the weight decay is set to  <dig> ; and the training batch size  is set to  <dig> 

additionally, in order to facilitate the evaluation of variable parameters, we set each parameter a default value: the distance threshold is set to  <dig> ; the group element threshold n
cgf is set to 5; the non-zero element threshold n
isr is set to 800; the hidden layer number and parameters per layer of the dnn classifier are set to  <dig> and  <dig>  respectively.

evaluation metrics
for all the evaluations in our experiments, we randomly select 90%  samples for training, and the rest 10%  samples for testing. in parameter optimization steps for deepgene, we adopt the 10-fold cross validation accuracy on the training set as the evaluation metric; on the other hand, in the comparison with widely adopted models, we adopt the testing accuracy as the evaluation metric.

implementation
the cgf and isr steps are implemented by original coding in matlab, while the dnn classifier is implemented on the matconvnet toolbox  <cit> , which is a matlab-based convolutional neural network  toolbox with various extensibilities.

evaluation of design options
determination of cgf’s variable
there are two variables that need to be experimentally determined for the cgf step, namely the distance threshold d
cgf and the group element threshold n
cgf. to determine the two variables, we change them in 2-dimensional manner, while keeping all the other variables the default values as described in the “constant parameters” section. the corresponding 10-fold cross validation accuracies are listed in table  <dig>  and the corresponding 3d bar-plot to present sensitivity is shown in fig. 3a. we adopt d
cgf =  <dig>  and n
cgf =  <dig> for the following experiments based on the observed experimental results, since they contribute to the optimal performance.table  <dig> 10-fold cross validation accuracies  of deepgene with different n
cgf  and d
cgf 

the optimal result is marked in red. mean accuracy:  <dig> %; standard deviation:  <dig> %; maximum accuracy:  <dig> %; minimum accuracy:  <dig> %. the corresponding 3d bar-plot is shown in fig. 3a for sensitivity review


fig.  <dig> 3d bar-plots of parameter estimations for sensitivity review. the z-axis stands for 10-fold cross validation accuracy. a parameter estimation for d
cgf and n
cgf, corresponding to table 4; b parameter estimation for layer number and parameter number per layer for the dnn classifier, corresponding to table 5; c parameter estimation for cost and gamma for svm, corresponding to table 6; d parameter estimation for table 7





determination of isr’s variable
the non-zero element threshold n
isr needs to be experimentally determined for the isr step. we monitor the number of non-zero elements for each sample in the dataset, and plot the corresponding histogram in fig.  <dig>  it is seen that  <dig>  of the  <dig> samples have less than  <dig> non-zero genes among the total  <dig>  genes. we thus adopt n
isr =  <dig>  which not only concentrates the data to the non-zero elements, but also greatly shrinks the data length.fig.  <dig> non-zero element distribution of the gene samples in the tcga-deepgene dataset. ninety-seven percent of all the  <dig> samples have no more than  <dig> non-zero gene elements




determine the network architecture
we also need to determine the network architecture for the dnn classifier, which involves two variables: the hidden layer number  and the parameter number per layer . enlightened by  <cit> , we monitor the classifier’s 10-fold cross validation accuracy with various hidden layer numbers and parameter numbers, the results of which are listed in table  <dig>  and the corresponding 3d bar-plot to present sensitivity is shown in fig. 3b. we see that the performance reaches optimal at #layer =  <dig> and #param =  <dig>  these values are thus adopted in our following experiments.table  <dig> 10-fold cross validation accuracies  of deepgene with different #layer  and #param 

the optimal result is marked in red. mean accuracy:  <dig> %; standard deviation:  <dig> %; maximum accuracy:  <dig> %; minimum accuracy:  <dig> %. the corresponding 3d bar-plot is shown in fig. 3b for sensitivity review




evaluate the effect of combining cgf and isr
after determining the related parameters for the three steps of deepgene, we evaluate the impact of our two major innovations, i.e. cgf and isr. it is mentionable that we conduct cgf and isr separately and concatenate their results  instead of conducting them consecutively. the reason is that the outputs of cgf and isr are binary data and index data, respectively. consecutive conduction will only leave the index data , while separate conduction can benefit from both the binary data and the index data, thus introduces less bias.

based on fig.  <dig>  we compare the performances of the dnn classifier with different configurations:cgf and isr ;

only cgf ;

only isr ;

neither cgf nor isr .




the 10-fold cross validation results are shown in fig.  <dig>  it is clearly observed that the complete cgf + isr outperforms both cgf and isr when conducted alone, and also significantly outperforms the raw data without any pre-processing.fig.  <dig> 10-fold cross validation accuracy of deepgene with different design options. performance comparison of the complete deepgene input structure , cgf only, isr only and raw gene data. the complete deepgene shows significant advantage against the other three options




comparison with widely adopted models
we then select three most representative data classifiers that are commonly used in smcc as comparison methods, namely support vector machine   <cit> , k-nearest neighbors   <cit>  and naïve bayes   <cit> . in order to exhibit the pre-processing effect of cgf and isr, all the comparison methods use raw gene data as inputs. the three methods are set up as below.


svm: we use the libsvm toolbox  <cit>  in implementing the svm. based on the results of a previous work for gene classification  <cit> , the kernel type  is set to  <dig> . note that due to the feature set is high dimensional, the linear kernel is suggested over the rbf  kernel  <cit> ; this suggestion is consistent to our trial and error experience on this problem. a 10-fold cross validation is conducted to optimize the parameters cost  and gamma , and the other parameters are set as their default values. the cross validation results are shown in table  <dig>  and the corresponding 3d bar-plot to present sensitivity is shown in fig. 3c. we adopt 22 =  <dig> and 2‐ 5 =  <dig>  for -c and -g, respectively, which lead to the best results in table  <dig> table  <dig> 10-fold cross validation accuracy  of svm with different cost  and gamma  parameters

the optimal result is marked in red. mean accuracy:  <dig> %; standard deviation:  <dig> %; maximum accuracy:  <dig> %; minimum accuracy:  <dig> %. the corresponding 3d bar-plot is shown in fig. 3c for sensitivity review





knn: we compare the performances of euclidean distance and pearson correlation coefficient, which are the two most commonly used similarity measures in gene data analysis  <cit> . the 10-fold cross validation results of the two similarity measures with different neighborhood numbers are shown in table  <dig>  and the corresponding 3d bar-plot to present sensitivity is shown in fig. 3d. we adopt the pearson correlation coefficient and set the neighborhood number to  <dig>  which lead to the optimal validation accuracy.table  <dig> 10-fold cross validation accuracies  of knn with different similarity measures  and neighborhood numbers 

the optimal result is marked in red. mean accuracy:  <dig> %; standard deviation:  <dig> %; maximum accuracy:  <dig> %; minimum accuracy:  <dig> %. the corresponding 3d bar-plot is shown in fig. 3d for sensitivity review





nb: following  <cit> , the average percentage of non-zero elements in the samples of each cancer category is set as the prior probability.

in the performance comparison between different models, the testing accuracy is adopted as the evaluation metric , which is generally slightly lower than the 10-fold validation accuracy of the corresponding model. the experiment results are plotted in fig.  <dig>  deepgene shows significant advantage against all the three comparison methods. the performance improvements are  <dig> % ,  <dig> %  and 710%  against svm, knn and nb, respectively. to further validate the performance of the dnn classifier itself without cgf and isr, we also record the accuracy of the dnn classifier with raw gene data, which is the same input as the comparison methods. the results are shown in fig.  <dig>  in which the dnn classifier still has the optimal accuracy  against all of the comparison methods.fig.  <dig> testing accuracy of deepgene against three widely adopted classifiers. deepgene is clearly advantageous to the comparison methods


fig.  <dig> testing accuracy of deepgene against three widely adopted classifiers with raw gene input data. all methods use raw gene data as input. the dnn classifier is still favorable against the other methods




discussion
clustered gene filtering
the main purpose of the cgf step is to filter out irrelevant genes in the samples and locate the candidate discriminatory gene subset. it first groups the genes based on popularity  and inter-sample similarity, and then selects the top genes in each group, and finally unites all the genes selected as the output.

the two required parameters, d
cgf and n
cgf, are experimentally determined . the two adopted values, d
cgf =  <dig>  and n
cgf =  <dig>  are in the midstream of the evaluation ranges, which are more reliable than the marginal values.

by comparing the performances of the cgf against raw gene data, as the second and fourth bars in fig.  <dig> indicate, the cgf has exhibited significant performance boosting. it raises the validation accuracy by  <dig> % , and also contributes to the high performance of the combined cgf + isr input structure. the advantage of cgf lies in its ability to mask out the majority of irrelevant genes, thus maximally suppress their negative influence, and only focus the data to the discriminatory gene subset.

indexed sparsity reduction
the isr step is meant to reduce the data sparsity by converting the gene data into the indexes of its non-zero elements. in that case only the non-zero elements’ information is left, while all the zero elements are discarded. the data sparsity will thus be tremendously reduced, making the subsequent classifier only focus on the informative non-zero elements.

the required parameter n
isr is experimentally determined. we monitor the non-zero element distribution among all of the  <dig> samples in the tcga-deepgene dataset, and record the non-zero element range of each sample. figure  <dig> indicates that 97% of the samples have no more than  <dig> non-zero elements . we thus set n
isr =  <dig>  which is able to reduce the majority of the data sparsity while maximally reserving the discriminatory information of the samples.

like cgf, isr has exhibited significant contribution in improving the performance of our classifier, as the third bar in fig.  <dig> indicates. it raises the accuracy against raw gene data by  <dig> % , which is even more significant than what the cgf contributes. we attribute isr’s advantage to its remarkable reduction of the gene data sparsity. it is also mentionable that isr exhibits more strength when combined with cgf, as the first bar in fig.  <dig> indicates. this can be explained by the synergy effect of binary gene data and indexed gene data.

furthermore, we note that isr conducts lossless conversion when n
nz ≤ n
isr, i.e. the indexed data can be readily converted back to the original binary data if necessary.

data optimization by cgf and isr
besides aiding our deepgene method, the cgf and isr steps can also benefit other classification methods for input data optimization. to evaluate the optimization effect, we apply cgf + isr to the three classifiers svm, knn and nb discussed in the “comparison with widely adopted models” section, and record their testing accuracies before and after the input data optimization. for fair comparison, the parameters of the classifiers remain the same.

figure  <dig> shows the accuracy change before and after the input data optimization of cgf + isr. it is observed that applying cgf + isr can notably refine the input data, thus improve the testing accuracies of the classifiers. we also note that by applying cgf + isr, the accuracy improvements of the three classifiers are not as large as that of deepgene. since deepgene is based on dnn, it is more advantageous in processing complicated data structures, thus can benefit from cgf + isr more.fig.  <dig> testing accuracies of three widely adopted classifiers with and without cgf + isr for input data optimization. applying cgf + isr can notably refine the input data, thus improve the testing accuracies of the classifiers




dnn classifier
the dnn classifier is the mainstay of deepgene, which conducts the classification and generates the final output. figure  <dig> has shown the significant advantage of deepgene against three widely adopted classifiers, among which deepgene exhibits at least 24% of performance improvement. to examine the performance of the dnn classifier itself without the pre-processing steps of cgf and isr, we also record the accuracy of the dnn classifier with raw gene data in fig.  <dig>  which has shown that the dnn classifier still generates the best accuracy .

to further validate that the 10-fold validation accuracy of dnn is indeed higher than that of svm, we assume that these two classifiers are independent of each other, and conduct t-test with the null hypothesis that these two classifiers have equal validation accuracy under the significance of  <dig> . the sample standard deviation of dnn and svm are recorded as sx1= <dig> %= <dig>  and sx2= <dig> %= <dig> , respectively. the t statistic is then calculated as: t=x¯1−x¯2sx1x2⋅2n= <dig> − <dig> .387e−4×210= <dig> , 


where sx1x2=sx12+sx222= <dig> e− <dig>  


here, the degree of freedom is n − 1 =  <dig>  by checking the one-tailed significance table, the corresponding t statistic of the p-value  <dig>  is  <dig> , which is far less than our t =  <dig> . hence the null hypothesis is rejected in favor of the alternative hypothesis, and we prove that the 10-fold cross validation accuracy of dnn is indeed higher than that of svm. it is notable that using the dnn alone is the lowest configuration of deepgene , and svm has the highest performance out of the three comparison classifiers. as a result, our t-test above has also proved that deepgene is indeed higher in performance against all of the three comparison methods.

we attribute the advantage of the dnn classifier to its capacity in extracting the complex features of the input data. the multiple nonlinear processing layers make the dnn especially suitable in processing complex data that are too tough for conventional linear classifiers such as linear kernel svm. we also note that deepgene is just one of our initial trials for dnn-based gene data processing, but has already shown promising results against widely adopted methods. the dnn classifier has the potential to show greater advantages towards more complex  and large-scale data to conventional classifiers, which will be discussed in our future works.

limitation and future study
currently deepgene is only tested on datasets of somatic point mutations with known cancer types, i.e., the histological biopsy sites are already known. therefore, in this study, deepgene only demonstrates the power of capturing complex association between somatic point mutation and cancer types, and more of its application potentials will be evidenced by tumor samples with completely unknown cancer type information  in our future works. the association between point mutation and other genetic aberrations such as copy number variance, translocation, and small insertion and deletion will also be covered in our future works. it will be proved that to a large extent, adopting point mutation alone is good enough for cancer type or subtype classification.

CONCLUSIONS
in this paper, we propose the deepgene method for somatic point mutation based cancer type classification. deepgene consists of three major steps. the cgf step concentrates the gene data with mutation occurrence frequency; the isr step reduces the gene data sparsity with the indexes of non-zero elements; and in the final step, the dnn-based classifier takes the processed data and generates the classification result with high-level data feature learning.

we conduct experiments on the compiled tcga-deepgene dataset, which is a reformulated subset of the tcga dataset with mutations on  <dig> types of cancer. controlled variable experiments indicate that cgf, isr and dnn classifier all have significant contribution in improving the classification accuracy. we then compare deepgene with three widely adopted data classifiers, the results of which exhibit the remarkable advantages of deepgene, which has achieved > 24% of performance improvement in terms of testing accuracy against the comparison classification methods.

we demonstrated the advantages and potentials of the deepgene model for somatic point mutation based gene data processing, and we suggest that the model can be extended and transferred to other complex genotype-phenotype association studies, which we believe will benefit many related areas. as for future studies, we will refine our model for other complex and large-scale data, as well as broadening our training dataset, so that the classification result can be further improved.

