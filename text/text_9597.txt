BACKGROUND
in mass spectrometry based proteomics, whether bottom-up, top-down, or middle-down  <cit> , the matching of a single tandem mass spectrum, or a spectral tree  <cit>  to a peptide is an integral part of most methods for identifying peptides and proteins. existing methods fall into one of three broad categories: sequence database searches  <cit> , spectral libraries  and de novo sequencing  <cit> . most recent methods can be applied to data from collision-induced dissociation  <cit> , electron capture dissociation  <cit>  or other fragmentation techniques, individually or in combination  <cit> . the identification may be based on ms <dig>  ms <dig> or a combination of these. several groups have also published efforts in combining multiple algorithms for peptide-spectrum matching, for instance the framework developed by searle et al.  <cit> , the msblender software from kwon et al.  <cit>  or the fdranalysis algorithm of wedge et al.  <cit> . recently, in de bruin et al.  <cit>  and mohammed et al.  <cit>  we have shown how some of these algorithms can be integrated with other algorithms in scientific workflows  <cit> . scientific workflows enable researchers to concentrate on their research purpose rather than on computational challenges. however, all these algorithms use a number of user-defined input parameters, such as the specificity and fidelity of the enzymatic digestion, the sequences or library to search spectra against, mass measurement uncertainty or error  and score or probability thresholds in the assembly of peptide-spectrum matches to peptide or protein sets. typically, the choice of algorithm and parameters is determined from the users’ experience and expert knowledge about the experiment, instrumentation and data quality. previously, piehowski et al. have used a “systematic trial-and-error parameter selection” to optimize peptide identification using sequest  <cit> , showing significant improvement over using default search parameters. here we describe the usage of a framework  <cit>  for automated optimization of scientific workflows with two very different analysis tasks: peptide-spectrum matching and chromatographic retention time prediction. the optimization process can be reproduced by other researchers with the same or a different target and workflows. one must ensure to install all required applications, taverna and the optimization plugin as described at http://ms-utils.org/taverna_optimization.pdf.

methods
test samples and sequences
in this study, we used six representative datasets from two different organisms and three different types of mass analyzers. three datasets were generated in our own lab and three fetched from the pride repository  <cit> . as a prokaryote with a small genome and limited number of modified peptides, we used an e. coli whole-cell lysate, prepared as described by mostovenko et al.  <cit> . this sample was analyzed both by high-resolution tof mass spectrometer and in an ion trap. as a eukaryote with a larger genome and frequent occurrence of modified peptides, we used a sample of human plasma isolated from blood drawn from a self-declared healthy individual after verbal informed consent according to local guidelines approved by the medical ethics committee at the leiden university medical center. the human plasma sample was analyzed on the same ion trap as the e. coli digest. the three additional datasets were downloaded from pride were an orbitrap dataset from a study of label-free absolute proteome quantification methods using e. coli  <cit>  , an orbitrap dataset from glioma-derived cancer stem cells  <cit>   and a tof dataset of human induced pluripotent stem cells  <cit>  . these datasets cover three common types of mass analyzers with varying resolving power and mass measurement accuracy as well as organisms with small and large genomes. uniprot reference proteomes data for e. coli  and h. sapiens  was used for peptide identification using the x!tandem  <cit>  sequence search engine.

liquid chromatography – tandem mass spectrometry
the ion trap only datasets were generated as follows. two μl of each tryptic digest were loaded and desalted on a 300 μm-i.d. 5-mm pepmap c <dig> trap column  and separated by reversed-phase liquid chromatography using a 15-cm, 300 μm-i.d. chromxp c <dig> column  connected to a splitless nanolc-ultra 2d plus system  with a linear 90-min gradient from  <dig> to 33 % acetonitrile in  <dig>  % formic acid and a constant flow rate of 4 μl/min. the lc system was coupled to an amazon etd ion trap  via a captivespray™ esi source. after each ms scan, up to  <dig> abundant multiply charged species in m/z 300- <dig> were selected for ms/ms and excluded for one minute after having been selected twice for ms/ms. each individual scan or tandem mass spectrum was saved to disk. the lc system was controlled by hystar  <dig>  and the ion trap by trapcontrol  <dig> . to generate a hybrid tof/ion trap dataset, the e. coli digest was loaded and desalted as above, separated on a 15-cm, 75 μm-i.d pepmap c <dig> column in an ultimate  <dig> lc system  with a 180-min 300 nl/min piece-wise linear gradient with the following breakpoints: 2 % b at  <dig> and 10 min, 5 % b at 25 min, 25 % b at 165 min, 30 % b at 175 min and 35 % b at 190 min, where b is 95 % acetonitrile and  <dig>  % formic acid. the lc system was coupled simultaneously to a maxis high-resolution-tof  and an amazon speed ion trap using a post-column flow splitter , both with the captivespray™ esi source.

optimization of the x!tandem workflow
scientific workflows are becoming more common in large-scale proteomics data analysis  <cit> . some of the authors already designed parts of the current use case as scientific workflows within the taverna workflow management system. these workflows included the decomposition of mass spectrometry data and peptide identification via x!tandem or spectrast  <cit> . the workflows were made highly parallel for an optimal execution in a cloud environment  <cit> . we extended the x!tandem workflow and shifted the computationally intensive x!tandem execution to the grid using the taverna unicore plugin  <cit> . the x!tandem workflow is highlighted in fig.  <dig>  representing the following major steps:  <dig>  decomposing the input files,  <dig>  database search by x!tandem,  <dig>  recomposing the output files  <dig>  statistical analysis of the result by peptideprophet and  <dig>  modeling of chromatographic retention times using an optimum choice of training set and prediction algorithm. this workflow can be used for conventional execution and for the optimization procedure in taverna. the workflow in fig.  <dig> is shown from the optimization perspective, which integrates graphical user interface elements for the optimization specification and run. the perspective is provided by the optimization framework  <cit>  and partly more specifically detailed by the respective optimization method plugin. figure  <dig> also illustrates that the workflow does not need any modification for the usage in the optimization framework. the optimization perspective offers different panels that enable the user to:  <dig>  define the sub-workflow for optimization,  <dig>  set optimization-specific values,  <dig>  determine required input parameters, such as mmes, and the optimization target  and  <dig>  specify parameter data types, ranges and dependencies . using all these optimization specification parameter, expert knowledge can be taken into account during the optimization procedure to limit the search space from the outset. depending on the runtime of one workflow execution and the total runtime of a complete optimization, such limits are often required to make optimization feasible. additionally, some parameter combinations might be obviously useless, and should be omitted. for the optimization of x!tandem and peptideprophet only the highlighted workflow is used.fig.  <dig> the complete peptide matching and retention time workflow within the optimization perspective. we optimized the workflow in two stages. the figure above shows the optimization of the x!tandem peptide identification with following major steps:  <dig>  decomposing the input files,  <dig>  database search by x!tandem,  <dig>  recomposing the output files  <dig>  statistical analysis of the result by peptideprophet and  <dig>  modeling of chromatographic retention times using an optimum choice of training set and prediction algorithm. the parts of the workflow, here the retention time prediction, are made partially transparent as they were not part of the optimization within this stage. the bottom left window also shows the dependency settings for the two input parameter mme+ and mme



it is reasonable to assume the optimum mme tolerance for peptide-spectrum matching has some relationship with the mme itself . an mme tolerance window narrower than the mme distribution will discard many potential peptide-spectrum matches , while a much broader window will fail to take advantage of high mass measurement accuracy. the discrimination between correct  and incorrect  peptide-spectrum matches decreases the more sequences a spectrum is matched against, as the score of the best matching random  peptide increases with the number of sequences searched. in the workflow optimizations here we allowed both the maximum positive and negative mmes to vary between  <dig> and 25 da for all test datasets. the mmes are not necessarily symmetric. often, mass spectrometers are not perfectly calibrated, and a small but significant bias can be found after identifying the peptides. in addition, the instrument sometimes selects an isotopic peak other than the monoisotopic, resulting in a systematic error of + <dig> or perhaps +2 da. we therefore allowed the maximum positive and negative mme to be independently varied over the entire range and the “isotope error” in x!tandem. in this optimization process, strict tryptic enzyme specificity was also assumed, allowing for two missed cleavages. carbamidomethylation of cysteines was considered a fixed post-translational modification in addition to the variable modifications included by default, such as n-terminal pyroglutamate from glutamine or glutamic acid. as we used the default k-score with the tpp version of x!tandem, the fragment ion tolerances are not used in the scoring, which is based on a dot product with a fixed bin size.fig.  <dig> 
a mass measurement error distribution  in the e. coli maxis tof-amazon ion trap dataset  and  estimated number of identified correct 2+ spectra as function of mme tolerance in an x!tandem search, using equal positive and negative mme tolerances and allowing for “isotope error” . when the gaps between the windows centered on each isotope  close, the number of 2+ psms gained per unit mme  drops drastically 



in x!tandem and many other search engines, it is possible to define not only a number of allowed missed cleavage sites within a peptide, but also the fidelity of the enzyme. the latter allows for zero or one of the peptide termini not conforming to the enzymatic specificity and are in x!tandem referred to as “full” – strict tryptic cleavage – meaning that both termini have to be the result of tryptic cleavages unless the peptide is from the protein n- or c-terminus, and “semi”, meaning that only one site of the termini has to result from cleavage by trypsin. in software such as mascot, this is not an independent parameter but implemented as a virtual enzyme . in order to fully demonstrate the advantage of the optimization framework, we performed a second optimization on the e. coli ion trap data, starting from the mme tolerance optimum, with two additional parameters included in the optimization process: the number of missed cleavages  and the enzymatic fidelity defined by a boolean representing’full’  or’semi-tryptic’.

there are many methods available for finding the optimum of a given function. one should take care if using a method based on derivatives . for instance, when allowing isotope errors , the derivative of the number of psms as a function of the allowed mme is discontinuous where the sum of the negative and positive error is 1 da . in search engines having only one mme tolerance parameter, i.e. the same positive and negative maximum mme, this happens exactly at  <dig>  da maximum mme. this is easy to understand, as the two or three searched mass windows become one, and the window is expanding further along two edges rather than four or six. there are also a number of discrete variables that can be modified and that influence the peptide-spectrum matching, for example isotope error, missed cleavages, minimum and maximum peptide length, and both fixed and variable post-translational modifications. these parameters are often binary  but can sometimes take on any integer value in a small range . additionally, the choice of search algorithm itself can be subject to optimization. most database search engines have equivalent parameters, such as mme, missed cleavages, peptide size and considered ptms. in order to optimize the described parameters above, we use the taverna workflow optimization framework that employs an evolutionary algorithmto optimize multiple continuous, discrete or binary parameters and find the combination that gives the best global performance according to a user-defined target, or fitness function. here we use as fitness function the number of estimated correct psms from doubly charged precursors given by peptideprophet using decoys and the non-parametric model divided by the total number of tandem mass spectra or the root-mean-square deviation of predicted peptide retention time, as these are robust and easily calculated metrics. we use the psms as they are closer to the data and better represent discrete units of information in a bottom-up proteomics experiment – in quantitation by spectral counting for example – than the perhaps biologically more relevant number of unique peptides or proteome coverage. however, there is no reason to assume that optimizing for the number of psms would not also provide good parameters for unique peptides and proteins.

the taverna optimization framework used in this paper offers a generic application programming interface to extend taverna with various types of optimization as well as optimization algorithms. for non-linear and partially discrete problems such as algorithms and simulations used in scientific workflows, the fitness landscape may be rugged and not assessable in many places. properly dealing with these issues requires a robust and versatile method, such as metaheuristic optimization. the intrinsic parallelizability of such methods is a major advantage in large optimization problems such as those addressed here. evolutionary algorithms are the parallel metaheuristic of preference  <cit>  and thus the optimization pluginwe used in this paper was implemented with evolutionary algorithms, in detail genetic algorithms   <cit> . additional motivations for using gas are their simplicity, proven performance, versatility and success in the life sciences  <cit> . the plugin uses an existing genetic-algorithm-library, jgap  <cit> , and was adapted to workflow parameter optimization by coding each input parameter as a “gene” on a “chromosome”, where each chromosome contains a particular combination of input parameters. in each generation, individual instances of the workflow are executed; one for each chromosome . after a user-defined number of generations or other abort criteria, the framework presents the user with the optimal or best parameter set found. additional statistics, which we will also use in this paper, can be saved after the optimization phase. by using this generic optimization framework and the extended parameter optimization plugin, we obtain a better and more robust parameter set than by using defaults or refining parameters by trial and error. additionally, there is no need for any prior knowledge about optimization techniques, as the framework and plugin manage all aspects of the optimization. the framework enables researchers to easily optimize scientific workflows and thus increase the scientific output more efficiently than using trial and error or parameter sweeps. more information about the optimization framework, the optimization process and other examples can be found at http://ms-utils.org/taverna_optimization.pdf or  <cit> .

all computing intensive executions  performed during the optimizations in this work were conducted on a grid that was set up by the grid software unicore  <cit> . the calculations were executed on a cluster within the grid with  <dig> compute nodes, each of which consists of two  <dig>  ghz intel xeon 6-core processors and  <dig> gib main memory. for the execution on the grid,  <dig> cpus per job were requested by the user. the scheduling and execution of the jobs were handled by unicore, as described previously  <cit> .

optimization of retention time prediction
to illustrate a different type of optimization, comparing not only parameters but also algorithms, we included a retention time prediction in the workflow. the workflow in fig.  <dig>  shown in grey was used for the optimization. it can be accessed at http://www.myexperiment.org/workflows/ <dig> html. in addition to peptide-spectrum matching, we may choose to incorporate additional information about the peptides in the identification or removal of false positives. one way to do this is to train a retention time predictor and use this to remove peptides that do not fit the predicted chromatographic behavior from the list of peptide matches  <cit> . there are a number of algorithms for this purpose, including the original software “rt”  <cit>  and two different algorithms included in the rtcalc utility in tpp: one based on ssrcalc  <cit>  and one based on the artificial neural network  method by petritis and co-workers  <cit> . to demonstrate how a scientific workflow can choose an optimal path for proteomics data analysis, we designed a workflow to balance the quality  of the training set and the prediction model, to find the model that can best predict the retention times of peptides within the same dataset. the rationale is that the simpler retention time predictors have fewer free parameters and will be trained more robustly by smaller training sets than the potentially better but more complex models requiring much more training data. rtcalc has its own hardcoded internal quality checks that generates an error message and aborts rather than produce a poor or overfitted model. we disabled these checks in the rtcalc source code to level the playing field and allow the optimization framework to independently find the right combination of parameters and algorithm. alternatively, and for increased robustness, the root-mean-square deviation can be set to a very large  value if rtcalc or rt returns an error due to too few peptides or non-convergence to avoid having the genetic algorithm explore regions where no good solutions could be expected. in addition to the choice between the three algorithms, we simultaneously optimized the psm probability cutoff as calculated by peptideprophet for the peptides included in the training sets. this workflow is shown as a stand-alone workflow in fig.  <dig> and is also embedded in fig.  <dig>  the quality of the retention time prediction was evaluated as root-mean-square deviation for 10 % of the peptides held back as a validation set, using the remaining 90 % of the peptides to train the model. these 10 % were then chosen at random  <dig> times so that each peptide was used exactly once for validation. the psm probability cutoff for the validation set was constant at p =  <dig> . the result of this optimization was then used in a downstream workflow removing psm outliers with measured retention time deviating from the predicted retention time by more than a user-defined absolute z-score, here  <dig> .fig.  <dig> stand-alone workflow for retention time modeling and prediction. each of the three embedded subworkflows corresponds to one particular retention time model. the subworkflows can be switched on and off by a flag. in each workflow run, only one of the subworkflows is executed. this flag was used as a parameter in the workflow optimization. taverna workflows visualize workflow inputs by a red triangle and outputs by a green triangle. this also holds for embedded workflows



RESULTS
results of x!tandem optimization
the optimum mme search windows found for the six test datasets using the ranges for x!tandem as described above can be found in tables  <dig> and  <dig>  remarkably, in only one of these, the human orbitrap dataset, does the optimum mme search window appear to directly correspond to the mass measurement uncertainty of the instrument . the optimum upper mme  limits for both tof datasets and the human ion trap dataset as well as the lower mme  limits for both ion trap datasets were between  <dig>  and  <dig>  da. these correspond to  <dig> – <dig>  m/z units for a doubly charged peptide and are related to the widths of the precursor ion selection window in the quadrupole or ion trap rather than the mass measurement uncertainty. already in small mme tolerance windows , one can observe such outliers that cannot be explained by poor instrument performance, but are caused by a second, co-eluting peptide in the same m/z window as the peptide selected for ms/ms . the selection window for ms/ms also does not have infinitely sharp boundaries, but allows a fraction of ions through, even if their m/z is just outside the window as defined in the mass spectrometer control software. this behavior results in a mixed tandem mass spectrum with fragment ions from two or more peptides. when the second, “freeriding” peptide produces more intense fragment ion peaks than the selected peptide, the former peptide is more likely to be identified, as long as it is within the searched mass measurement window. this is especially true when there is no penalty for mmes, which is the case for sequest, mascot, x!tandem and a number of other common search engines. although retrospectively making sense, we had not predicted that this effect would dominate the benefit of searching a narrow m/z window corresponding to the actual mme for all but one of the test datasets.table  <dig> results from the x!tandem and peptideprophet optimization of the six test datasets with information on number of unique peptides and the optimal mme


e. coli

e. coli

e. coli

h. sapiens

h. sapiens

h. sapiens






fig.  <dig> peptide identifications from non-selected precursors. with larger mme tolerances, here ±5 da plus isotope error, x!tandem identified co-eluting peptides with lower  or higher  m/z than the selected precursor but within the precursor isolation window. in a, a peptide with monoisotopic m/z  <dig>  was identified  instead of a peptide  at m/z  <dig>  triggering the ms/ms event. in b, a peptide with m/z  <dig>  is identified  instead of a peptide at m/z  <dig> . both precursors were the ninth to be acquired out of ten sorted by intensity for their corresponding ms scans, more than  <dig>  s after the ms scans themselves, and both precursors disappear into the background in the subsequent ms scans. these are two examples of almost  <dig> such psms in the e. coli ion trap dataset



in all e. coli datasets we gain a number of these peptide-spectrum matches when we allow mmes of 5 da compared to  <dig>  da . naïvely, one may assume that outside this ~5 da window, no more true psms will be found by stretching the search window further. however, this is not the case. when expanding the search to allow mmes of up to 15 da or more , a number of new psms appear. these are caused by the matching of theoretical spectra from peptides with n-terminal pyroglutamate from glutamine or glutamic acid with measured spectra of unmodified peptides experiencing ammonia or water loss from the n-terminal glutamine/glutamic acid during fragmentation. this shifts the b-ions by  <dig>  or  <dig> , indistinguishable from the difference between glutamine/glutamic acid and pyroglutamate. as these are now identified as regular b-ions rather than b-ions with ammonia  or water  loss, the peptide-spectrum match receives a higher k-score by x!tandem, bringing a number of these psms from just below to above the 1 % fdr threshold, leading to a larger number of correct psms . of course, a scoring scheme that gives equal weight to b-, b*- and bo-ions and weighs in the mme would still identify the  correct peptide. however, this is not the case in most common search engines that allow arbitrarily large mmes in the search. this phenomenon shifts the global mme+ optimum to  <dig>  da and  <dig>  da for the ion trap and orbitrap e. coli datasets respectively  and produce local optima along a ridge with mme+ 15–18 da in the other datasets. in the orbitrap e. coli dataset searched with ±25 da mme tolerance and filtered for 1 % fdr by peptideprophet, there were also  <dig> psms with mme  <dig> – <dig> ,  <dig> of which contained at least one methionine and  <dig> more at least one histidine or tryptophan. there were  <dig> psms with mme  <dig> - <dig> ,  <dig> of which contained an n-terminal glutamine,  <dig> psms with mme  <dig> – <dig> ,  <dig> with n-terminal glutamic acid and  <dig> with n-terminal glutamine, and  <dig> psms with mme  <dig> – <dig> , out of which  <dig> contained an n-terminal glutamic acid. in total,  <dig>  out of  <dig>  psms in this search were found outside the  mme window. similar patterns were observed for the other datasets. extending the mme tolerance to ±25 da actually identifies more spectra  than when searching the same dataset with x!tandem with the ±5 ppm mme tolerance  used in the originally published analysis of the dataset  <cit> .fig.  <dig> optimization of mme window for x!tandem on the hybrid ion trap/maxis dataset described above, with fitness defined at the number of correctly identified spectra from doubly charged precursor divided by the total number of tandem mass spectra. the surface was interpolated and visualized outside taverna using gnuplot with the dgrid3d and countour base functions, although similar graphics could in the future possibly also be created in an rshell inside the taverna workflow. the clearly visible ridge between  <dig> and 21 da positive mme corresponds to the pyroglutamate/ammonia loss resonance adding  <dig> peptide-spectrum matches  with 1 % fdr and mmes 15-20 da, nearly all by assigning actual nh3-loss b-ions as regular b-ions with 17 da mme. as comparison, there are only  <dig> psms in the mme window between  <dig> and 15 da. similar ridges are seen in at least four of the six datasets , although the global optimum is not always found along this ridge. it should be noted that the standard error in the actual mass measurement is below 2 ppm in this dataset, but that this number has very little relevance for the optimum mme window for x!tandem in a search of this dataset with only one variable modification and in a small sequence database



as we make the mass error tolerance window larger, we also retrieve more random, or false, peptides. the score for the best matching random peptide increases monotonously as a function of mme. in peptideprophet, this corresponds to a translation of the negative distribution to higher discriminant scores while the positive distribution remains unchanged. at some point, the cost of allowing better random matches will exceed the gain of additional psms. in addition, searching a larger window is more computationally expensive, scaling roughly linearly with the width of the error tolerance window. the x!tandem run time at the optimum varied from  <dig> to 10 min for the e. coli datasets and from 9 min to 3 h for the human datasets . execution time and computational cost were not explicitly considered in the optimizations, and for datasets such as the human tof data used here, the relatively marginal improvement of  <dig>  % additional psms may not motivate the  <dig>  h additional computational time, though all computationally intensive components of these workflows have been parallelized and can be run on clouds, grids or supercomputers  <cit> . as mentioned above, it is generally recommended to run these database searches in parallel. when considering the optimization runtime, the entire computational cost consists of the sum of each workflow run. the real runtime of an optimization process is therefore the sum of the longest workflow execution within each generation. for example, if in generation  <dig> the longest workflow execution took 10 min and in the second generation 12 min, the total time for this optimization was 22 min, with  <dig> workflows having been executed in these two generations. this is feasible due to the parallel execution mechanism implemented within the optimization framework in taverna. in any case, the researcher should be aware of the required total compute resources needed for the execution of the workflows. table  <dig> also lists the runtimes of the workflow using the default mme tolerances , the maximum tolerances  and the optimum window. the times required for the entire optimizations are also included, although the optimization should only be required once for each combination of sample type, instrument and method parameters. additionally, the time required to perform the specific optimization is given. again, the researcher should be aware that the actual times may be dependent on the availability of the computing resources and the queuing time.

the results from the second optimization including missed cleavage sites and enzyme fidelity are shown in table  <dig>  it is clear that allowing only one missed cleavage is slightly better than allowing two missed cleavages and that the default value for the specificity  was also the best value. the optimum mme tolerances did not change dramatically. the improvement in fitness  was less than 1 %, suggesting the initial values were sensibly chosen. additional parameters can easily be included in the optimization process.table  <dig> results from the second optimization, in which different numbers of missed cleavages and different enzyme fidelities were also investigated for the e. coli hybrid ion trap/tof data



results of retention time prediction optimization
as expected, the number of peptides in the training sets used here were not sufficient to produce an accurate model using the artificial neural network algorithm. when there were more than ca.  <dig> peptides in the training set, the rtcalc coefficient  model performed best. when there were between  <dig> and  <dig> peptides, rt performed better, and for 21– <dig> peptides in the training set, only rt produced a model at all. no model was returned when having  <dig> peptides or fewer in the training set. in absence of quality checks, the minimum number of peptides required to produce a model is solely determined by the number of free parameters  in the model. it is possible that for very large training sets , the ann model will outperform the ssrcalc-derived model in rtcalc  <cit> . the optimum algorithm, ssrcalc, was then selected for use in a new workflow . a few outliers could be removed from the e. coli ion trap x!tandem results with peptideprophet p ≥  <dig>  and maximum absolute z-score of  <dig>  the most conspicuous having probabilities p <  <dig>  or log10 > - <dig> of being correct in the first place .fig.  <dig> workflow using the best retention time predictor  to filter a list of psms based on the agreement between measured and predicted retention time, assuming the identification is correct . the user selects a z-score threshold to remove outliers, which are likely due to false identifications. here we used a maximum absolute z-score of  <dig> to demonstrate the workflow, although this may be overly conservative, as a few psms of very high peptideprophet probability are also removed . the workflow is available on myexperiment 



discussion
the two examples shown here demonstrate that systematic exploration of parameters and algorithms for data analysis in mass spectrometry based proteomics can achieve at least two things. first and foremost, re-evaluating legacy parameter and model choices allows more peptides and proteins to be identified, which may allow more biologically relevant information to be extracted from the raw mass spectrometry data. the optimization should be done on a representative dataset, or a fraction of all the spectra, for instance sampled using random data decomposition  <cit> . secondly, exploring different combinations of parameters and algorithms leads to new insight into the data and the algorithms themselves – for example the ammonia loss b-ion spectra identified as peptides with n-terminal pyroglutamate and the behavior of the retention time predictors for different size training sets. these phenomena were not chosen for investigation, but uncovered during the parameter optimization when allowing the parameters to vary over a wide range. the optimum mme windows were found to be asymmetric with a larger tolerance of positive mmes. in one dataset, the optimal positive mme was found along the ridge  corresponding to the pyroglutamate/nh3-loss psms. the other optimal mmes were found either just outside the actual mass measurement errors  or just outside the mme corresponding to the precursor isolation window as illustrated in fig.  <dig> . similar observations were independently reported by three different groups at a recent international conference , including data from a q exactive orbitrap  <cit> . the phenomenon makes perfect sense given the distribution of mmes observed when allowing very large mmes in the x!tandem search, with few psms with mmes below - <dig> or between  <dig> and 15 da. an important point here is that the genetic algorithm searches a very large parameter space, and would also be able to find an optimum very close to zero if one exists for very accurate precursor mass measurements.

it is also important to be aware of a number of effects that can mislead optimization procedures such as the ones followed here. for some combination of parameters, possibly very far from optimal, the peptideprophet expectation-maximization  may fail to find the globally best fit to the measured discriminant score distribution. this can sometimes be explained by a noisy discriminant score distribution, but sometimes the peptideprophet em algorithm gets stuck in a local minimum. we therefore settled for the target/decoy and the non-parametric model of “2+” spectra in peptideprophet, as this does not fail over the range of parameters investigated in this study, whereas it occasionally fails for “1+” and “3+” spectra, especially when using the parametric model. the optimum found should still be a very good parameter choice for slightly different targets, as roughly two thirds of the identifiable spectra are from doubly charged precursors. the workflow feedback in the form of parameter surfaces is helpful in visually validating the optimization, and catching numbers returned from a failed em that are obviously erroneous . over smaller ranges and for more or better data and algorithms, the parametric model may still function sufficiently well for use in optimization. a different optimization target, such as the number of unique identified peptides, may theoretically produce a smaller optimum mme tolerance, as many of the peptides identified in the larger windows, such as the co-eluting peptides in fig.  <dig>  would have also been selected for ms/ms and identified from different spectra in the same dataset. however, it is good to remember that random  matches tend to be to unique peptides, and that optimizing for the number of unique peptides or proteins will have a positive bias toward spurious identifications.

the usage of the taverna workflow management system and the optimization framework produced only a small overhead in this experiment. even if scientific workflows are still new in the proteomics field  <cit> , many researchers are already familiar with the usage of scientific workflow management systems like taverna. as taverna is implemented in java, it can be executed as a java application without installation and thus typically on every machine. with the taverna graphical interface, users can design their own workflows or reuse existing ones from a repository  <cit> . some workflows require access to or installation of applications that will be called by the workflow. adaptation is sometimes needed in order to run the workflows on one’s own machine. this procedure is very dynamic in taverna and cannot be described in general. references and further literature can be found at http://www.taverna.org.uk. the workflow optimization plugin is designed as a standard taverna plugin and can be installed automatically by adding the download page to taverna . to enable the optimization process on a workflow, a graphical user interface is offered to select the sub-workflow, define termination criteria, and specify parameters, along with their ranges and dependencies. a modification of the workflow is not required for the optimization. after the optimization process, the result is presented to the user, who can store the entire optimization process including execution statistics and other information. for more detailed information on the optimization plugin, please refer to  <cit> .

CONCLUSIONS
we used a new optimization framework to optimize a scientific workflow for peptide-spectrum matching and retention time prediction. the two steps were optimized separately from each other in the taverna workflow manager. with the optimization framework users can optimize various parameters of any algorithm or tool within a scientific workflow. in our use case we allowed a much larger mme window for x!tandem than typically used. with this setup we had been able to find new psms outside of the commonly searched mme window. these psms were primarily due to the unpredicted matching of spectra from peptides with n-terminal pyroglutamate from glutamine or glutamic acid with measured spectra of unmodified peptides experiencing ammonia or water loss from the n-terminal glutamine/glutamic acid during fragmentation.

in conclusion, we suggest an open mind and perhaps a more widely open search window is needed whenever looking at data from new types of experiments or new mass spectrometers. scientific workflows, for example in taverna, have many advantages for analysis of large proteomics datasets, such as comprehension, shareability, provenance, interfacing with cloud or grid computing. in combination with the taverna optimization framework, the workflow can then be optimized with respect to parameters as well as algorithms, on-the-fly and fully transparently. additional search parameters and exclusion criteria, such as minimum number of peaks, minimum fragment m/z and minimum peptide length, may also deserve investigation, although short peptides tend to less protein-specific and therefore of less value in practice.

availability of supporting data
all software and workflows are freely available at http://unicore-dev.zam.kfa-juelich.de/taverna/plugins/ and from myexperiment.org. the installation and usage guide is available at http://ms-utils.org/taverna_optimization.pdf. at http://www.myexperiment.org/workflows/ <dig> html the x!tandem and peptideprophet workflow is available. the workflow for the retention time prediction optimization can be accessed at http://www.myexperiment.org/workflows/ <dig> html. the liquid chromatography-tandem mass spectrometry datasets produced in-house, including the hybrid ion trap/maxis data, are available from http://cpm.lumc.nl/export/public_datasets/.

competing interests

the authors declare that they have no competing interests.

authors' contributions

sh and mp conducted the research. sh and oz developed the optimization method for scientific workflows. mp collected, prepared and analyzed the test samples. sh, mp and ym developed the workflows. sh performed the data analyses and mp evaluated the results. all authors read and approved the final manuscript.

