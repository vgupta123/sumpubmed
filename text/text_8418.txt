BACKGROUND
serological diagnostics have received increasing scrutiny recently  <cit>  due to their potential to measure antibodies rather than low-abundance biomarker molecules. antibodies avoid the biomarker dilution problem and are recruited rapidly following infection, chronic, or autoimmune episodes, or exposure to cancer cells. serological diagnostics using antibodies have the potential to reduce medical costs and may be one of the few methods that allow for true presymptomatic detection of disease. for this reason, our group has pursued immunosignaturing for its ability to detect the diseases early and with a low false positive rate. the platform consists of a peptide microarray with either  <dig>  or  <dig>  peptides per assay. this microarray is useful with standard mathematical analysis, but for a variety of reasons, certain methods of classification enable the best accuracy  <cit> . classification methods differ in their ability to handle high or low numbers of features, the feature selection method, and the features’ combined contribution to a linear, polynomial, or complex discrimination threshold. expression microarrays are quite ubiquitous and relevant to many biological studies, and have been used often when studying classification methods. however, immunosignaturing microarrays may require that we change our underlying assumptions as we determine the suitability of a particular classifier.

in order to establish the question of classification suitability, we examine a basic classification algorithm, linear discriminant analysis . lda is widely used in analyzing biomedical data in order to classify two or more disease classes  <cit> . one of the most commonly used high-throughput analytical methods is the gene expression microarray. probes on an expression microarray are designed to bind to a single transcript, splice variant or methy variant of that transcript. these one-on-one interactions provide relative transcript numbers and cumulatively help to define high-level biological pathways. lda uses these data to define biologically relevant classes based on the contribution of differentially expressed genes. this method often uses statistically identified features  that are different from one condition to another. lda can leverage coordinated gene expression to make predictions based on a fundamental biological process. the advantage of this method is that relatively few features are required to make sweeping predictions. when features change sporadically or asynchronously, the discriminator predictions are adversely affected. this causes low sensitivity in exchange for occasionally higher discrimination. tree-based methods use far more features to obtain a less biased but less sensitive view of the data. these methods can partition effects even if the effect sizes vary considerably. this approach can be more useful than frequentist approaches where it is important to maintain partitions in discreet groups.

immunosignaturing has its foundations in both phage display and peptide microarrays. most phage display methods that use random-sequence libraries also use fairly short peptides, on the order of 8– <dig> amino acids  <cit> . epitope microarrays use peptides in the same size range, but typically far fewer total peptides, on the order of hundreds to thousands  <cit> . each of these methods assumes that a single antibody binds to a single peptide, which is either detected by selection  or by fluorescent secondary antibody . immunosignaturing uses long 20-mer random-sequence peptides that have potentially  <dig> or more possible linear epitopes per peptide. although immunosignaturing must make do with only  <dig>  to ~ <dig>  peptides, the information content derived from partial binding makes these data useful in ways quite different from phage display  <cit> .

the complexity in analysis arises from the many-to-many relationship between peptide and antibody . this relationship imposes a particular challenge for classification because a simple one-to-one relationship between probe and target, idiomatic for gene expression microarrays, allows a coherent contribution of many genes that behave coordinately based on biological stimuli. that idiom is broken for immunosignaturing microarrays, where each peptide may bind a number of different antibodies and every antibody might bind a number of peptides. unless disease-specific antibodies find similar groups of peptides across individuals, very little useful information is available to the classifier. the aim of this work is to assess the performance of various classification algorithms on immunosignaturing data.

we have considered  <dig> diverse data mining classification methods. for feature selection, we used a simple t-test when we examined two classes, and a fixed-effects 1-way anova for multiple classes with no post-hoc stratification. we have assessed these algorithms’ ability to handle increasing numbers of features by providing four different sets of peptides with increasing p-value cutoff. the four levels include from  <dig>  to > <dig>  peptides. each algorithm is thus tested under conditions that highlight either synergistic or antagonistic effects as the feature numbers increase.

methods
technology
a peptide microarray described previously  <cit>  was used to provide data for analysis. two different sets of  <dig>  random peptide sequences are tested. the two peptide sets are non-overlapping and are known as cim10kv <dig> and cim10kv <dig>  peptides are printed as in  <cit> .

sample processing
samples consist of sera, plasma or saliva – each produces a suitable distribution of signals upon detection with an anti-human secondary igg-specific antibody. samples are added to the microarray at 1: <dig> dilutions in sample buffer ), igg antibodies are detected through a biotinylated secondary anti human igg antibody , littleton, co), which binds the primary. fluorescently labeled streptavidin is used to label the secondary antibodies and scanned with an agilent ‘c’ laser scanner in single-color mode. 16-bit images are processed using genepix pro  <dig>  which provides the tabular information for each peptide in a continuous value ranging from 0– <dig> . four unique data sets have been used in this analysis,  <dig> run on the cim10kv <dig> and  <dig> on the cim10kv <dig>  each individual sample was run in duplicate; replicates with > <dig>  pearson correlation coefficient were considered for analysis.

datasets
center for innovations in medicine, biodesign institute, arizona state university has an existing irb  <dig>  which allows analysis of blinded samples from collaborators.

a.) type  <dig> diabetes data set: this dataset contains  <dig> sera samples . these samples were tested on the cim10kv1microarrays.

b.) alzheimer’s disease data set: this dataset contains  <dig> samples . these were tested on the cim10kv <dig> microarrays.

c.) antibodies dataset: this dataset contains  <dig> samples and has  <dig> groups monoclonal antibodies, arbitrarily arranged. all monoclonals were raised in mouse, and use the same secondary detection antibody. samples were run on the cim10kv <dig> microarrays.

d.) asthma dataset: this dataset consists of  <dig> unique samples containing serum from patients with  <dig> distinct classes, corresponding to the household environment. condition a consists of  <dig> control subjects who had no environmental stimuli. condition b consists of  <dig> subjects who had stimuli but no asthma-related symptoms. condition c consists of  <dig> subjects who had no stimuli but with clinical asthma. condition d consists of  <dig> subjects who have both stimuli and clinical asthma. samples were tested on the cim 10 kv <dig> microarrays. asthma datasets were been analyzed by considering all four conditions using anova in order to study the combined effect of stimuli and asthma on subjects and then by considering pair wise comparison of condition a vs. b, a vs. c, and b vs. d.

data preprocessing, normalization and feature selection
the 16-bit tiff images from the scanned microarrays were imported into genepix pro  <dig>  . raw tabular data were imported into agilent’s genespring  <dig> . <dig> . data were median normalized per array and log <dig> transformed. for feature selection we used welch-corrected t-test with multiple tested . for multiple groups  we used 1-way fixed-effects anova.

data mining classification algorithms
four distinct peptide features are chosen for the comparison study. for each analysis, peptides are selected by t-test or anova across biological classes, with  <dig> different p-value cutoffs. cutoffs were selected to obtain roughly equivalent sized feature sets to assess the ability of each algorithm to process sparse to rich feature sets. once the significant features were collected, data was imported into weka  <cit>  for classification. the algorithms themselves spanned a wide variety of classifiers including bayesian, regression based methods, meta-analysis, clustering, and tree based approaches.

we obtained accuracy from each analysis type using leave-one-out cross-validation. we obtained a list of t-test or anova-selected peptides at each stringency level. the highest stringency uses peptides with p-values in the range of 10- <dig> to 10- <dig> and contains the least ‘noise’. the less-stringent second set uses p-values approximately 10-fold higher than the most stringent. the third contains the top  <dig> peptides and the forth contains ~ <dig> peptides at p <  <dig> . although different numbers of peptides are used for each dataset, each peptide set yields the same general ability to distinguish the cognate classes. the weka default setting of parameters were used for every algorithm to avoid bias and over fitting. these default parameters are taken from the cited papers listed below for each algorithm. brief details of default parameters and algorithms are listed

i. naïve bayes: probabilistic classifier based on bayes theorem. numeric estimator precision values are chosen based on analysis of the training data. in the present study, normal distribution was used for numeric attributes rather than kernel estimator  <cit> .

ii. bayes net: probabilistic graphical model that represents random variables and conditional dependencies in the form of a directed acyclic graph. a simple estimator algorithm has been used for finding conditional probability tables for bayes net. a k <dig> search algorithm was used to search network structure  <cit> .

iii. logistic regression : a generalized linear model that uses logistic curve modeling to fit the probabilistic occurrence of an event <cit> . the quasi-newton method is used to search for optimization. 1x <dig> has been used for ridge values in the log likelihood calculation  <cit> .

iv. simple logistic: classifier for building linear logistic regression models. for fitting the logistic model ‘logitboost’, simple regression functions are used. automatic attribute selection is obtained by cross validation of the optimal number of ‘logitboost’ iterations  <cit> . heuristic stop parameter is set at  <dig>  the number of maximum iterations for logitboost has been set to  <dig> 

v. support vector machines : a non-probabilistic binary linear classifier that constructs one or more hyper planes to be can be used for classification. for training support vector classes, john platt’s sequential minimal optimization algorithm was used which replaces all missing values  <cit> . here multiclass problems are used using pair-wise classification. the complexity parameter is set to  <dig>  epsilon for round off error is set to 1x10*- <dig>  polykernel is the set to be kernel. the tolerance parameter is set to  <dig>   <cit> .

vi. multilayer perceptron : a supervised learning technique with a feed forward artificial neural network through back-propagation that can classify non-linearly separable data  <cit> . the learning rate is set to  <dig>  and momentum applied during updating weights is set to  <dig> . the validation threshold use to terminate the validation testing is set to  <dig> 

vii. k nearest neighbors : instance based learning or lazy learning which trains the classifier function locally by majority note of its neighboring data points. linear nn search algorithm is used for search algorithm  <cit> . k is set to  <dig> 

viii. k star: instance based classifier that uses similarity function from the training set to classify test set. missing values are averaged by column entropy curves and global blending parameter is set to  <dig>  <cit> .

ix. attribute selected classifier : ‘cfs subset’ evaluator is used during the attribute selection phase to reduce the dimension of training and test data. the ‘bestfit’ search method is invoked after which j <dig> tree classifier is used  <cit> .

x. classification via clustering : simple k means clustering method is used where k is set to the number of classes in the data set  <cit> . euclidean distance was used for evaluation with  <dig> iterations.

xi. classification via regression : regression is a method used to evaluate the relationship between dependent and independent variables through an empirically determined function. the m5p base classifier is used which combines conventional decision tree with the possibility of linear regression at the nodes. the minimum number of instances per leaf node is set to  <dig>  <cit> .

xii. linear discriminant analysis : prevalent classification technique that identifies the combination of features that best characterizes classes through linear relationships. prior probabilities are set to uniform and the model as homoscedastic.

xiii. hyper pipes: simple, fast classifier that counts internally defined attributes for all samples and compares the number of instances of each attribute per sample. classification is based on simple counts. works well when there are many attributes  <cit> .

xiv. vfi: voting feature interval classifier is a simple heuristic attribute-weighting scheme. intervals are constructed for numeric attributes. for each feature per interval, class counts are recorded and classification is done by voting. higher weight is assigned to more confident intervals. the strength of the bias towards more confident features is set to  <dig>  <cit> .

xv. j48: java implementation of c <dig>  algorithm. based on the hunt’s algorithm, pruning takes place by replacing internal node with a leaf node. top-down decision tree/voting algorithm  <cit> .  <dig>  is used for the confidence factor. no laplace method for tree smoothing  <cit> .

xvi. random trees: a tree is grown from data that has k randomly chosen attributes at each node. it does not perform pruning. k-value  + 1) is set at zero. there is no depth restriction. the minimum total weight per leaf is set to  <dig>  <cit> .

xvii. random forest : like random tree, the algorithm constructs a forest of random trees  <cit>  with locations of attributes chosen at random. it uses an ensemble of unprune decision trees by a bootstrap sample using training data. there is no restriction on the depth of the tree; number of tress used is  <dig> 

time performance
cpu time was calculated for every algorithm at the four different significance levels. this time was measured on a standard pc  that was completely dedicated to weka. to measure cpu time, open source jar files from weka were imported to eclipse where the function ‘time ()’ was invoked prior to running the classification including the time required for cross validation. most windows  <dig> services were switched off; the times reported were an average of  <dig> different measurements.

RESULTS
overall performance accuracy of classification algorithms over all data sets
for each dataset, accuracies are measured at four levels  at various levels of significance. overall average performance measure is calculated for each algorithm for a given data set. table  <dig> shows the overall average percentage score for each algorithm calculated by averaging accuracy, specificity, sensitivity and area under roc curve under all levels of significance. scores >90% are marked in bold. mlp algorithm did not finish due to huge memory requirements on last level of significance and is averaged based on first three levels of significance. for type  <dig> diabetes, alzheimer’s and antibodies dataset, > <dig> algorithms scored >90% average score. overall, naïve bayes had the highest average score  and was always among top  <dig> algorithms among all datasets.

t1d: type  <dig> diabetes datasets, az: alzehemer’s dataset, ab: antibodies dataset. table showing algorithms overall performance in each datasets based on average score. score >90% are marked in bold. naïve bayes scored the overall highest average score of  <dig> %.

performance accuracy of classification algorithms at different levels of significance over all data sets
for each data set, different levels of significance are chosen to measure the performance accuracy of each algorithm. these levels contain approximately equal number of peptides for each data set. the first level contains  <dig> peptides selected from the t-test  and hence contains the least noise. next, approximately  <dig> peptides,  <dig> peptides and  <dig> peptides were chosen for the other three levels.

tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> shows  <dig> different performance measures  at different levels of significance over  <dig> datasets. for the asthma dataset, we considered all conditions a-d together, then performed the pair-wise comparisons of condition a and b, condition a and c, and condition b and d at three different levels of significance. measures >90% are marked in bold. for the diabetes dataset,  <dig> algorithms achieved >90% score. for alzheimer’s and the antibodies dataset,  <dig> algorithms achieved >90% score. naïve bayes scored 100% in all  <dig> measures at the first level of significance in the alzheimer’s dataset and scored  <dig> % average score on the antibodies dataset. for the asthma datasets, the highest score was <80%. only naïve bayes had >90% specificity for more than one level of significance. for two conditions in asthma datasets, naïve bayes and vfi scored >90% average score.

acc: accuracy, sp: specificity, sn: sensitivity, auc: area under roc curve, avg: average score in % for each algorithms, dnf: “did not finish”, * denotes avg. from  <dig> significance levels. measures >90% are marked in bold.

acc: accuracy, sp: specificity, sn: sensitivity, auc: area under roc curve, avg: average score in % for each algorithms, dnf: did not finish”, * denotes avg. from  <dig> significance levels. measures >90% are marked in bold.

acc: accuracy, sp: specificity, sn: sensitivity, auc: area under roc curve, avg: average score in % for each algorithms, dnf: did not finish”, * denotes avg. from  <dig> significance levels. measures >90% are marked in bold.

acc: accuracy, sp: specificity, sn: sensitivity, auc: area under roc curve, avg: average score in % for each algorithms, dnf: did not finish”, * denotes avg. from  <dig> significance levels. measures >90% are marked in bold.

acc: accuracy, sp: specificity, sn: sensitivity, auc: area under roc curve, avg: average score in % for each algorithms, dnf: did not finish”, * denotes avg. from  <dig> significance levels. measures >90% are marked in bold.

acc: accuracy, sp: specificity, sn: sensitivity, auc: area under roc curve, avg: average score in % for each algorithms, dnf: did not finish”, * denotes avg. from  <dig> significance levels. measures >90% are marked in bold.

acc: accuracy, sp: specificity, sn: sensitivity, auc: area under roc curve, avg: average score in % for each algorithms, dnf: did not finish”, * denotes avg. from  <dig> significance levels. measures >90% are marked in bold.

comparative analysis of worst time performance of classification algorithms over data sets
the amount of time taken by each algorithm to build the model and perform cross validation was measured. table  <dig> shows the time in milliseconds for each algorithm at the lowest level of significance when the number of peptides nears  <dig>  random tree was the fastest, at ~ <dig> milliseconds  to complete the task, while mlp was the worst which did not finish due to high memory requirements. random tree, hyper pipes, naïve bayes, vfi and knn were the five fastest algorithms; each took less than ~ <dig> milliseconds to complete classification of > <dig>  peptides. logistic regression and attribute selected classifier, mlp were among the slowest algorithms taking more than  <dig> minutes to perform classification of > <dig>  peptides. the absolute ranking for every algorithm was consistent per dataset; only three datasets have been considered to measure time performance.

table showing time performance in milliseconds over > <dig> peptides for three datasets. random tree, knn, hyper pipes and vfi were among the fastest. mlp were among the slowest with dnf: “did not finish”. time measurements less than  <dig> seconds are marked in bold.

comparative analysis of time performance of classification algorithms at different levels of significance over three data sets
for each level of significance, time was measured for each algorithm to build the model and for cross validation. at the highest level of significance , each algorithm were fast enough to complete the task in under  <dig> seconds. execution times increased as the level of significance was lowered due to the higher number of features and increased difficulty in constructing the model. table  <dig> shows classification algorithms time performance at various levels of significance.

table showing time performance in milliseconds on all level of significance for three datasets. mlp were among the slowest with dnf: “did not finish”. time measurements less than  <dig> seconds are marked in bold.

results summary
we have explored several disparate classifiers using a relatively new type of microarray data: immunosignaturing data. the tested algorithms come from a broad family of approaches to classify data. we chose algorithms from bayesian, regression, trees, multivariate and meta analysis and we believe we have sampled sufficiently that the results are relevant. from table  <dig> we found that naïve bayes had a higher average performance than all other algorithms tested. naïve bayes achieved > 90% average for  <dig> classes datasets where there is a clear distinction between two classes. for the multi-class the antibodies dataset, where there is a clear difference between different types of antibodies, naïve bayes scored 88% average accuracy and was ranked third, close to the  <dig> % accuracy of random forest. on the asthma dataset, containing four classes, none of the algorithms were able to achieve more than 75% accuracy. this matches the biological interpretation very well. naïve bayes outperformed all algorithms for speed and accuracy, achieving  <dig> % average score overall. naïve bayes was one of the top five fastest algorithms, ~ <dig> times faster than the logistic regression. a summary of the all algorithms performance measures and time is given in below and described in table  <dig>  distance metrics have been defined to access performance measures for all algorithms compared to the highest scoring algorithm on a given dataset.

i. naïve bayes: naïve bayes performed best overall with > 90% overall average score. it was always among the top  <dig> algorithms in all  <dig> comparisons. it ranked first  <dig> out  <dig> times when comparing all datasets. it was on an average just  <dig> % behind the rank  <dig> algorithm in overall comparison. it is 2x slower than the fastest algorithm due to its mathematical properties. it would be feasible to perform large-scale classification studies using naïve bayes.

ii. multilayer perceptron : it ranked second with overall score of  <dig> % and was very close to svm. the overall score is biased since mlp did not finish for level containing ~ <dig> peptides and hence scored was averaged from just the three levels. it was the slowest algorithm and infeasible to perform large-scale classification.

iii. support vector machines : although it ranked third, it was not significantly different from the mlp in terms of performance measures. it was 700x faster than mlp and achieved >90% measured accuracy  <dig> times. both mlp and svm were <5% behind the rank  <dig> algorithm on average.

iv. vfi: vfi ranked fourth in overall performance measures and was the among top  <dig> fastest algorithms due to its voting method. four times it obtained >90% average overall accuracy and ranked 2nd twice.

v. hyper pipes: hyper pipes ranked fifth overall in performance measures and was among the fastest of the tested algorithms, likely due to its inherently simplistic ranking method. it was <8% from first place  <dig> times.

vi. random forest: random forest ranked sixth in overall performance measures and performed better on datasets having multiple classes . it was  <dig> times slower than the fastest algorithm due to bootstrapping.

vii. bayes net: ranked in the middle for overall accuracy and time. it scored >90% overall measures twice. it was slower than the naïve bayes due to construction of networks in the form of an acyclic graph and it is relatively inefficient compared to naïve bayes due to the change in network topology during assessment of probability.

viii. k means: k-means ranked eighth in overall performance measures and was 34x slower than the fastest algorithm in time performance due to the multiple iterations required to form clusters. it performed far better for  <dig> classes compared to multiple classes because guaranteed convergence, scalability and linear separation boundaries are more easily maintained.

ix. logistic regression: logistic regression ranked ninth in overall accuracy. it was >90% three times. it was among the worst in time performance, being ~ <dig> times slower than the fastest algorithm as it needs to regress on high number of features. it is efficient for small numbers of features and sample sizes >  <dig> 

x. simple logistic: it ranked tenth in overall performance measures and ranked first on the diabetes dataset. it ranked second in multiclass asthma dataset. it was slow in time performance due to logitboost iterations.

xi. k nearest neighbors: it performed well on the  <dig> classes dataset but didn’t perform as well for multi class datasets. it was >90% performance for only rather difficult diabetes dataset. this may be related to evenly defined but diffuse clusters related to the subtle differences between the asthma patients.

xii. k star: it performed >90% for only the diabetes dataset and was  <dig> times slower than the fastest algorithm. this algorithm may also be sensitive to the even and diffuse clusters described by this dataset.

xiii. m5p: it did not perform well on either time performance or accuracy. it never achieved >90% average score and was  <dig> times slower than the fastest algorithm due to formation of comprehensive linear model for every interior node of the unpruned tree.

xiv. j48: top  <dig> fastest algorithm due to rapid construction of trees. it was >20% behind from the rank  <dig> algorithm on an average; its lower performance may possibly be due to formation of empty/insignificant branches which often leads to overtraining.

xv. random trees: it was the fastest algorithm since it builds trees of height log where k is the number of attributes, however it achieves poor accuracy since it performs no pruning.

xvi. attribute selected classifier : one of the slowest algorithms as it had to evaluate attributes prior to classification. it underperformed in performance measures due to the c <dig>  classifier limitations that prevent overtraining.

xvii. linear discriminant analysis : its performance accuracy decreased as the number of features increased due to its inability to deal with highly variant data. it was slow  since it tries to optimize class distinctions but the variance covariance matrix increases dramatically as the number of features increased.

#rank  <dig>  rank 2: no. of times algorithm ranked 1st and 2nd on  <dig> datasets, # > 90%: no. of times algorithm scored overall average score >90% on  <dig> datasets, distance: magnitude an algorithm trails behind on average from the rank  <dig> for the datasets . time: performance slower with respective to fastest algorithm. time performances slower by  <dig> folds to fastest algorithm are marked in bold.

discussion
the comparisons provided in this article provide a glimpse into how existing classification algorithms handle data with intrinsically different properties than traditional microarray expression data. immunosignaturing provides a means to quantify the dispersion of serum  antibodies that result from disease or other immune challenge. unlike most phage display or other panning experiments, fewer but longer random-sequence peptides are used. rather than converging to relatively few sequences, the immunosignaturing microarray provides data on the binding affinity of all  <dig>  peptides with high precision. classifiers in the open-source program weka were used to determine whether any algorithm stood out as being particularly well suited for these data. the  <dig> classifiers, which were tested, are readily available and represent some of the most widely used classification methods in biology. however, they also represent classifiers that are diverse at the most fundamental levels. tree methods, regression, and clustering are inherently different; the grouping methods are quite varied and top-down or bottom-up paradigms address data structures in substantially different ways. given this, we present and interpret the results from our tests, which we believe will be applicable to any dataset with target-probe interactions similar to immunosignaturing microarrays.

from the comparisons above, naïve bayes was the superior analysis method in all aspects. naïve bayes assumes a feature independent model, which may account for its superior performance. it relies on the degree of correlation of the attributes in the dataset; for immunosignaturing, the number of attributes can be quite large. in gene expression data, where genes are connected by gene regulatory networks, there is a direct and significant correlation between hub genes and dependent genes. this relationship affects the performance of naïve bayes by limiting its efficiency through multiple containers of similarly - connected features  <cit> . in peptide-antibody arrays, where the signals that arise from the peptides are multiplexed signals of many antibodies attaching to many peptides, there is no direct correlation between peptides, but there is a general trend. moreover, there is a competition of antibodies attaching to a single peptide, which makes it difficult for multiple mimotopes to show significant correlation with each other. thus, the  <dig>  random peptides have no direct relationships to each other each contributes partially to defining the disease state. this makes the immunosignaturing technology a better fit for the assumption of strong feature independence employed by the naïve bayes technique, and the fact that reproducible data can be had at intensity values down to  <dig> standard deviation above background enables enormous numbers of informative, precise, and independent features. presence or absence of a few high- or low-binding peptides on the microarray will not impact the binding affinity for any other peptide, since the kinetics ensures that the antibody pool is not limiting. this is important when building microarrays with > <dig>  features per physical assay, as in our newest microarray. more than 90% of the peptides on either microarray demonstrate normal distribution for binding signals. this is important since feature selection methods used in this analysis  and the naïve bayes classifier all assume normal distribution of features.

the naïve bayes approach requires relatively little training data, which makes it a very good fit for the biomarker field. the sample sizes usually range from n = 20- <dig> for the training set. naïve bayes has other advantages as well: it can train well on a small but high feature data set and still yield good prediction accuracy on a large test set. any microarray with more than a few thousand probes succumbs to the issue of dimensionality. since naïve bayes independently estimates each distribution instead of calculating a covariance or correlation matrix, it escapes relatively unharmed from problems of dimensionality.

the data used here for evaluating the algorithms were generated using an array with  <dig>  different features, almost all of which contribute information. we have arrays with > <dig>  peptides per assay  which should provide for less sharing between peptide and antibody, effectively spreading out antibodies over the peptides with more specificity. this presumably will allow resolving antibody populations with finer detail. this expansion may require a classification method that is robust to noise, irrelevant attributes and redundancy. naïve bayes has an outstanding edge in this regard as it is robust to noisy data since such data points are averaged out when estimating conditional probabilities. it can also handle missing values by ignoring them during model building and classification. it is highly robust to irrelevant and redundant attributes because if yi is irrelevant then p  becomes uniformly distributed. this is due to that fact that the class conditional probability for xi has no significant impact on the overall computation of posterior probability. naïve bayes will arrive at a correct classification as long as the correct classes are even slightly more predictable than the alternative. here, class probabilities need not be estimated very well, which corresponds to the practical reality of immunosignaturing: signals are multiplexed due to competition, affinity, and other technological limitation of spotting, background and other biochemical effects that exist between antibody and mimotope.

time efficiency
as the immunosignaturing technology is increasingly used for large-scale experiments, it will result in an explosion of data. we need an algorithm that is accurate and can process enormous amounts of data with low memory overhead and fast enough for model building and evaluation. one aims for next-generation immunosignaturing microarrays is to monitor the health status of a large population on an on-going basis. the number of selected attributes will no longer be limited in such a scenario. for risk evaluation, complex patterns must be normalized against themselves at regular intervals. this time analysis would require a conditional probabilistic argument along with the capacity of accurately predicting the risk with low computational cost. the slope of naïve bayes on time performance scale is extremely small, allowing it to process a large number of attributes.

CONCLUSIONS
immunosignaturing is a novel approach which aims to detect complex patterns of antibodies produced in acute or chronic disease. this complex pattern is obtained using random peptide microarrays where  <dig>  random peptides are exposed to antibodies in sera/plasma/saliva. antibody binding to the peptides is not one-to-one but a more complicated and multiplexed process. the quantity and appearance of this data appears numerically, distributionally, and statistically the same as gene expression microarray data, but is fundamentally quite different. the relationships between attributes and functionality of those attributes are not the same. hence, traditional classification algorithms used in gene expression data might be suboptimal for analyzing immunosignaturing results. we investigated  <dig> different kinds of classification algorithm spanning bayesian, regression, tree based approaches and meta-analysis and compared their leave-one-out cross-validated accuracy values using various numbers of features. we found that the naïve bayes classification algorithm outperforms the majority of the classification algorithms in classification accuracy and in time performance, which is not the case for expression microarrays  <cit> . we also discussed its assumptions, simplicity, and fitness for immunosignaturing data. more than most, these data provide access to the information found in antibodies. deconvoluting this information was a barrier to using antibodies as biomarkers. pairing immunosignaturing with naïve bayes classification may open up the immune system to a more systematic analysis of disease.

competing interests
us patent compound arrays for sample profiling:  <dig>  us patent ‘naïve bayes classification for immunosignaturing m12-104l, saj is cofounder of healthtell diagnostics which owns the patent to immunosignaturing.

authors’ contributions
mk completed the analysis of all data, and the original manuscript draft. ps completed all revisions and consulted on analysis. saj/ps co-invented immunosignaturing, saj funded the project. all authors read and approved the revised manuscript.

ethics statement
consent was obtained for every sample in this manuscript and was approved by asu irb according to protocol number  <dig> entitled "profiling human sera for unique antibody signatures". humans were consented by the retrieving institution and a materials transfer agreement was signed between the biodesign institute and the collaborating institute. the collaborating institutes' protocols were current and each human subject signed an approved consent form and released their sera.

