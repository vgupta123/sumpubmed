BACKGROUND
finding regulatory interactions between cell products is one of the most important objectives of systems biology and has stimulated considerable research efforts  <cit> . dna microarray technology enables us to measure mrna concentrations in a cell for a large number of genes at the same time. these levels can be viewed as a snapshot of the expression levels of genes under certain conditions. with a large enough set of snapshots, it should be theoretically possible to uncover the underlying gene regulatory network   <cit> .

one approach is to mathematically model the grn and to find parameters of the model from available data. once built, these models can be used to predict the behaviour of the organism under certain conditions, related to different treatments or diseases. also, once the basic mechanisms of life are revealed, it has been postulated that it should be theoretically possible to create synthetic organisms,  <cit> . a large number of mathematical models and inferential algorithms have been developed. generally, the process of modelling grns consists of a few main steps: choosing an appropriate model, inferring parameters from data, validating the model and conducting simulations of the grn, to predict its behaviour under different conditions.

in order to model a grn, genes are viewed as variables that change their  values in time. depending on the type of variables used, methods can be classified as discrete or continuous, deterministic or stochastic, or as hybrid methods, . two different approaches are distinguished in the literature,  <cit> : coarse-grained and fine-grained models, where the former contain less detail on interactions between genes. usually, coarse-grained models rely on discrete variables, while fine-grained models are based on continuous variables. a grn can be very large and may contain complicated interactions, so that a fine-grained model, typically, will have an enormous number of parameters to deal with. both inference and analysis of this kind of model are difficult tasks, thus global, , analysis of the network, has its attractions. this includes coarse-grained models, such as boolean networks, bayesian networks, petri nets, rule sets:  <cit> . other authors  <cit>  have chosen to focus on detailed models, i.e. systems of differential equations, artificial neural networks, thermodynamic models, hybrid petri nets, inter alia, analysing only subnetworks of the entire grn. a useful approach is to combine levels of detail, in a top-down or bottom-up approach, i.e. to move from a coarse to a more detailed model or vice versa  <cit> . further information can also be found in  <cit> , providing a collection of reverse engineering attempts and new challenges for researchers.

this paper concentrates on quantitative modelling of gene regulatory networks  using dna microarray data, as this is more informative than qualitative analysis of biological data. although more sophisticated high throughput technologies have been developed lately, , that may give more accurate results,  <cit> , microarrays are still widely used, not only as an established, well-understood technology, but also as one for which appropriate automatic analytical tools exist. even as newer methods become more pervasive, microarrays will remain both faster and less expensive for smaller genomes, . consequently, use of the less-sophisticated technology is likely to persist for exploratory data analysis, , at least in the medium term. furthermore, from the viewpoint of this study, algorithms developed for one high throughput method can be applied to different measurement techniques, as the model of biological behaviour is the focus, not the data type as such. our aim is to analyse different algorithms, used for such model inference, and to provide a comparison framework indicative of the advantages and disadvantages of each approach. we have chosen to analyse evolutionary algorithms  as suitable search methods for inferring grn model parameters, as these are known to cope well with a large solution space,  <cit> . in particular, eas can achieve good solutions from searching a relatively small section of the entire space, and have been widely used in genetic data analysis, .

previous  algorithm comparisons for the methods analysed here have been reported,  <cit> . however, to provide a valid comparison of existing eas for continuous models, algorithms assessed need to be applied not only to the same datasets, but also under the same framework. this work aims to achieve this and provide a consistent evaluation of ideas reported in the literature. the models used are not evaluated here, but only the algorithms that build models from data.

methods
in order to analyse the performance of evolutionary algorithms for model parameter inference, we have implemented seven different approaches and compared them on several datasets. these methods use different continuous fine-grained models to represent the grn, and rely on eas to find the model that best fits the experimental data. further information on the implemented techniques can be found in . the algorithms were developed using eva <dig>  a java framework for eas  <cit>  and the implementation and data sets used are available online. more information on downloading and using the framework can be found in ,  and  contain the source code of the implementation, while the datasets used are available as .

s-systems
generally, grn models, based on systems of differential equations, express the change in the expression level of each gene in time as a function of the expression levels of the other genes,  <cit> . s-systems are a special type of differential equation systems based on the power-law formalism and are capable of capturing complex dynamics,  <cit> . the disadvantages, compared to linear differential equations, , are an increase in the number of parameters and a reduction in the available choices of reverse engineering techniques. the equations in s-systems are of the form:   

in the case of grn modelling, the two terms in equation  <dig> correspond, respectively, to synthesis and degradation, influenced by the other genes in the network; αi and βi, the rate constants, represent the basal synthesis and degradation rate, and gij and hij, which indicate the strength of the influence of gene j on the synthesis and degradation of the product of gene i, are the kinetic orders. in real grns, it is, of course, possible that the expression level of a gene does not depend on the other genes, but only on its own concentration or that of metabolites or other external factors. self regulation is modelled by s-systems , and metabolite concentrations can also be introduced in the model, when measurements are available.

due to the fact that they are considered one of the most complete models for grns, s-systems have received a lot of attention in the literature . this is also reflected in the work presented here, where six of the methods analysed use this type of model.

artificial neural networks 
these are naturally-inspired models, which mimic the activity of the animal nervous system  <cit> . the network consists of units, called neurons, connected through weighted edges. by changing network topology and by using supervised learning algorithms to adjust the edges connecting neurons, an ann is capable of approximating, theoretically, any possible function. consequently, they are very well-suited for modelling purposes, especially when the underlying form of the model is unknown, which is also the case for grns. the disadvantage of this method is its complex topology; the regulatory causal interactions can not be extracted from the model, which can be a loss from the biological point of view .

two different ways of modelling grns with anns are common. the first one computes as the output of the ann the change in expression value, with time, of one gene, while the other calculates the expression value itself at a certain moment in time. inputs are the expression values of the regulators at the previous time point. the latter has been used here, ,  <cit> .

evolutionary algorithms
eas are a family of population based optimisation algorithms inspired by darwinian evolution, sharing a set of common features. included are: genetic algorithm, , evolution strategy, , genetic programming, , evolutionary programming, , differential evolution, . each maintains a population of solutions to the optimisation problem, , which evolve over a number of generations. the goodness of each individual, i.e. its fitness, is given by a function defined for the specific optimisation problem. evolution is performed using genetic operators that depend on the specific problem and encoding, e.g  mutation, which modifies one solution from the population, to obtain a new one and  crossover, which uses several parents to create a number of offspring. for each generation, a new set of solutions is produced from the previous population, either by replacing some parent individuals by children, or by performing fitness -based selection on all parents and children, .

although these are common features of eas, , they are also the elements that differentiate one type of ea from the others. for instance, individuals of gas are typically encoded as binary arrays, de and es use arrays of real numbers as an encoding for the solution, while gp evolve tree-encoded expressions. at the same time, these methods use different genetic operators, , or use one main operator; . even given strict differences between each individual in the ea family of methods, the distinction has become fuzzier with time, as new hybrid approaches have appeared, such as the real-encoded ga used in this paper.

the generic methodology of fitting a grn model to data using eas involves a given model, a set of data, and evolution of the model parameters. for a population of parameters, representing different models, genetic operators are applied and the fittest individuals in the population are selected for the next generation. usually, in the case of grns, the fitness function is defined as the difference between the observed data and the output of the model, . since every model has its individual features, algorithm steps differ from one approach to another, but the main skeleton is usually preserved.

here, we have implemented and analysed seven such algorithms: clga , moga , ga+es , ga+ann , peace <dig> , glsdc  and de+aic  . their analysis consists of two stages:  five hybrid eas  were assessed for scalability, robustness to noise and performance with real microarray data, and  two classical eas , , were compared in a small-scale setting to evaluate the improvement introduced by the multi-objective approach.

comparison of different eas can be performed using several criteria. the most common are fitness value of best individuals at the end of optimisation and the number of fitness evaluations required for obtaining an observed fitness value. robustness of fitness values and solutions obtained over multiple runs can also be analysed. additionally, in this paper, a problem dependent criterion was used: the obtained solutions are also compared to the initial models , or to previous biological knowledge . robustness to noise analysis is performed by maintaining as fixed the number of fitness evaluations and other ea meta-parameters , and observing the decrease in fitness and solution quality. scalability analysis involves increasing the number of fitness evaluations allowed and observing the quality of results obtained. the number of fitness evaluations was empirically chosen to allow the population to converge towards a stable fitness value . table  <dig> lists the criteria used for comparison of implemented algorithms.

RESULTS
in order to be able to evaluate our implementation on the chosen criteria, , six datasets generated by s-system models of regulation and five for the artificial neural network  model were used. the models for two and five-gene s-system synthetic regulatory networks were taken from the literature,  <cit> , and the ones for larger systems, , and for anns  were randomly generated so that they conform to well known characteristics of real grns, i.e. scale-free sparse networks. real grns are also known to display other characteristics such as modularity and feedback mechanisms,  <cit> . however, only sparsity is taken into account by the implemented methods, so using random sparse networks is a good indication of comparative algorithm performance. nevertheless, we acknowledge that this could represent a limitation with respect to the significance of the synthetic experiments for the algorithm ability to reverse engineer the correct network from real data.

robustness to noise was tested on the synthetic data for the five-gene networks to which 1%, 2%, 5% and 10% gaussian noise was added to all values. the assumption of gaussian noise has been used before in relation to gene expression data,  <cit> , and, although it may not be true in all situations, it provides a good indication of the behaviour of the algorithm with real noisy data.

ideally, in order to be able to build an s-system model, or to train an ann, for a large scale network, a large number of measurements  is required. this number increases further when data are noisy,  <cit> . however, in reality, due to the high cost of these experiments, only limited data are available. this leads to under-specification of the system, , which implies other parameter sets are able to reproduce the data . under these circumstances, eas become a good alternative to other fitting methods, as they provide an efficient way of spanning the promising areas of the solution space. in order to simulate experiments with real data, we reduced the number of  experimental time points used for inference to  <dig> for the 5-, 10- and 20-gene datasets,  <dig> for the 30-gene dataset and  <dig> for the 50-gene dataset. through this, we aimed to obtain a balance between the need for an increased number of experiments and the cost of these experiments in the real setting.

as evolutionary algorithms are stochastic in nature, multiple runs were performed for each experiment. multi-objective analysis was performed over  <dig> runs for each algorithm. the methods analysing the entire system were applied seven times on each dataset, while those using the divide and conquer approach were run five times for each of the first five genes, resulting in  <dig> runs per dataset. the quantitative results for the algorithms are displayed using notched box plots,  <cit> , which show, for each result set, , the minimum, maximum, and quartile values. the notches around the median allow for a significance analysis of the differences between algorithms: if the intervals defined by notches around the medians do not overlap, then the observed difference between the medians is statistically significant; . the graphs have been created using the free statistics software from wessa.net,  <cit> . the notches were reduced to the quartile limits, , in all the graphs displayed in this paper.

performance on small scale networks
for a first analysis, we applied five algorithms to the five-gene synthetic dataset from  <cit> . we chose this benchmark dataset due to the fact that it has been already used to validate most of the methods we are comparing. at the same time, the small dimensionality allows for easier analysis of the ea parameters and for multiple runs to be performed. figure  <dig> displays the box plots representing the data fit obtained by each algorithm, while figure  <dig> presents the quality of parameters obtained over all runs performed. table  <dig> contains numerical values for three more evaluation criteria . note that peace <dig> and ga+es analyse all genes simultaneously, while the others find interactions one gene at-a-time. however, the numerical values for all the genes in the latter type of methods are used, allowing for a direct comparison between them.

as figure  <dig> indicates, all five methods demonstrate good performance in fitting the data . ga+ann displays better fitness, followed by glsdc, while peace <dig> performs least. the fact that the notches around the mean do not overlap proves these differences to be statistically significant at a 5% level. however, these are insufficient alone to choose a specific algorithm, as other options may exist and alternative model parameters may give a good fit to the data. consequently, we provided  the parameter mse values that show how close the resulting models are to the real one, which, in this case, are known, . these values indicate again that the approach using the ann model compares favourably to the rest. by analysing the values in table  <dig>  ga+ann also appears more robust and better able to identify correct interactions. however, it should be noted that this model has fewer parameters compared to the others, , hence reducing the solution space for the ea, and, possibly, increasing algorithm performance.

although methods using the s-system model display similar average performance, , ga+es and de+aic obtain the best parameters overall , while, , glsdc has higher value, indicating that the latter is more suitable for a quantitative analysis than the two former, which, despite finding parameter values close to the real ones, can miss smaller values.

performance on noisy data
an important feature for inferential grn algorithms, in a real biological setting, is robustness to noise. we have analysed the behaviour of the algorithms implemented on noisy datasets, and the results are displayed in figures  <dig> and  <dig>  which show the evolution with noise of data fit and parameter quality, using the same type of box plots for significance analysis. figure  <dig> shows average sensitivity and specificity values for the algorithms at the different noise levels.

the sensitivity and specificity criteria allow for a qualitative analysis of results. from the sensitivity point of view, the methods can be divided into three categories: with  stable sensitivity values ,  decreasing sensitivity with noise , and  increasing sensitivity with noise . specificity values, on the other hand, decline with noise for all methods, which is explainable by the fact that algorithms concentrate on finding null interactions, so the number of true negatives discovered decreases with noise. however, the first two categories seem to exhibit significantly better behaviour than the third. this explains why peace <dig> achieved a maximum sensitivity with maximum noise: a very small proportion of parameters were found to be null, so almost all genes were found to interact. this results in a large number of true positives, however, accompanied by a very large number of false positives, which is not desired here.

the quantitative perspective has been analysed using the two criteria in figures  <dig> and  <dig>  for peace <dig>  both data and parameter fit are inferior to the rest, indicating limited ability to handle noise. however, only data mse differences are statistically significant at all noise levels. the other four methods are stable and have comparable performance up to 5% noise, . concerning the 10% noisy dataset, two trends can be indentified: glsdc and ga+ann decrease the data fit but preserve a good parameter quality , while for de+aic and ga+es both data fit and parameter quality decrease significantly. this means that the former set have the ability to find good parameters in spite of noise, while the latter over fit the noise in the data, implying low quality solutions. good performance may be, in the case of ga+ann, due to the nature of the ann model, which has been proven to cope well with noise in multiple practical applications,  <cit> , while glsdc has a mechanism built in the local search phase that specifically handles noise.

in conclusion, the ann model and the glsdc mechanism for controlling noise seem to give good quantitative results even with a high noise rate. the best balance for sensitivity-specificity is achieved with ga+ann, while ga+es, de+aic and glsdc exhibit the best qualitative behaviour with noise under the s-system model, .

scalability
scalability analysis was performed on four synthetic datasets corresponding to four different networks:  <dig>   <dig>   <dig> and  <dig> genes. for these data, quantitative results using box plots are displayed in figures  <dig> and  <dig>  while the best qualitative results of all runs are shown in figure  <dig>  given small sensitivity on the  <dig> and  <dig> gene datasets , and the dimensionality achieved by the authors themselves, , no further runs were performed with peace <dig> for the larger datasets. ga+es was run on the 10-gene dataset with low performance , while on the 20-gene dataset, having doubled the allocated memory for the java virtual machine, one generation lasted approximately  <dig> hours, and, after  <dig> generations , the best fitness value was  <dig> e <dig>  this indicates that this method does not scale very well in a single cpu setting, and was thus discarded from the analysis. for the three methods that analyse one gene at-a-time, we performed experiments on a limited number of genes, , and averaged criteria values on them. the results obtained in this way are indicative of the performance of the methods for all the genes in the network. the rest of this section concentrates on these three methods.

due to the characteristically low connectivity of the networks, all methods analysed displayed good specificity, . however, the sensitivity values tend to decrease with the increase in size, which indicates that, for larger networks, these methods tend to set more and more parameters to zero, so that more interactions are missed. however, the number of false positives remains small. ga+ann maintains a good qualitative performance up to  <dig> genes, while de+aic and glsdc display good behaviour with the 10-genes dataset, but do less well as the size of the gene set increases. on the  <dig> gene dataset, all methods perform poorly, with respect to the sensitivity values.

in order to analyse the quantitative behaviour of the methods implemented, values for two criteria were provided: ability to reproduce data  and parameter quality . considering the fact that each benchmark dataset has a different number of parameters to be inferred, of which most are zero, the parameter mse displayed in figure  <dig> is computed per gene rather than per parameter. given the similar connectivity of the four different networks , this offers a good measure of parameter quality that neither depends on the number of genes in the network, which would have been the case if we had chosen the total squared error, nor is biased by the large number of null parameters usually discovered by the algorithms.

as figure  <dig> indicates, all methods, except for those eliminated from this analysis after the first two experiments, , display a good data fit for all datasets. however, de+aic exhibits a significantly better data fit at all scales.

ga+ann achieves good parameter quality, , up to  <dig> genes, confirming conclusions from the qualitative measures. de+aic exhibits a behaviour comparable to ga+ann up to  <dig> genes, but displays lower parameter quality for  <dig> genes, possibly due to the limited data. the superiority of the first method could be partly due to the smaller number of model parameters, , compared to the other methods, the resulting system being less markedly under-specified than in the case of s-systems and the solution space being reduced.

in conclusion, the method using the ann model displays superior behaviour again with larger networks, while the methods that analyse the whole system at the same time failed to scale up for a single cpu situation. the other two methods behaved reasonably up to  <dig> genes, indentifying the most important interactions to enable them to closely reproduce the synthetic time series.

real dna microarray data
in order to assess performance of the chosen algorithms on real microarray data, the spellman dataset  <cit>  was used, which has become a benchmark for validating this type of method. this contains  <dig> time points measured during two saccharomyces cerevisiae cell cycles. the known interactions between genes and proteins were retrieved from the kegg,  <cit> , database for validation purposes. three subsystems of this network were analysed; two small-scale  and one medium-scale network, , of which the former were subsets, . the two small-scale networks contain the genes known to be involved, respectively, in the regulation of genes cln <dig> and pho <dig>  the large-scale analysis focused on these two genes as well, to investigate how inclusion of additional genes, either not connected or distantly linked to the initial system, influences algorithm performance. the algorithms were applied five times for each gene under analysis.

due to noise and the limited number of time points available, it is possible that, although a model is capable of reproducing the experimental data, the connections identified are false positives, and the model invalid. we have analysed the connections obtained, using data from the kegg database and previous descriptions of the cell cycle from the literature,  <cit> . table  <dig> displays the percentage of known interactions out of the total number of interactions identified by each algorithm in each experiment. the remaining percentage of the interactions predicted are clearly wrong, . both overall values and values corresponding to the fittest individual over multiple runs are presented, in order to facilitate a global view over algorithm performance. these known interactions considered correspond not only to transcriptional activation or repression, but also protein interactions, , that activate or repress transcription factors, hence influencing gene expression. for example, it is known that cln <dig> and cdc <dig> work together to activate, , transcription factor sbf, , which in turn activates gene cln1/2; hence, cln <dig> and cdc <dig> can also be considered as activators of these genes. the methods implemented often identify this type of interaction. table  <dig> presents the average number of previously known direct interactions missed by each algorithm in each experiment.

note that, for some methods, the fittest individual identifies fewer interactions than the overall value, which confirms that good ability to reproduce data does not necessarily correspond to a model containing biologically relevant connections. qualitative analysis indicates that, for the small networks, where all the genes are known to interact, the connections identified by the best-fitting methods are mostly correct. for the 7-gene experiment, two of the known interactions, , have been consistently assigned parameters with the wrong sign, by all the methods, in multiple runs. this indicates noise interference, which explains lower values compared to the similar 6-gene experiment. glsdc, however, seems to identify a number of interactions comparable to the 6-gene experiment, which confirms that it is more robust to noise than the others. ga+es and peace <dig> also seem to correctly identify many interactions, but, the fact that the simulated gene values are highly dependent on the rest of the network, means they are unable to reproduce the experimental data.

introducing more genes into the analysis triggers a different response from each method and gene analysed. in the pho <dig> experiment, the percentage of correct interactions identified by ga+ann and de+aic decreases markedly when analysing more genes, while the amount of overlooked direct interactions increases, although data fit remains very good or even increases, . this relies on connecting nodes that are not immediately linked in the real network, and, given that a large part of the added nodes may not be connected at all in reality, this leads to a low percentage of true positives. ga+ann suggests a positive auto-regulation of pho <dig>  both with the small and large dataset, which can compensate for other missed interactions, and explain the improvement in data fit for the larger network. on the other hand, gldsc maintains both quality of data fit, , and percentage of interactions, and adds fewer false interactions outside the pho gene family, . this suggests that, when the added nodes are not connected to the existing ones, the algorithm is better at finding correct qualitative interactions, although fit obviously suffers.

in the second experiment, where most of the new nodes are connected to the initial network, ga+ann and de+aic perform better both from the data fit and validity of interactions point of view. however, the number of false positives increases when moving to the larger dataset. gldsc finds many effects of pho genes on cln <dig>  but these are not biologically plausible. at the same time, when moving to the larger dataset, it correctly adds a positive effect from fus <dig>  that affects the gene through far <dig>  but fails to identify the sbf complex  as an activator. the fact that it does not succeed in identifying the main activation link explains the poor performance when reproducing the data. de+aic and ga+ann preserve the connections from swi <dig>  swi <dig> and cln <dig> from one analysis to the other, but at the same time add some false connections to pho <dig>  pho <dig> and apc/c.

all in all, the results indicate ga+ann and de+aic as better choices when a continuous simulation of the system is required, with less concern for qualitative analysis of connections, . gldsc seemed to identify correct interactions in most experiments, but, however, is not able to reproduce the data as well as the other two methods. the methods aiming to analyse all genes simultaneously displayed very poor performance in reproducing the data, although succeeded in qualitatively identifying some correct interactions for the small-scale datasets.

single versus multi-objective optimisation
as clga  and moga , described in , were found not to be suitable for large networks, they were compared only to each other in a small network setting, i.e. a two-gene grn. the approach used in moga is to split the squared error fitness of clga into separate objectives for each gene. hence, in our experiments, we had  <dig> objective functions to minimise. the aim of this experiment is to compare clga with this multi-objective  approach and to identify the benefits of introducing fuzzy domination. the results of this experiment should be indicative of the improvement of other, more advanced ea approaches, when using mo optimisation.

in order to ensure the validity of our comparison we performed twenty  <dig> -fitness call runs for each of the three algorithms and the results are summarised in table  <dig> and figures  <dig> and  <dig>  the averaged values in the table have been computed after eliminating the worst two and best two of the results for each algorithm.

a more general observation is that, if we perform two rankings of the  <dig> solutions obtained, , results differ, for all three methods. so, improved fitness does not necessarily mean better parameters. this indicates that some parameters may be more important than others, so that a slight change in the values of the more meaningful ones strongly influences the ability of the model to reproduce the data. another argument for this is the observed difference between the robustness of kinetic orders and that of rate constants, which suggests that the latter can vary more without affecting goodness of fit too much. these observations also suggest that alternative models are possible, so that more precise discrimination is needed.

in conclusion, we have shown that, splitting the squared error objective into smaller sub-objectives, for a mo approach, significantly speeds up convergence for eas. nevertheless, after a large number of iterations, final results are comparable. this could be due to the fact that this approach forces the algorithm to fit all parts of the time series at the same time, instead of allowing it to converge more slowly by improving only some of the objectives, which is an advantage, especially when dealing with large dimension problems as performing a very large number of iterations is not viable. this suggests that, even when analysing only one gene at-a-time, we can still split the time series into shorter parts, to speed up convergence in a mo setting. further analysis, to investigate to what extent this objective division is useful and at what point the overhead becomes greater than the gain, would be valuable.

divide et impera?
two different approaches for grn model parameter inference are advocated in the literature: finding relations for the entire network,  <cit> , or analysing a single gene at-a-time,  <cit> . among the methods implemented in this work, three use the latter . an obvious question is which of the two approaches is more reliable.

the argument in favour of division found in the literature is increased scalability due to decrease in number of parameters, , and ease of solution evaluation, as only the time series for the current gene needs to be simulated. however, these arguments do not take into account the fact that this method has to be iterated for all genes, so, ultimately, the number of parameters and the number of simulated time series is the same, . also, when simulating one series at-a-time, the values of the rest of the genes are considered to be those of the experimental data. however, the effect of the current gene on the others is not taken into account, and this can give the impression of finding a good solution when, in reality, the difference between the data and the simulation in a whole system setting could be larger. this effect is exacerbated for real noisy data. in order to compensate for this disadvantage, a complete network analysis can be performed, to fine tune the parameters obtained for each gene in each sub-problem.

in order to avoid the resource problem and be able to scale up even when analysing the entire network simultaneously, parallelisation is clearly desirable. in a parallel setting, division loses its advantages, becoming less viable than the complete network analysis, which can be parallelised in a more convenient way, to avoid simulating only part of the network when evaluating individuals.

during our experiments, division proved to be more useful when analysing real data, statistically significant differences being observed in one of the small scale experiments. nevertheless, in both of these experiments, probably due to noise, the two methods analysing the complete networks failed to reproduce the time series, even for a small number of genes. however, a more detailed analysis, in a multi-cpu setting, is required with respect to their behaviour with real microarray data.

inclusion of prior knowledge
although microarray data provides measurements for a large number of genes, the number of time points available is usually not enough for a quantitative analysis of the underlying grn  <cit> . a very large pool of biological knowledge and prior information on possible interactions exists in the literature, but the effort made to integrate these has been sparse so far. eas in general, and in particular these approaches implemented, have the benefit of flexibility in terms of adding prior information to the optimisation process. this can be done at several stages, such as initialisation, fitness evaluation, mutation or crossover, etc. an example of integrating biological knowledge in the algorithms implemented is using the sparsity of the grn, . further improvement could be introduced in these algorithms by adding additional knowledge, .

for instance, previously known interactions could be introduced during initialisation, and links maintained until the end of the optimisation . in the same manner, statistical information on possible interactions, obtained by preliminary  analysis of gene expression data,  <cit> , can be integrated in the optimisation to accelerate convergence and improve solutions. in particular, the nested optimisation algorithms implemented, , could benefit from this type of knowledge, as structure is already separated from parameter values during optimisation, and this could help avoid evaluation of completely impossible structures, .

similarly, binding affinities and gene sequence structure could boost performance for the algorithms. this type of knowledge has been used before with a bayesian model  <cit> , however, not with eas, to our knowledge. the prior information can be used both in initialisation and during fitness evaluation . this can be easily introduced in any of the methods presented here.

producing long time-series experiments is very costly and not feasible for most laboratories. however, short series from different sources, but describing the same process, are available. nevertheless, no efforts have been made to combine these for model inference. it is possible, by using adequate normalisation techniques, to combine these heterogeneous datasets, and be able to model the common features. the same gain could be obtained by fitting different replicates of the same experiment as a separate time series. this should also increase the ability of algorithms to handle noise, as, by combining data with heterogeneous perturbations, over fitting of the noise is reduced.

CONCLUSIONS
this article presented a comparison of existing methods of inferring parameters for continuous models of gene regulation, based on dna microarray data. we have implemented seven algorithms  and compared these for different time series data sets in order to analyse their behaviour under a common framework. the main aim was to identify which methods perform better under different grn criteria, in order to assess directions for improvement.

a first observation derived from our experiment is that pure evolutionary algorithms are powerful enough to analyse only very small-scale systems, as found for clga and moga. in order to increase power, hybridisation is typical and results show that hybrids are suitable for larger networks. we have shown that the methods implemented can achieve good performance up to  <dig> genes.

we applied five of the methods to real microarray experimental data, which had been considered only for de+aic and ga+ann, to date, and, for the latter, in a discrete setting only. ga+ann and de+aic proved to be capable of closely reproducing the original time series even for a larger dataset, , while identifying, at the same time, some of the known interactions in the data. glsdc also identified known interactions, but had limited ability to reproduce the data. the two methods analysing the entire network simultaneously, , failed to reproduce real data, which suggests that existing methods are not as yet capable of simulating the entire network in a real experimental setting, even when analysing small-scale systems.

we have shown that splitting the evolutionary algorithm objective into smaller sub-objectives, , speeds up convergence. this suggests that, even when analysing only one gene at-a-time, we can still split the time series into shorter parts. furthermore, we believe that using multi-objective optimisation along with a hybrid approach can improve learning performance.

importantly, it should be noted that parallel implementation of the evolutionary algorithms is necessary, . hybrid methods are computationally expensive and, although these work well with small networks on a single machine, they tend to become less efficient for larger networks, especially those analysing the entire network simultaneously. in order to achieve scalability, parallelisation can be performed at several levels, ranging from individual evaluation to iterations and division of the entire problem into sub-problems.

a very important issue with gene regulatory network inference from microarray data is both the limited and noisy nature of these data. this indicates the need to use time-series from different sources and other types of biological data, , in order to underpin relationships between genes. these data include chip chip data and binding affinities, which identifies which proteins bind to which genes, indicating possible interactions,  knockout microarray experiments, which allow for mutant behaviour to be analysed,  protein-protein interactions, which indicate groups of co-regulated genes,  mirna interference data, which indicates other causes for a gene to be under-expressed. these data can be potentially included in the evolutionary algorithm in a multi-objective setting, in order to speed up convergence.

list of abbreviations
ea: evolutionary algorithm; es: evolution strategy; ga: genetic algorithm; grn: gene regulatory network; ann: artificial neural network; mse: mean squared error; mo: multi-objective; ga+es: method nesting a genetic algorithm with an evolution strategy; aic: akaike's theoretic criterion; de+aic: method using differential evolution as a search strategy, and aic-based fitness; glsdc: method using genetic local search; peace1: iterative algorithm based on ga; clga: classic ga; moga: multi-objective ga; ga+ann: method using an ann as a model and ga for parameter inference.

authors' contributions
as implemented the methods in java and applied them to the different datasets. all three authors participated in the design of the experiments, interpretation of results, composition and drafting of the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
implemented evolutionary algorithms for gene regulatory network inference. this pdf file gives details on the  <dig> algorithms implemented and analysed here.

click here for file

 additional file 2
using the framework. this pdf file provides information on downloading and using the java implementation for algorithm comparison.

click here for file

 additional file 3
eva <dig> framework. this archive contains the code and resources published by eva <dig> authors . more details on how to use it can be found in additional file  <dig> 

click here for file

 additional file 4
algorithm implementation. this archive contains the code for the seven methods implemented . more details on how to use it can be found in additional file  <dig> 

click here for file

 additional file 5
data sets. an archive containing the datasets used for the experiments presented here.

click here for file

 acknowledgements
this work has been developed with support from the irish research council for science, engineering and technology 'embark initiative'.
