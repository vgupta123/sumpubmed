BACKGROUND
second-generation sequencing technology has provided deep insight into the complexity of the transcriptome. early sequencing of cellular mrna resulted in a level of transcript quantitation that was in broad concordance with microarrays  <cit> . subsequent studies with improved mapping tools  <cit>  and increasingly deep sequencing depth  <cit>  suggested that, with enough depth of coverage, most annotated genes could be observed at some level. a key unanswered question, however, is whether these low-abundance transcripts are biologically significant  <cit> .

a recent study by hebenstreit et al.  <cit>  demonstrated that gene expression in mammalian cells measured by rna-seq follows a bimodal distribution of high and low expression genes, and suggested that the high-expression genes comprise the active, functional transcriptome of the cell. the results of several studies constrain the range of the threshold that divides active from low-expression genes: at the upper bound, hebenstreit et al. and mortazavi et al.  <cit>  calculated that fragments per kilobase of gene model per million mapped reads  values of  <dig> to  <dig> correspond to ~ <dig> mrna molecule per cell, though a deep proteomic sampling of hela cells detected proteins from several genes expressed below this level  <cit> . at fpkm of about  <dig> , rna-seq reads were shown to map to exonic regions and intergenic regions at similar rates  <cit> , suggesting lower confidence in measured expression below this level. however, the data used in these studies were from short read rna-seq  of moderate depth . advances in rna-seq library preparation and sequencing technology now regularly yield tens to hundreds of millions of paired-end reads of  <dig> to  <dig> or more bases in length. increased read length improves mapping accuracy and lowers the odds of spurious multiple mapping, while greater read depth allows more accurate assessment of the relative abundance of low-expression transcripts as well as the detection  of a greater number of genes  <cit> . these advances undermine the assumptions upon which previous heuristics for evaluating gene expression were based, highlighting the need for updated understanding of the signal and noise present in rna-seq data.

in this study, we integrate current-generation rna-seq and chromatin state data from the encode project to understand the relationship between gene expression level and promoter activity signatures. we explore the effect of varying read depth on transcript detection and quantitation, and offer a novel normalization method that robustly identifies the subset of active genes observed in an rna-seq experiment and provides guidance regarding efficient experimental design.

RESULTS
expression state from chromatin state
we examined the transcript levels of  <dig> human cell lines from the encode  <dig>  rna-seq data set. using the tophat/cufflinks pipeline  <cit> , we determined gene expression levels of ~ <dig>  protein coding genes, using gencode gene models  <cit>  . in all cases, and consistent with prior studies, the log <dig> distribution shows a primary peak of high expression genes, with a long left shoulder of low-expression transcripts .

an important question arising from this observation is whether the low-expression transcripts of the shoulder are comprised of functional genes or merely by-products of leaky gene expression, sequencing errors, and/or off-target read mapping. to explore this question, we compared gene expression profiles to the results of an integrated analysis of chromatin state derived from encode chip-seq data  <cit> . each gene promoter was tagged as "active" or "repressed" based on the local chromatin state . genes were rank-ordered by expression level, binned , and the fraction of genes in the bin with either active or repressed promoters was plotted against the genes’ mean expression level . as expected, ~100% of highly expressed genes have active promoters. however, transcripts detected at low levels tend to be associated with repressed promoters, suggesting that they do not play a functional role in the cell.

we judged that a reasonable expression cutoff describing the active genes in a cell would be the point where the ratio of active to repressed promoters drops below  <dig>  identifying this point by linear interpolation yielded fpkm values from  <dig>  to  <dig>  or log <dig> values from − <dig>  to − <dig>  across the  <dig> encode samples, a three-fold range of expression . however, some of the variability in these values is explained by small positional shifts in the log <dig> distributions. to normalize the distributions, we fit the right half of each gene expression curve to a half-gaussian curve, mirrored the half-gaussian into a full gaussian distribution, and transformed log <dig> into zfpkm derived from this fit . after applying this transformation, and removing an outlier, we find that the active/repressed promoter threshold is zfpkm − <dig>  +/−  <dig>  . thus the zfpkm transform can be used with gene expression data alone to determine with high consistency the range of gene expression defined by active chromatin. hereafter, we define this threshold as zfpkm > = − <dig>  preferring to err on the side of capturing too many noisy genes rather than too few active ones.

log
 <dig> 
the distribution of log <dig>  expression for each sample was calculated and the right side of the major peak was fit by a gaussian distribution with parameters μ and σ. the threshold of active gene expression, defined as the intersection between the linear fit of the active promoter fraction and the repressed promoter fraction, was calculated in log <dig> and zfpkm.  h <dig> embryonic stem cells were removed as an outlier.

data from the encode cell lines is the product of a controlled set of experimental and analytical protocols. it is therefore not surprising that the fpkm distributions are highly consistent; in fact, the normalized zfpkm threshold of − <dig> corresponds to a raw fpkm in a fairly tight range of  <dig>  to  <dig>  across the  <dig> encode cell lines. however, many if not most other data sets lack this internal consistency. figure  <dig> shows the log <dig> distributions from several public data sets, including the illumina bodymap set of  <dig> healthy human tissues, pancreatic cancer rna-seq from icgc , and the recently published geuvadis project rna-seq of  <dig> lymphoblastoid cell lines derived from different individuals  <cit> . we fit a gaussian to the major peak of each distribution and plotted the mean and standard deviation of each fit . the resulting scatter plot demonstrates the variability of some rna-seq data , and strongly signals that a single heuristic for such diverse data may not be appropriate. the zfpkm approach offers a useful data normalization strategy in these cases.

while we do not have corresponding information on chromatin state for these samples, other cell line data do corroborate the relationship between promoter activation level and gene expression in the major peak. additional file 2: figure s <dig> shows rna-seq distributions and corresponding paired histone h3k <dig> trimethylation chip-seq data. as with the encode chromatin state data, the fraction of genes with promoter-associated h3k4me <dig> is high for genes expressed in the primary peak and drops to negligible levels for transcripts detected at trace levels.

the zfpkm threshold is robust to changes in read depth
to evaluate the robustness of the zfpkm transform, we applied it to rna-seq data derived from different read depths. human cd4+ memory t cells were costimulated with anti-cd3/cd <dig> beads for 48 hours and rna-seq was performed using the illumina platform, yielding a total depth of ~ <dig> million mapped reads . subsets of reads, with depths at  <dig>   <dig>   <dig>   <dig>  and  <dig> mmr, were analyzed using the same pipeline. increasing read depth has two main effects on the log <dig> distribution: it increases the proportion of mass in the noisy left shoulder , and it subtly shifts the main peak of the distribution . this occurs because, as deeper sequencing discovers new transcripts , each doubling of mapped reads is divided across a larger number of genes, thus subtly lowering the inferred fpkm of moderate-expression genes . the zfpkm transform normalizes this shift and captures essentially the same set of active genes  across all read depths, with a coefficient of variation of less than  <dig> %.

as noted in this study and others  <cit> , greater read depth increases the total number of genes detected , with newly discovered genes tending to show very low expression . the corresponding fraction of newly detected genes that are expressed in the active region drops rapidly with read depth . beyond  <dig> mmr, though transcripts from over  <dig>  new genes are putatively detected, only  <dig> are observed in the active region. this suggests 20– <dig> mmr is a reasonable target for rna-seq studies of gene expression, as it captures virtually all active genes in a sample while allowing sample multiplexing on sequencing machines to reduce costs. this result is consistent with encode recommendations for rna-seq best practices  <cit> ; moreover, at an expression level of log <dig> of − <dig>  , this read depth yields ~ <dig> mapped reads per typical 3 kb transcript, the minimum coverage recommended for analysis of differential expression using count-based statistics  <cit> .

other normalization methods have been proposed to deal with the change in calculated fpkm induced by, e.g., changes in read depth and mapping quality. one such option is transcripts per million , implemented in the rsem software package  <cit>  and used to compute gene expression values in, e.g., the cancer genome atlas  <cit> . while the tpm transform should in principle be more stable than raw fpkm, the software implementation  calls bowtie with lax mapping parameters that result in dozens to hundreds of genes being called highly expressed in one pipeline vs. trace or zero expression in the other. additional file 2: figure s <dig> shows the tophat/cufflinks-derived fpkm vs. rsem-derived tpm for nine encode cell lines and highlights the genes unique to each pipeline. comparing the fraction of active and repressed promoters among these genes suggests that the default tophat/cufflinks pipeline delivers more accurate results , and that end-users should carefully consider the command line parameters when using rsem as a wrapper for bowtie.

CONCLUSIONS
second-generation sequencing technology has provided a detailed view of the transcriptome. assays that previously required multiple platforms, or which were simply not available, now can be performed from a single sequencing data set; e.g. transcript quantitation, isoform identification, alternative splicing and transcription start sites, allele-specific transcription, and discovery of novel transcripts. for common assays of gene expression, however, the remarkable sensitivity of rna-seq has generated many questions regarding how to most efficiently design an experiment and analyze the resulting data.

previous transcriptome studies have suggested that many rare transcripts may be the product of biological noise, although few have provided evidence that these products are non-functional. we show that low-abundance transcripts are associated with chromatin signatures consistent with repressed promoters, and we provide the zfpkm normalization method that accurately determines the expression regime defined by genes controlled by active promoters. the method provides several advantages over widely used heuristic approaches of accepting expression values above a fixed threshold, typically fpkm values ~  <dig>  we show that, while most human rna-seq experiments yield similarly shaped distributions of gene expression values, different samples and experimental protocols can result in pronounced changes in the location and scale of these distributions that add variability to the results from the application of such heuristics. in the more extreme cases, however, it would be worth carefully re-evaluating the quality of the primary data before applying any normalization techniques.

with a finite population of biologically active transcripts in a cell, it stands to reason that experimental methods can be optimized to provide requisite coverage of those transcripts while maximizing the multiplexing capability of a sequencer. our work shows that 20– <dig> million mapped reads are sufficient to detect virtually all active transcripts in a cell line, and provides deep enough coverage to undertake analysis of differential expression across the bulk of the active transcriptome. rna-seq at ever greater depth continues to detect new transcripts, but the overwhelming majority are expressed at trace levels and, in the encode data, are associated with repressed promoters, indicating that these are not biologically active genes.

it is worth noting that these results are derived primarily from homogeneous samples of human cell lines. heterogeneous samples present their own set of challenges. a gene that is moderately expressed in a small fraction of cells in a sample might be indistinguishable from the background transcripts of the whole sample. at the other extreme, an equal mix of two or three cell types would likely result in a similar top-end distribution of constitutively expressed genes but an enlarged left shoulder of tissue-specific genes whose observed expression is reduced by averaging over the whole sample. while none of these issues are unique to rna-seq—microarray studies have long faced the same problems—there may be an opportunity to formally quantify this behavior by in silico combinations, across a range of proportions, of the encode matched transcript and chromatin state data from different samples.

more broadly, the encode data provides a unique and comprehensive data set from which to evaluate the quality of rna-seq studies generally. having independent chromatin state data for multiple cell lines provides a vital "ground truth" against which to measure the performance of rna-seq analysis tools. we point to the differences between rsem and tophat/cufflinks quantitation presented here as a case study for using this framework to evaluate computational methods against real-world data.

sequencing technology has evolved significantly since the early proof-of-concept rna-seq studies. through a combination of bioinformatic and biochemical advances, modern rna-seq data represents a deeper and more accurate sampling of the transcriptome than the moderate-depth, short-read data from which many current rules of thumb for analysis were derived. improved library prep techniques have increased the fraction of total sequenced bases that map to mrna and reduced the bias toward reads mapping at the 3′ end of known transcripts, while splice-aware mappers align longer reads with greater accuracy and less likelihood of multiple hits. the net result is that many of the features of early rna-seq data which drove the development of heuristics in use today are not always applicable. we evaluate latest-generation data and offer an updated framework for extracting relevant gene expression information from rna-seq experiments.

