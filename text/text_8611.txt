BACKGROUND
hidden markov model  decoding is a basic problem in sequence analysis, as hmms are used throughout the field to divide discrete sequences into regions corresponding to features. hmms decode a sequence by assigning each position in the sequence a label; intervals with the same label then correspond to the same feature in a sequence.

two common decoding procedures for hmms are the viterbi and posterior algorithms. the viterbi algorithm computes the maximum probability path through an hmm, and returns the labelling of the states of that path. many states may share a single label. while it is easy to compute this single path, it often gives poor annotations  <cit> . we might want to compute the labelling of a sequence that has maximum probability, but that is np-hard  <cit> . posterior decoding computes the most probable state or label of each position of a sequence and joins those together into a single labelling. this is quickly computable, but has no guarantee that it will actually correspond to a feasible labelling of the sequence, since it may not satisfy the constraints of the model.

recently, variant posterior methods have also appeared, which seek to maximize the expected number of positions in the sequence that are correctly labelled, or the geometric mean of the probability that a position is correctly labelled, while requiring that a labelling has nonzero probability  <cit> . however, the overall labelling may be of extremely low probability relative to the true explanation; again, it is np-hard to maximize the overall labelling probability  <cit> . also, several heuristic algorithms, such as the 1-best by krogh  <cit> , exist to work around these limitations, but do not guarantee optimality.

we study an alternative approach: we investigate the feasibility of computing the k most probable paths, and how examining the labellings corresponding to these paths can serve as a good alternative to more traditional hmm procedures. we have used the hmm for the phobius  <cit>  transmembrane topology predictor to investigate the usefulness of the k-best paths in a real world example and show that alternative paths can provide a wealth of information. we show that k-best paths can be good predictors and investigate ways to extract such predictions. we also show how to judge our confidence in a particular prediction by looking at the alternatives that result from the k-best paths.

finally, we investigate the use of the k-best paths to predict more then one topology in cases when it is biologically proven that such alternatives exist.

notation and definitions
a hidden markov model is a probabilistic generative automaton that produces a sequence while traversing stochastically through a finite set of states. an hmm is defined by a collection of parameters: a start state i, a set of transition parameters , and a set of emission parameters ). let m be the number of states. a path π through the hmm is a sequence of states π <dig> = i, π <dig>  π <dig>  ..., πn such that for all i, . for each path π of length n +  <dig> the probability that this path emits sequence x = x1x <dig> ... xn is . the viterbi algorithm finds the most probable path through the hmm for a given sequence. the natural implementation of the viterbi algorithm  uses dynamic programming to construct a Θ sized matrix, in which cells correspond to state-position pairs. there is a natural extension to finding the k-best paths in the hmm: store the k highest-scoring paths for each position-state pair. the k highest probability paths for x <dig> .. xi that end in state πi must be from the k-best paths to each of the states for the sequence from x <dig> to xi- <dig>  this observation leads to a viterbi-like algorithm with runtime k times as large, and requiring Θ storage. this approach has been used in speech recognition as early as the 1990s  <cit> , but its space requirements make it infeasible for finding the k-best paths for large values of k on substantial hmms for long sequences.

alternatively, finding the k-best paths though an hmm can be seen as finding k shortest paths in a graph with nm nodes . the graph has o edges, each edge corresponding to a potential transition between hmm states. on this graph, one can apply eppstein's algorithm  <cit>  to find k shortest paths in o time. this algorithm keeps the whole Θ-size graph in memory as well as an Ω size structure used in the algorithm; it also has high constant factors in the runtime and only implicitly represents the paths.

a space savings approach for viterbi was presented by sramek et al.  <cit> , which uses a compressed tree approach to actively free memory used by unnecessary back pointers in the state-position matrix. we create an m by n +  <dig> grid of nodes where each node corresponds to a cell of the viterbi matrix : each column corresponds to the m possible choices for the last state in a prefix of x. we create an edge between node vi of column i and node vi+ <dig> of column i +  <dig> if the viterbi path to the position-state pair  goes through vi. edges give the back pointers in the dynamic programming for the viterbi calculation. if we remove all nodes and edges that are not on paths from column n of the graph to state i in column  <dig>  what remains is a tree. in the approach of sramek et al., this tree is actively pruned, and nodes with exactly one parent are merged into their parents as they are created. an example of a compressed tree is in figure  <dig>  after compression, each node corresponds to a sequence of states which emits a particular substring of the given string, found in a potential viterbi path to one of the leaves of the tree. for details of how to maintain a compressed tree efficiently see  <cit> .

methods
the viterbi algorithm  actually consists of two different calculations: computing the probabilities of the best path  to every state for every prefix x <dig> .. xi of a given sequence x <dig> .. xn, and also storing the pointers necessary to reconstruct those paths. our primary emphasis is to store enough information to reconstruct these pointers for the final paths in as little space as possible, again using an active pruning procedure, which becomes a bit more complicated with k paths than with just one. using this method, we typically use much less memory than the Θ required by the obvious approach.

computing the probabilities
to compute the k highest path probabilities to each state at position i, we assume that we have a sorted list of the k-best path probabilities to each state at position i -  <dig>  then, if we are considering a state a whose possible predecessors in the hmm are pred, we can find the k-best probabilities for state a at postion i by performing an operation very similar to the first k steps we would undertake in merging |pred| lists for mergesort. the viterbi probability of the ℓ-th best path to state v is  

we can compute this quantity in o time per path; it is an interesting algorithmic question whether this can be sped up heuristically, since all paths to state v that were in state c at position i -  <dig> will have their probabilities multiplied by the same constant, acv bv.

this calculation, then, takes k times the cost of a standard viterbi calculation and Θ space. by contrast, the posterior algorithms yielding non-zero probability paths of fariselli et al.  <cit>  and of kall et al.  <cit>  run asymptotically as fast as viterbi, but yield only a single path, which may be overall an odd one from a macroscopic view.

storing and pruning the paths
the k best paths can be computed by using a three-dimensional grid, where entry  corresponds to the a-th best path to state j for sequence x <dig>  ..., xi. treating this grid as a graph, we draw an edge from  to  if the b-th best path to state k at position i +  <dig> uses the a-th best path to state j at position i. as in the viterbi algorithm, we wish to actively maintain only the entries in the graph that correspond to paths to the current frontier, position i of the sequence.

we will describe two types of nodes in the graph: path nodes, which correspond to a single value of , and state nodes, which correspond to all paths  where i and j are kept constant . an edge between two state nodes exists if any of their path nodes have an edge.

as we move forward in the sequence, we compute the k best paths to each state at a new position i, drawing edges in the structure to the predecessor of each. then, we prune and compress. first, we must prune away all of the path and state nodes from the previous level that are no longer found on paths to a leaf ; see figure 2b. then, we merge pairs of consecutive state nodes for which all paths from one state node go to the other and vice versa; see figure 2c.

each path node stores its number of children and pointers to its state node, and the path node that is its parent. each state node stores the list of hmm states associated with it , the list of at most k path nodes it includes, pointers to its children, and the number of state nodes that are its parents. we can find the parents of a state node by examining its associated path nodes and identifying the state node corresponding to the parent of each path node.

a state node is deleted if it has no path nodes, and it is merged with another state node if that node is its only parent and it is the parent's only child. this corresponds to the situation where all paths that include a sequence of states πi... πj for the subsequence si... sj are followed by the same set of states πj+ <dig> ... πk for sj+ <dig> ... sk, so we can merge the sequences together.

pruning and compressing details
suppose we are about to incorporate sequence letter, xi. we calculate the probability of all k best paths to each possible state. if the ℓ-th best path to state c uses path node  at the previous step , then we set that node to be the parent of , updating the child counters for , and we also set the state node  to have  as a child.

after performing this set of operations for all the new graph nodes corresponding to sequence position i , we prune all nodes not reachable from the new leaves, by seeing which leaves at level i -  <dig> are no longer accessible. for each path node in this "removal list", we remove the path node from its state node, and update appropriate counters. if its parent's child counter reaches zero, it is moved to the removal list as well. if a path node removed was the last path node for that state node, then the state node is removed. we also detect if a state node enters the condition that it has only one child and its child has only that one parent: if so, the states are merged.

because this algorithm is done online, the active footprint of memory used by the algorithm is dramatically less than Θ in practice, though it may be that large in extreme cases.

recovering the paths
once we have produced the final structure, we must extract the k paths with highest probability. the k path probabilities at each of the m leaves are the probabilities of the best paths to those states. from these km paths, we select the k of highest probability, as the first k steps of an m-way mergesort, in o time, and we construct the k-best paths then in o time after the merging. the total run time is o as for the naive algorithm, as the overhead in doing the paths compression is smaller then the basic calculations. the is no guarantee that the new procedure is more space-efficient: our heuristic may not always result in compression.

RESULTS
having k different high-probability explanations for a sequence might offer some assistance in decoding it. here, after briefly showing that our pruning and compression methods make finding the k best paths for large values of k possible, we explore several different uses for these multiple explanations: first, to see if any of them is a good explanation, second, to see if we can reconstruct a good single explanation from a set of paths, third, to see if the topologies and probabilities assigned by the hmm to the k best paths help us identify easy and hard sequences to decode, and finally  to see if k paths can help us decode sequences with different true explanations.

our experiments use the domain of transmembrane protein topology prediction, where viterbi-style decoding has not been successful. we have used the 188-state hmm from phobius  <cit> , which divides alpha-helical membrane protein sequences into segments corresponding to membrane-spanning segments and the parts found either inside or outside the cellular membrane. the topology of a membrane protein is the number of the membrane-spanning helical regions, along with the sidedness, which is the identification of the first residue of the protein as being either intracellular or extracellular.

we use the data set of proteins provided with phobius  <cit> :  <dig> membrane proteins with no signal peptide and  <dig> proteins with a signal peptide. we focus on the larger of these two sets. note that kall et al. evaluated cross validated data sets, making direct comparisons to their published results inappropriate. we also note that their hmm has been trained for success with the 1-best algorithm, while we are using a quite different decoding approach. in our last experiment, we study five proteins known to have two topologies  <cit> .

a fundamental question for this study is what makes a good prediction. transmembrane topology prediction is somewhat imprecise because the actual boundaries of membrane-spanning segments are inexact, but can be identified to within a residue or so based on solved protein structures  <cit> . the authors of phobius describe a prediction as correct if it identifies the correct topology, and if each true helix overlaps with the corresponding helix in at least five positions. helices tend to be approximately twenty-two residues long, so this measure is lax.

we also studied a more stringent correctness measure, parameterized by a tolerance τ. in it, a prediction is correct if topology is correct, and if all predicted helix boundaries are no more than τ residues away from the true boundary. we used this measure with values of τ from  <dig> to  <dig> in our experiments.

memory and runtime
our algorithms do dramatically reduce the memory use of finding the k best paths in this hmm. table  <dig> shows the maximum memory required in decoding the  <dig> proteins with a signal peptide for different values of k: while these values do grow with k, the memory usage for the tree-based approach is approximately fifty times smaller than for the naive approach; it takes twice as much memory as an approach that only computes the probabilities, and does not store back pointers at all. we note that in this application, we could have used the naive algorithm in the memory footprint of a typical computer. however, we also computed the  <dig>   <dig> best paths for the dual-topology proteins described below, which would not have been possible with the naive algorithm  <cit> .

this value corresponds to the memory required for the longest protein. 'probability only' corresponds to a run where no backtracking was performed, 'tree-based' refers to our tree-based implementation, 'matrix-based' refers to naive matrix approach.

meanwhile, table  <dig> shows the runtimes for these three approaches: our algorithm is the slowest, but the overhead required for pruning and compression has the effect of doubling the runtime of the more naive methods.

finding at least one good labelling
now, we explore the k best paths to see if any of them gives a good labelling. we compare with the results of the 1-best algorithm  <cit> , decoding algorithm for which the model is trained. our results are in table  <dig>  there is much information in the k-best paths; the challenge is in extracting this into a single prediction. for example, we find a good labelling in the set of  <dig> best paths much more often than in the single labelling found by the 1-best algorithm. it is also striking that for  <dig> of the  <dig> proteins , the exactly correct labelling is found among the top  <dig> paths.

for k paths and a correctness measure, we count the number of proteins for which at least one of the top k paths gives a prediction that satisfies that correctness measure. the bottom line gives the results for the 1-best decoding algorithm used in phobius. presented is the data set without signal peptide .

from many labellings to one
once we have identified the top k paths, the next challenge is in summarizing them into a single labelling. we divide the k paths into "groups", where all paths in a group predict the same topology, and then form a consensus from the heaviest group, for which the sum of the conditional probabilities of all of the chosen paths in that group is highest. note that this method cannot succeed when the chosen group is not, in fact, of the correct topology.

many natural ways to form a consensus all gave essentially equivalent results. for example, we might average the positions of start points and end points of transmembrane helices for all paths in a group. this approach always produces consistent results. this method is fairly good at retrieving the information contained in the group, as shown in table  <dig>  for the phobius correctness measure, at most four proteins for which the largest group of k paths gave the correct topology were mis-annotated, after building the consensus . in general, this straightforward approach to moving from k paths to a single labelling did less well than the 1-best algorithm, but better than viterbi, though the results are much closer for our τ =  <dig> measure then for the phobius measure.

other approaches to forming a consensus, such as allowing per-position voting among the k paths on the correct label of a position , or allowing predictions to vote on the start position and length of helices yielded similar results. both of these methods can yield labellings inconsistent with the model after generating a consensus.

using k paths to increase confidence
another potential use of multiple paths is to reinforce our belief that a particular protein is easy or hard to properly annotate.

in particular, we hypothesized that if the top k paths all correspond to the same number of helices , this can be seen as supporting evidence for that number of helices. indeed, this is confirmed by the results in table  <dig>  if the top k paths all give the same number of helices, this prediction is very likely correct, and in the majority of cases, the full topology is also correctly chosen. such proteins almost always form a good consensus.

by contrast, for the proteins where multiple different numbers of helices are predicted, the results are much weaker: the largest group of predictions gives the correct number of helices in 20% to 28% fewer cases, and gives the overall correct topology in 32% to 37% fewer cases.

another use for k paths is to identify proteins for which the top k paths use up a significant part of the conditional probability space of the model, given the sequence. if so, then we hypothesize that their consensus labelling is likely to be good. this hypothesis is confirmed, as shown in table  <dig>  consider the  <dig> proteins where the top  <dig> paths take up more than conditional probability of  <dig>  given the sequence. for  <dig> of them , at least one of those paths satisfies our correctness measure with τ =  <dig>  by contrast, for the  <dig> with total probability of the top  <dig> paths less than  <dig> , only  <dig>  have a path this good. thus, the total conditional probability of the top k paths is a good predictor of the existence of paths with a good labelling among these k paths.

two true answers: dual-topology proteins
an interesting final testbed for our ideas are proteins known to have two topologies; such dual-topology proteins were only confirmed to exist in  <dig> by rapp et al.  <cit> : the five proteins identified are emre , suge , crcb , ydgc , and ynfa .

for all of these proteins, the two topologies differ only in their sidedness: they all are short proteins with four transmembrane helices, and have very little information in their non-membrane segments to indicate which set belong inside or outside the membrane.

the phobius model, by itself, does not give a good prediction for most of these proteins: the topology from uniprot  <cit>  is approximated by any of the top  <dig> paths in the model in two of five cases. one challenge for these proteins is that because they are quite short, the signal peptide module in the phobius model often gives them a bad labelling; if we remove that module, then even the top ten paths gives this one correct topology in four of five cases .

however, much more interesting is the question of finding two good answers. if we look at the top  <dig> or  <dig> paths, then for all of these proteins, the top paths support two or more different topologies. in three cases, superficially ynfa, crcb and suge, the two heaviest groups do give both correct overall topologies, and the consensus of the these paths is correct for the phobius distance measure. for the other two proteins, emre and ydgc, the top two groups are not correct: in both cases one of the two groups incorrectly predicts three helices, not four.

for proteins with two  correct topologies, the top k paths let us explore that space effectively. we expect that similar explorations may also be fruitful in other contexts where multiple correct answers are to be found. we did not investigate the question of how often proteins with a single topology might appear to have two topologies using a similar approach.

CONCLUSIONS
we have developed a memory-efficient algorithm for finding k-best paths in an hmm. considering the k best paths of an hmm is not novel; the idea has been considered by speech recognition experts, for example  <cit> . however, previous algorithms for this have either used too much memory or been heuristic in nature. our method has a significantly lower memory footprint in practice than the naive implementation. using this algorithm we investigated the use of k-best paths in topology prediction for transmembrane proteins. while better than the viterbi algorithm, forming a consensus of the k-best paths does not perform as well as the 1-best algorithm; largely, the issue is in finding the correct overall topology, which 1-best does better, possibly for training reasons. where the k-best paths gives the overall correct topology, we can almost always compute a good consensus prediction. we can extract other interesting data from the k-best paths. in particular, we can estimate our confidence in a prediction by looking at the content and probability distribution of the k-best paths. finally we have shown that in the special case of dual-topology proteins, a simple processing of the k-best paths can often predict both of the correct topologies of a protein.

competing interests
the authors declare that they have no competing interests.

authors' contributions
db conceived of the study, wrote most of the manuscript, and directed the experiments. dg designed most of the algorithms, wrote some of the manuscript, conducted the experiments, and did much of the evaluation.

