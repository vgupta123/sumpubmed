BACKGROUND
multiple-testing problems occur in many bioinformatic studies where we considere a large set of biological objects  and we want to test a null hypothesis h for each object. typically, h may be 'the expression level of the gene is not affected by the treatment' or 'the pattern is as frequent as expected in the observed dna sequence'. the control of the number of false positives, i.e. falsely rejected hypotheses, is the crucial issue in multiple testing. to this end, several error rates, such as the family-wise error-rate  or the false discovery rate , have emerged and various strategies to control these criteria have been developed .

in the last decade the fdr criterion introduced in  <cit>  has received the greatest focus, due to its lower conservativeness compared to the fwer. the fdr is defined as the mean proportion of false positives among the list of rejected hypotheses. it is therefore a global criterion that cannot be used to assess the reliability of a specific hypothesis, i.e. that of a given gene, snp or pattern.

more recently, a strong interest has been devoted to the local version of the fdr, called 'local fdr'  <cit>  and denoted hereafter ℓfdr. the idea is to quantify the probability for a given null hypothesis to be true. even if many different strategies were designed to estimate the ℓfdr, some of them based on the estimation of fdr itself  <cit> , most of them rely on a mixture model assumption  <cit> , which is a general and statistically convenient framework: the score  on which the testing procedure is based follows a mixture distribution depending on the unobserved status of the hypothesis . different approaches have been proposed: fully parametric  <cit> , semi-parametric  <cit> , bayesian  <cit>  or empirical bayes  <cit> .

the semi-parametric approach developed by  <cit>  uses the knowledge of the distribution f <dig> of the score under the null hypothesis, to provide a flexible non-parametric estimation of the alternative distribution , i.e. under the alternative hypothesis. however, some important questions remain partially or not addressed in this reference.

in this paper we provide an implementation of the method with several important and practical generalizations. the results and discussion section recalls the theoretical framework underlying our method, the properties of the estimation algorithm as well as the main steps of its implementation.

performances are then studied via simulations, and compared to other existing methods. finally, applications to various bioinformatic data sets, such as gene expressions, dna sequence patterns and genome-wide associations, are carried out and proposed to the reader

RESULTS
semi-parametric mixture model
our estimation of the local fdr  relies on the semi-parametric mixture model proposed in  <cit> . e have at our disposal n hypotheses {hi}i =  <dig> ...,n we want to test. suppose that an unknown proportion π <dig> of them are true nulls. for any hypothesis, we define a random variable hi that equals  <dig> if it is under h <dig> , and equals  <dig> under h <dig> . for each hi, we compute a score denoted by xi . we assume that these scores are independent and identically distributed, with mixture distribution

  f = π <dig> f <dig>  + π <dig> f <dig> , 

where π <dig> =  <dig> - π <dig> states for the proportion of false null hypotheses, f <dig> denotes the probability density function  of scores under h <dig> and f <dig> is the pdf of scores under h <dig>  note that f <dig> is completely specified. for instance if xi is the p-value of a student statistic, f <dig> is the uniform distribution on  <cit> . if any transformation  is applied, f <dig> remains completely known. on the contrary, f <dig> needs systematically to be estimated so as to π <dig> 

in our framework, ℓfdr defined the probability that hi =  <dig> given the observed value xi of the score xi:

 ℓfdr=defτi=pr⁡=π0f0f. 

this quantity may be interpreted as a measurement of how likely the hypothesis at hand could be falsely rejected.

since f <dig> is unknown, we use the following  kernel estimator for a given bandwidth h > 0

  f1^=/, 

in which we replace the unknown hi's by their conditional expectation e  = pr  =  <dig> - τi.

these expectations are themselves thanks to

  τi^=π^0f0/f^, 

where π^ <dig> is a given estimator of the unknown proportion and f^=π^0f0+f1^. thus, we obtain

  f1^=/. 

as τi^'s and f1^ depend on each other, we alternate the computation of  and  until convergence, which is proved in  <cit> .

implementation
the method may require to apply a transformation to the sample of p-values , to estimate the proportion of null hypotheses , to determine an optimal value for the bandwidth  used in the kernel estimator and to compute the estimation of f <dig>  these technical points are further developed and discussed in the methods section.

moreover, the corresponding r package allows a simple and straightforward use. for instance the command try = kerfdr for a given sample of p-values  returns the estimates of π <dig> and ℓfdr in try$pi <dig> and try$localfdr respectively. in addition the running time is very fast thanks to an efficient implementation using convolution through fast fourier transforms and a list of customizable options for more advanced users such as the choice of π <dig>  h or the kernel function. the complete r code and a pseudo-r code of kerfdr are available on the webpage.

practical generalizations
semi-supervised cases
prior information is actually available in many experiments. among all the null hypotheses to be tested, some are known to be true  while some others are known to be false . such a knowledge is taken into account in the estimation procedure described previously: known a priori the τis are kept fixed throughout the steps of the algorithm. they contribute to the estimation of f <dig> in eq. , but are not updated in eq. .

truncation
let us suppose now that we have at hand truncated data within an interval i = . by 'truncated', we mean that the support of the p-values distribution is strictly smaller than  <cit> . for instance, if b denotes the number of simulations, p-values smaller than 1/b are often truncated to  <dig> . how this will affect our method?

in order to deal with densities, the restrictions of f <dig>  f <dig> and f to i need to be normalized. denoting by q <dig>  q <dig> and q the corresponding normalization factors, the mixture definition gives:

 q=∫ifdx=π0∫if0dx︸q0+π1∫if1dx︸q <dig> 

despite q <dig>  q <dig> can not be easily computed as f <dig> is unknown. fortunately, we can estimate q from a sample x <dig> ..., xn of non-truncated data using

 q^=1n∑i=1nixi∈i 

from which we derive

 q1^=q^−π0q0π <dig> 

one should note that this estimator does not necessarily belong to  <cit> . in order to overcome this, we replace its value by  <dig> if q1^ <  <dig> and by  <dig> if q1^ >  <dig> 

for example, if the p-values are estimated through monte-carlo using b =  <dig> simulations, the smallest non-null p-value is 1/b =  <dig>  and i = . let us assume that among a set of n =  <dig> p-values,  <dig> are equal to  <dig> , π <dig> =  <dig>  and π <dig> =  <dig> . we hence have q^ = /n = 946/ <dig> and as q <dig> =  <dig> - 1/b = 499/ <dig> =  <dig>  we easily get the expression of q1^ .

simulation study
a comparison with other estimation methods of ℓfdr is provided in  <cit> . it shows that the semi-parametric approach we propose performs as well as the empirical bayes approach  <cit>  and the gaussian mixture model  <cit>  when the distributions f <dig> and f <dig> are well separated. however, it outperforms them in more difficult situations, especially in terms of stability. we focus here on the particular cases described below  that are not handle by the aforementioned methods.

simulation design
we simulated sets of p-values according to the mixture model , where f <dig> is the uniform distribution over . we considered  <dig> different proportions of false null hypotheses ,  <dig> different means for the p-values coming from the alternative distribution f <dig> . f <dig> is either an exponential distribution ℰ or a uniform distribution over . the exponential distribution can provide values greater than one and a beta distribution as used in  <cit>  can appear more appropriate; however it occurs very rarely with the taken value for μ. for each of the  <dig> ×  <dig> ×  <dig> =  <dig> configurations, s =  <dig> samples of size n =  <dig>  were generated.

for each proportion π <dig> and distribution f <dig>  the ℓfdr of the i-th p-value τi has a theoretical expression that is computed. denoting by τ^is, the local fdr estimate of the i-th p-value for the simulation s , the performances of the method are assessed by means of the root mean square error

 rmse=1s∑s1n∑i <dig>  

the smaller the rmse, the better the performances.

semi-supervised
to see how prior information improves the estimation of ℓfdr, we randomly select some hypotheses for which the status is known. the proportion κ of these hypotheses is fixed, so that the true value of the local fdr is also known . figure  <dig> shows that even a small proportion  of known hypotheses improves significantly the ℓfdr estimation.

truncation
in purpose of comparison, we truncate p-values to a given threshold p*  and compare the generalized method that takes account of truncation with the naive one, in terms of the rmse criterion. in figure  <dig>  the original non-truncated p-values provide a reference that can not be outperformed. we see that the correction improves the quality of the estimates, especially when the truncation is severe  and that the corrected estimates can be almost as good as the best achievable.

applications
gene expression data
as a first illustration, we apply our method to the classical example of hedenfalk  <cit>  in which the expression levels of n =  <dig>  genes are studied. the aim is to compare patients with two different breast cancers:  <dig> brca <dig>  and brca <dig>  corresponding to two different gene mutations predisposing to the disease. we use the modified t-test statistic proposed in  <cit>  which avoids false-positives due to bad variance estimates.

applying our method, we obtain a proportion of null genes of π^ <dig> =  <dig> % which is consistent with the proportion estimated in  <cit>  . figure  <dig> displays the estimated densities: although the proportion of modified genes is quite high , the local fdr is lower than 1% for only  <dig> genes; it is below 5% for only  <dig>  this shows that the local fdr is an efficient tool to reduce the type-i error-rate in difficult cases.

the choice of the bandwidth is known to be a crucial step in density estimation problems. in this example, we selected a bandwidth of  <dig> . to check to influence of this choice on the results, we tried several values of h between  <dig>  and  <dig> . figure  <dig> shows that the estimated local fdr is not sensitive to this choice.

dna sequence patterns
it is well known that most biological patterns in dna sequences have unusual frequencies due to selection mechanisms. it is hence natural to search for new functional patterns among those whose number of occurrences is statistically significant. in order to do so, it is classical to adopt a test framework where the null hypothesis is that the dna sequence is generated according to a order m ⩾  <dig> markov model .

we consider here the complete genome of the pathogen bacteria mycoplasma genitallium  on which we estimate an order m =  <dig> homogeneous markov model. for each of the  <dig> =  <dig>  oligomers  of length  <dig>  we compute the exact expectation  and standard deviation  of its frequency n from which we derive the z-score:

 z=nobs−ev~h0n 

where nobs is the observed frequency of the oligomer in the genome.

thanks to a simple clt argument, we get that the distribution of z is approximately a standard gaussian under the null hypothesis. it is hence possible to use this approximation either by working directly with the z-score or by computing the two-sided p-value associated to each observation:

 p-value=ℙ<−|z|)+ℙ>+|z|) 

the natural approach is to estimate the densities from the p-values  where all the 'exceptional' oligomers  accumulate on the left side of the resulting density. but the flexibility of our method allows us to make the estimations directly on the basis of the z-scores  by taking into account their bimodal distribution under h <dig> and distinguishing the oligomers that are under-represented  from those that are over-represented . if both strategies provide the same estimation for the proportion of 'null' oligomers , ℓfdr estimations are sensibly different in particular for the ligomers that are over-represented .

quality control in genome-wide association studies
in association studies, deviations from hardy-weinberg equilibrium  can be due to inbreeding, population stratification or selections. they can also be a symptom of lack of quality in genotyping because of a tendency to misscall heterozygous genotypes as homozygous for instance  <cit> . as a result, testing for hwe has often been proposed as a data quality check with the aim to discard loci that deviate from the equilibrium. testing for deviations from hwe can be carried out using the pearson chi-square statistic  that quantifies the distance between the observed genotype proportions and the ones expected under the equilibrium.

here, the hwe test is applied to controls of genome-wide case-control data on the multiple sclerosis from france . the data set consists in  <dig>  single nucleotide polymorphisms . since the usual chi-square approximation can be poor when there are low genotype counts, p-values are computed via monte-carlo simulations  which represents a typical case of truncation of p-values for those that are below the level of precision given by the number of simulations.

applying our method, we obtain a proportion of null snps of π^ <dig> =  <dig> %. figure  <dig> displays the estimated densities, showing a large overlap between the two distributions f <dig> and f <dig>  by considering a threshold of 1%, then  <dig> snps would be declared to deviate from hwe, and up to  <dig> for a threshold of 5%. these quantities come down to  <dig> and  <dig> respectively when local fdr are estimated in the naive way . consequently and in addition to our simulations, this application underlines an inflation of excluded snps when the information about a truncation, when it exists, is not taken into account in the estimation procedure.

CONCLUSIONS
a simple computational approach to local fdr considers a two-components normal mixture model for modeling the observed empirical distribution  where the null distribution  is the standard normal and the alternative distribution  is a normal density with unspecified mean and variance. but the reliability of this approach obviously depends on how well the proposed two-components normal mixture model approximates the real distribution.

our semi-parametric approach does not assume any constrained alternative distribution and is hence much more flexible. nonetheless it requires a complete specification of the null distribution, the a priori proportion of true null hypotheses , as well asthe bandwidth  for which efficient estimation methods have been developed. the performances of the approach compared to existing methods were assessed in a preceding publication  <cit>  which showed its advantages in difficult situations where the distributions f <dig> and f <dig> are not well separated. we focused here on the implementation of the approach, and on two interesting extensions such as the possibility to use prior information in the estimation procedure  and the ability to handle truncated distribution such as those generated by monte-carlo estimation of p-values. our simulation showed that these informations can significantly improve the quality of estimates. as an illustration, we analyzed three high-throughput biological dataset concerning genes expressions, dna sequence patterns, and genome-wide association studies. the corresponding r package available at  is fast, thanks to fast fourier transforms, straightforward to use and propose customizable options to advanced users.

finally, most of the local fdr estimation procedures derived from the benjamini and hochberg framework, including our approach, assume that p-values testing true null hypotheses are independent observations. if it may well be the case for patterns, in practice this assumption does not hold for all the genes or snps. a proposed solution is to cluster highly correlated genes  together, and to represent a cluster by a single gene or a linear combination of the associated genes  <cit> . theses approaches also generally assume that p-values testing true null hypotheses are continuous and uniform over  <cit> . these issues are likely to be alive fields of research in the near future.

