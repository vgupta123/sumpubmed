BACKGROUND
over the past four decades, sequencing technologies have been one of the major driving forces in the life sciences producing, for instance, the full genome sequences of thousands of viruses and bacteria, and dozens of eukaryotic organisms, from yeast to man  <cit> . this trend is being accentuated by modern high-throughput sequencing  technologies: several human genomes were recently produced  <cit>  and a project to sequence  <dig>  human genomes in the next few years is under way  <cit> . different hts technologies are competing to be able to sequence an individual human genome for less than $ <dig>  within a few years  <cit>  and reaching the point where human genome sequencing will be a commodity. furthermore, not only are hts technologies useful for sequencing and resequencing genomes, but they are also instrumental to accurately identify and measure mrna and other nucleotide sequences in new important high-throughput applications such as digital expression microarrays, chip-seq  <cit>  and snp genotyping. in all cases, the amount of data produced by hts technologies as experiments scale up creates significant bioinformatics challenges to understand, store and share data. to address some of these challenges, we develop data structures and compression algorithms for the efficient management and storage of hts data.

several different hts technologies have been conceived and developed to differing degrees of maturity. they can be classified into four broad classes: amplification followed by mass spectrometry, in vitro cloning, in vivo cloning, and single molecule  <cit> . there are currently three commercially advanced hts systems: solid , solexa ,  <dig> , all based on the in vitro cloning approach. each system depends on a sheared dna sample which is diluted onto some type of matrix, clonally amplified, and then transformed via repetitive enzymatic cycles into a series of four distinct fluorescent signals  monitored at each cycle by a ccd camera. the series of fluorescent signals at each position is converted into a dna sequence and a quality score for each position. a typical run  can generate tens of millions of sequence reads, and with a set of experiments that includes biological replicates, control and treatment samples, etc., then the total number of reads can reach into the billions. important variations between technologies exist, for instance in terms of the length and quality of the sequences. however, all existing systems rely on the parallel sequencing of many short sequences and produce outputs of very long lists of relatively short sequences. thus the fundamental problem we wish to address is the storage and compression of such lists.

because of the variations that may exist between different technologies and different constraints associated with different deployment scenarios, our goal is not to provide a single solution, but to describe general methods by which customized solutions can be developed. thus after presenting the basic idea, we review several relevant representations and compression algorithms. the approach is illustrated on several hts data sets.

methods
general approach
in the standard text format, a file of n short sequences of average length l requires n bytes  bits) to store, using one ascii byte per character, and including a character  to separate two consecutive sequences. the ranges of n and l can vary depending on the experiment and the technology, but to fix the ideas one can imagine current values of n in the  <dig> -  <dig> range and of l in the  <dig> -  <dig> range, with most typical values in the 20- <dig> range. additional information regarding, for instance, the quality of the sequences can be included in the output files.

to store and compress this information, we imagine first that the short sequences can be mapped to a reference genome. this is the typical situation for resequencing experiments, including large-scale sequencing of diploid human genomes using mapping software such as illumina's eland, maq  <cit> , zoom  <cit> , or bowtie  <cit> . in this case, each short sequence si can be represented by its address ai in the reference genome. if the length of each sequence is not fixed and known in advance or stored in the header of the file, the length of the sequence li must also be included. if the match is not exact, variations from the genomic sequence must also be included by recording their address and type. for simplicity, we will assume only substitutions. thus something like "" could be used to record a short sequence whose starting point matches position  <dig> in the reference genome, with a length of  <dig> nucleotides, and a substitution by a c in position  <dig>  relative addresses, rather than absolute addresses, can be used not only to record variations within sequences, but also for the address of the sequences themselves. the same sequence could be encoded by "" to indicate that it is found  <dig> nucleotide downstream of the previously occurring sequence, provided the file has been preprocessed to reorder the sequences linearly along the genome. with relative addresses, the dynamic range of the integers to be encoded may be considerably smaller than with absolute addresses. if a sequence can be mapped equally well to multiple locations on the genome, any one of them can be chosen to represent the sequence. finally, specific experiments could come with additional information. for instance, in snp mapping experiments, the locations and types of variations could be constrained and leveraged to increase compression. it is worth noting that for this approach, the availability of a reference genome is not as restrictive as it may seem; minimally, a reference of dna sequences to which experimental sequences can be reasonably mapped is needed. for simplicity in this work, we focus on the case of interest where a reference genome is available.

the idea of compressing dna sequences is not new. compression algorithms such as biocompress- <dig>  <cit> , ctw  <cit> , offline  <cit> , dnacompress  <cit>  and dna compressor  <cit>  consider the task of directly compressing large sequence strings. while others consider alternative compression tasks such as coil  <cit>  which compresses a large database of unrelated sequences or dnazip  <cit>  which compresses variations to a reference genome. our goal is different as we want to compress a large number of very short sequences while using a large reference sequence. whether or not it is advantageous over the standard text format, however, depends on the details of the implementation and, more often than not, the data being compressed. a successful implementation of the basic idea depends crucially on careful consideration of the encoding scheme. in particular, the choice of the function converting integers to binary strings, has a great effect on the resulting compression. for our application, these integers are the absolute or relative addresses and lengths of sequence reads. it is essential to understand that simply converting integers to their binary value  does not work since one does not know where one integer ends and the next starts. no symbol other than  <dig> or  <dig> is available to separate consecutive integers. furthermore, such a simple encoding scheme does not take into account any entropy considerations. likewise, a general purpose compression scheme for text data, such as lempel-ziv , is likely to be far from optimal for hts data. thus we are interested in binary encoding schemes for sequences of integers that can be parsed automatically and, consistent with information theory, are entropy efficient, in the sense that fewer bits are used to encode more frequent events.

a simple back-of-the-envelope calculation, however, can show why the situation is hopeful. suppose the information associated with the integer j representing an address can be stored in about  <dig> log j bits . this corresponds to a penalty factor of two over the plain binary encoding and can be achieved with the coding methods described in the next section. then the equality  <dig> =  <dig> log j shows that some degree of compression is achieved as long j is less than  <dig>  even with l as small as  <dig>  this yields  <dig> which is much larger than the length of any genome. assuming that the length of each sequence must also be stored, this may require at most a fixed number of bits c. if the lengths are between  <dig> and  <dig>  for instance, they can be described with  <dig> bits. from the relation  <dig> =  <dig> +  <dig> log j we find again that compression is achievable as long as j is less than 24l+ <dig>  which is again easily achieved in the current environment. furthermore, with for instance l =  <dig> these relations show that a 20-fold or so compression rate should be achievable with reasonable values of j. a similar calculation can be made including information about the number of substitutions, their locations, and types for each sequence.

specific encoding strategies
to begin with, we illustrate these issues here by considering how the integer addresses ai, relative or absolute, can be encoded into a binary string. from shannon's entropy coding theory  <cit> , optimal encoding of these integers from a compression standpoint depends on their distribution in order to assign shorter binary codes to more probable symbols . for simplicity, we distinguish two broad classes of codes: fixed codes, such as golomb  <cit>  and elias codes  <cit>  and their more recent variants  <cit> , and variable codes, such as huffman codes  <cit> . in a fixed code, the integer i is always encoded in the same way, whereas in a variable code the encoding changes.

fixed codes: golomb and golomb-rice codes
both golomb codes and elias codes encode an integer j by catenating two bit strings: a preamble p, that encodes j's scale, and a mantissa. golomb codes were specifically developed to encode stationary coin flips with p ≠  <dig> . thus they are known to be optimal and asymptotically approach the shannon limit if the data is generated by random coin flips or, equivalently, if the distribution over the integers is geometric, although they can be used for any other distribution. the more skewed the probability p is  the greater the level of compression that can be achieved.

golomb codes have one integer parameter m. given m, any positive integer j can be written using its quotient and remainder modulo m as j = ⌊j/m⌋ + . to encode j, the golomb code with parameter m encodes the quotient and remainder by using:

• ⌊j/m⌋ 1-bits for the quotient;

• followed by a  <dig>  as a delimiter ;

• followed by the phased-in binary code for j mod m for the remainder .

the encoding of integers  <dig>  ..., m -  <dig> normally requires b = ⌈log m⌉ bits. if m is not a power of two, then one can sometimes use b -  <dig> bits. more specifically, in the "phased-in" approach:

• if i <2b - m, then encode i in binary, using  bits;

• if i ≥ 2b - m, then encode i by i + 2b - m in binary, using b bits.

for instance, for m =  <dig>  i =  <dig> is encoded as "10" using  <dig>  bits, and i =  <dig> is encode as "111" using  <dig>  bits. thus the encoding of j requires in total ⌊j/m⌋ +  <dig> + ⌊log m⌋ or ⌊j/m⌋ +  <dig> + ⌈log m⌉ bits and the codeword for the integer j + m has one more bit than the codeword for the integer j. unless otherwise specified, all logarithms are taken to base  <dig>  we use also "" to denote "⌊log m⌋.or ⌈log m⌉".

finally, golomb-rice codes are a particularly convenient sub-family of golomb codes, when m = 2k. to encode j, we concatenate ⌊j/2k⌋ 1-bits, one 0-bit, and the k least significant bits of j. the length of the encoding of j is thus ⌈j/2k⌉ + k +  <dig>  the decoding of golomb-rice codes is particularly simple. first, read and count the number of  <dig> bits until the first  <dig> bit is found. the number of  <dig> bits is the quotient q = ⌊j/m⌋. then, read the next log m bits to get the binary representation of the remainder r = j mod m. the decoded value equals j =  + r.

elias codes
in the elias gamma coding scheme, the preamble p is a string of zeroes of length ⌊log j⌋, and the mantissa m is the binary encoding of j. more precisely, to encode the scale and value of j:

• write ⌊log j⌋ 0-bits;

• followed by the binary value of j beginning with its most significant 1-bit.

the length of the encoding of j is 2⌊log j⌋ +  <dig>  the decoding is obvious: first read n 0-bits until the first 1-bit is encountered, then read n more bits to get the binary representation of j.

each integer j is encoded by concatenating ⌊log j⌋ 0's with the binary value of j

applying the relationship

  −logp≈2⌊logj⌋+ <dig> 

to the integer probabilities, shows that elias gamma encoding asymptotically approaches the shannon limit for p  ≈ cj- <dig>  this is a power law relationship with exponent - <dig> and c is a normalizing constant. note that for both golomb and elias gamma codes, several different consecutive integers can be encoded into a bit vector with the same length, hence the relationships -log p  ≈ length is only approximate with respect to geometric or power-law distributions over the integers.

monotone value coding 
more recently, new families of efficient fixed codes for integers have been developed  <cit> , for instance in the case of increasing or quasi increasing sequences of integers, by encoding only the deltas of the preambles. here we introduce a modification of the codes described above, presented with the elias gamma codes, for messages consisting of monotone sequences of integers, such as consecutive absolute addresses of sequence reads. when the value of the integers being encoded increases monotonically, additional lossless compression can be obtained by encoding only the scale increases and their location .

the principle is illustrated using the vector of addresses . each integer j is converted to a binary representation of length ⌊log j⌋ which begins with a 1-bit. 0-bits are used between two consecutive integers only when the length  increases. the number of 0-bits is equal to the increase in the length. the final encoding of the vector is  <dig>  <dig>  <dig>  <dig>  <dig>  <dig>  <dig>  <dig>  <dig> 11101

more precisely, if a sequence of increasing addresses is given by  with j <dig> < j <dig> ... <jk:

• encode j <dig> using elias gamma encoding;

• for i =  <dig>  ..., k:

- write ⌊log ji⌋ - ⌊log ji-1⌋ 0-bits;.

- followed by the binary value of ji beginning with its most significant 1-bit.

the mov-encoded vector of addresses can be decoded by a simple algorithm:

• set k = 1;

• decode each integer in succession by repeating the following steps:

• increment k by the number of 0-bits in the input stream before reaching the first 1-bit;

• counting this first 1-bit as the first digit of the integer, read the remaining k -  <dig> bits of the integer from the input stream.

another variation called monotone length coding  can be used for quasi-monotone sequence tolerating occasional deviations from a monotone behavior  <cit> . another scheme that may be useful for encoding integers but cannot be described for conciseness reasons is the binary interpolation  <cit>  scheme, together with several variants.

variable codes
in genomic applications, in general the integers may not have a well defined distribution, in which case it is always possible to use a general entropy encoding scheme, such as huffman coding  <cit>  which essentially builds a prefix code by using a binary hierarchical clustering algorithm starting from the events  with the lowest probability. while huffman coding achieves compression close to the entropy limit, the price to pay over fixed coding schemes such as golomb and elias gamma, or the more recent codes mentioned above, is the storage of the huffman table which can be quite large in some applications. however this is a fixed cost with respect to the database size, and therefore whether this cost is acceptable or not depends on the specific application. small gains in compression over huffman coding may be obtained using arithmetic coding  <cit> , but at a non-trivial price in the complexity of computations. for more information about integer encodings, refer to references  <cit>  and  <cit> .

byte arithmetic
direct implementations of the decoding algorithms process the compressed representations bit-by-bit; however, it is possible to implement faster decoders, which decode the compressed data byte-by-byte. these faster decoders work by looking up information from pre-computed tables. these tables are indexed by:  all possible bytes b ; and  a bit-index i  which marks the position of the decoder within the byte. these tables may store quantities such as the binary value of byte b starting from bit i, the number of bits turned on in byte b starting from bit i, and the unary value of byte b starting from bit i. the exact quantities stored depend on the details of a particular decoder implementation.

in practice, byte arithmetic considerably increases decoding speed, sometimes approaching as much as an eight-fold improvement over the corresponding bit-by-bit implementation. the exact value of the speedup depends on several factors including the characteristic of the data, the exact compression scheme, and the hardware used.

RESULTS
for conciseness, we present a subset of representative results focusing on elias, mov and variable codes. golomb code results are in general comparable to elias gamma results, typically with a slight decrease in performance. furthermore, golomb's code require tuning one additional parameter  and thus are not reported here.

for each data set, we transformed the data into a uniform flat file format, separating the location information from the mismatch information for each read, then performed encoding on the location and mismatch information separately. the following sections describe the different encoding strategies used for location and mismatch data and their corresponding results.

data extraction and statistics
we selected three data sets representative of typical short read sequence data derived from different experimental settings aimed at addressing different biological questions, from genome sequencing to transcription factor binding site mapping. each of the data sets correspond to a different combination of genome coverage, repetitiveness and locational specificity in the genome, so that our encoding results provide insights into how different strategies can be applied and tailored to different data. table  <dig> gives some basic statistics for each data set, including the sizes for the original standard text format for the sequence reads, the uniform flat file format as described above, and the bowtie alignment output. the uniform flat file format sizes are further split into the sizes of the location data and mismatch data. the bowtie alignment output contains additional information beyond the minimal location and mismatch data required to reconstitute the reads, so table  <dig> provides compression sizes of that additional information with a set generic compression tools. those sizes will be used later for comparison of compression results for bowtie output.

dataset 1
the first data set is obtained from the laboratory of dr. suzanne sandmeyer at university of california irvine and comes from an experiment aimed at mapping retrotransposon ty <dig> insertion sites in the yeast genome. it consists of  <dig> , <dig> sequence reads, all of length  <dig> bp. by the nature of the underlying experiments, the sequences in this data set are highly clustered, often with a high degree of repetition. the reads have at most two substitutions. the numbers of sequences with  <dig>   <dig>  and  <dig> substitutions are given by  <dig> , <dig> ,  <dig>  , and  <dig> , <dig>  respectively.

dataset 2
the second data set comes from a chromatin immunoprecipitation assay  used to map the in vivo binding site locations of the neuron-restrictive silencer factor  in humans  <cit> . it consists of  <dig> , <dig> sequence reads, all of length  <dig> bp and mapped to the most recent human genome sequence . the reads have at most two substitutions. the numbers of sequences with  <dig>   <dig>  and  <dig> substitutions are given by  <dig> , <dig> ,  <dig>  , and  <dig>   respectively. figure  <dig> shows the number of mismatches found at each position along the read, as well as the types of substitutions. the number of mismatches increases towards the end of the read, as expected with the solexa sequencing technology where the error rate increases further along the length of the read. our interest is encoding the read sequence as is without attempting to differentiate between true snps and sequencing errors, but the distribution clearly shows that the majority of these mismatches are observed at the end of the read which can be used to advantage when encoding the variations.

dataset 3
the third data set corresponds to a full diploid human genome sequencing experiment for an asian individual  <cit> . this is a very large data set with enough reads to provide 36-fold average coverage, and we utilized the existing mapping of the reads provided by the yh database  <cit>  to the human reference genome. for illustrative purposes, we report only the results corresponding to the reads associated with chromosome  <dig>  for chromosome  <dig>  there are  <dig> , <dig> reads that vary in length from  <dig> to  <dig> bp for a total of  <dig> , <dig>  bp of sequence data. the numbers of sequences with  <dig>   <dig>  and  <dig> substitutions are given by  <dig> , <dig> ,  <dig> , <dig> , and  <dig> , <dig>  respectively.

encoding of location information
the location information for a mapped read consists of a chromosome identifier, a position along that chromosome, the strand, the length of the read, and the number of mismatches it contains. in the flat file format, each read is specified on a single line with the values separated by a comma. one technique is to encode each of the attributes individually. for this standalone technique, we compute the frequency of occurrence of each of the attributes, order them, and then use eg encoding on their ordered index. an alternative method is to combine all of the attributes together. the lookup method takes the attributes combined together as tuples, for example , then computes the frequency of a subset of these. the combination method of reg indexed is described in detail below.

all of the results are based upon theoretical calculations without doing the actual encoding. in the implementation section, we describe our gencompress software package which implements some of the encoding methods. the methods implemented in gencompress are marked with a † in table  <dig>  and the best compression achieved by gencompress is also shown. in the following, we describe in more detail the various methods used for encoding the location information.

 <dig>  elias gamma  absolute: we assume that the reads cannot be reordered in any way and thus must be processed exactly as specified. the chromosome, absolute start coordinate, strand integer values, and for dataset  <dig> the read length were encoded using elias gamma codes.

 <dig>  elias gamma  relative: we assume that the reads can be ordered in any way in order to achieve better compression results. we group all of the reads for each chromosome together. within each chromosome, the reads are sorted by increasing position number; therefore the relative distance between adjacent reads is encoded rather than their absolute positions. these relative addresses correspond in general to significantly smaller integer values than the absolute addresses, especially for long chromosomes or reads with high-coverage. the chromosome, strand integer values, number of mismatches, and the read length , were encoded using elias gamma codes.

 <dig>  relative elias gamma indexed : we again assume that the reads can be ordered in any way in order to achieve better compression results. we group all of the reads for each chromosome together, then group reads for each strand together within a chromosome, and further group them for the number of mismatches they contain. within each bin of , we then encode the relative distances as stated above. because the reads are grouped by chromosome, strand, and number of mismatches, there is no need to encode that information for each read. instead, those values are stored along with a count of the number of reads for that group in a header structure. thus, except for the additional read length information for dataset  <dig>  only the relative distances between the reads are encoded using elias gamma.

 <dig>  monotone value : like the eg relative encoding above, we reorder the reads for monotone value encoding according to chromosome and position. however, we use mov codes for the absolute locations as the positions are now in increasing order.

 <dig>  huffman: we can use the start positions  to compute a huffman tree which we use to encode. the resulting size encoding with this method also includes storing the huffman tree, which is needed for decoding.

encoding of mismatch information
the mismatch information for a mapped read consists of the positions of one or two mismatches located somewhere along the read and the nucleotide value  for the mismatch. the flat file format for mismatch information has one read per line; the line is blank if no mismatches otherwise it contains a comma separated list of mismatches .

the sequence mismatches can be encoded in multiple ways. one possibility is to encode the position of the mismatch directly from the start of the read. however, the mismatches have a clear tendency to occur towards the end of the read as illustrated in figure  <dig>  so measuring the mismatch from the end of the read can reduce the number of bits to encode the position . for both strategies, the position is encoded using elias gamma codes.

the nucleotide substituted at a particular position must also be encoded; the straightforward naïve approach is to map them to integers . this can be optimized by ordering the nucleotides by their frequency of occurrence, so the most frequent substitution maps to the lowest integer. in both cases, the value is encoded using elias gamma codes.

we also tried another strategy where the position and nucleotide substitution are combined together into a single value. the combination are ordered by frequency and then encoded using elias gamma codes. this technique may possibly be effective if there is a large number of duplicate mismatches across all the reads.

we investigated all of these strategies and the results can be seen in table  <dig>  for each data set, a different method ended up having the best compression.

final encoding
with the location and mismatch information combined together, we have a final encoding for a representative set of short read sequence data produced by hts technologies. using encoding techniques that consider the inherent structure of sequence data consistently perform well for all the data sets we tested. for our test data sets, it is interesting to note the size ratios for encoding the location information versus the mismatches. for data set  <dig>  the mismatches clearly dominate the total compression size while the location information is very small due to the clustered nature of the reads on the genome. on the other hand, data set  <dig> has much fewer mismatches and the read locations are sparsely distributed across the genome, so the location information dominates the total compression size. data set  <dig> is balanced between the two because it has full coverage of the genome. for data sets  <dig> and  <dig> which have a large number of mismatches, using dbsnp data as reference variation data may offer the opportunity for further compression.

implementation
we have implemented a subset of the encoding techniques describe in this article into a software package called gencompress. the implementation is primarily based on the relative elias gamma indexed  encoding for the location information and the combined encoding for the mismatch information because they offer the all around compression. currently, the quality scores are not yet encoded, and the decoded data recapitulates only the location and mismatch information. gencompress performs two efficient passes on each dataset obtaining statistics on the mismatches during the first pass prior to actual encoding on the second pass. a more advanced data structure is currently being developed to avoid these two passes. however, this implementation allows for stream processing with minimal memory requirements. gencompress is currently only compatible with output of the bowtie short-read aligner. a future implementation of a decoder will use the bowtie libraries to obtain the actual genomic sequences in the decoded output, and a framework exists for supporting multiple aligner suites.

CONCLUSIONS
we have presented a set of data structures and compression algorithms for high-throughput sequencing data. we have transformed the nucleotide sequences into location and mismatch information through a mapping procedure to a reference genome, then applied fixed codes to encode that location and mismatch information in an efficient manner. we note that the mapping procedure does not need to be precise and find the correct position in the genome. we require only that a position is found because we use it only for the purpose of compressing and later decompressing the sequence. in fact, any arbitrary genome sequence can be used for mapping the reads, but it is likely that the genome which most closely matches the organism for the read data will provide the best performance. the methodology we have proposed is general, and we have illustrated its effectiveness on a representative set of hts data. results show that some of the information in the hts data can be compressed by a factor of  <dig> or more. the proposed algorithms are comparable or slightly better than the best general compression algorithms such as bzip <dig> and 7zip, but those programs require a much greater processing time compared to our algorithms.

the term "post-genomic era" has become somewhat fashionable, it is clear that the genomic era is far from over, and may in fact be only at an early stage of development. with the advent of hts technology and increasingly new experimental protocols for using the technology, the sequence databases are only expected to continue rising in size. while the local storage of this data is not especially burdensome with inexpensive hard drives of greater than  <dig> tb available, sharing and transferring the data is time-consuming because network speeds are an order of magnitude slower than disk. take for example the yan huang genome  <cit>  we used for dataset 3; the total amount of read data generated by the project is almost  <dig> gb. uncompressed, a typical university network can upload this data in  <dig> hours at roughly  <dig> gb per hour; that may actually take a couple of weeks of real work time by a researcher handling the upload process, and that doesn't even consider restarts that may occur due to network outages or that the network has to be shared with many other users. extrapolating the compression results we obtained for chromosome  <dig> would reduce the total read data down to  <dig> gb, reasonable to upload in a single day. the situation becomes worse for downloading as numerous researchers may attempt to download the data, quickly saturating the server network bandwidth; advanced compression techniques such as we have introduced would allow more researchers to obtain the data in a timely fashion.

it is not likely that exactly one encoding strategy will be optimal for all types of hts data. different experimental conditions are going to generate various data distributions whereby one encoding strategy can be more effective than another. for encoding the location information, we have shown that two different strategies are effective. furthermore in this article, we have only been able to consider solexa data. it would be worthwhile to investigate both  <dig> data with its wide range of read lengths and solid with its color space representation as the sequencing error distributions may be different for these technologies, thus affecting the mismatch locations and the strategy used for encoding them. it isn't necessary that a single strategy be picked; our software computes the respective compression metrics for all of the strategies, so they can be compared and the best one chosen automatically. we have focused exclusively on the nucleotide sequence data of the short reads from hts; however, there is additional data that is also present including read identifiers and quality scores. a complete solution would require that this information also be encoded so that it can be recovered later. the quality scores are needed as they are used by assembly programs for determining the statistical significance of the final assembled sequence, and also by programs that call snps and other structural variations. read identifiers are often just sequential numbers with little meaning, but they do become needed for cross referencing when mate paired reads are sequenced. however in both cases, the encoding techniques described in this paper can be applied to significantly reduce the size of this data.

the techniques we have described assume that the reads have already been mapped to a reference genome before they are encoded, and in our analysis we have only discussed encoding mapped reads. however in all sequencing experiments, there are reads which do not map to the reference genome. in many cases, these reads are contaminants such as bacteria and might not be relevant for the particular experiment, but in other cases those reads may be important if it is a de novo sequencing project or the reference genome is unfinished or has poor coverage. the simple solution is to use a generic compression program like gzip for those unmapped reads. another possibility is to use multiple reference genomes. if those reads map successfully to other genomes, then equivalent levels of compression can be expected with just a small amount of header overhead required to reference the genome used.

it is also common for reads to map to multiple places on the reference genome. our current implementation just takes the first best match, so additional optimization might be worthwhile to investigate. for example, if a read maps to one location which is very far from other reads, but has another mapping which is close to other reads, it would be advantageous to take the latter mapping because its relative starting position should be a smaller number. optimizing all of the reads in this fashion is possibly a very difficult problem though because minimizing the distances between reads corresponds to doing clustering, and many optimal clustering algorithms are known to be np-complete. however, approximate clustering which can be done with an online algorithm may offer some advantage. there is other data beside the read starting location which can be considered including the chromosome, strand and number of mismatches. the reg indexed strategy performed consistently well, so taking alternative mappings for reads might create denser data bins which can be effectively encoded with fewer bits. future research needs to consider whether the extra computational expense of performing these optimizations are a worthy compromise to the compression gains.

one might consider that because there is a lot of repetitive sequence within genomes, that an optimal encoding strategy like huffman codes directly on the sequence itself would work very well. we investigated this possibility by encoding k-mers for data sets  <dig> and  <dig>  the results can be seen in table  <dig>  the results seem to bear this out. data set  <dig> which is a mapping experiment of retrotransposons would be expected to have considerable repetitive sequence, and huffman coding does a very good job of compressing this data set. it does better than our algorithms and the general purpose compression programs. on the other hand, huffman coding gets worse compression than our algorithms for data set  <dig> 

the data is preprocessed by counting the frequencies of k-mers, and this is used to build a huffman tree. the tree is used to encode the data, and the number of bits needed to store the data as well as the tree are given

similar encoding ideas have been applied by us to a related but different problem, the storage of entire genomes  <cit> . results indicate that almost 1000-fold compression can be obtained for the human genome by encoding only the variations of the genome against a reference genome and a reference snp database. these techniques might be further improved if we consider that most snps in the human genome are biallelic , are clustered together into haplotypes, and shared among many individuals. instead of storing the variation for each read, a snp map might be utilized which summarizes the variation across all of the reads or subsets of reads. implementation requires careful consideration of the difference between true snps and sequencing errors, and whether an existing database like dbsnp is appropriate to use as reference  or a custom constructed database would work better.

authors' contributions
pb conceived the study and some of the basic compression algorithms. pb and xx coordinated and supervised the study. pb, kd, sc, and pr drafted the manuscript. kd and pr wrote the software and performed the simulations. all authors analyzed the results and read and approved the final manuscript.

