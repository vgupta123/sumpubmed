BACKGROUND
of the many attributes of real-time quantitative pcr , the ability to conduct absolute quantification is arguably the most significant. from a technical perspective, absolute quantification allows assay performance to be precisely defined, from which sensitivity, effective quantitative range and quantitative accuracy can be expressed in absolute terms. from an application perspective, assessing biological significance within the context of absolute number of target molecules can enhance the utility of most, if not all, quantitative assays. many prominent examples come from biomedical diagnostics where absolute quantification can have direct clinical relevancy, as is evident for monitoring viral load and residual disease. although more general applications such as environmental screening and pathogen detection would also benefit greatly, it is the application of absolute qpcr to gene expression profiling that holds some of the most substantive implications.

historically, real-time qpcr has largely been relegated to a supportive role in large-scale gene expression studies, most frequently used for the verification of dna microarray datasets  <cit> . nevertheless, absolute qpcr has the potential to extend gene expression analysis beyond what is possible with microarray analysis, based upon the innate capability to overcome two of the greatest limitations of microarray quantification, which are limited sensitivity and lack of absolute scale  <cit> . some of the most illustrative examples come from the application of microarrays to clinical research and diagnostics. founded upon the expectation that gene expression analysis can be used as a diagnostic tool to both predict and follow therapeutic outcomes  <cit> , many studies have reported that effective diagnoses can be achieved with relatively small groups of biomarker transcripts, numbering between  <dig> and  <dig>  <cit> , a range that is potentially within the capacity of absolute qpcr technologies.

development of diagnostic assays for disease prediction could thus exploit the reduced technical complexity, speed of analysis, sensitivity and substantively greater resolution provided by real-time qpcr, as compared with microarray analysis  <cit> . indeed, absolute quantification could increase the efficacy of any gene expression profiling initiative, irrespective of the experimental context. nevertheless, a prominent inadequacy of current real-time qpcr technologies is the limited capacity for conducting absolute quantification, due to reliance on target-specific standard curves  <cit> . not only does this necessitate preparation of a quantified standard for each target under investigation, the technical difficulties and extensive resources required for standard curve construction present considerable challenges for conducting absolute quantification, even for a modest number of targets.

a number of studies have attempted to overcome the innate limitations of standard curves by analyzing the fluorescence readings generated by individual amplification reactions. these include determining amplification efficiency through the application of exponential mathematics to the log-linear region, using either linear regression  <cit>  or nonlinear regression  <cit> . attempts to model the entire amplification profile have included sigmoidal modeling using nonlinear regression  <cit> , in addition to the application of various biochemical-based models  <cit> , and other forms of mathematical modeling  <cit> . demonstration that absolute quantification can be achieved by combining optical calibration with sigmoidal modeling  has led several groups to evaluate this approach as an alternative to standard curve-based quantification  <cit> . unfortunately, effective implementation of scf has been impeded by errors produced by distortions within the plateau phase, which can severely compromise the accuracy of scf-based quantification  <cit> .

the study presented here extends scf quantification by adapting the sigmoid function upon which scf is based, to directly model pcr amplification without the need to conduct nonlinear regression. based upon a linear relationship between amplification rate and amplicon quantity, this allows target quantification to be conducted using linear regression analysis. in addition to eliminating errors produced by plateau phase anomalies, this provided the foundation for development of a new quantitative paradigm that does not require standard curves and is able to resolve quantitative differences on the order of  <dig>  fold, while providing unprecedented quality control capabilities. a prototypic java program which automates implementation of this kinetic-based methodology further illustrates the potential to develop high-throughput applications, in addition to providing a visual illustration of the underlying principles.

RESULTS
amplification efficiency is dynamic and is coupled to amplicon dna quantity
new insights into the dynamics of pcr amplification have been gained through the application of sigmoidal modeling  <cit> , in which nonlinear regression analysis is used to fit real-time sybr green i fluorescence readings to the sigmoid function:

  fc=fmax⁡1+e−+fb 

where fc is the reaction fluorescence at cycle c and is proportional to the mass of amplicon dna present in the reaction, fmax is the maximal reaction fluorescence that defines the end point of the amplification process, referred to as the plateau phase, c1/ <dig> is the fractional cycle at which reaction fluorescence reaches half of fmax, k is related to the slope of the curve, and fb is the fluorescence background. despite its apparent novelty, this equation is in fact a simple derivative of the classic boltzmann four-parametric sigmoid function that is commonly used to model sigmoidal datasets.

notwithstanding the complexities of conducting nonlinear regression analysis, the remarkable precision that can be achieved is indicative of the potential for sigmoidal modeling to fundamentally revolutionize real-time qpcr  <cit> . an essential insight into this potential comes from examination of pcr amplification kinetics, as described by a second sigmoid function  <cit> :

  ec=1+e−1+e−− <dig> 

where ec is the amplification efficiency at cycle c, also referred to as "cycle efficiency"  <cit> . under a sigmoidal model, amplification rate is maximal at the onset of thermocycling, but progressively decreases such that each cycle has a unique amplification efficiency, with entry into the plateau phase occurring as amplification efficiency approaches zero.

the principle of this and other insights can be illustrated by comparing plots generated with equations  <dig> and  <dig>  the most notable being the striking symmetry between amplicon dna accumulation and reduction in cycle efficiency . importantly, this implies that an association exists between amplicon dna quantity and amplification efficiency, a contention supported by the mathematical prediction of a linear relationship between reaction fluorescence and cycle efficiency .

recognition of this linear relationship not only impacts the practicalities of modeling pcr amplification, but also has unparalleled implications for how real-time qpcr can be implemented. central to this is the prediction that the dynamics of pcr amplification can be described by a linear equation, defined here as:

  ec = Δe × fc + emax 

where the slope defines the rate of loss in cycle efficiency  and the intercept defines the maximal amplification efficiency  when fc =  <dig> . it is also evident that as pcr amplification enters the plateau phase, reaction fluorescence approaches a maximum  as ec approaches zero, such that equation  <dig> becomes:

  <dig> = Δe × fmax + emax 

so that:

  fmax⁡=emax⁡−Δe 

in addition to greatly simplifying the mathematics describing amplification kinetics, of practical significance is the ability to obtain estimates of emax and Δe via linear regression analysis, utilizing the fluorescence readings produced by individual pcr reactions. termed "linear regression of efficiency" or lre, this approach not only generates a linear representation of pcr amplification, but as described in later sections, allows target quantity to be determined directly from individual fluorescence readings. an important qualification, however, is the extent to which experimental data comply with these mathematical predictions.

conformity of real-time amplification profiles generated with sybr green i
a fundamental approach to analyzing the kinetics of pcr amplification is based upon defining amplification efficiency as the relative increase in amplicon dna over a single cycle:

  ec=fcfc−1− <dig> 

where fc- <dig> is the reaction fluorescence of the preceding cycle. this provides an estimate of cycle efficiency from which kinetic analysis can be conducted without having to resort to nonlinear regression analysis. it should be noted that alvarez et al.   <cit>  have utilized a similar approach for determining emax , although they concluded that a two-parametric sigmoid function was superior to the linear model upon which lre is based. nevertheless, the validity of equation  <dig> is dependent on the assumption that reaction fluorescence remains proportional to amplicon dna quantity throughout the amplification process. while this may be most evident for fluorescent dyes such as sybr green i, other detection chemistries may not conform well to this assumption   <cit> ). in fact sybr green i detection may not be free of anomalies, particularly at high amplicon dna quantities. this presumption is based upon the low quantities of sybr green i present in commercial real-time pcr formulations , made necessary by the inhibitory nature of sybr green i. this in turn could distort the apparent cycle efficiency as predicted by equation  <dig>  potentially invalidating, or at least distorting, lre-based estimates of emax and Δe.

a major goal of this study was therefore to evaluate the efficacy of sigmoidal modeling for conducting absolute quantification, with the explicit objective of validating the quantitative competency of a lre-based methodology; that is, beyond simply generating target quantities that lack context. central to this initiative was the utilization of bacteriophage lambda genomic dna  as a highly defined, commercially available quantitative standard. not only does this approach make much of this work amenable to experimental replication, it serves to illustrate some of the exceptional attributes of employing lambda gdna as a universal quantitative standard. paramount is the ability to establish quantitative scale, which is essential to conducting absolute quantification. as will be illustrated in later sections, this is also requisite to eliminating the need for standard curves.

notwithstanding the potential utility of lre-based quantification, an essential starting point was to examine the potential distortion of reaction fluorescence generated by low quantities of sybr green i. it was initially surmised that increasing sybr green i quantity would be informative; however, the sensitivity of quantitect  based formulation) to sybr green i inhibition precluded the ability to apply anything but a moderate increase in sybr green i quantity. subsequently, two alternative non-taq formulations were evaluated, based on the speculation that they could be more resilient to higher quantities of sybr green i. these were dynamo, formulated with an engineered t. brockianus dna polymerase fused to a non-specific dna-binding region, and fullvelocity, formulated with an unspecified archaeal dna polymerase.

as summarized in figure  <dig>  three series of replicate amplification reactions supplemented with progressively greater quantities of sybr green i demonstrate, based on similarity in amplification profile position, that addition of 1-2x sybr green i was marginally inhibitory to both dynamo and fullvelocity. in contrast, quantitect was greatly impacted by addition of  <dig> x sybr green i, as reflected by the large shift in position, and extensive scattering, of the replicate amplification profiles . more significant, however, is the substantive increase in fmax produced by increasing sybr green i, establishing that reaction fluorescence intensity is dependent on sybr green i quantity. notwithstanding this dependency, increasing sybr green i quantity had no apparent impact on the general sigmoidal shape of the amplification profiles. the amplification profiles produced by dynamo and fullvelocity further indicate that the sigmoidal shape of sybr green i amplification profiles is not unique to taq.

the sigmoidal character of these amplification profiles was further substantiated by lre analysis, which revealed a linear domain corresponding to the central region of each amplification profile . these plots further reveal that the fluorescence readings generated by later cycles do not conform well to the lre model, as reflected by the "spilling" of points off of the lre line. importantly, this is consistent with what was previously observed during development of the scf method, where anomalies associated with the plateau phase were common, and significantly distorted nonlinear regression analyses. this subsequently required exclusion of the plateau cycles from the nonlinear regression, based upon a somewhat esoteric method for selecting a "cutoff cycle"  <cit> . nevertheless, as is apparent in the lre plots presented in figure  <dig>  a kinetic-based approach provides the ability to identify anomalous fluorescence readings based on loss of conformity with the lre model.

thus, while testing of the quantitative capabilities of lre analysis requires additional tools, this initial assessment does provide substantive evidence that sybr green i real-time profiles can conform well to that predicted by sigmoidal modeling. furthermore, these results suggest that despite the apparent complexities of the classic boltzmann sigmoid function, it should be possible to develop a sigmoidal model for the polymerase chain reaction, derived from the two kinetic parameters predicted by the lre model to govern pcr amplification.

derivation of a kinetic-based sigmoid model for the polymerase chain reaction
conformity of pcr amplification to the classic boltzmann sigmoid function  poses the question as to how c1/ <dig>  k and fmax relate to Δe and emax. although equation  <dig> predicts that fmax is defined by the ratio of emax to Δe, it is less clear how k and c1/ <dig> relate. as summarized in additional file  <dig>  c1/ <dig> and k can be eliminated through a series of rearrangements and substitutions, producing two functions that allow modeling of pcr amplification based solely on Δe and emax. on the assumption that the fluorescence background  is zero, these are:

  f0=fmax⁡1+c 

  fc=fmax⁡1+−c 

thus, once values for Δe and emax have been obtained via lre analysis, equation  <dig> can be used to convert individual fc readings into target quantity expressed in fluorescence units . target quantification is then based on averaging the f <dig> values derived from the cycles used in lre analysis, followed by conversion into the number of target molecules via optical calibration. furthermore, once an average f <dig> value has been obtained, the corresponding amplification profile can be modeled by using equation  <dig> to calculate predicted fc values for each cycle. as illustrated in the next section, this series of computations is capable of modeling pcr amplification to a very high degree of precision, and without the need to conduct nonlinear regression analysis.

it should noted that chervoneva et al.   <cit>  have recently described modeling of real-time pcr using a logistic function identical to equation  <dig>  however, the primary objective of their study was to determine emax for relative quantification, based on nonlinear regression analysis. this contrasts the kinetic approach taken in this study, in which absolute quantification is based on converting fluorescence readings into target quantity using equation  <dig> 

implementation of lre quantification
initial setup
with equations  <dig> and  <dig> in hand, the practical and analytical capabilities of lre quantification can be tested. furthermore, the computational simplicity of the methodology makes it amenable to manual implementation using a spread sheet. additional data file  <dig> contains the ms excel templates used for the data analysis conducted in this study. the general approach involves a series of steps that culminates in a recursive process in which the conformity of individual fluorescence readings is used to optimize the analysis.

fluorescence readings are first imported into the spreadsheet and background fluorescence subtracted, as estimated by averaging 6– <dig> baseline cycles . replicate fc datasets are then averaged , cycle efficiency  calculated for each cycle using equation  <dig>  and ec plotted against fc to generate what is called the "lre plot" . a contiguous group of points is then selected for linear regression analysis, from which estimates of Δe  and emax  are obtained, a process called "lre analysis". referred to as the "lre window", this region is elemental to defining the sigmoidal model of an amplification profile. the linear integrity of the lre window is thus crucial to the quantitative accuracy of the assay.

various methods for selecting the lre window size and position have been tested. in practice, details of lre window selection for many amplicons are somewhat inconsequential, due to the high degree of conformity that can be generated. nevertheless, as noted earlier, later cycles of some amplification profiles do not conform well to sigmoidal modeling. this is apparent in figure 2b, d and 2f in which points corresponding to cycles within the upper region of each amplification profile "drift" off the lre line. a key objective is therefore to avoid inclusion of these non-conforming cycles into the lre window, while at the same time selecting the largest possible lre window in order to maximize the precision of the linear regression analysis.

a logical starting point is to initially select a small lre window positioned within the lower region of an amplification profile, and to progressively add consecutive cycles to the lre window , repeating the linear regression analysis as each new cycle is added to the lre window. this is continued until encountering a cycle that clearly does not conform, based upon divergence from the lre line. although this approach can be reasonably effective, a more objective method that has proven to be both simpler and more sensitive becomes evident during assessment of lre quantitative precision. however, before this can be described, another foundational principle of lre quantification must first be implemented.

target quantification, recursive analysis and the precision of lre modeling
a fundamental attribute of lre quantification, as exemplified by equation  <dig>  is the ability to convert individual fc readings into a target quantity, once estimates for Δe and emax have been obtained. indeed, several intriguing behaviors become evident when this is applied across an entire amplification profile, generating what is referred to as the "f <dig> plot". the most notable is that f <dig> values encompassed by the lre window are very similar, a trend that can extend to fc readings close to the baseline fluorescence . in addition to providing multiple estimates of target quantity from each amplification profile, this illustrates the extraordinary precision that lre modeling can achieve. typically, high-quality fluorescence datasets generate a coefficient of variance  for f <dig> of < ±  <dig> % over a 4– <dig> cycle lre window .

a second attribute of f <dig> plots is characterized by a rapid divergence of f <dig> values derived from later cycles within many amplification profiles. as would be anticipated, this coincides precisely with the loss in conformity to the lre line generated in the corresponding lre plot. although the examples presented in figure  <dig> produced a sweeping upward arc, downward arcing have also been commonly observed. in either case, the abruptness of this divergence provides a dependable marker for loss of conformity, which in turn allows the upper limit of the lre window to be objectively defined.

based upon the general principle of conformity, the utility of using recursive analysis to optimize the lre window size was explored, primarily to develop an algorithm for automating lre data analysis. the approach starts by defining a small lre window in the lower region of a profile. although software implementation can provide several methods for determining the lower boundary of the lre window, manual implementation was based upon empirical determination of the lowest fc reading that generates sufficient precision to produce a reliable ec value for the first cycle of the lre window . linear regression analysis is conducted on this preliminary lre window, and the fc readings across the entire profile converted to f <dig> . an average f <dig> value is then calculated from the fc readings encompassed by the lre window. it should be noted that this includes the fc generated in the cycle immediately preceding the start cycle, due to the fact that this fc reading is the denominator used to calculate ec for the start cycle.

the recursive nature of this approach derives from comparing the average f <dig> produced by the lre window, to that generated by the cycle immediately following the last cycle of the lre window. the difference, expressed as a percentage of the average f <dig>  is then used to assess the level of conformity. a threshold value  is then used to determine whether this cycle should be subsumed into the lre window. if so, the lre window is expanded to include this next cycle, and the lre analysis is repeated. this recursive process is continued until a cycle is encountered that produces a f <dig> difference larger than the threshold. an example of this recursive approach is presented in figure  <dig>  which provides a more detailed illustration of the process. worthy of note are the relatively small differences  in the average f <dig> produced by expanding the lre window to include cycles that are clearly nonconforming, as is illustrated in figure  <dig>  even though a more formal documentation of the robustness of lre analysis will not be attempted here, the prototypic java program introduced at the end of the study provides the ability to quickly assess the impact of lre window size and position.

f <dig> plots thus illustrate the two key attributes that underpin the quantitative capability of the lre method. the first is the ability to objectively define the lre window based on conformity of the derived f <dig> values. the second is estimating target quantity based upon averaging f <dig> values encompassed by the lre window . furthermore, the cv of the lre window f <dig> values is an effective indicator of the general quality of the fluorescence dataset , which is a major determinant of the general efficacy of lre-based quantification . another indicator of the precision that can be achieved with lre modeling is the correlation between actual fc readings and that predicted by equation  <dig>  the profiles presented in figure  <dig> exemplify high-quality datasets, which can generate predicted fc values that differ on average <  <dig> % of the actual fc readings within the lre window.

maximizing optical precision
notwithstanding the high degree of precision that can be achieved, it became evident during the early stages of lre implementation that a number of optical factors can compromise, sometimes severely, the quality of a fluorescence dataset. before addressing further the quantitative capability of lre analysis, the role of reaction fluorescence determination and assay optics should first be considered. as might be anticipated, optical precision is a central determinant of the quantitative accuracy and reliability that can be achieved with lre quantification. two simple steps can be taken to increase optical precision. the first is to take multiple fluorescence readings at the end of each cycle  and to use the average for determining reaction fluorescence. this reduces the error-of-measurement produced by the instrument's optical system. the second is to conduct technical replicates for each sample  and to construct a single fc dataset by averaging the fluorescence readings generated by the replicates.

although conducting lre analysis on each individual replicate amplification profile and averaging the resulting Δe, emax and f <dig> values generally produces comparable results, averaging fc readings from replicate reactions prior to lre analysis can increase fc precision substantially, particularly for fc datasets of marginal quality. an exception is for samples containing <  <dig> target molecules, due to the fact that poisson distribution generates extensive scattering of the replicate amplification profiles, such that fc averaging becomes less effective.

another key aspect is to monitor run-to-run optical variances by including a quantitative standard within each run. as described in the next section, lambda gdna has proven to be a reliable quantitative standard that allows monitoring of the many factors impacting assay optics, through the ability to express the fluorescence intensity of an assay in quantitative terms. this introduces the concept of calibrating the optical component of real-time qpcr, which is the last foundational principle of lre quantification.

derivation of absolute scale
one of the principal attributes of employing a universal quantitative standard in combination with sybr green i detection is the ability to calibrate fluorescence intensity by expressing assay fluorescence in terms of amplicon mass. referred to as "optical calibration", this can be accomplished by amplifying a known quantity of a standard and dividing the resulting f <dig> value by the predicted target quantity, expressed as nanograms of amplicon dna . for lambda gdna, this takes the form:

  m0=nglambda×as <dig>  

where nglambda is the mass of lambda gdna in nanograms, as is the amplicon size and  <dig>  is the genome size of lambda  so that:

  ocf=f0m <dig> 

where ocf is defined as the "optical calibration factor", expressed in this study as fluorescence units per nanogram of double-stranded dna . the ability to express fluorescence intensity in quantitative terms provides a simple, centralized quality control component that incorporates all of the many factors impacting the optics of an assay. these include factors related to reaction setup, such as batch-to-batch variations in enzyme formulation, reaction vessels and closures, in addition to the performance of the optical system.

an illustration of this approach is presented in figure  <dig>  which summarizes optical calibrations conducted with five reaction formulations supplemented with various quantities of sybr green i. derived from the analyses of  <dig> individual amplification runs, this large dataset provides a general indication of the variances generated by each lre parameter, culminating in an ocf average cv of ±  <dig> % across all five reaction formulations. all things being equal, this should be indicative of the resolution that a lre-based quantitative assay can achieve. indeed, similar variances were generated by repeated quantifications of eleven mrna targets, as described in the next section. of greater significance, however, is the ability to use optical calibration to establish an absolute quantitative scale.

the concept of optical calibration was first introduced within the context of standard curve-based quantification, in which it was recognized that correlating reaction fluorescence to dna mass could provide a simplified method for establishing quantitative scale  <cit> . optical calibration in a form similar to that presented here was subsequently described within the context of absolute quantification using nonlinear regression   <cit> . in either case, it was evident that the ease of conducting absolute qpcr would be greatly increased by implementing optical calibration using a universal quantitative standard. in addition to eliminating the need to prepare target-specific standards, when combined with lre analysis optical calibration circumvents the need to construct standard curves.

under this approach, absolute quantification is achieved by first converting the derived f <dig> value into dna mass using a lambda-based optical calibration factor :

  m0=f0ocf 

for double-stranded dna targets, or:

  m0=f <dig> 

for single-stranded dna targets, where m <dig> is target mass expressed as nanograms of amplicon dna. conversion into the number of target molecules then simply requires relating m <dig> to the molecular size of the amplicon:

  n0=m0× <dig> ×1011as 

were n <dig> is the number of target molecules, as is amplicon size in base pairs, and  <dig>  ×  <dig> is the number of base pairs per nanogram of dsdna  <cit> . in view of the operational simplicity provided by this approach, few additional requirements would be necessary to fully automate absolute quantification. however, one important caveat remains to be addressed, which is the implicit assumption that all amplicons generate similar fluorescent intensities.

application to gene expression profiling
key to utilizing a single absolute scale across multiple targets is the underlying assumption that amplicon-specific factors such as amplicon size and/or base pair composition, do not significantly impact the intensity of sybr green i fluorescence . one approach to testing this assumption would be to compare quantifications generated by a large number of diverse amplicons targeted to a quantified standard , with the expectation that differences in optical intensity would generate quantitative biases. although such an analytic approach can be effective, the approach chosen for this study was based upon applying lre quantification to gene expression analysis. an in-house initiative to develop large-scale expression profiling in arabidopsis thaliana provided a large database from which to select candidate targets for absolute quantification. eleven transcripts were selected based primarily on encompassing a quantitative range indicative of transcriptional factors, which for this group of targets was estimated to be 10– <dig>  transcript molecules per  <dig> ng of total rna. furthermore, these targets encompass amplicon sizes of 85– <dig> bp and amplicon gc contents ranging from 40–53% , from which it was expected that any substantive differences in amplicon fluorescence would generate recognizable quantitative biases.

the most notable initial outcome was that all amplicons generated amplification profiles that conformed well to the lre model, as reflected by an average lre linear regression correlation coefficient  of >  <dig>  . indeed, of the >  <dig> primer pairs tested to date, all have conformed well to lre modeling. unexpectedly, increasing sybr green i quantity had little or no impact on the r <dig> of the lre analysis, despite a greater than x <dig> increase in fmax at the highest sybr green i quantity examined . increasing sybr green i quantity also did not reduce the quantitative variance of the derived f <dig> values .

this high level of precision is further reflected by the low variances generated by the corresponding emax and Δe determinations . worthy of note is the standard deviation of emax determination, which averaged ±  <dig> % across all targets and reaction formulations. in addition to demonstrating the robust nature of lre analysis, such low variances support the contention that pcr amplification is an inherently precise process. furthermore, without delving into the mathematics of the lre model, it should be noted that emax has by far the greatest impact on target quantification , such that emax estimation is the primary determinant of the quantitative resolution that can be achieved. based upon the low variances in emax and Δe determinations, it is not surprising that the derived f <dig> values for each target generated an overall average cv of ±  <dig> % . as noted earlier, this is very similar to the average ocf cv of ±  <dig> %, indicating that lre has the potential to resolve quantitative differences of < ± 25%. quantitative resolution also appears not to be impacted by increasing sybr green i quantity, with each of the five reaction formulations producing similar f <dig> cvs .

this dataset also illustrates the limitations of expressing target quantities in units dependent on assay setup and instrument optics. in this case, it is not possible to directly compare f <dig> values across different enzyme formulations, due to differences in reaction intensity and gain settings. however, converting f <dig> quantities into the number of target molecules provides an absolute context that allows comparison of target quantities generated under disparate assay conditions. although this may seem obvious, it is worth stressing that provision of a universal context is what makes absolute quantification so compelling, in that absolute values transcend issues of assay design, instrumentation, and even the type of data analysis applied. table  <dig> provides an illustrative example, which demonstrates that sybr green i quantity and enzyme type had a modest, if any, impact on lre quantification. furthermore, the average cv of ±  <dig> % produced across all five reaction formulations is in general agreement with the contention that lre quantification is able to resolve quantitative differences smaller than ± 25%.

following lre analysis , the fc readings encompassed by the lre window were converted to f <dig>  and the average f <dig> value converted to the number of target molecules  using the ocf generated by each respective reaction formulation . the gain setting was adjusted to prevent saturation of the photomultiplier . gain: the photomultiplier gain setting, ocf: optical calibration factor , qt: quantitect, dyna: dynamo; fv: fullvelocity, sg: sybr green i, sd: standard deviation, nd: not determined.

notwithstanding the high level of precision that can be achieved, it is important to note that despite expressing quantities as the number of target molecules, the quantitative context is still confined to the lre analyses used to generate the dataset. that is, this type of comparison is unable to verify absolute accuracy, in that any biases generated by lre analysis and/or optical calibration will generate quantitative biases. as is presented in the following two sections, the quantitative context can be expanded by determining how closely target quantities produced by other methods correlate with the lre-derived quantities.

absolute quantification via ct
positional analysis, as exemplified by the threshold method, has predominated since the introduction of real-time pcr over  <dig> years ago  <cit> , and is the quantitative methodology upon which all commercial platforms currently rely. based upon the fractional cycle at which reaction fluorescence reaches a threshold value, the threshold method defines profile position through a common reference point called the threshold cycle or "ct". absolute quantification is accomplished via standard curves constructed with target-specific quantified standards  <cit> , which are technically challenging and prone to generating quantitative errors. however, previous recognition that a sigmoidal-derived emax is analogous to a slope-derived amplification efficiency generated from a standard curve  <cit>  suggests an alternative method for converting ct values into target molecules, using the exponential equation:

  f0=ftct 

were ft is the fluorescence threshold used to derive ct, and emax is the amplification efficiency derived from lre analysis. importantly, this approach allows the application of optical calibration so that ct-based absolute quantification can be conducted without standard curves.

ct values generated by the amplification profiles summarized in figure  <dig> were converted to f <dig> using equation  <dig>  and then converted to target molecule number via the ocf as used for lre quantification .

accuracy is typically determined by repeatedly measuring some traceable reference standard. while this approach could be effective for well-characterized targets, the use of reference standards is impractical for applications involving large numbers of diverse targets. what would be more effective is an alternative quantitative method that would, as much as possible, be free from potential biases generated by real-time qpcr. a superbly effective solution has been provided by wang and spadoro  <cit> , which exploits the single-molecule sensitivity inherent to pcr amplification. founded upon the principles of poisson distribution, this approach provides an elegant means for absolute quantification that would be familiar to any microbiologist or virologist; that is, the method of "limiting dilution assay".

absolute quantification via limiting dilution assay 
the efficacy of lda derives from its ability to achieve absolute quantification independent of the kinetic and optical principles upon which real-time qpcr is dependent, and does not require a quantitative standard. lda relies solely on the frequency of reactions that fail to produce an amplification product, utilizing pcr to only determine whether a target molecule is present within an individual aliquot. furthermore, the assay is not dependent on how the pcr amplification is actually conducted, requiring only that it generate single-molecule sensitivity, and that amplification reactions that generate false positives  are either absent or can be identified. thus, an economical sybr green i-based lda could, for example, be used to evaluate the quantitative accuracy of a probe-based assay.

as the name implies, lda involves diluting the sample to a limit, which in this case are individual target molecules. as dictated by poisson distribution, when a sample is diluted to a point near to one target molecule per aliquot, a high proportion of aliquots will not contain a target molecule. it is the frequency of these "nil" aliquots that allow the average target quantity to be calculated using the equation:

  average target molecules per aliquot = -ln 

where nil and total are the number of reactions that fail to produce amplicon dna and the total number of reactions conducted, respectively  <cit> . target quantity is then calculated based on the dilution factor used to reach single a molecule concentration. it is important to note that lda is self-validating, such that if the sample is under-diluted no nil reactions will be produced, whereas if the sample is over-diluted all reactions will be nil. as a result of this intrinsic self-validation, lda has in practice proven to be exceptionally reliable.

based upon the n <dig> values produced by lre quantification, a sample of each reverse transcriptase reaction was diluted to a predicted  <dig> – <dig>  target molecules per aliquot for each of the eleven cdna targets, and replicate amplification reactions were conducted to determine the frequency of aliquots lacking a target molecule . the number of target molecules  per aliquot was then calculated using equation  <dig> and the predicted number of target molecules in the reverse transcriptase reaction  was calculated by multiplying by the dilution factor .

recognized anomalies impacting lre quantification
during the development and testing of lre quantification, a number of factors were identified that either compromise or prohibit effective application of lre quantification. referred to here as "anomalies", these actually encompass a broad variety of factors, ranging from primer pair performance to optical precision and data processing of fluorescence readings.

compromised efficiency of target priming and elongation
a fundamental principle impacting the quantitative accuracy of real-time qpcr is simple in principle, but not necessarily apparent; that is, disrupting target priming and elongation. best exemplified by single nucleotide polymorphisms , base-pair mismatches between a primer and the target can severely disrupt initiation of pcr amplification, but once an amplicon molecule is formed these mismatches are lost, such that amplicon amplification proceeds unhindered. lre analysis of the resulting amplification profile is thus incapable of detecting target priming anomalies. indeed, any factor that disrupts the efficiency of target priming and elongation that does not equally reduce the efficiency of amplicon priming and elongation, will necessarily generate an under-estimation of target quantity.

such a situation can be prevalent for species or genotypes where snp occurrence is undocumented. a simple but effective solution is to compare quantifications generated by multiple amplicons per target . disruption of target priming and elongation then becomes evident as a shift to later cycles, of the profile generated by the impacted primer. although not yet formally documented, preliminary work has also indicated that certain pcr inhibitors can also selectively disrupt target priming and elongation, an issue that is fundamentally more difficult to address.

compromised optical integrity
factors impacting assay optics can severely compromise lre quantification. a simple example encountered during an early set of experiments, was a problem eventually traced back to optical variance in the reaction tubes, which produced up to a 40% difference in fluorescence intensity between identical amplification reactions . as described earlier, monitoring assay optics via amplification of quantitative standard such as lambda gdna, has proven effective for identifying such optical anomalies via differences in the resulting ocf values.

the initial processing of fluorescence datasets is another factor that can impact optical precision. it is paramount to ensure that beyond fluorescence background subtraction, no additional modifications are performed on the fc datasets. the most commonly encountered data manipulation is often referred to as "curve smoothing" in which a running average is applied in order to generate more aesthetically pleasing amplification profiles. even though the apparent sigmoidal character of the resulting profiles can be improved , such modifications significantly distort lre analysis, the most evident being a large reduction in emax. some forms of optical normalization could also be expected to be problematic, although this has not been formally investigated. as a general rule, it is recommended that as few modifications as possible be conducted on fc datasets, even to the point of conducting background subtraction manually in order to ensure an accurate baseline estimate, as was necessary in this study .

baseline drift
another form of fc data modification attempts to correct one of the most acute anomalies we have recognized to date; that is changes in the background fluorescence. referred to as "baseline drift", we have found that some primer pairs, in addition to sample-specific factors can produce a progressive increase in fluorescence background. importantly, some data processing packages attempt to correct for fluorescence drift by adjusting the values of the fc readings. while this can be somewhat effective in reducing the quantitative inaccuracies produced by baseline drift, it also leads to unintended distortions that remain hidden if the user is unaware of the underlying fc manipulation . baseline drift can also be quite subtle and is most effectively recognized by visual examination of raw fluorescence data before subtraction of background fluorescence.

our investigation into the source of baseline drift using sybr green i detection has revealed it to be a complex phenomenon, likely generated by several mechanisms. one clear trend is a strong primer-specific effect . unfortunately, repeated attempts to identify reliable predictors, such as sequence complementarity or secondary structural elements  within the primers, have to date been largely unsuccessful. unidentified sample-specific factors have also been found to generate extensive baseline drift across multiple primer pairs that normally do not generate baseline drift. raising the annealing and elongation temperature and/or diluting the sample, as well as re-purifying the rna before conducting reverse transcription, have been found to reduce baseline drifting in some cases.

automation
lre quantification is primarily a matter of data processing. in practice, the only component that requires any significant insight is lre window selection. if implementation of lre analysis could be relegated to a computer program, the user would only need to supply the size of the amplicon and an optical calibration factor in order to complete the quantification. a prototypic java program  supports this contention by automating start cycle selection and optimizing lre window size using the recursive approach described earlier. this program brings together all of the elements of lre quantification, providing the opportunity to examine the interrelationships between lre window selection, the resulting f <dig> and n <dig> values, and the predicted fc values generated by the lre model. this includes the ability to manually adjust the lre window, to change the associated ct value by adjusting the fluorescence threshold, and to enter values for the ocf and amplicon size . entry of an fc dataset via pasting from clipboard also provides limited data processing capabilities.

comparison to other automated data processing packages
to further examine the performance capabilities of lre, the automated lre quantification provided by the prototypic java program was compared to two other publicly available automated data processing packages, which also analyze the fc readings generated by individual amplification reactions. the first called linreg, has become a commonly used package for determining amplification efficiency without the use of a standard curve, and employs linear regression analysis of the log-linear region present within the lower region of an amplification profile. based upon the presumption that amplification efficiency is constant within this region, amplification efficiency is determined from the slope of a log vs. cycle plot, with target quantity  determined by the intercept, that is, when cycle =  <dig>  <cit> . the second package called miner  <cit>  determines amplification efficiency via nonlinear regression using the same exponential model upon which linreg is based. miner also generates a ct value based upon a dynamic fluorescence threshold . although miner does not provide an f <dig> value, equation  <dig> allows a f <dig> value to be calculated using the amplification efficiency, ft and ct values generated by miner.

the approach taken for the comparison was to select a single, representative amplification profile for each of the eleven cdna targets, across all five assay formulations. this provided a total of  <dig> amplification profiles for analysis, which are provided in additional file  <dig>  each amplification profile was then subjected to automated analysis by each of the three packages using default values, from which an amplification efficiency estimate and an f <dig> value were obtained . the ultimate objective, however, was to assess the absolute quantitative accuracies by comparison to the lda quantifications presented in table  <dig>  in order to accomplish this, it was first necessary to conduct individual optical calibrations for each of the three analysis packages, in order to compensate for the biases specific to each package. most notable is that both linreg and miner generated amplification efficiency estimates that were generally lower than that produced by lre . all of the lambda gdna amplification profiles  were thus subjected to analysis by each of the three packages, producing an optical calibration factor specific to each analysis package for each of the five assay formulations. f <dig> values were then converted to the number of target molecules using the corresponding ocf, as summarized in additional file  <dig> 

discussion
what should be expected from real-time quantitative pcr?
often for historical reasons expectations for quantitative pcr can differ greatly, and can be influenced as much by personal perspective as by methodological considerations. the plethora of choices currently available for detection chemistry, enzyme formulation, cycling regime and instrumentation provide many compelling examples. it is not uncommon, for example, for widely differing protocols to be applied to seemingly identical applications, with little or no supporting evidence that any single assay design is superior. exacerbated by a profound lack of standardization, it is not surprising that many reports caution about the limitations of real-time qpcr  <cit> .

evaluating real-time qpcr technologies is further complicated by the fact that performance expectations can be highly context dependent, and that the application of performance standards  can vary as widely as context. biomedical diagnostics, for example, provide many poignant scenarios in which assay performance supersedes assay design. without verifiable accuracy and reliability, methodological details can become immaterial. arguably, gene expression analysis often represents the other extreme where, for reasons of technical simplicity, transcript quantities are frequently expressed as relative differences. in addition to providing a very limited quantitative context, relative quantification provides little or no opportunity to assess quantitative accuracy.

despite a broad range of performance expectations, it should be evident that absolute qpcr can enhance the efficacy of any quantitative assay, irrespective of context. not only does absolute quantification impart a universal perspective that facilitates data interpretation, it also allows assay performance to be defined in absolute terms. furthermore, absolute quantification allows decoupling of target quantification from assay implementation, such that quantitative data generated by disparate assay designs and/or data processing methodologies can be directly compared.

notwithstanding the apparent utility of absolute quantification, the technical complexity and resources required by current protocols is daunting. not only has this greatly impeded broad adoption of absolute quantification, the complexities of implementing even the most basic quantitative assay hinders access to real-time qpcr technologies, particularly for casual users. furthermore, the necessity for constructing target-specific standard curves severely limits both the efficacy and capacity of absolute quantification.

founded on recognition that pcr amplification is inherently sigmoidal, this study describes methodologies that provide effective, and in some cases simple solutions for conducting absolute quantification without standard curves. utilizing a kinetic-based approach, lre analysis can be applied to any sybr green i-based assay, with few qualifications other than that the fc datasets be of reasonable quality. lre quantification does not rely on user-supplied standards and, if automated data processing is implemented, requires little or no training beyond that required for preparing amplification reactions. particularly in view of the impact that absolute qpcr could have on a broad range of applications, these attributes alone provide a compelling argument for moving beyond the historical, often dogmatically held concepts that have persisted since the introduction of real-time qpcr. key to this endeavor is to develop an effective understanding of the fundamental principles of absolute quantification, many of which transcend details of assay design and data analysis methodology.

the two founding principles of absolute quantification
despite the seemingly complex mix of technologies and methodologies, absolute quantification requires measurement of only two fundamental parameters – amplification kinetics and quantitative scale – regardless of detection chemistry, enzymology or instrumentation. historically, this has been accomplished by constructing target-specific standard curves, in which amplification efficiency is derived from the slope and quantitative scale is derived from the intercept  <cit> .

amplification efficiency determination
assessing amplification kinetics has long been recognized as a major factor impacting qpcr, due to the fact that errors in amplification efficiency determination can lead to large quantitative errors. nevertheless, early real-time qpcr protocols simplified target quantification by assuming amplification efficiency to be identical for all amplicons and all samples  <cit> . indeed, although the slope of a standard curve provides an estimate of amplification efficiency, a similar assumption must still be made; that is that the amplification efficiencies of all samples are identical, or at least similar, to that predicted by a standard curve. thus, even if a standard curve can be effectively constructed, quantitative accuracy cannot be ensured due to the potential for sample-specific factors to reduce amplification efficiency. this can be a major concern, particularly for samples originating from sources known to contain inhibitory compounds, such as for environmental samples and for many types of biomedical samples. this deficiency alone would be expected to exclude real-time qpcr from a potentially large category of applications, where unidentified quantitative errors cannot be tolerated. lre analysis provides a fundamental solution to this issue, through the ability to monitor amplification efficiencies within individual amplification reactions.

notwithstanding the innate limitations of standard curve-based quantification, the implications of amplification efficiency determination extend beyond issues of quantitative accuracy, playing a predominant role in the operational component of real-time qpcr. amplification kinetics is determined by a combination of reaction setup and cycling regime. thus, amplification efficiency must be determined for every new amplicon, and re-determined if changes are made to reaction setup and/or cycling regime. as such, difficulties in determining amplification efficiency limit the number of targets and reaction conditions that can be tested. under this context it becomes apparent that lre confers operational attributes important to developing reliable, high-capacity qpcr applications, which are beyond what are possible using current technologies. for example, lre analysis allows performance assessments to be based on amplification of bona fide samples, as opposed to the common practice of using artificial targets such as plasmids or oligos, and with a capacity limited only by the number of amplification reactions that can be run. of a more fundamental nature is the potential to extend performance assessment beyond amplification efficiency, to what could be termed as "assay robustness". this could allow assay performance to encompass parameters such as resilience to inhibitors and/or response to changes in cycling regimes , which are only two among many possible examples.

derivation of absolute scale via optical calibration
although it is generally recognized that amplification efficiency can be derived from the slope of a standard curve, little or no attention has been given to the fact that the intercept establishes absolute scale by relating dna mass to reaction fluorescence intensity  <cit> . recognition of this fundamental principle presents a simple solution to the greatest limitation associated with conducting high-capacity absolute quantification using current protocols, which is reliance on target-specific standards. based on the presumption that sybr green i generates similar fluorescence intensity for all amplicons, lambda gdna can be exploited as a universal quantitative standard for establishing absolute scale, using a simple, standardized protocol referred to as optical calibration  <cit> . furthermore, by relegating establishment of quantitative scale to a universal standard, error of scale is restricted to a single, well-defined entity.

importantly, utilization of optical calibration is not limited to sigmoidal-based quantification. ct-based quantification can utilize the same strategy if ct values are converted into f <dig>  this can be accomplished using the fluorescence threshold in combination with the emax derived from lre analysis . additionally, when the fluorescence threshold is not fixed  <cit> , the impact of inter-run differences in ct values are eliminated, as differences in ft are compensated for during the conversion of ct to f <dig>  finally, it should be noted that even though the principles of optical calibration described here have been developed using sybr green i, swillens et al.  describe an optical calibration methodology for hydrolysis probes that correlates fluorescence intensity to probe mass  <cit> . this suggests that probe-based assays could also implement optical calibration for conducting absolute quantification.

assay validation via limiting dilution assay
for any analytical technique, the ultimate performance benchmarks are accuracy and reliability. the relevancy of this to real-time qpcr is particularly evident in view of the large number of available choices for detection chemistry, enzyme formulation and instrumentation, all of which generates an enormous number of options to choose from. indeed, many choices are based on the presumption of superior performance, even at the expense of reducing the practicalities of assay implementation and/or of increased cost. however, despite the many claims of superior performance, the paucity of supporting evidence can be striking.

limiting dilution assay provides a fundamental, potentially universal solution to the dilemma of how to effectively assess quantitative accuracy, through the ability to conduct absolute quantification independent of real-time qpcr. lda is simple to conduct, is independent of the kinetic and optical parameters upon which real-time qpcr is founded, does not require a quantified standard and is intrinsically self validating. as such, lda provides potential solutions to the long-standing challenge of effectively determining true differences in assay performance, whether comparing reaction formulation, instrumentation, or as is the case for this study, data processing models. in view of the multitude of choices that currently confounds real-time qpcr, lda could be instrumental to establishing standards in which absolute accuracy is the hallmark of assay performance.

CONCLUSIONS
founded upon a new paradigm for real-time qpcr, this study introduces several novel concepts and methodologies that extend the fundamental capabilities of absolute quantification. most notable is the ability to monitor amplification kinetics within individual amplification reactions, providing the capability to reveal sample-specific inhibition that would otherwise generate unidentified quantitative errors. in addition, utilizing lambda gdna for optical calibration not only eliminates reliance on target-specific standard curves, but as well, contributes to the standardization of real-time qpcr by centralizing the provision of quantitative scale to a single, highly defined, universal quantitative standard. exploiting limiting dilution assay for absolute quantification provides the capability to independently evaluate absolute accuracy, irrespective of assay methodology, which could also contribute greatly to the standardization of real-time qpcr technologies. in relation to operational issues, lre provides several attributes that facilitate large-scale absolute quantification, with the potential to extend assay performance to include the general concept of assay robustness. ultimately, however, the ability to automate lre quantification is most illustrative of the potential for developing high-capacity applications, reducing the resources required for conducting absolute quantification to little beyond that needed for reaction preparation.

