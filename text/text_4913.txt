BACKGROUND
the strict relationship between protein structure and function, together with the increasing gap between the number of sequences and known structures in bioinformatics databases, has lead to the need of automatic protein structure prediction systems. unfortunately, protein structure prediction is a very difficult task. chemical properties of aminoacids and complex interactions between different parts of the same protein and/or the protein and the surrounding environment should be taken into account for an accurate folding reconstruction. on the other hand, protein structure can be simplified by considering only the secondary structure, i.e. the local conformation which each residue belongs to, usually represented as: alpha-helix , beta-sheet , and random-coil . although the knowledge about secondary structure is less informative than the tertiary one, an accurate secondary prediction can help in building the complete protein folding  <cit> . furthermore, it can be a useful starting point to determine the protein function. in fact, most active sites are associated to particular conformations  of secondary structures , more conserved during the evolution than the corresponding primary sequence.

artificial neural networks  are the most widely-used technology for predicting secondary structures. typically, the input required to predict the secondary structure of a target residue is extracted from a windows of aminoacids centered on the residue itself, whereas three  or eight  outputs represent the relevant structural classes. the prediction accuracy is commonly measured by the percent of residues correctly predicted versus the overall number of residues in the dataset . it is worth pointing out in advance that the typical output encoding adopts three values, and that the mapping between eight- to three-class encoding is not unique. in this paper, we adopted the "default" mapping proposed in  <cit> , where α←{h, g, i}, β←{e, b}, and c represents everything else. the q <dig> index is evaluated in accordance with this choice.

there are a variety of secondary structure prediction methods proposed in the literature. early prediction methods were based on statistics headed at evaluating, for each amino acid, the likelihood of belonging to a given secondary structure  <cit> . the main drawback of these techniques is that, typically, no contextual information is taken into account, whereas nowadays it is well known that secondary structures are determined by chemical bonds that hold between spatially-close residues. a second generation of methods exhibits better performance by exploiting protein databases, as well as statistic information about amino acid subsequences. in this case, a limited window of aminoacids  is taken into account, centered around the residue to be predicted. several methods exist in this category, which may be classified according to  the underlying approach, e.g., statistical information  <cit> , graph-theory  <cit> , multivariate statistics  <cit> ,  the kind of information actually taken into account, e.g., physico-chemical properties  <cit> , sequence patterns  <cit> , and  the adopted technique, e.g., k-nearest neighbors  <cit> , artificial neural networks   <cit> .

the most significant innovation introduced in prediction systems was the exploitation of long-range and evolutionary information contained in multiple alignments. the underlying motivation is that active regions of homologous sequences will typically adopt the same local structure, irrespective of local sequence variations. phd  <cit>  is one of the first ann-based methods that make use of evolutionary information to perform secondary structure prediction. in particular, after searching similar sequences using blastp  <cit> , clustalw  <cit>  is invoked to identify which residues can actually be substituted without compromising the functionality of the target sequence. to predict the secondary structure of the target sequence, the multiple alignment produced by clustalw is given as input to a multi layer ann. the first layer outputs a sequence-to-structure prediction which is sent to a further ann layer that performs a structure-to-structure prediction aimed at refining it.

further improvements are obtained with both more accurate multiple alignment strategies and more powerful neural network structures. for instance, psi-pred  <cit>  exploits the position-specific scoring matrix  built during a preprocessing performed by psi-blast  <cit> . this approach outperforms phd thanks to the psi-blast ability of detecting distant homologies. in more recent work  <cit> , recurrent anns  are exploited to capture long-range interactions. the actual system that embodies such capabilities, i.e., sspro  <cit> , is characterized by:  psi-blast profiles for encoding inputs,  bidirectional ranns, and  a predictor based on ensembles of ranns. further information about protein secondary structure prediction can be found, for instance, in  <cit> .

methods
in this paper a system that resorts to multiple experts for dealing with the problem of predicting secondary structures is presented, whose performances are comparable to those obtained by other state-of-the-art predictors. the system, called massp <dig> , performs an overall processing based on two main steps: first, a "sequence-to-structure" prediction is performed, by resorting to a population of hybrid genetic-neural experts, and then a "structure-to-structure" prediction is performed, by resorting to a feedforward ann.

sequence-to-structure prediction
in this subsection the module that has been devised to perform the first step, which stems from the one proposed in  <cit> , will be briefly described -focusing on the internal details that characterize an expert and on the behavior of the overall population of experts.

experts' "internals"
in its basic form, each expert e can be represented by a triple <g, h, w> , where:  g is a function used to select inputs according to the value of some relevant features,  h is an embedded predictor whose activation depends on g, and  w is a weighting function used to perform output combination. in particular, the output of e coincides with h for any selected input, otherwise it is not defined. in the case e contributes to the final prediction , its output is modulated by the value w of its weighting function, which represents the expert confidence  about its own prediction.

in the proposed system, the guard g of an expert is implemented by a "genetic" classifier able to match inputs according to a set of selected features deemed relevant for the given application. matching is "flexible", meaning that, for any given input x, g ∊  <cit> , thus allowing experts to show different "ranges of authority" on x. moreover, an expert can participate to the output combination activity only when the matching with x returns a value greater than a given threshold μ. in so doing, experts do not have complete visibility of the input space, i.e., they operate on different regions with "soft" boundaries.

as for the embedded predictor h, it typically consists of a feedforward ann trained and activated on the inputs acknowledged by the corresponding guard. each embedded predictor has three outputs corresponding to α, β, and c, all normalized in  <cit> , to facilitate output combination. the value w, ranging over the interval  <cit> , depends on the expert fitness, on the result of the match, i.e. on g, and on the reliability of the prediction made by the embedded predictor. the reliability r of a prediction is evaluated as the difference between the two highest values, as proposed in  <cit> . in the current implementation of the system, the weighting function of an expert e is: we = fe*ge*re.

in the task of predicting secondary structures, genetic guards are entrusted with processing some biological features, normalized in  <cit> , deemed relevant for the given task. this makes it possible to split the input space, with the goal of simplifying the training of neural classifiers. the corresponding features are reported in table  <dig>  for each feature, the operation actually performed on the current window r of residues, together with the underlying conjecture, are reported. the "aaindex" database  <cit>  has been used for retrieving relevant information about residues, e.g., hydrophobicity, polarity, charge, and volume.

it is relatively easy to fulfil the constraint that each feature must be limited in  <cit> , although the specific technique used to quantitatively evaluate a feature is strictly dependant on the feature itself. for instance, feature  <dig> is obtained by performing a matching – which may include a preliminary shift between the actual window  and a prototypical "target" window in which hydrophobicity fulfils the hypothesis about periodicity. in this case, the normalization in  <cit>  is included as part of the matching activity. in other cases, e.g., feature  <dig>  the normalization is obtained by filtering with a suitable sigmoid the difference between the value of charge averaged on the given window and the corresponding expected value.

in so doing, for each given position to be predicted, a vector m of values in the range  <cit>  is first evaluated, and then supplied as input to the guard of each expert in the population. each guard may accept or not the input depending on an embedded genetic pattern e, which is a string in { <dig>   <dig> #}. in particular, the i-th component of a genetic pattern being  <dig>  forces the corresponding feature to be close to  <dig> , whereas the dont-care symbol  allows the corresponding feature to be disregarded. in practice, the i-th component of e controls the evaluation of the corresponding input features, so that only non-"#" features are actually taken into account. hence, hg≠ φ being the set of all non-"#" indexes in e, g can be defined, according to the minkowski's l∞ distance metrics, as:



dealing with a population of experts
the overall population of experts is dealt with according to the typical guidelines that hold for evolutionary environments. as shown in figure  <dig>  experts interact with:  a selector,  a combination manager,  a rewarding manager, and  a creation manager. the selector is devoted to collect all experts whose guard covers the given input, thus forming the match set m. the combination manager is entrusted with combining the outputs of experts belonging to m, so that a suitable voting policy can be enforced on them. the main task of the rewarding manager is to force all experts in m to update their internal parameters, according to the reward obtained by the external environment. the creation manager is responsible for creating experts, when needed.

in the current implementation of the system, the training activity encompasses two steps:  discover a population of guards aimed at soft partitioning the input space, and  train the embedded predictors of the resulting population.

in the first step, experts are generated and selected concentrating only on their soft-partitioning capability, rather than on the precision of the system. in fact, in this phase, embedded predictors play a secondary role, their training being deferred to the second step. until then, embedded predictors output only the statistics  computed on the subset of inputs acknowledged by their guard .

triggered by an input x of class tx∊ {α,β,c}, the match set m is built, consisting of all experts whose guards cover x. when m is empty a new expert is created, whose genetic guard is able to cover the current input. in this case, an embedded pattern  is evaluated by averaging for each class, the output of all experts in m and then selecting the class with highest value. let us note that the contribution of each expert e is weighted according to we.

after that, the reward from the environment is calculated and the parameters that characterize each expert in m, including fitness, are updated according to the guidelines of wilson's xcs systems  <cit> . if the average time elapsed from the last mating operation  exceeds a predefined threshold, a pair of experts is selected in m according to a probability that increases with fitness. these experts are used to generate a new pair of experts using genetic operators , to be inserted in the population. single-point crossover and mutation operate on the embedded patterns of the parents. if the population limit has been reached, a pair of experts to be deleted is selected. the probability of deleting a given expert is proportional to the estimated average size of the match sets in which it occurred, and increases for experts with low fitness.

it is worth pointing out that, at the end of the first step, a globally-scoped expert  is inserted in the population, to guarantee that the input space is completely covered in any case. upon completion of the first step, no further creation of experts is performed.

in the second step, the focus moves to embedded predictors, which are actually multi-layer perceptrons  trained using the backpropagation algorithm on the subset of inputs acknowledged by their corresponding guard. in the current implementation of the system, all mlps are trained in parallel, until a convergence criterion is satisfied or the maximum number of epochs  has been reached. in its basic form, mlp training is performed by enforcing backpropagation on the set of inputs acknowledged by the corresponding guard. a further training technique has also been experimented, explicitly devised for this specific application: given an expert consisting of a guard g and its embedded predictor h, h is trained on the whole training set in the first five epochs. the visibility of the training set is restricted on the subsequent epochs according to the inputs filtered in by g. in this way, a mixed training technique is performed, whose rationale lies in the fact that experts must find a suitable trade-of between the need of enforcing diversity  and the need of preventing overfitting.

structure-to-structure prediction
technologies that adopt a simple residue-centric approach, in which secondary structures are predicted independently, often generate inconsistent and unrealistic secondary structure assignment -e.g., isolated alpha-helices. to deal with this problem, a suitable post-processing is usually performed, aimed at improving the prediction accuracy. the post-processing module can be either hand-coded or automatically generated. in the former case, it follows the guidelines of suitable empirical rules, whereas in the latter an architecture typically based on anns is devised and trained on the inputs generated by the subsystem responsible for the sequence-to-structure processing. in the implementation of massp <dig> we adhered to the latter approach. in particular, post-processing is performed by a single mlp fed with the sequence-to-structure prediction expressed in terms of α, β, and c propensions. to augment the autocorrelation of the input signal, a suitable "low-pass" filtering is also preliminarily performed by resorting to a suitable gaussian shape . in so doing, the mlp takes as input the resulting three-dimensional signal on a window of  <dig> residues and generates three outputs in  <cit>  -to be considered as pseudo-probabilities. each aminoacid of the given sequence is then labelled with α, β, or c according to a criterion of maximum-likelyhood.

RESULTS
to perform experiments, the same datasets adopted in  <cit>  have been adopted. the training set  contains  <dig> sequences, obtained from the pdb database  <cit>  by disregarding proteins with more than 25% of identity with the test set. the test set  contains  <dig> non-redundant proteins . let us point out in advance that mlp inputs are evaluated on a moving window of  <dig> aminoacids centered on the aminoacid to be predicted , each position in the window beng represented by a vector of  <dig> values in  <cit>  .

several experiments have been performed, aimed at assessing the impact of different aspects on the behavior of the system:  optimization of genetic experts,  input encoding,  experts' specialization technique, and  post-processing technique. let us stress that post-processing  has been deferred to the final experiment.

the first experiment has been performed using a population of  <dig> experts with randomly-generated guards. mlps were supplied with inputs obtained by enforcing a blast-based encoding of aminoacids, and were equipped with one hidden layer of 10– <dig> neurons, depending on the amount of inputs that can be selected by the corresponding guards. each mlp has been trained using backpropagation and considering only the inputs filtered in by the corresponding guard. since no selective pressure has been enforced, "good" experts have in fact been counterbalanced by "bad" ones, giving rise to a q3=  <dig> %. this result, considered as a reference, is comparable to the one obtained by a single mlp trained on the whole training set.

the second experiment was aimed at evaluating the impact of the genetic selection on the performance of the system. in this case, the above population has been evolved using covering, single point crossover and mutation operators. the underlying genetic algorithm performed  <dig> epochs and the final population contained  <dig> experts . mlps have been trained using backpropagation and considering only the inputs filtered in by the corresponding guard. results obtained in this case  show an improvement of about 2%, seemingly dependent on the domain knowledge embodied by the most "successful" experts.

in the third experiment, mlps were trained using backpropagation with a two-tiered policy: in the first  <dig> epochs each expert was trained on the overall dataset, whereas in the subsequent epochs only the subset of inputs selected by the corresponding guards was used for training. the underlying rationale is that "specialization" may not necessarily occur from scratch; rather, it can originate from a common starting point, obtained by allowing experts to have a global visibility over the input space.

results  show an improvement of about  <dig> %. in the fourth experiment, the position-specific scoring matrices that characterize psi-blast profiles  <cit>  have been adopted to encode mlp inputs. mlps have been trained using using backpropagation, together with the two-tiered policy described above. results  show an improvement of about  <dig> %.

final experiments have been performed by adopting a post-processor, consisting of a single mlp with a moving window  <dig> aminoacids. to better highlight the correlation among neighbor residues, a preliminary encoding through a lowpass gaussian filter  has been performed on the output supplied by the module based on multiple experts. these experiments allowed to reach a q3=  <dig> %. a 7-fold cross-validation performed on the selected training set essentially confirmed this result, with an average q3=  <dig> .

CONCLUSIONS
in this paper, an approach for predicting protein secondary structures has been presented, which relies on an architecture based on multiple experts. in particular, a population of hybrid experts – embodying a genetic and a neural part – has been devised to perform the given application task. experimental results, performed on sequences taken from well-known protein databases, point to the validity of the approach. as for the future work, we are investigating the applicability of more powerful features, to be embedded within genetic guards, able to improve their ability of performing context identification. furthermore, the task of implementing a community of heterogeneous experts is currently being investigated.

