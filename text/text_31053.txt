BACKGROUND
with the publication of the human hapmap, phase i  <cit> , and the completion of genotyping for phase ii, association mapping is entering a new era. whole genome scans using  <dig> k or  <dig> k snp chips, currently available  <cit> , and their higher density descendants, will be performed in large sets of cases and controls in a search for high-frequency, low penetrance variants that increases susceptibility to common, complex diseases  <cit> . recently published genome scans in very large cohorts have yielded such variants that show reproducible effects  <cit> . knowledge about these variants is expected to lead to a better understanding of disease initiation and progression, to identify pharmacological targets for prevention of the disease in high risk individuals as well as individually based treatment. case-control association mapping allows zooming in on relatively smaller regions than linkage mapping in pedigrees because recombination events in the  unknown genealogy of the sample have decoupled all but the closest markers from the  susceptibility variants. furthermore, it is usually easier to recruit large numbers of unrelated individuals for these studies.

with the immense data generation , there is an evident need for efficient methods for localisation of susceptibility variants which are also very fast and which can guide the subsequent selection of further markers in two- or multi-stage designs  <cit> . many studies resort to a marker by marker approach, i.e. single marker tests for independence between cases and controls, typically by a  <dig> ×  <dig>  or  <dig> ×  <dig>  fisher's exact test or χ2-test. the best set of markers  will then be selected for scrutiny in a larger set of individuals or a different population for replication of the association. however, unless the susceptibility variant is included among the markers typed, a marker by marker approach does not seem efficient  <cit>  since it disregards the dependency of close markers caused by their sharing of common genealogical history, a sharing that decreases with the level of recombination between the markers. for instance, if a strongly associated marker is flanked to the right by another associated marker , then one would expect a higher probability for the true location slightly to the right of the marker than slightly to the left.

a popular alternative to single marker association is to  model the whole data generating process. in this case this is termed the ancestral recombination graph   <cit> . this has most often been done by treating the disease locus as an unknown parameter, whose posterior distribution is estimated from genotype data and an underlying model of the process that generated the data. the posterior is usually numerically evaluated through monte carlo methods such as importance sampling  <cit>  and markov chain monte carlo  integration  <cit> . for the mathematical model of the underlying process, the complexity varies from assuming no relationship among diseased individuals except through the original mutation of the disease gene   <cit> , through coalescent theory based methods where either a tree  <cit>  or an ancestral recombinaion graph is modelled  <cit> . these models have shown to be accurate on small data sets  <cit> , but so far they are not able to treat much more than hundreds of cases and controls and hundreds of markers in a reasonable time.

furthermore, being based on very specific models, these methods are also sensitive to a bad fit to the model, which may well be the case for the human genome where recombination hotspots seem prevalent as do signs of recent population growth, and where the ascertainment biases introduced by the choice of markers are generally unknown and therefore difficult to control for  <cit> .

thus, there is a need for more sophisticated methods than single marker associations; methods that include further aspects of the data generating process or observable patterns in the haplotype structure of the data without being prohibitively slow. some data mining methods have been proposed, e.g. haplotype pattern mining  <cit>  and hapminer  <cit> , as well as cladistic methods that aim at associating the disease chromosomes to some subset of a reconstructed tree for either the whole or a part of the region surveyed  <cit> . these methods have been applied to small real data sets and have shown to perform well, especially when more than one mutation in the gene is responsible.

we propose to follow a similar approach by building perfect phylogenies around each marker. we accomplish this by using as many markers as possible  that fit on a single phylogenetic tree. we assume the infinite sites model  <cit> , i.e. each segregating site has a unique mutation. thus, the procedure is equivalent to defining a region around each focal marker, such that all markers in this region are compatible with the marker in focus as judged by the four gamete test  <cit> . following, we use different clustering measures of case chromosomes in the phylogeny thus defined. the idea behind the clustering measures is that we treat the perfect phylogeny as a decision tree and measure how well it explains the case/control classification: if the tree explains the classification well, there is an association between the tree topology and the disease, if the tree does not explain the classification, the tree and disease status are considered independent.

RESULTS
we have implemented our new method, named blossoc for block association, and evaluated it on a number of simulated datasets and two real datasets, and compared the accuracy with a number of existing methods. our implementation is efficient enough to handle very large datasets, analysing simulated sets of  <dig> mb regions of  <dig> case and  <dig> controls with  <dig> snps in ~ <dig> seconds on a 3ghz intel xeon. scaling this to  <dig> k snps over the entire genome, the running time will be less than two hours.

in evaluating our algorithm, we calculated scores for each marker locus in a dataset; an example is shown in fig.  <dig>  our method does not in any way restrict us to calculating scores for marker loci only, and perhaps the most appropriate approach would be to select the points uniformly placed on the region of interest. however, in our simulated data, the density of markers is high and the marker positions are uniformly randomly placed, so scoring only marker loci is not a major limitation, and has the benefit that we can immediately compare with single marker association .

comparing our scoring to sma, shows a high correlation between high clustering scores for our method and small p-values for sma. however, our scoring gives a smoother "curve" since neighbouring markers are included when scoring a locus and neighbouring scores are therefore more dependent .

to test significance of scores in our method we generally have to rely on time-consuming permutation tests. relatively time-consuming tests, that is. although doing thousands of permutation tests slows down our method significantly, it still completes in around  <dig> hours for our simulated data sets. compared to this, many model-based bayesian approaches requires days of cpu time. when using the bic score, however, theoretical results in  <cit> , described in the methods section, suggest to simply use the score to judge evidence of significance. the hqc score is very similar to the bic score, and hence we would expect to be able to use the hqc score to evaluate significance as well. we tested this suggestion by two different experiments. in the first experiment, we simulated  <dig> data sets under the null model, i.e. where cases and controls are simulated under the coalescent process, but where status is assigned randomly. this way we collected the null distribution of scores, for which the 95%, 99% and  <dig> % percentiles were  <dig> ,  <dig> , and  <dig> , respectively.

in the second experiment, we compared scores with p-values obtained by running permutation tests on a subset of our simulated data, selecting  <dig> random sets with one mutation and  <dig> random sets with two mutations, each with  <dig> individuals  and  <dig> permutations. using scores  <dig>   <dig>  and  <dig> to mean positive, strong, and very strong evidence for association, as suggested in  <cit>  – and consistent with our null-model simulations – we examined the p-values corresponding to loci with those scores higher, see table  <dig>  these data indicate that, at least as a good approximation, the scores alone can be used to indicate significant association.

statistics for p-values for different levels of significance as determined by the hqc score.

in the following, we measure the accuracy of our algorithm by taking simply the maximal scoring locus as a point-estimate of the disease locus, and measure the distance from this inferred locus to the true locus. this, of course, does not give as much information about the analysis as does knowledge of the scores over the entire region, but greatly simplifies summarising results over large number of data sets.

performance on simulated data sets
we initially investigated the different scoring criteria  on simulated data sets, either haploid or diploid  data, with a single or two disease mutations, see fig.  <dig> and table  <dig> 

summary of the accuracy of the four scoring functions  in units of ρ =  <dig>  i.e.  <dig>  corresponds to a recombination rate of ρ =  <dig>  for each simulation setup, the best performing score  is highlighted.

the best scoring function varies from data set to data set, and no scoring scheme is consistently superior to the others, either on individual data sets  or on average . it seems, however, the prob. scoring performs best on smaller datasets while the prob. score and the hannan and quinn criterion are about equally accurate on larger datasets. of the two, however, the hqc is much faster to compute, especially for large datasets, making it our preferred scoring function. as a result of this, we used the hqc score in the following experiments whenever analysing more than  <dig> individuals and the prob. score whenever analysing fewer. another experiment  showed that performance is slightly better when a minimum of  <dig> markers are forced to be included. it also showed that it did not affect the accuracy significantly if more than  <dig> markers where forced to be included, because the following pruning  removes the additional edges in the tree if they are insigninficant. in all reported experiments, we have therefore required at least  <dig> markers to be included.

the data sets were simulated assuming constant population size, but the human population is believed to have gone through a number of expansions. to test robustness of our method under population growth we also simulated data under exponential growth; growth parameter β =  <dig>  see . results are shown in fig.  <dig>  encouragingly, the results are not much affected by growth. compared to data sets without growth, the accuracy for blossoc drops – which is to be expected since the growth drives the genealogy towards more star-shaped topologies, in which our method will find no signal – but it still behaves similar to single marker association on the one-mutation data sets and it is more accurate on the two-mutations data sets.

to avoid systematic bias due to our simulation setup, we also generated data in a completely different manner, by boosting data from the hapmap project . results for this setup are shown in fig.  <dig> and resemble the results obtained from our simulated data.

comparison with other methods
the large number of association mapping methods developed makes it infeasible to compare all methods, and thus we are forced to make a choice as to which we should compare our new method to. our primary criterion in this choice was to compare with methods in the same niche as ours: methods aimed at fast exploration of very large data sets . under this criterion we found the following methods: hapminer  <cit> , haplotype pattern mining  <cit> , and hapcluster  <cit> . all three methods are aimed at finding areas in the data where cases appear more similar than controls, but define such areas, and scores loci accordingly, in different ways.

comparison with hapminer
an implementation of the hapminer method is available from the authors' homepage as a binary executable. we downloaded this implementation, and ran it on our simulated data sets with default parameters except for parameter number  <dig> which was set to  <dig> to prevent permutation testing. for data sets with a single mutation, whether large or small, the hapminer method and our method perform similarly , but for two mutations blossoc is slightly more accurate, see fig.  <dig> 

compared to our new method, hapminer is significantly slower, with an average running time of ~ <dig> minutes per data set when run on a 3ghz intel xeon, compared to a few seconds for our blossoc method. the long running times for hapminer made it impractical to compare the two methods on the larger hapmap based data sets.

comparison with haplotype pattern mining
the haplotype pattern mining  implementation is not freely available, but the simulated data sets used to evaluate the method in  <cit>  can be obtained from the authors' homepage. we therefore compare the two methods by running blossoc on these data sets and compare them with the results reported in  <cit> , see fig.  <dig>  these data sets are much smaller than our own simulated data sets – with  <dig> cases and  <dig> controls – so following the guidelines described earlier in this section, we use the prob. score for blossoc. the data sets come in four different classes, of increasing complexity, based on the number of cases carrying the disease mutation. as shown in fig.  <dig>  blossoc is more accurate on the two easiest classes  while the two methods are comparable on the two hardest . only on the last data set does single marker association compare to the other two; in the three easiest cases both blossoc and haplotype pattern mining outperforms single marker association.

comparison with hapcluster
though an implementation of the hapcluster method is available from the authors, this implementation is in the scripting language r and therefore much too slow to practically compare blossoc and hapcluster on our simulated data. instead, as for hpm, we rely on results reported by the authors, and compare blossoc with hapcluster on the simulated data described in  <cit> . the data sets in  <cit>  come in five different classes, based on the spacing of markers, the frequency of the causal allele, and the minor allele frequency  of the markers: all data sets represent regions of  <dig> kbp, but the number of snp markers come in three sizes:  <dig> ,  <dig>  and  <dig> ; the frequency of the causal allele comes in two ranges: common  and moderately rare ; and finally, the maf was either 5% or 10%. this was combined in five combinations: the  <dig> kbp spacing had maf 10% and common causal allele; the  <dig> kbp spacing had maf 10% and moderately rare causal allele; and the  <dig> kbp spacing had two moderately rare causal allele versions, with maf 5% and 10%, respectively, and one common causal allele version with maf 10%. all data sets consisted of  <dig> cases and  <dig> controls, selected with genotype relative risk in ratio 1:3: <dig> for the  <dig> kbp and  <dig> kbp spacing data sets and in the ratio 1:2: <dig> for the  <dig> kbp spacing data sets. each parameter class was used to simulate  <dig> data sets.

for the  <dig> kbp data set with common causal variant, we saw no difference between blossoc, hapcluster, and single marker association; a comparison of the three methods is shown for the four other parameter combinations in fig.  <dig>  for the  <dig> kbp spacing data sets, hapcluster seems slightly better than blossoc and single marker association, while in the  <dig> kbp spacing data sets, blossoc and hapcluster seem equally good and both better than single marker association. for the remaining two data sets, blossoc is slightly better than the other two.

since the blossoc method uses a very simple model of the data to achieve very fast running times – simple tree topologies based on segregating sites only – we don't expect it to be as accurate as more time consuming methods with more detailed data models. nevertheless, it is interesting to compare our method with such methods to get an impression of how great a loss in accuracy we accept by using the faster method.

since it is impractical to run cpu intensive methods on our simulated data sets, we choose again to run blossoc on previously published data sets and compare with only a single, but state of the art, representative of this class of methods, the latag method from  <cit> .

we used our blossoc method to analyse simulated data sets from and compared it with reported results from  <cit> . the data consist of  <dig> sets  <dig> cm regions with 45– <dig> markers for  <dig> diploid cases and  <dig> diploid controls, where the causal allele frequency is in the range  <dig> – <dig>  and the disease status is assigned with probabilities

p  =  <dig> 

p  =  <dig> 

p  =  <dig> 

compared to the other simulated data sets described previously, these data sets are very small. complex methods, such as latag, do not scale to large data sets – even these very small data sets require hours of cpu time to analyse – but are often capable of detecting signals in data sets where faster methods, such as ours, have insufficient power.

results are shown in fig.  <dig> and blossoc is again more accurate than sma. we were not able to get the exact results from figure  <dig> in  <cit> , but a visual comparison of fig.  <dig> with figure  <dig> in  <cit>  suggest that blossoc is slightly less accurate than latag, with an accuracy somewhere half-way between single marker association and the latag method. the improved accuracy of latag, however, is at the expense of running time where a reported ~ <dig> hours per data set for latag needs to be compared with a few seconds per data set for blossoc.

results on real data sets
cf data set
we ran our method on the Δf <dig> mutation for cystic fibrosis data from  <cit> . the data set consists of  <dig> cases and  <dig> controls, genotyped for  <dig> markers. because of the small size of the data set we used the prob. score. the results from this analysis are shown in figure  <dig>  the markers are very un-evenly spaced in the region, and this is a case where a uniform placement of scoring points would probably be preferable to scoring only the marker loci, but to be able to immediately compare it with single marker association  we still use the marker loci. the wide tail to the right of the disease locus, however, is probably an artifact of this choice.

blossoc takes its maximal score in the true locus, but this maximal score stretches over the range  <dig> – <dig>  with center at  <dig> . this is comparable to other fine-maping tools .

comparison of location estimates of the Δf <dig> mutation for cystic fibrosis data  <cit>  by blossoc and other coalescent-based fine mapping tools. the mutation is located at  <dig> . blossoc only gives a point estimate of the true locus and no credibility interval, but the maximal score stretches over the interval  <dig> – <dig> .

cyp2d <dig> data set
the cyp2d <dig> gene plays a role in drug metabolism and in  <cit>   <dig> snps in a  <dig> kb region spanning the cyp2d <dig> locus were genotyped in  <dig>  individuals, of which  <dig> were classified as having the "poor drug metaboliser"  phenotype. we ran blossoc on this dataset and tested for association with the pm phenotype; the results are shown in fig.  <dig>  the entire region around the cyp2d <dig> scores very strongly  with a hqc-score of  <dig> at the actual gene, but with a higher score,  <dig>  about  <dig> kb upstream of the gene .

in  <cit> , waldron et al. finds two modes in the cyp2d <dig> dataset when analysed with hapcluster. the smaller of the two corresponds to our maximal peak, while the highest mode of hapcluster is located at the cyp2d <dig> gene.

discussion
our blossoc method is based on the very simple idea of compatibility in a region around a marker combined with perfect phylogenies based on binary traits assuming the infinite sites model. this idea has not previously been explicitly exploited for association mapping. a single marker often belongs to different intervals of compatible sets of markers but here we choose the most symmetric interval around the marker in focus . an interval with only compatible markers may well have experienced recombination events in the history of a large sample, but these recombination events have left relatively little imprint on the data. thus, the method uses parts of the observable haplotype structure without making any reference to the underlying evolutionary process that created this haplotype structure – aside from the infinite sites model that is typically considered reasonable for snp data. apart from what compatibility tells us about recombination away from a focal marker, the method can be classified as a data mining approach. this can be considered a strength in cases where little is known about the process that generated the data, e.g. the demographic process, the recombination process and the ascertainment biases in choice of markers used.

the blossoc method is mainly designed to handle a very dense set of markers with high linkage disequilibrium since, in this case, blocks of compatibility are expected to include several markers. to extend the use of the method to cases of more distantly spaced markers we have investigated the effect of forcing a preset minimum number of markers to be included. while this has little effect on densely spaced markers where the number of compatible markers usually exceeds this minimum, it was found to increase the mapping accuracy for less densely spaced markers and to be relatively insensitive to minimum values, as long as they exceed  <dig>  due to the subsequent pruning of the tree.

the method is evidently heuristic and its efficiency can therefore only be evaluated through the application of the method to well defined data sets where the result is already known and comparing its accuracy to other methods. due to the almost absence of publicly available case-control data sets where susceptibility mutations are known, we have mainly evaluated the performance of the method on artificial data sets either simulated under simple demographic models or based on augmentation of data from the public hapmap project.

we initially used these test data sets to evaluate which scoring function to use for establishing significant clustering, and were able to conclude that the hqc scoring is generally superior except for small data sets  in which case the prob. score is superior. we thus recommend choice of hcq if more than  <dig> sequences are available and otherwise the prob. score. our experiments also prompt us to recommend that a minimum of  <dig> markers are always included.

using our recommended choice of scoring function, the results encouragingly show that the simple method generally outperforms marker by marker association as well as competing fast methods where we could get access to the software and/or previous analyses which we could directly compare our results against. if only a single mutation affects the disease status, then most of the competing methods are indistinguishable from just choosing the most strongly associated single marker. however, when disease heterogeneity or non-additive genotypic effects are modelled, single marker association is inferior to the other methods. among these, our blossoc method generally outperforms hapminer, hpm and hapcluster, even on the data sets used in the original publications of these alternative methods. blossoc is also very competitive in regards to running time. for our simulated data sets, blossoc completes the analysis within a few seconds on a 3ghz xeon processor, compared to less than a second for sma and ~ <dig> minutes for hapminer. we do not compare running time with hpm, since we do not have access to the tool, or to hapcluster, since this method has only been implemented as an r prototype, and a runtime comparison with this will not be a fair comparison. blossoc is thus only slightly slower than the simple single marker association and much faster than hapminer.

surprisingly, blossoc also performs reasonable compared with more involved methods such as the method by zöllner and pritchard. however, we expect that model based approaches always will be superior to our method as long as the model is a reasonable approximation to the data generating process, and our experiments also show that zöllner's and pritchard's latag outperforms blossoc. model based approaches, however, are many orders of magnitude slower than our new method, and more importantly will not scale to the size of our own simulated data sets. blossoc can analyse  <dig> million snps in  <dig> cases and  <dig> controls in a few days, while the method of zöllner and pritchard will not be able to handle such data sets. we expect that boosting the power in a study, by increasing the number of samples and being able to analyse the larger data sets, will more than compensate for the cruder model.

CONCLUSIONS
we have presented a fast method, blossoc, for accurate localisation of disease causing variants in high density case-control association mapping experiments. blossoc has the same accuracy as single marker association in the simplest case of a single disease causing mutation and a constant recombination rate. however, when it comes to more complex scenarios of mutation heterogeneity and more complex haplotype structure such as found in the hapmap data our method outperforms sma as well as other fast, data mining approaches such as hapminer and haplotype pattern mining  while being significantly faster.

several extensions to the method are currently under investigation, e.g. the ability to handle unphased genotype data directly without the pre-phasing step , an ability to alleviate gene conversion events by skipping single incompatible markers in the centre of a block of compatible markers, and the ability to handle quantitative traits. multi-allelic markers such as micro-satellites can already be handled if they are assumed to evolve under the infinite alleles model, but the efficiency of the algorithm has not been investigated in this case. furthermore, the method is sufficiently fast that investigation of interactions between regions should also be feasible to score by a similar principle  <cit> .

