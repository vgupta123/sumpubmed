BACKGROUND
one of the most important innovations in information retrieval over the past decade has been the development of algorithms that exploit inter-document relationships. in most cases, documents do not exist in isolation – their environments provide an important source of evidence for ranking results with respect to a user's query. this insight is captured in algorithms such as pagerank  <cit>  and hits  <cit>  . both have been successful in web environments, where hyperlinks provide the inter-document relationships. the two algorithms operationalize in different ways the basic idea that a hyperlink represents an endorsement of the target page by the source author.

this article considers the application of these algorithms to a different type of graph structure for text retrieval in the biomedical domain. in the absence of manually-created hyperlinks, we argue that related article networks, or networks defined by content similarity, can be treated in the same manner as hyperlink graphs. experiments show that incorporating evidence extracted from such networks yields statistically significant improvements in document retrieval effectiveness, as measured by standard ranked-retrieval metrics. these results are consistent with previous work and generalize the applicability of graph analysis algorithms to the biomedical domain .

the pubmed search engine provides the context for this work. whenever the user examines an abstract in pubmed, the right panel of the browser is automatically populated with titles of articles that may also be of interest, as determined by a probabilistic content-similarity algorithm  <cit> ; see figure  <dig> for an example. in other words, each abstract view automatically triggers a related article search: the top five results are integrated into a "related articles" panel in the display. note that although medline records contain only abstract text, it is not inaccurate to speak of searching for articles since pubmed provides access to the full text when available. we use "document" and "article" interchangeably in this article.

related article search provides an effective browsing tool for pubmed users, allowing them to navigate the document collection without explicitly issuing queries. any given medline citation is connected to a number of related articles, which are in turn connected to even more related articles, and so on. thus, any single citation represents a node in a vast related document network defined by content-similarity links. we explore the hypothesis that these networks can be exploited for document retrieval, in the same manner as hyperlink graphs in the web environment.

 <dig> 
RESULTS
 <dig>  experimental design
retrieval experiments were conducted using the test collection from the trec  <dig> genomics track  <cit> , which used a ten-year subset of medline. the test collection contains fifty information needs  and relevance judgments, which take the form of lists of pmids  that were previously determined to be relevant for a given topic by human assessors. see section  <dig>  for more details. this work examines two well-known algorithms that exploit link structure to score the importance of nodes in a hyperlink graph such as the web: pagerank  <cit>  and hits  <cit> . see section  <dig>  for a brief overview. adapting these techniques to medline, we evaluated the impact of features extracted from related document networks in a reranking experiment.

the starting point of our experiments was a ranked list containing  <dig> hits, retrieved with the open-source java-based terrier information retrieval platform using the in_expb <dig> retrieval model . terrier's retrieval algorithm is based on the divergence from randomness framework  <cit> . template instantiations from the genomics track topics were submitted as the queries, with no special processing.

from the ranked list for each query, we constructed several related document networks by varying the number of related document expansions for each hit in the terrier result set. that is, for each hit, we added links to its  <dig>   <dig>   <dig>  and  <dig> most similar "neighbors". naturally, adding more related documents resulted in greater network density, which as we show has a significant impact on results. pubmed's eutil api allowed us to programmatically retrieve the related documents, which we post-processed to eliminate those not in our collection. to avoid combinatorial explosion, we did not perform second order expansions , although that is a possibility. the pagerank and hits algorithms were then applied to these networks, using the implementation in jung , an extensible open-source toolkit for network analysis. for pagerank, we set the random jump factor to  <dig> , a value frequently suggested in the literature.

scores extracted from the networks were combined with terrier retrieval scores using weighted linear interpolation, controlled by the parameter λ, i.e., weight of λ to terrier scores, weight of  to network scores. we ran three separate sets of experiments, using pagerank scores, hits authority scores, and hits hub scores. the output of this scoring process was a new ranking of the documents in the original terrier ranked list. note that related document expansions were only used to define the network over which our graph analysis algorithms  operated – we focused only on reranking hits retrieved by terrier.

we evaluated reranked output in terms of three standard ranked-retrieval metrics: precision at  <dig> documents , relative mean average precision at  <dig> documents , and also at  <dig> documents . the cutoffs were selected to match the current pubmed interface, which displays  <dig> hits per page. early precision is important in a web search context, since users in general examine relatively few results. these metrics capture the quality of the first two result pages . finally, note that we measure relative map – that is, with respect only to relevant documents contained in the original terrier ranked list. this modification was made since we were only working with the top  <dig> retrieved documents; for topics with more than  <dig> known relevant documents, a perfect score was impossible. different ranges on possible map values make meaningful cross-topic comparison and aggregation difficult. since the test collection we used contained a mean of  <dig> relevant documents per topic, this was a real concern – computing relative map addresses these issues.

 <dig>  retrieval effectiveness
results of our reranking experiments combining terrier and pagerank scores are shown in figures  <dig>   <dig>  and  <dig> . expansions with different numbers of related documents are shown as separate lines. the x-axis represents a sweep across the λ parameter space in tenth increments. thus, the right edge of each line  represents baseline terrier results . for all experiments in this paper, we applied the wilcoxon signed-rank test to assess the statistical significance of performance differences. points that represent significant improvements over the terrier baseline  are denoted by solid markers. see section  <dig>  for a discussion of statistical testing and caveats on interpreting these results.

the graphs confirm that incorporating pagerank scores using our simple combination approach improves ranked retrieval effectiveness, reaching optimal scores between  <dig> – <dig>  in terms of λ values. lower values of λ, representing heavier emphasis on pagerank scores, consistently results in below-baseline effectiveness. in general, we note that more related article expansions improve retrieval effectiveness. it appears that denser networks yield a better estimation of a document's importance.

corresponding graphs for interpolating terrier scores with hits authority scores are shown in figures  <dig>   <dig>  and  <dig> ; and for hits hub scores, in figures  <dig>   <dig>   <dig> . to facilitate comparison between the different methods, we use the same vertical scale for each metric. as with previous figures, we denote statistically significant improvements over the baseline  with solid markers. the only cases observed were with hits authority scores in p20; in all other cases, gains were not statistically significant. we note the same general trends with both sets of hits scores, although they appear to be less valuable than pagerank for document ranking. once again, see section  <dig>  for a discussion of these results.

to provide context, it is worthwhile to compare our results to previous runs submitted to the trec  <dig> genomics track. as hersh et al.  <cit>  report, the best mean average precision for an automatic run  was  <dig> ; the best precision at  <dig> was  <dig>  . the mean for all  <dig> submitted runs was  <dig>  in terms of map and  <dig>  in terms of p <dig>  as a separate experiment, we generated a comparable baseline run : it achieved  <dig>  map and  <dig>  p <dig>  since we are using terrier "out of the box" with minimal modification, we naturally did not expect superlative performance – the best-performing runs all involved techniques to address domain-specific terminology, e.g., through query expansion  <cit> . nevertheless, these results confirm that we are starting with a competitive baseline, suggesting that improvements contributed by link analysis are meaningful.

although a sweep across the λ parameter space allows us to understand the relative importance of graph analysis and terrier retrieval scores, it doesn't tell us if optimal values are realistically achievable. focusing specifically on pagerank scores , we conducted a series of five-fold cross-validation experiments to try and automatically learn λ settings . results are shown in table  <dig>  along with relative improvements over baseline ; all improvements are statistically significant  based on the wilcoxon signed-rank test. we confirmed that it is possible to obtain optimal effectiveness for a particular metric given appropriate training data.

improvements are statistically significant based on the wilcoxon signed-rank test .

our experiments suggest that pagerank is more effective than hits for analyzing the link structure of related document networks. this makes sense, as the notion of hubs and authorities does not find a natural analog in our application . whereas hits assumes a particular linking behavior , pagerank models a random walk over an arbitrary graph – and appears applicable to both explicit web hyperlinks and automatically-computed content-similarity links.

 <dig> discussion
 <dig>  hyperlink graphs and related document networks
despite superficial similarities, the analogy between hyperlink graphs of the web and related document networks in medline is far from perfect. hyperlinks are created by humans and represent intentionality. that is, an author links to another web page because he or she "likes it". thus, inbound links can be interpreted as votes of confidence with respect to the quality, authority, etc. of the web page. algorithms such as pagerank and hits take advantage of this idea. related document networks, on the other hand, are artificial. since they are automatically computed by a content-similarity algorithm, the networks reflect inherent characteristics of the document collection, i.e., term distributions. furthermore, the nature of content-similarity algorithms means that every document is related to every other one to some degree; thus, we face a thresholding problem when deciding how expansive a related document network might be. given these important differences, why does reranking with pagerank scores improve retrieval effectiveness? here, we provide independent explanations for our experimental results.

one source of support comes from the cluster hypothesis in information retrieval  <cit> , which is the simple observation dating back several decades that closely associated documents tend to be relevant to the same requests. another interpretation is that relevant documents tend to occur in clusters. many researchers have explored and confirmed this hypothesis as a basic property of document collections, albeit to varying degrees  <cit> . therefore, the underlying topology of related document networks might provide clues as to where relevant documents might lie in the collection space.

similar support comes from cognitive psychology. the theory of information foraging  <cit>  hypothesizes that, when feasible, natural information systems evolve toward states that maximize gains of valuable information per unit cost. furthermore, the theory claims that information seekers behave in a manner that is not unlike our hunter-gatherer ancestors foraging in physical space. one basic assumption in information foraging theory is the notion of information patches – the tendency for relevant information to cluster together. an information seeker's activities are divided between those that involve exploiting the current patch and those that involve searching for the next patch – the user is constantly faced with the decision to pursue one or the other activity. these claims can be understood as a different formulation of the cluster hypothesis: relevant documents co-occur in similarity space, and thus the structure of this space is an important consideration in retrieval. whereas the cluster hypothesis adopts a system-centered view, information foraging theory focuses on search behavior. nevertheless, both converge on the same idea.

additionally, empirical support comes from usage patterns of related article search. a recent analysis of pubmed query logs indicates that searchers click on suggested article titles with significant frequency  <cit> . data gathered during a one week period in june  <dig> indicate that approximately 5% of page views in non-trivial user sessions  are generated from users clicking on related article links. approximately one fifth of all non-trivial user sessions involve at least one click on a related article link. furthermore, there is evidence of sustained browsing using this feature: the most frequent action following a click on a related article is a click on another related article . thus, related document networks appear to be an integral part of pubmed searchers' activities – suggesting that characteristics of these networks might provide an important source of evidence for document ranking.

 <dig>  related work
the related article search feature in pubmed is an instance of "query by example" and can also be understood as a form of single-point relevance feedback. many commercial search engines provide similar capabilities, through links labeled "similar pages" or "more like this". a number of studies have demonstrated the effectiveness of this feature as a browsing tool  <cit>  using simulations of searcher behavior. however, the focus has been on interactive tools for navigating text collections, and not on result ranking.

cluster-based retrieval has historically received much attention in the information retrieval literature, most recently in the language modeling framework  <cit> . clustering can also be used as an interactive search tool, as in scatter/gather  <cit> ; cf.  <cit> . despite similar goals , clustering represents a different approach from this work. clustering algorithms typically group together similar documents based on a high-dimensional vector representation. thus, the relationship of interest is group membership . in contrast, related document networks focus on pairwise content similarity, and require different algorithms for exploiting structure. diaz's framework of "score regularization"  <cit> , which formalizes the idea that similar documents should have similar retrieval scores, provides an interesting theoretical basis for understanding the relationship between these different classes of techniques.

this work can be viewed as an extension of kurland and lee  <cit> , who rerank documents based on generation links induced from language models. they examine application of pagerank and hits to such graphs, concluding that pagerank outperforms hits in retrieval experiments. to our knowledge, this article describes the first application of similar techniques to text retrieval in the biomedical domain. thus, our work generalize the applicability of graph analysis algorithms to a different application area. furthermore, we relate the effectiveness of this class of techniques to existing browsing features in the pubmed interface and theories of information-seeking behavior, thus establishing a link between interactive retrieval and backend algorithms.

another interesting use of graph algorithms is seen in the recent work of lin et al.  <cit>  in the biomedical domain, who apply hits to a bipartite graph consisting of keywords and documents from medline. their goal was to identify important keywords to describe clusters of documents. although that work is fundamentally different from ours in both aims and methods, it highlights the promise of applying graph algorithms to problems in the biomedical domain – an underexplored approach to an important area.

in addition to information retrieval applications, link analysis has also been adapted for natural language processing tasks. for example, lexpagerank  <cit>  computes pagerank scores over a network defined by sentence cosine similarity, and has been shown to outperform centroid-based techniques for extractive summarization. other applications of graph-based algorithms in summarization include the work of mihalcea  <cit> .

 <dig> 
CONCLUSIONS
based on this work, we see a number of future directions worth exploring. our current approach builds related document networks directly from an initial ranked list – the result is a link graph that is query-biased, i.e., it represents the local neighborhood around a particular region of the document space. we do not know if this is an essential component of our scoring model, or if alternative formulations are equally effective. one might, for example, perform link analysis over the entire document collection . this is likely the preferred approach for operational environments, as it avoids link analysis on the fly . although medline  is relatively small by web standards, we lack the computational resources and appropriate implementations to perform either pagerank or hits on the entire document collection. another interesting possibility is to use related document networks not only for rescoring, but also for expanding the result set. in our reranking experiments, nodes in the related document networks that were not part of the initial result set were used only to define the "local neighborhood" for the pagerank or hits computation. in principle, however, these nodes might be integrated into results returned to the user.

in summary, we demonstrate that in the absence of explicit hyperlinks, it is possible to exploit networks defined by automatically-generated content-similarity links for text retrieval. evidence from link structure derived from pagerank scores can be combined with retrieval scores from an off-the-shelf retrieval engine in a straightforward manner. together, the combination yields significant improvements in ranked-retrieval effectiveness.

 <dig> methods
 <dig>  pagerank and hits
this work examines two well-known algorithms that exploit link structure to score the importance of nodes in a hyperlink graph such as the web: pagerank  <cit>  and hits  <cit> . we overview both algorithms, but refer the reader to the original articles for details.

pagerank conceptually models a random web surfer. given a tireless, idealized user who randomly clicks on hyperlinks , the measure quantifies the fraction of time that the user is expected to spend on any given page. thus, pages with many in-links or highly-ranked in-links will have high pagerank scores – this is consistent with our intuition of an "important" web page. the distribution of pagerank scores can be interpreted as the principal eigenvector of the normalized link matrix. as an additional refinement, pagerank incorporates a jump factor, which models the probability that the surfer will randomly jump to another page . typically, pagerank is computed iteratively, and has been empirically shown to converge in surprisingly few iterations, even for extremely large networks.

the hits algorithm views the hyperlink graph of the web as containing a set of "authoritative pages" joined together by a set of "hub pages". the task, therefore, is to discover which nodes are hubs and which are authorities from the link structure . operationally, hubs and authorities are recursively defined in terms of each other: a good hub is a page that points to many good authorities, and a good authority is a page that contains links from many good hubs. this gives rise to an iterative technique for computing hub and authority scores, although kleinberg provides a theoretical foundation for his formulation in terms of eigenvectors of matrices associated with the hyperlink graph.

 <dig>  test collection
we evaluated the impact of graph analysis algorithms on retrieval effectiveness with the test collection from the trec  <dig> genomics track. a test collection is a standard laboratory tool for evaluating retrieval systems, consisting of three major components:

• a collection – documents on which retrieval is performed,

• a set of information needs – written statements describing the desired information , which translate into queries to the system, and

• relevance judgments – records specifying the documents that should be retrieved in response to each information need .

the use of test collections to assess the performance of retrieval algorithms is a well-established methodology in the information retrieval literature, dating back to the cranfield experiments in the 60's  <cit> . these tools enable rapid, reproducible experiments in a controlled setting without requiring users. in modern information retrieval research, test collections are created through large-scale evaluations, such as the text retrieval conferences  sponsored by the u.s. national institute of standards and technology   <cit> .

experiments in this article were conducted with data from the trec  <dig> genomics track  <cit> . one salient feature of this trec evaluation was its use of generic topic templates , which consist of semantic types, such as genes and diseases, embedded in prototypical information needs, as determined from interviews with biologists and other researchers. in total, five templates were developed, each with ten fully-instantiated topics; examples are shown in table  <dig>  in some cases, the actual topics deviate slightly from the template structure .

the trec  <dig> genomics track employed a ten-year subset of medline  containing  <dig>  million citations, or approximately a third of the entire database at the time it was collected in  <dig> . a total of  <dig> groups submitted  <dig> runs to the evaluation. system output was evaluated using the standard pooling methodology for ad hoc retrieval, with relevance judgments supplied by an undergraduate student and a ph.d. researcher in biology. no relevant documents were found for one topic, which was dropped in our experiments.

 <dig>  statistical testing and cross-validation
when comparing the effectiveness of different retrieval runs, it is of course important to assess the statistical significance of the results. following established conventions in the information retrieval community, the non-parametric wilcoxon signed-rank test was used because it makes minimal assumptions about the underlying distribution of differences. effectiveness metrics  on a per topic basis formed the paired observations.

for the reranking experiments which involved interpolating scores from terrier and either pagerank or hits, we compared the effectiveness at each λ setting with the baseline terrier-only run. for each, statistical significance was assessed with a wilcoxon signed-rank test. however, since each family of tests  involved multiple comparisons, there is a danger of making type i errors when considering the entire family of tests. to account for this possibility, we applied the Šidák correction for multiple hypothesis testing  <cit>  – under which we would consider a test significant if its associated probability was smaller than  <dig>  . unfortunately, none of the p values associated with the individual wilcoxon signed-rank tests passed this stringent check . therefore, we cannot claim statistical significance for the entire series of reranking experiments that involved testing multiple λ settings. however, it is noted that the Šidák correction is known to be rather conservative; see  <cit>  for discussion.

the complexities associated with multiple hypothesis testing partially motivated our cross-validation runs. here we provide more details about the experimental procedure for results presented in section  <dig> . we conducted three separate five-fold cross-validation runs, tuning on each metric in turn. topics were stratified in such a way that each fold contained a proportional representation of each template. we trained on four folds and tested on the fifth . that is, for each fold we trained on eight topics from each template and tested on the remaining two topics for that template. this process was repeated five times, with a different set of training and held-out test topics each time. although topics varied in terms of difficulty and in terms of the number of relevant documents, they represented the most natural unit for sampling since each topic corresponds to an information need. the stratification procedure across templates ensured that each fold contained a balanced representation of each class of information need.

elaborating on the results in section  <dig> , details of our cross-validation experiments with pagerank are presented in table  <dig>  we show three separate runs, tuning on map <dig>  map <dig>  and p <dig>  for each metric, the effectiveness on the training set and on the held-out test set for each fold is presented. each cell in the table contains the mean followed by the standard deviation. for reference, in the final column we show the baseline effectiveness  on the same held-out test topics. in terms of all three metrics, we see consistent improvement over the baseline for all folds. these results suggest that the improvements achieved by combining pagerank and terrier scores are indeed statistically significant.

mean and standard deviation for each fold are shown; for reference, baseline results on the test topics are provided. note that optimized effectiveness metrics show consistent improvements over baseline.

