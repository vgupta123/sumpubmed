BACKGROUND
joint analysis of dna orthologues from multiple species conveys important information about sequence properties. this comparative approach is a powerful concept in genome analysis today. dna sequences with unexpected conservation across species have gained particular interest  <cit>  as they are likely to encode important and constrained functionality across species. throughout the paper the term conserved will refer to primary sequence conservation among multiple species. there are many types of conservation acting at different constraint levels upon the genome. secondary and tertiary structures as well as interactions of non-coding rna may be preserved with little primary sequence information remaining conserved  <cit> .

the problem of measuring the conservation of sequences across multiple species has been addressed in a number of publications,  <cit> . stojanovic et. al. compared  <dig> different methods for scoring the conservation of a multiple sequence alignment in gene regulatory regions  <cit> . blanchette et. al. developed an exact algorithm, limited to short multiple sequences, for the detection of conserved motifs based on a parsimony approach  <cit> . margulies et al. presented two alignment based methods that incorporate phylogenetic information and are suitable for whole genome analysis  <cit> . siepel and haussler presented an approach  using a phylogenetic hidden markov model  allowing for high throughput measurement of evolutionary constraint  <cit> . cooper et al. introduced gerp and more recently asthana et al. presented scone which produce per-base scores of conservation and constraint.

phastcons, gerp and scone scores have been used as comparisons in this paper and are briefly reviewed in the discussion. these methods require the a priori estimation of a neutral evolutionary rate and measure conservation as the "surprise" of observing the analyzed data assuming the neutral model. neutral substitution rates are usually estimated from fourfold degenerated sites or ancestral repeats  <cit> .

the encode project revealed that about half of the analyzed functional elements found in non-coding dna had been classified as unconstrained  <cit> . pheasant and mattick  <cit> , among others, have argued that this could partly be explained by questioning the neutral rate of evolution used by existing sequence conservation studies. wrong assumptions about the neutral rate would lead to biased conservation measures and eventually to an over- or underestimate of the fraction of the genome under evolutionary constraint. for example, ancestral repeats are often assumed to evolve neutrally, but have been previously shown to include a nontrivial amount of constrained dna  <cit> . here, we propose a method that tries to avoid such a priori assumptions. we suggest that the maximum likelihood  estimate of rate heterogeneity is a more direct measure for sequence conservation. different estimators for these rates have been presented and reviewed in the literature  <cit> . here, we obtain the ml estimate of the rate process using an optimized window function. while this approach does not require assumptions about neutral rates, prior distribution of rates or transition probabilities between rate categories, we show in silico that reliable estimation in the mean squared error  sense is achieved in regions of conserved sequence. we present a qualitative comparison of the scores calculated by kulcons and the established methods phastcons, gerp and scone that assume a neutral model. encode regions were used for comparison.

furthermore, we present an information theoretic projection of local multiple parameter estimates to a score which allows for richer or more complex parameter models like the consideration of insertion and deletion  rates. results taking gaps in the alignment as indels into account are presented.

probabilistic modeling in phylogenetics
we will summarize the basic concepts of mathematical phylogenetic modeling in order to introduce the notation. a more thorough introduction can be found for example in  <cit> . throughout, we assume a given multiple sequence alignment a ∈ {a, c, g, t, -}n × l of length l comprising the orthologous sequences of n species. we denote ai as the ith column of a. an evolutionary model is commonly described by a set of parameters ψ that imposes a probabilistic model on how a base of a common ancestor evolves along a phylogenetic tree. the realizations of this process are the columns of the multiple sequence alignments. a single column a of such an alignment follows the distribution p. different sites evolve differently and, hence, each column ai could be associated with a different model ψi. most often, ψ = {t, λ, r, π, θ} comprises at least the following parameters: t = {v, e} denotes the topology of the binary phylogenetic tree relating the n species with nodes v and branches e ⊂ { : u, v ∈ v, u ≠ v}. it is often useful to distinguish between the set of inner nodes i ⊂ v and the set of leaves q = {q <dig>  ..., qn} = v\i.

furthermore, a map λ:e→ℝ+, e ↦ λ assigns positive branchlengths to e. the time continuous substitution process between two nodes is assumed to satisfy the markov property and to be identical for all branches with discrete state space a = {a, c, g, t}. such a process is specified by a rate matrix r and a stationary distribution π = . the transition probability matrix between two nodes connected by branch e is then given by pe = eλr  <cit> . reversibility is an additional constraint, often assumed when modeling dna sequences. in a time reversible process, the amount of substitutions from μ ∈ a to ν ∈ a is equal to the amount of substitutions from ν to μ, i.e. πμ rμν = πν rνμ. the parameters presented so far model the evolution of sequences along a phylogenetic tree . however, different sites in the genome are subject to different evolutionary processes, e.g. due to selection pressures resulting in varying substitution rates . this characteristic of evolution over sites, often called rate heterogeneity, is commonly modeled by introducing a stochastic process Θ = {Θi : i =  <dig> ... l}, where the realizations θi of the random variables Θi are scalars from ℝ+ that can be thought of as "scaling the tree" t leading to different substitution rates between two nodes at different sites i:

  pe=eθiλr. 

different models for the space process have been introduced: yang modeled Θ by an independently and identically distributed  process with the random variables Θi following a gamma distribution  <cit>  and later proposed process models with memory  <cit> . felsenstein used hidden markov models and showed how to calculate the likelihood and estimate rates using the viterbi algorithm  <cit> . in our work however, we assume the θi to be deterministic parameters, assigned to every column in a, without prior distribution. more complex models of evolution ψ are possible, e.g. including rates of insertions and deletions  <cit> .

likelihood in phylogenetics
efficient calculation of the likelihood function p has been introduced by felsenstein over  <dig> years ago  <cit> . the felsenstein algorithm  reduces the global likelihood problem to message passing along the branches of the tree from the leaves up to the root with local message calculation at the nodes. consider an alignment column ai, i.e. an observation at the leaves of the phylogenetic tree t resulting from the evolution of the unknown ith base in the sequence of the common ancestor. let u, v, w ∈ v be three nodes in t, u being the parent of v and w. denote bu, bv, bw the bases at the respective node. the essential observation of the fa is that, given the base bu, the observations at the leaves of the subtree rooted on v, ai, are independent of those of the subtree rooted on w, ai. the conditional likelihood of the observation ai= is then given by  <cit> 

  p|bu)=|bv)p)×|bw)p), 

with the transition probabilities p obtained from . clearly, eq.  depends on ψi which we omitted for the simplicity of notation. the initial message at leaf qj ∈ q is

 p|bqj)={1if ai=bqj0else,j= <dig> ..n. 

at the root node r we finally obtain the likelihood for the ith column as p=∑brπbrp|br) and using the i.i.d. assumption

  p=∏i=1lp. 

RESULTS
application to encode data
  1−σimax⁡i{σi}, 

where σi denotes the conservation score as derived in the section methods. a similar transformation was applied to the gerp scores. this has the effect that  <dig> represents the highest possible conservation and zero the lowest, which is already the case in phastcons and scone scores. the transformation serves solely visualization purposes. here, we would like to note that while normalized to be in the interval  <cit>  the scores can only be compared qualitatively as different scores are based on different models . for the calculation of our score, all parameters in ψ have been replaced by estimates except the rate heterogeneity parameter θ. we used the global average rate matrix r  published by siepel et. al.  <cit> . however, using different realistic matrices had minor impact on the scores which is in accordance with previously published observations  <cit> .

single base resolution results in highly varying scores among columns. one can suggest that functional units, such as binding sites, are constraint at least over several neighboring base pairs. assigning conservation to short regions and smoothing scores might thus be desirable. furthermore, more reliable estimates on rates may be achieved using a sliding window when rates are correlated among adjacent sites. therefore, kulcons uses a window function which results in smoother scores . the result in changing the size of the sliding window has a similar effect to the phastcons smoothness parameter.

phastcons achieves smoothing by tuning the transition probabilities between the conserved/non-conserved states of its model and this smoothness parameter is chosen such that a predetermined coverage of conserved regions is achieved. our method estimates the substitution rate incorporating neighboring columns in the maximum likelihood estimate and the specific smoothing effect of changing the window size will also depend on the window type used. choosing a window size of one will result in single base resolution but the scores will be highly variable among neighboring columns . here, we applied the same window to smooth scone and gerp scores for comparison. it can be observed in figure  <dig> that our score signal is in good agreement with the conservation estimate obtained by visual inspection of the multiple sequence alignment. the phastcons signal shows a binary characteristic and does not allow for discrimination among different conservation degrees. consequently, phastcons shows a relatively rough-scale pattern of conservation which is different from the pattern by kulcons, gerp and scone. this is explained by its underlying two-state phylo-hmm model .

interestingly, the smoothed gerp and scone scores show a very similar characteristic to kulcons with still some notable exceptions: in the region around  <dig> –  <dig> kulcons and gerp indicate a relatively weak conservation while scone indicates higher conservation. on the other hand, kulcons and scone both indicate higher conservation around  <dig> –  <dig> while gerp deviates significantly indicating weaker constraint. a different pattern can be observed in region  <dig> –  <dig> with kulcons being intermediate. a plot over a  <dig>   <dig> basepair subregion of enm <dig> is provided in additional file  <dig>  in order to evaluate our method more thoroughly, we present simulation results in the next sections .

sliding window ml estimation of a markov gamma process
in this section, we show via simulations of synthetic data generated by a markov gamma process that our approach described in methods is well suited for the estimation of conservation. i.i.d and markov, continuous and discrete space models have been proposed for the rate process {Θi : i =  <dig> ..l} along sites  <cit> . in the continuous case, the stationary distribution of {Θi} is commonly assumed as a gamma distribution pΘ=xα−1e−θ/ββαΓ=gΘ <cit> . correlation among sites is introduced to account for the fact that neighboring sites are likely to experience similar substitution rates  <cit> . discrete markov models can be obtained by quantizing the range of θ in rate categories and calculating transition probabilities from the bivariate distribution of   <cit>  or using a hidden markov model and estimating rate categories and transition probabilities from data  <cit> .

rate estimation has a long history in studies of molecular evolution. yang derived the conditional mean estimator  for θi under a continuous i.i.d. gamma model which is known to minimize the mean squared error  and having the highest correlation ) between true θ and estimated θ^ value. however, the method requires knowledge about the prior distribution of Θ and it was shown in  <cit>  that rate estimates are sensitive to the choice of the parameters of the distribution. in addition, in the context of application to whole genome alignments the method is computationally too time consuming. a low complexity version of the cme approximates the rates via discrete rate categories  <cit> . the discrete cme has also been derived in a markov chain framework with rate categories derived from an underlying bivariate gamma distribution of adjacent sites. it was shown that the discrete approximation achieves almost the same accuracy as the continuous version when using a sufficient number of categories  <cit> . however, in order to find a good partitioning of the categories, a prior distribution on t has to be assumed. models of among-site rate variation were reviewed in  <cit> .

simulation model
in the context of conservation measurement, the estimator is not required to give reliable results on the whole spectrum of possible rates, but to provide a good estimate for the degree of conservation of a region. the situation that we simulate mimics a moderately conserved region with "islands" of more or less conservation due to variance and autocorrelation of the rate. a good conservation estimator will take into account autocorrelation among sites while retaining the sensitivity of reporting variability within regions. using a markov gamma rate model, we generated alignment columns and estimated the rates using site-by-site ml estimation and the sliding window maximum likelihood procedure described in methods. simulation of markov gamma processes was performed as described by moran  <cit>  and phatarfod  <cit> . the rates θi follow a process with a stationary distribution g, i.e. e{Θ} =  <dig>  and var =  <dig> , and correlation corr = ρθj among sites. analysis of substitution rates has shown that θ is mostly in the range  <cit>   and we simulate an overall moderately conserved region  with varying conservation inside, which is modeled by the rate variance  and autocorrelation. in figure  <dig> a sample realization of the rate process {Θi,  <dig> ..l} is shown for l =  <dig> with the parameters described above and ρθ =  <dig>  revealing several regions with different degrees of substitution rates. alignment columns were simulated under the described model on a subtree of the  <dig> species encode tree comprising  <dig> species.

simulation results of rate process estimation using sliding window maximum likelihood
the true simulated θ is compared to its estimate θ^ obtained by the different methods. in figure  <dig> two performance measures are shown, the mse and corr, for different window types over the range of among site rate autocorrelation ρθ. for site-by-site ml estimates we restricted the maximum value of θ^ to  <dig> because it was reported by nielsen that estimates of highly variable columns tend to go to infinity  <cit> . around 99% of θ will have values lower  <dig> under the assumed gamma distribution. choosing different maximum values had minor effects on the results.

the best mse is achieved with the gauss window of variance  <dig>   with σw =  <dig> ) in the complete range of ρθ. for very slowly changing rates  the performance coincides with the large rectangular window. interestingly, for uncorrelated sites, the large gauss window clearly gives the best results, outperforming the small rectangular window and site-by-site estimation. apparently, even though the window introduces a bias, the error variance is reduced, obviously leading to an overall performance improvement. the maximum correlation corr and the minimum mse are achieved. this suggests that the method is very well suited for estimating θ with unknown prior distribution and with arbitrary autocorrelation among adjacent sites. a similar processing could be based on a window version of the bayesian approach with rate categories  <cit> .

statistical analysis of the proposed ml based estimate
as the proposed ml estimate is based on a relatively small sample size, we study the density of the estimated rate variation θ^ and compare it to the theoretically achievable pdf. we assumed all parameters in ψi to be fixed except for θi, reducing the problem to scalar parameter estimation. we check whether the ml estimator  attains the cramér-rao lower bound for the small sample size

 e{2}≥−e{∂2log⁡p∂θ2}−1=1i. 

it is well known that the mle asymptotically achieves this bound for large sample sizes, i.e.θ^~an−1), where n denotes the normal distribution with mean μ and variance σ <dig>  we performed a computer simulation using  <dig> realizations of alignments of length , generated according to a fixed evolutionary model ψ. we estimated θ^ and computed i for each sample. figure  <dig> shows the theoretical achievable pdfs n-1) versus the observed pdfs of θ^ for different simulated θ. even for small window sizes, e.g. δ =  <dig>  the mle closely approaches its asymptotic distribution. at low values of θ, the variances are relatively small, i.e. different values of θ can be distinguished with high probability. it can also be observed that the variance of the estimation increases with increasing θ. hence, our estimator is best discriminating between different degrees of conservation in relatively conserved regions even at small window sizes whereas in non-conserved regions, the information revealed by the window is not enough to allow for precise differentiation. the accuracy increases with the number of species in the alignment. these results can be used to identify whether a region is more conserved than another: we propose an estimation model for θ with a multiplicative error

 θ^=θ, 

where η~n is a normally distributed random variable. this has the effect that the variance of the estimation will depend on its mean and higher values will have a higher variance such as observed in figure  <dig>  the best fitting variance ση <dig> can be determined via simulations on synthetic data and a log likelihood ratio test can subsequently be performed to detect differentially evolving regions with statistical significance. the multiplicative variance will depend on the tree and other parameters used. a simulation of the multiplicative model is also shown in figure  <dig>  demonstrating that it fits very well the distribution of estimates obtained from the simulated genomic data.

conservation score respecting indel history
the kl projection allows a whole set of parameters to contribute to the conservation score in a probabilistic framework. as a possible application, we considered an extended evolutionary model to obtain a score that probabilistically incorporates insertion and deletion events. these indels give rise to gaps in the alignment which are usually neglected when measuring the conservation. figure  <dig> shows two different scores for a  <dig> bp fragment of an encode region. one score represents conservation estimation based only on local substitution rate estimates, neglecting gaps. for the other score,  <dig> parameters have been estimated: the substitution rate θ, and indel parameters c^i and c^d. the program indelign  <cit>  was used to estimate c^i and c^d. all parameters were estimated in a rectangular sliding window of length  <dig> over the alignment. note that in this case ψ comprises  <dig> additional parameters ci and cd. probabilities p^i and p^d of an insertion or deletion of length k =  <dig>   <dig>  .., 2δ +  <dig> on branch e were derived from c^i and c^d as described in  <cit> . the probability of a fully conserved column is then given as the probability of absence of mutations  in each branch and the score is the kl divergence between the probabilities of a fully conserved column under the estimated model and under the maximum conserving process . obtained kulcons scores are further compared to phastcons, gerp and scone. the latter method is also accounting for indel events. in  <cit> , siepel et al. present an extension of phastcons accounting for lineage-specific "gained" or "lost" elements. similar to our approach the authors use a separately reconstructed indel history and compute emission probabilities of indels for a phylo-hmm. however, to our knowledge phastcons has not yet been further developed in this direction and the signal of phastcons shown in figure  <dig> treats gaps as missing data. as expected, the kulcons score including the indel estimation is always lower or equal to the indel neglecting version. the scores coincide where no gaps are observed in the sliding window  and differ when one or more gaps are observed . a significant difference in the scores is observed in regions with many gaps. while the score based solely on the substitution rate indicates high conservation, the score respecting the gaps indicates low conservation. compared to kulcons, gaps seem to be far less penalized by the scone score which does not show notable deviations in the gappy regions.

discussion
in figure  <dig> we showed a comparison of kulcons to phastcons, gerp and scone. the methods aim to detect sequence conservation and/or constraint based on different models: phastcons quantizes the rate heterogeneity parameter in two different categories. one category represents constrained evolution and the other neutral evolution which are modeled as the states of a phylogenetic-hidden markov model  each associated with different ψ  <cit> . phastcons scores reflect the a posteriori state probabilities of the hmm and thus express the probability of constraint, based on the underlying degree of conservation and the assumptions about neutral evolution imposed on the hidden-markov model. while this is very well suited for high throughput processing, a simplistic binary model on genome evolution is imposed. the two state hmm implies that evolution is either conserving or neutral. the model has to be tuned with a priori information such as transition rates among the conserved and the neutral state, which implicitly imposes assumptions about the expected length and coverage of conserved regions. the result of the binary model can be clearly observed in figure  <dig> providing clear indication for strong or weak conservation but lacking sensitivity for different degrees of conservation. gerp compares observed and expected substitution rates on a phylogenetic tree with fixed topology. the branch lengths of the observed tree are estimated for each column separately and branch lengths of the expected tree are based on the average of estimates from neutral sites. the final score is the difference of the observed to the expected substitution rate induced by the corresponding estimated trees  <cit> . gerp predicts constraint elements using a null model of shuffled alignments.

scone scores express the p-value that a position evolved neutrally given a model that accounts for context-dependency, indel events and neutral evolution. hence, the score can as well be interpreted as a probability of constraint  <cit> .

another method used in the encode analysis, bincons developed by margulies et al.  <cit> , was not included in the comparison because it was noted by siepel  <cit>  that scores of bincons and phastcons give qualitatively similar results. in contrast to the approaches mentioned above, kulcons considers the direct estimation of the rate heterogeneity θi ∈ ℝ+ or more parameters from an evolutionary model ψ via maximum likelihood using an optimized sliding window. the kullback-leibler divergence is used to project the estimated parameters to a conservation score. the rate parameter θ is the crucial parameter for detecting evolutionary conservation and the ml sliding window approach in silico can achieve high estimation accuracy assuming a model of gamma distributed rates with autocorrelation. we believe that kulcons has the following advantages:

 <dig>  the presented algorithm is free of assumptions about neutral evolutionary rates that are notoriously hard to determine  <cit> . furthermore, it uses few a priori parameters that require biological considerations. we have shown that our ml estimation of substitution rates in an optimized gauss window without assumptions on the rate prior leads to good performance in the mse sense.

 <dig>  our score reflects well the different degrees of conservations and is in accordance with state-of-the-art methods. this soft score may disclose new possibilities in comparative genome analysis allowing the comparison of different finescale conservation patterns within conserved regions of interest.

 <dig>  it is possible to extend the phylogenetic model as long as a distribution on the columns of the alignment is induced. a whole set of different process parameters can then be mapped to a conservation score via the kullback-leibler divergence. a score was shown in figure  <dig> that uses co-estimated indel rate parameters. another possibility would be to assign different θ to different subtrees thus allowing for lineage-specific rate heterogeneities.

our results show that the kulcons score qualitatively exhibits similar conservation patterns in different regions as gerp and scone. this observation has two important consequences: first, it is possible to score the conservation of dna sequences without having assumptions or estimates on neutral rates. the estimation and potential bias of these rates have been controversially discussed in the past  <cit> . secondly however, our results suggest that conserved elements inferred from this method will probably not be very different from those discovered by gerp and scone opposed to the conjecture raised in  <cit> . this would mean that the discrepancies of experimentally verified functional elements and computationally predicted conserved regions  <cit>  cannot be explained in majority by biased assumptions on neutral rates. one explanation might be that low scoring sequences experience constraints at a different information level  that is not directly detectable by simple sequence alignments but rather structural alignments. an alternative explanation is that species specific functional elements that are not conserved across a given set of species are more important in functional evolution than currently discussed.

CONCLUSIONS
we presented and evaluated a novel method for the calculation of sequence conservation scores over multiple sequence alignments. opposed to existing methods, we avoid estimates of neutral substitution rates by testing divergence from perfectly conserved columns on the assumption that these represent maximum conservation. furthermore our method does not assume a prior distribution on the rate heterogeneity and does not require prior tuning. our simulation results suggest that local ml estimation of substitution rates in a sliding gauss window can achieve a high accuracy in detecting patterns of conservation. we qualitatively compared our score to the scores of established methods  in encode regions and found that our algorithm is well suited for discriminating among different degrees of conservation and reveals good accordance with scores produced by gerp and scone. we find that even though kulcons differs from gerp and scone in several regions it does not seem to indicate surprisingly different conserved elements. a strong advantage of our approach is that it also allows for multiple parameters to contribute to the conservation score in a probabilistic framework and thus can for example account for insertions and deletions which many other known methods do not.

