BACKGROUND
since most genetic regulatory systems involve many components connected through complex networks of interactions, formal methods and computer tools for modeling and simulating are needed. therefore, various formalisms were proposed to describe genetic regulatory systems, including boolean networks and their generalizations, ordinary and partial differential equations, stochastic equations and bayesian networks .

while differential and stochastic equations describe the biophysical processes at a very refined level of detail and prove useful in simulations of well studied systems, bayesian networks appear attractive in the field of inferring the regulatory network structure from gene expression data  <cit> . the reason is that their learning techniques have a solid basis in statistics, allowing them to deal with the stochastic aspects of gene expressions and noisy measurements in a natural way. other formalisms applied to this task include boolean networks  <cit> , weighted graphs  <cit> , ordinary differential equations  <cit>  and information-theoretic approaches  <cit> .

a bayesian network  is a representation of a joint probability distribution over a set of random variables. it consists of two components:

• a directed acyclic graph whose vertices correspond to random variables and edges indicate conditional dependence relations

• a family of conditional distributions for each variable, given its parents in the graph.

together, these two components determine a unique joint distribution.

when applying bayesian networks to genetic regulatory systems, vertices are identified with genes and their expression levels, edges indicate interactions between genes and conditional distributions describe these interactions. given a set of gene expression data, the learning techniques for bayesian networks allow one to infer networks that match this set well. however, as it was shown in  <cit> , the problem of finding an optimal network is np-hard. consequently, one has to choose between restricting to small gene networks  and inferring suboptimal networks by heuristic search methods .

it should be also pointed out that the basic bn formalism has some major limitations. first, several networks with the same undirected graph structure but different directions of some edges may represent the same distribution. hence, relying on expression levels only, the origin and the target of an interaction become indistinguishable. second, the acyclicity constraint rules out feedback loops, essential in genetic networks. third, the dynamics of a gene regulatory system is not taken into account.

the above limitations may be overcome by dynamic bayesian networks , which model the stochastic evolution of a set of random variables over time. in comparison with bns, discrete time is introduced and conditional distributions are related to the values of parent variables in the previous time point. moreover, in dbns the acyclicity constraint is relaxed.

given a set of time series of expression data, the learning techniques adapted from bns allow one to infer dynamic networks that match well the temporal evolution contained in the series. the papers  <cit>  and  <cit>  initiated a series of biological applications of dbns  <cit> .

a special treatment is required for experiments in which expression of some genes was perturbed . since perturbations change the structure of interactions , the learning techniques have to use data selectively.

it should be also pointed out that not every perturbation experiment may be realized in practice. the reason is that some perturbations of a regulatory mechanism may be lethal to the organism. on the other hand data from perturbation experiments are particularly rich in information regarding causal relationships.

inferring networks from perturbed expression profiles by means of bns was investigated in  <cit>  and  <cit> . to our knowledge the dbn technique has not been applied in the context of perturbation experiments. in the present paper we extend the framework of dbns to deal with microarray data from perturbation experiments. we propose an exact algorithm for inferring an optimal network under bde scoring function. moreover, we propose a method of discretization of expression levels, suitable for the data coming from time series perturbation experiments. the above methodology is applied to realistic simulations data. we perform statistical analysis, via a suitably defined p-value, which assesses the statistical significance of the inferred networks. as a way of assessing the quality of the scoring function we estimate the percentage of networks with scores better than the score of the original network. we show that the quality of inferred networks dramatically improves when using data from perturbations. we also show some advantages of our exact algorithm over heuristics like markov chain monte carlo  method.

data and preprocessing
when analysing learning procedure's efficiency, the procedure should be applied to the data generated by a known network, which then might be compared with the inferred one. to this aim, most studies apply procedures to gene expression data and compare inferred interactions with those found in biological literature. the disadvantage of this approach is that our knowledge of the structures of real biological networks is far from being complete even in the most deeply investigated organisms. although many interactions between genes are known, there are very few results stating the absence of some interactions. thus no conclusion can be drawn from the fact that the procedure inferred unknown interaction. the above disadvantage is no longer present when data are generated from a mathematical model simulating real networks. however, a danger of this approach is that the employed model simplifies the real process, losing important biological features. in that case, analysis of a learning procedure is aimed at its ability to infer an artificial model rather than real biological networks.

husmeier in  <cit>  suggests that a satisfactory compromise between the above two extremes is to apply the learning procedure to data generated by a system of ordinary differential equations.

basic model
in the present study we generate data using the model proposed in  <cit> . the model consists of  <dig> species of molecules, representing  <dig> genes with their transcription factors, promoters, mrnas, proteins and protein dimers, connected through  <dig> elementary reactions, including transcription factor binding, transcription, translation, dimerization, mrna degradation and protein degradation. the dynamics of the model is described by the system of ordinary differential equations of the following form:

ddt=kdig2−kudg−kbihg+kubhg−kdeg2ddt=−kbihg+kubhgddt=−kdmh+ktrh+ktrhgddt=−kdeh+ktlh
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqabeabbaaaaeaadawcaaqaaiabdsgakjabcufabjabdeeahnaabaaaleaacqaiyagmaeqaaogaeiyxa0fabagaemizaqmaemidaqhaaiabg2da9iabdugarnaabaaaleaacqwgkbazcqwgpbqacqwghbwraeqaaogaei4waslaem4rackaeiyxa01aawbaasqabeaacqaiyagmaagccqghsislcqwgrbwadawgaawcbagaemydaunaemizaqmaem4raceabeaakiabcufabjabdeeahnaabaaaleaacqaiyagmaeqaaogaeiyxa0laeyoei0iaem4aas2aasbaasqaaiabdkgaijabdmgapjabdieaijabdeeahbqabagccqggbbwwcqwghbwrdawgaawcbagaegomaidabeaakiabc2fadjabcufabjabdchawjabdieaijabc2fadjabgucariabdugarnaabaaaleaacqwg1bqdcqwgibgycqwgibascqwghbwraeqaaogaei4waslaem4rac0aasbaasqaaiabikdayaqabagccqghfly1cqwgwbaccqwgibascqggdbqxcqghsislcqwgrbwadawgaawcbagaemizaqmaemyzaumaem4rac0aasbaawqaaiabikdayaqabaaaleqaaogaei4waslaem4rac0aasbaasqaaiabikdayaqabagccqggdbqxaeaadawcaaqaaiabdsgakjabcufabjabdchawjabdieaijabc2fadbqaaiabdsgakjabdsha0baacqgh9aqpcqghsislcqwgrbwadawgaawcbagaemoyaimaemyaakmaemisagkaem4raceabeaakiabcufabjabdeeahnaabaaaleaacqaiyagmaeqaaogaeiyxa0laei4waslaemicaanaemisagkaeiyxa0laey4kasiaem4aas2aasbaasqaaiabdwha1jabdkgaijabdieaijabdeeahbqabagccqggbbwwcqwghbwrdawgaawcbagaegomaidabeaakiabgwsixlabdchawjabdieaijabc2fadbqaamaalaaabagaemizaqmaei4waslaemyba0maemisagkaeiyxa0fabagaemizaqmaemidaqhaaiabg2da9iabgkhitiabdugarnaabaaaleaacqwgkbazcqwgtbqbcqwgibasaeqaaogaei4waslaemyba0maemisagkaeiyxa0laey4kasiaem4aas2aasbaasqaaiabdsha0jabdkhayjabdieaibqabagccqggbbwwcqwgwbaccqwgibascqggdbqxcqghrawkcqwgrbwadawgaawcbagaemidaqnaemocainaemisagkaem4raceabeaakiabcufabjabdeeahnaabaaaleaacqaiyagmaeqaaogaeyyxictaemicaanaemisagkaeiyxa0fabawaasaaaeaacqwgkbazcqggbbwwcqwgibascqggdbqxaeaacqwgkbazcqwg0badaagaeyypa0jaeyoei0iaem4aas2aasbaasqaaiabdsgakjabdwgaljabdieaibqabagccqggbbwwcqwgibascqggdbqxcqghrawkcqwgrbwadawgaawcbagaemidaqnaemibawmaemisageabeaakiabcufabjabd2gatjabdieaijabc2fadbaaaaa@f62c@

where t represents time, kx are kinetic constants of related reactions,  means concentration, px, mx, x and x <dig> are promoter, mrna, protein and dimer x, respectively, finally x·y stands for a transcription factor bound to a promoter.

the system is composed of structures reported in the biological literature  <cit> , i.e. a hysteretic oscillator, a genetic switch, cascades and a ligand binding mechanism that influences transcription . the whole network is shown in fig.  <dig> 

the total time of each simulation is set to  <dig> minutes. at time  <dig> minutes the ligand is injected for  <dig> minutes, changing the expression levels of all genes. the influence of the injection to expression decays throughout the interval 1500– <dig> minutes , but at time  <dig> minutes system dynamics becomes similar to the initial one.

all the equations and parameters of the model, as well as the matlab code to integrate it, are available in the supplementary materials to  <cit> .

this model is chosen for two reasons. first, differential equations formalism and biological origin of the structure guarantee realistic simulations. second, small size of the system  allows to avoid a noise arising from heuristic search methods, which are necessary when dealing with large networks. such noise might influence comparison of methods.

modified models
since genes g and k from the model are regulated by the same gene c and have the same kinetic constants, trajectories of their concentrations are identical. consequently, their contributions to the regulatory interactions are indistinguishable, given the expression data. for this reason, husmeier in  <cit>  tests efficiency of dbn based learning techniques using the model slightly modified by identifying both genes.

in the present study we introduce perturbations to the model. it is done by replacing the differential equation regarding the mrna of a perturbed gene by the following equation:

ddt=kpex
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaaiabdsgakjabcufabjabd2gatjabdifayjabc2fadbqaaiabdsgakjabdsha0baacqgh9aqpcqwgrbwadawgaawcbagaemicaanaemyzaumaemiwagfabeaakiabcicaoiabdogajjabgkhitiabcufabjabd2gatjabdifayjabc2fadjabcmcapaaa@4585@

which makes the concentration exponentially converging to c. taking c =  <dig> yields system with gene knocked out, while setting c to maximal  expression level of a gene makes it overexpressed.  <dig> simulations altogether are executed: one simulation with the basic system and  <dig> simulations with one gene knocked out or overexpressed.

octave scripts for generating expression time series are available in the supplementary materials  <cit> .

sampling and discretization
husmeier in  <cit>  chooses for his test  <dig> time points in equal length intervals between  <dig> and  <dig> minutes. he argues that more information is contained in the data derived from the system which is driven out of equilibrium by the ligand injection. in our tests husmeier's choice is repeated and other intervals are tried, as reported below.

before applying our learning procedure , the expression levels need to be discretize. one of the simplest methods of discretizing is partition of the interval of real numbers covering mrnas concentrations of each gene into subintervals of equal length, relevant to particular discretized values. another strategy is to base the discretization procedure on the meanings of introduced discrete expression levels .

husmeier in  <cit>  applies the former approach with  <dig> discretized expression levels, and we follow him in the case of unperturbed data.

the specificity of perturbed data suggests the latter approach. the simulation of an unperturbed system specifies the reference point for expression levels of perturbed data. thus discretization consists in comparing each concentration value from a perturbed system simulation with the concentration value of the same gene at the same time point of the unperturbed system simulation. when the values are close to each other, i.e.



the expression level is set to 'baseline', otherwise it is set to 'over-' or 'under-expressed'. the log-ratios of concentration values in knockout simulations are shown in fig.  <dig>  the threshold  <dig>  seems to minimize the loss of information, inevitable in the discretization process. however, this choice, as well as the choice of the number of thresholds, is arbitrary.

discretized expression data files are available in the supplementary materials  <cit> .

methods
dynamic bayesian networks
a dynamic bayesian network n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfnevtaaa@383b@ is a representation of stochastic evolution of a set of random variables x = {x <dig> ..., xn} over discretized time. represented temporal process is assumed to be markovian, i.e.

p|x, x,..., x) = p|x)

and time homogenous, i.e.p|x) are independent of t. the representation consists of two components:

• a directed graph g =  encoding con ditional dependencies

• a family of conditional distributions p|pai), where pai = {xj ∈ x| ∈ e}

by assumption, the joint distribution over all the possible trajectories of the process decomposes into the following product form

p,x,…,x)=p)∏t=1tp|x)
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgqbaucqggoaakieqacqwfybawcqggoaakcqaiwaamcqggpaqkcqggsaalcqwfybawcqggoaakcqaixaqmcqggpaqkcqggsaalcqwimayscqggsaalcqwfybawcqggoaakcqwgubavcqggpaqkcqggpaqkcqgh9aqpcqwgqbaucqggoaakcqwfybawcqggoaakcqaiwaamcqggpaqkcqggpaqkdaqewbqaaiabdcfaqjabcicaoiab=hfayjabcicaoiabdsha0jabcmcapiabcyha8jab=hfayjabcicaoiabdsha0jabgkhitiabigdaxiabcmcapiabcmcapawcbagaemidaqnaeyypa0jaegymaedabagaemivaqfaniabg+givdaaaa@5c7e@

consequently, the evolution of the random variables is given by

p,…,x|x)=∏t=1tp|x)=∏t=1t∏i=1np|pai)==∏i=1n∏t=1tp|pai)     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqacegabaaabagaemiuaalaeiikagccbegae8hwaglaeiikagiaegymaejaeiykakiaeiilawiaesojgskaeiilawiae8hwaglaeiikagiaemivaqlaeiykakiaeiifawnae8hwaglaeiikagiaegimaajaeiykakiaeiykakiaeyypa0zaaebcaeaacqwgqbaucqggoaakcqwfybawcqggoaakcqwg0badcqggpaqkcqgg8bafcqwfybawcqggoaakcqwg0badcqghsislcqaixaqmcqggpaqkcqggpaqkcqgh9aqpdaqewbqaamaarahabagaemiuaalaeiikagiaemiwag1aasbaasqaaiabdmgapbqabagccqggoaakcqwg0badcqggpaqkcqgg8bafcqwfqbaucqwfhbqydawgaawcbagaemyaakgabeaakiabcicaoiabdsha0jabgkhitiabigdaxiabcmcapiabcmcapiabg2da9awcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabg+givdaaleaacqwg0badcqgh9aqpcqaixaqmaeaacqwgubava0gaey4diunaasqaaiabdsha0jabg2da9iabigdaxaqaaiabdsfaubqdcqghpis1aagcbagaeyypa0zaaebcaeaadaqewbqaaiabdcfaqjabcicaoiabdifaynaabaaaleaacqwgpbqaaeqaaogaeiikagiaemidaqnaeiykakiaeiifawnae8huaalae8xyae2aasbaasqaaiabdmgapbqabagccqggoaakcqwg0badcqghsislcqaixaqmcqggpaqkcqggpaqkasqaaiabdsha0jabg2da9iabigdaxaqaaiabdsfaubqdcqghpis1aawcbagaemyaakmaeyypa0jaegymaedabagaemoba4ganiabg+givdaaaogaaczcaiaaxmaadaqadaqaaiabigdaxagaayjkaiaawmcaaaaa@9f49@

inferring networks
the problem of learning a dbn is understood as follows: find a network graph that best matches a given dataset of time series of x-instances. the notion of a good match is formalized by means of a scoring function, usually bayesian dirichlet equivalence  score  <cit> , derived from the posterior probability of the network, given the data . owing to the product decomposition , bde score decomposes into the sum over the set of random variables. this property is extremely useful in learning procedures, since the parents of each variable may be computed independently. when the set of variables is small enough , one may score each subset as a possible parent set for each variable and choose the best match. otherwise, heuristic search methods have to be applied and the decomposition property is helpful in reducing the computation cost when scoring locally changed networks.

since our training datasets consist of mrna concentrations of  <dig> genes only, we can apply an exact algorithm. this choice allows us to avoid the noise caused by using heuristic search methods.

edges in the inferred network graph witness conditional dependence between variables in neighboring time points, which is interpreted as interaction between corresponding genes. however, a special care is required when inferring self-regulation. in this case it is clear that xi depends on xi because of natural inertia of mrna production and degradation. such dependence cannot be distinguished from actual auto-regulation by the scoring function currently used to select the best dbn model for the data. in the particular case of our experiments we have observed that with different choice of the number of time points we obtained all or none of the genes with self loops. this issue was addressed in other studies  <cit>  by explicitly forbidding or forcing the presence of self-loops in all considered models. we take the same approach in the present paper. however it remains an open question whether the dbn scoring functions could be extended to distinguish between inertia and self-dependence.

perturbations
when expression of a gene is perturbed in an experiment , its natural regulation is blocked and replaced by the perturbation scheme. consequently, regarding that gene's regulation mechanisms, the experiment contributes noise to the model instead of information. on the other hand, the remaining interactions might be significantly reflected in data, in particular those in which the gene acts as a regulator. therefore our learning procedure has to make use of data from perturbation experiments selectively.

recall that the parent sets of each gene may be inferred independently. thus, when inferring parents of a particular gene, we apply the standard learning procedure to the dataset restricted to those experiments, in which this gene's expression was not perturbed. when parents of all genes are computed, the network graph is composed. a related method in the framework of static bns was successfully used in  <cit>  and  <cit> . summarizing, our exact algorithm can be expressed as follows:

for each gene g

   choose all experiments with unperturbed expression of g

   for each potential parent set pa of g

      compute the local score in g for pa and chosen experiments

   choose the parent set of g yielding optimal score compose the network from the chosen parent sets

software for finding optimal dbns is available in the supplementary materials  <cit> .

RESULTS
in the present section our exact algorithm is applied to the datasets from the model modified by introducing perturbations. the results are compared with those obtained from the basic model, as well as with those obtained by bayesian learning with markov chain monte carlo  method  <cit> .

experiments
in the first experiment we followed the procedure of husmeier  <cit>  . the sensitivity of our exact algorithm was similar to husmeier's heuristics – see fig.  <dig> 

next we turned to the knockout data. recall that the entire system with all  <dig> genes was considered and the discretization was made according to the comparison of expression levels from perturbed and unperturbed profiles.

the first set of time points was chosen as in the above experiment, resulting in  <dig> edges corresponding to direct transcriptional regulation,  <dig> edge due to an interaction triggered by the ligand and  <dig> spurious edges ).

the dataset used in the experiment was quite large:  <dig> series,  <dig> time points each gives  <dig> slices. on the other hand, the variability of discretized expression levels is rather low – as is shown in fig.  <dig>  the thresholds are usually crossed at most one time per series. therefore the steadiness of expression is represented in enormous proportion. moreover, the sampling rate fails to match the delay of regulation processes. since edges in dbns represent conditional dependencies in neighboring time points, the learning process is affected. consequently, a large number of false positives appears in the inferred network. the variability of expression is reduced by the discretization process. the choice of our discretization threshold  <dig>  is aimed at minimizing this reduction . the variability may be increased by allowing more discretization levels, but it can disturb inferring. the reason is that the bn formalism disregards the structure of sets of possible values of random variables. for example, the information that the discretized expression level '0' is closer to the level '1' than to '2' is ignored. consequently, the learning procedure treats equally the situation in which some configuration of regulators causes a regulon to assume the value '0' or '1' with the one in which it is caused to assume the value '0' or '2'. our experiments with gene expression discretized into more than  <dig> levels do not improve results .

the disproportion between a large dataset and a low variability may be avoided by decreasing a number of samples. hence we decided to choose for the next experiment  <dig> time points in equal length intervals between  <dig> and  <dig> minutes. the accuracy significantly improved – the inferred network contains  <dig> edges corresponding to direct transcriptional regulation,  <dig> reflecting posttranscriptional regulation and  <dig> spurious edges ).

another time intervals were tried, resulting in networks less accurate than the two above , which confirms husmeier's assertion of low information content of signals from a system being in equilibrium.

corresponding experiments were also executed for the overexpression data, as well as for both kinds of perturbed data together. accuracy of overexpression experiments does not match that of knockout ones. however, it is worth pointing out that, unlike the knockout data case, the edges a→c, b→c and e→f were inferred correctly.

the best results were obtained when both kinds of perturbations were used together. as it is shown on fig.  <dig>  the inferred network contains  <dig> edges corresponding to direct transcriptional regulation,  <dig> edge due to an interaction triggered by the ligand,  <dig> reflecting posttranscriptional regulation and  <dig> spurious edges. the last experiment aimed at comparing our exact algorithm with heuristic methods of searching networks with optimal scores. we adapted the mcmc algorithm of husmeier  <cit>  to work with perturbations and applied it to our data. the accuracy of obtained networks was lower then the one of networks resulting from our exact algorithm – see fig.  <dig>  moreover, the experiments showed two disadvantages of this method. first, when the number of genes is small , the mcmc algorithm is significantly slower than the exact one. second, due to the non-deterministic character of the algorithm, the networks inferred in succeeding simulations were highly variable. for example, husmeier's experiment on unperturbed data, repeated  <dig> times, resulted in the set of networks with the number of correctly inferred edges varying from  <dig> to  <dig>  moreover, the network obtained by husmeier ) did not appear among them.

statistical analysis
we define the p-value of a network with k true out of m inferred edges to be the probability of finding at least k true when choosing m edges at random. according to the hypergeometric distribution, the probability of n successful selections out of m from a set of n true and m - n false edges is given by

pn=
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgwbacdawgaawcbagaemoba4gabeaakiabg2da9maalaaabawaaewaaeaafaqabegabaaabagaemota4eabagaemoba4gaaagaayjkaiaawmcaamaabmaabaqbaeqabiqaaaqaaiabd2eanjabgkhitiabd6eaobqaaiabd2gatjabgkhitiabd6gaubaaaiaawicacaglpaaaaeaadaqadaqaauaabeqaceaaaeaacqwgnbqtaeaacqwgtbqbaaaacagloagaayzkaaaaaaaa@4182@

consequently,the p-value of a network is defined by

pval=∑n=kmin
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgwbaccqwg2bgdcqwghbqycqwgsbabcqgh9aqpdaaewbqaamaalaaabawaaewaaeaafaqabegabaaabagaemota4eabagaemoba4gaaagaayjkaiaawmcaamaabmaabaqbaeqabiqaaaqaaiabd2eanjabgkhitiabd6eaobqaaiabd2gatjabgkhitiabd6gaubaaaiaawicacaglpaaaaeaadaqadaqaauaabeqaceaaaeaacqwgnbqtaeaacqwgtbqbaaaacagloagaayzkaaaaaawcbagaemoba4maeyypa0jaem4aasgabaacbigae8xba0mae8xaakmae8nba4maeiikagiaemyba0maeiilawiaemota4kaeiykakcaniabgghildaaaa@5350@

where m equals  <dig> for the full considered network or  <dig> for the network restricted to  <dig> genes . the value of n depends on it if we allow direct transcriptional regulation only. p-values of the networks from fig.  <dig> and  <dig> are grouped in the table  <dig> 

the above considerations refer to inferring local interactions between genes, represented by particular edges. in order to analyse the ability to infer a global interaction system, one has to compare the score of the original network with the scores of other networks. since it is impossible to compute the scores of all graphs , we sampled randomly  <dig>  <dig> graphs. for each graph, edges were generated independently, each with the same probability. the uniform distribution on the space of all graphs could be obtained by setting this probability to 1/ <dig>  but it would cause scores of most of graphs to be dominated by high penalties due to excessive structures. in order to get networks with scores close to the original one, there was chosen the probability resulting in the expected number of  <dig> edges in the graph .

original and randomly generated graphs are available in the supplementary materials  <cit> .

the tables show that using perturbed data significantly improves the possibility of inferring the original network. the results obtained in the experiments with  <dig> time points are usually better than those in the experiment with  <dig> time points, but the differences between them are relatively small.

comparison of the values for particular versions of the algorithm shows that the best results are obtained when self-loops are forbidden, slightly worse when self-loops are permitted and significantly worse when they are forced. the analysis of the best scored networks with permitted self-loops leads to the statement that self-regulation of genes cannot be handled within our framework correctly and requires more refined methods. therefore, with respect to our algorithm's variants, the best choice is to forbid self-loops.

CONCLUSIONS
in the present paper the framework of dynamic bayesian networks is extended in order to handle gene expression perturbations. a new discretization method specialized for datasets from time series gene perturbation experiments is also introduced. networks inferred from realistic simulations data by our method are compared with those obtained by dbns learning techniques.

the comparison shows that application of our method substantially improves quality of inference. moreover, our results lead to the suggestion that the exact algorithm should be applied when it is possible, i.e. when the set of genes is small enough. the reason is high variability of the networks resulting from heuristics and their lower accuracy.

since self-regulating interactions appeared to be intractable by dbn learning techniques, we also suggest to forbid self-connecting edges in inferred networks. our experiments show that this choice makes the learning procedure more sensitive to other interactions than it would be with self-loops permitted or forced. an important problem for designing time series expression experiments is to determine sampling rates properly. our experiments show that assuming too short rate results in noisy expression profiles, just as when the samples are chosen from the system being in equilibrium. consequently, networks inferred from over-sampled datasets have low accuracy.

the reason of this surprising finding is the markovian assumption of dbns, which states that the value of an expression profile from a particular time point depends on the value of the profile from the preceding time point only. it means that the sampling rate has to match the delay of regulation processes. most learning procedures working with time series gene expression data make similar assumptions. this is unlike those working with independent expression profiles , which assume that considered profiles represent steady states.

authors' contributions
nd designed the extension of dbn framework incorporating perturbations, performed the statistical analysis and participated in executing the experiments. ag participated in the design of the study. am implemented and tested the exact algorithm. bw implemented modifications concerning perturbations: to the system of differential equations of regulatory network  <cit>  and to the mcmc learning procedure  <cit>  and participated in executing the experiments. jt coordinated the study. nd, bw and jt edited the final manuscript. all authors read and approved the final manuscript.

