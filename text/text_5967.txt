BACKGROUND
microarray has become a standard tool in many biological studies. typically, classification analyses, where gene expressions of distinct biological groups are compared and classified according to their gene expression characteristics, are frequently performed in various clinical situations such as tumor diagnosis  <cit> , anti-cancer drug response analysis  <cit> , and prognosis analysis  <cit> . kernel methods  <cit>  play important roles in such disease analyses, especially when classifying data with support vector machines   <cit>  based on the feature or marker genes that are correlated with the characteristics of the groups. in most of those studies, only standard kernels such as linear, polynomial, and rbf , which take vectorial data as input, have been popularly used and generally successful.

other than the above vectorial data kernel family, there is another family called structured data kernel family that has been studied in many other fields including bioinformatics and machine learning  <cit> . the structured data kernel family conveys structural or topological information with or without numerical data as input to describe data. for example, the string kernel for text classification  <cit> , the marginalized count kernel  <cit>  for biological sequences, the diffusion kernel  <cit>  and the maximum entropy  kernel  <cit>  for graph structures are well known in the biological field.

in microarray analysis, one of the main issues that hamper accurate and realistic predictions is the lack of repeat experiments, often due to financial problems or rarity of specimens such as minor diseases. utilization of public or old data together with one's current data could solve this problem; many studies combining several microarray datasets have been performed  <cit> . however, due to the low gene overlaps and consistencies between different datasets, the vectorial data kernels are often unsuccessful in classifying data from various datasets if naïvely integrated  <cit> .

among the structured data kernels, the me kernel can take any distance data as input, and is thus applicable to vectorial data as well when converted into the euclidean or other types of distance relationships among vectors. since the me kernel increases the distances among different sample vectors , while keeping similar samples in close distance, discriminative boundaries may be found more explicitly than in the case of the vectorial kernels due to the sparse distribution of heterogeneous samples . furthermore, the me kernel has, unlike the rbf kernel, a special property of excluding arbitrary gene values composing vectorial data in calculating the distances among samples. hence, by ignoring only spurious gene values in each sample without deleting those genes entirely from a dataset, the me kernel can effectively utilize gene expression information in heterogeneous data containing mosaic-like missing or noisy values.

this paper is constructed as follows. we first show how the me kernel can effectively work in heterogeneous microarray data using the euclidean distance among sample vectors. then, we show the unique and powerful noise reduction ability of the me kernel in microarray data. finally, we demonstrate that the me kernel performs better than the standard kernels in classifying practical microarray data, namely, squamous cell carcinoma metastasis in the oral cavity.

RESULTS
we describe herein the classification performance of the me kernel, compared to that of the three standard kernels, linear, polynomial and rbf. we also test two types of distance-based kernels, ekm and saigo  <cit> , for comparison. the schematic view of the entire analysis process is shown in figure  <dig>  note that the rbf kernel also uses euclidean distance as the metric of sample  similarities but cannot use the k-nearest neighbor gene distance  since it violates the positive semidefiniteness of kernels.

we first use heterogeneous kidney carcinoma data to confirm the me kernel's superior discrimination ability against a highly mixed heterogeneous dataset. then, we demonstrate the me kernel's interesting denoising ability based on knnd using homogeneous leukemia microarray data with artificial noise. finally, we further apply the me kernel with knnd denoising to a more practical problem, i.e., heterogeneous data of squamous cell carcinoma metastasis in the oral cavity, to assess its total performance.

data normalization and classification analysis
before testing the performance, all the data are properly normalized by being first log-transformed, and then scaled to mean  <dig> and standard deviation  <dig>  in each sample and then each gene. all the normalized datasets are available for free at our internet server  <cit> . also the me program that runs on linux os is available upon request. many genes have a large number of missing values because heterogeneous data are combined; thus, we adopt a simple imputation method that all the missing values are replaced with the mean value, i.e.,  <dig>  input genes that show high correlation to class labels, or feature genes, are selected by the standard two sample t-statistics  <cit>  in each iteration of the leave-one-out cross-validation  test. the distance constraint matrices  are also generated from the same feature genes. if a sample contains missing values, we again adopt a simple imputation; we replace the one-dimensional euclidean distance  <dig> with  <dig> if xih or xjh is missing. the six kernels are tested with svms to analyze their classification performance with various numbers of feature genes and various parameters described in methods. the maximum accuracy among the tested parameters for each number of feature genes is recorded as the accuracy for each kernel.

heterogeneous kidney carcinoma data
the human kidney data of normal tissues and renal clear carcinoma tissues are collected from the public gene expression database, geo-gene expression omnibus  <cit> . this dataset is comprised of ten platforms, two of which are spotted dna/cdna arrays and eight are variations of affymetrix-type oligonucleotide arrays. to uniformly analyze the array data from different platforms, we converted as many probe names as possible to unigene identifiers and combined all the data. the total number of unigenes in the integrated table is as large as  <dig> , all of which contain missing values in some platforms; i.e., there are no genes common to all platforms. the total number of normal and carcinoma data is  <dig> . the characteristics of each data in the composite dataset, such as platform id, array type, number of data, and experimental comments, are shown in table  <dig> 

platform ids, array types, number of data, and experimental comments are shown.

classification analysis is performed between normal and carcinoma data. the results of the loocv test of  <dig> samples against various numbers  of feature genes are plotted in figure  <dig>  the figure shows typical prediction curves, namely, accuracy increases with increasing number of feature genes, plateaus at some region, and decreases. clearly, the me kernel performs much better in all cases than the other five kernels for small numbers of feature genes . as regards accuracy, the me kernel records maximum accuracies as high as  <dig>  % for  <dig> of feature genes. statistically, the accuracies of the me kernel are superior to those of the other five kernels in  <dig> % of the tested points  of feature genes. this percentage increases to  <dig> % when accuracies are limited only to the increase and plateau regions  of the me kernel.

knnd denoising for aml and all data
acute myeloid leukemia  and acute lymphoblastic leukemia  data for cancer subtype classification have been reported by golub et al.  <cit> . there are  <dig> samples , all of which are quite homogeneous and of good quality, and are thus suitable for artificial noise experiments. to assess the denoising ability of our me kernel, we first replace the νadd × 100% of original data in a gene expression profile with artificial white noise, i.e., the noise is added according to a normal distribution model with a mean of  <dig> and a standard deviation of twice that of each gene value distribution in the original dataset. then, we extract  <dig> feature genes from the training dataset for each iteration of the loocv test by standard t-statistics.

as the control experiments using linear and rbf kernels, the standard singular value decomposition  denoising method is applied to reduce noise immediately after the noise is introduced. in the svd denoising, three levels of noise removals by dfferent cumulative proportions,  <dig>   <dig>  and 95%, of eigenvalues are explored. for the me kernel, the knnd denoising method with the following noise level settings is applied. first, raw noise that is assumed to internally exist in the original data is arbitrarily set at νraw =  <dig> . then, we define the total noise level as the sum of the raw noise and the above artificially added noise, νadd. for example, if 10% noise is added, the total noise level is νraw + νadd =  <dig>  +  <dig>  =  <dig> , and  <dig> ×  <dig> =  <dig> % of the nearest distance genes out of the feature gene set are considered in calculating the knnds between samples .

we repeat the above random noise-adding test ten times and average the highest accuracies among various parameter combinations. the results are shown in figure  <dig>  the artificial noise added is within the range of 0–50%. since the raw data are quite homogeneous, all kernels except linear and polynomial show the same prediction accuracy of  <dig> % when no noise is added. this value decreases gradually with increasing noise levels  for the vectorial kernels; for example, the accuracies of the rbf kernel decrease in the order of  <dig> ,  <dig> ,  <dig> ,  <dig> , and  <dig> %. svd denoising boosts up these accuracies to  <dig> ,  <dig> ,  <dig> ,  <dig> , and  <dig> %, respectively. the linear and polynomial kernels also show similar accuracies to the rbf kernel when svd denoising is used.

interestingly and surprisingly, the three knnd-distance-based methods show high accuracies; for example, the knnd-me kernel has an accuracy of  <dig> % even at 20% noise level and maintains high accuracies of  <dig>  and  <dig> % at 30–40% noise levels. the ekm and saigo kernels using knnd-distance also show similar accuracies to the knnd-me kernel. to verify our results, we extensively analyzed the same data with various parameters including many cumulative proportions in the svd but obtained similar tendencies, confirming the superior denoising ability of the knnd-based method  <cit> .

heterogeneous oral cavity carcinoma metastasis data
we further analyze the total performance of the knnd-me method with a more practical problem-heterogeneous oral cavity carcinoma metastasis data. the data consist of two geo datasets  from different authors  <cit> . one dataset  is derived from primary squamous cell carcinoma dataset of the oral cavity  <cit> , containing  <dig> metastasis  and eight non-metastasis samples. the other oral squamous cell carcinoma dataset  is comprised of nine metastasis and nine non-metastasis samples   <cit> . both are from the same platform, affymetrix hg-u133a, where  <dig>  genes are analyzed. the size of each dataset is too small and not suitable for svm classification if analyzed separately. however, combining the two datasets, we obtain as many as  <dig> metastasis and  <dig> non-metastasis samples, making it possible to carry out the classification analysis.

the results of the loocv test of the  <dig> samples against various numbers  of feature genes with four different kernels, namely, linear, polynomial, rbf, and me with knnd denoising , are shown in figure  <dig>  in the knnd-me kernel, five different noise levels, ν =  <dig> ,  <dig> ,  <dig> ,  <dig> , and  <dig>  are evaluated. for comparison, we also classify the two datasets separately and average the accuracies . the results clearly show that the knnd-me kernel surpasses the other kernels in both averaged and mixed datasets. statistically, the accuracies of the knnd-me kernel are superior to those of the other three kernels in the averaged and the mixed datasets in 98% and 48%, respectively, of all the tested points. the difference in accuracy is much greater in the averaged dataset than in the mixed dataset. the mean differences between the knnd-me kernel and either of the linear, polynomial or rbf kernel with highest accuracy at each point in the averaged and the mixed datasets are  <dig> % and  <dig> %, respectively. the accuracies increase and plateau at around 3– <dig> feature genes in the mixed dataset, while no clear increase or plateau is found for the averaged dataset. the overall maximum accuracy of  <dig>  % is observed for the knnd-me kernel at two points,  <dig> and  <dig> feature genes, in the mixed dataset. those accuracies are obtained with ν =  <dig> and  <dig>  denoising parameters. the result also indicates that the knnd-me kernel shows more stable and higher accuracies than the other kernels for large numbers of feature genes.

incidentally, the top  <dig> feature genes that show the highest average ranks by t-statistics in the loocv test and that are considered to be associated with oral carcinoma metastasis are: hfe , flj <dig> , cxorf <dig> , heatr <dig> , mgam , apol <dig> , pyy <dig> , rbp <dig> , ube2v <dig> , kcnj <dig> , gls , arhgef <dig> , mdm <dig> , zc3h <dig> , and c9orf <dig> .

we further investigate the effect of the svd denoising when applied to the mixed dataset before learning and classification. table  <dig> summarizes the results of using all the six types of kernels for raw and svd pre-denoised data. the accuracies are averaged in each of the ten gene windows. in the svd denoising, three levels of noise removals , which are the same as the aml-all experiment, are tested.

maximum accuracy in each range of feature gene number is shown in bold.

although a sufficient number of genes  are used for svd denoising, the denoised dataset does not significantly improve the raw accuracies in small numbers  of feature genes, where the overall maximum range accuracy  exists. svd denoising affects only large numbers  of feature genes. this is probably related to the property of svd denoising that affects the ratio of information to noise content. further analysis is needed to understand the full property of the svd denoising method. in summary, the maximum accuracy  of the knnd-me kernel in raw data is not improved by svd denoising .

discussion
using kidney carcinoma data, we show that the me kernel generally gives better classification results for heterogeneous microarray datasets than the three vectorial data kernels, linear, polynomial and rbf. as an alternative approach using vectorial data kernels, it is theoretically possible to train multiple svms for all distinct sub-data contained in the composite dataset. however, this approach has practical diffculties in that  there are too many heterogeneous sub-data,  some sub-data contain only a few samples, and  some sub-data contain all positive  samples. the svms cannot be trained properly with only a few samples or data with one-sided  labels. in addition, if we do not know the origin  of the test samples, it would be diffcult to determine which svms should be used for the classification. the me kernel is much simpler yet quite flexible in this regard.

another remarkable property of our me kernel is that the generated kernel matrices always hold positive semidefiniteness, even when the distance matrices for input to our optimization algorithm violate the triangle inequalities. this allows one to arbitrarily choose genes from among a set of feature genes to build the distance matrices in a distance-by-distance fashion. utilizing this property, we devised the knnd denoising method for the distance-based kernels, which show better performance than the linear, polynomial and rbf kernels for leukemia data, even though the data are pre-denoised by svd. this is quite important in a situation where there are few or heterogeneous samples where svd may not work properly for denoising because the quality of the eigenvalue decomposition depends on the number of homogeneous samples. since the knnd denoising method only concerns the set of genes between sample pairs, it seems quite robust with regard to the number of samples or the degree of heterogeneity.

furthermore, the results of kidney carcinoma and oral cavity carcinoma metastasis data in figure  <dig> and table  <dig> clearly show that the accuracies of the me kernel exceed those of the other two distance-based kernels, ekm and saigo. however, in the aml-all data shown in figure  <dig>  the me kernel and the other two distance-based methods show similar accuracies although all of them use the same knnd distance data. from these observations, we can conclude that the entropy maximization process works favorably for 'heterogeneous' data and allows svms to find the discriminant boundaries more easily than the other two distance-based methods, ekm and saigo.

it is also important to point out that combining similar but distinct data in the microarray analysis may enhance the diagnosis of cancer or other diseases. as shown in our example of metastasis prediction for oral squamous cell carcinoma, each dataset contains only around  <dig> samples, which is not suitable for training of good svm predictors, especially in the case of the vectorial data kernel family . when the datasets are combined, however, our knnd-me kernel demonstrates higher and more robust classification performance than the linear, polynomial, and rbf kernels and even the other two distance-based kernels, regardless of svd denoising.

CONCLUSIONS
we conclude that the me kernel-based svm classification method will generally be useful for the analysis of promiscuous microarray data of rare specimens, e.g., minor diseases or species, that present difficulty in compiling homogeneous data in a single laboratory.

