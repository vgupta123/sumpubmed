BACKGROUND
it has become common practice to use microarray technology to find "interesting" genes by comparing two or more different phenotypes. modern methods of microarray data analysis typically employ two-sample statistical tests to test differential expression of genes, combined with multiple testing procedures to guard against type  <dig> errors . such methods are biased towards selecting those genes that display the most pronounced differential expression. once the list of genes showing statistically significant differential expression has been generated, these genes are often ranked using purely statistical criteria and this ranking is thought to reflect their relative importance. quite typically, a certain number of genes with the smallest p-values are finally selected from the list of all "significant" genes. while most biologists recognize that the magnitude of differential expression does not necessarily indicate biological significance, in the absence of better methods, this remains the dominant means to initially prioritize candidate genes. from a biological perspective, the above-described paradigm is far from a perfectly valid approach, because genes are not independent entities – they can interact with each other in many ways. as an example, a "chain reaction" type of a dependence structure of gene expressions was documented in the literature  <cit> . in such situation, even a very small change in expression of a particular gene may have dramatic physiological consequences if the protein encoded by this gene plays a catalytic role in a specific cell function. many other downstream genes may amplify the signal produced by this truly interesting gene, thereby increasing their chance to be selected by formal statistical methods. for an upstream regulatory gene, however, the chance of being selected by such methods may diminish as one keeps hunting for downstream genes that tend to show bigger changes in their expression. as a result, the initial list of candidates may be inflated with many effector genes that do little to elucidate the fundamental mechanisms of biological processes.

there are two natural ways to remedy this situation. one way is to use bioinformatics tools that utilize prior biological knowledge, such as partially known pathways, to prioritize candidate genes. this approach is now routinely used in biological studies and there are ongoing efforts to enrich it with specially designed algorithms  <cit> . the main weakness of the above approach is that the current biological knowledge is still quite limited and sometimes inaccurate. another way is to extract additional information on the changes of the relationships between different genes from microarray data by using pertinent statistical methods.

for example, if an upstream gene ceases to be catalytic in one phenotype, or this gene is active in two different biological pathways in two phenotypes, a carefully designed statistical test based on the intergene dependence structure should be able to detect this change. in more general situations, intergene dependence structure alone may be insufficient to pick up those upstream genes directly, but knowing the relationship changes across conditions points out possible directions for searching the interesting genes.

notwithstanding the importance of testing for differential expression of genes, we suggest approaching the problem of microarray data analysis from a different angle. we designed a new method to select those genes that are likely to change their relationships with other genes. more specifically, we suggest selecting candidate genes using a statistical test that detects changes in the whole correlation vector associated with each gene. this additional information will be instrumental in making the final selection of candidate genes more meaningful.

we propose to enrich the statistical inference from microarray gene expression data by testing the following hypothesis: the ith gene does not change its relationships with all other genes across the two phenotypes  under study. this can be accomplished by comparing the joint distribution of the correlation coefficients between this gene and other genes in different conditions.

we conducted a series of simulations with different configurations. the results obtained by our method were compared with those of a similarly designed univariate selection procedure. we observed that our method correctly selected those genes which change correlations with other genes but retained the same marginal distributions.

we also conducted various experiments with biological data. a large set of childhood leukemia data available from st. jude children's research hospital  <cit>  were used. our method selected some genes which were not selected by a comparable univariate approach.

biological data
the biological dataset used in this study was from the st. jude children's research hospital  database on childhood leukemia  <cit> . two groups of data were employed: patients  with hyperdiploid acute lymphoblastic leukemia  and patients  with a special translocation type of acute lymphoblastic leukemia. to make two data groups more comparable, only the first  <dig> patients in hyperdip were used.

since the original probe set definitions in affymetrix genechip data were known to be inaccurate  <cit> , we updated them by using a custom cdf file to produce values of gene expressions. the cdf file was obtained from . after that, each patient was represented by an array reporting the logarithm  of expression level on the set of  <dig> genes. for both datatsets, each gene was standardized so that it had zero mean and unit standard deviation. this was to avoid introducing false correlation coefficients when doing permutations.

methods
correlation vectors
let us denote m as the number of genes. for the ith gene, we computed the -dimensional random vector ri = . here rij is the sample correlation coefficient between the ith and the jth gene. this vector represents the relationships between the ith gene and all other genes. denote the -dimensional joint distribution functions of ri in two different conditions by fri and fri. a pertinent statistical test can be designed to test the basic null hypothesis

  hi:fri=fri. 

to increase the sensitivity of our test to departures from hi, especially when the correlation coefficients are very high, we applied the fisher transformation to the sample correlation coefficients:

  wik=12log⁡1+rik1−rik, 

where k =  <dig>  ⋯, i -  <dig>  i +  <dig>  ⋯, m. the power improvement was confirmed by our simulation . we denote the correlation vectors in two conditions by wi and wi, respectively.

in cv method with effect size  <dig> , the tp drops to  <dig>  without fisher transformation.

total number of genes:  <dig>  number of differentially correlated genes:  <dig>  method: group method. extended bonferroni threshold:  <dig> .

instead of testing hi, we tested

  h′i=fwi=fwi, 

where fwi and fwi are the joint distribution functions of wi and wi, respectively. if h′i was rejected, we declared the ith gene to be a differentially correlated gene.

in order to test the hypotheses based on the joint distribution functions of correlation vectors, we needed to create samples of correlation vectors. the following two methods were employed for this purpose:

• group method: divide each dataset into  <dig> subgroups, each containing  <dig> slides . by computing correlation vectors from each subgroup, we obtained a sample of size  <dig> 

• resampling method: randomly select  <dig> slides to calculate the correlation vector. repeat  <dig> times to get  <dig> correlation vectors in each group, respectively.

through the simulations, we found that these two methods were comparable, with the resampling method being slightly better in terms of testing power . however, the resampling method was much more computationally demanding. as an example, it took approximately  <dig> hours to analyze the biological data  with the group method.

total number of genes:  <dig>  number of differentially correlated genes:  <dig>  method: group method. extended bonferroni threshold:  <dig> .

total number of genes:  <dig>  number of differentially correlated genes:  <dig>  method: resampling method. extended bonferroni threshold:  <dig> .

total number of genes:  <dig>  number of differentially correlated genes:  <dig>  method: resampling method. extended bonferroni threshold:  <dig> .

for the resampling method, the computation time was  <dig> hours instead. all computations were done in a saturn cluster computer which includes  <dig> nodes each with 8× amd opteron dual-core processors  <dig> ghz , 16×2gb sdram.

throughout this paper, we use the group method unless otherwise noted.

n-statistic
we chose a multivariate nonparametric test based on n-statistic with euclidean kernel for testing the hypothesis h′i. this statistic has been successfully used to select differentially expressed genes and gene combinations in microarray data analysis  <cit> . the n-statistic is defined as follows:

  ni=2ns2∑k=1ns∑l=1nsl,wi)−1ns2∑k=1ns∑l=1nsl,wi)−1ns2∑k=1ns∑l=1nsl,wi), 

where ns is the number of correlation vector samples in each group, wi indicates the kth replication of the correlation vector , and l=||x−y||=∑s=1d <dig> is the kernel defined by euclidean distance.

after this step, the ith gene was assigned a non-negative number ni, a measurement of how much intergene correlation structure had changed from condition a to condition b.

 <dig>  resampling based p-values
we used the following algorithm to obtain resampling based p-values for each gene:

 <dig>  randomly shuffle the slides in two different conditions, then split them into two groups.

 <dig>  compute correlation vectors for each gene by using the group method or the resampling method.

 <dig>  compute n-statistic for each gene based on the correlation vectors.

 <dig>  repeat the above steps for k =  <dig>   <dig> times, record the resampling based n-statistics as nik, i =  <dig>  ..., m, k =  <dig>  ..., k. they can be used to construct the  null distribution for each index i.

 <dig>  compute ni, the n-statistic for each gene .

 <dig>  obtain the resampling based p-value, pi, by comparing ni with the null distribution constructed from nik. specifically, pi is defined to be #k, the proportion of nik which is greater than or equal to ni.

finally, we applied the extended bonferroni adjustment  <cit>  with threshold  <dig>  to control pfer . extended bonferroni adjustment is less conservative than the fwer  controlling procedures and more stable than fdr  controlling procedures in the context of microarray analysis. more details about this multiple testing adjustment procedure can be found in  <cit> .

from the computational perspective, it was very tempting to reduce the number of permutations by pooling all nik to construct one grand null distribution. however, we noticed that the null distributions for different genes can be very different. based on our biological data, the density functions for the significant genes tended to shift to the left compared to those associated with the non-significant genes .

univariate gene selection method
we would like to emphasize that our method  is nonparametric. because of this, we decided to compare the cv method to a nonparametrc univariate gene selection method: wilcoxon rank-sum test with the same extended bonferroni adjustment .

RESULTS
simulations
to gain better insight into the performance of the cv method, we simulated several sets of data. all sets had two groups of  <dig> arrays representing two different biological conditions . each array had m =  <dig> genes. denote the genes in the first condition by xi,  <dig> ≤ i ≤ m and genes in the second condition by yi,  <dig> ≤ i ≤ m. for both groups, all genes were identically distributed with marginal distribution n. with different baseline correlation structure, we had the following two classes of simulated datasets:

• simu1: any two distinct genes that were both in the set of the first  <dig> genes were correlated with coefficient ρd in condition a,  <dig>  in condition b. otherwise the correlation coefficient was  <dig> . here ρd was a constant taking value in { <dig> ,  <dig> ,  <dig> ,  <dig> }. condition b can be thought of as the control condition where genes were independent of each other. we called this dataset the independent base data.

• simu2: any two distinct genes that were both in the set of the first  <dig> genes were correlated with coefficient  <dig>  + ρd in condition a,  <dig>  in condition b. otherwise, the correlation coefficient was  <dig> . again, ρd was a constant taking value in { <dig> ,  <dig> ,  <dig> ,  <dig> }. unlike simu <dig>  the baseline intergene correlation was  <dig> . we called this dataset the dependent base data.

by this design, the differentially correlated genes were the first  <dig> genes for both simu <dig> and simu <dig>  ρd can be seen as a parameter indicating how much correlation structure had changed across two conditions.

for simu <dig> and simu <dig> and every ρd, we applied both the cv method and wrs method, and recorded the true/false positives. we also repeated this process  <dig> times with different random seeds to get the mean and standard deviation of the true/false positives. the results are shown in table  <dig> and table  <dig>  as expected, the cv method detected differentially correlated genes while the wrs method did not. the power of the cv method clearly increased as the effect size gets larger. also, it was easier to detect differentially correlated genes in the independent base data than in the dependent base data. this means that high baseline correlation structure deteriorated the power of the cv method.

simulations with biological data
the difference between simu <dig> and simu <dig> was that the baseline intergene correlation was much higher in simu <dig>  this was an attempt to model the intergene dependence structure in biological data. in some sense, a better way of modeling the actual dependence structure is through resampling from the biological data.

first, we combined hyperdip and tel data and randomly permuted the slides. we then divided them into two groups of an equal number of slides, mimicking two biological conditions without differentially correlated genes. for both conditions, genes were standardized so that the sample means equaled zero and the sample standard deviations were one. we denoted the entries in two groups by xij and yij,  <dig> ≤ i ≤  <dig> and  <dig> ≤ j ≤  <dig>  and the correlation matrix of these two groups by {ρik},  <dig> ≤ i, k ≤  <dig> 

next, we generated a 79-dimensional random vector with i.i.d. standard normal components. denote this vector by a = {aj},  <dig> ≤ j ≤  <dig>  we added a to the first  <dig> row vectors in the first condition with a tuning parameter ρ as follows: 1−ρxij+ρaj. these transformed entries are denoted as xij again, and we name this dataset simu <dig> 

the first condition had the correlation coefficients as follows:

 corr={ρik+ρfor 1⩽i,k⩽300 and i≠k,ρik1−ρfor 1⩽i⩽300 and 301⩽k⩽ <dig> ρikfor 301⩽i,k⩽7084 and i≠k, 

noticeably, the correlation coefficients between any two of the first  <dig> genes of the first group differed substantially from those of the second group, and these were considered as differentially correlated genes. the correlation coefficients between any two of the remaining  <dig> genes of the first group were the same as the corresponding correlation coefficients between those of the second group, so they were not considered as differentially correlated genes.

there was one caveat in this approach. even if xi was one of the  <dig> genes that were not differentially correlated, corr was still different in two conditions if xk happened to be a gene from the first  <dig> genes. in other words, the first  <dig> differentially correlated genes induced some small changes for those genes that were not differentially correlated. in practice however, when summarized through the n-statistic, these differences for the latter  <dig> genes were negligible and it was reasonable to view them as random fluctuations.

we set ρ =  <dig>  in simu <dig>  as before, we applied both the cv method and the wrs method. the above procedure was repeated  <dig> times, the results were summarized in table  <dig> 

total number of genes:  <dig>  number of differentially correlated genes:  <dig>  method: group method. extended bonferroni threshold:  <dig> .

the cv method selected most of differentially correlated genes while wrs method did not. the cv method also produced fewer false positives than the wrs method.

data analysis based on biological data
using the wrs method, we found  <dig> differentially expressed genes. using the cv method , we detected  <dig> differentially correlated genes. out of these,  <dig> were differentially expressed and  <dig> were not. these  <dig> genes were: cd1c , hdhd1a , basp <dig> , cyb5a  and tfpi . in the original study, two of these genes were found as differentiating among two leukemia subtypes  and the other three were never mentioned. differential correlation of these genes in two leukemia subtypes might provide some valuable information for better understanding the underlying subtypes' differences; however, these genes could not be captured by conventional tests. the results of this study  can be found in table  <dig> 

total number of genes:  <dig>  method: resampling method.

discussion and 
CONCLUSIONS
our method represents a radical conceptual change from current approaches focused solely on differentially expressed genes. however, this method is not intended to replace the existing methodology but rather to provide biologists with an additional source of information for decision making. as an example, the univariate method failed to detect gene cyb5a, which had a modest unadjusted p-value  <dig>  based on wilcoxon ranksum statistic. yet cyb5a was detected as a differentially correlated gene with an unadjusted p-value  <dig>  . to get a rough idea of how many genes had different correlation coefficients with cyb5a across two conditions, we looked at the marignal distributions of cyb5a' correlation vector.  <dig> genes were detected to have changed correlations with cyb5a dramatically across conditions. the selection procedure of these  <dig> genes can be summarized as follows: first, we splitted the slides in two conditions into  <dig> subgroups, respectively, as in the group method. second, we calculated  <dig> correlation vector samples of gene cyb5a in each condition. finally, for all  <dig> correlations , we applied wilcoxon rank-sum test to each of them to obtain an unadjusted p-value. after extended bonferroni adjustment,  <dig> genes were selected at significant level  <dig> . according to biocarta, about one third of all pathways associated with these  <dig> genes are related to cell cycle progression, cell division and control of centrosome duplication. hyperdip phenotype is characterized by the presence of more than  <dig> chromosomes. as a consequence, all pathways working for cellular maintenance and proliferation should be higly activated in hyperdip phenotype. the differential correlation of genes involved in pathways, related to cell proliferation between hyperdip and tel seemed reasonable and might deserve future studies.

the microarray technology yields unique multidimensional information on the functioning of the whole genome machinery at the level of transcription so that much can be learned about relationships between genes and mechanisms by which the cell assigns tasks to different genes to maintain a specific function. it is unfortunate that such an advanced technology continues to be used as a simplistic screening tool with a focus on big differences between mean values of expression measurements. the true potential of microarray technology has yet to be unveiled. it is noteworthy that recent years have seen a growing interest in correlations between gene expression levels in statistical methodologies for microarray analysis  <cit> . the correlation coefficient has been used extensively as a measure of similarity in gene clustering since a seminal paper by eisen et al.  <cit> 

however, very few studies have examined the possibility of using the intergene correlation structure to find important genes that are linked to disease. one obstacle lies in the fact that there are m different sample means but m <dig> different sample correlation coefficients. it is much harder to catch the differences hidden in the correlation matrix that has much higher degrees of freedom . furthermore, it is much more computationally intensive to compute the sample correlation coefficients than the sample means. consequently, we could not afford to use more than  <dig>   <dig> permutations to get finer p-value estimation, and we are thus reluctant to recommend the slower permutation based resampling method to the biologists, despite the fact that we know this method is more powerful than the group method.

as illustrated by our study on biological data  we were able to identify  <dig> genes, which changes the medians of their  distributions, yet only  <dig> genes were reported as differentially correlated . this seeming inadequacy of power was also shown in the simulation studies . this phenomenon might be subject to a number of explanations. it might be caused by the small sample size. due to the nature of the cv method, its statistical perspective is to compare the distribution of sample correlation coefficients instead of expression levels in different biological conditions. with the group method, we splitted  <dig> slides into subgroups and computed the sample correlation vectors from each group. as a result, we only had eight sample correlation vectors for each condition. we could divide the data into more subgroups, and then there would be fewer slides per subgroup so that the sample correlation coefficient computed from each subgroup would be less accurate. this was a trade-off. with the resampling method, we had  <dig> correlation vectors. having more than  <dig> resamplings would enhance the accuracy of the estimated n-statistics and improve the power; meanwhile, it would demand more computing time, making this another trade-off.

the choice of the euclidean distance as the kernel for the n-statistics might be another culprit. the euclidean distance kernel is a generic kernel that is invariant under any orthogonal transformation. in other words, it is symmetric and indifferent to all departure from the null distribution. a specifically designed kernel that is sensitive to the likely departure from the null distribution caused by the changes of correlation might significantly increase the selection power of the cv method. last, it may have been that indeed fewer genes were differentially correlated than were differentially expressed in the biological data. the hypotheses that the cv method was testing were entirely different from those tested by the univariate selection methods, such as the wrs method. it is absolutely possible that one gene is differentially expressed but not differentially correlated, or vice versa. the very fact that  <dig> out of  <dig> differentially correlated genes were differentially expressed in our study is an interesting phenomenon that is worth further investigation.

we believe many improvements can be made to enhance the selecting power of the cv method. we also firmly believe, as larger sets of microarray gene expression data become readily available, quantitative insights into dependencies between gene expression levels will gain increasing importance.

authors' contributions
the basic idea was first proposed by ay, lk and xq. the detailed study design was developed by all members in the research team. rh carried out the needed computations and simulations and the majority of the software development. gg conducted the pathway analysis for differentially expressed and differentially correlated genes. rh and xq were responsible for most of the write-up of the findings.

