BACKGROUND
high-throughput sequencing , also known as next-generation sequencing , is widely used to identify biological features such as rna transcript expression and histone modification to be quantified as tag count data by rna sequencing  and chromatin immunoprecipitation sequencing  analyses  <cit> . in particular, differential expression analysis based on tag count data has become a fundamental task for identifying differentially expressed genes or transcripts . such count-based technology covers a wide range of gene expression level  <cit> . several r  <cit>  packages have been developed for this purpose  <cit> .

in general, the procedure for identifying degs from tag count data consists of two steps: data normalization and identification of degs , and each r package has its own methods for these steps. for example, the r package edger <cit>  uses a global scaling method, the trimmed mean of m values  method  <cit> , in the data normalization step and an exact test for the negative binomial  distribution  <cit>  in the identification step. the estimated normalization factors are used within the statistical model for differential analysis and gene lists ranked in ascending order of p-value  are produced. naturally, a good normalization method combined with a deg identification method, should produce well-ranked gene lists in which true degs are top ranked and non-degs are bottom ranked according to the confidence or degree of differential expression . recent studies have demonstrated that the normalization method has more impact than the deg identification method on the gene list ranking  <cit> .

note that the normalization strategies employed by most r packages assume that there is an approximately balanced proportion of degs between the compared samples   <cit> . however, a loss of function of histone modification enzymes will lead to a biased distribution of degs between compared conditions in the corresponding chip-seq analysis; i.e., there will be data with biased de. as a result, methods assuming unbiased de will not work well on data with biased de. to normalize data that potentially has various scenarios , we recently proposed a multi-step normalization procedure called tbt  <cit> . tbt consists of three steps: data normalization using tmm  <cit>  , deg identification by using an empirical bayesian method implemented in the bayseq package  <cit>  , and data normalization using tmm  <cit>  after eliminating the estimated degs  comprising the tmm-bayseq-tmm pipeline. different from conventional methods, our multi-step normalization strategy can eliminate the negative effect of potential degs before the normalization in step  <dig> 

while the three-step tbt normalization method performed well on simulated and real two-group tag count data with replicates  <cit> , it is practically possible to make different choices for the methods in each step. a more comprehensive study regarding better choices for the deg elimination strategy  is needed. to our knowledge, only the r package, tcc , provides tools to perform multi-step normalization methods based on deges. our work presented here enables differential expression analysis of tag count data without having to worry much about biased distributions of degs.

implementation
the tcc package was developed in the r statistical environment. this is because r is widely used and the main functionalities in tcc consist of combinations of functions from the existing r/bioconductor  <cit>  packages . since the main purpose  of these three packages is essentially the same as that of tcc and many users may be experienced in their use, we will illustrate the main functionalities of tcc by contrasting them with the corresponding functions in those packages . while tcc employs object oriented programming design utilizing the r <dig> reference class, it has interface functions that do not change the object passed as the argument in order to be compatible with the semantics of the standard r environment. detailed documentation for this package is provided as a vignette:

 vignette”tcc” 

preparations
differential expression analysis between compared samples based on tag count data typically starts by preparing two objects: i) a count table matrix where each row indicates a gene, each column a sample, and each cell the number of counts  mapped to each gene in each sample and ii) a vector that indicates which group each sample belongs to. these data are stored in a tcc class object using the new function. similar functions of other packages are the dgelist function in the edger package, the newcountdataset function in the deseq package, the new function in the bayseq package, and so on . consider, for example, a matrix object hypodata consisting of  <dig>  rows and six columns and a numeric vector group consisting of six elements, i.e., . the first three samples in the matrix are from group  <dig> , and the others are from group  <dig> . the tcc class object is constructed as follows:

 librarytccdatahypodatagroup<−c <dig> , <dig> , <dig> tcc<−new”tcc”,hypodata,group 

normalization
normalization of two-group count data with replicates
when obtaining normalization factors from count data with replicates, users can select a total of six combinations  coupled with an virtually arbitrary number of iterations  in our deges-based normalization pipeline . here, we describe two representative choices .

deges/tbt
the tcc package provides robust normalization methods based on the deges recently proposed by kadota et al.  <cit> . the original three-step normalization method  is performed by specifying the two major arguments  as follows:

 tcc<−calcnormfactors 

in relation to the other deges-based methods, we will call the method “deges/tbt” for convenience. as mentioned in ref.  <cit> , the multi-step normalization can be repeated until the calculated normalization factors converge. the iterative version of the deges/tbt  can be described as a tmm-n pipeline with n > =  <dig>  accordingly, the tmm normalization method  <cit>  and the deges/tbt can be described as pipelines with n =  <dig> and  <dig>  respectively, and n can be specified by the iteration argument.

deges/edger
a major disadvantage of the tbt method is the long time it requires to calculate the normalization factors. this requirement is due to the empirical bayesian method implemented in the bayseq package. to alleviate this problem, a choice of alternative methods should be provided for step  <dig>  for instance, using the exact test  <cit>  in edger in step  <dig> enables the deges normalization pipeline to be much faster and entirely composed of functions provided by the edger package. the three-step deges normalization pipeline n pipeline with n =  <dig> or deges/edger, for convenience) can be performed by changing the test.method argument to “edger”.

the tbt pipeline automatically calculates the percentage of degs  by virtue of its use of bayseq. in contrast, a reasonable threshold for defining potential degs should also be provided when using the exact test in edger . here, we define the threshold as an arbitrary false discovery rate  with a floor value for pdeg. the default is fdr <  <dig>  , and the default floor pdeg value is 5% ; different choices are possible. for example, in the case of the default settings, x%  of the top-ranked potential degs are eliminated in step  <dig> if the percentage  of genes satisfying fdr <  <dig>  is over 5%.

although ideges/tbt would not be practical because of its long computation time, the iterative version of deges/edger  is potentially superior to both non-iterative deges/edger and deges/tbt. a suggested choice of n =  <dig>  consisting of seven steps, can be performed by changing the iteration argument  as follows:

 tcc<−calcnormfactorstcc,norm.method=”tmm”,test.method=”edger”,fdr= <dig> ,floorpdeg= <dig> ,iteration= <dig> 

the suggested number of iterations is determined from the results of ideges/tbt; it is a number at which the accuracies of deg identifications corresponding to the calculated normalization factors converge  <cit> . the number of iterations for ideges/edger is determined in the same way .

normalization of two-group count data without replicates
most r packages are designed primarily for analyzing data including biological replications because the biological variability has to be accurately estimated to avoid spurious de calls  <cit> . in fact, the functions for the deg identification method implemented in edger  do not allow one to perform an analysis without replicates, even though the tmm normalization method in the package can be used regardless of whether the data has replicates or not. although the edger manual provides users with some ideas on how to perform the de analysis, it is practically difficult to customize the analysis with deges to data without replicates.

however, there are still cases in which we have to perform de analysis of tag count data without replicates. when obtaining normalization factors from two-group count data without replicates, users can select from a total of four combinations  and a virtually arbitrary number of iterations  in our deges-based normalization pipeline. that is, the calcnormfactors function with the norm.method = “deseq” or “tmm” and test.method = “deseq” or “bayseq” can be selected. a pipeline n pipeline with n = 3) using functions in the deseq package for analyzing two-group count data without replicates can be performed by changing the two arguments  as follows:

 tcc<−calcnormfactorstcc,norm.method=”deseq”,test.method=”deseq”,fdr= <dig> ,floorpdeg= <dig> ,iteration= <dig> 

normalization of multi-group count data with replicates
many r packages  support de analysis for multi-group tag count data. tcc provides deges-based normalization methods for such data by virtue of the three packages that are internally used in tcc. similar to the analysis of two-group count data with replicates, users can select from a total of six combinations  and a virtually arbitrary number of iterations  when obtaining normalization factors from multi-group data with replicates.

retrieving normalized data
the calculated normalization factors can be obtained from tcc$norm.factors. similar functions are the calcnormfactors function in the edger package and the estimatesizefactors function in the deseq package, . note that the terminology used in deseq  is different from that used in edger  and ours. the effective library size in edger is calculated as the library size multiplied by the normalization factor. the size factors in the deseq package are comparable to the normalized effective library sizes, wherein the summary statistics for the effective library sizes are adjusted to one. our normalization factors, which can be obtained from tcc$norm.factors, have the same names as those in edger. since biologists are often interested in such information  <cit> , we provide the getnormalizeddata function for retrieving the normalized data. the normalized data can directly be used as, for example, the input data of another package noiseq <cit> .

differential expression
the goal of the analysis would be to obtain a list of degs. to this end, we provide the estimatede function. the function internally calls the corresponding functions implemented in three packages: exacttest in edger, nbinomtest in deseq, and getlikelihoods.nb in bayseq . if the user wants to perform the de method implemented in edger and to determine the genes having an fdr threshold of 10% as degs, one can do as follows:

 tcc<−estimatedetcc,test.method=”edger”,fdr= <dig>  

a similar analysis based on de methods in deseq or bayseq can be performed by changing the test.method parameter to “deseq” or “bayseq”. the results of the de analysis are stored in the tcc class object. the summary statistics for the top-ranked genes can be retrieved by using the getresult function. in general, the identified degs at fdr <  <dig>  should be up-regulated in either g <dig> or g <dig>  the plot function generates an m-a plot, where “m” indicates the log-ratio  and “a” indicates the log average read count /2) based on the normalized count data.

generation of simulation data
as demonstrated in our previous study  <cit> , the deges-based normalization methods implemented in tcc theoretically outperform the other normalization methods when the numbers of degs between groups are biased. however, the validation of the biased de in real data is very difficult in practice  <cit> . thus, the performance of methods handling biased de needs to be evaluated using simulation data. the simulatereadcounts function generates simulation data with various conditions. currently, this function can generate simulation data analyzed in the tbt paper  <cit> , thereby enabling further comparisons of our deges-based methods with methods developed by other researchers in the near future. for example, the hypodata object, a hypothetical count dataset in tcc, was generated by using this function with the following arguments:

 tcc<−simulatereadcountsngene= <dig> pdeg= <dig> ,deg.assign=c <dig> , <dig> ,deg.foldchange=c <dig> ,replicates=c <dig>  

the simulation conditions for comparing two groups  with biological replicates are as follows:  the number of genes is  <dig>  ,  the first 20% of genes are degs ,  the first 90% of the degs are up-regulated in g <dig> and the remaining 10% are up-regulated in g <dig> ),  the levels of de are four-fold in both groups ), and  there are a total of six samples  ). the empirical distribution of read counts is built from arabidopsis data in nbpseq <cit> .

the output of the simulatereadcounts function is stored in the tcc class object with information about the simulation conditions and is therefore ready-to-analyze. this function can generate several kinds of simulation data, such as those for comparing four groups  with replicates and those for comparing two groups without replicates. see the vignette for details.

RESULTS
accurate data normalization is essential for obtaining well-ranked gene lists from tag count data. similar to other r packages such as edger, the tcc package has functionalities for de analysis of tag count data. of these functionalities, tcc provides multi-step normalization methods  that internally use the functions implemented in edger, deseq, and bayseq. here, we demonstrate that the deges-based normalization methods are more effective than the methods implemented in the other packages. all analyses were performed using r  and bioconductor  <cit> . execution times were measured on a linux system , intel® xeon® e5- <dig>   <dig> cpu, and 512 gb memory). the versions of major r libraries were tcc ver.  <dig> . <dig>  edger ver.  <dig> . <dig>  deseq ver.  <dig> . <dig>  and bayseq ver.  <dig> . <dig> 

following our previous study  <cit> , we here demonstrate the performance of these methods by using the same evaluation metric, simulation framework, and real experimental datasets. we use the area under the receiver operating characteristic  curve  as a means of comparison. the simulation conditions are as follows: 5-25% of the genes are degs , 50–90% of the degs are up-regulated in g <dig> compared to g <dig> , and the levels of de are four-fold in both groups. the count dataset consists of  <dig>  unique small rnas  and a total of four arabidopsis thaliana leaf samples  knockout  samples)  <cit> . this dataset was originally analyzed with bayseq. the data has  <dig> provisional true de srnas , and all of the srnas can be regarded as up-regulated in the wild-type . this is because they uniquely match tasrna, which is produced by rdr <dig>  and they are down-regulated in rdr <dig> mutants. in addition to the rdr <dig> knockout dataset, we also analyze four other experimental datasets  obtained from the recount database  <cit> . the four datasets are used for evaluating the deges-based methods aimed at two-group count data with replicates.

simulation data with replicates 
we assessed the performance of a total of six normalization methods:  tmm,  deges/tbt,  deges/edger,  ideges/edger,  ideges/tdt n pipeline), and  ideges/deseq. the ranked gene lists were obtained using the individual deg identification methods in three packages , together with normalization factors calculated from each normalization method. accordingly, a total of  <dig> combinations  were evaluated. table  <dig> shows the average auc values of  <dig> trials between the ranked gene lists and the truth for various simulation conditions . while the n iterations for the three ideges-based methods roughly require an n-fold computation time, the improvement due to increasing the number of iterations plateaued around n =  <dig> when performing ideges/tbt  <cit> . therefore, we decided to show auc values for the three ideges-based methods with n =  <dig> 

average auc values  of  <dig> trials for each simulation condition are shown. simulation data contain a total of  <dig>  genes: pdeg% of genes is for degs, pg1% of pdeg in g <dig> is higher than in g <dig>  and each group has three biological replicates . a total of nine conditions  are shown. the highest auc value for each condition is in bold. auc values with asterisks indicate significant improvements  compared with deges/tbt. we used a bootstrap resampling size of  <dig>  in bayseq when performing the normalization  and  <dig>  when performing the deg identification after normalization .

deges/edger performed comparably to tbt, whereas ideges/edger outperformed the others, irrespective of the choice of deg identification method  after normalization. that is, ideges/edger followed by any deg identification method yyy  performed the best among the six normalization methods. these results demonstrate that the alternative deges approaches  implemented in tcc generally outperform the original deges approach . in other words, the use of the exact test  <cit>  implemented in edger is sufficient to determine the potential degs to be eliminated. advantageous characteristics for the exact test also revealed themselves after performing any normalization method: comparing the three deg identification methods , we see that the xxx-edger have the highest auc values. overall, ideges/edger-edger performed the best. it should be noted that, however, the auc values for the pipeline look very close to those of the original recommendation   <cit> , e.g.,  <dig> % for ideges/edger-edger and  <dig> % for deges/tbt-edger under one simulation condition of pdeg = 25% and pg1 = 70%.

although the auc values for the individual combinations are under  <dig> % of the standard deviation  and are statistically significant , some researchers may think the current recommendation  does not look compelling. we do not argue the fact that ideges/edger performs comparably to deges/tbt , regarding the absolute auc values . we rather want to emphasize that ideges/edger outperforms deges/tbt in terms of computation time . this table shows that ideges/edger takes roughly three-times longer than deges/edger but it is over  <dig> times faster than deges/tbt. in light of the absolute computation times , ideges/edger finishes execution within one minute. although the tmm normalization method has the shortest computation time, its auc values are clearly lower than those of the others, especially when the pg <dig> value is displaced from 50%. an evaluation based on sensitivity and specificity should take precedence over one based on the computation time. these results show that ideges/edger is good for analyzing two-group tag count data with replicates because of its sensitivity, specificity, and practical computation time.

average computation times  of  <dig> trials for the six normalization methods in table  <dig> are shown. the results of deges/tbt were obtained by using a suggested parameter setting for performing bootstrap resampling .

simulation data with replicates 
next, let us show the effect of iterations in the ideges approach to see whether the iteration can truly produce a convergent result or not. table  <dig> summarizes the results under three simulation conditions . of a total of  <dig> trials, ideges/edger yielded  <dig> convergent and  <dig> non-convergent  results. we got similar results for the three other ideges methods , irrespective of the simulation conditions. these results clearly indicate that the ideges-based normalization methods do not always produce convergent normalization factors, contrary to previous expectations  <cit> .

the numbers of convergent and non-convergent  results of  <dig> trials under pdeg = 20% are shown:  ideges/tbt,  ideges/edger,  ideges/tdt, and  ideges/deseq. we defined the trial as ‘convergent’ if potential degs estimated in the th iteration was the same as those in the th iteration and the number of iteration n required for obtaining the convergent result as nc . we defined the trial as ‘cyclic’ if potential degs estimated in the th iteration were the same as those in the ith iteration and the cycle as np .

of practical interest when using the ideges approach is the number of iterations required for obtaining a convergent result. we defined n as nc if the potential degs estimated at the th iteration are the same as those in the th iteration. the distribution of nc values for the four ideges-based methods are given in additional file  <dig>  in our trials, the maximum nc value was  <dig> . the distribution suggests that the ideges approach with n =  <dig> could be sufficient for obtaining convergent results under various simulation scenarios. however, the improvement had by ideges/edger with n > =  <dig> compared with that with n =  <dig> is actually negligible despite the requirement of additional computation time . therefore, the use of our ideges pipeline with n =  <dig> can be recommended for reducing useless computation time.

we observed that the number of iterations needed for obtaining convergent results  tends to increase when the degree of biased de is high : the average nc values for the ideges/edger under the three conditions of pg1 =  <dig>   <dig>  and 90% were  <dig> ,  <dig> , and  <dig> , respectively . this is reasonable because, in the case of the simulation with pg <dig> > > 50%, a relatively large number of n in the tmm-n pipeline is theoretically needed for obtaining accurate normalization factors and because, in the case of the simulation with pg1 = 50%, the theoretical pg <dig> value obtained from the potential degs in any ideges-based pipeline at n =  <dig> is 50% .

now, let us briefly discuss the non-convergent results. all the non-convergent results showed cyclic characteristics, that is, there exists a same set of potential degs estimated in both the ith and the th iterations within a trial . we found that the most frequent np value was  <dig> . for example, ideges/edger yielded  <dig> %  non-convergent results with np =  <dig>  this result indicates that two different sets of potential degs are alternately eliminated in the ith and th iterations. note that different normalization methods do not seem to produce consistent results  or non-convergent ) within a trial. under one simulation condition , for example, ideges/edger and ideges/tbt got  <dig> and  <dig> non-convergent results, respectively. of these, only three trials  showed the same non-convergent results .

we confirmed that the pdeg values  originally estimated by three methods  in every iteration in table  <dig> were above the predefined floor value of 5%, indicating that the floor value of 5% for pdeg has no effect on whether the results converge or not. the iterative method can be viewed as a discrete dynamical system since the number of state is finite and determined by the combination of genes as potential degs. oscillations in the trajectory are common phenomena in such dynamical systems. thus, a method based on an optimality criterion should be developed to select the best point in the cycle after detecting an oscillation. nevertheless, we observed an overall improvement regarding non-convergent results when more iterations were used.

simulation data without replicates
tcc also provides deges-based methods for normalizing two-group data without replicates. as described previously, the deg identification method  in edger does not allow for an analysis without replicates. accordingly, we evaluated a total of eight xxx-yyy combinations . here, the deseq-deseq combination indicates the original procedure in deseq. different from the results for the data with replicates, ideges/deseq  performed better than deges/tbt in this case . the same trend can be seen in the accuracies of the estimated degs: the accuracies calculated from bayseq  were clearly inferior to those from deseq, irrespective of the choice of normalization method . the advantageous characteristics of the nb test in deseq become apparent when we compare the two deg identification methods  for any normalization method xxx: the xxx-deseq combination outperforms the xxx-bayseq combination. these results indicate that the higher auc values of the ideges/deseq-deseq are primarily by virtue of a well-ranked gene list produced from the nb test and that constructing a model based on bootstrap resampling employed in bayseq is difficult in a non-replicate situation.

we observed that the numbers of potential degs satisfying fdr <  <dig>  in the ideges/deseq were nearly zero  in all of the simulations, although the pdeg values were 5–25%. this is reasonable because any attempt to work without replicates will lead to conclusions of very limited reliability: deseq employs a conservative approach to prevent spurious de calls. accordingly, a predefined floor pdeg value  was used; i.e., 5% of the top-ranked genes were not used when calculating the normalization factors in the ideges/deseq and ideges/tdt methods. these facts indicate that  ideges/deseq performs almost as well as the original normalization method in deseq when the floor pdeg value is decreased from the default ,  the methods perform equally well when the floor pdeg value of 0% for ideges/deseq is used, and  ideges/deseq  with a floor value of x% tends to work better when analyzing simulation data with the same pdeg value. note that we set the floor pdeg value to 5% in order to obtain certain differences between the ideges-based methods and the original deseq, but we should consider the appropriate choice of the parameter in the future. results of iterations in the ideges approach  when analyzing two-group data without replicates were given in additional file  <dig> 

real data 
here, we describe the results for a real dataset with replicates, i.e., {wt <dig>  wt2} vs. {ko <dig>  ko2}. the experiment was originally reported in ref.  <cit> . following the evaluation scheme in table  <dig>  we calculated the auc values by using the  <dig> combinations shown in table  <dig>  consistent with the results shown in table  <dig>  we found that the xxx-edger combination  was the best, and consequently, we recommend using this combination to analyze two-group data with replicates. although the ideges/edger-edger combination performed the best on simulation data with replicates , ideges/edger did not distinguish itself among the six xxx-edger combinations. note also that the auc value for ideges/edger with n =  <dig>  was inferior to that for ideges/edger with n =  <dig> , suggesting that the iterative approach had a negative impact on this tmm-n pipeline.

the auc values  for a total of  <dig> combinations with default setting  are shown.

to see the effect of iteration, we investigated the changes in the auc values for the four xxx-edger combinations  when the default floor pdeg value  was used . the figure shows that the values from the ideges/edger pipeline  has a cyclic characteristics with np =  <dig>  and the cycle starts at the 7th iteration . in contrast, the auc values from the other pipelines had convergent characteristics: nc =  <dig> for ideges/tbt,  <dig> for ideges/deseq, and  <dig> for ideges/tdt. interestingly, the pdeg values estimated by ideges/edger, ideges/tdt, and ideges/deseq were 0% in every iteration. because of that, the floor pdeg value  was employed instead of the original values. meanwhile, the estimated pdeg values for ideges/tbt were above 5% in every iteration . these results indicate that the floor value of 5% for pdeg has no effect on whether the results converge or not.

we observed that the compositions of the potential de srnas were identical when the auc values were the same . this was true even for cyclic results . the different auc values, in turn, were due to the difference in the normalization factors at each iteration and the different compositions caused the next normalization factors to be different. we found that the relatively large difference in the auc values among cycles for the non-convergent results  were due to  the paucity of variations among the  <dig> provisional true de srnas and  their low expression levels. of the  <dig> srnas, there were only  <dig> unique patterns of counts across the four samples . for example, there were  <dig> de srnas that had one tag count only in wt <dig>  i.e.,  and  <dig> de srnas that had one count only in wt <dig>  i.e., . these two count patterns occupied  <dig> % of the  <dig> de srnas. such low-count de srnas cannot be distinguished from other non-de srnas if both patterns are identical. indeed, we found that the positions for the low-count de srnas in the ranked gene list varied from iteration to iteration.

similar to the paucity of variations regarding the count vectors for the  <dig> de srnas, we found that out of a total of  <dig>  srnas, there were only  <dig>  unique patterns of srna counts. this indicates that many srnas displayed the same degrees of de, and therefore, their ranks  will vary if slight changes are made to the calculated normalization factors. indeed, we observed considerably different changes in the auc values when a higher floor pdeg value of 10% was used ; e.g., the results for ideges/tbt converged when a 5% floor was used, but they became cyclic when 10% was used . we also found that ideges/ tbt, ideges/edger, and ideges/tdt performed about the same when a 10% floor was used. these results indicate that the large difference between auc values in this dataset might be within the error range.

all results described above were obtained with the original raw count data, in accordance with the edger and bayseq design. however, we should note that the current procedure is different from one recommended in the tbt paper  <cit> , where the data was scaled to counts per million  in each sample when the deg identification method in bayseq  was executed. this recommendation was derived from a comparison of uncertain auc values between the raw count data and the cpm data of this dataset, and it is now questionable. accordingly, all the procedures implemented in the current tcc are based on the original raw count data.

real data 
lastly, let us show the results of four experimental datasets: three human rna-seq datasets  and one mouse dataset . these datasets were obtained from the recount database  <cit> . different from the rdr <dig> dataset and simulated datasets, we do not know the true degs for the four datasets, indicating that we cannot calculate the auc values. we therefore investigated the effect of iteration regarding the potential degs to be eliminated . consistent with the rdr <dig> result, we observed several cyclic  characteristics for the recount datasets .

it is important to evaluate the degree of impact for the cyclic results. in the gilad dataset, for example, we observed cyclic results for two deges pipelines: ideges/edger  and ideges/tbt . figure  <dig> shows the results of hierarchical clustering applied to a total of  <dig> ranked gene lists . two distinct clusters can be seen: each cluster  consists of  <dig> gene lists with the same deg identification methods after performing different normalization methods  and  has five cyclic results  followed by combination name”; e.g., “ideges/tbt-edger”). it is clear that the five cyclic results are quite similar to the eight other results. we also got similar results for another dataset . these results suggest that cyclic results are not of concern in practice.

CONCLUSIONS
the r package tcc provides users with a robust and accurate framework to perform de analysis of tag count data. tcc has an improved data normalization step, compared with existing packages such as edger. while the other normalization strategies assume that there is an approximately balanced proportion of degs between compared samples , our multi-step deges-based normalization methods are designed to deal with various scenarios : the internally used deg identification method eliminates the effects of biases, if any, of potential degs. our study demonstrated that the ideges/edger-edger combination can be recommended for analyzing two-group data with replicates  and that the ideges/deseq-deseq can be recommended for analyzing two-group data without replicates . the success of these methods primarily owes to the high scalability of the normalization and deg identification methods in the r packages used in tcc.

the functionality of tcc can be extended in many ways. first, the current study focuses on the analysis of two-group data, but some users may want to utilize the deges-based methods for data consisting of two or more groups. as can be seen in the vignette, some prototypes of deges-based pipelines for analyzing data in three or four groups have already been implemented. evaluations such as these and further improvement are our next tasks. second, the current approach of tcc is somewhat reminiscent of a microarray analysis of the count matrix for genes. we know that  one or more isoforms can be transcribed in a same gene region,  those transcripts may have distinct expression levels, and  it could lead to a de result at the transcript level but a non-de result at the gene level. to prevent this possible problem and fully utilize the resolution of rna-seq data, advanced r packages  have recently been developed. we believe that the use of those packages together with deges enables us to obtain a more reliable result, because the idea of deges can, of course, be applied to de analysis at both gene-level and transcript-level resolutions.

finally, the current deges-based normalization methods implemented in tcc only employ linear scaling normalization methods and statistical methods for identifying degs. this is because these scaling normalization methods do not change the shape of the original count distribution and the statistical methods for identifying degs in edger, deseq, and bayseq assume the model. technically speaking, we can construct many other pipelines consisting of, for example, a non-linear normalization method  and a deg identification method originally developed for microarray analysis . as we learned from the microarrays, there are suitable combinations of normalization methods and deg identification methods  <cit> : we speculate that a non-linear normalization method would be incompatible with a statistical de method. a critical evaluation of those pipelines will be of interest in the future and we will continue to offer up-to-date guidelines.

availability and requirements
project name: tcc

project home page:tcc is available at http://www.iu.a.u-tokyo.ac.jp/~kadota/tcc/ and will appear in bioconductor  from ver.  <dig> .

operating systems: platform independent

programming language: r

other requirements: requires the edger, deseq, bayseq, and roc packages

license: gpl-2

any restrictions to use by non-academics: none

abbreviations
de: differential expression; deg: differentially expressed gene; deges: deg elimination strategy; fdr: false discovery rate; cpm: counts per million ; srna: small rna; tasrna: tas locus-derived small rna; tmm: trimmed mean of m values ; tbt: the tmm-bayseq-tmm pipeline; tcc: tag count comparison.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
js improved the tcc package, performed the analysis, and drafted the paper. tn drafted, maintained the tcc package, and refined the paper. ks supervised the critical discussion and refined the paper. kk drafted the r scripts, refined the paper, and conducted this project. all the authors read and approved the final manuscript.

acknowledgments
the authors thank dr. tj hardcastle for providing the dataset used in the bayseq study. this study was supported by grants  from the japanese ministry of education, culture, sports, science and technology .

supplementary material
additional file 1
additional results for simulation data with replicates. sheet 1: standard deviations of auc values in table  <dig> are shown. legends are the same as given in table  <dig>  sheet 2: estimated values for pdeg and accuracies of the potential degs are shown. averages of  <dig> trials are shown. values for  tmm do not exist because it does not estimate potential degs. following the original tbt study, the pdeg value for deges/tbt was directly obtained from the posterior probability output of bayseq  and the accuracy was calculated on the basis of the estimated degs. the estimated pdeg values for the other deges-based methods were calculated as relative numbers of genes satisfying fdr <  <dig>  . the accuracy was calculated on the basis of the potential degs if the pdeg value was over 5% of the predefined floor pdeg value  and, otherwise, the accuracy of the 5% of top-ranked genes was calculated as the surrogate degs.

click here for file

 additional file 2
effect of iterations for simulation data with replicates. details of the results in table  <dig> are shown. sheet 1: the nc and np values are shown:  ideges/tbt,  ideges/edger,  ideges/tdt, and  ideges/deseq. sheet 2: raw results  or non-convergent ) for each of the  <dig> trials are shown. the number to the right of the c  indicates the nc  value. sheet 3: average auc values  for the ideges/edger-edger. auc values with n = 1– <dig> for  all trials,  trials only having convergent results, and  trials only having non-convergent results are shown.

click here for file

 additional file 3
results for simulation data without replicates. results of four normalization methods  deseq,  ideges/deseq,  ideges/tdt, and  deges/tbt) under each simulation condition are shown. sheet 1: average auc values  of  <dig> trials and the standard deviations from the xxx-deseq and the xxx-bayseq combinations are shown. legends are the same as in table  <dig>  sheet 2: estimated values for pdeg and accuracies of the potential degs are shown. averages of  <dig> trials are shown. values for  deseq do not exist because it does not estimate potential degs. other legends are the same as in additional file  <dig>  sheet 3: average computation times  of  <dig> trials for the four normalization methods are shown.

click here for file

 additional file 4
effect of iterations for simulation data without replicates. sheet 1: summary of convergent or non-convergent  results of  <dig> trials for three ideges-based normalization methods:  ideges/deseq,  ideges/tdt, and  ideges/tbt. sheet 2: raw results  or non-convergent ) for each of the  <dig> trials are shown. sheet 3: average auc values  for the ideges/deseq-deseq. auc values with n = 1– <dig> for  all trials,  trials only having convergent results, and  trials only having non-convergent results are shown. the characteristics of np =  <dig> for non-convergent results can be seen.

click here for file

 additional file 5
effect of iterations for real data. sheet 1: basic information for the four recount datasets as well as the rdr <dig> dataset are shown. the dataset names except for the rdr <dig> one are the same as those provided in the recount database. the numbers in the “non-zero counts” column indicate the numbers of genes  having non-zero counts for at least one of the compared groups. the numbers in the “unique patterns” column indicate the numbers of unique patterns among the genes having non-zero counts. sheet 2: results  or non-convergent ) of four ideges-based normalization methods when using floor pdeg value settings of  5% and  10% are shown. the number to the right of the c  indicates the nc  value. the analysis were performed using non-zero count data. note that the results of ideges/edger for the maqc dataset were not shown because the method cannot be applied to data without replicates.

click here for file

 additional file 6
dendrogram of average-linkage hierarchical clustering for a katz.mouse dataset. legends are basically the same as those in figure  <dig>  gene lists having iteration numbers on the right side correspond to the results of the ideges/tbt-yyy combinations: the ideges/tbt pipeline for this dataset converged after the fifth iteration . it can be seen that the ranked gene lists obtained from the same combination with different iteration numbers  are quite similar to the lists obtained from the other xxx-yyy combinations if yyy is the same.

click here for file
