BACKGROUND
genome sequence alignments are a priceless resource for finding functional elements  and charting evolutionary history  <cit> . many genome alignment algorithms have been developed, e.g. reviewed by  <cit> . all of these algorithms require selection of various mundane but critical parameters. in the most classic approach to alignment , these parameters include the scoring matrix and gap costs, which determine alignment scores, and thus which alignments are produced. this study aims to reveal the influence of these and other parameters, and to guide their selection for accurate genome alignment. specifically, we investigate the following six facets of genome alignment:

alignment score cutoff
in the classic alignment framework, it is necessary to choose an alignment score cutoff: low enough to find weak homologies, but high enough to avoid too many spurious alignments. a rational approach is to calculate the e-value--the expected number of alignments between two random sequences scoring above the cutoff--and choose a cutoff that has an acceptable e-value. surprisingly, this approach does not seem to be used for genome alignment . the authors of blastz tested their score cutoff by aligning two genomes after reversing, but not complementing, one of them  <cit> . homology between reversed and non-reversed dna is  impossible, so this is a good measure of the spurious alignment rate, but it is inconvenient to repeat it with each new pair of genomes.

we have calculated the e-values implicitly used for several alignments in the ucsc genome database  <cit>  . they vary between 5e- <dig>  and  <dig> . often, higher e-value thresholds are used for genome alignment than would commonly be used for database searches . this is reasonable because genome comparison produces many thousands of local alignments, and a few hundred or even a few thousand spurious alignments would only amount to a small fraction of these.

repeat-masking
there is a general awareness that repeat-masking is important for genome alignment, but the efficacy of repeat-masking methods has not been assessed in this context. "repeats" can be categorized into two types: simple  sequences such as atatatatat, and non-simple repeats such as alu elements. simple repeats cause spurious  alignments with high scores, but non-simple repeats do not, because e.g. every alu is genuinely homologous to every other alu. non-simple repeats cause a different problem: too many alignments. in pursuit of accurate  alignment, we focus on simple repeats.

many blast-like alignment tools have a capability known as "soft masking". this means that masking is applied for the first phase of the algorithm, when initial matches are found, but not for the second phase, when alignments are extended from the initial matches. this promises the best of both worlds: avoid purely repetitive alignments, but allow repeats within larger alignments.

scoring matrix
the scoring matrix specifies a score for aligning every kind of base with every other kind of base. the simplest scoring matrix, which is actually quite good for dna, is: + <dig> for all matches and - <dig> for all mismatches. given a set of trusted alignments, a scoring matrix is often derived using log likelihood ratios  <cit> . this is because, under simplifying independence assumptions, log likelihood ratio derived scores are theoretically optimal for discriminating between random and true alignments  <cit> . unfortunately, real pairs of homologous sequences vary greatly in composition, and even more in conservation level; which means that the optimal matrix varies as well. to deal with this, matrices are sometimes constructed from alignments with low percent-identity, under the assumption that high percent-identity alignments will be found anyway  <cit> . such matrices, however, will be worse at discriminating short alignments with high percent-identity from chance similarities  <cit> . another approach is to develop a small number of compromise matrices that cover a range of percent-identities close-to-optimally  <cit> . a deeper problem is that, while log likelihood scores are optimal at distinguishing true from chance similarities , they are not necessarily optimal for accurate base-level alignment. thus, although log likelihood ratios are useful to suggest features of scoring matrices, it is not self-evident that they will work best in practice. 

gap costs
effective gap costs for protein alignment have been studied empirically  <cit> , but not for dna alignment.

x-drop parameter
blast and similar methods, including blastz and last, have an important but rarely considered x-drop parameter.  when extending gapped alignments, these methods terminate the extension when the score drops by more than x below the maximum previously seen  <cit> . this serves two purposes: to reduce computation time, and to prevent spurious internal regions in alignments. without any x-drop criterion, maximum-score alignments can contain arbitrarily poor internal segments  <cit> . thus the x-drop parameter is not merely an algorithmic detail: it is one of the parameters that define what a good alignment is. 

base-level accuracy
sequence alignments are inherently uncertain  <cit> . this uncertainty has traditionally been disregarded, leading to errors in many kinds of inferences made from alignments  <cit> . fortunately, it is possible to quantify this uncertainty, by estimating an alignment probability for every possible pair of aligned bases  <cit> . we have made it easy to obtain these probabilities, by integrating their calculation into our alignment software, last. below, we confirm that they are useful for measuring reliability and controlling the sensitivity/specificity tradeoff at the level of aligned bases.

previous work
surprisingly, we could find only one publication that attempts to determine optimal score parameters for dna alignment using actual genome data: that of chiaromonte et al., which recommends the hoxd <dig> matrix  <cit> . this matrix has been adopted by several genome alignment tools  <cit> . that assessment, however, has several limitations: it used gapless alignment, it tested only nine scoring matrices, and it only used human-versus-mouse alignments. most importantly, there are flaws in its evaluation procedure . thus, to date, there has been no rigorous assessment of dna scoring schemes.

measuring alignment accuracy
it is notoriously difficult to measure the accuracy of genome alignments  <cit> . some previous studies have used simulated alignments . simulations are suitable for some tests, e.g. establishing the limits of various algorithms as in  <cit> , but not for assessing alignment parameters, because the optimal alignment parameters will depend on the simulation parameters. another study used mobile elements to assess noncoding dna alignments, arguing that: "alignment of human alu elements to any non-primate mammalian sequence is a false orthology prediction"  <cit> . this is not suitable for testing homology predictions, because, e.g. primate alus and rodent b <dig> elements both originate from retrotransposition of 7sl rna, and are thus homologous  <cit> . 

we solve this problem only in a limited way, by using as gold-standards partial genome alignments implied by multiple alignments of proteins and of structural rnas. thus we measure alignment accuracy only in the parts of genomes that encode these molecules, and our conclusions might not apply to other parts of genomes. our assessment is nevertheless useful. although protein- and rna-coding regions are thought to comprise only a small minority of large  genomes, these regions are the focus of many downstream studies that use genome alignments. moreover, small genomes  consist mostly of coding sequence, as do the alignable parts of large but distantly related genomes . finally, we submit that there is little justification for genome alignment parameters used up till now, and a limited assessment is better than none.

alignment accuracy can be measured at two levels: correctness of whole  alignments, or correctness of aligned bases. for genome alignment, the latter is arguably more relevant. this is because many downstream analyses, such as rna structure prediction or detection of positively selected sites, depend on the base-level accuracy of the alignments  <cit> . in such analyses, more information is obtained from long alignments than from short alignments, making it inappropriate to weight the correctness of long and short alignments equally. this stands in contrast to protein database searches , where residue-level accuracy is not always important, since we may wish to know only whether a protein is evolutionarily related to another protein. unfortunately, the classic theory of alignment statistics has been developed from the standpoint of database searches and alignment-level accuracy  <cit> .

this study does not address the problem of distinguishing orthologous from paralogous alignments. this important problem is very different from that of aligning homologous bases, and requires its own specialized methods.

in this study we empirically assess many parameter choices on whole genome alignments of several organisms. we were able to perform many thousands of genome alignments only by using our new alignment software, last. our results give a practical guide for choosing repeat masking strategy, substitution and gap costs, and x-drop parameter - along with empirical estimates of false and true positive rates. we find that the best parameters significantly outperform current standard practice, often decreasing false positives by a factor of two or more at the same true positive rate.

RESULTS
e-values and repeat-masking
we tested e-value calculations and repeat-masking by aligning two genomes after reversing  one of them. reversed genomes are convenient for estimating the rate of spurious alignments  <cit> , because the reversed genome has the same composition and sequence complexity as the actual genome, but has no homology to any real genome. figure  <dig> shows the results for five genome pairs and six repeat-masking methods  <cit> . for the bacterial genomes , the observed numbers of alignments closely follow the theoretical e-value distribution, even with no masking. for the other genomes, with no masking, there are many spurious alignments with significant e-values . since the alignment algorithm  does not guarantee to find all high-scoring alignments, these results are lower bounds. only one of the repeat-masking methods eliminates spurious alignments fairly effectively for all the genome pairs: trf  with non-standard parameters and hard masking . it works well for mammalian genomes too .

suppression of spurious alignments requires careful masking of tandem repeats. for example, figure  <dig> shows a high-scoring, spurious alignment that dustmasker fails to eliminate. it is caused by tandem repeats: the repeat unit from c. elegans has a chance similarity to a similar-length reversed repeat unit from c. brenneri. because these units are tandemly repeated, the total length of the similar sequence region is amplified - increasing the apparent statistical significance of the match. dustmasker and runnseg can detect short-period tandem repeats, but this example shows that it is important to detect longer-period repeats too.

we obtained the non-standard trf parameters by trial and error: we simply lowered trf's mismatch cost, gap cost, and score cutoff until it worked. it is likely that a more principled and effective repeat-masking method than this can be found.

in our tests, soft-masking fails to eliminate spurious alignments, but soft-masking is typically used for genome comparison, presumably out of reluctance to prevent any sequences from being aligned. table  <dig> shows the proportions of genomes masked by several methods: typically under 10% for trf. it might be argued that a certain level of spurious alignment is an acceptable price for using soft instead of hard masking. to examine this, figure  <dig> compares quantities of spurious versus total alignment for a convenient soft-masking procedure   <cit> . with an appropriate score cutoff, the amount of spurious alignment is generally less than 1% of the amount of total alignment. however, if the e-value calculation is not reliable, it becomes harder to choose a rational score cutoff. furthermore, it is not clear that tandem repeats can be meaningfully aligned even if they are known to be homologous. in summary, if our top priority is to avoid spurious alignments then we should use hard-masking, but if we are more concerned to align as much as possible then soft-masking may be appropriate.

in the center column, dustmasker was run with option "-level 16".

assessment of scoring schemes
here, we assess which score parameters give the most accurate alignments, with high sensitivity and specificity. in order to measure accuracy, we need "gold-standard" alignments. for gold-standards, we used genome alignments implied by multiple alignments of proteins in the treefam database and of structural rnas in the rfam database  <cit> . these gold-standards will not be perfectly correct, but they were constructed using considerably more information than pair-wise nucleotide similarity.

gg/hs rfam
gg/hs treefam
scoring matrices derived from rfam  or treefam  based gold standards for the indicated genome pairs are shown. species names abbreviated as: gg: g. gallus, hs: h. sapiens, tr: t. rubripes, at: a. thaliana, os: o. sativa, sc: s. cerevisiae, sp: s. pombe. additional file  <dig>  table s <dig> shows corresponding matrices for other genome pairs.

we tested alignment accuracy using  <dig> combinations of score parameters, which we denote in the format: match score: transition cost: transversion cost: gap existence cost: gap extension cost, e.g. "2:1:2:16:1". we also tested the blastz/ucsc scoring schemes: the hoxd <dig> and hoxd <dig> matrices with a gap existence cost of  <dig> and a gap extension cost of  <dig>  each scoring scheme was combined with five x-drop values, for a total of  <dig> parameter combinations. in all cases, we hard-masked all the genomes with trf, and aligned them with last using score cutoffs corresponding to an e-value of 1; an arbitrary choice. all genomes except the yeasts were additionally soft-masked with windowmasker, to avoid wasting time on non-simple repeats. all parameters and results are tabulated in additional file  <dig> 

our test results are shown in figure  <dig> . there are too many parameter combinations to show them all with distinct symbols, so we just highlighted a few interesting ones.  the tests using treefam as a gold-standard give very consistent results, even though we used genomes with different levels of similarity . the parameter combination marked with circles  gives an excellent balance of sensitivity and specificity, while hoxd55:400: <dig> is the worst of all those tested, and hoxd70:400: <dig> is mediocre.  the combination 1:1:1:7: <dig> is decent but conservative, and superior to 2:1:2:16: <dig> for the most closely-related genomes . the tests using rfam as a gold-standard give less consistent results, but some trends are still evident. for example, the combinations 3:3:4:24: <dig> and 4:4:5:24: <dig> often perform well. in general, good scoring schemes have slightly lower transition costs than transversion costs, but not so much lower as in the hoxd matrices. they also have high gap existence costs relative to the other scores, compared to the hoxd schemes.

the poor performance of hoxd55:400: <dig> is not surprising, since these parameters tend to produce alignments with large, random flanks  <cit> . this explains why it sometimes gives many false positives, but why does it sometimes give few true positives? this is because the score cutoff corresponding to an e-value of  <dig> is high, e.g. much higher than for hoxd70:400: <dig> . we note that many genome alignments in the ucsc database, including human/chicken and human/pufferfish, have been made using hoxd55:400: <dig>  our results suggest that those alignments could be improved by using different score parameters.

the hoxd <dig> matrix itself would no doubt perform better if combined with higher gap costs. indeed, the parameter combination 4:1:4:28: <dig>  which approximates hoxd <dig> with a large gap opening cost, produces more accurate alignments . this combination is still mediocre in most tests, however: the best results are obtained using higher transition costs.

to confirm that our results are not specific to last, we repeated the assessment for the yeast genomes using blastz . the results are very similar to those using last , supporting the generality of our conclusions. one difference is that the blastz alignments have somewhat fewer true positives and false positives. this might be because we ran last with a spaced seed that is sensitive to protein-coding sequence .  both blastz and last have algorithmic options that we did not explore, however, so we cannot draw general conclusions from this difference.

larger x-drop values are not always better
as the x-drop parameter increases, both true positives and false positives usually increase . this is as expected, since larger x-drop values make the algorithm try to extend alignments more aggressively. surprisingly, however, large x-drop values sometimes lead to fewer true positives and/or false-positives. for example, in several panels of figure  <dig>  the parameter combination 2:1:2:16: <dig> produces more true positives with x =  <dig>  than with x =  <dig> . the likely explanation is sketched in figure  <dig>  when x is large, the alignment can extend over a dissimilar region with large negative score, producing an alignment with a negative-scoring flank  <cit> . last discards such alignments, which can cause it to align fewer bases when x is larger. we do not know exactly how blastz works, but we make two empirical observations:  it never seems to align fewer bases when x increases;  it does sometimes produce alignments with negative scoring flanks. ncbi blast also exhibits the latter behavior   <cit> .

these observations caution against x-drop values much greater than the alignment score cutoff. above this value, the x-drop algorithm starts to merge alignments that can otherwise be found separately . using the alignment score cutoff as the x-drop value, often corresponds to a maximum gap size of about 50- <dig> , which is fortunately short enough that the gapped extension does not dominate last's run time .

cpu time measured for one core of a  <dig> ghz xeon e <dig> processor is shown. each alignment time is the median among  <dig> alignments.

problems with scoring matrix evaluation by chaining
our results seem to conflict with those of chiaromonte et al.  <cit> . in our tests the scoring scheme 1:1:1:7: <dig> performed rather well, whereas in their gapless tests 1:1:1:∞:∞ performed poorly. chiaromonte et al. used the following evaluation procedure: they aligned human and mouse sequences thought to have evolved without rearrangements, counted aligned bases in the maximal co-linear chain as "correct", and the rest as "incorrect". the human sequence was first masked using repeatmasker. we replicated their procedure, and confirmed that 1:1:1:∞:∞ performs worse than hoxd70:∞:∞ at all score cutoffs . however, most of the high-scoring "incorrect" matches are tandem repeats that were missed by repeatmasker. so we additionally applied trf to both sequences. this gives a mixed picture: 1:1:1:∞:∞ performs better than hoxd70:∞:∞ for high score cutoffs, but worse for low score cutoffs . the advantage of hoxd <dig> at low cutoffs comes mainly from its finding longer "correct" alignments, rather than more "correct" alignments . these extended alignments are not always truly correct: an example that is surely incorrect is shown in additional file  <dig>  figure s8d. even in less clear-cut cases, the extensions have weak similarity and thus uncertain homology. so the fundamental problem is that co-linear matches need not be correct. in summary, this evaluation procedure based on chaining is not reliable .

reliable pairings found by alignment probabilities
an obvious and useful application of alignment probabilities is to annotate alignments with column reliability estimates. these probabilities can also be used, however, to compute pair-wise alignments that are expected to contain more correct base pairs than standard maximum-score alignments. several methods have been proposed for making alignments using such probability estimates, including: centroid alignment  <cit> , mpd alignment  <cit> , and ama alignment  <cit> . all of these methods can benefit from an appropriate choice of alignment parameters. here we used γ-centroid alignments  <cit> , which maximize the expected value of: γ tp + tn, where tp is the number of true positive aligned bases, tn is the number of true negative aligned bases, and γ is a user defined parameter to control the tradeoff between sensitivity and specificity. this optimization function fits well with the criteria we used for evaluating alignment quality. when γ =  <dig> it is equivalent to centroid alignment  <cit> , and when γ = ∞ it is equivalent to the alignment method described in  <cit> . when γ ≤  <dig>  γ-centroid alignment is especially simple: it just aligns all pairs of bases that have alignment probability greater than 1/.

we attempted to find highly reliable aligned bases by performing γ-centroid alignment with γ = 1/ <dig>  this means that we aligned all pairs of bases whose estimated probability of aligning is greater than 90%  <cit> . figure  <dig> shows the change in true positive and false positive aligned bases, compared to standard maximum-score alignment . in general, true positives decrease only slightly, but false positives decrease greatly. thus, we do indeed obtain more reliable pairings, with only a modest sacrifice of sensitivity. interestingly, the effect is more dramatic for scoring schemes that perform badly with standard maximum-score alignment, such as the hoxd scoring schemes. therefore, the performance of the hoxd schemes catches up with, and sometimes surpasses, that of schemes such as 2:1:2:16: <dig> .

we also investigated centroid alignment . this means that we aligned all pairs of bases whose estimated probability of aligning is greater than 50%  <cit> . generally, sensitivity  changes very little compared to standard maximum-score alignment, while false positives sometimes decrease substantially . the improvement may not be sufficient to justify using an exotic alignment method.

discussion
taken together, these results allow us to align genomes with greater accuracy and better estimates of the error rates.

repeat masking
we have shown that standard e-value calculations predict the rate of spurious alignment quite accurately, if tandem repeats are carefully masked. it is interesting that masking tandem repeats is sufficient: this suggests that simple  sequences other than tandem repeats are rare. our trf masking protocol leaves room for improvement, e.g. it works less well than dustmasker for arabidopsis/rice . nevertheless, it suppressed all spurious alignments with e-value less than 10- <dig> : a powerful result.

score parameters
the scoring scheme 2:1:2:16: <dig> gives very accurate alignments of distantly-related protein-coding sequences, whereas 1:1:1:7: <dig> is a good, conservative all-round choice. the main difference from the ucsc hoxd schemes is a relatively larger gap existence cost. kent et al. used high gap existence costs when aligning nematode genomes  <cit> . lunter found high indel rates for human/mouse  <cit> , which could be taken to suggest that gap existence costs should be low, but with simulated data they also found that lowering gap costs to make the gap frequency of alignment match the true gap frequency did not help - "the number of gaps can be made to approximate the true indel count, but only at the expense of placing the gaps in the wrong positions"  <cit> . a secondary reason for poor performance of the hoxd schemes may be overly weak transition penalties compared to the matrices in table  <dig> 

since our assessment of score parameters is restricted to protein- and rna-coding regions, there remains much uncertainty about these parameters for genome alignment in general. we can at least say this: there is both statistical and now  empirical evidence that hoxd55:400: <dig> is a poor choice, and, ironically, there is little reason to reject the simplest possible  scoring matrix.

range of parameter search
the scoring schemes tested here use small integers, compared to the hoxd matrices. small integers have some practical advantages: they work better with several methods for calculating statistical parameters  <cit> , and they reduce the risk of computer overflow when comparing a genome to itself. besides, our ability to discriminate effective parameters does not justify more than one significant figure.

another constraint we applied to our parameter search is that all matches were given the same score. one might expect better performance by tuning the match scores to reflect a+t content. however, to be effective, this adjustment must consider not only the overall a+t content of the concerned genomes, but also the a+t content of the target homologous sequences. note that for many genomes the a+t content of the rfam sequences exceeds 50%, while the a+t content of the treefam sequences falls below the 50% level . thus for the genomes studied, we believe that it is a reasonable compromise to treat all matches equally. we leave for future work the consideration of score parameters for highly a+t skewed genomes, such as that of the malaria parasite plasmodium falciparum.

local versus global alignment
our conclusions concerning alignment parameters may not apply to global alignment, where a whole pre-defined sequence is forced to align.

some genome alignment methods have aspects of global alignment. blastz has an option to perform high-sensitivity local alignment between adjacent "anchoring" local alignments. some other methods force global alignment between such anchors: the motivation is presumably to increase sensitivity, but this may also decrease specificity, especially if dissimilar sequences are forced to align. another common approach is to find chains of co-linear local alignments: the hope may be to increase sensitivity by lowering the alignment score cutoff, while avoiding spurious alignments by the chaining requirement. all these approaches are intuitively reasonable, because they reflect a prior expectation of finding co-linear homologies. on the other hand, they obviously introduce a bias towards finding co-linear similarities, which might be best avoided when studying genome rearrangement history, for example. in summary, local alignment is a conservative approach with well-understood statistics, whereas the more global methods are more aggressive.

relevance to more complex alignment methods
this study focuses on classic smith-waterman type pair-wise alignment  and affine gap penalties. although more complex methods are available , widely used genome alignments are still based on classic alignment  <cit> , and so our results have practical relevance. in general, the more complex methods seem to differ from classic alignment in three ways:

i) they use explicit probabilistic models.

ii) they use probabilistic algorithms such as posterior decoding, rather than viterbi/maximum-score alignment.

iii) they use more intricate models with more parameters.

the first is not really a difference, because classic alignment is equivalent to viterbi decoding with a probabilistic model   <cit> . as for the second, previous studies have shown that probabilistic algorithms can be more accurate than maximum-score alignment  <cit> . in our results, a probabilistic algorithm  improved accuracy for bad score parameters such as hoxd55:400: <dig>  but the benefit is not so clear for good score parameters . this may be because the good parameters have high enough gap costs that there is not much uncertainty in the alignments. nevertheless, alignment probabilities are clearly useful for indicating the confidence that each pair of bases is homologous. finally, the benefit of intricate modeling is unclear: lunter et al. found only modest improvements, even though their test data consisted of simulations from their model  <cit> . they concluded that use of alignment probabilities is more important than model accuracy.

one limitation of these more complex alignment methods is that they seem to fit more easily with global than local alignment. in particular, it is unclear how to calculate e-values that discriminate  homologies from chance similarities. it is also not obvious how to adapt their parameter-training algorithms to local alignment: we tried to do this, with poor results . in fact, many of these methods use blastz alignments as a starting point, so they directly depend on accurate classic alignments. we expect that both simple and complex genome alignment methods will be useful in future, as is the case for protein alignment.

some sophisticated methods attempt not merely pair-wise but multiple genome alignment  <cit> . this is a much harder problem. the only comment we make is that multiple genome alignments are always built from pair-wise alignments in one way or another, and thus accurate pair-wise alignment is beneficial for accurate multiple alignment.

finally, parametric alignment  <cit>  has been suggested as a useful technique for genome alignment score parameter selection  <cit> . parametric alignment computes the set of maximum likelihood alignments obtained by all possible settings of the score parameters. more to the point, parametric alignment also provides a finite  set of parameter settings guaranteed to cover all alignments which can be optimal for any parameter setting, and thus can guide an efficient search for a parameter setting which maximizes some desired alignment quality. unfortunately, practical parametric alignment techniques have only been developed for global alignment. indeed dewey et al. first used a local alignment based technique to identify similar segments and only applied parametric alignment to the resulting segments. in any case, they did not make specific recommendations for appropriate parameter settings. a related approach, inverse parametric alignment  <cit> , finds parameters that cause given example alignments to have near-optimal scores. this approach was found to improve global alignment of proteins, but its efficacy for local alignment of dna has yet to be tested.

CONCLUSIONS
we have conducted the first large-scale assessment of repeat masking strategy and genome alignment parameters using real genomes - producing a practical guide to alignment parameters. we have tested a sufficient number of parameter combinations and genome pairs so that our results will be relevant to most genome alignment tasks. with our results, researchers will not only be able to produce more accurate alignments than with previous standard practice, but they will also have a much better idea of the reliability they can expect from such alignments.

