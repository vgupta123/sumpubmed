BACKGROUND
protein properties that are relevant to real-world applications are often difficult to manipulate using either of the current protein engineering paradigms  <cit> : structure-based protein design  <cit>  or directed evolution  <cit> . both methods have shortcomings and advantages that have been discussed and compared elsewhere  <cit> . chief amongst the limitations of both methods is the requirement for high throughput computational or physical tests to evaluate protein variants for suitability to a specific application. a common problem with both approaches is that frequently there are no high throughput tests for real applications. for example, there are no high throughput tests for measuring how well a protease will remove grass stains from jeans, how quickly an antibody will shrink a tumour, or how immunogenic a potential vaccine antigen will be. as a consequence, protein engineers are frequently forced to compromise. thus a structure-based approach in which the effects of large numbers of amino acid changes on the active site are calculated may require the protein engineer to consider only the affinity of an enzyme for its substrate and product while ignoring the effects that temperature and solvent conditions may have on the enzyme. similarly an empirical library based approach in which large numbers of randomly produced viral antigen variants are tested for activity may allow the protein engineer to measure their binding to antibodies already known to be neutralizing, but would prohibit direct measurement of the production of such antibodies in animals exposed to the antigens.

many non-biotechnological engineering endeavours pose similar challenges to those found in protein engineering: a large number of independent variables and cost-prohibitions against exhaustive search. such diverse tasks as fuel formulation, clinical trial design and chemical process optimization are solved using experimental designs to combine variables in specific ways, and regression analysis techniques to dissect out the contribution of each variable to the outcome  <cit> . the common goal in all these areas of optimization is to keep the total number of activity measurements small enough to allow complex functional tests that are directly relevant to the final application.

multivariate data analysis has been used to optimize small molecules and peptides for nearly a quarter of a century  <cit> . in their paper describing chemical synthesis of a gene in  <dig>  benner and colleagues suggested that systematic variation of amino acids could provide an understanding of the relationship between a protein's sequence and its function  <cit> . until recently, however, synthesis of specifically designed individual genes has been sufficiently difficult to effectively preclude the construction of designed gene sets and meaningful testing of analytical predictions. such efforts have thus been largely confined to the synthesis of very small numbers of discrete polynucleotide  <cit>  or protein variants  <cit> , or to the analysis of variants produced in a library  <cit> .

a synthetic biology approach to protein engineering has been enabled by recent advances in gene synthesis technology  <cit>  that permit cost-effective synthesis of individually specified gene sequences instead of relying on creation of libraries of variant sequences  <cit> . the feasibility of producing tens or hundreds of protein variants in which all amino acid changes are precisely specified allows the sequences and activities of these variants to be analyzed using multivariate regression and machine learning techniques adapted from optimization tasks found in other engineering disciplines.

we have tested this protein engineering approach by increasing the activity and heat stability of proteinase k. we selected  <dig> amino acid substitutions, then designed, synthesized and tested  <dig> genes containing combinations of these changes. we tested  <dig> different machine learning algorithms for their ability to identify the amino acid changes with a beneficial effect on proteinase k activity by using them to design new variants with improved combinations of substitutions. in  <dig> design cycles we synthesized genes encoding a total of  <dig> enzymes , some of which had  <dig> times higher activity than the wild type protein. all  <dig> algorithms produced enzyme designs that were substantially improved over wild type. the results show that machine learning models of protein sequence and activity combined with efficient gene synthesis can be valuable tools in engineering proteins with improved properties.

RESULTS
 <dig>  selection of proteinase k as a test system
to test machine learning-based protein engineering we chose to optimize proteinase k-catalyzed hydrolysis of the tetrapeptide n-succinyl-ala-ala-pro-leu p-nitroanilide following a heat-treatment of the enzyme. we selected this activity because it mimics a key characteristic of practical protein optimization; target activities frequently result from a combination of protein properties, in this case expression and post-translational processing in a heterologous host, catalytic activity and thermostability.

the gene encoding proteinase k from tritirachium album  <cit>  was re-synthesized with an e. coli codon bias  <cit>  and cloned into an arabinose-inducible e. coli expression vector. the nucleotide and amino acid sequences of this initial  proteinase k sequence are shown in additional file  <dig> 

 <dig>  engineering proteinase k: design methods
 <dig>  overview of the method
the protein engineering method described here involves the following steps.

i) selection of amino acid substitutions to incorporate into a target protein.

ii) design of protein variants containing different combinations of those substitutions.

iii) synthesis of genes encoding the protein variants.

iv) expression of the protein variants.

v) measuring the activity of the protein variants.

vi) analysis of protein variant sequences and activities to assess the contribution of each amino acid substitution.

vii) design of a new set of variants using the information from vi).

viii) iteration of steps iii) to vii).

these steps are described in detail for the engineering of proteinase k in the results sections noted in figure  <dig> 

 <dig>  selection of amino acid substitutions
we planned on synthesizing a total of less than  <dig> variants containing combinations of a limited set of amino acid substitutions. to define a search space that could be effectively explored by synthesizing such a small number of variants we chose to use twenty four amino acid substitutions within the ~ <dig> amino acid proteinase k: less than  <dig> % of the total number of single amino acid changes possible.

to select the substitutions, a set of serine proteases with >30% amino acid identity to proteinase k were identified by using the blast algorithm to search genbank. this search produced  <dig> groups of homologous sequences. group a contained the wild type and  <dig> close homologs . group b contained  <dig> more distant homologs . group c contained  <dig> homologs  that were either reported in the literature to be thermostable or were >90% identical to a known thermostable sequence. genbank accession numbers are provided in additional file  <dig> 

the homologs were aligned using clustalw <cit> , to identify the amino acids in each homolog that corresponded with the amino acid found in wild type proteinase k at each position. to increase the probability that at least some of the substitutions would increase activity, we selected  <dig> substitutions based on several different criteria  <cit> :  the substitution was reported in the literature to increase the stability of the serine protease subtilisin   <cit> ;  the amino acid occurred within the homolog set ;  the amino acid occurred within >70% of homologs from thermophilic organisms and within other homologs ;  the amino acid was found within the homolog set and the substitution from the wild type residue is favourable in the dayhoff substitution matrix   <cit> ;  principal component analysis of amino acids responsible for clustering of homologs from thermophilic organisms   <cit> ;  literature reports that the p5s substitution has a stabilizing effect in subtilisin  <cit>  and appearance of a p to s substitution in the closest proteinase k homolog ;  the change occurred in a close homolog that is also thermostable  and  a random mutation identified during synthesis of the wild type . this information is summarized in table 1

we emphasize that the method used for choosing amino acid substitutions is independent of the subsequent machine learning analysis. substitutions could be selected by any of the many available methods including analysis of protein structures  <cit>  or comparison of homologous sequences  <cit> . a more detailed review of combined methods for substitution selection has been published elsewhere <cit> .

 <dig>  design of variant set 1
in order to test the machine learning algorithms, we needed to obtain a set of variants with corresponding activity measurements. for the most accurate analysis each substitution should be approximately equally represented, and should occur with as many different substitutions  as possible. we encountered two somewhat related obstacles to creating such a variant set. firstly, all of the substitutions were previously untested, so we did not know how many would completely inactivate the enzyme. secondly we did not know how tolerant proteinase k would be to changes, that is how many amino acids we could change in a single variant and retain activity. the initial set of  <dig> variants was therefore designed in several stages as we obtained information about these parameters. the sequences of all variants synthesized are shown in additional file  <dig> 

i) first a set of  <dig> variants was designed with combinations of substitutions selected randomly but with the constraint that each of the  <dig> substitutions occurred  <dig> times and each variant contained  <dig> substitutions . of these  <dig> variants only one  was active after heat-treatment. to determine whether the low survival rate was because the substitutions destroyed all proteinase k activity, or because the substitutions primarily affected the heat sensitivity, we also measured the activity of all  <dig> variants without heating. under these less stringent conditions we found three additional variants that were active . eighteen of the  <dig> substitutions were present in  <dig> or more of these  <dig> variants with detectable proteinase k activity and thus did not completely inactivate the enzyme.

ii) to see which of the remaining  <dig> substitutions destroyed proteinase k activity, we synthesized variants containing the substitutions that had not occurred within an active variant . variants containing n95c, p97s, e138a, a236v and l299c were completely inactive, so these substitutions were eliminated from further designs.

iii) to obtain a larger number of active variants for modeling using machine learning we designed  <dig> variants by arbitrarily combining  <dig> substitutions that had appeared previously in active variants  and  <dig> variants by arbitrarily combining  <dig> substitutions that had appeared previously in active variants .

iv) finally we performed a manual analysis of the activity data from the first  <dig> variants, combining substitutions that occurred frequently within active variants .

 <dig>  testing variant set 1
to associate protein sequences with functions, we tested the ability of the proteinase k variants to hydrolyze n-succinyl-ala-ala-pro-leu p-nitroanilide. proteinase k variants were expressed in e. coli and purified over a ni-nta column. the purified proteins were heated to 68°c for  <dig> minutes, cooled and diluted into reaction buffer containing substrate. the activities measured are shown in figure  <dig> and in additional file  <dig>  which also show the activities of variants designed in two subsequent design cycles.

only  <dig> of the  <dig> enzymes in variant set  <dig> had detectable activities after heat treatment . as described in section  <dig> , we wished to analyze this dataset using machine learning algorithms to calculate the values of  <dig> parameters. using a dataset this sparse will cause inaccuracies for the machine learning algorithms. to increase the number of datapoints without increasing the number of sequences synthesized we also measured the activities of all of the enzymes in variant set  <dig> without the heating step. we reasoned that this would provide additional information, differentiating combinations of substitutions that eliminated enzyme activity entirely from those which were simply unable to confer thermostability. more than half  of the variants in set  <dig> were active without a heating step .

 <dig>  choice of machine learning algorithms and analysis of variant set 1
we wished to learn which amino acid substitutions increased activity and which were detrimental by analyzing the sequences and activities of the proteinase k variants. the initial dataset was rather sparse: only two variants in the initial set  had an activity exceeding that of the wild type, by  <dig> -fold and  <dig> -fold respectively, while  <dig> possessed less than 10% of the wild type activity. despite the generally low activities of the first set of variants, there was a range of activities that we analyzed by machine learning.

to do this we first eliminated five substitutions  because variants with any of these substitutions did not have any detectable activity . we then considered the reduced set of  <dig> substitutions, representing each variant as a  <dig> dimensional bit vector xi, where xi,j is  <dig> if there is a substitution in the variant at position j. we used a bit vector since only one possible amino acid substitution was used at each position. a test of a protein variant was encoded as a pair , where xi represents the variant and yi the activity measured for this variant.

we selected  <dig> different machine learning algorithms to analyze the data. we used  <dig> different algorithms because we had no way of knowing which, if any, would be suitable for analyzing protein sequences and activities. the algorithms differ in two main ways. first in the way in which they calculate the differences between the measured activity and the predicted activity , for example whether they use the square of the differences between measured and predicted activities , or whether they place more weight on differences between measured and predicted activities for the more active variants . second, the algorithms use different regularization functions, which determine for example whether preferred solutions use many small weights  or fewer large weights . the algorithms used were: ridge regression   <cit> ; least absolute shrinkage and selection operator   <cit> ); partial least square regression   <cit> ; support vector machine regression   <cit> ; linear programming support vector machine regression   <cit> ; linear programming boosting regression   <cit> ; matching loss regression   <cit> ; one-norm regularization matching-loss regression   <cit> . see additional file  <dig> for detailed descriptions of the algorithms.

each algorithm was used to build linear models of the sequence and activity by calculating a 20-dimensional weight vector w, where the activity of a variant xj is estimated as y˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaieaacuwf5bqegaacaaaa@2e3b@i =  + w <dig>  the weight wj is associated with the j-th substitution, providing a measure of the effect of the j-th substitution on proteinase k activity. the last weight w <dig> is an additive shift. the machine learning algorithms were used to select values for wj that resulted in the best correlation between the activities that had been measured for each variant and the activities predicted by the weight vectors w. to do this we created  <dig> subsamples of the training set  pairs used for a cycle of machine learning) by leaving out  <dig> randomly chosen variant sequences for each such subsample. for all  <dig> algorithms we calculated the mean value and the standard deviation of each substitution weight wj over the  <dig> subsamples of the training set.

 <dig>  design of variant set 2
one objective in designing a second variant set was to see whether variants based on the results of machine learning analysis had improved activity relative to the training set. we also wished to obtain additional data so that we could perform a second round of machine learning-based variant design, should the first round prove successful. variant set  <dig> was therefore designed in  <dig> parts.

initially we used each machine learning algorithm to select the sequence that it predicted would have the highest activity using the heated activity data from the first set . the effect of any substitution may depend upon the other substitutions with which it occurs . the fewer times that a substitution has been seen, the less accurately its average effect is known. we reasoned that a substitution should be seen at least three times to estimate a meaningful average effect . we therefore excluded substitutions that had occurred in active variants fewer than  <dig> times. to further reduce the chances of incorporating an apparently positive substitution that actually had a negative effect we only included those substitutions whose weights exceeded a threshold. we first normalized all our activities against the wild type, resulting in activity  <dig> for the wild type. we then chose the threshold as  <dig>  = 1/ <dig>  where  <dig> is the original number of weights . note that the final number of substitutions somewhat smaller . this led to the exclusion of m145f, s123a, e132a and v267i from variants 2- <dig> to 2– <dig>  in designing variant set  <dig>  we improved our design method, using the standard deviation for each substitution weight instead of an arbitrary threshold .

we designed a further  <dig> variants in set  <dig> to more thoroughly explore the search space close to already tested variants and thus to provide sufficient data for a further cycle of machine learning. six of these  were designed using each machine learning algorithm in turn to select the variant with the largest predicted activity based on the mean weight for each substitution. to ensure that we designed sequences that were different from the first six, we only allowed variants between  <dig> and  <dig> amino acid changes from any tested variant of set  <dig> or any variant already chosen for inclusion in set  <dig>  the lower bound of distance  <dig> assured that new and significantly different variants were chosen, and the upper bound of distance  <dig> limited the risk of encountering non-viable combinations.

the last  <dig> variants of set  <dig>  were designed in the same way as 2– <dig> to 2– <dig>  except that instead of using the activities after heating for the machine learning, we used the activities before heating. the reason for this was that only  <dig> of the  <dig> variants had detectable activities after heating, but  <dig> had detectable activities before heating. the unheated measurements thus provided a better dataset for machine learning and we reasoned that they would increase the diversity of designs of active proteinase k variants. this is discussed in more detail in section  <dig> . the  <dig>  threshold was not subtracted from the weights for designs h and i  and the four substitutions that were excluded from 2- <dig> through 2– <dig>  were included in 2– <dig> through 2– <dig> 

 <dig>  testing and machine learning analysis of variant set 2
the first set of machine learning-based designs significantly outperformed those based on a manual "expert" analysis in set  <dig>  thirteen of the twenty variants in set  <dig> were more active than wild-type proteinase k, with  <dig> more active than the most active variant from set  <dig> . encouraged by this result we performed a second cycle of machine learning.

the sequence and activity data from the first and second set of variants was combined and analyzed as before using each of  <dig> different machine learning algorithms to build linear models of the sequence and activity as described in section  <dig> . the mean weights and standard deviations calculated by each algorithm are shown in table  <dig>  and shown graphically for one algorithm  in figure 3a.

because we now had more sequence and activity data, we also performed a rudimentary test of regression models that consider epistatic interactions between the selected substitutions. we did this by asking whether models containing epistatic interactions would result in a better fit between the observed and predicted activities of the variants in the training set.

ideally we would like to know for each pair of positions  which of the  <dig> combinations of amino acids present at a and b maximize the activity. for  <dig> substitutions there are a total of  <dig> total possible pairs to consider . each pair consists of  <dig> possible states: both wild-type, both substituted and  <dig> possibilities in which only  <dig> is substituted. thus to perform one separate test for each possible combination in every pair would require  <dig> variants. to test each of these combinations in at least  <dig> different sequence contexts could require > <dig>  variants, which would be quite impractical. since computational resources are cheap we instead used the  <dig> subsamples of the training sets to "virtually" test which pairs of substitutions led to better predictions by our algorithms.

to do this we expanded one amino acid pair at a time into its four possible combinations and optimized the new weight vector. thus, for each of the  <dig> possible position pairs , we built a model from each of the  <dig> subsamples using one weight for each position except for the  pair for which we used  <dig> weights: one for each possible combination of the substitutions at position a and b. we computed the loss of the linear models on predicting activities of the  <dig> variants held out from each subsample  and averaged the loss over the  <dig> subsamples. if one or more pairs of substitutions improved the model  we fixed the pair that produced models with lowest mean loss and then repeated the process. each time we picked the pair of positions that produced the largest reduction in the average loss. we stopped expanding amino acid pairs when no further reduction of the mean loss occurred.

examples of weights calculated by considering amino acid pairs are shown in figure 3b. a comparison of figure 3a with 3b shows that the expansion of  <dig> amino acid pairs into  <dig> combinations produced a model that also modified the weights of the single substitutions. thus s123a goes from being less than  <dig> standard deviation above zero to more than  <dig> standard deviation above zero. one substitution  goes from being very positive to negative . this substitution was only represented twice in active variants in the training set, and both of these variants had very low activities . the consequences of these weights for variant design are discussed in sections  <dig>  and  <dig> .

 <dig>  design and testing of variant set 3
in designing variant set  <dig> we aimed to obtain further increases in proteinase k activity. we also wanted to test whether new variant designs were improved either by accounting for the general context dependence of a substitution, or by considering epistatic interactions. variant set  <dig> was therefore designed in  <dig> parts to answer these  <dig> questions.

our first two designs used only linear models. we selected one sequence for each algorithm by combining substitutions whose weights were calculated by that algorithm to be greater than zero . we selected a second sequence for each algorithm by combining substitutions whose weights were calculated by that algorithm to be at least  <dig> standard deviation greater than zero . values for the mean and standard deviations for substitution weights calculated by each method are shown in table  <dig> 

the third design used models that considered amino acid pairs. we selected one sequence for each algorithm by combining substitutions or substitution pairs whose weights were calculated by that algorithm to be at least  <dig> standard deviation greater than zero . when more than one pair had a positive substitution weight, we selected the pair with the highest value when the standard deviation was subtracted from the mean. thus in figure 3b we chose 337s, 355p over s337n, 355p and s337n, p355s although the weights for all three combinations were more than  <dig> standard deviation above zero. as for designs from the linear models, we only included a combination for a pair if that combination was present in the training set at least  <dig> times. thus in figure 3b we rejected i132v, 208k because it was present only twice, but instead chose 132ik, 208h which had a slightly higher value than 132i, 208k.

for every machine learning algorithm, the design that incorporated substitutions only when the mean substitution weights were at least  <dig> standard deviation above zero, outperformed the design that incorporated substitutions when the mean substitution weights were simply greater than zero. there was no clear pattern when epistatic models were used: the data is shown in figure  <dig> and discussed in more depth in section  <dig> .

 <dig>  analysis of the design methods
 <dig>  functional contributions of amino acid substitutions
ten of the initial set of  <dig> substitutions that we selected had a beneficial effect on proteinase k activity, a success rate of 40%. the substitutions were selected from a total of more than  <dig>  possible , by using alignments of homologous sequences without the use of any structural information.

the positions of all substitutions used are shown mapped onto the structure of proteinase k . we could see no obvious pattern distinguishing the locations of beneficial from detrimental substitutions, nor were we able to identify simple structural reasons for the effect of the substitutions.

for future extensions of this method, if a target activity is not achieved with the initial set of substitutions, additional substitutions can be chosen and incorporated into a new set of variants along with the best substitutions that have already been tested. results from previous experiments can improve a second cycle of substitution selection. for example, to obtain further improvements in proteinase k activity by incorporating new substitutions, we would pick more substitutions that appear in alignments of thermostable homologs and avoid those reported to confer stability on subtilisin. more data from other systems will be required to determine whether the best method for picking substitutions varies depending on the protein target or the desired application. in either case, using a variety of methods for the initial selection, then analyzing the functional contributions of substitutions selected by different methods is likely to provide a good starting point for other protein engineering projects.

 <dig>  representation of substitutions in the training data set
in our initial set of variant designs  we aimed to have each amino acid substitution represented more than  <dig> times. because so many of our random combinations of substitutions were inactive this resulted in a training set where different substitutions were represented very unevenly.

there are two major consequences of underrepresentation of a substitution for machine learning analysis. one can be seen in figure 3a, where the mr algorithm assigned y194s a high weight even though both variants in the training set have very low activity. because there are only  <dig> active variants encoding y194s, the machine learning algorithms tend to assign weights to the substitution that improve the fit of other substitutions to the model, but do not really reflect the contribution of the underrepresented substitution. the more the substitution is represented the less likely this is to occur because there are more datapoints that the weight has to be consistent with. table  <dig> also shows that this phenomenon is dependent on the machine learning algorithm used. two of the three algorithms that use one-norm regularization  and use fewer larger weights to fit the data give very low scores to y194s.

a second consideration that arises when a substitution is underrepresented is that it is difficult to assess effects of context upon the contribution of a substitution. sometimes a substitution may be beneficial with one set of other substitutions, but deleterious with a different set. the fewer times a substitution has been tested, the less likely such interactions are to be detected.

in this study we required that a substitution occur at least  <dig> times for us to use it in a subsequent design. for future designs of variant sets it will be important to ensure that each substitution is adequately represented in the training data set.

 <dig>  accounting for interactions between amino acid substitutions
the extent to which one substitution affects the contribution of another substitution to protein function is difficult to predict. for proteins that have evolved by the sequential accumulation of point mutations, most of those mutations must work well in many different contexts. this is because each new mutation will produce a new sequence context, so an enzyme whose amino acids were predominantly very context dependent would be largely immutable. this view is supported by a study in which all amino acid differences in  <dig> natural subtilisins were recombined by dna shuffling  <cit> . almost all possible pairwise combinations of amino acid differences were found in functional subtilisin enzymes produced by this recombination, suggesting that amino acid covariation seen in the original  <dig> orthologs resulted from common ancestral derivation rather than functional constraints. selecting amino acid substitutions that occur in natural homologs should therefore provide a useful bias towards variations that are tolerated in many contexts.

different subsamples of the training set produced different values for the weights of each substitution. this difference probably arises from noise in the data as well as from possible context effects. to accommodate this variation in our designs we used the standard deviation of each weight as a measure of its variability of effect. we compared the activities of third cycle variants designed by combining all substitutions whose mean weights were positive , with those designed by combining only substitutions whose mean weight was more than  <dig> standard deviation above zero . for every machine learning algorithm, the variant that contained only substitutions whose mean weights were at least  <dig> standard deviation above zero was more active than the corresponding variant that included all substitutions with positive weights. the standard deviation of a substitution weight thus appears to provide a useful evaluation of the likely contribution of that substitution to protein function.

as described in sections  <dig>  and  <dig>  we also tested designs based on regression models that considered epistatic amino acid interactions. activities of variants designed in this way are also shown in figure  <dig> . only the algorithms plsr and lpboostr produced more active designs based on modeling amino acid interactions than the corresponding designs produced when all substitutions were modeled independently. one of these, lpboostr, found the most active of all the  <dig> sequences we tested . however we note that different machine learning algorithms selected different amino acid pairs for expansion. it is therefore unclear to us whether these pairings are actually related to epistatic interactions between the amino acids themselves, or result from differences in the machine learning methods' ways of minimizing discrepancies between measured and predicted activities in a small and unevenly distributed dataset.

understanding interactions between specific pairs of amino acid substitutions is unlikely to limit the protein engineering method described here. to test every combination of all pairs of amino acid substitutions would rapidly become prohibitively expensive: for  <dig> substitutions it would require more than  <dig> variants . however it is relatively simple to instead select substitutions that work well in many contexts and to reject those that work well in some contexts but poorly in others. this can be done by using the standard deviation of the substitution weight over many subsamples of the training set, keeping only those whose mean weights are more than one standard deviation above zero. this will also ensure that if additional substitutions are incorporated subsequently, those substitutions already accepted and fixed are likely to be generally tolerant to further change.

 <dig>  comparison with other design methods
as a control to determine whether the same degree of activity improvement could be achieved by simpler means, we analyzed the activity distribution for  <dig> sets of variants. the first set, taken from the first  <dig> variants synthesized, comprised  <dig> variants which contained arbitrarily selected combinations of the  <dig> substitutions considered in the machine learning designs . the second set comprised  <dig> variants that were designed by our "expert" analysis of the sequence and activity data from the first  <dig> variants . the third and fourth sets comprised  <dig> and  <dig> variants designed using machine learning analysis . the activities of the randomly designed variants are predominantly extremely low: 80% are less than 3% of wild type activity and just one is more active than wild type. the activities for the variants designed by manual data analysis are a little more evenly distributed, but still only one is more active than wild type. by comparison 70% of the variants designed in the first cycle of machine learning were more active than wild type and all of the variants designed in the second cycle of machine learning were at least 3-fold more active than wild type. while it is not possible for us to compare machine learning with all available protein engineering methods, this control shows that machine learning identified highly functional combinations of substitutions that could not be readily obtained either by random selection or by manual analysis.

the machine learning designs, which resulted in enzyme activity increases of up to 20-fold, differed from classical experimental designs  because of the epistatic effect of some amino acid changes. amino acid changes or pairs of changes that completely eliminate protein activity will mask any positive or negative contributions made by other substitutions that occur with them. experimental designs such as taguchi matrices  <cit>  minimize the number of experiments by combining many variables at once. a taguchi orthogonal design for testing  <dig> substitutions would have produced  <dig> variants containing different combinations of  <dig> substitutions and  <dig> containing all  <dig>  our initial design incorporated only  <dig> changes into each of  <dig> variants. although this was a less complete testing of the combinations, because  <dig> substitutions abolished enzyme activity, we did obtain  <dig> variants with detectable activity. by synthesizing an additional  <dig> variants we were able to identify the  <dig> functional amino acids in our set of substitutions. by contrast the taguchi design would almost certainly have produced only inactive variants and thus no information.

machine learning has also been used in the related domain of drug design to search large libraries of small molecules for compounds with maximal activity towards a biological target. the activity levels of some compounds are known and "active learning" methods  <cit>  are used to select the next batch of compounds to be tested  <cit> . there are a number of significant differences between these searches and those in a protein engineering setting. for example small molecules are described by feature vectors of sizes between  <dig> and  <dig>  <cit> , while each protein variant in this study is described by  <dig> binary features. another difference is that in drug discovery large datasets are available for testing various machine learning methods  <cit>  while no such data exists for proteins. small molecule datasets are also generally quite large, typically with  <dig> to  <dig> compounds: fang et al estimate that training sets of  <dig>  member compounds are required to build a predictive model but in this study we tested a total of less than  <dig> variant proteins. in part this is possible because our protein descriptors are so much simpler and the relatedness of any pair of variants is unambiguous. this allowed us to identify improved proteins and then to focus on highly related proteins.

 <dig>  multiple protein properties are modified simultaneously
the activity of proteinase k that we targeted depends on activity towards the substrate and heat-stability of the protein. we were interested in knowing whether we had modified one or both of these properties. a second motivation was that we were unable to measure the concentration or proteinase k: it autodigested so efficiently that we were unable even to visualize it on a gel. since the half-life of the protein at 68°c should be essentially independent of the protein concentration, changes in half-life reflect changes in the protein itself and not possible influences of expression levels.

we measured the activity towards the substrate and the half-life at 68°c of  <dig> of the best variants. figure 6a shows the activity of wild type proteinase k following different exposures to 68°c, figure 6b shows one of the third cycle variants  after the same heating times. figure  <dig> shows the activity without heating  and the half-life  for wild type proteinase k and  <dig> third cycle variants, as well as the substitutions in each variant. with combinations drawn only from a small set of  <dig> selected substitutions, a significant diversity of functional combinations provided the desired outcome, from variants in which the primary effect was increasing overall activity  to those in which both activity and half-life were improved . we expect that this pattern would continue if we attempted to deconvolute further. for example, the increase in activity without heat treatment is probably a combination of increased specific activity and increased protein expression levels, with varying contributions from each activity in each variant. most importantly for the approach described here, several properties were altered simultaneously to improve an activity that depended on multiple properties.

 <dig>  different machine learning predictions from different data sets
different parts of variant set  <dig> were designed using different data sets . variants 2- <dig> to 2– <dig> were designed using the activity of the first set of protein variants after heat-treatment, while variants 2– <dig> to 2– <dig> used the activity without heat treatment. variants 2– <dig> to 2– <dig> contained new combinations of the substitutions incorporated into 2- <dig> to 2– <dig>  as well as  <dig> that were not included using only the heated data . five variants designed using the unheated data were more active than wild type, approximately the same proportion as those that were designed using the activities after heating .

although some variants designed using the unheated activities were active after heating, the activities without heating were in no way intended as a surrogate for the activity after heating. we used the unheated activities only to obtain a larger set of sequences, but did not measure the activities of variant sets  <dig> or  <dig> without heating. there are clearly combinations of substitutions that produce increased activity over wild type without heat treatment, but lower activity than wild type after heating . if we had performed several cycles of engineering using only the unheated activities, we would therefore expect only a subset to be active after heating.

 <dig>  differences in predictions of the machine learning algorithms
the different machine learning algorithms did not converge to the same sequence design. the eight algorithms produced  <dig> different variant designs  using the same training data set. these differences in turn arose from differences in calculated mean and standard deviations for the substitution weights , which themselves resulted from differences in the way in which the algorithms model the data. the activities of the variants designed using different machine learning algorithms were very comparable , and we were unable to really distinguish between them by their performances. the comparable performance of all the machine learning algorithms we used is probably due to the fact that we have too few example proteins. we expect that with more examples, clear differences between the algorithms could appear. testing this hypothesis will require analysis of additional datasets.

it is unclear from the activity data whether there is a single optimal sequence, although there are clearly many improved sequences. for example the two most thermostable variants, 3– <dig> and 3– <dig>  share  <dig> substitutions but differ at  <dig> positions . the substitutions i132v and l180i appear in 3– <dig> but not in 3– <dig>  addition of either of these  <dig> substitutions to 3– <dig> leads to variants with lower thermostability than either 3– <dig> or 3– <dig> . thus the effect of these two substitutions appears to be influenced by other changes in the protein. this context dependence of substitutions suggests a limitation for approaches such as site saturation mutagenesis  <cit> , in which all changes are considered independently and then combined based only on their behaviour in the wild-type context.

CONCLUSIONS
we have developed a new synthetic biology approach to protein engineering in which amino acid substitutions are selected, incorporated in different but defined combinations into a small number of variant enzymes which are individually synthesized and tested functionally. machine learning algorithms are then used to assign values to the functional contribution of each substitution, which serves as the basis for a further set of variant designs. the process is repeated until a target activity is achieved. we have tested the approach using proteinase k as a target protein.

substitutions that improved the activity of proteinase k were primarily identified using alignments of naturally occurring homologous proteins, structural information was not used. the exponential accumulation of natural dna sequences  <cit>  could facilitate the use of phylogenetic information for substitution selection in many other systems, helping to remove the prerequisite of obtaining high resolution crystal structures before initiating a protein engineering project.

we tested  <dig> different machine learning algorithms and found them all able to produce predictive models describing the contributions of individual amino acid substitutions to the activity of proteinase k. we also found that it was unnecessary to consider all possible amino acid interactions to obtain substantial improvements in protein activity. however, it was advantageous to use the machine learning models to identify  substitutions whose effect appeared to vary significantly depending on the sequence context.

by designing, synthesizing and testing a total of only  <dig> specific proteinase k variants, of which  <dig> were designed using machine learning algorithms, we obtained a 20-fold increase in protein activity. application of the strategy described here to other systems should allow proteins to be optimized using functional measurements for small numbers of protein variants. this would obviate the need for library construction and high throughput screening. instead, variants could be directly tested in complex low-throughput assays that accurately reflect the combination of properties desired for the final application of the optimized protein.

