BACKGROUND
massively parallel shotgun rna-sequencing  has become the technology of choice for transcriptome analysis because of its potential to yield extensive biological information with digital precision. the development of effective statistical data analysis methods has been essential to the utility of rna-seq and has been a focus since the original reports on the technology
 <cit> . the statistical analysis of rna-seq variability has been the focus of several comprehensive studies
 <cit>  and remains a topic of active investigation
 <cit> . a common task in rna-seq statistical analysis is to determine whether two rna-seq datasets are faithful replicates and, if not, whether two datasets differ only slightly or very markedly. sophisticated statisitical tools for analysis of next-generation sequencing data are beginning to appear, e.g. the edger package
 <cit>  and deseq package
 <cit> . here we focus on a more targeted approach that is useful in both quality control and early analysis. an ideal measure for this task should be easy to compute and have three features:  sensitivity: the measure is sensitive to actual differences;  calibration: there is a known baseline value that corresponds to success;  stability: the behavior is independent of the sequencing depth, total number of exons, and other experimental conditions not relevant to the question. simple computation is a desirable but not essential attribute.

pearson’s correlation coefficient r has been widely used to affirm that pairs of rna-seq datasets are faithful replicates
 <cit>  and continues to be in use
 <cit> . however, as a quasi standard in the rna-seq literature r may be problematic as it may suffer not only from the general pitfalls that have long been recognized  but from additional shortcomings specific to count data.

as an alternative, mcintyre et al. recently suggested a measure of concordance based on the kappa statistic to compare rna-seq samples
 <cit> . applying the kappa procedure to multiple rna-seq samples, the authors concluded that replicates of the same rna-seq library from different “lanes”  were subject to a systematic bias, a finding that appeared to contradict previous observations by others
 <cit> . similarly to pearson’s r, kappa may be subject to confounding factors of the experimental design such as the total read count. furthermore, interpreting kappa under the premise that it should be  <dig> for perfect replicates may be naive.

here we propose a new candidate statistic for rna-seq sample comparison based on the ratio of observed variation to what would be expected from an ideal poisson experiment. we show that the simple error ratio estimate , unlike r and kappa can be expected to be  <dig> for perfect replicates, only affected by the random sampling effect. we evaluated the  <dig> statistics on the above criteria  using original rna-seq data from rat neural tissue that contained multiple technical and biological replicates. from this we created ideal in silico replicates by randomly splitting an observed lane into  <dig> pseudo-lanes and also simulated various degrees of contamination. this allowed us to examine the behavior of the methods under known outcomes and revealed serious deficiencies in the correlation and concordance approach. finally the methods were compared on the actual datasets.

RESULTS
candidate statistical measures
pearson’s correlation coefficient r was the main competitor because of the ubiquity of its use in the current rna-seq literature. the kappa statistic was recently proposed as an alternative
 <cit> . this requires the counts to be binned. here, we used the same binnig as suggested in that paper. the sere statistic is a ratio of the observed standard deviation between replicates divided by the value that would be expected from an ideal experiment.

sensitivity experiment
figure
 <dig> shows results from constructed datasets representing two lanes from an ideal replication experiment , and pairs of lanes where one of the two has various amounts of contamination. the contaminating sample in this experiment was from a litter mate under the same experimental condition  in order to make detection purposefully difficult. each point of the figure represents the average of  <dig> independent realizations of the simulation experiment. the last point for each method represents a duplication, i.e., comparing a lane with itself .

for the correlation and concordance measures the value  <dig> is usually viewed as the “ideal”. this is only achieved for the duplication, a situation where the randomness inherent to the process of read sampling is not allowed and instead a greater than expected congruence between two sample pairs is forced, resulting in an extreme case of “underdispersion”. data from an actual ideal experiment  had on average correlation values of  <dig>  and concordance values of  <dig> . sere on the contrary yielded the expected baseline value of  <dig> for perfect in silico replicates  and detected contamination as early as 25%. marked differences appeared when contamination reached 50%. the sere measure also clearly marks the duplication comparison as unusual . the sensitivity of both the correlation and concordance measures is much lower, making it difficult to distinguish contaminated samples from the ideal experiment.

stability experiment
another characteristic, stability, interrogates whether the behavior of the underlying statistic is independent of ancillary aspects of the experiment; the obvious such factor in rna-seq is the sequencing depth. therefore, rna-seq perfect replicate datasets of different sizes were generated by drawing random reads from the universal read pool. we simulated two types of scenarios: in our first experiment  we decreased the number of reads in both lanes from  <dig> to  <dig>  million. pearson’s r fell markedly from  <dig>  to  <dig>  when the read counts of both datasets in a pair were reduced . kappa was equally sensitive to the total read count, decreasing from  <dig>  for perfect replicate pairs with  <dig> reads per sample down to  <dig>  for pairs with only  <dig> x <dig> reads. all datasets represented perfect replicates by definition as they were generated in silico by sampling from a common pool. therefore, low values of pearson’s r such as < <dig>  and kappa < <dig>  are not in all cases indicative of poor rna-seq experimental replication. sere was unaffected by the total count of rna-seq reads, remaining stable at  <dig> .

in our 2nd experiment  we kept the total read count of  <dig> in silico replicates constant, but varied the relative size of both samples, simulating multiplexed rna-samples, where both samples will not always yield the same number of reads. pearson’s r and the kappa statistic performed continuously worse, as the relative difference between the two perfect replicates became bigger, reaching values of approximately  <dig>  and  <dig>  respectively at the extremes . sere on the contrary stayed at a stable value of  <dig>  through all the scenarios. a minor increase of the confidence intervals could be observed as the relative sample size tended to the extremes, yet remaining between ±  <dig> .
 <dig>  but keeping the total read count for each pair constant while systematically varying the relative size of both datasets, e.g., 1x <dig> plus 9x <dig>   <dig> x <dig> plus  <dig> x <dig>   <dig> x <dig> plus  <dig> x <dig>  pearson’s r and kappa fell for unequal sample sizes demonstrating a maximum for equal sample sizes. sere remained stable at  <dig>  while the 99% ci of repeat measures was optimal  for equal sample sizes.

performance of the statistics on empirical data
to put the above findings into perspective, we studied the candidate statistics on an empirical dataset which included technical and biological replicates, as well as samples from different experimental conditions . figure
 <dig> shows the result for  <dig> lanes of data, consisting of  <dig> replicate lanes for each of the  <dig> “control” rats and  <dig> replicate lanes for each of the  <dig> “snl” rats. first the replicate lanes of each rat were compared , second the biological replicates  and third the animals belonging to different experimental groups . note that sere results in a single value for a set of lanes that are being compared, while the correlation and concordance measures apply only to pairs of lanes.

r and kappa were slightly lower for the biological replicates as compared to the technical replicates and further decreased when comparing the two experimental conditions. however, differences were small compared with those caused by total read counts  suggesting that both measures are poor candidates for detection of global alterations in practice. sere was highly sensitive to global differences, with scores of approximately  <dig>  for biological replicates and  <dig>  to  <dig>  for comparisons between different experimental conditions.

the sere statistic can also be computed pairwise. for the  <dig> technical replicates of “control 1” for instance, the overall ratio for the three lanes is  <dig> , with pairwise values of  <dig> ,  <dig> , and  <dig> . when the overall sere statistic for a set of lanes is large we can use these individual comparisons to further sort out which lane is the source of concern. a simple way to display this is to use sere to create a cluster map. figure
 <dig> shows the resulting dendrogram for the  <dig> lanes of data used in figure
 <dig>  the dendrogram clearly reflects to the experimental design by first distinguishing between the two conditions, then separating the biological replicates within an experimental configuration and finally grouping the technical replicates. the vertical axis of dendrograms is often left unlabeled since the values are on an arbitrary scale, but in this case they have a direct interpretation as “excess dispersion”. a more interesting example is shown in additional file
1: figure s <dig> where we applied sere on a drosophila melanogaster dataset  that was employed by mcintyre et al. interestingly, it revealed  <dig> distinct groups  although the  <dig> samples originated from the same rna-seq library suggesting a possible batch effect.

the drawbacks of r and kappa
this study was focused on a global approach that is useful in both quality control and early analysis of rna-seq experiments. therefore, an ideal measure for this task was defined to be easy to compute and have three features of sensitivity, calibration and stability. the sere measure does well, but the correlation and concordance have serious flaws. why?

deficiencies in the correlation coefficient have long been known. chambers et al.
 <cit>  for instance showed a panel of  <dig> graphs with very different patterns all with the same value of r. of most relevance here is that r can be dominated by values at the extremes of the data. count data for rna-seq is very skewed. for example, in the first lane of “control 1”,  <dig> % of the observed exons had a count of  <dig> or less out of  <dig> , <dig> million total uniquely mapped reads, while the highest had a count of  <dig>  even under a log-transformation, the largest few counts have in inordinate influence. further examination of the results underlying figures
 <dig> and
 <dig> shows that the value of r for the ideal samples is essentially determined by the range of the counts, which in turn is closely related to the total sequencing depth . this causes r to have both a varying target value  and high variability. even a low value  might result from an “ideal” experiment. at the same time, markedly different samples can yield correlation coefficients of >> <dig>  if the total read count is high. moreover, pearson’s r can be computed in several ways: on square root of the raw counts, on count data after addition of a pseudo-count and log-transformation or on rpkm data after adding a pseudo-count and log-transformation. the latter is the most commonly used normalization and therefore employed here. these different approaches can substantially change the outcome since they influence the distance between the extremes .

by categorizing the data into bins, as performed by the kappa statistic, one avoids the susceptibility to values on the extreme of the scale. however the choice of the bin sizes becomes the driving factor for this statistic. additional file
1: figure s <dig> demonstrates how the binning influences the result of kappa for one and the same dataset. if the bins are chosen as reported previously by mcintyre et al.  kappa is  <dig> . when the bins are chosen wider , the value for kappa raises to approximately  <dig> . by choosing very small bin sizes , the kappa value decreases to approximately  <dig> . we also computed a weighted kappa
 <cit>  employing the most common definition where disagreement is proportional to the distance from the diagonal. the result shown in additional file
1: figure s <dig> demonstrated the same characteristics of kappa shown for the unweighted procedure.

for the simulation study, we chose the unweighted kappa. we took the same bin sizes as proposed by mcintyre et al., which used  <dig> counts as the smallest bin. therefore, whenever the expression of an exon is so sparse that only a single read is detected among two or more samples  the exon will be scored as “off the diagonal” since it will fall into the bin “0” for one sample and in “1-10” for the other sample. the fraction of singletons in our in silico samples with  <dig> million umrs is  <dig> - <dig> %, which alone limits the kappa to a maximum of about  <dig> . the total fraction of singletons tended to decrease by increasing the total read count and the calculated kappa value rises as seen in figure
 <dig> 

computational simulation can be helpful in estimating the expected values for pearson’s r or kappa but need to take the specific experimental condition into account and cannot be generalized easily. therefore the use of r and kappa to investigate whether two rna-seq datasets are faithful replicates or subject to systematic differences or bias leads to ambiguity in most cases.

the simple error ratio estimate 
the third candidate statistic appears to be a useful measurement to identify global differences between rna-seq data by fulfilling the set criteria of a good measure. a primary reason is that it compares the observed variation to an expected value, and the latter accounts for the impact of varying read depth. it is easy to compute and satisfies our three primary criteria.

calibration
a “perfect” sere of  <dig> indicates that samples differ exactly as would be expected due to poisson variation. if rna-seq samples are truly different, this is identified by values >  <dig> . values below  <dig> are well interpretable and indicate “underdispersion,” e.g. through artefactual duplication of data. a value of  <dig> would constitute perfect identity, such as might occur from accidentally duplicating a file name. interestingly, detection of underdispersion has been important in detecting data falsification
 <cit> ; faithful randomness is difficult to fabricate.

sensitivity
a constructed replicate with 25% contamination was successfully indicated as overdispersed by sere. as soon as one dataset contains 50% of its reads from another biological replicate, the indication of overdispersion becomes even more obvious. thus, sere is a qualified measure to detect processing errors and other sources of variation.

stability
in rna-seq experiments the read counts per exon in a sample vary, either due to rareness of the exon within the sample or due to total number of reads. the expected variation between lanes for that exon also changes. because sere explicitly accounts for this, comparing observed to expected counts, it is largely unaffected by these changes, regardless of the sequencing depth. this was confirmed by  <dig> in silico simulations performed for various numbers of reads, where sere was  <dig> on average. however, each simulation is subject to variation and therefore will slightly deviate from  <dig> either in the direction of under-  or overdispersion . to characterize the range of this variation we calculated the confidence interval  for all the simulations. as seen in figure
 <dig>  the 99% ci was narrow, ranging from  <dig>  to  <dig>  regardless of the total read count.

as shown in figure
 <dig>  we can also use the measure to more finely dissect the variation found in an experiment. this is a useful extension to the quality control assessment. however, when comparing samples on a single gene level, e.g. multiple treatment groups, both the expected poisson variation exploited by sere and the biological variation between and within treatment groups play an important role, and methods that take both into account would be preferred for deeper inquiry . yet, sere remains useful as an initial diagnostic tool.

li et al. recently introduced the “irreproducible discovery rate”  as a measure of reproducibility
 <cit>  and demonstrated its utility for the analysis of a variety of sequencing-based high-throughput data types
 <cit> . we tested the idr on two scenarios considered in this study, namely a comparison of identical datasets and a comparison of perfect replicates. in the case of identical datasets, the change of correspondence curve generated by idr analysis indicated perfect correspondence  lacking an indicator that this is a case of extreme underdispersion indicative of an unexpected finding such as human error or mischief. in the case of perfect replicates  the change of correspondence curve became noticeably noisy when the total number of read counts in the simulation was small . the idr was developed for comparison of datasets with unknown or differing distribution types. it is therefore unsurprising that it appears to be less useful than sere for the detection of under- and overdispersion in the comparison of replicate datasets affected by poisson variation. idr was developed for situations where two experimental replicates or methods could reasonably expected to agree without anticipating perfection. the statistical concept underlying sere is unrelated to idr and was developed for situations where two experimental replicates could reasonably be expected to differ only due to stochasticity.

CONCLUSIONS
sere provides an efficient single-parameter statistical measure of reproducibility for rna-seq datasets. unlike two other measure currently in use, pearson’s correlation coefficient r and the concordance measure based on kappa statistic, sere is independent of typically varying experimental circumstances . the interpretation of sere is straightforward staying clear of the ambiguities resulting from misinterpretation of r and kappa. a sere of  <dig>  corresponds to the normal degree of dispersion resulting from the poisson variation of raw read counts. because sere is a measure of dispersion, its interpretation extends to two situations of practical importance: underdispersion indicative of for example data duplication and overdispersion usable as a global measure of the degree of alterations, which is agnostic to the relative importance of difference genomic regions because sere weights each observed read identically. sere may in principle be applicable to a broad range of read count datasets such as from chip-seq or for comparison of alternately processed read data such as the counts of a k-mer analysis. the present study suggests that sere has superior characteristics to previously used measures in the case of rna-seq.

