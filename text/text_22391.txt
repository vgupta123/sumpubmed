BACKGROUND
microarray gene expression data contains thousands of hundreds of genes . biologists are interested in identifying the expressed genes that correlate with a specific disease, or genes with strong interactions. the high dimensionality of microarray data is a challenge for computational analysis. feature selection by data mining may provide a solution because it can deal with high dimensional datasets  <cit> .

the goal of feature selection is to find the best subset with fewer dimensions, but that also contributes to higher prediction accuracy. this speeds up the execution time for the learning algorithms before data analysis as well as improving the prediction accuracy. a simplistic way of obtaining the optimal subset of features is to evaluate and compare all of the possible feature subsets and select the one that yields the highest prediction accuracy. however, as the number of features increases, the number of possible subsets also increases according to a geometrical progression. for example, using a dataset with  <dig> features, the number of all possible feature subsets is 21000 ≈  <dig>  ×  <dig> , which means that is virtually impossible to evaluate them in a reasonable time. even if the problem space is reduced from  <dig> to  <dig>  the number of subsets for evaluation is 2100 ≈  <dig>  ×  <dig> cases, which will still require a long computational time. therefore, it is practically impossible to calculate and compare all of the possible feature subsets because of the prohibitive computational cost.

various approaches have been proposed to deal with feature selection from high dimensional datasets  <cit> , which can be divided into two general categories: the filter approach and feature subset selection. in the filter approach, each feature is evaluated using a specific evaluation measure, such as correlation, entropy, and consistency, to choose the best n features for further classification analysis. frequency-spatial domain decomposition   <cit> , relief  <cit> , chi-squared  <cit> , and gain ratio  <cit>  are filter approaches. a feature selection algorithm based on a distance discriminant  can identify features that allow good class separability among classes in each feature. the relief algorithm randomly selects an instance and identifies its nearest neighbors, i.e., one from its own class and others from the other classes. the quality estimator is then updated for all of the attributes to assess how well the feature distinguishes the instance from its closest neighbors. chi-squared is a well-known discrete data hypothesis testing method used in statistics, which evaluates the correlation between two variables and determines whether they are independent or correlated. the gain ratio is defined as the ratio between the information gain and the intrinsic value. the features with a higher gain ratio are selected.

filter methods are effective in computational time, but they do not consider the interactions among the features. in particular, during gene expression data analysis, gene-gene interactions are an important issue that cannot be ignored. feature subset selection is a better approach to this analysis  <cit>  because it evaluates a set of features instead of each feature in a dataset. therefore, the interactions among features can be measured in a natural manner using this approach. an important issue during feature subset selection is how to choose a reasonable number of subsets from all the subsets of features. some heuristic methods have been proposed. thus, forward search  <cit>  starts from an empty set and sequentially adds the feature x that maximizes the evaluation value when combined with the previous feature subset that has already been selected. by contrast, backward elimination  <cit>  starts from the full set and sequentially removes the feature x that least reduces the evaluation value. hill climbing  <cit>  starts with a random attribute set and evaluates all of its neighbors and chooses the best. best first search  <cit>  is similar to forward search but it also chooses the best node from those that have already been evaluated and it is then evaluated. the selection of the best node is repeated approximately max.brackets times if no better node is found. minimum redundancy maximum relevance feature selection   <cit>  combines forward search with redundancy evaluation.

many feature  selection methods have been proposed and applied to microarray analysis  and medical image analysis  <cit> . feature subset selection is a better approach for gene expression data than the filter approach, but it does not evaluate whole subsets of features because of the computational cost involved. previous experimental results indicate that all pairs of two features can be evaluated within a reasonable time after appropriate preprocessing of all the features. thus, if the interactions between pairs of two features are known, the interactions can be measured based on the classification accuracy for a given pair of features. feature selection should be improved by applying this information in the filter method and feature subset selection approaches.

in the present study, we propose a method for improving the performance of feature selection algorithms using the pairwise classification accuracy results for two features by modifying previous feature selection algorithms. the results obtained in various experiments using microarray datasets confirmed that the proposed approach performance better than the original feature selection approach.

methods
before describing the proposed approach, we need to define some notations. the input of feature selection is a dataset ds, which has n features and class labels cl for instances of features in ds. we denote ds as the i-th feature in ds. the output of feature selection chosen is the subset of features in ds. from a practical point of view, chosen contains indexes of the selected features in ds. these notations are summarized as followsds: input dataset, which has n features

ds: ds for the i-th feature in ds

cl: set of class labels for instances of features in ds

chosen: subset of selected features in ds



figure 1a depicts the flow of the general feature selection process. the initial pre-filtering step removes highly irrelevant features according to feature evaluation and then extracts novel features by applying feature  selection algorithms. the quality of the derived feature subset is evaluated by classification algorithms, such as k-nearest neighbor  and support vector machine . figure 1b shows the flow of the proposed feature selection process. our aim is to use evaluation information for the  pair. evaluating the subsets of all features is impossible, but evaluating every  pair can be achieved within a reasonable amount of time. including this information in the original feature selection should improve the quality of feature selection. the evaluation measure for  is not fixed and we use the classification accuracy as an evaluation measure in this study. we created a pairwise classification table, combn, and modified the original feature selection algorithms to use the combn.fig.  <dig> general  vs. proposed  feature selection processes



in the experiments, each dataset contained about 12000– <dig> features. a mutual information test was performed for all of the features in a dataset and the best  <dig> features were chosen in the pre-filtering step. in the proposed method, the input dataset ds for feature selection is this pre-filtered dataset. the combn pairwise classification table contains the  vector set, where i, j are the index of features ds, ds and i ≠ j, and vij is the classification accuracy for ds and ds. various algorithms could be used to obtain the classification accuracy, but we employed a svm. the length  of the pairwise classification table is 1000c2 =  <dig> . figure  <dig> describes the pseudo-code used to derive combn.fig.  <dig> algorithm of creating pairwise classification table



after producing combn, four filter algorithms, two feature subset selection algorithms, and mrmr are modified so the pairwise classification table is used in the original algorithms. table  <dig> summarizes the modified feature selection algorithms.table  <dig> feature selection algorithms modified according to the proposed approach



the modification of the original feature selection algorithms is similar in most cases. therefore, we present the pseudo-code for three selected algorithms, where figs.  <dig>   <dig> and  <dig> show the pseudo-codes of the original and modified algorithms.fig.  <dig> algorithms of original and modified chi-squared

fig.  <dig> algorithms of original and modified forward search

fig.  <dig> algorithms of original and modified mrmr algorithms



figure  <dig> presents the chi-squared pseudo-code as an example for the filter method. the original chi-squared algorithm only calculates the chi-squared value between each feature ds and cl, and sorts the results in descending order. finally, it returns the sorted list of feature indexes, chosen. in the modified chi-squared algorithm, we also use chosen in the first step like the original method. we then pick the first feature index first_feature from chosen, which is stored in mchosen and removed from chosen . the next step is finding first_feature from combn. there may be multiple rows that match, so two features of matched rows are stored in mchosen and removed from chosen . this process is repeated until chosen is empty. as a result, the order of the feature index in mchosen is different from that in chosen. users then select the first m features from mchosen to use in the classification test. mchosen is expected to obtain better accuracy than chosen. the modified chi-squared algorithm considers the chi-squared evaluation value of each single feature and the interactions between pairs of features by referring to the pairwise classification information in combn.

the pseudo-codes of the original and modified forward search algorithm  are used to modify the feature subset selection methods. the original forward search first algorithm finds a single feature with the highest evaluation value based on the eval() function and adds it to chosen. in the second step, it repeatedly finds the next feature that can obtain the highest evaluation value together with the feature in chosen until no more features can increase the evaluation accuracy . various methods are available for implementing the eval() function, but we employ svm classification as an evaluation function. the modified algorithm finds the best two features from combn in the finding loop , whereas a single feature was searched from the feature list of ds in the original algorithm. this idea can be applied to other feature subset selection algorithms.

figure  <dig> summarizes the pseudo-code for the original and modified mrmr algorithms. mrmr adopts the forward search method and evaluates the redundancy between target features, but there is no breaking condition for finding the feature subset. therefore, it has characteristics of both the filter method and feature subset selection. furthermore, mrmr uses mutual information for feature evaluation, so we need to convert the data values in ds into discrete values if the data values are continuous. the pseudo-code in fig.  <dig> is similar to fig.  <dig>  however, the eval() function in fig.  <dig> is substituted by the mrmr() function and breaking conditions in fig.  <dig> are omitted .

after obtaining the selected feature subsets produced by several algorithms, a classification test was performed using svm and knn because they are recognized for their good performance. the leave-one-out cross-validation test was used to avoid the overfitting problem. the fselector package  <cit>  in r  was used to test the original feature selection algorithms. fsdd and mrmr are not supported by the fselector package, so they were implemented using r.

RESULTS
to compare the original and proposed feature selection algorithms, we used five microarray datasets from the gene expression omnibus  website , which provides accession ids for geo datasets. a brief description of the datasets is provided in table  <dig> table  <dig> descriptions of the datasets



tables  <dig>   <dig>   <dig>   <dig> and  <dig> and figs.  <dig>   <dig>   <dig>   <dig> and  <dig> show the experimental results obtained by the filter methods and mrmr to compare the classification accuracy of the original feature selection algorithms and proposed methods. the filter methods evaluate each feature and the user must select the best n features from the evaluation results. for most of the datasets and with various numbers of selected features, the proposed modified algorithms obtained higher classification accuracy than the original methods. in some cases for fsdd and relief, the original algorithms were marginally more accurate than the proposed methods with the knn test. the svm test always improved the classification accuracy, excluding one result obtained by relief. in general, the svm yielded greater improvements than knn, possibly because the pairwise classification table was produced by the svm, and thus the knn test might have made greater improvements if it was used instead. in general, the proposed method increased the classification accuracy by 2–11 % and it was most accurate when the number of features selected was  <dig> table  <dig> comparison of the classification accuracy using the original mrmr and the proposed method

 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 

orig original algorithm, modi proposed modified algorithm

values in the first column are presented as the number of features selected for the classification test and the others are presented as classification accuracy. the bold numbers denote the highest value of knn and svm of each column

 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 

orig original algorithm, modi proposed modified algorithm

values in the first column are presented as the number of features selected for the classification test and the others are presented as classification accuracy. the bold numbers denote the highest value of knn and svm of each column

 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 

orig original algorithm, modi proposed modified algorithm

values in the first column are presented as the number of features selected for the classification test and the others are presented as classification accuracy. the bold numbers denote the highest value of knn and svm of each column

 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 

orig original algorithm, modi proposed modified algorithm

values in the first column are presented as the number of features selected for the classification test and the others are presented as classification accuracy. the bold numbers denote the highest value of knn and svm of each column

 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 

orig original algorithm, modi proposed modified algorithm

values in the first column are presented as the number of features selected for the classification test and the others are presented as classification accuracy. the bold numbers denote the highest value of knn and svm of each column

fig.  <dig> comparison of maximum classification accuracy between original mrmr and proposed method. a knn classification b svm classification

fig.  <dig> comparison of maximum classification accuracy between original fsdd and proposed method. a knn classification b svm classification

fig.  <dig> comparison of maximum classification accuracy between original relief and proposed method. a knn classification b svm classification

fig.  <dig> comparison of maximum classification accuracy between original chi-squared and proposed method. a knn classification b svm classification

fig.  <dig> comparison of maximum classification accuracy between original gain ratio and proposed method. a knn classification b svm classification



tables  <dig> and  <dig>  and figs.  <dig> and  <dig> show the experimental results obtained by the feature subset selection algorithms. in the case of forward search , the svm test obtained a marginal improvement in the classification accuracy compared with the original method, whereas the knn test decreased the accuracy. the difference between knn and svm may have been due to the method employed for the preparation of the pairwise classification table. thus, if the eval() function in figs.  <dig> and  <dig> had been changed to knn, the results in fig.  <dig> would be different. the proposed method markedly improved the accuracy of the filter methods compared with feature subset selection. the filter methods only evaluate each feature and they do not consider interactions between features, whereas feature subset selection methods consider feature interactions. therefore, the proposed method performed well with the filter methods. the proposed method selected features with greater numbers than the original algorithms and improved the classification accuracy . in the case of forward search , the original algorithm did not reduce the number of features, whereas the proposed method reduced the initial  <dig> features by 90 %. the proposed method removed a large number of features, but the knn and svm tests improved the classification accuracy. thus, the proposed method has greater selective power than the original.table  <dig> comparison of the classification accuracy using the original forward search and the proposed method


orig original algorithm, modi proposed modified algorithm


orig original algorithm, modi proposed modified algorithm

fig.  <dig> comparison of maximum classification accuracy between original forward search and proposed method. a knn classification b svm classification

fig  <dig> comparison of maximum classification accuracy between original backward elimination and proposed method. a knn classification b svm classification



to evaluate the execution time for the proposed method, we tested the execution time using a personal computer equipped with an intel core i5- <dig> @  <dig>  ghz cpu,  <dig> gbyte main memory, and the windows  <dig>  operating system. the proposed method requires an extra step for the pairwise classification accuracy table and table  <dig> summarizes the computational time needed for this step. the average time was  <dig>  min. this step is performed only once for given datasets and it is not a great burden for the overall feature selection process. table  <dig> summarizes the computational time required by various algorithms using the gds <dig> dataset. the proposed modified algorithms were faster than the original algorithms in the case of relief, forward search, and mrmr, but slower for fsdd and chi-squared. in general, the proposed algorithms produced the results within a reasonable amount of time.table  <dig> execution time required to create the pairwise classification table for each dataset



discussion
the proposed algorithms are useful but their implementation may be a difficult task for users. thus, to facilitate further research, we have built an r package called “fspair” and posted it on the web site . this package includes executable codes, source codes, a user manual, usage examples, and a sample dataset. we have added three more classifiers, i.e., random forest, naive bayes, and neural network. we have also added multi-core parallelism to allow the rapid generation of pairwise classification tables. users are free to download this package and test the proposed feature selection methods using their own datasets.

next, we consider the application of the proposed methods to the solution of real problems. kurgan et al.  <cit>  proposed a method for cardiac diagnosis using single proton emission computed tomography  images, where they built the spectf dataset containing  <dig> features and  <dig> instances. each of the features contained values extracted from a specific region of interest. each of the patients  was classified according to two categories: normal and abnormal. they aimed to produce a good classifier for diagnosing the problem. the accuracy of their proposed clip <dig> algorithm was 77 %. we tried to find “marker features” that might be helpful for cardiac diagnosis. thus, using our fspair package and the original algorithms, we test different combinations of feature selection algorithms and classifiers, and table  <dig> summarizes the results obtained. using the spectf dataset, the results produced by the original and modified algorithms differed little because the dataset had a small number of features. however, the proposed algorithms selected a smaller numbers of features than the original algorithms, but their accuracy was similar. for example, the original algorithms had the best accuracy using mrmr and random forest with  <dig> features, whereas the modified algorithms had the best accuracy using fsdd and random forest with five features. thus, five features referred to as f21s, f17r, f20s, f3s, f13s, and f8s are highly informative features for cardiac diagnosis. we performed a bootstrap test using the five features from the dataset and a very good area under the receiver operating characteristic curve  score was obtained, as shown in fig.  <dig>  this suggests that the five features selected may be of practical value for future diagnosis.table  <dig> classification accuracy and number of features selected with five classifiers and six feature selection algorithms

 <dig> /15
 <dig> /05


fig.  <dig> roc analysis for new dataset that has five selected features



CONCLUSIONS
feature  selection has various applications in bioinformatics. however, the selection of a novel feature set from a huge numbers of features is a critical issue, which involves the evaluation of each feature, feature interaction, and redundancy in the features. in this study, we proposed a method that improves the quality of feature selection. using information about the interactions between two features is very helpful for enhancing the original feature selection algorithms. if the computational power increases in the future, then information about the interactions between three or more features in a given dataset could further improve the feature selection process. the generation of interaction information is another issue. in this study, we used the classification accuracy as an evaluation measure for interaction but the evaluation measure could be changed if the aim of feature selection is not classification. the proposed method does not include redundancy among its features. thus, the addition of a redundancy removal algorithm may yield better results and this will be explored in future research.

abbreviations
fsddfrequency-spatial domain decomposition

geogene expression omnibus

knnk-nearest neighbor

mrmrminimum redundancy maximum relevance

svmsupport vector machine

