BACKGROUND
modern high-throughput genomic datasets are exquisite in their detail. the comprehensive range of measurements contained therein not only ameliorates, at least to a degree, reliance on narrow and specific a priori hypotheses, but also makes possible an appreciation of genetic behaviour at its fullest – i.e. at the level of interconnected gene networks. in this sense, modern genomics opens the door to forms of biological knowledge and thinking which would be difficult to attain with traditional methods of experimental biology.

there are challenges to be met in going from the rich detail in the datasets to a systemic understanding of genes. in our view, these can be separated into two main stages – first the establishment of reliable units of evidence, second the discovery of what these might mean at a global, systemic level. to illustrate via an example, suppose a genome-wide gene expression dataset resulting from an experiment comparing cellular response to a particular treatment against a control group. the first stage is to establish a reliable and exhaustive list of genes that are differentially expressed under the two conditions. the second is to go from the list of individual genes to a functional understanding of gene pathways, activated as a result of the treatment. batch effects, the topic of this manuscript, belong to the first stage of challenges. they are a pervasive form of technical noise, which compromise individual measurements to varying degrees, and affects significantly the ability of analytical means used to identify those that vary between experimental conditions. batch effects are found in gene expression microarray  <cit> , sequencing  <cit> , dna methylation , copy number variation  and proteomic  datasets.

batch effects are structured patterns of distortion
high-throughput technologies in biology typically require a sequence of delicate and labour intensive procedures, involving a combination of reagents and specialist machinery, conducted under strictly controlled conditions. frequently, the volume and nature of the work means that the laboratory process is broken into ‘batches’ – each batch consisting of a certain number of replicates to process – performed over a number of days. batch effects consist of a series of structured patterns of measurement noise each of which permeates all replicates in a given processing batch, and which vary markedly from batch to batch. we describe batch distortion as being structured, because it has a spatial character – in the case of microarrays for example, it imprints upon the expression values of probesets depending on the location of their constituent probes  <cit> . a large number of probesets can have their values altered significantly by this kind of distortion, without it being reflected in measures that are not spatially sensitive. to illustrate the point, it is possible to distort the expression value of all probesets completely  without at all changing, say, the quantile distribution of probeset values. as such, quantile normalisation techniques such as rma  <cit>  would be of limited use in correcting batch effects  <cit> . a helpful visual metaphor may be to think of a dried watermark, formed by an unintended splash of brush water on a fresh painting. or rather, a printing machine with a software virus, which makes prints of paintings, produces a certain number of copies at a time, each set with the same ‘watermark’, and that watermark changes randomly from set to set. these ‘watermarks’ cannot be removed from a digital poster, simply by adjusting its mean or quantile intensities of red, green and blue. they can be altered, along with the unaffected parts of the painting and hence causing a ‘smearing’ effect, but not removed.

result of a stochastic interaction of process variables?
batches being processed in different laboratories, by different personnel, subtle ambient differences  in the same laboratory from one processing day to the next, and changes in reagents have been suggested and explored as the cause of batch effects  <cit> . evidence suggests batch effects are pervasive and persistent under best practice. indeed, in the studies we conducted  <cit> , all the above mentioned factors were well controlled – the same laboratory , the same operator, and the same re-agents. yet the data revealed significant batch effects, accounting for as high as 40 % of the variance in the data. leek et al.  <cit>  make the insightful observation that structured measurement noise such as batch effects are in fact not unique to high-dimensional genomic datasets , or other types of high-dimensional data , but also affect traditional ‘low-dimensional’ data where just a few measurements are involved. the distinction, they propose, is that batch effects are identifiable in high-dimensional datasets, but not so in traditional datasets and as such go unnoticed. if so, it may be useful to think of batch effects as stemming from a stochastic combination of many of the factors at play during laboratory processing of data capture equipment, which is not readily controllable or avoidable. a more achievable way of managing batch noise may be to dissociate it from the genuine biological signal component of the dataset, and remove it in an effective manner.

batch effects have a detrimental effect on the utility of datasets
in terms of scientific inference, batch effects are most problematic when they are aligned  with treatment effects. table 1a depicts one such example, an extreme yet not uncommon one, where each processing batch contains one type of treatment or experimental condition. the difference between a pair of treatments will be completely confounded by the typically larger difference between the two distinct patterns of batch distortion. an entire group of genes, invariant across the two experimental conditions yet with probesets altered differentially by batch distortion will appear to be differentially expressed  <cit> . moreover, these false positives may dominate those genes that are differentially expressed across the two experimental conditions, because they are likely to appear to have a larger difference in their expression levels. the common practice of selecting top differentially expressed genes for further analysis and exploration, as ranked by magnitude, may further exacerbate this problem – resulting in the exclusion of differentially expressed genes, in favour of false positives.table  <dig> separating samples into processing batches

t1r1 + b1
t1r2 + b1
t1r3 + b1
t1r4 + b1
t1r1 + b1
t2r1 + b1
t3r1 + b1
t4r1 + b1
b denotes batch effects, t is treatment and the subscript r is the replicate of that treatment. : in this design, each batch consists of one type of treatment. batch and treatments effects are completely confounded. when we attempt to measure the difference between two treatments, say t <dig> and t <dig>  what we are actually measuring is  + . moreover,  is typically likely to be much larger than . : this represents the optimal experimental design strategy, where all treatments are distributed equally across all batches. there is no confounding here, but differences between b <dig>  b <dig>  b <dig> and b <dig> artificially inflate within-treatment differences, and reduce the power of subsequent statistical tests



it is possible to avoid this issue by making batch and treatment effects orthogonal to one another via modified experimental and procedural design. table 1b depicts the optimal case, where the replicates of each and every treatment are distributed equally across the batches, avoiding any confounding between batch and treatment effects. the closer we come to this ideal design, the less the confounding effect. however, even with ideal experimental design and no confounding of batch and treatment effects, there remains a fundamental problem. differences between individual batch effects, bn in table 1b, will inflate within-treatment variances, diminishing the power of any between-treatment comparison tests. as a result genes that are actually differentially expressed between two experimental conditions will have their p-values elevated and will appear to be not differentially expressed . moreover, different probesets on a particular array are affected differently by batch effects, meaning that some genes will have their p-values altered a lot, some less so, and some not at all. this will distort the ranking of genes based on their p-value, also distorting the results of rank based false discovery correction methods such as benjamini-hochberg .

the ideal solution to batch effects is to completely dissociate batch noise from genuine biological signal in the dataset, remove all of batch noise and none of the biological signal. in practice, however, removing noise carries with it the risk of also removing biological signal. one fundamental reason for this is that the distinction between signal and noise components, if attainable, is likely to be probabilistic rather than absolute. if genuine biological variance is removed along with batch noise, within-group variances are then artificially deflated making genes that are not differentially expressed appear as though they are. if we had multiple batch correction methods to choose from, the score by which we measure their effectiveness would have two dimensions – how much of the batch noise they remove, and how much of the biological signal they preserve.

outline
in this paper we describe a novel method which dissociates and removes the batch noise component in a dataset, with the constraint that the associated risk of also removing genuine biological signal is quantified and kept to a fraction set by the end user. if we set our confidence limit to . <dig>  this would mean that the probability of some of what we remove not being batch effect but a feature of genuine biological signal is . <dig>  the method works by first separating the data into its principal components. it scans each principal component for variance arising out of batch noise – as manifest by clustering of scores belonging to the same batch – and removes any that is found up to a point where the risk of removing biological signal is no more than the tolerance level set by the user. as the principal components collectively explain all the variance to be found in the dataset, scanning and if necessary correcting each of them means that batch effects are found and corrected, irrespective of how big or small they may be with respect to other factors accounting for the data variance. the principal components after removal of batch noise are recombined and transformed back into the original dataset format, ready to be used for any downstream analysis tailored for the initial dataset, without necessitating any additional data processing. we call this new method harman, meaning  threshing yard where grain was separated from chaff in the days before industrialisation. harman has a precedent in and can be seen as a refinement of the work of alter and colleagues  <cit> , who transformed genome-wide expression data into principal components, and then removed some of them entirely which they inferred to be dominated by batch effects.

combat  <cit>  is a popular batch removal method, which has been shown to have the best overall performance in a recent comparative study  <cit>  of six approaches including . as such it makes for a good standard against which to compare any novel batch removal method. we compare the performance of harman with that of combat in the context of three distinct, publically available genome-wide gene expression datasets. two of these – an in vitro  <cit>  and an in vivo  <cit>  study – were generated in our laboratory. the third is the in vitro dataset used in combat’s development  <cit> . while all three are microarray datasets, it is important to note that both combat and harman would be applicable in correcting rna-seq datasets . we also use harman regularly to correct large methylation datasets.

the performance measures used in the study are the removal of  noise, and preservation of  signal. for the sake of objectivity, and in the absence of knowing categorically what is signal and what is noise, we use a third party batch noise quantification to evaluate the two methods, the “guided-pca” statistic developed by reese et al.,  <cit>  . guided-pca p-values can be used as a measure of the probability of batch effects being present in the dataset. as p-values are a continuous rather than discrete score, they provide a continuum against which the batch noise suppression of different methods or trade-off settings can be measured. the  proportional relationship between g-pca p-value and the magnitude of the batch effect as measured by g-pca is further demonstrated in the additional file  <dig>  we compute this for each of the three datasets before correction, and after correction by the two methods. against this metric, we measure what proportion of the raw data variance is preserved in the corrected datasets. a two-dimensional plot of the probability of batch effect existence and proportion of preserved variance post correction depicts the relative merit of the two batch effect removal methods .

RESULTS
figure 1a above shows the batch correction results for dataset  <dig>  and fig. 1b shows the pc plot for the first and second components. with a gpca p-value of . <dig>  the uncorrected dataset has a prevalent batch noise component, also evident in the pc plot. consistent with this, the most conservative harman setting with a confidence limit of . <dig> – which means correction stops when there is just 1 % chance that what is being removed may not be due to batch effects alone – results in a 32 % reduction in data variance. after correction by either method, p-value increases significantly suggesting the methods are capable of removing batch noise. the figure also reveals how the confidence limit for harman operates as a trade-off coefficient between noise rejection and signal preservation. as the threshold is decreased, noise rejection increases as reflected by the gpca p-value, and data variance decreases. the resulting harman points can be thought of as constituting a performance curve for the correction method – one can choose to be at different points on the curve depending on the trade-off coefficient, but nevetheless is constrained to be on the curve. the combat point on the graph is below this curve.fig.  <dig> 
a gpca p-value vs preserved data variance plot for dataset  <dig> , showing the scores for data before correction , and after correction by combat and harman batch effect removal methods. for harman, the fractions in the labels denote the adjustable confidence threshold  for batch noise removal. hn-. <dig> is highlighted as it may be the setting of choice for a typical dataset. on the vertical, the larger the p-value the lower the probability of batch noise presence as detected by gpca . raw data p-value of . <dig>  indicates a prevalent batch noise component in the uncorrected dataset. the figure shows that combat falls below the harman curve, indicating harman’s superiority in terms of removing batch noise and preserving biological signal in the dataset. b first and second pcs for dataset  <dig>  before correction. the four colours represent the four processing batches. the shapes represent seven distinct treatments. the clustering of batches indicate the presence of batch effects in the first and second pcs of the data



dataset  <dig> results are depicted in fig.  <dig>  at . <dig>  the gpca p-value for the uncorrected dataset is small enough to indicate the presence of batch effects, if not to the same extent as in dataset  <dig>  once again, this is consistent with the pc plot. figure 2b indicates a batch effect but not to the same extent as fig. 1b. accordingly, both batch effect correction methods result in higher proportions of preserved data variance when compared to dataset  <dig>  as with dataset  <dig> the gpca p-value increases significantly after correction by either method. for harman the confidence limit has the same trade-off characteristic between noise rejection and data variance preservation. the combat point falls below the harman curve.fig.  <dig> 
a gpca p-value vs preserved data variance plot for dataset  <dig> , showing the scores for data before correction , and after correction by combat and harman batch effect removal methods. for harman, the fractions in the labels denote the adjustable confidence threshold  for batch noise removal. hn-. <dig> is highlighted as it may be the setting of choice for a typical dataset. on the vertical axis, the larger the p-value the lower the probability of batch noise presence as detected by gpca . raw data p-value of . <dig>  indicates a batch noise component in the uncorrected dataset. the figure shows that combat falls below the harman curve, indicating harman’s superiority in terms of removing batch noise and preserving biological signal in the dataset. b first and second pcs for dataset  <dig>  before correction. the three colours represent the three processing batches. the shapes represent four distinct treatments. the clustering of batches  indicate the presence of batch effects in the first and second pcs of the data



dataset  <dig> shows , as with datasets  <dig> and  <dig>  that gpca p-value increases after correction by combat or harman, and that for the latter the confidence limit sets the trade-off between noise rejection and data variance preservation. it also produces some distinct results. the gpca p-value for the uncorrected data is . <dig>  which indicates that there is much less batch noise in data  <dig> than in the other two datasets, if any at all. indeed, fig. 3b indicates that treatment variability  is a larger source of data variance than batch effects in the first two principal components. harman  removes 17 % , compared to the 37 %  it removed from dataset  <dig>  combat removes 49 %  of the data variance, about the same proportion it removed from dataset  <dig>  which has the most prevalent batch effect of all datasets . furthermore, harman  matches combat’s gpca p-value of  <dig> while removing  <dig> percentage points less data variance.fig.  <dig> 
a gpca p-value vs preserved data variance plot for dataset  <dig> , showing the scores for data before correction , and after correction by combat and harman batch effect removal methods. for harman, the fractions in the labels denote the adjustable confidence threshold  for batch noise removal. hn-. <dig> is highlighted as it may be the setting of choice for a typical dataset. on the vertical axis, the larger the p-value the lower the probability of batch noise presence as detected by gpca . raw data p-value of . <dig>  indicates that the batch noise component in dataset  <dig> is not as predominant as datasets  <dig> and  <dig>  the worst case scenario for both methods is that there is no batch effect in the dataset and what they do remove is genuine biological signal. combat removes 49 %  of the data variance, which is about the same proportion it removed dataset  <dig> , which had the most prevalent batch effect . harman  removes 17 % , when it removed 37 %  from dataset  <dig>  hn- <dig> matches combat’s gpca p-value of  <dig> while removing  <dig> percentage points less data variance. b a plot of first and second pcs for dataset  <dig> before correction . the three colours represent the three processing batches. the shapes represent four distinct experimental conditions. the figure indicates that within-treatment variability is a larger source of data variance than batch effects in the top two principal components



given the unexpectedly high p-value for the raw data, it is worth exploring further. with harman, it is possible to dissociate the principal components in which it finds and removes batch effects, and whether these are in any way different for dataset  <dig> than the other datasets. table 2a below shows the amount of batch correction applied to the first  <dig> principle components for the three datasets. a score of  <dig> means there is no correction. the closer the number to  <dig>  the bigger the correction. the remaining principal components not included in the dataset show no or negligible batch correction. table 2b shows the proportion of overall data variance explained by each principal component.table  <dig> the varying nature of batch effects in the three datasets as detected by harman

 shows the ‘correction vector’ spanning the first eight principal components for the three datasets resulting from harman . no or negligible correction were detected for the remaining pcs. a score of  <dig> means no correction, whereas a score of  <dig> means maximum correction within the confines of harman.  shows the relative proportion of overall variance explained by each of the  for the three datasets



for datasets  <dig> and  <dig> the most of batch related variance is accounted for before the third principal component, which is typically the case given the relative size of batch noise compared to other sources of variation captured in the data. in the case of dataset  <dig>  there is some correction at the first principal component, none at the second, and the largest correction occurs at the third and fourth principal components. the plot of third and fourth principal components in fig.  <dig> shows a clear grouping of scores into processing batches, which suggests that what harman is identifying and removing as batch noise may indeed be so.fig.  <dig> a plot of third and fourth pcs for dataset  <dig> . the three colours represent the three processing batches. the shapes represent four distinct experimental conditions. the clustering of batches indicates the likely presence of batch effects in the third and fourth pcs of the data



variance in the first two principal components is mainly due to within-treatment variability rather than batch effects  as mentioned, and yet combat removes nearly half of the overall data variance. it may therefore be interesting to see how the first two pcs of the combat corrected data look. for a fair comparison, we do the same for harman at the lowest confidence setting, which maximises the amount of data variance removed. as fig. 5a shows harman brings the batches closer to one another by reducing batch means towards zero, but without changing the distribution of samples within them. combat, on the other hand, rearranges samples within batch , and in particular brings the outlying member of the “*” treatment group within about two thirds of the original distance from the remaining three samples in the batch. more broadly, fig.  <dig> displays the compressed nature of samples belonging to the same batch in combat corrected data  relative to harman . combat, in effect, seems to alter and partially remove the biological variance in the data along with removing batch effects. an analysis of variance also confirms this. while both methods drive variance attributable to batch effects to virtually zero , combat also removes 23 % of the variance attributable to treatment , and about 32 % attributable to within treatment variation . the analyses of datasets  <dig> and  <dig> also show loss of biological variance resulting from combat, but to a lesser extent than dataset  <dig> fig.  <dig> 
a a plot of first and second pcs for dataset  <dig>  after correction by harman . the three colours represent the three processing batches. the shapes represent four distinct experimental conditions. b a plot of first and second pcs for dataset  <dig>  after correction by combat. the three colours represent the three processing batches. the shapes represent four distinct experimental conditions



finally, considering all three datasets, harman, for a given confidence limit, has a tight range of gpca p-values. for example, for harman  p-values range between . <dig> and . <dig> across the three datasets. combat varies from . <dig> to  <dig> 

discussion
we developed harman, first and foremost, to tackle the double edged problem with batch effects – to optimise batch noise removal with the constraint that the risk of also removing genuine biological variance is quantified and kept to a sensible level determined by the user. we evaluated harman, comparing its performance as a batch noise removal method to that of combat. we chose combat as the benchmark, as it is overall the best performing one amongst the existing techniques  <cit> . we used three independent, publically available datasets for this purpose, two of them produced by our laboratory, and the third originally utilised by the developers of combat  <cit> .

first of all, gpca measure we used indicates that harman and combat perform their primary function – they remove batch noise. for all three datasets gpca p-value for batch effect existence increased markedly following batch removal by either method. the confidence limit for harman does operate as a trade-off coefficient between noise rejection and data variance preservation as expected. as the confidence limit decreased , gpca p-value went up and preserved data variance went down.

second, the data provide compelling evidence that harman on the whole may be the one with superior performance. at the outset, our expectation was that combat would fall somewhere on the curve formed by harman at different trade-off settings, except that this point may not always be the optimal one for any given application. as it turned out, for dataset  <dig> and dataset  <dig> combat fell below the performance curve of harman, meaning that there was always a trade-off setting for harman which results in better noise rejection and better signal preservation at the same time. in the case of dataset  <dig>  this was true for all trade-off settings. to put it in perspective, harman with an extremely cavalier confidence limit of . <dig>  not only displayed better noise rejection, but preserved more data variance than combat . at the conservative extreme, harman  which stops removing variance if there is just 1 % chance that it might also be removing genuine signal achieved better noise suppression  than combat while preserving  <dig> percentage points more data variance. at the typical trade-off setting of . <dig>  the value used in the actual studies  <cit> , harman returned 63 % data variance with a gpca p-value of . <dig> against combat’s 52 % and with a lower gpca p-value of . <dig> for dataset  <dig>  for dataset  <dig>  harman  returned 93 % data variance to combat’s 79 %, and had a higher gpca p-value .

its peculiarities notwithstanding, dataset  <dig> also provides evidence that harman’s performance may be superior. the gpca p-value for the raw data was . <dig>  significantly larger than those of dataset  <dig> and dataset  <dig>  interpreting this result as there not being a batch effect in dataset  <dig> is the worst possible scenario for both methods. it means that whatever the methods removed from the dataset was biological signal, not batch noise. combat preserved less data variance than harman for all confidence limit settings. harman  matched combat’s gpca p-value of  <dig> yet preserved  <dig> percentage points more data variance. the difference between harman  and combat was a sizable  <dig> percentage points.

fortunately for the two batch correction methods, and in particular harman, further exploration revealed that there may have been a batch noise component in dataset  <dig>  harman had identified that the noise component in dataset  <dig> was predominantly in the third and fourth principal components. a plot of the two principal components  showed clearly that samples cluster according to which batch they belong, providing at least subjective evidence that there was a batch noise component. it is unusual for third and fourth principal components to account for more batch noise than the first and second. as a general rule, and as a consequence of batch effects being typically the greatest source of variation in genomic datasets, the earlier the principal component the greater the proportion of batch noise explained. datasets  <dig> and  <dig> constitute typical examples of batch effects, in that first and second principal components account for the bulk of that data’s batch noise component.

this raises another pertinent point. it has been argued that pca based batch correction approaches do not work well if batch effects are not the greatest source of variation  <cit> . as exemplified by dataset  <dig>  harman investigates all principal components for batch effects, and is able to identify and remove them no matter what their relative size compared to other sources of variation.

a further exploration of dataset  <dig>  revealed that combat removed biological variance from the data in the process of removing batch effects. a visual comparison of fig. 5a and b reveals the within-batch compression combat causes. an analysis of variance confirmed that harman, in distinction to combat, removed only the variance attributable to batch effects without altering the biological  variance. removing treatment variance leads to an expected increase in false negatives in comparison tests, and removing within-treatment variance leads to an expected increase in false positives. we should also note that analysis of variance attributes all that is attributable to batch effects. this still makes analysis of variance a revealing metric to compare the two methods, when they are set to remove the entirety of the batch effect as identified by it. in the general case, however, it does not replace a metric like gpca, which is also sensitive to the underlying likelihood of any variance attributed to batch effects.

the final point we will discuss is harman’s consistency in achieving comparable noise suppression - signal preservation trade-offs across different datasets, which is of particular advantage when conducting meta-analyses and genomic data integration from several distinct datasets  <cit> . it would be possible to falsely infer differences between two equivalent datasets, just by being bullish in the removal of batch effects in one, and overly cautious in the other. the three datasets varied in the relative magnitude  and also nature  of their batch noise components. they also varied in the number and size of their processing batches. yet, after correction by harman , the resulting datasets had a tight range of gpca p-values, from  <dig>  to  <dig> . this is not accidental. what harman removes as batch noise is driven directly by a trade-off coefficient constraining it to approach, but not exceed, a set risk of overcorrection. furthermore this risk calculation is internally normalised for different batch numbers and sizes . combat on the other hand, resulted in a relatively wide range of gpca p-values, from  <dig>  to  <dig>  this difference in consistency between the two methods is similarly reflected in resultant preserved data variance post correction as a function of the level of batch noise in the raw data. dataset  <dig> had a much more prevalent batch noise component than dataset  <dig>  accordingly harman  removed 37 % variance from dataset  <dig> and 17 % from dataset  <dig>  settling for comparable gpca p-value scores . combat, on the other hand removed 48 % from dataset  <dig>  and yet 49 % from dataset  <dig>  producing quite different gpca p-value scores  in the process.

CONCLUSIONS
considering the issue of batch noise in its totality – the potential impact of its presence  as well as overcorrection, and the importance of being able to control the trade-off between batch noise rejection and signal preservation especially in relation to studies that span multiple datasets – it is reasonable to state that harman’s performance as explored in this study makes it the more effective approach to deal with batch effects in high-throughput genomic datasets. harman is flexible in terms of the data types it can process . given its mathematical underpinnings its potential use extends beyond genomic datasets. of practical significance, it is also able to work with datasets where batch compositions – i.e. the number of experimental conditions, and replicates they contain – are not necessarily the same. it is freely available online as an r package, as well as a compiled matlab package which does not require a matlab license to run.

