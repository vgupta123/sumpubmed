BACKGROUND
the u.s. food and drug administration's  critical path white paper ) identifies pharmacogenomics and toxicogenomics as a promising tool in advancing medical product development and personalized medicine, and the guidance for the industry on pharmacogenomic data submissions has been released ). however, standardization is much needed before microarrays – a core technology in pharmacogenomics and toxicogenomics – can be reliably applied in clinical practice and regulatory decision-making  <cit> . many commercial and in-house microarray platforms are in use, and a natural question is whether the results from different platforms are comparable and reliable  <cit> . as the u.s. fda is actively assessing the applicability of microarrays as a tool in pharmacogenomic and toxicogenomic studies, we are particularly interested in information regarding the reliability of microarray results and the cross-platform comparability of microarray technology. several studies that specifically address cross-platform comparability report mixed results  <cit> . receiving particular attention is the tan et al. study  <cit>  which compares the results from three commercial platforms  and finds strikingly low cross-platform concordance, i.e., only four of the  <dig> unique genes identified as significantly up- or down-regulated by the three platforms are in common. the results of tan's study are extensively cited in a recent report in science  <cit>  and quoted by other media ; they collectively portray a disturbingly negative picture regarding the cross-platform comparability and reliability of microarray technology.

the science report  <cit>  and the original article  <cit>  appear to convey the message that the observed poor cross-platform concordance is largely due to inherent technical differences among the various microarray platforms. however, cross-platform comparability depends on intra-platform consistency that, unfortunately, is not sufficiently achieved and addressed in tan's study  <cit> . as we know, many factors affect microarray data reproducibility and large differences in the quality of microarray data from different laboratories using the same platform exist  <cit> . therefore, it is important not to confuse the poor performance obtained in a particular study with that achievable by the technology. we believe that appropriately assessing the reliability of microarray results and the cross-platform comparability of microarray technology is essential towards the proper use of microarray data and their acceptance in a regulatory setting.

because tan et al.'s paper  <cit>  and the related science report  <cit>  have caused a lot of confusion to the microarray community, in this paper we set to closely re-examine the dataset of tan et al. to determine the exact causes of the widely cited poor cross-platform concordance. we describe an alternative analysis of tan's dataset with the intention to address several common issues related to cross-platform comparability studies such as intra-platform  consistency and the impact of different gene selection and data  filtering procedures. we demonstrate that the main reason for the lack of concordance among the three platforms from tan's study does not appear to be "because they were measuring different things"  <cit> , but instead appears to be more likely because the original data  <cit>  are of low intra-platform consistency and analyzed with a poor choice of methods. by analyzing the same dataset with a simple fold-change ranking and sam   <cit> , we found a much higher cross-platform concordance than tan et al.'s original analysis suggested.

we should point out that the purpose of our work is by no means a criticism of the study of tan et al. in fact, the approach by which the data were analyzed by tan et al. is statistically correct and widely used in microarray data analysis. the purpose of our work is to bring the issue on the assessment of the merits of statistical methods to the attention of statisticians and bioinformaticians while analyzing high-dimensional biological data such as microarray data  <cit> . only after the validity of the data analysis methods is established can the biological significance of microarray results be reliably trusted.

our results illustrate the need for establishing calibrated reference rna samples and "gold standard" datasets  to objectively assess the performance of various platforms and individual microarray laboratories. equally importantly, the merits of various data analysis procedures proposed for microarray data analysis must be rigorously assessed and validated before the regulatory utility of microarray data can be realized.

methods
dataset
the dataset, consisting of  <dig> genes commonly tiled across the three platforms based on matching of genbank accession numbers, is made publicly available by the original authors  <cit>  <cit> . briefly, differential gene expression in pancreatic panc- <dig> cells grown in a serum-rich medium  and  <dig> h following the removal of serum  is measured using three commercial microarray platforms, i.e., affymetrix , agilent , and amersham   <cit> . rna is isolated from three control-treatment pairs of biological replicates  of independently cultured cells. for the first biological replicate pair , the same rna preparations are run in triplicates on each platform, resulting in three pairs of technical replicates  that only account for the variability of microarray technology. therefore, for the one-color platforms , five hybridizations are conducted for the control samples and five hybridizations are done for the treatment samples. for the two-color platform , dye-swap replicates are conducted, resulting in a total of  <dig> hybridizations. more details can be found in the original article  <cit> .

for each platform, raw intensity data were logarithm  transformed and then averaged for genes with multiple representations on the microarray. the log ratio  data were calculated based on the difference in log intensities  between the two samples in a control-treatment pair. for the affymetrix and amersham platforms, the pairing of the control and treatment was conducted in such a way that it matched the pairing on the two-channel platform . lr data for the dye-swap pair were averaged for the agilent platform.

metrics for assessing data reproducibility
data reproducibility was assessed according to three metrics, i.e., log intensity correlation , log ratio correlation , and percentage of overlapping genes , where r <dig> is the squared pearson correlation coefficient. pog represents the number of genes common in two or more "significant" gene lists  divided by l, the number of genes in a gene list. unless indicated otherwise, in this study l was set to  <dig>  so that the total number of unique genes  identified by our analysis from the three platforms is close to that  shown in the venn diagram presented in the original article  <cit>  and the report in science  <cit> .

data  filtering
it has been suggested that expression data for genes marked with "present"  appear to be more reliable than those marked with "absent"   <cit> . without the "absent" call information from the dataset made available by tan et al., we adopted a data filtering procedure proposed by barczak et al.  <cit>  by excluding 50% of the genes with the lowest average intensity across all hybridizations on each platform, resulting in a subset of  <dig> genes . this subset of  <dig> genes is presumably more reliably detectable on all the three platforms, whereas data points with lower intensity would more likely reflect platform-dependent noise structures or cross-hybridization patterns instead of real information of biological significance. the reduced subset of  <dig> genes was subjected to the same procedures for data quality assessment and gene selection.

gene selection methods
three gene selection methods were applied for identifying differentially expressed genes between the two groups of samples:  fold-change ranking,  p-value ranking, and  sam  <cit> . for fold-change ranking, lr data were rank-ordered and an equal number of genes  were selected from each of the platforms or replicates being compared in order to avoid ambiguity in calculating concordance. the method of fold-change ranking applies to situations where two or more replicates  are being compared. however, both the p-value ranking and sam methods are applicable where there is a sufficient number of replicates. in this study, both p-value ranking and sam were only applied to select the same number of genes from each platform with the three biological replicate pairs , but not for the comparison of two replicate pairs. the p-values were calculated for each gene using a two-tailed student's t-test. in practice, the ranking was performed based on the t-statistic, which carries the information regarding the direction  of regulation. cross-platform concordance was measured as the overlap of genes identified from different platforms. most discussions in this study were based on results from fold-change ranking with a selected number of genes l =  <dig>  unless otherwise indicated. different numbers of genes were also selected by the three gene selection methods.

RESULTS
intra-platform technical reproducibility
the intra-platform technical reproducibility can and should be high, but appears to be low in tan's study  <cit> , particularly for the affymetrix platform. specifically, intensity correlation of technical replicates for the affymetrix data is low compared to data from others researchers  <cit>  and our collaborators. a direct consequence of low lir <dig>  is very low lrr <dig> : an average of  <dig>  and  <dig>  for before and after data filtering, respectively, corresponding to an average pog  of 13% and 51% , respectively . that is, when all  <dig> genes are considered, only about 13% of the genes are expected to be in common between any two pairs of affymetrix technical replicates, if  <dig> genes  are selected from each replicate. in contrast, the percentage of commonly identified genes from two pairs of technical replicates is expected to be around 51% when the analysis is limited to the subset of  <dig> highly expressed genes. figure  <dig> gives typical scatter plots showing the correlation of log intensity  and log ratio  data from the affymetrix platform that indicate a low intra-platform consistency, especially before data filtering. the low intra-platform consistency is much more apparent for data in the log ratio space . since a primary purpose of a microarray gene expression study is to detect the difference in expression levels , it is important to assess data consistency in the log ratio space  in addition to in the log intensity space .

technical reproducibility appears to be reasonable on the amersham platform: average lrr <dig> is  <dig>  and  <dig>  for the three pairs of technical replicates before and after data filtering, corresponding to a pog of 76% and 89%, respectively. for the agilent platform, technical replicate pairs t <dig> and t <dig> appear to be very similar, but markedly different from t <dig> . it is notable that the cy <dig> intensities for a subset of spots with lower intensities for one hybridization of the dye-swap pair of t <dig> are significantly different from those of t <dig> and t <dig> . the difference between t <dig> and t <dig> or t <dig> is much reduced after data filtering , largely owing to the removal of the outlying lower intensity spots in t <dig>  overall, average lrr <dig> on the agilent platform is  <dig>  and  <dig>  for the three pairs of technical replicates before and after data filtering, corresponding to a pog of 62% and 84%, respectively.

it is evident from figure  <dig> that intra-platform consistency of the affymetrix data from tan's study is much lower than that of the amersham and agilent platforms. a thorough evaluation of experimental procedures would be needed to better understand such poor performance of the affymetrix platform from tan's study.

intra-platform biological reproducibility
the intra-platform biological reproducibility appears to be low  for all three platforms. biological replicate pairs b <dig> and b <dig> appear to be quite similar in the agilent platform . b <dig>  however, which is represented by the average of the three pairs of technical replicates , appears to be quite different from b <dig> and b <dig>  with an average lrr <dig> of  <dig>  and  <dig> , and pog of 37% and 49%, respectively, for before and after data filtering. the difference between b <dig> and b <dig> or b <dig> on the amersham platform is also noticeable: with average lrr <dig> of  <dig>  and  <dig> , and pog of 44% and 54%, respectively, for before and after data filtering; whereas b <dig> and b <dig> shows a higher lrr <dig> of  <dig>  and  <dig> , and pog of 49% and 71% for before and after data filtering, respectively. because of the low technical reproducibility of the affymetrix data, it is not surprising that the biological reproducibility from the affymetrix platform is low: with average lrr <dig> of  <dig>  and  <dig> , and pog of 14% and 45% for before and after data filtering, respectively . one possible cause of the observed low biological reproducibility could be large experimental variations during the processes of cell culture and/or rna sample preparation.

impact of data  filtering
all  <dig> genes, regardless of their signal reliability, are used in tan's original analysis  <cit> . after adopting barczak et al.'s data filtering procedure  <cit>  by excluding 50% of the genes with the lowest average intensity on each platform, a subset of  <dig> genes having more reliable intensity measurement is obtained. as expected, a significant increase in both technical and biological reproducibility is observed . the impact of data filtering on data reproducibility is more apparent from figures 1b and 1d when log ratios from technical replicate pairs t <dig> and t <dig> on the affymetrix platform are compared. this simple data filtering procedure appears justifiable for cross-platform comparability studies, assuming that genes tiled on a microarray represent a random sampling of all the genes coded by a genome, and that only a  portion of the genes coded by the genome are expected to be expressed in a single cell type under any given biological condition; such is the case for the panc- <dig> cells investigated in tan's study  <cit> .

another subset consisting of  <dig> genes that showed intensity above the median on at least one platform was subjected to the same analyses discussed for the datasets of  <dig> and  <dig> genes. gene identification was also conducted individually on each platform using the 50% of genes above the median average intensity, and the concordance was then compared using the three significant gene lists. in both cases, the identified cross-platform concordance was somewhere between that of the 2009-gene and 537-gene datasets .

cross-platform comparability
for each platform, the lr values of the three pairs of biological replicates  were averaged gene-wise and rank-ordered, and a list of  <dig> genes  was identified. without data filtering,  <dig> genes were identified to be in common by sam . with data filtering,  <dig> to  <dig> genes were found in common between any two platforms , and  <dig> genes were in common to the three platforms, which identified a total of  <dig> unique genes . while the overlap of  <dig> out of  <dig> is still low, the cross-platform concordance is some 10-fold higher than suggested by tan's analysis . the higher concordance reported here is a direct consequence of the data analysis procedure that incorporates filtering out genes of less reliability, selecting genes based on fold-change ranking rather than by a p-value cutoff, and selecting gene lists of equal length for each platform and for each regulation direction.

impact of gene selection methods on cross-platform comparability
as increasingly advanced statistical methods have been proposed for identifying differentially expressed genes, the validity and reliability of the more simple and "conventional" gene selection method by fold-change cutoff have been frequently questioned  <cit> . to compare the aforementioned results based on fold-change ranking with more statistically "valid" methods, we also applied sam  <cit>  and p-value ranking to the filtered subset of  <dig> genes to select  <dig> genes  from the three pairs of biological replicates on each platform. for sam, the pog between any two platforms ranged from 48%  to 58% , and  <dig> genes were found in common to the three platforms . of the  <dig> genes,  <dig>  also appeared in the list of  <dig> genes selected solely based on fold-change ranking. furthermore,  <dig> genes were also selected from each platform solely based on p-value ranking of the t-tests on the three pairs of biological replicate pairs, and  <dig> of them were found in common to the three platforms. among the  <dig> genes,  <dig>  appeared in the list of  <dig> genes selected by fold-change ranking.

however, when the three gene selection methods  were applied to the dataset of  <dig> genes to select  <dig> genes from each platform , much lower cross-platform concordance was obtained : only  <dig>   <dig>  and  <dig> genes were found in common to the three platforms by using p-value ranking, fold-change ranking, and sam, respectively. the results indicate the importance of data  filtering in microarray data analysis and the larger impact of the choice of gene selection methods on cross-platform concordance when the noise level is higher.

it is important to note that in both cases , p-value ranking yielded the lowest cross-platform concordance . one explanation is that the p-value ranking method selected many genes with outstanding "statistical" significance but a very small fold change. such a small fold change from one platform may be by chance or due to platform-dependent systematic noise structures . thus, such a small fold change is unlikely to be reliably detectable on other platforms, leading to low cross-platform concordance. for example, the gene  ranked as the most significant in up-regulation from the affymetrix platform, exhibited a very "reproducible" log ratio measurement for the three biological replicate pairs . the p-value of the two-tailed student t-test was  <dig> , representing the most statistically significant gene from the affymetrix platform. however, the average log ratio of  <dig>  corresponds to a fold change of merely  <dig>  . such a small fold change is generally regarded as questionable by microarray technology currently available. on the amersham platform, log ratios for the three replicates were - <dig> ,  <dig> , and  <dig> , with a mean of - <dig>  , standard deviation of  <dig> , and p =  <dig> . on the agilent platform, log ratios for the three replicates were - <dig> ,  <dig> , and  <dig> , with a mean of  <dig>  , standard deviation of  <dig> , and p =  <dig> . in terms of p-value, this gene  was ranked as # <dig> and # <dig> out of  <dig> genes on the amersham and agilent platforms, respectively; neither of these two platforms selected this gene as significant. when fold-change and sam were applied for ranking genes based on the same affymetrix data, the ranking of this gene was very low . obviously, this gene was not selected by fold-change ranking owing to its small fold change .

although fold-change ranking showed reasonable performance in terms of cross-platform concordance when applied to the subset of  <dig> genes, it is susceptible to selecting genes with a large fold change and large variability when the dataset is of low reproducibility, as is the case for the dataset with all  <dig> genes. for example, one gene  was ranked as the 11th largest fold change in up-regulation on the affymetrix platform, but was only ranked in the top  <dig> and  <dig> by p-value ranking and sam, respectively. the reason is that although this gene exhibited an average log ratio of  <dig>  , there was a large variability in the three biological replicate pairs , with a standard deviation of  <dig>  and p =  <dig> . the detected log ratios on the amersham and agilent platforms were  <dig>   and  <dig>  , respectively, leading to a low ranking by both platforms either with fold-change ranking or p-value ranking.

sam ranks genes based on a modified statistic similar to t-test: delta = u/, where u stands for mean log ratio, s is defined as sqrt, and n is the number of replicates. by incorporating a fudge factor s <dig> in the denominator, in the calculation of delta, hence the ranking of genes, sam effectively ranks genes relatively low in situations where either both u and sd are small, or when u and sd are both large  <cit> . genes falling into these two situations will be ranked high by p-value ranking and fold-change ranking, respectively. intuitively, sam finds a tradeoff between fold-change and p-value, and should be regarded as a preferred gene selection method over pure p-value ranking or pure fold-change ranking.

it should be noted that many combinations of thorough statistical analyses and fold-change cutoff were conducted in tan et al.'s original study  <cit> . however, the results that were emphasized and shown in the venn diagram  <cit>   are obtained from gene selection solely based on a statistical significance cutoff regardless of fold-change or signal reliability. furthermore, because of the use of the same statistical significance cutoff, tan's analysis resulted in an unequal number of selected genes from the three platforms and the two regulation directions. therefore, the calculation of concordance becomes ambiguous and can underestimate cross-platform concordance.

results with different numbers of genes selected as significant
in addition to selecting  <dig> genes  from each platform , different numbers of genes were selected by applying the three gene selection methods to both the 2009-gene and 537-gene datasets. the results are shown in figure  <dig> and agree with the general conclusions discussed above when  <dig> genes were selected, i.e., data filtering increased cross-platform concordance and p-value ranking resulted in the lowest cross-platform concordance. within the same dataset, the difference in pog by different gene selection methods diminishes as the percentage of selected genes increases. however, the pog difference due to gene selection methods is much more significant when the percentage of selected genes is small. the pog by p-value ranking is consistently lower than that by fold-change ranking or sam. the extremely low pog when only a small percentage of genes are selected as significant indicates the danger of using p-value alone as the gene selection method.

considering the large technical and biological variations identified in tan's study, we conclude that the level of cross-platform concordance with the subset of  <dig> genes and by fold-change ranking or sam is reasonable. importantly, we observed no statistical difference between cross-platform lrr <dig> and intra-platform biological lrr <dig> after data filtering when all three platforms were considered . however, it should be pointed out that the cross-platform lrr <dig> was based on the correlation of the averaged log ratios over the three pairs of biological replicates from each platform as represented as aff , ame , and agi  in the right-bottom of table  <dig> 

relationship between lrr <dig> and pog
from hundreds of pair-wise lrr <dig> versus pog comparisons made on tan's dataset , a strong positive correlation  between lrr <dig> and pog  was observed. therefore, it is essential to reach high log ratio correlation in order to achieve high concordance in cross-platform or intra-platform replicates comparisons.

pog by chance
it should be noted that, in addition to cross-platform lrr <dig>  pog also depends on the percentage p  of the total number of candidate genes selected as "significant". as an illustration, figure  <dig> shows simulated pog results from random data of normal distribution of n, where there is no correlation between replicates or platforms . for the comparison of two replicates or platforms, a pog of 100* is expected by chance and the other 100* is expected to be dis-concordant in the directionality of regulation. for example, if all genes  are "selected" as significant  for both replicates or platforms, by chance one would expect 50% of the total number of selected genes to be concordant in regulation direction . for the comparison of three replicates or platforms, the percentage of genes expected to be concordant by chance is 100*2; therefore, 25% of genes are expected to be concordant if all genes are "selected". for the comparison of k platforms , the pog expected by chance would be 100*k- <dig>  the pog by chance is independent on the choice of gene selection methods.

discussion
we analyzed the dataset of tan et al.  <cit>  using an alternative approach and illustrated a number of unaddressed issues of their study. briefly, tan et al.'s study suffered from low intra-platform consistency and poor choice of data analysis procedures. our analysis reiterates the importance of data quality assessment and the need for guidelines on data analysis. the impact of data  filtering in microarray data analysis is shown and the problem of using p-value ranking as the only criterion for gene selection is highlighted. for microarray studies including cross-platform comparisons, it is essential to ensure intra-platform consistency by using appropriate quality control metrics and thresholds against the performance achievable on each platform.

our data analysis procedures first involved a data  filtering procedure that excludes 50% of the genes with the lowest average intensity on each platform. secondly, an equal number of differentially expressed genes were selected from each platform, with half from up- and half from down-regulation, in order to avoid ambiguity in the calculation of concordance. notice that the number of genes identified as up- and/or down-regulated depends on many factors such as the intrinsic nature of the biological samples, the number of gene probes present on the platform, the reproducibility  of the platform, and the cutoff value of significance level. therefore, the number of genes to be identified from each platform in a given study could be arbitrary, but in practice is limited by the number of genes that the biologist is interested in or is capable of examining in greater detail. it should be noted that for platforms with different reproducibility, the p-value or false discovery rate  cutoff will most likely be different when the same number of genes are selected based on fold-change ranking. however, for dataset of reasonable consistency, most genes selected by fold-change ranking also pass a p-value cutoff. alternatively, when the same statistical cutoff  is applied to different platforms, a platform that demonstrates higher consistency will select more genes than that with lower consistency, as shown in figure 3a. thirdly, we compared three different gene selection methods  and compared the cross-platform concordance. the results illustrate the danger of solely using p-value ranking in gene selection without considering fold change. on the other hand, fold-change ranking appears to perform well in identifying gene lists with large cross-platform overlap, which is a reasonable surrogate for assessing the accuracy of microarray data  <cit> . the most reliable results should be those genes showing low p-value and large fold change.

overall, based on the same dataset of tan et al., our reanalysis gives a cross-platform concordance  some 10-fold higher than reported by the original authors  <cit>  and extensively cited in science  <cit> , where only  <dig> out of  <dig> genes are found in common. due to the limited quality of the dataset of tan et al., it is reasonable to expect a higher cross-platform concordance when the quality of data from each platform increases to the best achievable levels. reasonable cross-platform concordance can and should be attainable if microarray experiments are conducted at the level of performance achievable by the technology and if the resulting data are analyzed with validated methods.

it should be noted that pog depends on the percentage  of genes selected out of the candidates; the higher percentage selected the higher the pog . when the results identified from the dataset of  <dig> genes were compared to those from the subset of  <dig> genes, the results were based on the selection of the same number of  <dig> genes  from each platform, corresponding to  <dig> % and  <dig> %, respectively, out of the total numbers of the candidate genes. the corresponding percentages of concordant genes by chance for the comparison of any two platforms are  <dig> % and  <dig> %. for the comparison of three platforms, the corresponding percentages of overlap by chance are approximately  <dig> % and  <dig> % for the 2009-gene dataset and 537-gene subset, respectively. therefore, such a bias of pog towards a higher percentage of selected genes should be kept in mind when reading the numbers from comparing the two datasets, especially when two platforms are compared.

increasingly complicated statistical methods have been continually proposed for identifying differentially expressed genes, and the validity and reliability of the simple gene selection method by fold-change ranking  have been questioned  <cit> . the preference of using a statistical significance metric  as the gene selection method  <cit>  is biased to random noise and platform-dependent systematic errors, resulting in the selection of genes with tiny fold changes that are indiscernible by currently available microarray technology. the fact that fold-change ranking identified a much higher percentage of concordant genes among the three platforms than p-value ranking is not difficult to understand when we consider microarray as a measurement tool and its fluorescence intensity detection is subject to various sources of variability. therefore, only those fold changes that are above random intensity variation are reliable.

multi-factorial nature of cross-platform concordance
one of the goals of gene expression studies is to reliably identify a subset of genes that are differentially expressed in two different types of samples. our results  demonstrate that it is essential to reach high log ratio correlation  between two replicates or platforms in order to achieve high consistency between the lists of identified genes. there are several ways to increase lrr <dig> and the most important steps should be setting quality control checkpoints to make sure that experimental variability is kept as small as possible so that, in turn, data from the same platform are reliable. after data collection, a reasonable data  filtering procedure should be applied to exclude a portion of genes with the lowest intensity that likely reflects platform-specific noise structures . increasing the number of replicates is theoretically important, but in practice is limited by the available resources. it is worth noting that the log ratio correlation of replicates largely depends on the magnitude of true biological differences in expression levels between the two groups of samples compared. for the comparison of dramatically different types of samples , the expected fold change for many genes is large, resulting in reproducibly measurable fold change for many genes. on the other hand, when the inherent biological differences between the two groups of samples are small , the reproducibility of the measured fold change is expected to be lower. for the detection of such subtle changes in gene expression, it is essential to optimize microarray protocols to obtain the best performance that is achievable.

inherent differences among various platforms
our analysis amplifies the need for appropriate metrics and thresholds to objectively assess the quality of microarray data prior to devoting effort to more advanced statistical analysis. our work also reiterates the urgent need for guidance on consistency in analyzing microarray data  <cit> . we agree that inherent technical differences among various microarray platforms exist because of differences such as probe length and design, patterns of cross-hybridization and noise structures, as well as experimental protocols. for example, the intra-platform consistency for the amersham and agilent platforms is significantly higher, but the concordance between these two platforms was not higher than the cross-platform concordance involving the affymetrix platform , which showed the lowest intra-platform consistency. in addition, as shown in table  <dig>  the three technical replicate pairs  on both agilent and amersham platforms showed the same average lrr <dig> of  <dig>  and an average pog of 84%  and 89% , but the cross-platform lrr <dig>  was only  <dig> , corresponding to a pog of 61%. such a difference  could be a result of inherent platform differences, e.g., cross-hybridization patterns due to differences in probes , and differences in detection methods . the "true" cross-platform differences, e.g., whether the probes from different platforms supposedly measuring the same gene are in fact targeting different regions or splicing variants of the same gene  <cit> , should be resolved with more reliable datasets. the lack of gene identity information from the dataset made public by tan et al. prevented us from using probe sequence matching to determine gene overlap across different platforms  <cit>  and to assess the degree of improvement of cross-platform concordance.

calibrated reference rna samples and "gold standard" datasets
because the u.s. fda is expected to receive microarray-based pharmacogenomic data as part of product submissions from the industry, data quality is of great concern. although cross-platform concordance is important, what is more important is the accuracy of each platform. however, the accuracy of microarray technology has not been extensively assessed due to the lack of calibrated reference rna samples and "gold standard" measurements. we are coordinating the maqc  project  <cit>   aimed at assessing the performance achievable on various microarray platforms through a collaborative effort among six fda centers, the national institute of standards and technology , the u.s. environmental protection agency , major microarray platform providers , rna sample providers , selected microarray users , and other stakeholders. reference datasets will be generated on a pair of readily accessible rna samples for each species  by multiple laboratories using multiple platforms, and will be made publicly available for objective assessment of intra-platform consistency, cross-platform comparability, and the comparison of various data analysis methods. importantly, the relative expression levels for over one thousand genes in these samples will be measured by qrt-pcr and other independent technologies. the resulting "gold standard" datasets will be used to assess the accuracy of various microarray platforms. we expect that the "calibrated" reference rna samples, reference datasets, and the resulting quality control metrics and thresholds will facilitate regulatory review of genomic datasets. individual microarray laboratories can optimize and standardize their sops by using the same pair of rna samples and checking their data quality against the reference datasets. by using these tools, a procedural failure may be identified and corrected, and the intrinsic technical differences among platforms can be better understood and addressed. the maqc project, which is highly complementary to on-going efforts of the external rna controls consortium  and nist's metrology for gene expression program ), will help move the process of standardizing microarray technology one step further.

quality control metrics and thresholds
quality control metrics  need to be established for assessing the quality of microarray data. equally important, thresholds for the quality control parameters should be established to determine whether the data quality from a study is acceptable. before any advanced statistical analysis, exploratory analysis of microarray data in terms of the quality metrics  may be used to identify irregularities in the data. the reference rna samples and the reference datasets mentioned above will be essential to determine quality control thresholds.

the need of guidance for microarray data analysis
guidance on data analysis is needed in the standardization of microarray technology. a significant portion of the more than  <dig> literature references on microarrays  <cit>  deals with various strategies on data analysis. however, many of the methods or procedures have not been independently validated for their merits and limitations  <cit> . it is expected that reference datasets will enable a more reliable assessment of the merits of various procedures and methods for microarray data analysis. it is important not to compromise accuracy for the sake of reproducibility in microarray data analysis  <cit> . unfortunately, many methods  currently used in microarray data analysis appear to focus on reproducibility because of the lack of independent datasets for cross-validation. with the availability of "gold standard" measurements and cross-platform datasets from the calibrated reference rna samples, it is possible to judge the performance of individual data analysis methods against the "true" values, not against themselves .

we realize that the absence of control  data, e.g. from qrt-pcr analysis, limits the conclusions that can be drawn from tan's dataset. ultimately, it is the accuracy of the platform that determines its usefulness in research. it is also possible that different data analyses need to be used for specific platforms. as already indicated in the "note added in proof" of tan et al.  <cit> , a comparison between the affymetrix platform and a long-oligo platform have revealed high concordance when used with identical rna preparations  <cit> . however, before qrt-pcr data become available for a large subset of genes for the same pair of reference rna samples, we suggest the use of cross-platform concordance as a surrogate of accuracy in order to evaluate the performance of different data analysis methods. preliminary results illustrated in this paper indicate the limitations of p-value ranking  when used alone as the gene selection method. the reliability of gene selection based on fold-change ranking has been demonstrated for datasets of higher quality when compared to the results from more sophisticated sam method.

CONCLUSIONS
our reanalysis of the dataset of tan et al.  <cit>  illustrates two paramount challenges facing the microarray community. the first challenge is to ensure that individual microarray laboratories perform the bench work in a proficiency that is achievable by the technology. the second challenge is to critically evaluate and validate the merits of various data analysis methods . currently, there is a lack of appropriate tools for microarray users to objectively assess the performance of microarray laboratories. in addition, as a community, we are not in short of "novel" methods for analyzing microarray data; on the contrary, the user is being faced with too many options and the true merits of such methods  have not been adequately evaluated. the outcomes of the ercc and maqc efforts will greatly help address the two challenges facing the microarray community, leading to more reliable, wider applications of the microarray technology.

abbreviations
lir2: squared log intensity correlation coefficient; lrr2: squared log ratio correlation coefficient; pog: percentage of overlapping genes; sam: significance analysis of microarrays.

authors' contributions
ls had the original idea on the method and performed all data analysis and simulations, and wrote the manuscript. wt, hf, us, jh, zs, hh and qx were involved in discussions on the data analysis and verified some of the calculations. jcc provided advices in statistics and suggested the presentation of results shown in table  <dig> and figure  <dig>  jh, rkp, fwf, fmg, lg, th, jcf, zax, tap, rgp, jcc and dac provided additional insights regarding issues on cross-platform comparison and microarray quality control. wt, rkp, jh, lg, jcf, rgp, jjc and dac assisted with writing the manuscript. all authors participated in the design of the study and approved the final manuscript.

