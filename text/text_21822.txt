BACKGROUND
understanding crosstalk and feedback among oncogenic pathways is critical in order to predict and overcome resistance to targeted anticancer therapy. the topology of biological networks has increasingly been used to complement studies based on individual genes or gene sets. several network applications are relevant to the study of pathway crosstalk in drug resistance. the identification of modules and sub–networks that are relatively isolated from the rest of the network can lead to an understanding of the direct interaction and cooperation among molecules and to more detailed or dynamic models of the network. network topological characteristics can potentially be predictive biomarkers through network based classification  <cit> .

protein interaction networks and gene co-expression networks potentially represent patterns of network connectivity among genes/proteins that differ between clinically relevant phenotypes. various topological measures that identify relationships between genes, such as node degree, betweenness  <cit> , or bridging  <cit> , may contribute to the ability to predict phenotype-gene association.

here, we apply several techniques for network analysis to demonstrate their utility in studying biological networks in breast cancer. we utilize network topological measures to expose the important nodes  within the network, and identify marker genes  from gene co-expression networks, protein interaction networks, or integrated functional networks.

in the present work, we have extracted thirteen topological measurements  from a publicly available gene co-expression network and a protein interaction network. we then use classification models to investigate the phenotype-gene association in breast cancer. moreover, we apply this approach to the integrated functional network of ppi and gene expression in order to investigate the hidden patterns of breast cancer that might not be revealed in the protein network or gene co-expression network.

related works
gene expression datasets  have been used extensively for the purpose of phenotype-gene association, where the gene expression profiles are fed as features into the classifier .

recently, the network–based approach has also been used for this purpose. for instance, zhang et al.  <cit>  proposed a network–based cox regression model . the proposed model was intended to investigate the gene expression signatures that contribute to the result of death or repetition in ovarian cancer treatment. moreover, ruan et al.  <cit>  proposed a general co-expression network-based technique that allows analysis of genes and samples obtained from microarray datasets. this technique uses a rank–based network construction method, a parameter-free module discovery algorithm, and a reference network-based metric for module evaluation. the study utilized a number of different datasets for evaluation purposes, such as yeast and human cancer microarray.

yuanfang et al.  <cit>  proposed an approach that utilized a mouse genome-wide functional relationship network and support vector machine classifier to investigate the bone mineral density  of a phenotype related to osteoporotic fracture. two genes were revealed  that are related to bone density defects that were not identified in other statistical methods .

wu et al.  <cit>  developed a naive bayes classifier  to reveal a functional interaction  network that combines both curated protein–protein interaction networks and pathway information.the computed fi network was used to investigate two glioblastoma multi–form  datasets and projected the cancer candidate genes onto the fi network.

methods
our proposed methodology consists of four steps: 
step 1: extract topological measures from biological networks.

step 2: identify the breast cancer signature genes.

step 3: apply smote in order to make a balanced dataset.

step 4: use classification models in order to investigate the phenotype-gene association in breast cancer.



details about these steps are described below:

topological measures
we study several topological measures in order to understand their capability in identifying disease markers from the biological network. table  <dig> illustrates the relation among these measures. first, we need to define some graph  concepts.


the degree of a vertex v in a graph g= is the number of connections it has. here v is the set of vertices  in the graph and e is the set of edges  in the graph. the distance σvw of a vertex v from another vertex w is the number of edges in the shortest path between them. a path in a graph is a sequence of edges that connect a sequence of vertices . the walk is a path in which vertices or edges may be repeated.

the betweenness value of a vertex v is defined by the following equation: 
 b=∑s∈vs≠v∑t∈vt≠s,t≠vσstσst. 

the numerator in the fraction shows the number of shortest paths joining s and t on which v is an intermediate vertex.

the closeness value of a vertex v is defined by the following equation: 
 c=∑t∈vt≠v1σvt. 

the proximity prestige measure  <cit>  could be measured as the ratio of the proportion of vertices that can reach v to the average path length of these vertices from v. 
 pp=iv/∑t∈vt≠vσvt/iv,  where iv is the number of vertices in the domain of node v.

bary center score ranks each vertex of the graph depending on the total shortest path of the vertex. it computes the shortest path distances for each vertex in the graph and a score will be assigned for each vertex based on the lengths of the shortest paths that go through the vertex.

clustering coefficient measures the degree of cohesiveness in a given graph. for a given vertex v, ccc is defined as the ratio of actual number of edges ei within its neighborhood and the maximum number of possible edges in that neighborhood.

the coreness value measures the set of vertices that are highly and mutually interconnected. the k-core is the largest subgraph, comprising vertices of a degree at least k, and is derived by recursively removing vertices with a degree lower than k until none remain.

eigenvector centrality value expresses the centrality of a vertex as dependent on the centralities of its directly connected neighboring vertices. for a given undirected graph g= and its adjacency matrix a, the eigenvector centrality is the eigenvector of the largest eigenvalue λmax in absolute value. the eigenvector centrality ceiv could be obtained from the following system of equations: 
 λceiv⃗=aceig⃗. 

katz status index centrality ranks a vertex as highly important if a large number of vertices are connected to it. both direct and indirect neighbors of a vertex contribute to its importance. katz status index centrality  and its adjacency matrix a, the subgraph centrality for a node that has length of close walk k is computed as follows: 
 csg=∑k=0∞vvk!. 

within–module z-score measures how vertices are related. modules could be organized in different ways. if ki is the number of edges of vertex i to other vertices in its module mi, k¯mi is the average of k over all the vertices in mi, and smi is the standard deviation of k in mi, then, the within-module z-score is computed as follows: 
 zi=ki−k¯mismi. 

the within–module z-score measures how well connected vertex i is to other vertices in the module.

k-step markov technique calculates the relative probability that the system will spend some time at any particular vertex, such that it is given the start set of roots r and ends after k steps. let pu,v be the probability of reaching v from u in one step. so, this probability is the weight of the edge between u and v. then, let n be the set of neighbor vertices of u. after that, the probabilities are constrained by the following equation. 
 ∑u∈vv∈npu,v= <dig>  

furthermore, a random walk is defined as a walk that starts at a particular vertex and traverses the graph based on pu,v. k-step markov centrality is the probability with which a random walk of length k brings a system to a particular vertex v  <cit> , and could be obtained from the following equation. 
 cksm=p0ak,  where p <dig> is an initial probability distribution of the vertices in g, and a is the adjacency matrix of g containing the transition probabilities. in this study, we consider k to be  <dig> 

to apply the structural hole concept, we identify nodes utilizing burt’s aggregate constraint measure . burt’s structural hole argument is that social capital is created by a network in which individuals in the social network can broker connections between otherwise disconnected segments. this concept builds on a metaphor of ‘social capital’ that is made concrete with network models in which topological measures rank nodes by their connectivity and lack of redundancy. the argument further posits that since there is some cost of maintaining connections, non-redundancy increases the influence of a node.

breast cancer signature genes
in this study, three major databases have been utilized to identify the breast cancer signature genes : 
the genetic association database   <cit> .

the mammalian phenotype   <cit> .

the human phenotype ontology  <cit> .



we have extracted  <dig> genes that related to breast cancer from the databases mentioned above. we fed this gene data as class labels into classifiers. thereby the class labels in the dataset are represented as ‘yes’  and ‘no’ .

synthetic minority oversampling technique
synthetic minority oversampling technique   <cit>  is a sampling approach used to transform an imbalanced dataset to a balanced one. a dataset can be considered imbalanced if there is one group of observations with a very minimum number of samples compared to the other group of observations in the same dataset. it is well known that a machine learning classifier cannot perform well if the dataset is highly imbalanced. the dataset we used in this study is imbalanced by nature and hence application of smote could transform the dataset to a balanced one.

the smote approach over–samples the minority class by creating synthetic samples rather than over–sampling with replacement. in other words, the positive  samples are over–sampled with replacement to match the number of negative  samples, as shown in fig.  <dig>  this method operates in ‘feature space’ rather than ‘data space’: i.e each feature is over–sampled. in line with this, the minority class is over-sampled by taking each sample belonging to the minority class and generating synthetic samples to increase the sample size. this is done using a k-nearest neighbor algorithm among the minority samples. the sample that appears to be the closest k neighbor is joined together to generate a new sample.
fig.  <dig> smote’d data example 



classification models
in this study we have used two different classifiers: decision tree bagger   <cit>  and random under sampling boost rusboost  <cit>  in order to classify the data, based on the extracted topological measures as features and breast cancer signature genes as the class label.

decision tree bagger employs a classic decision tree as the classifier and then a bagging methodology is used to further enhance the classification performance of the classifier. a decision tree is a widely used classifier that divides the dataset such that the impurity level in each partitioned dataset is reduced when compared to the dataset that has been partitioned. the impurity level of a dataset is measured using the class label of each of the records. the most popular measurement for measuring impurity level is the gini index. following a tree structure view, the source dataset is considered as a root node of the tree, while each partitioned dataset is considered as a child node that is rooted at the corresponding root node. dataset partition is repeated at each of the sub-partitions with the aim of achieving a pure partitioned data at the leaf node of each of the branches of the tree. once the tree is induced from the training dataset, traversing the tree from the root to each of the leaf nodes generates rules. these rules are then applied to classify an unknown dataset. since the decision tree is induced from the training dataset, the tree structure might vary with varying sets of data of the same problem. hence, the performance of the respective decision tree also could vary. to overcome this and achieve an enhanced classification performance a number of bootstrap replicas of the dataset are generated. this process of generating multiple replicas of the dataset, by varying sets of data in each of the datasets, is called the bagging or bootstrap methodology. through application of the bagging methodology, the resultant individual replica of the training dataset is used to induce a decision tree. thus, there will be as many decision trees as there are generated dataset replicas. the bagging replica could be sampled randomly choosing from n observations out of n with replacement, where n is the total data events in the dataset. furthermore, the average of the classification performances from individual trees is considered as the output of the decision tree bagger.

random under sampling boost  decision tree is another approach used to enhance the performance of the base decision tree classifier to better deal with an imbalanced dataset. in this approach, the data that belongs to the minority class is considered as the basic population, while data belonging to the majority class is under–sampled, such that the data for each of the classes becomes balanced. let us consider that there are observations that belong to the minority class in the training data. following the rusboost approach, these n observations are considered as the basic population for sampling. thus, a total n observation from the data belonging to the majority class is sampled. note: if there is more than one class that is considered as a majority class, n observations are sampled from each of the classes. all of the sampled data is merged with the n observations from the minority class to form a balanced dataset. after achieving a balanced dataset, a decision tree is induced using this dataset.

performance metrics
we consider several measures in order to evaluate each classifier performance:

accuracy  is one of the most widely used performance metrics in evaluating a classifier. acc is defined by the following equation: 
 acc=tp+tnn,  where  represents all samples that are classified correctly  and true negative ) and n is the total number of samples available  + ).  represents all samples that are classified incorrectly  and false negative ).

positive predictive value  is the proportion of cancerous genes in the prediction that are correctly predicted as cancerous genes: 
 ppv=tptp+fp. 

sensitivity  refers to the proportion of cancerous genes which are correctly predicted as cancerous and the total cancerous genes: 
 sn=tptp+fn. 

specificity  refers to the proportion of non-cancerous genes that are correctly eliminated and the total non-cancerous genes: 
 sp=tntn+fp. 

f-measure  is the harmonic mean of sensitivity and positive predictive value, which is defined as: 
 f=2·sn·ppvsn+ppv. 

geometric mean  has been introduced to overcome the problem that is associated with the accuracy metric in imbalanced dataset learning: 
 gm=. 

the receiver operating characteristic  curve  <cit>  is a well known performance measurement metric used to evaluate the performance of a classifier, particularly when the dataset is highly imbalanced. the roc curve can be generated by considering a two-dimensional cartesian plot, where the x-axis represents the amount  and the y-axis represents sn. it should be noted that by varying the threshold level of classifying the data into two classes , the above mentioned measures will also vary. hence the roc plot reflects these variations in terms of both sensitivity and specificity. in summary, through analysis of the roc plot it can be easily identified which threshold level provides the best performance for a classifier. it is worth mentioning here that the best possible performance for a classifier can be achieved if both sensitivity and specificity yield  <dig> %. in other words, the roc curve that exactly matches the upper part of the roc space yields the best performance. hence, the closer the curve to the upper part of the roc space, the better the performance is. alternatively, the area under the curve can reveal the quality of the classifier’s performance. if the curve covers the whole roc space, the classifier is called the perfect classifier. as such, the area under the curve  can also be used as an indication of classifier performance. an auc value equal to  <dig> is called the best classifier, while anything close to  <dig> can be considered as good as that of the perfect classifier. an auc value less than  <dig>  is considered to be a random classifier performance.

validation
to achieve a generalized performance of the proposed method, we applied the well known k-fold cross validation schema. in this schema, the dataset is divided into k equal partitions and a computational model is generated using k− <dig> partitioned datasets, while the kth partitioned dataset is kept untouched in order to test the model later. these steps are repeated k times such that each individual data is used to test the efficacy of the proposed model. it is worth mentioning that for k-fold partition, a total k number of models with varying training datasets are generated. as our proposed model consists of identification of features that are based on the performance of the proposed model, while selecting features we considered only the total k− <dig> partitions of dataset by keeping the data belonging to the kth partition aside. by doing so we achieve a more general performance of the proposed model without having any bias towards any class of data.

RESULTS
in this study three public networks are utilized to extract network topological features: a) the gene co-expression network obtained from hedenfalk et al.  <cit> ; b) the protein interaction network of homo sapiens obtained from the biogrid database   <cit> ; and c) the integrated functional interaction network which made publicly available by wu et al.  <cit> . we compare the performance of the classification models in predicting the phenotype-gene association using features extracted from these networks. we report the performance measures that were mentioned earlier. table  <dig> presents a comparison of the performance of classification models.


we applied 10−fold cross validation schema. we then compute the  <dig> % confidence interval for the mean with the following formula: q=m±z.95σm, where z. <dig> is the number of standard deviations extending from the mean of a normal distribution required to contain  <dig>  of the area and σm is the standard error of the mean. clearly, the dtb classification model, which adapts smote sampling and uses topological features extracted from the integrated functional interaction network , has the highest g-mean value , as illustrated in table  <dig>  a high g-mean value indicates that a high proportion of the cancerous genes  are predicted correctly. on the other hand, the dtb classification models that adapt smote sampling and use topological features extracted from the other two networks — gcn and ppi — have lower g-mean values of  and , respectively. this indicates that using an integrated functional interaction network can reveal more information about phenotype-gene association in breast cancer. rusboost has similar results but has one major drawback: the rusboost uses its own sampling method, which creates a conflict with the smote sampling method.

moreover, we compare the performance of the dtb classification model that adapts smote sampling with the one that does not adapt smote sampling. the performances were computed using areas under the roc curves . clearly, the dtb classification that adapted smote sampling has the largest area under the roc curve , as shown in fig.  <dig> 
fig.  <dig> comparison of classification results on the fi network data



in addition, we compute the significant level of each of the selected topological measurements by using two well known statistical measurements: accuracy and the gini index. clearly, the gini index outweighs the accuracy score . to overcome this problem we compute the geometric mean of accuracy and gini index as a combined score. we compute the accuracy, the gini index and the combined score based on the dtb classification model that adapts smote sampling and using topological features extracted from the integrated functional interaction network. the results are illustrated in table  <dig>  it can be seen from that table that ‘structural holes’ and ‘degree’ features outperform the other features by a significant margin in terms of combined score values. in addition, a backward elimination method has been computed that identifies a subset of five features as important features in predicting phenotype-gene association. the identified features are ‘structural holes’, ‘degree’, ‘coreness’, ‘k-step markov’ and ‘subgraph’.



finally, we investigate genes that not classified correctly, particularly the ones from the group where genes are not cancerous but the method misclassifies them as cancerous genes. table  <dig> illustrates some of these genes. each gene is listed according to its symbol, name and related omim disease. the table shows that our method is capable of identifying new genes that may be related to breast cancer.

cd4

app

cdk2

fn1

irf1

psen1

stat1

slc25a3

sos1


CONCLUSIONS
we have compared various topological measures that have the potential to identify phenotype-gene association for breast cancer. we have extracted thirteen features from publicly available gene co-expression networks and protein interaction networks. we have used two classification models to investigate the phenotype-gene association in breast cancer. moreover, we have applied this approach to the integrated functional network of ppi and gene expression in order to investigate the hidden pattern of breast cancer that might not be revealed in the protein networks or gene co-expression networks.

in conclusion, our approach is capable of effectively detecting the phenotype-gene association in breast cancer.

