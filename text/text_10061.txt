BACKGROUND
recent advances in computer science and in the acquisition of new genetic sequences from a variety of organisms have opened up a wide spectrum of new possibilities in molecular evolutionary modeling. in particular, codon substitution models explicitly formulated in terms of a balance between mutation and selection constitute an attractive strategy  <cit> . by deriving the substitution process from basic principles of population genetics, their aim is to bridge the gap between population genetics and phylogenetics, and thus to offer a better understanding of the driving forces of the long term evolutionary process. more specifically, these mutation-selection models propose that the substitution rate from a sequence s to another s'  depends on the rate of mutation from s to s' (), and on the probability for this mutation to be fixed in the population ):

   

the mutation matrix  depends only on the underlying mutation model, and is generally assumed to be fixed along the lineages and uniform along the sequence. the fixation probability pfix depends on the particular model chosen.

among the mutation-selection codon models, we focus on the structurally constrained  models  <cit>  which attempt to explicitly link a protein's tertiary structure to the evolution of its sequence. they consider that a protein is under a purifying selection maintaining a stable and constant tertiary structure. importantly, and unlike most probabilistic models currently used in molecular evolutionary studies, sc models are explicitly site-interdependent, and therefore, require complex monte carlo methods to be implemented and applied to empirical data  <cit> .

in sc models, the fixation probability of a given mutation depends on a score function assessing the adequacy of a sequence s to the tertiary structure of the protein, c. this score should be devised so that the fixation probability is low if the proposed mutation destabilizes the structure or complicates the folding process. since anfinsen's experiments  <cit> , the relations between protein structure and sequence have been carefully studied and an intuitive approach consists in relying on first principles of protein thermodynamics, using all-atom force fields . however, in our case, the instantaneous rate of substitution , and thus the structure/sequence score function, have to be computed for each possible nearest neighbor mutant, and for each substitution, along the entire evolutionary tree. therefore, we need a fast computation of the fixation probability which precludes the use of all-atom force fields.

an attractive alternative is provided by knowledge-based  potentials. they mimic the boltzmann law  <cit>  and usually rely on a coarse-grained description of the structure, implicitly integrating out the degrees of freedom of the side chains and thus avoiding the complexity and the computation requirements of all-atom force fields  <cit> . in addition, they are trained empirically from databases of natural proteins. this latter point is of particular interest in evolutionary studies, where we are interested in all aspects of the relations between sequence and structure prevailing in natural sequences, and not only in the specific problem of the thermodynamic stability. in this respect, one expects that learning potentials from native structure-sequence databases using blind machine learning methods will capture all such aspects.

many statistical potentials have been proposed  <cit> , either to predict the fold of a given sequence  or to find a sequence or a set of sequences folding into a given tertiary structure . however, the same potential may not be best-suited to both goals since the spaces of optimization are very different: in the protein folding problem the search is done over the structure space, while in the protein design problem the search is done over the sequence space. the phylogenetic context described here is more akin to a protein design perspective, as the structure of the protein is assumed constant during evolution, representing a constraint under which the sequence is evolving.

several methods have been developed to train statistical potentials in a protein design perspective  <cit> . in a previous work, we introduced a probabilistic framework for protein design purposes based on the maximum likelihood principle  <cit> . the likelihood we considered was the probability of the sequences s given their native structures c and the model parameters , p . this probability was then maximized with respect to the potential parameters  by a gradient method. however, the probability p  involves a normalizing factor, summing over all possible sequences, which cannot be analytically calculated. we thus had to resort to a markov chain monte carlo  numerical procedure: at each step of the gradient descent, we generated a set of sequences by gibbs sampling, conditional on the current values of the potential. this set of sequences was then used to estimate the gradient. the gibbs sampling procedure was the limiting step of our algorithm, restricting the set of alternative potentials that we could explore and empirically test. the potentials we obtained using this method are called joint potentials hereafter.

interestingly, kuhlman and baker  <cit>  used a leave-one-out procedure to estimate a restricted set of parameters of a free physical energy function in order to do protein design. in this procedure, only one site of the protein is changed at a time, while the other positions are kept fixed in their native state. the procedure is thus similar to training a potential to recognize acceptable sequence variants, given the target structure, among all possible point mutants. the leave-one-out criterion seems to give good results. however, it has never been assessed against alternative methods. here, we adapt the statistical framework we defined in  <cit>  now using the leave-one-out definition of the likelihood to perform the gradient descent instead of the joint likelihood. we compare the potential parameters obtained by the two methods, and we establish that we can be highly confident in the results obtained using the leave-one-out likelihood. overall, the leave-one-out procedure allows much faster computations while giving sensibly the same results as the joint one.

RESULTS
likelihood framework
as in  <cit> , we formulate the problem in terms of a probabilistic model, considering a sequence s =  <dig> .n of length n according to a probability distribution p , conditional on the conformation c and on a set of potential parameters θ. the parameters are estimated by maximizing the probability of observing a database of n independent sequence-structure pairs , with , c = p =  <dig> .n. here,  is the p-th native sequence of the dataset, np is the lenght of this sequence and cp is the native conformation associated with . in practice, a native sequence-structure pair corresponds to a protein taken from the pdb.

the probability that we want to maximize can be expressed as follows:

   

as a function of θ, this term can be seen as a likelihood. hereafter, we define the methodology with one protein, but it can be easily generalized to a set of proteins.

borrowing from  <cit> , we set:

   

where y is called the normalization factor, and g the inverse potential, defined as

   

where e is the statistical potential and f  is analogous to a free energy term and can be approximated using the random energy model  <cit> :

   

where μa, a = { <dig> .20} are unknown parameters, analogous to chemical potentials  <cit> .

optimization method
joint likelihood maximization
in our previous work  <cit> , we defined a score function ω  as:

   

this score function should be minimized conditional to θ. its gradient is:

   

where ⟨·⟩ stands for the expectation over sequences drawn from the probability defined by eq.  <dig>  given the size of the sequence space , this expectation cannot be computed analytically, and therefore, in  <cit>  we used a mcmc method to numerically estimate this expectation.

leave-one-out likelihood maximization
we define for site i, i =  <dig> .n, the leave-one-out probability

   

which is the probability of having an amino acid a at site i, in the context of the native sequence at all other sites . this leave-one-out probability can easily be obtained by a normalization over all possible twenty outcomes at site i:

   

we can write this probability for any amino acid a, and in particular for the native amino acid at site i,  i.e.. taking the product over all positions i =  <dig> .n, and by analogy with our previous definition of likelihood, we define the leave-one-out likelihood:

   

note that this leave-one-out likelihood is normalized over the sequences, exactly as in the case of eq.  <dig>  therefore it yields a valid probability distribution over the sequence space. on the other hand, the probability depends not only on c and θ, but also, in some sense, on the native sequence itself. to make this point explicit, we make  appear on both sides of the conditioning bar.

we define the corresponding scoring function:

   

the gradient of which is immediately obtained :

   

this gradient can be analytically calculated, at each step of a gradient descent. we note that the term corresponding to the normalization factor  can be seen as an expectation over the leave-one-out probability. it is thus analogous to the expectation appearing in the right hand of eq.  <dig>  however, it is defined on a much more restricted universe .

for implementing both methods, we used a simple form of potential  <cit> , consisting in two terms: one related to contact interactions and the other to the solvent accessibility .

potential optimization
we first run our leave-one-out method on dsl . we consider that the optimization is complete when the overall maximum gradient is smaller than 10- <dig>  this corresponds to a variation of 10- <dig>  at most, in the value of the potential parameters. using this stopping condition on the dataset dsl with empirically tuned general steps , we compare three different gradient descent methods : the simple gradient descent, the inertial gradient descent, and the controlled inertial gradient descent. the values of the parameters stabilized after  <dig>  gradient steps for the simplest gradient descent, versus  <dig>  gradient steps for the inertial gradient, and  <dig>  gradient steps for the controlled inertial gradient. concerning the last method, if we choose a different general step  the procedure automatically reaches the optimal step for that dataset. at the beginning of the optimization procedure, the inertial component of the gradient greatly speeds up the optimization, but is automatically deactivated when the values of the potential parameters are near the optimum, thus avoiding the numerical instabilities usually observed using less adaptive gradient methods.

independent runs from different and randomly chosen initial values for the parameters of the leave-one-out potential , lead to the same final values of ωl  and of the potential parameters . these computations were done with the three gradient descent methods, and resulting always in the same final values, which suggests that, in the present case, we do not have local minima in the space of parameters. similarly, the potential parameters obtained by two independent runs on the same dataset are very similar, indicating that our stopping condition is sufficient to have a good precision in our estimates . in fig.  <dig> we have also represented the evolution of some parameters of the potential during optimization. we can see that the values of these parameters oscillate at the beginning of the gradient descent and then reach their optimal values. this behavior is caused by the evolution of the other parameters, as they influence each other during optimization. the complete series of parameter values obtained by our optimization method are presented in the additional file  <dig> 

the contact potentials obtained with the leave-one-out optimization criterion make sense from a biological point of view : as expected, favorable interactions between amino acids in the contact potentials are represented by large negative value , and by large positive value for unfavorable interactions . concerning the accessibility potentials, it is important to note that we are working in a protein design context . accordingly, the accessibility potentials have to be interpreted row-wise. if one wants to compare the accessibility potentials between classes for a given amino acid , one solution is to remove the logarithm of the frequency of the accessibility classes to each potential . also, note that there is a lack of identifiability between α and μ, which has been be resolved by including the chemical potentials in the accessibility terms.

complexity
in our previous work, we had to use a mcmc protocol to numerically evaluate the derivative of the gradient , which was a computationally demanding task. at each step of the gradient descent, we had to sample a set of sequences by gibbs sampling, under the current values of the parameters, so as to numerically estimate the gradient of the log-likelihood.

to compare the joint and the leave-one-out potentials, we first define an elementary calculation as the evaluation of the inverse potential at a particular site i for one particular amino acid a , eq. 9). this calculation has to be made in both cases. it is explicitly defined in the leave-one-out procedure , and is implicitly used in the joint context: an elementary step of the gibbs sampling algorithm consist in computing, at a given site i the leave-one-out probability  for each possible amino-acid at this site, conditional on the rest of the sequence, and to choose the new aminoacid at site i according to these probabilities. performing such an elementary update for every site in turn corresponds to one gibbs sampling sweep and represents 20·n elementary computations. a reliable estimate of the joint expectation requires k sweeps  and so, for a gradient step, we need k·n· <dig> elementary calculations .

in the case of the leave-one-out potential, we only have to make the equivalent of one sweep to exactly compute the gradient . thus, we only need n· <dig> elementary calculations for a gradient step, which thus represents a  <dig> -fold increase in computational speed compared to the joint method. in practice, and after the addition of the acceleration of the gradient descent, it took about one week to have a good estimate when we used the joint method, versus less than fifteen minutes when using the leave-one-out approach.

potentials are indistinguishable
we applied the two optimization procedures  to the same dataset dsj, and found a high correlation between the two resulting potentials . the correlation coefficient r <dig> was about  <dig>  for the contact potential parameters and about  <dig>  for the accessibility potential parameters. for comparison, we applied the leave-one-out procedure on the two datasets ds <dig> and ds <dig>  and found a correlation coefficient of  <dig>  for the contact parameters and of  <dig>  for the accessibility parameters, indicating that the difference between the joint and the leave-one-out potentials is small compared to the sampling error due to the finite size of the training set. altogether, the leave-one-out method appears to be a fast and reliable optimization procedure, yielding potentials that are virtually indistinguishable from those obtained under the joint method. as presented in  <cit> , the contact potentials present a correlation  with those of miyazawa and jernigan  <cit> .

phylogenetic evaluation
in eq.  <dig>  we defined the substitution process of the sc model as a process depending on a mutation rate and a fixation probability. there are many ways the fixation probability could be expressed. here, we do as in robinson et al  <cit>  and assume that this probability depends only on the potential difference  between the original and the mutated sequences. let us denote by snuc and , two sequences which differ only by a nucleotide, and saa and , the corresponding amino acid sequences . then, the rate of substitution between s and s' is:

   

where  is the mutation term depending only on the two sequences snuc and .  is the energy difference between saa and , and β ≥  <dig> can be considered as the strength of the structure-sequence constraint enforced by the model. thus, a negative  Δg means that the mutation is more  likely to be accepted than a purely neutral  mutation.

note that the substitution process defined by eq.  <dig> is reversible and has a stationary distribution defined by:

   

where ∏ <dig> is the stationary distribution induced by the pure mutation process (). given the way our potentials are optimized  and assuming that natural sequences are sampled at equilibrium from the process defined by eq.  <dig>  we then expect that the optimal value of β should be close to  <dig> . in the following, we explore the entire range β ∈  <cit> .

we denote by  the sc model defined using the leave-one-out potential and  the sc model defined using the joint potential; the two models depend on β. obviously, when β =  <dig>   =  = sc <dig>  and the model reduces to a pure mutation model which will be considered as our reference model.

we implemented our potential in the sc model as described in  <cit>  and applied it to the globin15- <dig> dataset, with an underlying mutational specification inspired by the codon model in  <cit>  and denoted as mg in  <cit> . this mcmc framework allows one to obtain a sample of parameter values and substitutional histories along the tree, drawn from the posterior distribution under the  model. such a sample can then be marginalized over quantities of interest. here, we briefly illustrate the approach by displaying the logo of the reconstructed mammalian ancestor hemoglobin sequence .

since the leave-one-out procedure can be seen as an approximate but faster training method, compared to the joint method developed previously, we evaluated its impact on model fit via bayes factors evaluations . in this section we consider the three versions of the sc model, , based on a contact + accessibility leave-one-out potential, , based on a contact + accessibility joint potential, and  based on a contact only joint potential. as explained in the methods, in the present case, the thermodynamic integration method yields a complete fitness curve  of each model . in this way, we can readily spot the optimal value of β under each model, and report the bayes factors under this optimal value .

as can be seen from fig.  <dig> and table  <dig>  the models based on the joint and the leave-one-out potentials have a very similar fit across the whole range of value of β that we tested. interestingly, in all but one cases, the bayes factor appears to be slightly in favor of the leave-one-out potential, although the differences are not significant. as a point of comparison, we also measured the fit of the contact only potential , to illustrate that the difference between the joint and the leave-one-out methods is small compared to the differences observed between the alternative forms of statistical potential that we would like to empirically compare .

discussion
in a previous work  <cit> , we defined a statistical framework for protein design, using the maximum likelihood principle, with the aim of devising statistical potentials to be used in phylogenetic studies. however, the optimization procedure we introduced at that time requires a mcmc protocol to cope with the proportionality constant entailed by the normalization of the probability over the sequence space. here, we introduce a different likelihood, which we called leave-one-out, to optimize the potentials. a similar procedure was previously used by kuhlman and baker  <cit> , but was not statistically assessed against alternative procedures. we found in this work that the joint and the leave-one-out potentials are virtually indistinguishable, both by direct comparison and by bayes factor evaluation in a phylogenetic context.

we note that the optimal β for the  model is not  <dig> , as one may expect given the way our potentials were normalized . several explanations can be proposed. first, strictly speaking, this expectation is valid under the joint procedure, and not under the leave-one-out procedure. but the very high similarity between the two resulting potentials, and the fact that a similar phenomenon  can be observed also under a potential optimized using the joint method  <cit>  do not favor this explanation. alternatively, it may appear at first that this could be due to the fact that the underlying mutation model  was not explicitly taken into account when optimizing the potential , whereas our phylogenetic model does involve an explicit mutational process. in this sense, in the phylogenetic analysis, there is a potentially  redundant modeling of mutational features, in having explicit parameters devoted to these, in combination with the use of the sc setting. this might explain the optimal value of β lower than  <dig> . the phenomenon may also be the result of model violations, which are very likely to be present given the simple form of the potentials. finally, it is also likely that the mutation pressure, or the selection strength  is not the same for each protein. accordingly, two possible improvements to the method can thus be proposed here: the first is to optimize the potential while allowing for different values of β for each protein or each family of protein. the second is to cluster proteins into classes, and optimize a potential specifically for each class.

CONCLUSIONS
apart from these two possible improvements, many other directions of research should now be explored: alternative functional forms for the potential should be implemented and empirically tested. several methods accounting for negative design, through the use of explicit decoys  <cit>  such as the use of a normalized energy gap between a native structure and misfolded structures  <cit> , or using variational methods  <cit> , also deserve further investigation. the supervised learning described here depends on structure-sequence pairs. in the present case, we have used native pairs, but this could be relaxed by taking a set of structures  as the reference structure or by taking a set of homologous sequences instead of a unique sequence  <cit> . a more appealing method would consist in doing the optimization directly within the phylogenetic context. importantly, the fact that the leave-one-out procedure is much faster than the joint method , has obvious practical consequences, as it allows a much larger diversity of alternative models and methods to be tested.

