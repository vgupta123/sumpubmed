BACKGROUND
the human genome contains variants ranging in size from small single nucleotide polymorphisms  to large structural variants . svs include variations such as novel sequence insertions, deletions, inversions, mobile-element insertions, tandem duplications, interspersed duplications and translocations. in general, svs include deletions and insertions larger than  <dig> base pairs , while smaller insertions or deletions are referred to as indels, though the threshold of 50 bps is somewhat arbitrary and based on the fact that different bioinformatics methods are usually used to detect svs vs. small indels and snps. svs have long been implicated in phenotypic diversity and human diseases  <cit> ; however, identifying all svs in a whole genome with high-confidence has proven elusive. recent advances in next-generation sequencing  technologies have facilitated the analysis of svs in unprecedented detail, but these methods tend to give highly non-overlapping results  <cit> . in this work, we calculate “annotations” from features in the reads in and around candidate svs, and we then develop methods to evaluate candidate svs based on evidence from multiple ngs technologies.

ngs offers unprecedented capacity to detect many types of svs on a genome-wide scale. many bioinformatics algorithms are available for detecting svs using ngs including depth of coverage , paired-end mapping , split-read, junction-mapping, and assembly-based methods  <cit> . doc approaches identify regions with abnormally high or low coverage as potential copy number variants. hence, doc methods are limited to detecting only deletions and duplications but not other types of svs, and they have more power to detect larger events and deletions. pem methods evaluate the span and orientation of paired-end reads. read pairs map farther apart around deletions and closer around insertions, and orientation inconsistencies indicate potential inversions or tandem duplications. split reads are used to identify svs by identifying reads whose alignments to the reference genome are split in two parts and contain the sv breakpoint. junction-mapping methods map poorly mapped, soft-clipped or unmapped reads to junction sequences around known sv breakpoints to identify svs. assembly-based methods first perform a de novo assembly, and then the assembled genome is compared to the reference genome to identify all types of svs. by combining various approaches to detect svs, it is possible to overcome the limitations of individual approaches in terms of the types and sizes of svs that they are able to detect, but still difficult to determine which are true .

numerous methods have been developed to find candidate svs using ngs, but clinical adoption of human genome sequencing requires methods with known accuracy. the genome in a bottle consortium  is developing well-characterized whole-genome reference materials for assessing variant-call accuracy and understanding biases. recently giab released high-confidence snp, indel, and homozygous reference genotypes for coriell dna sample na <dig>  which is also national institute of standards and technology  reference material  <dig> available at https://www-s.nist.gov/srmors/view_detail.cfm?srm= <dig>  <cit> . in this work, we developed methods to integrate evidence of svs in mapped sequencing reads from multiple sequencing technologies. we used unsupervised machine learning to determine the characteristics of the different sv types, and we used one class classification to classify candidate svs as likely true positives, false positives, or ambiguous. using these methods, we classified three independently established “validated” call sets containing large deletions or insertions.

our classification methods use the machine learning technique one class classification   <cit> . in contrast to the more common two-class models that have two training sets , one-class methods have only a single training set and try to identify sites unlike the training set. in our occ methods, the algorithm tries to identify a region, r, of the annotation space that contains a specified, large proportion  of the non-svs. sites that have annotations falling outside r are classified as svs. in essence, these are outliers relative to the non-svs. for selecting r, only a representative set of non-svs is required for the training. in our model, we use random genomic coordinates as our one class because random coordinates are unlikely to be near true sv breakpoints. for our one-class model, we only include annotations that are likely to indicate a sv if they differ from random coordinates for a defined set of parameters . we do not include annotations like mapping quality that may not always distinguish svs from non-svs because atypical values may also indicate random regions of the genome that are difficult to sequence. we do not use a two class machine learning model because our potential training sv call sets are primarily easier-to-detect mid-size deletions and insertions and are not representative of all types of deletions, insertions, or other sv types, which is an important assumption of two-class models. therefore, a two class model trying to differentiate our sv sets from random genomic coordinates can do a very good job separating these two sets, but the model is likely to misclassify other candidate svs not in the “validated/assembled” call sets . because our one-class model does not rely on biased “validated/assembled” call sets, we expect our one-class model to be more generalizable to other types of svs by selecting annotations for which atypical values are usually associated with svs.

our methods, which classify based on evidence from multiple technologies, are complementary to the recently published metasv method  <cit> , which integrates svs using multiple bioinformatics methods, and the parliament method  <cit> , which generates candidate svs using multiple technologies and bioinformatics methods, and then uses a pacbio/illumina hybrid assembly to determine whether the candidate svs are likely to be true. similar to parliament, in the characterization of the performance of the lumpy tool  <cit> , the authors developed a high-confidence set that had breakpoints supported by long reads from pacbio or moleculo. in addition to using svviz  <cit>  to visualize and determine the number of reads supporting the alternate, we also combine the support from multiple sequencing technologies in a robust machine learning model.

RESULTS
to assess the utility of our classification methods, we compiled four whole genome sequencing datasets for coriell dna sample na <dig> . we used two deletion call sets from personalis and the  <dig> genomes project totaling  <dig> unique deletions, as well as  <dig> assembly-based breakpoint-resolved insertions. moreover, we generated several likely non-sv call sets with different size distributions and sequence contexts . we first generated annotations for the candidate sv and non-sv call sets from the four sequencing datasets. we used hierarchical clustering to show that svs generally cluster separately from non-svs using these annotations, and that svs cluster into several different types of deletions. we used one class classification methods to classify calls as high-confidence svs or uncertain. to confirm the accuracy of the high-confidence svs, we present pcr validation results for the personalis deletions as well as a comparison to mother-father-daughter trio consensus-based calls from metasv  <cit> .table  <dig> description of ngs data sets from coriell dna sample na12878

platinum genomesa
broad instituteb
mount sinai, nyc
illuminad
data sources:


a
http://www.illumina.com/platinumgenomes/



b
ftp://ftp.broadinstitute.org/pub/crd/na12878_clones/



c
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/working/20131209_na12878_pacbio/



d
ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/integrated_sv_map/supporting/na12878/moleculo




annotations are generated from multiple technologies for candidate svs
to assess the evidence for any candidate sv without the need to design primers for validation experiments, we developed svclassify to quantify annotations of aligned reads inside and around each sv . we generated  <dig> to  <dig> annotations  for each of the sv calls as well as likely non-sv regions from four aligned sequence datasets for na <dig> using svclassify. some of the annotations, such as depth of coverage , could clearly distinguish most personalis “gold” deletions from random regions by themselves. although annotations such as coverage can be used by themselves to classify most personalis deletions, additional annotations increase confidence that the deletion is real and not an artifact . in addition, other annotations are necessary to classify other types of svs like inversions and insertions that may not have abnormal coverage. therefore, we developed unsupervised and one-class supervised machine learning models to combine information from many annotations for clustering and classification .fig.  <dig> annotations are generated for each sv for five different regions in and around the sv: left flanking region , left middle flanking region , middle regions based on sv coordinates , right middle flanking region , and right flanking region 

fig.  <dig> depth of coverage distribution for personalis deletion calls  and random regions . see original data at https://plot.ly/337/~parikhhm/


fig.  <dig> flowchart of analytical approach to classify candidate svs into likely true or false positives. the subset of  <dig> annotations was chosen for illumina paired-end data  to reduce the number of annotations used in the model to those that we expected to be most important for clustering calls into different categories. the one-class model uses only the  <dig> random sites for training, and it assumes that sites with annotations unlike most of these random sites are more likely to be svs



for annotations for one-class classification, we included the coverage inside the sv region  because coverage should be directly proportional to copy number. we included soft-clipping in the flanking regions because we usually expect only part of reads to map when they cross the sv breakpoints. we included insert size in the flanking regions because we expect insert size to be larger than normal for deletions and smaller than normal for insertions. we included the proportion discordantly mapped reads because this can indicate either a large insert size or a mobile element insertion. we included the proportion of reads for which the other end is not mapped because this could indicate a novel insertion. for pacbio, we included the difference between inserted and deleted bases because the reads were often sufficiently long to be mapped across a deleted or inserted region and abnormally high insertion or deletion rate can indicate an insertion or a deletion event.

for clustering, we included additional annotations that may not distinguish true svs from non-svs but could be helpful for clustering different types of true svs. for example, low mapping quality might indicate a homozygous deletion, a mobile element deletion, or a biased non-sv region. also, sv size could be useful to cluster different types of svs. abnormally low or high coverage in the flanking regions could indicate imprecise breakpoints, regions with bias, or a much larger sv encompassing the region. we also only used for clustering the reference-specific or sample-specific annotations like repeatmasker regions, gc content, and number of heterozygous and homozygous snps. we did not include these in the one-class model because they are not technology-specific, so they could not simply be used as part of our heuristics requiring evidence from multiple technologies.

hierarchical cluster analysis separates random regions from multiple sv types
to understand the types of sv calls in the validated/assembled deletion sets and how they segregate from random genomic regions, we first performed unsupervised machine learning using hierarchical clustering with a manually selected subset of  <dig> to  <dig> annotations from svclassify, depending on the technology . this subset of annotations was chosen to reduce the number of annotations used in the model to those that we expected to be most important for clustering calls into different categories. we decided to focus our analyses on eight major clusters, which are visualized as a tree  in fig. 4a and with multidimensional scaling in figs. 4b and 4c. five of the clusters  were predominantly  svs, two clusters  were predominantly  non-svs, and one cluster  was 40 % svs and 60 % non-svs. the label  associated with each site was not provided to the clustering method, and yet the clusters showed a good separation of svs from non-svs based entirely on the annotation values. to ensure the  <dig> random regions sufficiently represented non-svs, we also generate random regions matching the size distribution of the personalis deletions, as well as random sines, lines, and ltrs. it is promising that even the randomly selected sines, lines, and ltrs generally segregate with the random genomic regions even though they are from regions of the genome that are difficult to map.fig.  <dig> hierarchical clustering results using l <dig> distance and ward’s method shown as  a dendrogram and  in multi-dimensional scaling plots.  the horizontal dotted red line shows the cut-off at a cluster dissimilarity index of about  <dig>  which results in  <dig> clusters. the clusters are number  <dig> to  <dig> from left to right, with  <dig> and  <dig> containing primarily non-svs,  <dig> containing a mixture of svs and non-svs, and  <dig>   <dig>   <dig>   <dig>  and  <dig> containing different types of deletions .  multidimensional scaling plots for visualizing the  <dig> clusters. we use a  <dig> dimensional representation of the data space which associates  <dig> mds coordinates to each site, one for each dimension.  plot of mds- <dig> against mds- <dig>  which clearly separates cluster  <dig> .  plot of mds- <dig> against mds- <dig>  in which the different types of svs are generally well-separated from each other and from non-svs



we further compared the annotations of these  <dig> clusters to understand whether they represent different categories of svs and random regions. clusters  <dig> and  <dig> contain close to 99 % non-svs, but cluster  <dig> generally contains larger sites than cluster  <dig>  cluster  <dig> is a mix of 60 % non-svs and 40 % svs, and sites in cluster  <dig> generally have a coverage between the normal coverage and half the normal coverage, and more sites have lower mapping quality, repetitive sequence, and high or low gc content. further subdivisions of cluster  <dig> might divide the true svs from non-svs.

 <dig>  % of sites in clusters  <dig>   <dig>   <dig>   <dig>  and  <dig> are from the personalis and  <dig> genomes gold sets, but the clusters contain different types of svs. clusters  <dig>   <dig>   <dig>  and  <dig> generally contain reads with lower mapping quality inside the sv, though the low mapping quality could arise from a variety of sources . clusters  <dig> and  <dig> appear to be true deletions of alu elements, since sites in these clusters are ~300 bps, are annotated as sines, lines, or ltrs by repeatmasker, have high gc content, and have low mapping quality. cluster  <dig> sites are primarily heterozygous alu deletions since they have about half the typical coverage, and cluster  <dig> sites are primarily homozygous alu deletions and a small fraction of other homozygous deletions because they contain less than half the typical coverage. all  <dig> sites in cluster  <dig> are from personalis and  <dig> genomes, and appear to be mostly larger homozygous deletions , and they have lower than half the normal coverage, low mapping quality, and more discordantly mapped reads. 86 % of sites in cluster  <dig> are from  <dig> genomes and appear likely to represent mostly true homozygous deletions with imprecise breakpoints that are too narrow, since the left and right flanking regions, in addition to the region inside the putative sv, have low coverage less than half the typical coverage.  <dig>  % of sites in cluster  <dig> are from personalis and  <dig> genomes, and they appear to be predominantly heterozygous deletions in relatively easier parts of the genome with high mapping quality. these results are summarized in table  <dig> table  <dig> analysis of  <dig> clusters from hierarchical cluster analysis, including the numbers of sites from each call set and a description of the predominant types of sites in each cluster



more sophisticated versions of our clustering approach are available. parametric approaches include gaussian mixture modeling, but there are also nonparametric mixture modeling approaches available. however, exploratory analyses showed that at best only a marginal improvement is realized using such more advanced methods for our datasets.

one-class classification of candidate svs using l <dig> distance integrates information from multiple technologies
we next developed a one-class classification model to classify candidate sites as high-confidence svs or uncertain. this one-class model uses only the  <dig> random sites for training, and it assumes that sites with annotations unlike most of these random sites are more likely to be svs. as shown in additional file 5: table s <dig>  we only used a subset of the annotations from the unsupervised hierarchical clustering because atypical values for some annotations  do not necessarily indicate that an sv exists in this location . the number of annotations used ranged from  <dig> for pacbio to  <dig> for illumina paired-end because certain annotations like insert size do not apply to all technologies .

results from the l <dig> distance one-class classification are summarized using roc curves. five different roc curves are shown in fig. 5a-5b, one from each classifier using one of the four data sets and one classifier based on all datasets combined. the classifier based on all datasets combined performs the best with platgen  alone being a close second. roc curves for the ensemble classifiers, based on the four l <dig> classifiers using each of the four data sets separately, are shown in fig. 5c-5d. four different ensemble classifiers are considered based on four different ways of combining the results from the individual classifiers. a typical ensemble classifier will classify a site as sv if k or more of the individual classifiers make an sv call. here k can be  <dig>   <dig>   <dig>  or  <dig>  the results show that using k =  <dig> provides the best ensemble classifier with k =  <dig> being a close second. performance is similar for the k =  <dig> classifier and all datasets combined, and we use k =  <dig> for our final results because we expect requiring evidence from  <dig> datasets will be more robust. for k =  <dig>  we calculated the proportion ρ of random sites that are closer to the center than each candidate site. we stratified candidate sites into those with ρ <  <dig> ,  <dig>  < ρ <  <dig> ,  <dig>  < ρ <  <dig> , or ρ >  <dig> , as shown in table  <dig> fig.  <dig> roc curves for one-class classification using the l <dig> distance, treating the  <dig> random regions as negatives and the personalis or  <dig> genomes calls as positives.  roc curves for one-class models for each dataset separately and for all combined for the personalis validated deletion calls.  roc curves for one-class models for each dataset separately and for all combined for the  <dig> genomes validated deletion calls.  roc curves for one-class model requiring  <dig> or more,  <dig> or more,  <dig> or more, or all  <dig> technologies to have high classification scores for the personalis validated deletion calls.  roc curves for one-class model requiring  <dig> or more,  <dig> or more,  <dig> or more, or all  <dig> technologies to have high classification scores for the  <dig> genomes validated deletion calls. the  <dig> or more classification method is used to produce the final high-confidence svs in this work. the horizontal axis shows the false positive rate  and the vertical axis shows the corresponding true positive rate . see original data at https://plot.ly/345/~parikhhm/, https://plot.ly/353/~parikhhm/, https://plot.ly/361/~parikhhm/, and https://plot.ly/369/~parikhhm/




to confirm our choice of  <dig> random regions for training, we compared the scores of the  <dig> random regions to the  <dig> different random regions with a size profile matching the personalis svs. we found that the distributions of scores were very similar, with  <dig> of  <dig> personalis random regions having ρ >  <dig> , close to the 1 % expected. similarly,  <dig> of  <dig> random sine, line, and ltr regions had ρ >  <dig> , close to the 1 % expected. to assess the performance of the classifier on random regions, we also found the distribution of ρ scores for the random personalis regions when assessed against the  <dig> random regions, and the distribution of scores was flat as expected .

one-class classification of candidate svs using svm gives similar results to l <dig> classifier
to compare to an alternative distance measure and method for one-class classification, we also developed a one-class svm model. we found that results were generally similar between the l <dig> one-class results and the svm one-class results in terms of roc curves . additional file 11: table s <dig> gives the concordance/discordance matrix for predictions from the l <dig> and svm one-class classifications for selected values of ρ. agreement between the two methods is 84 % with ρ >  <dig> , 98 % with ρ >  <dig>  and 99 % with ρ >  <dig> , on personalis validated/assembled set. the high agreement between svm and l <dig> at ρ >  <dig>  suggests that our one class classification method is robust to the type of model. we further examined the  <dig> sites consistently identified with only svm and  <dig> site consistently identified with only l <dig> that had low ρ  with one method and ρ >  <dig>  with the other method. we found that these were from difficult regions of the genome, such as telomeres, high coverage regions, and low mapping quality regions, so they are filtered from our final high-confidence calls. however, similar comparisons of predictions on  <dig> genome set with l <dig> and svm ensemble classifiers suggest that the l <dig> classifier has better efficiency in predictions on  <dig> genome set and better agreement on different technologies. therefore we use the simpler l <dig> method.

manual inspection of one-class results verifies accuracy of our classifier
we randomly selected a subset of sites from each call set in each selected ρ value range from table  <dig> for manual inspection. in general, personalis and  <dig> genomes sites with high ρ values were very likely accurate and mostly homozygous, while sites with lower ρ appeared to be questionable, small, and/or heterozygous. most of the spiral genetics insertions had very high ρ, indicating a true sv is likely in the region.

for personalis, we inspected  <dig> randomly selected sites with ρ >  <dig> , and all appeared to be accurate . only  <dig> of the  <dig> sites appeared likely to be heterozygous, since homozygous deletions generally are more different from random regions than heterozygous deletions.  <dig> out of  <dig> heterozygous sites had  <dig>  < ρ <  <dig> , whereas all  <dig> homozygous deletions had ρ >  <dig>  except for one small 52-bp deletion.  <dig> of the homozygous deletions had ρ >  <dig> . also, all  <dig> of the randomly selected personalis sites with  <dig>  < ρ <  <dig>  were likely to be true heterozygous deletions, and none were homozygous . there were only  <dig> sites with ρ <  <dig>  in the personalis set , and these were a mixture of likely true but very small deletions and other potential deletions that were difficult to determine whether they were true or artifacts since they were only supported by a small number of reads. therefore, we do not include these calls with ρ <  <dig>  in our final high-confidence set.

for  <dig> genomes, we similarly inspected  <dig> randomly selected sites with ρ >  <dig> , and all appeared to be accurate except for one in a low complexity region, which had few supporting reads in svviz. only  <dig>  of the sites with ρ >  <dig>  had ρ >  <dig> , in contrast to 65 % of the personalis calls.  <dig> of the  <dig> sites with ρ >  <dig>  were likely to be homozygous deletions. one likely true heterozygous deletion had ρ >  <dig> , and the remaining  <dig> sites with  <dig>  < ρ <  <dig>  appeared likely to be true heterozygous deletions except for one in a low complexity region . also,  <dig> of the  <dig> randomly selected  <dig> genomes sites with  <dig>  < ρ <  <dig>  were likely to be true heterozygous deletions, and none were homozygous . the other  <dig> sites contained 17 % and 58 % low complexity sequence and 68 % and 66 % gc content, and they appeared likely to be erroneous calls since no reads aligned to the alternate allele for any technology using svviz .  <dig> of the  <dig> randomly selected  <dig> genomes sites with  <dig>  < ρ <  <dig>  were smaller than 100 bps,  <dig> were likely to be true heterozygous deletions, and none were homozygous .  <dig> of the  <dig> randomly selected  <dig> genomes sites with ρ <  <dig>  were smaller than 110 bps and were possibly true heterozygous deletions, and none were homozygous . in general, the  <dig> genomes calls have lower ρ scores than the personalis calls because the personalis calls contain a higher fraction of homozygous deletions, fewer very small deletions, and are all breakpoint-resolved.

all of the complex insertions from spiral genetics had ρ >  <dig> , indicating that they are likely to be true svs. upon manual inspection of the svviz results ,  <dig> had evidence in all  <dig> technologies for a homozygous insertion,  <dig> had evidence in all  <dig> technologies for a heterozygous insertion, and  <dig> were inconsistent in terms of zygosity across the  <dig> technologies. the reason for the discordance between technologies for the  <dig> discordant sites is not always clear, but it appears that some are likely to be real svs with different breakpoints. for example, an insertion is called at 1: <dig> , <dig> with a length of 352 bp, but appeared likely to be much larger.

most candidate sites with ρ >  <dig>  appear to be true, but a few of the manually inspected sites appeared to be inaccurate or to have incorrect breakpoints. therefore, we further refined our final call set by using svviz to map reads to the reference or predicted alternate alleles, and we included only sites with at least  <dig> reads supporting the alternate allele in at least  <dig> of the  <dig> datasets. this filtered 13 % percent of the calls, leaving  <dig> deletions and  <dig> insertions for which we have high confidence. these calls are publicly available at ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/technical/svclassify_manuscript, and we will continue to update these with additional call sets as we further develop our methods.

pcr validates high-confidence svs
to obtain estimates of accuracy of the personalis deletion calls, we performed experimental validation for some of the calls. only  <dig> of  <dig> calls met the criteria for designing primers,  <dig> primer pairs failed, and in one case we were unable to make a call. we were able to validate  <dig> of personalis’ deletions with exact breakpoints  out of the  <dig> deletions that we could test. a 39th case was off by 44 bps on one side and the last case was a false positive call. all homozygous calls  were confirmed by the validation. only  <dig> out of  <dig> heterozygous calls had the correct zygosity call. of the heterozygous calls with incorrect zygosity,  <dig> were actually homozygous,  <dig> could not be determined by the validation and  <dig> was not a deletion. the remaining cases did not have a zygosity call, of which  <dig> were homozygous and  <dig> were heterozygous. all of the validated calls had ρ >  <dig> .  <dig> of the  <dig> sites that could not be pcr-validated also had ρ >  <dig> . the fifth site , which was the only likely false positive, had ρ =  <dig>  and is bounded by approximately 350 bp regions that only differ by a 9 bp indel.

trio analysis with metasv confirms most high-confidence svs
given the rate of de novo mutation is low  <cit> , most svs of an individual are expected to be mendelian consistent. we went on to assess the quality of our high-confidence svs by validating it with trio analysis, involving the child , her father  and her mother . sequences were retrieved from the illumina platinum genomes repository with an average sequencing depth of approximately 50x. metasv  <cit> , a recently published sv caller integrating multiple orthogonal sv detection algorithms, was used to generate three call sets for the trio individuals. for highest quality, only deletion calls > =100 bps were considered. calls detected by two or more algorithms in metasv with different detection methods were deemed as pass and regarded as high-quality  <cit> . there were  <dig>   <dig>  and  <dig> deletion calls for na <dig>  na <dig>  and na <dig> respectively, of which  <dig>   <dig>  and  <dig> were pass calls.

to validate the  <dig> high-confidence deletion svs from svclassify, we selected the  <dig> deletions > = 100 bps and matched them with the bina call sets. we used a 50 % reciprocal overlapping strategy for the matching. a call from svclassify was deemed as “validated” if one of the following criteria was met: level  detected in any of the two parents from the bina call sets; level  detected in the child as a pass from the bina call sets; and level  reported and validated in previous literatures on the child  <cit> . as shown on additional file 20: figure s <dig>  98 % of the high-confidence deletions were confirmed by the level  <dig> validation, i.e., also detected in the parents. there were an additional  <dig>  % confirmed by the level  <dig> validation, i.e., detected by at least two orthogonal algorithms. level  <dig> validation confirmed an extra  <dig>  %, resulting in a total validation rate of  <dig>  %. the extremely high validation rate indicates high quality of the high-confidence sv call set from svclassify.

performance of svclassify with 30x coverage dataset
to understand the effect of downsampling on the classification accuracy, we performed downsampling of the platinum genomes bam file to 30x mean coverage. as shown on additional file 21: figure s <dig>  even with 30x coverage, at a 5 % false positive rate, > 97 % of the personalis calls were classified as true positives, but this is lower than the >99 % of calls classified as true positives at a 5 % false positive rate with 200x coverage.

CONCLUSIONS
high-confidence sv and non-sv calls are needed for benchmarking sv callers. to establish high-confidence, methods are needed to combine multiple types of information from multiple sequencing technologies to form robust high-confidence sv and non-sv calls. therefore, in this work we developed methods to classify svs as high-confidence based on annotations calculated for multiple datasets. our classification method gives the highest scores to svs that are insertions or large homozygous deletions, and have accurate breakpoints. deletions smaller than 100-bps often have low scores with our method, so other methods like svviz are likely to give better results for very small svs. homozygous deletions generally receive the highest scores because they have annotations most unlike random regions of the genome. breakpoint-resolved deletions generally receive higher scores because reads near the breakpoint have distinct characteristics such as clipping and insert size that our method uses to classify svs. we produce a set of  <dig> high-confidence deletions and  <dig> high-confidence insertions with evidence from  <dig> or more sequencing data sets. these sets of svs are likely biased towards easier regions of the genome and do not contain more difficult types of svs. however, they can be used as an initial benchmark for sensitivity for deletions and insertions in easier regions of the genome.

in this work, we use data from multiple whole genome sequencing technologies to develop high-confidence svs for benchmarking, and it is important to understand strengths and weaknesses of this approach. alternative approaches might include targeted experimental validation and simulation of sv events. for reference materials that are characterized by multiple high-coverage whole genome sequencing technologies, we expect our approach to work well, though it may still be useful to confirm some svs with targeted methods. it is sometimes difficult or impossible to target svs in difficult regions of the genome, so it is important to understand that this type of confirmation may not help assess the accuracy of the most difficult svs. similarly, simulated svs in random regions of the genome may not represent the true locations of svs, which can often occur in challenging regions of the genome and may have sequencing errors that are not simulated. while svs with more accurate breakpoints are more likely to have high scores with our method, one or both breakpoints may be inaccurate for some high-confidence svs, and even when accurate it is often possible to represent svs in multiple ways in repetitive regions. it is also critical to understand that the high-confidence svs in this manuscript likely represent a subset of svs biased towards those that are easier to detect and are generally moderate sized insertions and deletions. in addition, although multiple sv callers and analysis groups generated the candidate calls, they may be biased towards existing callers and short-read data. therefore, sensitivity measured with respect to our high-confidence svs or most other high-confidence svs is likely an overestimate of the sensitivity of any method for all svs in the genome. further work is needed to develop more comprehensive high-confidence sv call sets.

the motivation for building the one-class model using random regions rather than true svs is that we do not have unbiased sets of true svs that we could use for training. unfortunately, current “true sv” call sets tend to be biased towards types of svs that are easier to call and in easier regions of the genome. for example, a model based on moderate-sized deletions is unlikely to be useful for insertions, duplications, large or small deletions, or svs in more difficult parts of the genome. a one-class model based on true svs also may result in overfitting, which is mitigated by using random regions because the model is generated independent of the true svs used in validation. a limitation of using random regions is that they may not perfectly represent false positive svs that are generated by callers. further work will be needed to understand the profiles of false positive svs and how they compare to random regions, since we did not have sufficient false positives to test this in this work.

the primary goal of this work is to develop methods to form high-confidence sv calls from candidates generated from multiple technologies and calling methods. however, future work might also examine using these methods to classify candidate sv calls from multiple callers using a single technology. the utility of svclassify in this case would be to generate and use consistent annotations for all candidate svs. while multiple technologies are likely to result in more robust classifications, even a single technology often separates most of our high-confidence svs from random regions.

our unsupervised clustering methods also show promise for classifying candidate svs into different types and potentially classifying more difficult types of svs. seven of the eight clusters obtained from an unsupervised hierarchical cluster analysis using l <dig> distances were relatively pure clusters consisting of either mostly svs or mostly non-svs. the overall successful separation of the svs from the non-svs by the unsupervised analysis suggests that the annotations for svs and non-svs occupy more or less disjoint regions in the data space. since each cluster contains a different type of sv or non-sv, future work might include further investigation of these clusters and sub-clusters to understand their meaning. in addition, we plan to apply these clustering methods to additional types of svs and develop more sophisticated classification methods that would place new candidate svs in one of these categories of different types of true or false positive svs.

we plan for the methods developed in this work to form a basis for developing high-confidence sv and non-sv calls for the well-characterized nist rms being developed by the giab. in this work, we apply these methods to produce a set of high-confidence deletions and insertions with evidence from multiple sequencing datasets, and we plan to continue to develop these methods to be applied to more difficult types of svs in more difficult regions of the genome. we also plan to incorporate calls from methods merging multiple callers, such as metasv  <cit> . for example, there were  <dig> high-quality metasv pass calls for na <dig> which were also mendelian consistent. finally, we hope to incorporate statistics from other tools, such as parliament  <cit>  and svviz  <cit> , in our machine learning models.

