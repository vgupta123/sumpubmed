BACKGROUND
there are two major methodological problems that deal with the issue of stochastic dependence between gene expression signals in microarray data. the first arises naturally when adjustments for multiplicity of tests are made by pooling across genes  in an effort to find differentially expressed genes in two-sample comparisons. the empirical bayes methodology in the nonparametric  <cit>  and parametric formulations  <cit> , and closely related methods exploiting a two-component mixture model  <cit>  represent typical examples. the common feature of such methods is that a test statistic  is first calculated for each gene to account for biological variability and then all the statistics  are pooled together and treated as a sample from which to estimate the sampling distribution of this statistic, the false discovery rate , q-values, etc. the same kind of pooling is typically used in maximum likelihood inference from microarray data  <cit>  and some other methods of testing for differential expression of genes.

in all such approaches, the stochastic dependence between gene expression values or test statistics is a nuisance that hinders their application. the independence assumption is frequently invoked when building a theoretical foundation for a particular method of statistical inference. some authors  allow for dependence between differentially expressed genes while assuming stochastic independence of those genes that do not change their expression between the two conditions under study. the biological rationale for such a hypothesis is unclear, because the normally functioning genes are involved in numerous biochemical pathways much like the altered ones.

the stochastic dependence between expression levels and thus between the associated test statistics is really a serious problem. it may cause high variability of statistical estimators and even deteriorate their consistency. to obtain theoretical results it is frequently assumed that weak or almost sure convergence holds for an empirical distribution function constructed from the data pooled across genes . however, this assumption is diffcult to validate biologically so that the required convergence to the true distribution function is always questionable; it may or may not be the case depending on the type and strength of stochastic dependence.

storey  <cit>  advocates the assumption of weak dependence between test-statistics when discussing some concerns raised in the paper by ge, dudoit, and speed   <cit> . it is worth quoting his line of reasoning at length:

"i hypothesize that the most likely form of dependence between the genes encountered in dna microarrays is weak dependence, and more specifically, "clumpy dependence"; that is, the measurements on the genes are dependent in small groups, each group being independent of the others. there are two reasons that make clumpy dependence likely. the first is that genes tend to work in pathways, that is, small groups of genes interact to produce some overall process. this can involve just a few to  <dig> or more genes. this would lead to a clumpy dependence in the pathway-specific noise in the data. the second reason is that there tends to be cross-hybridization in dna microarrays. in other words, the signals between two genes can cross because of molecular similarity at the sequence level. cross-hybridization would only occur in small groups, and each group would be independent of the others."

this hypothesis does not seem plausible from a biological standpoint because of the pleiotropic character of gene function: one gene participates in multiple molecular pathways. however, the possibility that it may approximately be true for all practical purposes cannot be ruled out. there are two key words in the above quotation: "small groups" and "weak dependence". whether or not such groups are small and stochastic dependence is suffciently weak can be deciphered only from real world data. to the best of our knowledge, no attempt has been made so far to systematically study dependence structures in microarray data using large data sets. in this connection we would like to continue quoting from  <cit> : "many assumptions that have been made for modeling microarray data have yet to be verified. hopefully evidence either for or against these assumptions will emerge... gds have stressed the dependence between the genes... i leave it as a challenge to them to provide evidence from real microarray data that the aforementioned assumptions do not hold. i have not been able to find it myself". in the present paper, we take the first step in this direction by conducting an empirical study of the correlations between test statistics associated with different genes.

the second research area where the dependence between gene expression levels plays a crucial role is the discovery  of molecular pathways and networks from microarray data  <cit> . a popular approach to pathway reconstruction is based on the sample correlation coeffcient or mutual information measures that are deemed to characterize interactions between genes via their products. these measures of interaction are computed from gene expression values observed across various experimental conditions. the snag here is that strong correlations in the raw  expression data may be induced by an array-specific technological noise, thereby producing numerous false-positive edges in the corresponding graph representing the underlying structure of a given pathway or network. however, if the data are normalized before the analysis, then the correlation structure of expression signals may be partially destroyed by the normalization procedure so that many edges in the resultant graph may be missing. the same applies to clustering techniques that utilize information on pairwise dependencies between the genes. the problem is less pressing where causal inference is possible from gene perturbation experiments. although the present paper does not have a direct bearing on such settings, our results suggest that associative networks built on microarray data alone may have little to do with biological reality. the problem merits careful investigation in order to make the reverse engineering of this type more credible.

the present paper is focused on the correlations between test-statistics associated with expression signals produced by each gene and the effects of normalization procedures on these correlations. we limit our consideration to the t-statistic which is the most popular choice in microarray data analysis. normalization is intended to mitigate the effect of technological noise that is inherent in microarray data. normalization procedures tend to reduce the variability of original microarray data , however no study has been carried out to assess the effect of such procedures on the correlation structure of microarray data in general and the correlation of t-statistics in particular. in a methodological study such as ours, it is a great advantage to have access to a large data set involving hundreds of arrays. we used the st. jude children's research hospital  database on childhood leukemia which falls into this category. computer simulations provide the necessary, albeit not very realistic, control where the actual model is known and arbitrarily large samples can be generated for testing various methodologies.

RESULTS
the design of our study is presented in the methods section. this design allows us to compute the t-statistics  for each gene and each pair of subsamples. this computation results in  <dig> values  of the t-statistic associated with each gene. then we compute the sample correlation coeffcients between the t-statistics thus obtained for every pair of genes. the resulting coeffcients are summarized in the form of a histogram. we interpret such histograms as pertinent summary characteristics and not as estimators of some population distribution densities. we also look at pre-selected individual genes to determine the range of their correlation with all other genes. this range can be characterized by the number of gene pairs formed by a given gene with the correlation coeffcient exceeding some threshold level. we adopt the value of  <dig>  as such a threshold.

using these tools we attempt to answer the following questions:

• what is the  correlation structure of the t-statistic in a large population of genes?

• what is the impact of normalization procedures on this structure?

• what is the impact of normalization procedures on the number of highly correlated pairs formed by a given gene?

the effects of three normalization procedures  are shown in figures 1b–1d. figure 1e presents an ideal case where the t-statistics were obtained from independent normally distributed data  produced by simulations . in this case, the proportion of gene pairs with correlation coeffcients greater than  <dig>  is only  <dig> %. while the normalization procedure geo destroys a large proportion of correlation, the procedures rank and quant outperform it as far as the reduction of between-gene dependence is concerned. the effects of the latter two procedures are largely similar. the procedure rank reduces the proportion of correlation coeffcients greater than  <dig>  to  <dig> %, while the procedure quant reduces this proportion to  <dig> %. for comparison, this indicator is equal to 14% for geo. thus the procedure rank has the strongest effect on the correlation structure. figure  <dig> in the additional material files  shows essentially the same effect for randomly selected non-overlapping pairs of genes.

the effect of normalization on the between-gene correlations observed in the simulated data simu2n and simu <dig> is stronger than that in the case of biological data . this can be seen in figures  <dig>   <dig>  where only the results for the quantile normalization are shown. again, if we look at the proportion of gene pairs with correlation coeffcients greater than  <dig> , this indicator equals  <dig> % for simu2n and  <dig> % for simu <dig>  the effects of geo and rank are displayed in figures  <dig>   <dig> included in the additional material files . the stronger effect of geo on the correlation structure of the simu2n data as compared to the sjcrh data comes as no surprise because the noise is simulated as an array-specific random effect, for which a heuristic justification of the geo procedure is possible  <cit> . the normalization procedures exert their effect both on the correlation induced by the noise and on the true correlation that reflects interactions between gene products.

the effect of the quantile normalization for the simu3n, shown in figure  <dig> in the additional material files , deserves special discussion. recall that each gene in the data set simu3n correlates only with a distinct group of genes termed a clump. even if the genes involved in the same clump are heavily correlated, the average  correlation coeffcient may still be quite low. when a uniformly distributed multiplicative random noise is imposed on each array, the genes pertaining to different clumps become highly correlated. the noise strengthens the intra-clump correlation as well. recall that the clumpy structure of simulated data serves as a simplistic model of gene interactions within distinct pathways. as seen in figure  <dig> , the normalization procedure quant is not nearly as effective as in the case of the simu2n data. this procedure does not eradicate the overall correlation between genes in the simu3n data. in this sense, the effects of normalization seen in the simu3n and in real biological data look similar.

another way of studying such effects is to look at the number of pairs characterized by a relatively high correlation with a pre-selected gene. tables  <dig>   <dig>   <dig> present the results for  <dig> genes that produce large numbers of highly correlated . these initiator genes were identified through each of the data sets  under study. the final column in every table gives the number of highly correlated pairs formed by a given gene before normalization. all the selected genes form such pairs with the overwhelming majority of genes. we term this type of dependence the long-range correlation. the number of highly correlated gene pairs remaining after a given normalization procedure serves as an indicator of its effciency.

consider first the results obtained with simulated data. each of the twenty initiator genes selected from simu2n form exactly  <dig>  highly correlated pairs. when applied to the simu2n data, the normalization procedures rank and quant bring this number down to  <dig> on average . the variability in the size of this set of genes is low. for example, the number of highly correlated genes ranges from  <dig> to  <dig> after the application of the quant procedure. both procedures indiscriminately reduce the true  correlation and its spurious  counterpart. although less effective, the procedure geo does a similar job.

the results for the simu3n data are different . while the number of highly correlated gene pairs tends to decrease significantly for each of the twenty initiator genes, the size of this effect depends on the group of genes from which the initiator gene was chosen. this increases the variability of the number of highly correlated pairs remaining after normalization. for the quant method the range is from  <dig> to  <dig>  showing that the remaining correlation extends far beyond the specified clumpy structure.

we then selected  <dig> initiator genes in the sjcrh data set representing real biological data. the number of highly correlated pairs formed by these genes before normalization ranges from  <dig>  to  <dig> , which is a very narrow range indeed. as is seen in table  <dig>  the procedure geo does not destroy the correlation effectively; it leaves huge numbers  of highly correlated gene pairs. the rank normalization results in much smaller numbers of highly correlated genes that range from the lowest of  <dig> to the highest of  <dig> . the average is  <dig> , which is about twice as much as we get from any normalized simu2n data. the variability is also very high, resembling a clumpy effect seen in the simu3n set. we do not consider this similarity as evidence for a clumpy structure of microarray data, but the results in table  <dig> suggest that, if such a structure exists, an average clump should be expected to involve at least an order of magnitude more genes than the clump size postulated by storey  <cit> .

another interesting finding in table  <dig> is that the quantile normalization tends to leave more highly correlated genes in comparison to the rank normalization. this is contrary to our expectations based on the comparisons of correlation histograms reported above. the effect of the quant is also more variable than that of the rank, which is another dissimilarity of practical importance. leaving aside the fact that the rank procedure is applied to gene expressions, while the quant works at the probe feature level, the difference between the two normalization methods is that we replace entries in an array by their ranks in the former case and by  in the latter. recall that  is the average of entries having the same rank over all arrays. obviously, the quant preserves more quantitative information in the data than does the rank procedure. this explains why the result of the rank normalization is less variable.

the effect of the normalization quant on the distribution of the t-statistics across the genes for the actual and simulated data is shown in figures  <dig>   <dig>   <dig>   <dig> included in the additional material files . from figures  <dig> and  <dig>  it is clear that, when applied to the simulated data simu <dig> and simu2n, this procedure makes the distribution of t-statistics similar to that in the ideal case shown in figure  <dig> . however, the effect appears to be somewhat less satisfactory with real data, especially in the tail regions of the resultant distribution of t-statistics.

the results shown in this section are obtained with a single initial random split of the pooled set of arrays into two groups. we have conducted several such splits in this study. all the above-described effects are highly reproducible, and reporting the results for other splits in the paper is not warranted.

discussion
it follows from our observations that normalization procedures are capable of destroying a significant part of correlations between gene expression signals and associated test-statistics. in doing so, they affect both the spurious correlation induced by the noise and the true correlation that reflects gene interactions. the clumpy structure  of the simu3n data set is more resistant to this effect than the simu2n data. this is even more so for real biological data. the weaker effect of normalization seen in the sjcrh data indicates that the actual noise structure may be more complicated than assumed in the simulation studies . a clumpy structure of gene expression signals may also play a role in this phenomenon. this observation explains why it is so diffcult to remove correlations from the data.

the destructive effect of normalization procedures on pairwise correlations in microarray data is good news for the methods of statistical inference that resort to "pooling across genes". however, it remains unclear whether or not the remaining correlation may still be substantial enough to invalidate such methods by affecting important properties of statistical estimators and tests. the problem invites further investigation. however, we would like to present an experiment specially designed to address the consistency question mentioned in the background section.

to this end, we applied the following algorithm to the sjrch data:

 <dig>  select randomly  <dig> genes and compute the arithmetic  mean of the t-statistics across these genes for each pair of subsamples.

 <dig>  compute the standard deviation of the sample mean across the  <dig> pairs of subsamples.

 <dig>  select randomly  <dig> from the remaining genes and compute the arithmetic mean for the  <dig> genes for each pair of subsamples.

 <dig>  compute the standard deviation from the sample means resulted from the previous step.

 <dig>  continue until the set of all genes is exhausted.

 <dig>  plot the estimated standard deviation of the sample mean as a function of the number of genes involved in each step of the algorithm.

 <dig>  repeat the procedure k times to generate k trajectories of the standard deviation of the sample mean.

the results of one such experiment are given in figure  <dig>  it is known that the sample mean is an unbiased and consistent estimator for the true mean value in the case of independent and identically distributed observations. this case is represented by curve  <dig> generated by simulations. it is clear that the standard deviation decreases very rapidly and tends to zero with increasing the number of genes. however, the same is not true for the biological data. for the raw data, the standard deviation does not show a distinct tendency to decrease . when the data are normalized using the quantile normalization procedure, the standard deviation first drops and then stabilizes at an approximately constant level, no matter how many  genes are involved in its estimation . this is clearly the effect of  correlation between the t-statistics associated with different genes. the pattern seen in figure  <dig> was highly reproducible across k =  <dig> experiments with different random starts. if the standard deviation of an unbiased estimator tends to zero, this estimator is consistent. this is the case for curve  <dig> but not quite so for curve  <dig>  while not a rigorous disproof of consistency of the sample mean in this case, the pattern seen in curve  <dig> suggests that the estimator is likely to converge to a random variable  rather than to the true parameter to be estimated. this is definitely not a good sign for estimation procedures based on pooling across genes such as those built in the empirical bayes methodology.

the observed effect of normalization procedures is definitely bad news for the associative network reconstruction from gene expression data. unless further technological advancements result in a significant reduction of the noise in microarray data, this kind of analysis will continue producing unreliable inferences. to normalize, or not to normalize: that is the question to which no scientifically sound answer is currently known as far as this kind of reverse engineering is concerned. although limited to cell cultures, the causal inference from gene perturbation  experiments seems to be the only solid alternative. from this standpoint the observations reported in the present paper add to the concerns expressed by several investigators regarding how much confidence to place in the thousands of papers already published using microarray technology  <cit> .

CONCLUSIONS
the present paper provides quantitative insight into correlation between the t-statistics associated with different genes. this study leads us to conclude that:

• there is a long-range correlation in microarray data manifesting itself in a huge number of genes that are heavily correlated with a given gene in terms of the associated t-statistics.

• using normalization of microarray data it is possible to significantly reduce correlation between the t-statistics computed for different genes.

• normalization procedures affect both the true correlation, stemming from gene interactions, and the spurious correlation induced by random noise.

• it is likely that some noise effects represent non-monotone transformations of the underlying gene expression signals because even the rank normalization does not make the t-statistics independent when applied to the biological data.

• even the most effcient normalization procedures are unable to completely remove correlation between the t-statistics associated with different genes in biological data. furthermore, the long-range correlation structure persists in normalized data. this remaining correlation may be strong enough to deteriorate consistency of statistical estimators built from measurements on the genes.

