BACKGROUND
the field of biomarker discovery or clinical proteomics has raised high hopes generated by reports on potential biomarkers, which in many cases subsequently could not be substantiated in validation studies  <cit> . prominent examples are the findings in  <cit> . this development has resulted in large scepticism from both clinicians and regulatory agencies, which will make the application of valid biomarkers into the arsenal of clinical diagnostics even more of a challenge  <cit> . further, it is now generally accepted that single biomarkers are unlikely to result in major advancements as the complexity of disease cannot be captured by a single marker; instead, a panel of such biomarkers must be employed  <cit> . however, it is equally evident that such a panel must consist of clearly defined and validated biomarkers in order to provide a well defined signature. this raises the issue of the definition of a valid biomarker. as this is obviously of central importance, we have revisited this issue, not only employing theoretical considerations, but also by using a tractable yet realistic case study. the theoretical considerations in this area apply to the following main challenges:

 <dig> is the change  of a certain molecule observed in a proteomics study of disease, the result of the disease, or does it merely reflect an artefact due to technical variability in the pre-analytical steps or in the analysis, biological variability, or bias introduced in the study ?

 <dig> how should we estimate the number of samples required for the definition of likely valid biomarkers?

 <dig> which algorithms can be employed to combine biomarkers into a multi-marker classifier, and how can the validity of a multi-marker classifier be assessed? is validation in an independent test set necessary?

in an effort to investigate these issues and propose answers to these questions, we have employed different analysis and statistical strategies towards biomarker definition and validation using a set of data obtained from real samples. while technical differences do exist between proteomics and peptidomics, these approaches investigate a highly similar chemical entity, and the problems and challenges associated with the identification of potential proteomic and peptidomics biomarkers  are essentially identical. therefore, we feel it is appropriate not to distinguish between peptidomics and proteomics throughout this manuscript. several platforms for proteomics or peptidomics are currently being used in biomarker discovery studies  ce-ms is being used in clinical trials and data from ce-ms are applied in clinical decision-making, b) sufficient datasets of ce-ms were available to us, and c) the analytical performance characteristics of the ce-ms platform are well documented  <cit> 

in order to permit a rigorous and realistic assessment of the methodology, the study must  represent a real proteomic dataset that is acquired using the same technologies and experimental design as for a biomarker study;  be a classification problem with "typical" complexity, but simple enough to be tractable by standard methods; and  permit the deployment of commonly used statistical analysis strategies in order to benchmark them against an unequivocal outcome. based on these considerations we choose as an example the definition of proteomic differences between apparently healthy adult males and females. this avoids any bias due to a non-verifiable physiological condition in the subjects, since gender can be assessed with close to 100% confidence  <cit> . this design avoids an important problem in biomarker discovery pipelines: the so called verification bias. this bias occurs if subjects are not equally likely to have the diagnosis verified by a gold-standard test and if selection for further evaluation is dependent on the diagnostic test result. of course, in general the clinical situation will not allow for such a sharp definition as in the male-female case, but standard methods exist for accounting for the verification bias if the clinical readout cannot be assigned with 100% confidence  <cit> . we also used a cohort of subjects with diabetes type ii either with normal kidney function  or diabetic nephropathy  to demonstrate the applicability of the methods to a case where the clinical readout may not be verified with 100% confidence. the difference in the male-female study turned out to be more subtle than in the cd versus dn case, as the differences between the proteomic profiles between males and females are less pronounced than in the cd-dn case.

as body fluid to be analysed we have chosen urine. the urinary proteome/peptidome is of high stability, reducing pre-analytical variability  <cit> . ce-ms was chosen as technology as it allows for the routine analysis of a large number of samples, and has been thoroughly validated as a platform technology for proteomic biomarker studies  <cit> . as result of the current study we demonstrate the importance of a strict and correct use of statistics, especially adjustment for multiple testing. we further describe algorithms that enable prediction of the number of samples required for the definition of biomarkers with high confidence. the results presented here also show that different machine learning algorithms perform similarly  in establishing discriminatory multi-marker models. however, it is equally evident that these only lead to meaningful results if the number of data points employed is sufficient to learn the difference between the groups, and that the performance of such models can only be assessed on an independent test set. although our results have been obtained with a particular proteomic technology, ce-ms, the principal conceptual considerations, and hence also the conclusions, are independent of the technology used. therefore, the results reported here should also be applicable to other datasets generated using alternative standard proteomics technologies such as lc-ms or maldi. unfortunately, to the best of our knowledge, there is currently no similar dataset publicly available for maldi or for lc-ms. hence, we cannot report on the application of the proposed methods for either platform.

RESULTS
biomarker selection
the design of the study is depicted in figure  <dig>  to detect possible biomarkers, we employed samples from  <dig> males and  <dig> females, aged 21- <dig>  as the training set. all relevant data on all samples used in the study are available in the additional file  <dig> and additional file  <dig>  we accepted only peptides that were present in at least 30% of the male or female samples, as a feature with a smaller frequency in both groups may hardly be seen as significantly associated with gender in this study. this threshold resulted in a total of  <dig> peptides for further consideration. the appropriateness of a statistical test is primarily determined by the data distribution. usually, after low-level data processing the resulting data exhibit a mixture distribution characterized by a proportion of observations in a point-mass at zero representing the samples where a peptide is not detected, and a continuous component . the origin of the point-mass at zero may either be biological, as the protein is really absent in these samples, or technical, as the protein is present but its signal is below the limit of detection   <cit> . the only known fact about the point mass at zero is that those values are between zero and the lod. in statistical terminology, the proteomics data are left censored. therefore, usage of standard statistical methods which focus on one part of this mixture at a time can fail to detect differences between classes. the data employed here contains  <dig> consonant differences ,  <dig> dissonant differences  and  <dig> without point-mass component. the higher number of consonant markers reflects the fact that markers showing a higher mean are better detectable than those with a mean near to the lod. a difference in means between the two groups may have its origin in a difference in the proportion of zeros, a difference in the mean of the continuous component, or both. the standard parametric t-test may be inappropriate for such data as the underlying assumptions of the test are strongly violated. non-parametric tests like the wilcoxon rank sum test  may be more appropriate  <cit> , but may still fail to distinguish the contributions of the two mixture components to the male and female profiles  <cit> . this suggests the usage of hypothesis tests specifically developed for point-mass mixture data, like the two-part t-test, two-part wt and empirical likelihood ratio test, which tests the null hypothesis of no difference in the point-mass proportions and no difference in the means of the continuous components  <cit> . as expected, owing to differences in statistical power, the number of biomarkers declared statistically significant strongly varies with the type of test adopted . when subsequently validating in the hold-out set, the majority of the initially defined potential biomarkers could not be confirmed. this result is likely due to the inherent multiplicity of the problem, strongly supporting the requirement for adjustment for multiple testing  <cit> . these results are even more pronounced when a smaller cohort is employed, resulting in ≤ 10% of the potential biomarkers being confirmed in the test set . to control the false discovery rate  as correction to multiple testing, the benjamini-hochberg  procedure was used  <cit> . in table  <dig> we report the number of potential markers with adjusted p-values less than  <dig> . after adjustment for multiple testing, the wt reports the largest number of significant markers . moreover, 78% of the  <dig> markers declared significant by the two-part wt are also significant when using the standard wt, indicating that using just standard wt, which is part of standard statistical software , should enable definition of reliable biomarkers. the fact that many of the values in the profiles are tied to zero only makes the wt conservative and the p-values more trustable  <cit> ; as in a pilot study, a false negative is less harmful than a false positive. to test the stability of the significant markers chosen by the different tests, we investigated which of the differentially expressed markers established will still be a valid marker when tested alone on an independent test set . as seen from table  <dig> the standard wt has the most markers holding up in the independent test set. furthermore, the concordance between the biomarkers found in the training set and in the test set is only slightly lower than that for the two-part wt. the results given above argue in favour of using the standard wt for any similar proteomics data. previous reports have already stressed that non-parametric statistical tests such as the wt may be more appropriate for proteomics data. however, the use of the standard t-test is still frequently used and reported in the literature  <cit> . we subsequently investigated the number of potential biomarkers that can be defined when employing only a subset of the original samples. statistically, if a real difference exists, it may always be detected when the sample size is ad-equate. hence, studies on small cohorts may over-look important markers. with appropriate sample size all the differentially expressed markers should be detected. of course, not every difference found with larger sample sizes will be of clinical relevance, hence the need for the incorporation of biological back-ground information. interestingly, even a subset of the markers found using moderate sample size may still be enough for building a good classifier. as expected, the number of significant markers increases with increasing sample size . our simulations, where populations of sizes up to  <dig> ×  <dig> were generated using resampling with replacement from the  <dig> ×  <dig> samples, showed that this behaviour stops at sample sizes around  <dig> ×  <dig> where a plateau is reached . the concordance of these potential biomarkers in the test set also increases with the sample size. with sample sizes less than  <dig>  no differentially expressed markers are detected at all.

the number of potential significant biomarkers when comparing the  <dig> cases and controls in the training set, based on unadjusted p-values  is shown for the t-test, wt, two-part t-test, two-part wt and empirical likelihood ratio test. in addition, the number of consonant, dissonant and no point-mass features among these is listed. all markers defined in the training set were investigated, aiming at validation, in an independent  <dig> ×  <dig> test set. as is evident, the vast majority of potential biomarkers could not be validated. lower panel: the number of potential significant biomarkers  after bh adjustments for the t-test, wt, two-part t-test, two-part wt and empirical likelihood ratio test, and their performance in the independent test set are shown. while the expectation, that 95% of the potential biomarkers remain significant in the test set, could not be met, the percentage of biomarkers that could be validated is almost 2-fold in comparison to the unadjusted testing.

resampling as means to define "better biomarkers"
variable selection may be seen as the first part in finding a good classifier and must be performed based on training data only. usually, variable selection is performed only once using all the available training data. this may, however, introduce a substantial bias in declaring a biomarker differentially expressed. this fact is due to the biological variability in the compared populations . cross-validation and monte carlo cross-validation  may be adopted to protect the analysis against such a bias. however, as the number of biomarkers may be quite high, these procedures are computationally challenging. holding out 30% randomly from the  <dig> male and female training samples and examining the distribution of these biomarkers in the  <dig> independent test samples, we can detect a clear advantage of the biomarkers that were found with higher frequency in the resamples. from table  <dig> we see, provided enough resamplings are done , that if a biomarker is found significant in more than 75% of the independent resamples then the chance that it could be confirmed in the test set was between  <dig> and 100%. however, this procedure also results in a further reduction of the available biomarkers, and appears to be only useful when a rather large number of potential biomarkers should be reduced. in depth analysis of the data indicated that for building classifiers , a reduction via resampling of the number of biomarkers may not be necessary . however, the implementation of such resampling is clearly advantageous for e.g. describing any association with  physiology, as this procedure allows for identifying those biomarkers that show the highest likelihood of actually being associated with the investigated physiology.

concordance of markers detected during resampling with those found significant in the test set: a total of  <dig> resamples are drawn. for each of the resamples, 30% of the  <dig> ×  <dig> training samples were held out. the number of significant markers with a p-value less than  <dig>  after bh adjusted wt test for 50%, 80% and 100% of the resamples is shown. their concordance in an independent test set of  <dig> ×  <dig> is also reported in the lower panel. for more than  <dig> resamples the concordance of the biomarkers remains almost unchanged.

estimation of the sample sizes
an important question in the design of clinical proteomics studies is the selection of an appropriate sample size  <cit> . the number of units to be included in the study should typically address two issues. first, the differential sample size ndiff should allow the identification of putative biomarkers that are differentially expressed between two conditions . second, the discriminative sample size ndisc of the training data should allow the learning of a confident rule for classifying blinded items.

estimation of the differential sample size
here the question is: what is the minimum sample size required to attain a desired statistical power for detecting a meaningful difference between samples? this can be answered by estimation of the differential sample size ndiff. this sample size depends on the false positive rate α, typically set at  <dig> , the statistical power 1-β  and the standardized effect size  for quantifying how the classes differ. indeed, the effect size and its variation turn out to be the most important factors influencing ndiff estimation. both, effect size and its variation, are traditionally estimated from previously reported experimental data. unfortunately, in proteomics typically no previous data are available and anticipation of the expected as in  <cit>  may hardly be justified. we therefore investigated a resampling-based approach by directly sampling from the pilot data at hand. to simulate a typical proteomics study, we randomly choose  <dig> samples of each gender from the total training set of  <dig> males and  <dig> females. from the  <dig> ×  <dig> data, we used the bootstrap  <cit>  to generate  <dig> sample sets of different larger sizes  without any assumption about the underlying distribution of the sampled population. to take into account the multivariate aspect of the problem, we ask for the sample size required for declaring all the markers significant while controlling the fdr at  <dig>  using the bh procedure. this is equivalent to conducting the single tests at a more stringent "average type one error" αave. using the result  <cit> 

 αave=aveq− <dig>  

where ave is the average power for a single marker , q is the expected value of the false discovery rate, i.e. q = e, and π <dig> is the proportion of markers that are differentially non expressed . to estimate π <dig> we use the method described in  <cit>  and fit the observed distribution of the wilcoxon p-values to the following two component model

 f=π0f0+fa, 

with f <dig> being the density of the null features  is given by the uniform distribution u, whereas fa is the alternative density for the differentially expressed markers. hence we may write

 f=π0+fa. 

this resulted in a estimate π <dig> =  <dig>  which plugged into equation  <dig> leads to αave =  <dig> .

to estimate ndiff we set the value of αave =  <dig>  to control the fdr at  <dig>  and examined for biomarkers that can be declared significantly differentially distributed. wt was applied to each data set generated. let n be the number of times the null hypothesis is rejected. discarding 5% of n  is essentially a power estimate. by examining the graphs as in figure  <dig> the sample size required for any predefined power can be deduced. obviously, the more precise the information about the effect size δ, the better the trial can be designed. if the sample size is "sufficiently large", then the central limit theorem guarantees that δ will be approximately normally distributed. the bootstrap provides a powerful tool to estimate the required differential sample size by directly sampling from the available data and has been shown to give an unbiased estimate of power  <cit> . however, the key issue here is that the available data be reliable and representative. in the absence of a reliable data set, bootstrapping is not appropriate  <cit> .

in the above considerations, we opted for simplicity for the standard definition of the sample size as the minimum number of samples necessary to achieve a specified power. alternatively, the "confidence probability formulation"  <cit>  may also be used as it relies on the permutation of pilot study data of small sample sizes.

estimation of the discriminative sample size
to estimate the effect of training sample size on a classifiers performance we employed learning curves  <cit> . we used the inverse power-law model

 e=Γ+β×ntrain−γ, Γ,β,γ≥ <dig>  

where e is the expected value of a performance metric, e.g. the misclassification error rate mer  or the area above the curve aac , given training sample size ntrain. Γ is the minimum classification error that can be expected as ntrain → ∞, the so called bayes error which provides the lowest achievable error rate for a given pattern classification problem ), γ is referred to as the learning rate, and β the scale. using svm classification, the learning curves for aac and mer are given in figure  <dig>  svm was chosen since this approach has been found to give the best or the near best performance for many microarray data sets  <cit> . for the actual male and female data, the fit resulted in the equations: aac= <dig> + <dig> ×ntrain− <dig>  and mer= <dig> + <dig> ×ntrain− <dig> . from these equations the required sample size can easily be deduced. e.g. for reaching aac or mer of 10%, ndisc =  <dig> and ndisc =  <dig>  respectively. hence, mer seems to overestimate the sample size ndisc, as this quantity holds only for a given threshold whereas the aac gives a global measure for all thresholds. it is important to note that different classifiers will result in different estimates for the aac and mer and hence another estimate of ndisc will be obtained.

in practice it is impossible to reach Γ and only upper bound estimates to it can be reached. the aim is to find the discriminative sample size ndisc, that guarantees that Γ  of the classifier is within some threshold  from the optimal bayes classifier obtained for infinite ndisc  <cit>   - Γ ≤ ϵ). ndisc may then be obtained by resolving the equation Γ - Γ = ϵ. interestingly, here again the effect size δ turns out to be the parameter that determines ndisc. in the classification context, the effect size measures the distance between the classes. if the pilot study shows a small effect size then it is unlikely that a good discriminator will be easily obtained. the required ndisc that maximizes the Γ implicitly depends on the false positive rate α  <cit> . consequently, using those markers that control the fdr should generally produce a good classifier  <cit> . for the  <dig> male and  <dig> female profiles, controlling the fdr at  <dig>  we are able to define  <dig> significant peptide markers requiring an ndiff <  <dig>  with their calculated effect sizes we found that ndisc =  <dig> is required to obtain a classifier with 10% performance short of the optimal bayes classifier. the analytical method described in  <cit>  relies on strong distributional assumptions and seems to be less conservative than the learning curve estimation of ndisc.

classification
once a classification rule has been built, its performance must be evaluated. frequently, complete leave-one-out cross validation  is employed for error estimation. we have investigated if such an approach is indeed appropriate. an svm-based classifier was built, based on randomly selected  <dig> ×  <dig>   <dig> ×  <dig>   <dig> ×  <dig> datasets, and the entire  <dig> ×  <dig> cases and controls. as shown in figure  <dig> assessment of the performance based on the complete leave-one-out cross validation  resulted in apparently excellent performance, with the classifier based on  <dig> cases and controls only appearing to be 100% correct. however, when the classifiers are then tested in the blinded dataset, the results of the classifiers that were built only on a small set of samples could not be verified. as expected, best performance was observed for the classifier based on all available data, where the results from the cross validation and the assessment in the independent dataset are quite similar. these data indicate that results based on the training set only remain questionable, evaluation in an independent set is indeed essential and the ultimate test any procedure must pass. this conclusion supports the findings of  <cit>  where the loocv error estimate was found to be biased for small samples sizes. for large sample sizes the loocv error estimates may be seen as reliable. therefore we employed the independent test set consisting of  <dig> male and  <dig> female samples for evaluation of the performance of all classifiers. the classification results are reported in table  <dig>  the results suggest that the performance of many machine learning algorithms is quite similar and outperforms a simple tree model. table  <dig> also suggests that the use of a generalized linear model  may not be suitable for similar data. glm, and the tree model seem to be the more sensitive to the variability and the censored structure of the data.

the test errors for different classifiers and different sizes of the training set . the results are based on a test set of  <dig> males and  <dig> females.

applications to the cd-dn case study
to further test the applicability of the reported methods we investigated the difference between cd and dn patients using a data set of  <dig> cd and  <dig> dn subjects randomly split into  <dig>  <dig> training and  <dig> ×  <dig> test datasets . the differences in this dataset are much more pronounced than the male-female case . using the  <dig> ×  <dig> training data and  <dig> different random splits we found that on average  <dig> peptides may be declared differentially expressed using the adjusted wt. 65% of those markers could be validated in the test data . the fact that using a pilot study of larger size results in more markers being declared significant clearly applies here too, as readily seen from the figure in the additional file  <dig>  the learning curve of this dataset also shows clearly the inverse power law behaviour  and suggests that for the cd-dn case fewer subjects than in the male-female comparison may be required to obtain a classification of comparable performance.

CONCLUSIONS
in this report we have examined what requirements have to be met in order to identify significant proteomic biomarkers and establish classifiers that have a high probability of being valid and can be generalized. to avoid misinterpretation: we did not aim at actually identifying biomarkers that we claim to be gender-associated. the aim of this study was purely to analyse and delineate approaches which ensure a robust study design. in addition, we realize that a study aiming at the identification of biomarkers for classifiers is associated with further challenges, like the above mentioned verification bias. however, some of the main challenges in biomarker discovery may best be investigated using a well defined experimental system, as the one chosen here. in regard to the first major challenge: how to improve the detection of biomarkers clearly associated to disease, we show that the wt test seems to be best suited for this challenge. however, it also is evident that statistical analysis must be adjusted for multiple testing  <cit> , and we demonstrate the deleterious effects of the avoidance of multiple testing. this effect is even more pronounced when only a small number of samples is being used for the analysis. the un-adjusted p-values obtained from a small sample set are essentially meaningless, and are not at all connected with the probability of a certain molecule to be a true biomarker in the test set. in fact, the commonly made silent assumption that among the apparently significant biomarkers , true significant biomarkers can be found with higher probability than in the apparently non-significant group, could not be verified . in our dataset the actual significant features were evenly distributed in these two artificial groups , which are only generated due to inappropriate statistics, hence they should be considered to be artefacts. this again underlines the notion that unadjusted p-values should not be reported in the absence of other evidence. the lack of statistical power, as well as the unadjusted p-values that erroneously are often considered significant, are mostly a consequence of an incorrect estimation of the true distribution. due to the relatively high variability observed , the true mean cannot be correctly assessed based on a small set of samples. the incorrect distribution suggests significant differences, which in fact are not true. only upon investigation of a sufficiently large number of samples can the true mean in the cases and controls be determined. this is also evident from the example shown in figure  <dig>  we also show that confidence in the identified biomarkers can be further improved by resampling of the data, thereby generating a larger number of experiments. biomarkers that appear significant in each of these experiments, are likely also significant in an independent test set, hence can be generalized. while such a strategy clearly comes at a cost: the number of biomarkers identified is significantly lower, this strategy may nevertheless represent a preferred option to define likely valid biomarkers, due to the high level of confidence that can be reached. based on a representative proteomic data set, we also presented methods for answering the second important question: how to estimate the required sample size, both for class comparison  and subject classification . our data demonstrate that estimation of the differential sample size required for achieving significance in detecting a certain number of specific biomarkers is possible based on resampling from a relatively small dataset. while we have successfully employed only  <dig> cases and controls, it seems advisable to slightly increase this number to  <dig>  <cit> . a similar strategy may be adopted for estimating the discriminative sample size required for achieving a predefined confidence of a given classifier. based on the data subsequently obtained, we used the approach of fitting learning curves. this approach may result in an overestimation of the required discriminative sample size. this is in fact beneficial, as it will generally avoid the initiation of an underpowered study. our data also indicate that testing of biomarkers  or biomarker models  in the training set will likely result in an overestimation of their quality. as a consequence, it appears that the quality of biomarkers or combinations thereof can only be addressed with confidence in an independent test set. even when analysing a significant number of samples, statistics appears to overestimate the value of the potential biomarkers. statistics is based on the assumption of an even distribution of the features across the training and test sets, that the findings can be generalized, and on the association with  gender only. this is apparently not even the case when using the data from  <dig> cases and controls. the expected result, that 95% of the significant biomarkers should stay significant in the test set, could not be observed. this may indicate that additional variables influence the outcome, and result in an overestimation of the statistical value. especially when sample sizes are small, even statistically valid results must be interpreted with caution. in such situations, findings should be viewed as tentative and exploratory rather than conclusive. our results further reveal that different machine learning algorithms perform similarly well, and seem to outperform linear classifiers. how-ever, we could also clearly demonstrate that the assessment of the performance of such a classifier can only be performed on an independent test set, the results obtained from the training set  may be misleading. based on the data presented here, it appears advisable to begin a study aiming at identification of biomarkers or classifiers by performing an analysis of  <dig> cases and controls, estimate sample size required for certain performance  based on re-sampling, and then perform the actual study with a sufficiently large set of samples. potential biomarkers must pass wt, adjusted for multiple testing, preferably consistently when employing a set of >  <dig> resamples that each contain e.g. 70% of the available data. classifiers are best established employing any of the available machine learning algorithms. the validity of both, biomarkers and classifiers, is generally overestimated in the training set, hence can only be addressed with confidence in an independent test set. the methods proposed here are independent on which clinical readout is considered. this fact has been shown by applying them to a dataset composed from diabetes type ii either with normal kidney function or diabetic nephropathy. this last case study shows that the male-female case is reasonably representative of situations where the search for biomarkers and the classification tasks are rather involved.

