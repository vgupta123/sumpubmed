BACKGROUND
with the concurrent increases in total data output and decreases in costs associated with sequencing technology over the last few years, sequencing has now become routine, with many small to large laboratories or institutes generating billions  to trillions  of base pairs of data using one or more of today’s high throughput, small read platforms . however, each technology, specific platform, and even each sequencing run can display a unique error profile, whose provenance is from a combination of incomplete chemical reactions and stochastic errors in the biological processes of both library creation and the specifics of the sequencing reactions, coupled with occasional issues in detecting the signal from polymerase extension  <cit> . because most sequencing analysis software assumes accurate data as input, these errors can sometimes bias or even grossly mislead biological interpretations. while there is no standard method to deal with low quality data, getting rid of poor quality data can in general, improve the result of downstream analysis  <cit> .

a number of tools have been developed to assess and summarize the quality of a given sequencing run. fastqc  for example, provides a rapid quality check by subsampling from the total sequence dataset. while this type of initial assessment may be useful for determining issues within the library creation and sequencing pipelines, for downstream analysis it is preferable to attempt to preprocess all of the data to remove likely or obvious errors. a few applications already exist that provide both quality checks and preprocessing capabilities for today’s ngs reads, however, they all differ, both in convenience and in capability. for example, the solexaqa package  <cit>  comes in three command-line components that need to be run successively to perform both quality control  and trimming, while prinseq  <cit>  is available as either a command-line program or via a web-interface. the fastx-toolkit  in contrast, is a collection of command-line tools that are also available via the galaxy platform  <cit> , but they must be developed into custom pipelines for data qc and trimming.

in addition to differences in user interfaces, all tools are currently missing some features that limit their utility. for example, prinseq was initially designed for preprocessing  <dig> pyrosequencing data, and while it has been extended to function on single illumina datasets, it is not efficient on large datasets in terms of runtime and memory usage for a complete statistical report. the trimming component of solexaqa does not have the ability to remove  unwanted reads, such as those with low complexity, and requires an additional step to pair the trimmed reads for downstream applications. in addition, while it can auto-detect the fastq format, the quality check component is restrictively strict on the sequence id format that creates problems for applying it to data from other sequencing platforms. lastly, fastx-toolkit can only process one fastq variant  and requires further processing to link paired reads . in addition to these tool-specific issues, none of the aforementioned programs take advantage of multiprocessors, which are common even on everyday laptops, therefore, these tools can become severely time-consuming and possibly limiting, given the very large datasets being generated today. the constant increase in sequencing throughput, and the democratization of sequencing to laboratories less experienced in sequencing, demands more efficient and adaptable software solutions.

in order to address some of the limitations listed above, we developed faqcs , an efficient, parallelized, all-in-one program that provides a simple yet tunable, user-friendly method to perform data quality monitoring of sequencing runs, coupled with data quality trimming. both textual and graphical reports for both the input next generation sequencing data and the processed results are generated by default, with a few optional functionalities and outputs. table  <dig> compares the full list of features of this new program with existing quality control and preprocessing tools. the program is publicly available at https://github.com/lanl-bioinformatics/faqcs, as both a standalone version and as a galaxy module that can be installed on any local galaxy instance  <cit> .table  <dig> 
features comparison for various qc tools



features\tools
faqcs
fastqc v <dig> .0
fastx-toolkits v <dig> .13
prinseq lite v <dig> .4
solexaqa v <dig> 

3’ end quality trimming
yes
yes
yes
yes

5’ end quality trimming
yes
yes
yes

cross quality spike trimming
%
yes

adapter/primer trimming
yes
yes

phix filtering
yes

low complexity filtering
yes
yes
yes

“n” base filtering
yes
yes
yes

length filtering
yes
yes
yes
yes*

sequence duplication filtering
yes
yes

kmer content/rarefaction
yes
yes

graphic quality report output
yes
yes
yes*
yes*
yes

gc  content distribution
yes
yes
yes
yes

fastq format conversion
yes

multiple fastq inputs
yes
yes
yes

process paired-end data
yes
yes
yes*

accept fastq variants format
yes
yes
yes
yes

support compressed gzip input data
yes
yes
yes

multi-threaded
yes
yes

stand-alone tool/command line
yes
yes
yes
yes
yes

web-interface
yes
#
yes
yes
#
yes
$

%faqcs records a minimum of five bases of quality scores from both ends.

*uses a separate program/script to generate the result.


#module for galaxy platform.


$a separate web version is required.



implementation
the faqcs program is written primarily in perl, takes as input sequence read files in any fastq format, and outputs the trimmed and filtered reads, along with summaries of the data that include textual and graphical outputs, which are also combined into a single user-friendly pdf file. faqcs requires r  <cit>  to be installed in order to generate the graphical outputs and summary pdf file, and also requires the perl parallel::forkmanager and string::approx modules from cpan  to allow parallel processing and adapter trimming. an optional k-mer counting application requires the jellyfish program  <cit> , but this is the sole other dependency. the data flow and processing steps are shown in figure  <dig> figure  <dig> 
faqcs flowchart. fastq files input are first checked for the format of quality encoding, then split into a set  of files which are subsets of the original input. each file is processed independently and managed using the parallel::forkmanager perl module. a global data structure is used to store results returned from each parallel process. all reports are merged and a processed fastq file along with a series of detailed graphics are output in pdf format.



faqcs can process one or more illumina-style fastq files as input, and these can be unpaired sequence reads, paired sequence reads in two separate files, or mixed paired and unpaired reads. fastq files contain information about each base call, as well as its associated quality score, encoded in ascii characters. as the technologies progressed over the years, and users became more adept at handling these new large data files, a number of fastq variants have emerged that utilize different encoding ascii ranges. this has had a severe impact on data interpretation, has confused many end-users of the data, and continues to create problems when using specific downstream analysis software  <cit> . we have therefore incorporated as a built-in default feature the ability to detect the quality encoding format and have the data converted, if needed, to sanger-style quality format, which is now accepted by the majority of downstream data analysis software. faqcs also supports the gzip compression algorithm, allowing direct processing of compressed data.

because today’s ngs platforms can generate a tremendous amount of data, we included parallel processing as part of faqcs functionality. to process the input data in parallel, faqcs uses the parallel::forkmanager perl module to control the sub-processes. the input dataset is initially split by default into multiple files of  <dig> million reads each, but users can tune this parameter via a command line flag. each split file is independently run through the qc process, controlled by forkmanager in parallel. if the number of files is bigger than the specified processor number, it sequentially fills the next available processor that has presumably finished performing qc on a previous file. faqcs reads through each subset of data in each sub-process and keeps all statistics in memory. once a sub-process finishes, a global data structure retains and combines the statistical outputs of each sub-process and the memory from that sub-process is made available for other tasks. when all sub-processes are complete, data matrices are written to text files, which are then used to generate graphical reports.

for convenience, faqcs provides a processing mode to perform only quality monitoring on a subset of the input dataset without trimming and filtering, similar to the behavior of fastqc, however this is not the default setting. faqcs can utilize either a trimming algorithm similar to bwa  <cit>  , or optionally, a simple hard cutoff on quality score. one novel feature is that faqcs implements trimming not only from the 3’ end of the reads, but will also trim from the 5’ end. in brief, the bwa-style trimming converts the ascii characters to decimal numbers and finds the position in the read where trimming will end  based on the following equation:  <dig> argmaxx∈ <dig> ,…,l∑i=xlqu−qiifqu>qu where l is the read length and qu is the user-defined quality threshold, and trimming ends after the summation of qu – qi becomes negative. the default value for qu is q = <dig>  since most bases with a quality score of  <dig> or lower have been shown to be erroneous  <cit> . to deal with cases where one or two bases near the ends display a spike in q score but then return to a poor q value , we implemented a novel function which forces the algorithm to record at lease five bases of quality scores from both the 5’ and 3’ ends, in addition to  <dig> further positions to account for these possible sudden spikes in quality.

in addition to identifying regions of low quality for trimming, we have added a variety of parameters to further trim and filter  reads. the sequences can be adapter- and primer-trimmed given user input on those specific sequences. after trimming, by default, reads with two continuous ambiguous  bases are removed, together with those under  <dig> bases or with >85% low complexity . the following parameters allow users to tailor their trimming and filtering criteria to their needs: minimum length cutoff, minimum average quality score, maximum number of continuous ambiguous  bases, and maximum percentage of low complexity bases, matches to phix.

after running one or more fastq files, the output consists of: 1) the trimmed and filtered sequences in paired and/or single end read fastq files ; 2) a summary pdf report and text file on the raw and trimmed data. the pdf report, using default parameters, includes eight sections: 1) summary statistics of the raw and trimmed data, 2) read length histograms of raw and trimmed data, 3) nucleotide content histograms, 4) nucleotide content per base graphs, 5) average quality per read histogram, 6) quality boxplots per base, 7) quality 3d plot of base position vs. quality score vs. frequency, and 8) quality score histogram . each graph provides users with a unique perspective on the data for thorough quality assessment. for example, figure  <dig> shows the quality boxplot output per base position of a miseq dataset both before and after the qc process.figure  <dig> 
boxplot graph for the quality scores. rectangular boxes show the inter-quartile range . the end of the whiskers shows outliers at max  <dig> *iqr. horizontal lines in the box are median values at each bp position. there is a horizontal line at quality  <dig> indicating the predicted per base error rate of 1/ <dig>  for easy comparison, faqcs generates two boxplots side by side where the left panel is the boxplot of the raw reads and the right represents the processed reads. this is but one set of figures generated in the final pdf report .



faqcs provides additional optional functionalities found in few, or none of the other existing sequence data trimming utilities. for example, faqcs can implement jellyfish, a fast k-mer counting tool, to allow visual interpretation of k-mers and their abundance , which can help predict the completeness of coverage of the sequencing target. by default, faqcs performs k-mer counting on a subset  of the split files, and merges the ten k-mer frequency profiles. the output of k-mer counting includes two additional graphs that are added to the final pdf. one of the graphs is a k-mer frequency histogram , which displays the distribution of k-mer abundances; i.e. how many k-mers  are represented at what frequency . de novo assembly of single genomes can use the k-mer abundance peaks to estimate the fold coverage of a target genome; and users can use this information to better estimate the target genome size given the number of different k-mers surrounding the peak abundance. the other output is a k-mer rarefaction curve that users may use to judge whether the sequencing effort is sufficient if sequencing a single organism . while useful, k-mer counting is provided only as an optional parameter, primarily because this step consumes non-trivial additional cpu memory and time, and may not always be a necessary calculation .figure  <dig> 
plots from of k-mer profiling. a) k-mer frequency histogram of e.coli miseq dataset shows an obvious peak k-mer coverage near 216x  and a minimum inflection point at ~41x . the k-mers below than the minimum inflection point are due to sequencing artifacts and errors. the other small peaks typically indicate repeats in the genome. b) k-mer rarefaction curve shows a reduction of k-mers when trimming. the blue and red soild lines are the k-mer rarefaction curves of raw and trimmed e.coli miseq data, respectively. the green and beige solid lines are k-mer rarefaction curves of raw and trimmed data of the hmp mock data, respectively. the dashed line represents the baseline where all observed k-mers are distinct.



for further user-friendliness, we have implemented faqcs as a galaxy module with detailed information about its operation. the galaxy interface allows easy manipulation of sequencing data as part of larger workflows and/or via point and click, if this is preferred to running the program via command line or incorporating this tool into a larger program.

RESULTS
performance
to illustrate the program’s capabilities, we consider different types of datasets, summarized in table  <dig>  to allow a fair comparison between the single-step faqcs and the other available, multi-step read-trimming quality control tools , we complied the serial steps of each tool and used the same quality thresholds for the read preprocessing of a few different samples. because fastqc does not have trimming and filtering features, it was not included in this comparison. all runs were submitted to a cluster node equipped with intel xeon cpu x <dig> @  <dig> ghz,  <dig> processors and 64g memory.table  <dig> 
data analyzed in this study



dataset
source
platform
pre-qc
after-qc

# of reads 
average length 
average quality
# of reads 
average length 
average quality

escherichia coli
mg1655

providencia stuartii
33672

escherichia coli
outbreak strain

hmp mock
*http://www.illumina.com/systems/miseq/scientific_data.ilmn.



table  <dig> shows the computational performance of all tools on an isolate genome dataset and a metagenome dataset. while taking only one cpu process, prinseq took over 20x more memory and  <dig> x wallclock time than faqcs. this is primarily due to prinseq’s effort to identify duplicate reads within the entire dataset. solexaqa consumed even less memory since it only processes a subset of the data for the quality report. fastx however, outperformed all others, due in part due to its implementation in c++ and resulting fine control on memory usage and overhead on cpu processors. however, fastx also does not track paired end reads, and requires multiple modules to be run on both the raw and trimmed reads to obtain pre- and post-trimming statistics.table  <dig> 
the comparison of the computational performance using
e.coli
mg <dig> miseq dataset and hmp mock gaii dataset



e.coli
mg <dig> miseq
faqcs
fastx
prinseq
solexaqa

parallel process

memory

wallclock time

cpu

hmp mock
faqcs
fastx
prinseq
solexaqa

parallel process

memory

wallclock time

cpu


to take advantage of multiprocessors that can be found in most computers, we tested the multi-threading capability of faqcs using  <dig>   <dig> and  <dig> parallel processers. faqcs performs up to 6 ~ 8x faster. as expected, the increase in number of processors is proportional to the increase in memory consumed, since additional memory is required to store the read information with each additional split file. despite this however, for the dataset tested, faqcs can complete in less than 15 minutes with  <dig> parallel processors, and given the memory consumption, can be readily run on a many of today’s laptop computers.

expected effect on downstream data analyses
because it has already been shown that trimming and filtering sequencing reads benefit downstream analysis  <cit> , here we only briefly provide examples of de novo assembly and read mapping effects of faqcs data processing , using raw data and the matching reference genome. after preprocessing four example datasets for three isolate genomes and a metagenome, faqcs retained a range from  <dig> % to  <dig> % of the original input reads and  <dig> % to  <dig> % of the original total bases. the trimmed and raw reads were normalized by the number of reads, and were submitted to velvet   <cit> , newbler   <cit>  or idba_ud   <cit>  for de novo assembly of illumina and ion torrent data. differences between the assembly and the reference were obtained using nucmer   <cit>  to map the contigs to the appropriate reference genome. the reads were also mapped to the reference genome using bwa   <cit>  and snps were then called by samtools   <cit> , followed by filtering of those that were located in repeatitive regions.

because de novo assembly results can vary depending on the parameters used, we explored a velvet k-mer spectrum from  <dig> to  <dig> for the isolate genome, but with all other default parameters set to default. we summarize the results in additional file 3: figure s <dig>  comparing the relative assembly size compared with the reference genome and the number of single nucleotide polymorphisms. the newbler assembly of ion torrent data from isolates, and idba_ud assembly of metagenome data are summarized in additional file 2: table s <dig>  almost invariably, the trimmed reads produced better results  when compared with untrimmed data, consistent with the expectation that trimming poor quality improves assembly results  <cit> . furthermore, additional file 2: table s <dig> indicates that trimming also improves read-mapping based analyses, the trimmed reads have a greater proportion of reads mapped to the reference sequence and fewer snps would be reported . these improvements are seen even with this high quality dataset, when only  <dig> % of the reads were discarded ; the benefit of trimming is expected to be much more drastic with datasets of lower quality.

an integrated k-mer counting utility
users can activate an optional k-mer calculation function that uses jellyfish, for a rarefaction curve and a k-mer histogram plot to be provided in the final report. by default, the function is not on because it significantly prolongs the execution time and increases memory usage. however, the result of k-mer counting can provide users with the ability to estimate genome size manually and evaluate whether the sequencing effort is sufficient or not.

the k-mer histogram displays the number of k-mers  versus the k-mer count , and can assist users to interpret their data. an isolate genome ideally will have one major k-mer coverage peak , with possibly smaller peaks of more frequently observed k-mers representing repeat regions within the genome. using the e.coli miseq dataset, an obvious k-mer coverage peak at  <dig> fold coverage can be observed, with a minimum infection point at  <dig> fold coverage . k-mers below the minimum inflection point are due to sequencing artifacts and errors.

for an isolate genome, one can use the largest k-mer peak to estimate the average nucleotide fold coverage using the formula:  <dig> foldcoverage=peakk‐mercoverage×l/l−k+ <dig> where l is the average read length and k is the k-mer size.

for example, given the average trimmed length of  <dig>  bp and k-mer size of  <dig> for the e. coli dataset, the nucleotide fold coverage is estimated as 274x. this differs from actual mapping fold coverage , likely due to the exactness of the k-mer counting function and the allowance for mismatches in the read-mapping procedure. the genome size can also be approximated by using the total number of non-erroneous k-mers , which in this example is  <dig> ,316 bp compared with the actual reference size of  <dig> ,675 bp.

a k-mer rarefaction curve is also provided. in figure 3b, the rarefaction curve of the isolate dataset is improved after trimming, showing a flatter curve, consisting of the finite k-mers within an isolate genome. metagenomes have a much larger k-mer composition profile, and the rarefaction curve will continue to climb until the metagenome is sufficiently sampled. the decrease in number of total k-mers observed in both the isolate and metagenome data is an indication of the removal of errors within the data during the qc process. generally this type of graph is utilized to estimate whether additional sequencing is required or if the sequencing has proceeded as expected.

considering that the e.coli reference genome size is  <dig> ,675 bp and assuming that all k-mers  are unique for both strands, the upper bound of unique k-mers should be ~ <dig>  m. in figure 3b, the raw untrimmed data shows approximately 100 m distinct k-mers  while the trimmed data  reduces this substantially to approximately 40 m distinct k-mers. while this is a large improvement, this figure is still four fold higher than expected for the e. coli genome, indicating that some sequencing errors still remain within the dataset. a single nucleotide error in the middle of a read can introduce up to  <dig> unique and distinct k-mers, when k = <dig>  this in turn corresponds to a minimum of  <dig> , <dig> errors remaining in this dataset, or a post-trimming error rate of  <dig> %. therefore, the rarefaction curve even for an isolate may not completely become a plateau, despite trimming, and will depend on the number of reads and the error rate post-trimming. we therefore suggest using both the k-mer histogram together with the rarefaction curve as well as the other quality statistics to interpret one’s data.

CONCLUSIONS
we present faqcs, a program that provides a rapid and parallelizable means to remove low quality data from large ngs data files, and provide users with adequate outputs to better interpret their data. this new quality control and quality assurance tool is highly flexible in terms of input, with built-in format detection, allows a number of read-filtering and trimming features, and provides user-friendly summary statistics and graphical outputs to allow in-depth assessment of the data. this tool is also implementable within the galaxy environment. the resulting trimmed output can yield improvements in downstream analyses, including snp calling and de novo sequence assembly. an integrated k-mer counting option can also be used to estimate genome size, and can allow the user to evaluate whether any additional sequencing effort required.

availability and requirements
project name: faqcs

project home page:https://github.com/lanl-bioinformatics/faqcs

operating system: platform independent with primary unix support

programming language: perl and r

other requirements: perl parallel::forkmanager and string::approx modules from cpan http://search.cpan.org, and optional requirements, jellyfish http://www.cbcb.umd.edu/software/jellyfish/

license: gnu gpl version  <dig> or later.

additional files
additional file 1: figure s <dig>  output file generated by faqcs. a) summary of trimming statistics; b) read length histogram; c) nucleotide composition histogram for the reads; d) per cycle nucleotide composition plot; e) k-mer rarefaction curve; f) k-mer frequency histogram; g) average read quality histogram; h) per cycle quality box plot; i) per cycle, per score frequency plot; j) average read quality histogram.

additional file 2: table s <dig> and table s <dig>  the de novo assembly and read mapping effects of faqcs data processing.

additional file 3: figure s <dig>  comparisons of assembly completeness and assembly snp error before and after faqcs data processing.



abbreviations
ngsnext generation sequencing

qcquality control

snpsingle nucleotide polymorphism

cpanthe comprehensive perl archive network

competing interests

the authors declare that they have no competing interests.

authors’ contributions

cl and pc conceived of, and planned project. cl implemented software and analyzed data. cl and pc wrote the manuscript. both authors read and approved the final manuscript.

