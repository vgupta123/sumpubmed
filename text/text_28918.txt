BACKGROUND
robust data processing tools for ms data are lagging behind the substantial advances occurring in instrumentation and protocol  <cit> . one reason for this is that few outside experts--mathematicians, computer scientists, and statisticians--have climbed the learning curve  to understand the terminology, chemical theory, workflows, and challenges of ms-omics . this sort of interdisciplinary learning curve is not unusual in bioinformatics; however, the influx of external experts to genomics has not been seen to date in ms-omics. one reason for this is the lack of a succinct and cogent introductory resource that can bring outside experts to a basic but functional level of ms-omics familiarity.

in this primer, we will elucidate the mechanisms of ms-omics, the problems it is used to solve, key concepts and terms found in the literature, and open problems and their salient literature. the purpose of this tutorial is to expedite the new researcher's acquisition of a functional knowledge of ms-omics sufficient for contribution to the field.

RESULTS
relationship of genomics, proteomics, lipidomics, and metabolomics
the exponential growth of genomics studies during the last ten years has not been matched by corresponding research in ms-omics  <cit> . genomics researchers have several peer-reviewed conferences in which to publish their results. to the best of our knowledge, there has not been a single peer-reviewed conference to date on lipidomics or metabolomics, let alone any specifically addressing algorithmic approaches to problems specific to either area, although there are periodic special genomics conferences dedicated to proteomics. several existing venues labeled as bioinformatics will not accept papers on ms-omics, as their stated area of interest is limited to a distinct subfield of bioinformatics such as genomics. this phenomenon of focus on genomics is also reflected in institutional research programs. in a recent review of  <dig> post-secondary degree-granting bioinformatics programs,  <dig> programs noted a research emphasis in genomics, while  <dig> noted a research emphasis in proteomics. not a single institution listed a research program in lipidomics or metabolomics  <cit> .

the biological reach and impact of research in ms-omics is so extensive that it can be argued that ms-omics should now be the highest priority of systems biology  <cit> . from a pragmatic perspective, the large set of fresh problems and substantial potential for impact in ms-omics ought to be very attractive to those in more crowded disciplines.

proteomics
proteomics is the study of biological processes via the analysis of protein expression or state in cells or tissue. proteins are ubiquitous building blocks of life, and they are composed of peptides, which are chains of amino acids built by translating mrna. there are  <dig> amino acids, uniquely abbreviated with a single letter. peptides thus can be described as a string of the letters corresponding to the amino acids. though protein sequences are determined by dna sequences, post translational protein modifications  are not as easily predicted. these modifications quickly diversify and regulate/complicate protein function and cellular protein composition and are characteristic in most cellular processes and diseases. therefore, the aim of ms-proteomics is to provide data that dna sequences cannot--namely, individual protein concentrations and identification of post-translational modifications.

lipidomics
lipidomics is the systems-level analysis of lipids  and their interactions  <cit> . it is a science still in its infancy but one that promises to revolutionize biochemistry  <cit> . lipids are grouped into eight categories that share common physical and chemical properties  <cit> , and there are currently some  <dig>  documented lipids.

lipids that occur rarely or in small quantities are often the most effectual lipids in biological processes, meaning they are particularly important in disease diagnostics and in understanding pathology  <cit> . lipidomics can elucidate the pathology and treatment of many diseases such as cancer, diabetes, obesity, cardiovascular disease, arthritis, asthma, inflammatory bowel disease, alzheimer's and others due to the associated disruption of lipid metabolic enzymes and pathways  <cit> . a better understanding of lipidomics could significantly advance diagnostic medicine as well as provide novel treatment options.

metabolomics
metabolomics is the study of metabolomes--small molecular end products of cellular regulatory pathways  <cit>  that can provide a snapshot of cell physiology. metabolites are much smaller than proteins and smaller than most lipids. their small size precludes the direct overlap of some techniques used in proteomics or lipidomics, but they may be generally analyzed in similar ways. lipids may be classified as a subset of metabolites; however, mass spectrometrists typically consider lipids distinct from metabolites because analytically they must be treated separately .

ms-omics pipeline
the workflow from sample preparation to result quantification, can be split into two consecutive pipelines: the wet-lab pipeline and the data processing pipeline. the data processing pipeline consists of many possible processing steps that take the data resulting from the wet-lab pipeline  to the end result: identification and quantification . the quality of each step in the pipeline affects the sensitivity and reliability of the outcome  <cit> . there are many optional steps, some of them very popular. we will describe the essential and some optional steps.

all ms experimental data share a set of descriptive keywords that are essential for referencing components of the output map . a comprehensive reference of key ms terms is provided in  <cit> .

sample preparation
the details of sample preparation are beyond the scope of this paper. however, at a general level, sample preparation strategies prior to mass spectral analysis are based on isolating analytes of interest and removing all other contaminating molecules. for instance, filters can be used to separate high molecular weight proteins from low molecular weight lipids and metabolites, or contaminates. other sample preparation techniques exploit analyte hydrophobicity, charge, and analyte-specific affinity. the degree of specificity in sample preparation is determined by the end goal of the experiment  <cit> . for example, if an experiment requires the analysis of only phosphorylated proteins, the sample preparation should isolate only phosphorylated proteins. of course, this is very challenging but using an appropriate sample preparation strategy specific to an experimental need significantly simplifies mass detection and data analysis and in some cases is required to identify analytes of interest. proteomics, lipidomics, and metabolomics each have unique considerations in sample preparation.

introduction methods
direct injection refers to infusing the sample directly into the mass detector. this is usually done with some sort of machine to make the flow constant.

while it is sometimes advantageous to allow all analytes to flow through detection at once, most ms experiments of complex samples will use chromatography due to its ability to spread out analytes over time, making it less likely that the ionization capacity will be overcome by large quantities of analyte or background ions, a phenomenon called ion suppression.

chromatography disperses the introduction of analytes into the mass detector through time based on some chemico-physico property . all chromatography systems have two phases: the stationary phase and the mobile phase. the stationary phase causes analyte separation and the mobile phase carries the analytes through the chromatographic column to the mass spectrometer. methods include:

• lc-ms - mass spectrometry coupled to liquid chromatography. liquid chromatography uses a liquid mobile phase and a column packed with chemically derivated beads as a stationary phase. the mobile phase is composed of a two-liquid gradient. changes in the gradient  cause analytes to be slowly released from the column and enter the mass spectrometer. different stationary phases can separate analytes based on hydrophobicity, charge, size, or affinity. however, the most common stationary phases for lc-ms on biomolecules are reversed phase  and strong cation   <cit> .

• gc-ms - mass spectrometry coupled to gas chromatography. in gas chromatography systems the mobile phase is an inert gas  and the stationary phase is a column designed to separate molecules based on polarity. the gradient is temperature increase; molecules with a high affinity for the column elute at higher temperatures.

• ce-ms - mass spectrometry coupled to capillary electrophoresis. electrophoresis differs from chromatography, relying on electric fields, rather than mobile and stationary phases, to separate molecules  <cit> . capillary electrophoresis uses an electric field applied to long narrow capillaries to separate molecules based on size, charge, and flow resistance through the capillary.

multidimensional chromatography  refers to two chromatographic systems applied to the same system. in the case of lc-gc-ms, for example, analytes are introduced into the gas chromatography system as they elute from the lc system, with each system causing analytes with specific properties to elute with precedence. a more common multidimensional system in ms-omics is mudpit. mudpit uses two orthogonal separation strategies like strong cation ion exchange  and reversed phase  chromatography to achieve greater resolution.

ionization methods
analytes must be ionized  in order to be detected by the mass spectrometer. electrospray ionization  was developed in  <dig> and is the most popular in ms-omics due largely to its ability to ionize unstable molecules without breaking chemical bonds and to the diverse range of analytes that can be ionized by the method  <cit> . other methods include atmospheric pressure chemical ionization   <cit> , matrix-assisted laser/desorption ionization   <cit> , and electron-ionization   <cit> . ionization methods for ms-omics are generally referred to as soft ionization methods and include esi and maldi. ei is a harsh ionization method and will destroy most biomolecules except for very stable lipids and metabolites.

mass detection
as charged particles are passed through the mass spectrometer, the mass-to-charge ratio  of detected particles is registered. a single scan on the resulting output represents a snapshot of the precursor ions passing through the mass spectrometer at that particular retention time . the ions in this stage are called precursor ions because in tandem mass spectrometry , ions in small m/z windows are captured for fragmentation and ms detection a second time, yielding a second set of ions called product ions that can be used to identify precursor ions by matching their ms/ms patterns to a database of possibilities. it is important to understand that the ratio of solution selected for ms/ms fragmentation is low, normally capturing only 10-20% of the precursor  data. because most ms/ms systems autoselect what segments to capture based on intensity, much of that portion overlaps between replicates. of that 10-20%, less than 60% are identified via database lookup, and even that is subject to false positive identifications  <cit> .

an analyte can contain certain naturally occurring rare isotopes, such as carbon- <dig>  these isotopes tend to occur in individual analytes in known quantities, causing a characteristic pattern called an isotopic envelope . the envelope is characterized by the number of and relative intensity between its isotopes. the monoisotopic peak, or peak that appears at the theoretical mass discounting any attached heavy isotopes, usually appears alongside the slightly heavier masses of any portion of the peptide or lipid in the sample that contains heavy isotopes.

when an analyte exists in a run in more than one charge state , its isotopic envelope will reappear in a compressed and shifted form due to increased charge, as illustrated in figure  <dig>  the equation for the shift is specific to the source of the charge. for instance, a charge can be induced by the addition of a proton, in which case the shift is defined by /charge m/z with a gap between ions in the isotopic envelope of 1/k, where k is the charge of the analyte  and µ is the m/z of the single-charged analyte .

mass spectrometers output raw data--a large collection of data points each consisting of a tuple of m/z, intensity, and time  either in profile or centroid form. profile data contains all data points registered by the mass spectrometer , while centroid data has been reduced to data points that represent the local maxima in a single spectrum, a distribution of data over an m/z range for a given rt . centroid data is much more concise than profile data, but the reduction incurs information loss.

experiments can run in full scan mode--where the full range of m/z values is read--or the mass spectrometer can scan only certain m/z values   <cit> .

mass spectrometers have varying characteristics depending on the mechanisms used for mass detection, each with a different resolution. resolution at a certain m/z is given by the ratio of that m/z to the smallest m/z gap between two distinguishable ions. higher resolution instruments yield narrower profile peaks , allowing the signals from two distinct ions to be distinguished despite their similarity in m/z.

data processing
data processing consists of each of the possible steps in the ms-omics pipeline  involving digital manipulation of the mass spectrometer data or products from that data. these methods are constantly being improved upon and are discussed in detail in section. here, we provide a high-level overview of the role of data processing in the ms-omics pipeline.

the first step in data processing is handling the raw data produced by the mass spectrometer. algorithms for noise reduction, feature detection, and correspondence exist that operate on the raw data. however, many require preliminary conversion out of the proprietary data format of the instrument and into an open data type . it is important to note that, due to the size of the data sets, random access data processing--where only a portion of the data file is loaded into memory at a time--is a must, although some current tools load the full file and are therefore prone to crashing and subject to file size limits as memory is exhausted.

prior to analyte identification, the data must be denoised, peak-picked, featuredetected, deisotoped, and deconvoluted. these are significant and open problems and are discussed in more detail below.

analyte identification follows data processing. here, one of several available databases are used to compare the experimental feature observations  to theoretical patterns. these include sequest  <cit>  for proteins, lipidmaps  <cit>  for lipids, and metlin  <cit>  for metabolites. due to incomplete/growing databases and noisy data, closest-match assignment is prone to false positives and mismatches. statistical analysis is almost always incorporated in this or prior steps in order to ascertain the significance of the identification.

the ultimate goal of data processing is to yield the quantity of each analyte. the identification and quantity of analytes, as well as the underlying raw data, must be stored in data structures that allow for efficient access and manipulation of the data.

data types
raw data is a general label that actually describes a set of data formats specific to the vendor of the instrument. many data converters from raw to open data formats exist. one popular converter is pwiz . the network common data form , a generic open science data format, is an early data format that is still in use in some applications. mzxml is an open xml based data format with wide support. mzml was developed to replace mzxml and has more information from the raw data encoded and uses extensible ontologies to encode meta-data. mzquantml is an open data format specifically intended for the storage of quantities associated with identified feature data. mzidentml and pepxml are standards designed to facilitate database identity searches. annotated putative peptide markup language  is an xml standard designed to provide a single data file encoding of the original data set and its modifications via data processing tools  <cit> .

data sets
lack of labeled data
the prevailing problem in developing and evaluating computational approaches to ms-omics problems is the lack of labeled data  <cit> . labeled data is difficult to obtain both because of the size of data sets--which can easily consist of millions of data points per file and hundreds of gbs of files for a replicate experiment series--and the undependability of hand-labeling--which is both time consuming and subjective. several approaches for mitigating this problem exist: qualitative metrics, spiked mixtures, and in silico simulated data.

qualitative metrics evaluation metrics that do not use ground truth avoid the need for labeled data. for example, replicate alignment quality can be assessed via the pearson correlation coefficient, feature overlap rate, or coefficient of variation. this approach is sub-optimal, as a good score on a qualitative metric does not necessarily translate into a good quantitative score using labeled data, but it is easy to compute and is comparable across problem instances.

spiked mixtures commercially available purified and quantified measures of a specific analyte are combined to produce a data set with known composition and quantity. these samples are not exactly ground truth, however. due to ionization inefficiencies, environmental contaminants, and the variability of mass spectrometry, no instrument will report the same quantity and composition predicted by a spiked mixture. what's more, a mixture of a few analytes, which often do not co-occur in nature, is hardly representative of real-world scenarios, in which complex samples can easily contain hundreds of thousands of distinct analytes. to create more realistic conditions, spiked mixtures can be added to samples where the spiked an-alytes are not expected to occur. however, a method's accuracy on a few analytes is not necessarily indicative of performance across all analytes, particularly given the variability and limitations of ms/ms, which is commonly used to single out the m/z of the expected analytes but cannot be expected to capture the gross majority  of the remaining sample.

in silico simulated data simulated data is used in the field to refer to real-world data sets that have been purtubed with m/z shifts or intensity value modifications in order to create psuedo-new data without having to rerun costly experiments. true simulated data, called in silico to identify that it as purely sourced from simulation algorithms on a computer, is a relatively new advent in ms-omics. creating realistic in silico data requires the analysis of many ground truth datasets, which creates a chicken and egg problem, as the difficulty of obtaining ground truth datasets is the very reason an in silico simulator would be beneficial.

sources of open data
to facilitate strictly algorithmic advances in ms-omics, to avoid the need for a costly wet lab for creating mass spectrometry data, and to aid in evaluative comparisons against existing methods, more and more practitioners are making their data freely available online. although any serious foray into ms-omics should certainly include a collaborator with mass spectrometry assets and formal training, we present a list of some of these open data sets in order to aid those who are interested in investigating ms-omics for the first time as well as more seasoned investigators who would simply like to make a case for the generality of their methods.

lange et al. have provided two proteomic and two metabolomic data sets  <cit>  which they have used to assess the quality of several alignment algorithms at http://msbi.ipb-halle.de/msbi/caap. the data is already segmented into reduced isotopic envelopes .

listgarten et al. provide centroided replicate data with spiked-in peptides  <cit> . there are two data sets: a set of  <dig> replicate lc-ms runs from ruptured e. coli cells and a set of  <dig> lc-ms runs of human serum samples.

jeffries provides a data set consisting of raw replicates of seldi data  <cit>  at http://data.ninds.nih.gov/jeffries/alignment/index.html.

the superhirn data set  <cit>  can be found at http://proteomics.ethz.ch/muellelu/web/latin_square_data.php. it consists of  <dig> lc-ms runs from tryptic digests of  <dig> nonhuman proteins spiked with different concentrations into a complex human peptide sample and includes the raw as well as processed data. the data was obtained on an ft-ltq.

problems of interest
among the data processing portion of the ms-omics pipeline, some problems are widely studied, and some are emerging. all provide future research potential.

in silico simulation
the lack of ground truth data for evaluation of data processing algorithms precludes effective validation and comparison. in silico data simulation is a relatively new approach to providing on demand ground truth simulated data. by modeling a list of analytes and a description of experimental conditions, simulators can provide estimates of mass spectrometer output combined with labels of the analytes and quantities used in silico to generate the data .

correcting mass shift
analyte detection on the m/z axis in mass spectrometers is subject to two types of error: systematic mass error--a functional deviation from true mass--and random mass error  <cit> . typically, systematic mass error is mitigated by routine machine recalibration--a process wherein analytes of known mass are processed in the mass spectrometer to create a model that is used to interpolate m/z shift for any given m/z value. however, the efficacy of this calibration reduces over time as the mass constantly continues to shift. additionally, some machines benefit from an injection of spiked standards during a normal experiment for internal calibration, which helps overcome the temporal effects of space charge effects, electric fields, peak intensity, and temperature  <cit> . internal standards are undesirable due to the additional cost of standards and the suppression implications of spiked standards. computational mass calibration techniques have been proposed in order to provide the mass accuracy of internal calibration but with better consistency and lower cost  <cit> . this is an active but not crowded area of research with practical implications.

correspondence
correspondence, the registration of recurring signals from the same analyte over replicate samples, is a crucial problem in any of the many ms experiments where multiple runs of similar samples are compared to each other . for a comprehensive review of current algorithms, see  <cit> . persisting problems are an abundance of user parameters, models that do not include known behavior, prohibitively long runtimes, and a lack of performance comparison between methods  <cit> .

denoising
ms-omics produces inherently noisy data. noise can consist of spurious data points or distortion of a data point's true value in retention time, m/z, or intensity. denoising as used in ms-omics refers to the removal of spurious data points. baseline subtraction is a common method in which signals with intensity lower than an adaptive threshold are considered to be noise and removed . this is an active area of research, though most experiments in the literature have not made an explicit and dedicated study of different techniques, instead describing the denoising method applied as a data processing step in a larger experiment.

feature detection
the most important step of an ms-omics workflow is undoubtedly feature detection  <cit> , a general term that can apply to the extraction of various signal elements from ms data. in chromatographic data, feature detection can refer to either extracting isotopic envelopes or isotopic traces from an ms sample output . many methods exist for isotope trace extraction, among them a promising new algorithm that performs well on existing evaluations  <cit> . sometimes this process is called peak picking or peak detection, but those terms should be avoided since they are also used to refer to the conversion from profile data to centroid data. in direct injection data, feature detection is sometimes referred to as peak summarization, since each spectra  must be combined into a tis through mitigating the variance inherent in m/z across spectra .

data structures
as described earlier, many data types exist for ms-omics data. new data formats continue to be proposed to meet unforeseen needs.

a recent prevailing expansion point has been the need to store the results of data processing tools in addition to the original data. truly modular pipelines require data structures that contain all necessary data to be used by any tool in the pipeline, meaning previous modifications are annotated in addition to retention of the original data. apml is one attempted solution to this problem, but, so far, the community has not embraced it, as it appears that there are only two extant algorithms which use it  <cit> .

there is still a need for compact, random access, and information rich data structures and access for ms data  <cit> . what's more, some proprietary formats can still only be converted to open formats on windows platforms.

identification
as discussed earlier, mass spectral identifications may be based on several factors, but two inputs, the precursor mass  and the fragmentation pattern  of the precursor mass, are by far the most common identifiers. this spectral information provides a fingerprint unique to most biological molecules; however, low quality spectra cause false positives and false negatives. while improving mass spectrometry will certainly improve spectral quality, improving spectral search algorithms and employing new identification inputs will allow for more confident identifications. this is particularly true for the relatively new fields of metabolomics and lipidomics.

predicting rt
retention time refers to the amount of time an analyte is delayed by chromatography before exiting and being detected by the mass spectrometer. retention time is correlated with physical and chemical analyte characteristics; therefore, predicting analyte retention time provides another factor for positive identification. many peptide retention time prediction strategies exist  <cit> . however, cross instrument retention times vary greatly due to changes in experimental parameters, creating a real need for retention time normalization as well as retention time prediction.

mass variance correction
mass variance, the difference between the theoretical and experimental  mass of analytes is an open problem. one way of correcting mass variance is by using the weights of the elements of each analyte to predict m/z locations where a lack of signal is impossible, allowing for the identification of systematic deviation from theoretical masses in a sample  <cit> . a similar approach is to model such theoretical gaps via a sine curve fitted via a fast fourier transform  <cit> . accurate m/z values are essential to analyte identification.

ontology
according to a recent survey of the field, the biggest problem in lipidomics is the need for a standardization of data acquisition and data processing, due to the huge variability in instruments, protocol and data processing for lipidomics <cit> . the many options and permutations in the ms pipeline would make for a very long methods section if explicitly described in a paper--much too long for any journal's page limits. although several partial ontologies exist , there is no concise way to uniquely identify an experiment from start to finish, including sample preparation, mass spectrometry protocol, and post-processing. existing ontologies are particularly lacking in terms of data processing terms. todo cite clarity in concepts

absolute quantitation
ms signal intensity is related to but not equivalent to analyte quantity  <cit> . factors that influence this discrepancy include  <cit> :

• ionization efficiency. not all analytes in a sample are ionized.

• enzyme digestion rate. when an enzyme--such as trypsin--is used to digest proteins into peptides, not all proteins are completely cleaved. this leads to less-than-expected signal abundance, as the true abundance will be diminished by whole proteins , and incompletely digested proteins .

• ion suppression. when the quantity of analyte entering the ionization mechanism at a given time exceeds the ionization capacity of the ionization mechanism, only a portion of the analyte is charged  <cit> .

accurate models of these effects would improve estimates of analyte population in samples, as well as further advance in silico simulation.

currently, quantification methods generally fall into one of three approaches: label free spectral counting, quantification via differential stable isotopes, and label free quantification based on the precursor ion signal intensities  <cit> . spectral counting is a method in which peptide signals are used to create a protein tally--the count of every protein containing a certain peptide is incremented each time one of its peptides is identified via ms/ms. despite its prevalence, the accuracy of spectral counting is limited by its dependence on ms/ms acquisition rates, which, as mentioned above, are very low, and its propensity for false positives, since all proteins containing each detected peptide are considered as present when in reality only one need be. stable isotope labeling methods  also have significant limitations . besides cost and sample prep complications, nearly all methods increase the number of co-eluting analytes, creating a bottleneck for the complexity of samples handled. what's more, because stable isotope methods target a small specific list of analytes a priori, they are not practical in terms of time and money for data-driven discovery, where sample composition is unknown  <cit> .

modeling dynamic range suppression effect
dynamic range is a term that describes the minimum intensity of a detectable signal given a co-eluting analyte of a higher intensity . all mass spectrometers have a dynamic range limitation. the current state of the art is  <dig> -  <dig>  meaning that at a given rt if one analyte has an intensity of  <dig>  ×  <dig>  any analyte with an intensity less than  <dig>  ×  <dig> would not be detected.

fragment ion intensities
because ms/ms acquisition captures not just the analyte of interest but also any surrounding precursor ions, and because fragmentation isn't a perfect process, fragment ion intensities are not as accurate as desired  <cit> . several machine learning approaches have been proposed for making more accurate fragment identifications  <cit> . however, this is still an open problem.

de novo peptide sequencing
de novo sequencing is an alternative method to database matching that accommodates peptides that don't match up with the database   <cit> . here, the original peptide sequence--defined by a series of letters, each representing an amino acid--is reconstructed based on the ms/ms fingerprint and the chemical properties of the analytes. a recent tutorial provides more detail and resources  <cit> .

fragmentation patterns for lipids
proteins have a known cleavage pattern, meaning that when peptides are fragmented by ms/ms, association to a peptide is straightforward. lipids, on the other hand, have a much more complex form due to a wider vocabulary of building blocks and a more complicated fragmentation pattern. to date, no fragmentation rules have been published, making ms/ms much less helpful in lipidomics than proteomics. because of the complexity of lipids, a machine learning approach could be appropriate in finding a solution to this problem.

biomarker detection
biomarker discovery is the use of comparative analysis  in order to identify analytes that correlate with certain diseases or other conditions for diagnostics or drug development. it is an active area of research with a lot of published work; however the problem is still wide open due to limitations in mass spectrometry, preprocessing, and identification. current methods struggle to highlight case/control differences in complex samples, requiring painstaking, time consuming, and error-prone manual detection.

deisotoping
deisotoping is the process of reducing several instances of the same analyte at different charge states into a single feature--usually a monoisotopic peak . this is necessary because the query to a data base search consists of only the single-charged feature m/z and  rt. adding to the complexity of registering differently charged versions of the same analyte is the fact that, in complex samples, the isotopic envelopes of different analytes can and do overlap, requiring deconvolution .

deconvolution
overlapping signals must be resolved prior to quantification . rt overlaps occur when two isobaric analyte elute without a gap between them, and are more common in complex samples. isotopic envelope overlaps occur in m/z where two analyte are not sufficiently separate in m/z at their current charge state. ion overlaps occur when particular ions of two given analyte are too similar to be resolved in m/z. all m/z overlaps are less likely in high resolution machines, which by definition are capable of better resolving power evinced by more narrow signals in m/z. rt overlaps can be minimized to some extent by sample preparation and protocol designed to separate similar molecules into different rt areas.

parameter reduction
in general, most algorithms require the user to optimize a host of parameters through manual tuning, which is time intensive. new algorithms should avoid free parameters. if included, they should also provide guidance or an automated method to fix them. research opportunities include developing methods for automatically optimizing parameters on existing and popular methods.

CONCLUSIONS
ms-omics is an exciting, developing field with many research opportunities for mathematicians, computer scientists, and statitisticians. although contribution to the field requires a functional understanding of many domain-specific concepts and terms, the open nature of most of the existing problems provides many opportunities for impact.

competing interests
the authors declare that they have no competing interests.

authors' contributions
rs, adm, dv, and jtp all contributed in writing this manuscript.

declarations
rs is supported by the nsf graduate research fellowship .

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2014: selected articles from the 10th annual biotechnology and bioinformatics symposium . the full contents of the supplement are available online at http://www.biomedcentral.com/bmcbioinformatics/supplements/15/s7
