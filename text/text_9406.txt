BACKGROUND
data clustering is based on the assumption that a population of objects can be subdivided into smaller subgroups, internally homogeneous for one or more features. since the work of eisen and colleagues  <cit> , clustering methods have become a key step in microarray data analysis, to identify groups of genes or samples displaying a similar expression profile. such partitioning has the main scope of facilitating data visualization and interpretation, and can be exploited to gain insight into the transcriptional regulation networks underlying a biological process of interest. as an example, promoters of genes belonging to the same transcriptional clusters in a yeast cell cycle experiment were found to be enriched for specific sequence motifs, likely to serve as transcription factor binding sites  <cit> . however, due to the complex nature of biological systems, microarray datasets tend to have very diverse structures, some of them even do not seem to have well-defined clustering structures. as a result, none of the existing clustering algorithms performs significantly better than the others when tested across multiple datasets  <cit> . commonly used algorithms, such as k-means, hierarchical clustering and self-organizing maps   <cit> , typically construct clusters on the basis of pairwise distance between genes. as a consequence, they may fail to reveal nonlinear relationships between gene expression profiles, and thereby fail to correctly represent a dataset with nonlinear structure  <cit> . as a typical example of non-linear relationship, a modest shift in the time-course response of two genes is sufficient to substantially reduce any linear similarity measurement value. moreover, k-means and som fail to capture non-globular clusters and to avoid solution trapping in local minima  <cit> . over the past years, more sophisticated clustering approaches have been developed specifically for microarray data clustering, such as geneclust  <cit> , biclustering  <cit>  and cliff  <cit> . though in some particular cases they perform better than standard methods, none of them proved consistently better across multiple different datasets . moreover, their high algorithmic complexity severely limited their use and the traditional algorithms remain more popular thanks to their conceptual simplicity. in particular, hierarchical clustering remains the most widely used clustering algorithm, although it has been described to suffer from a number of limitations mostly deriving from the local decision making scheme that joins the two closest genes or clusters without considering the data as a whole  <cit> .

recently, fuzzy clustering approaches have been taken in consideration because of their capability to assign one gene to more than one cluster , which may allow capturing genes involved in multiple transcriptional programs and biological processes. fuzzy c-means , also named fuzzy k-means, is a fuzzy extension of k-means clustering and bases its fuzzy assignment essentially on the relative distance between one object and all cluster centroids  <cit> . many variants of fcm have been proposed in the past years, including a heuristic variant that incorporates principle component analysis  and hierarchical clustering  <cit> , and fuzzy j-means, that applies variable neighborhood searching to avoid cluster solution being trapped in local minima  <cit> . a fuzzysom approach was also developed, to improve fcm by arraying the cluster centroids into a regular grid  <cit> . all these fuzzy c-means-derived clustering approaches suffer from the same basic limitation of k-means, i.e. using pairwise similarity between objects and cluster centroids for membership assignment, thereby lacking the ability to capture non-linear relationships  <cit> . another family of fuzzy clustering approaches is based on gaussian mixture model   <cit> , where the dataset is assumed to be generated by a mixture of gaussian distributions with certain probability, and an objective function is calculated based on the mixture gaussians as the likelihood of the dataset being generated by such model. then the objective function is maximized to solve the model and give a set of probabilistic assignment. a possible problem with this approach, as highlighted by yeung and colleagues, is that real expression data not always satisfy the basic gaussian mixture assumption even after various transformations aimed at improving the normality of the data distributions  <cit> .

the aim of this paper is to propose a conceptually novel clustering algorithm combining simplicity with good performance and robustness. the algorithm approaches fuzzy data clustering from a novel perspective. it is mainly based on two general assumptions:  clusters should be identified in the relatively dense part of the dataset;  neighboring objects with similar features  must have similar cluster memberships so that the membership of one object is constrained by the memberships of its neighbors. therefore, the membership of each single object  is not determined with respect to all other objects in the dataset or to some cluster centroids, but is determined with respect to its neighboring objects only. this approach brings the notable advantage of capturing non-linear relationships, in a way similar to a nonlinear data dimensionality reduction approach called locally linear embedding , originally developed for mapping multi-dimensional objects  into a lower-dimension space for their representation  <cit> . the idea behind lle is that, in a dataset, most nonlinear relationships can be effectively captured by subdividing the general network of relationships across all objects into locally linear relationships between neighbor objects. as an important consequence, information about one object can be correctly approximated by information obtained from its nearest neighbors. so for each object, lle used the original dataset to define its nearest neighbors and to assign a set of weights specifying how much each neighbor contributes to the reconstruction of the features  of the object. after this, the dataset can be represented in a lower dimensional space, where each object is mapped according to the lower dimensional representation of its nearest neighbors and the weights assigned to its nearest neighbors. in this way the local structure of the original dataset  is preserved also in a lower dimensional space such as the 2d or 3d views commonly used for data displaying. we therefore envisaged a fuzzy clustering approach based on neighborhood approximation, to capture non-linear relationships in multidimensional data and to provide a substantial improvement in the visualization and analysis of microarray data. the novel clustering method, flame, integrates the two above-mentioned key properties:  fuzzy membership assignment ;  definition of membership assignment by local approximation, where membership assignment of a gene depends on membership assignments of its neighbors genes .

RESULTS
the flame algorithm
data clustering by flame goes through three main steps, illustrated in figure  <dig>  the first is the extraction of local structure information and identification of cluster supporting objects . in this step, the distance/proximity between each object and its k-nearest neighbors is used to calculate object density. objects with the highest density among their neighbors are identified as csos and serve as prototypes for the clusters, based on the fact that many other objects show similar behavior. some outliers are also identified in this step, whose behavior is rare in the dataset. the second step is the assignment of fuzzy membership by local approximation. the initial number of clusters is defined by the number of csos. at the beginning, each object is assigned with equal membership to all clusters, with the exception of csos and outlier objects, each cso being assigned with full membership to itself as a cluster, and all outlier objects being assigned with a full membership to the outlier group. then, an iterative process is performed to approximate the fuzzy memberships of objects which are not csos or outliers, for which the membership is fixed. at each iteration, the fuzzy membership of each object is updated by a linear combination of the memberships of its nearest neighbors, weighted by their proximity. in this process the fixed, full memberships of csos and outliers exert an influence on the membership of their neighbors, which subsequently propagates in the neighborhood network during the following iterations so that the final membership of each object  is the result of a balanced influence  of the memberships of all other objects. to facilitate comprehension of the process of membership "propagation", a flash animation is available as additional movie . the last step is the construction of clusters from the fuzzy memberships, which can be made in two ways:  by assigning each object to the cluster in which it has the highest membership degree , or  by applying a threshold on the memberships, and assign each object to the one or more clusters in which it has a membership degree higher than the threshold . in the validation analysis presented here, we used the single membership approach.

before assessing the clustering performances of flame, we preliminarily estimated its computational efficiency by analyzing its time complexity . as a theoretic time complexity estimation of the membership approximation procedure is very difficult, we performed an empirical study of the time complexity of flame compared with other algorithms . the empirical result shows that for data matrices with many columns, flame has significant computational advantage over the other methods, except k-means. but actually in our implementation, no sophisticated techniques have been implemented for k-means to search for global minimum, while flame always guarantees global minimum. taking this into account, k-means may not have much computational advantage over flame.

flame implementation in the gedas software
the whole flame algorithm has been implemented as a part of gene expression data analysis studio , a c++ program with graphical user interface currently running on linux and microsoft windows. two user modes are provided, simple mode, which is enough for most usages, and advanced mode, which enables tuning of all parameters to optimize the clustering. the key parameter to tune during flame optimization is the knn number, because it affects the number of clusters in different ways. first, knn determines the smoothness of density estimation , which in turn limits the maximum number of csos. second, knn determines the range covered by one cso: the larger the knn, the larger the cso range, with fewer csos. in the end, in the neighborhood approximation step, knn determines the range of membership influence of each object: the larger the knn, the fuzzier the memberships of the genes. four other clustering algorithms, k-means, hierarchical clustering, fuzzy c-means  and fuzzy som  are implemented in gedas. multiple cluster validation metrics based on figures of merit  have also been implemented in the software, for selection of the best-performing clustering algorithm and parameters in a given dataset. more details about gedas and its use are provided in a manual, which is available together with the software.

comparative analysis of flame performances: expression partitioning
to assess the performance of flame and compare it with the other above-mentioned algorithms, we used gedas to cluster four different datasets:  reduced pheripheral blood monocytes  dataset  <cit> ,  yeast cell cycle  expression dataset  <cit> ,  hypoxia response  dataset <cit> , and  mouse tissues  dataset  <cit> . further details on data processing and clustering are provided in the methods section.

the clustering performance was initially assessed using three different figures of merit  <cit> : 1-norm fom, 2-norm fom and range fom . we noticed that fom analysis can not be applied to flame in the standard way, because there is no parameter in flame to directly fix the number of clusters: the cluster number is indirectly determined by the number of k-nearest neighbors  chosen. moreover, for the same knn number, when one experimental condition is left out during the analysis, the number of clusters generated by flame may change. therefore, when applying fom to flame, we use the median number of clusters generated by a given knn during the leave-one-out analysis as the representative cluster number. the fom analysis could not be performed on the mt dataset, because of its high sample diversity. 1-norm fom produced results very similar  to the more widely used 2-norm fom . 2-norm fom analysis  indicated that no clustering algorithm was the best in all datasets, with flame, hierarchical and fsom being the best in, respectively, rpbm, hr and rycc data. conversely, range fom highlighted a better performance for hierarchical clustering in all datasets, with flame being the second best . to validate clustering performance also on large datasets with reasonable computing time, we defined another validation index, named partitioning index, which does not require leave-one-out analysis and is defined as the ratio between the overall within-cluster variability and the overall between-cluster distance. according to this metric, a good data clustering results in low variability within each cluster and high distance between the various clusters. to calculate the overall within-cluster variability, the variability within each cluster is determined as the average distance between each pair of genes in the cluster, and then averaged for all clusters. the between-cluster distance is obtained by averaging all pairwise distances between clusters. in turn, each single between-cluster distance is calculated by averaging the distance between each pair of genes from the two clusters.

interestingly, according to the partition index analysis, flame emerged as the best algorithm in three out of four datasets . a possible explanation for the different results obtained with the partition index analysis is that flame may generate non-globular clusters with more heterogeneous size distribution. indeed, fom is calculated by averaging the deviations in the left-out condition not cluster by cluster, but by averaging over the whole dataset. therefore, large clusters with high internal variability have a higher weight in fom calculation than small, compact clusters. we verified that a modified fom calculation, where deviations are averaged at the cluster level, gives better values for flame .

comparative analysis of flame performances on function partitioning
as a consequence of partitioning of genes according to their expression, a good clustering algorithm should also generate clusters of functional significance, i.e. of genes that share both similar expression profiles and similar functional roles  <cit> . a particular caution should however be taken when using gene clustering for functional analysis, as the assumption that genes sharing the same expression profile have a similar function does not always hold true and requires extensive statistical validation  <cit> . to assess whether flame is better than other algorithms at partitioning genes into functionally homogeneous groups, we used gene ontology  annotation  <cit>  for a comparative assessment on three datasets . for go-based comparison, the first thing we investigated is how the go terms are spread among the expression clusters. the rationale is that a good clustering algorithm should highlight which gene functional classes  display a precise pattern of transcriptional regulation in a given dataset. for such classes, the algorithm should generate few expression clusters annotated with the respective go term and many clusters without annotation to that term. a high spreading of all go terms across the various clusters is an index of poor performance. we therefore calculated, for each go term, the percentage of clusters with at least one annotation to that term, and defined a global annotation spreading index as the median of such percentages across all go terms. as shown in figure  <dig>  flame has a substantially lower annotation spreading index in two out of three datasets.

a second metric to assess function partitioning is based on the principle that a good clustering method should generate clusters with asymmetric distribution of functional classes, in which specific groups of functions are enriched in specific clusters. to evaluate this, we calculated a vector composed of the number of occurrences of each of the represented go terms across the entire gene set. this vector was called the average annotation profile. a similar vector was then calculated for each expression cluster, the cluster annotation profile . we then calculated the correlation between the annotation profile of each cluster and the average annotation profile of the entire dataset. the median of the correlations between the annotation profile of each cluster and the average annotation profile finally yielded an index called correlation with average annotation . a high cava value indicates that the various functions are represented in the various clusters in a similar way, and therefore indicates poor function partitioning. as shown in figure  <dig>  the annotation profiles of clusters generated by flame display the lowest correlation to the average annotation profile in two out of three datasets. hypergeometric distribution analysis indicated that the enrichment of go terms in clusters generated by flame reached statistical significance .

in both types of analysis, we noticed that some go terms maintain a wide distribution across all clusters and do not display particular expression patterns. this is in line with the fact that not all gene functional categories are expected to be coordinately regulated at the transcriptional level in a given set of experimental conditions.

to provide a quantitative readout of the comparative analysis between the various algorithms, we defined a way to rank the algorithms in each validation analysis based on the area below the index line plots. the algorithm giving the smallest area below the index line plot was assigned a rank of  <dig> , and the others obtain a progressively higher value . the results of this ranking procedure, illustrated in table  <dig>  show that no single clustering algorithm has always the best performance in all datasets and with all validation metrics. however, flame proved the best in many cases and, more importantly, its "performance profile" across the various datasets and validation metrics is profoundly different from those of the other algorithms. this indicates that flame can be a truly alternative clustering strategy, while, as an example, fsom and fcm, that are tightly related clustering algorithms, display an overlapping performance profile.

* reduced versions of the datasets were used for these fom analyses, due to excessive computation time required for the full-sized dataset .

discussion
we present here a new algorithm for clustering microarray data, flame, that exploits a typical feature of "real-life" biological clusters, like sheep herds and fish shoals: the behavior of one element is dictated by the behavior of its neighbors. in other fuzzy clustering algorithms, like fuzzy c-means or fuzzy k-means, the fuzzy memberships of data points are directly determined by their similarity to a series of calculated cluster prototypes . conversely, flame uses pairwise similarity measures only to define the neighbors of each gene and how close each gene is to its nearest neighbors, and then approximates the fuzzy memberships of each object from its neighbors' memberships. in this approach, the cluster "prototypes", that we named cluster supporting objects , are defined as individual genes having a particularly high number of neighbors. the behavior of such genes would therefore be an "archetypal" behavior, shared with many other genes, and therefore likely to correctly represent the data structure. after defining the csos, the membership approximation propagates like a wave from the csos to other far objects through a network formed by the neighborhood relationships. in this way flame, essentially, performs the clustering using not the expression data, but the local information extracted from them, which allows reliable capturing of both linear and non-linear relationships.

in some sense, flame is also a kind of self-organization method. however, this self-organization process is quite distinct from the one of self-organizing maps  and fuzzy som, which is based directly on the expression measurements. som also defines a neighborhood, but this neighborhood is defined only for neurons , and set in advance to constrain the cluster orientations independently from the dataset. in flame, instead, the neighborhood relationships are calculated for all objects, and are used to constrain the fuzzy memberships with no external inputs on the cluster number and size.

in principle, the possible applications of flame are not limited to gene expression datasets. in particular, the assumption that neighboring objects should have similar fuzzy memberships is well described as a mathematical cost function . minimization of this cost function renders flame theoretically very valuable, because the local approximation error could possibly be used in combination with other clustering constraints to get new and more powerful clustering algorithms. flame can be applied to any dataset including category datasets if a neighborhood can be defined for each object. in fact, a set of neighborhood relationships among the objects is the minimum requirement of flame, since a rough similarity between neighboring objects can be estimated as the fraction of their common neighbors.

CONCLUSIONS
flame is a new algorithm for microarray data clustering that brings significant improvements in the partitioning of genes based on their expression profiles. its good performances derive from a combination of advantageous features, some of which are distinctive, like the ability to capture dataset-specific structures by defining neighborhood relations and the subsequent neighborhood approximation of fuzzy memberships, so that non-globular and non-linear clusters can also be captured and do not get fragmented by the process. in particular, it is the novelty of neighborhood approximation that makes flame distinct from all other clustering approaches. other interesting features are common to fuzzy clustering algorithms, like non univocal assignment of memberships to genes and definition of outlier genes whose expression pattern does not allow reliable assignment to any cluster.

flame implementation into newly developed, dedicated c++ software allows to fully benefit of the properties of the algorithm, and to apply it also to large datasets using a standard pc with very short calculation time. the fine-tuning of the algorithm, available in the advanced mode, renders flame very flexible and capable of handling datasets of different size and expression heterogeneity with good performances. our results also confirm that no clustering strategy is always the best for any data type, which renders the choice between different algorithms and the availability of various validation tools implemented in the gedas software extremely valuable during the optimization of the clustering.

