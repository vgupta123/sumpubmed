BACKGROUND
metabolomics is the term used to describe the study of small molecules or metabolites present in biological samples. examples of such metabolites include lipids, amino acids, bile acids, keto-acids. studies of the concentration levels of these molecules in biological samples aim to enhance understanding of the effect of a particular stimulus or treatment  <cit> . the most commonly applied analytical technologies to metabolomic studies are nuclear magnetic resonance spectroscopy   <cit>  and mass spectrometry   <cit> . with respect to nmr-based metabolomics the data are usually in the form of spectra which are binned into regions of a specified width. typically, the data generated by these methods are large and complex. firstly, the number of observations n in metabolomics experiments is typically much less than the number of peaks  p in a spectrum, n ≪ p. in such a situation, the application of standard parametric statistical methods such as regression is not straight forward as there are insufficient data for parameter estimation. secondly, many metabolites may not have any relationship with the trait under study and they can induce variation which is not relevant, hampering comprehensive data analysis  <cit> . in view of these difficulties, when analyzing metabolomic data there is a genuine need for multivariate dimension reducing techniques which can take into account the complexities of the data and expose any underlying relationships. principal components analysis   <cit>  is probably the most widely used technique for analyzing metabolomic data  <cit> . the popularity of pca in metabolomics is due to the fact that it is a simple non-parametric method which can project the nmr or ms spectra into lower dimensional space, revealing inherent data structure, and providing a reduced dimensional representation of the original data. despite its widespread use in metabolomics, pca has several shortcomings. most significantly, pca does not have an associated probabilistic model, which makes assessing the fit of pca to the data difficult and limits the potential to extend the scope of application of pca. additionally, pca can fail to reveal underlying groups of subjects in the data, therefore providing a spurious view of the underlying data structure  <cit> . other limitations include the inability of pca to deal with missing data appropriately  <cit> .

such limitations can be addressed by deriving pca from a probabilistic framework resulting in probabilistic pca   <cit> . the merits of a probabilistic approach are manyfold. firstly, the maximum likelihood solution of the ppca model corresponds to pca and hence the familiar characteristics  of pca are retained. with regard to model estimation, closed form solutions for parameter estimates exist. additionally, the expectation maximization  algorithm  <cit>  can be employed to estimate the parameters of the ppca model. the em algorithm is computationally efficient and also has the capacity to deal with missing data  <cit> . the probability density based approach facilitates comparison of different ppca models to determine the 'best' model for the data using statistically principled approaches. in practice, this allows selection of the number of required principal components in a statistically valid manner. given the probabilistic footing of the ppca model, the bayesian inferential framework  <cit>  can be employed for inference, facilitating the inclusion of any prior information the practitioner may have  <cit> .

perhaps the key advantage of approaching pca from a probabilistic modeling point of view is the facility to assess the uncertainty associated with the resulting model output. in practice, this translates to the ability to examine the uncertainty in an observations' principal component score, or in the estimated loadings. in this article, the construction of confidence intervals for model parameters such as loadings using the jackknife method is illustrated. thus a more principled and clear insight to the principal component solution is available under the probabilistic approach than under the traditional covariance matrix decomposition approach.

in general, metabolomics studies generate metabolomic data in addition to other phenotypic data, examples of which include age, gender and bmi  in the case of human based studies. inclusion of these covariates in multivariate models when analyzing metabolomic data is highly desirable in order to allow a comprehensive analysis of the data. in this article a novel extension of the ppca model called probabilistic principal components and covariates analysis  is introduced which incorporates covariates into the model and facilitates joint modeling of metabolomic data and covariates. another crucial benefit of the probability density based approach is that a collection of ppca models can be combined to form a mixture of ppca models  for nonlinear modeling purposes  <cit> . a mixture of ppca models can be used to cluster subjects whilst facilitating dimensionality reduction of metabolomic data. this model is closely related to the mixture of factor analyzers used to cluster microarray expression data in  <cit> . the application of mppca analysis to metabolomic data is presented and highlights the danger of assuming a single underlying ppca model in cases were  groups of observations are present.

these statistical methods  are illustrated through an application to two metabolomic datasets. a software package called metabolanalyze  <cit> , freely available through the r statistical software  <cit> , has been developed to facilitate implementation of the presented methods in the metabolomics community and elsewhere.

methods
probabilistic pca  is a probabilistic formulation of pca based on a gaussian latent variable model and was first introduced by tipping and bishop in  <dig>  <cit> . the ppca model reduces the dimension of high-dimensional data by relating a p-dimensional observed data point to a corresponding q-dimensional latent variable through a linear transformation function, where q ≪ p. given the statistical model underpinning ppca, extensions of the model are possible, and a wealth of statistical tools can be utilized. such extensions and tools are detailed in what follows.

probabilistic principal components analysis
let xi = t be an observed set of variables  for observation i and ui = t be a latent variable corresponding to observation i in the latent, reduced dimension space. in terms of traditional pca, ui can be viewed as the principal score of subject i. the ppca model can be expressed as follows

 x¯i = wu¯i+μ¯+ϵ¯i 

where w is a p × q loadings matrix, μis a mean vector and ϵ¯i is multivariate gaussian noise for observation i, i.e. p = mv np where i denotes the identity matrix. the latent variable ui is also assumed to be multivariate gaussian distributed, p = mv nq. the conditional distribution of the observed data given the latent variable can then be expressed as

  p = mvnp .

the distribution of the observed data, p, also known as the predictive distribution, can be derived from the convolution of p and p giving

 p = mvnp .

in contrast to the more conventional view of pca which is a mapping from the high dimensional data space to a low dimensional latent space, the ppca framework is based on a mapping from a latent space to the data space. the observed data xi is generated by first drawing a value for the latent variable ui from its unit variance multivariate gaussian distribution, p. the observed variable xi is then sampled, conditioning on the generated value for ui, from the isotropic distribution defined in .

any observed data point xi can be represented in a latent space by its corresponding q-dimensional latent variable ui. the distribution of the latent variable given the observed data can be derived using bayes' theorem to give

  p=mvnq,σ2m−1) 

where m is a q × q matrix defined as m = wt w + σ2i. a key benefit of the ppca approach is that, not only is an estimate of the location of each observation in the lower dimensional space available through its expected value e = m-1wt , an estimate of its associated uncertainty is available through the covariance matrix σ2m- <dig> in . this is in contrast to conventional pca where the lower dimensional location  of an observation is available, but the uncertainty associated with it is not. the parameters  of the ppca model can be estimated using maximum likelihood. maximizing the  likelihood function with respect to model parameters is non-trivial; in  <cit>  it is demonstrated that the estimates do however have closed form solutions. crucially, the log likelihood of the ppca model is maximized when the columns of w span the principal subspace of conventional pca  <cit> . thus the maximum likelihood estimate  of the loadings matrix Ŵ in ppca corresponds exactly to the loadings matrix in conventional pca. hence the model output in ppca is exactly that obtained in conventional pca, but with the additional advantages of uncertainty assessment and potential model extensions.

in this article maximum likelihood estimates of the model parameters are derived via the em algorithm  <cit>  because of its stability and widespread applicability. the em algorithm is typically used to compute mles in probabilistic models when the model depends on unobserved variables or when some data are missing. the algorithm alternates between two steps until convergence: the expectation  step and the maximization  step. in the e-step, the expected values of the latent variables are estimated given the observed data and the current estimates of the model parameters. in the m-step, the model parameters are re-estimated by maximizing the log likelihood function using the expected values of the latent variables derived in the previous e-step. the two steps are repeated until convergence. many convergence assessment criteria are available; some criteria are based on log likelihood gain between iterations while others use an estimate of the converged log likelihood value as a basis for stopping. here aitken's acceleration procedure  <cit>  is used for convergence assessment. specific details of the em algorithm for the ppca model are given in additional file  <dig>  an implementation of the algorithm is available in the package metabolanalyze through the r statistical software  <cit> .

probabilistic principal components and covariates analysis
with its basis in a statistical model, the ppca model can be extended in several ways. given the availability and relevance of subject covariates in metabolomic studies, here the ppca model is extended to facilitate joint modeling of covariates and metabolomic data, giving the probabilistic principal components and covariates analysis  model. this novel model extension is achieved by assuming that the latent variable distribution for observation i follows a multivariate gaussian distribution centered at δi rather than at the origin, i.e. p = mv nq , where

  δ¯i = αc¯i= 

here α is a q  matrix of parameters which capture the relationship between the latent variable and the covariates and ci is a  vector of an intercept term and the l covariates of observation i. the motivation behind this model extension is that a subject's covariates may influence their location in the principal subspace. conditional on this location, the observed data point is then generated as in . note that through the model definition  the covariates may have different effects on each of the dimensions of the principal subspace through the parameter vectors α <dig>  . . ., αq.

under the ppcca model, the conditional distribution of xi given ui is the same as that of the ppca model given in . the predictive distribution p differs from that of the ppca model and is now defined as

 p= mvnp .

the posterior distribution of the latent variable ui given the observed data xi is also affected by the inclusion of covariates and is defined to be

 p =mvnq+σ2δ¯i],σ2m−1) .

the location  of observation i in the latent space is given by e = m- <dig>  which depends on both the data point xi and the covariates of observation i through δi. thus when representing an observation in a reduced dimensional space the ppcca model takes account of both the spectra data and the associated covariates. deeper insight to the true underlying structure of the data is then feasible as possibly influential external factors are explicitly modeled.

the effect of covariates on the qth latent dimension can be explored through examination of the estimated regression parameter vector αq = t ; these parameters can be interpreted within the context of the problem to provide insight to the type and strength of influence some covariates may have on the  latent dimensions.

parameter estimation for the ppcca model can be achieved via an efficient em algorithm; specific details of the em algorithm for the ppcca model are given in additional file  <dig>  an implementation of the algorithm is available in the package metabolanalyze through the r statistical software  <cit> .

mixtures of probabilistic principal components analysis models
the models discussed so far assume that the association between the observed data and the latent variable is linear. this assumption can be inadequate in a situation where the observations in the data set have an underlying group structure. in such cases the linearity assumption may not reveal all of the internal structures of the data  <cit> . standard pca also suffers from this phenomenon.

in many high throughput technologies which result in high dimensional data, interest often lies in identifying underlying sub groups within a set of observations. exploring high dimensional data with underlying nonlinear structures therefore requires modeling attention. employing a single ppca model to model such data is not adequate, since ppca provides a globally linear projection of the data. a collection of single ppca models can be combined to obtain a mixture of probabilistic principal components analysis models   <cit>  which clusters observations into groups and reduces data dimension.

under a mppca model, with probability πg, observation i is modeled as

 x¯i = wgu¯ig+μ¯g+ϵ¯ig. 

here wg and μg are a p × q loadings matrix and the mean respectively for group g, and ϵ¯ig is a multivariate gaussian noise process for observation i, given that i is a member of group g. the latent location for observation i, given that i is a member of group g, is denoted uig. that is, with probability πg, observation i is modeled using a ppca model with group specific parameters.

observation i is assumed to have been drawn from a finite mixture distribution with g components , i.e.

 p = ∑g=1gπgp 

where p is a ppca model for group g with mean parameter μg and covariance matrix Σg=wgwgt+σ2i. the mixing proportion πg denotes the probability of an observation belonging to group g. note that for reasons of parsimony the error covariance  <dig> has been constrained to be equal for all groups  <cit> .

the mppca model can be fitted using a two stage em algorithm called the alternating expectation conditional maximization  algorithm  <cit> . for clarity, the details of the aecm algorithm for the mppca model are deferred to additional file  <dig>  under the mppca model, in addition to the latent location variable, the unobserved group membership of each observation is also viewed as a latent variable. specifically, for each observation, a latent binary vector zi = t is imputed where zig =  <dig> if observation i belongs to group g and  <dig> otherwise. at convergence of the aecm algorithm the estimate z^ig is the posterior probability of observation i belonging to group g. the mppca model clusters observations into groups by assigning them to the group to which they have highest posterior probability of membership. thus clustering of observations and dimension reduction, through the use of principal components, are achieved simultaneously.

model selection
a crucial advantage of working within a probabilistic framework is that a wealth of statistically based model selection tools can be utilized. this allows the determination of the "best" statistical model for the data, i.e. the optimal number of principal components q to retain and, in the case of the mppca model, the optimal value of g. such choices are made on the basis of statistical principles instead of using traditional ad-hoc approaches, such as a scree plot.

the bayesian information criterion   <cit>  is a popular model selection tool. the bic is defined to be

  bic = 2l−kln 

where l is the maximum log likelihood value, k is the number of free parameters in the model and n is the number of observations. the model which yields highest bic value is deemed the optimal model.

the bic can be viewed as a criterion that rewards model fit ) but penalizes model complexity ). the penalization in the bic is much stronger than that of the widely used akaike information criterion  <cit>  and typically selects more parsimonious models. within the context of mixture models, the bic has been widely employed, see  <cit> .

despite the tendency for the bic to select parsimonious models, in high dimensional data settings it often exhibits noisy behaviour-the bic can be undefined or perform poorly due to the occurrence of singularities for some starting values of the em algorithm, for some models , or for some numbers of groups . additionally, diagnosing convergence of the em algorithm in highly-parameterized models can be difficult, leading to noisy bic values.

to eradicate this issue, here a regularized version of the bic  <cit>  is employed as a model selection tool. this modified version of the bic evaluates the likelihood at the maximum a posteriori  estimator instead of the mle. the map estimator is derived within the em algorithm framework where a conjugate prior is included, and the convolution of the likelihood and prior are maximized at the m step. here, a conjugate inverse gamma prior on σ <dig> is employed throughout-the reported bic values are based on the map estimate for σ <dig> rather than on the mle. further details are provided in additional file  <dig>  this approach avoids singularities, and performs similarly to the bic when such issues are absent. it also has the effect of smoothing noisy behavior of the bic, which is often observed when parameter estimation is unstable.

jackknife resampling
the loadings of any probabilistic principal components based model can be used to identify variables responsible for the structure in the data. rather than examining the  point estimates of the loadings alone, a gauge of the uncertainty associated with the loadings can be obtained through estimation of their standard errors.

here the jackknife resampling method  <cit>  is implemented. standard errors are estimated by recomputing the loadings  with the ith observation removed from the dataset giving the loadings matrix w-i, for i =  <dig>  . . ., n. the jackknife standard error for the jth loading on the kth principal component is then estimated as

 se = n−1n∑i=1n <dig> 

where w_=1n∑i=1nw−i.

the standard errors can then be used to compute 95% confidence intervals  for the individual loadings. such cis can be used to identify loadings which differ significantly from zero on a selected principal component in the optimal model. those variables whose loadings are significantly different from zero relate to the variables responsible for the structure within the data. this approach therefore provides a sparse list of relevant variables.

computation time is often an issue when using the em algorithm to fit statistical models, and employing the jackknife technique to obtain standard errors would clearly exacerbate this problem. in practice computation times are considerably reduced by choosing good starting values for the algorithm for each of the n runs when using the jackknife. here the maximum likelihood estimates of the model parameters when fitted to the entire data set are employed as starting values for each of the jackknife runs, considerably reducing computational costs.

metabolomic datasets
the datasets used here were derived from a study previously reported  <cit> . brie y, animals were randomly assigned to two treatments groups and treated with pentylenetetrazole  or saline  for a period of  <dig> weeks. a third treatment group consisted of animals who received one injection only and these data are not used within this paper. throughout the treatment period urine was collected from the animals in collection tubes containing 1% sodium azide surrounded by ice. the animals had no access to food during this time but had free access to water. at the end of the treatment period brain regions were isolated and metabolites extracted as previously described  <cit> . nmr spectra were acquired and the spectra were integrated into bin regions of  <dig>  ppm using amix  excluding the water regions .

the urine dataset used herein was constructed from nmr spectra acquired from urine collected on day ten of the study; it consists of  <dig> spectral profiles  over  <dig> spectral bin regions.

the brain dataset comes from animals in the control group only with spectra acquired from tissues from four brain regions: the pre-frontal cortex, hippocampus, cerebellum and brainstem. in total, there are  <dig> spectral profiles over  <dig> spectral bin regions.

RESULTS
application of ppca to metabolomic data
to explore the effect of treatment with ptz, a ppca model was fitted to the urinary metabolomic data. parameter estimation was achieved via the em algorithm. a number of ppca models with varying numbers of principal components was fitted; a modified bayesian information criterion  was used to aid selection of the optimal model  where a higher value of the criterion indicates a preferable model.

the fitted ppca model is illustrated in figure  <dig>  figure 1a shows that the modified bic is maximized by a model with five principal components ; such a model explains 84% of the variation within the urine spectra data. should the principal scores be required as a reduced dimensional input to further statistical modeling of the data, the modified bic clearly indicates that a five dimensional representation is optimal. overall, it represents an unambiguous means of selecting the optimal number of principal components. for clarity, the scores and loadings on the first two principal dimensions are illustrated. the scores plot  reveals that grouping of animals with respect to their treatment status is evident on the first principal component. the 95% posterior sets are small, indicating little uncertainty in the scores. the associated loadings  are presented in additional file  <dig> 

the 95% confidence intervals  of the individual loadings are estimated using the jackknife technique -- these cis are used to identify loadings which are significantly different from zero. of the  <dig> spectral bins in the urine spectra dataset,  <dig> have loadings on pc  <dig> significantly different from zero.

in order to further identify metabolites that strongly influence the separation of the treatment and control groups and which will serve as markers for treatment response, significant loadings greater  than  <dig>  were selected. the cutoff value of  <dig>  was chosen by examining a frequency plot of the  loading values of the significant spectral bins. a region in the plot in which the number of selected significant spectral bins drops steeply while the loading values remain relatively constant may be used as an indication of a cutoff point. in the current analysis, the plot  drops at the value  <dig> .

seventeen spectral bins had  loading values greater than the cutoff point of  <dig>  and are illustrated in figure 1c, along with their 95% cis. further analysis was performed to identify which of these bin regions differ significantly between the two treatment groups . of the seventeen spectral bins, ten had signal intensities which were significantly different between the two groups. included in these changes were bin regions due to the drug administered . taurine levels , dimethylamine  and one unassigned peak  were significantly lower in the treated group, while isocitrate levels were significantly higher  in the treated group.

application of ppcca to metabolomic data
in addition to the urine metabolomic data the weight of each of the eighteen animals was recorded. inclusion of this covariate in the analysis was achieved using the ppcca model. specifically, the covariate is incorporated to the ppca model by allowing it to influence the score of each animal in the principal subspace. from , the expected value of the score of animal i, δi, is modeled as a linear function of its covariate :

 δ¯i=αc¯i= 

where α <dig>  . . ., αq <dig> are intercept parameters for each dimension of the principal subspace and α <dig>  . . ., αq <dig> are slope parameters for the weight covariate  for each dimension of the principal subspace. hence the influence of an animal's weight on their score in the principal subspace is explicitly modeled and can be interpreted through the parameter matrix α.

the em algorithm was employed to fit the ppcca model to the urine spectra and the weight covariate. figure 2a shows that the modified bic is maximized by a model with five principal components . examination of the  scores plot  and the loadings plot  indicates that while the parameter estimates on the first principal component dimension remain relatively unchanged from the fitted ppca model, the estimates on the second principal component dimension differ slightly. the general structure of the scores and loadings remains relatively unchanged suggesting that the animal's weights are not influencing the separation between treated and control animals on pc <dig>  additionally, it is apparent  that the uncertainty associated with the estimated scores increases under the fitted ppcca model. selection of inferential bin regions identified the same seventeen regions as those obtained using the ppca model .

the influence of the weight covariate can be quantified by examining the associated regression parameter matrix, detailed in table  <dig>  standard errors of the ppcca regression parameters were estimated using jackknife resampling. the parameter estimates and the associated 95% cis show that an animal's weight has a significant negative effect on the second principal component only  and is not contributing to the treatment effect observed on pc <dig> 

95% cis are given in parentheses. those estimates significantly different from zero are highlighted in bold.

application of mppca to metabolomic data
mppca was applied to the brain metabolomic data in order to determine the number of inherent groups  in the data. this application also illustrates the pitfalls of assuming a single ppca model when exploring heterogeneous data.

the scores plot resulting from fitting a single ppca model to the brain spectral data is illustrated in figure 3-it is immediately clear that there is a grouping  structure within the set of  <dig> observations. with such a strong clustering structure it would seem extremely unlikely that the same set of principal axes would be relevant to each group. fitting a mppca model simultaneously clusters the data into groups and reduces the dimension of the data within each group.

thirty-two different mppca models were fitted to the  <dig> spectra by varying the number g of ppca models in the mixture  from  <dig> to  <dig> and the number of principal components q from  <dig> to  <dig>  figure  <dig> is a heat map illustrating the modified bic value for each fitted mppca model; the bic suggests that the optimal model is the mppca model with four groups and seven principal components. this model can be used to cluster the observations into four groups and to visualize the data in each group within its principal subspace, hence exploring the structure relevant to each group. this method provides an objective means of identifying groups within the data.

in this illustrative example of the clustering and dimension reducing ability of the mppca model, the origin of each of the spectra was known. thus, treating the brain region of origin as an observations' 'true' group, the clustering performance of the method can be assessed, where each observation is assigned to the group for which they have largest posterior probability of membership, under the optimal mppca model. in the current example, the model correctly clusters all observations into their brain group of origin . furthermore, the model correctly separates the prefrontal cortex and hippocampus samples which overlap in the scores plot under the ppca model .

the usefulness of this approach in the metabolomics field lies in its application to studies where the number of underlying groups and the group membership of each subject is unknown-the mppca model can be used to identify g and the members of each group within a study. examples of such studies include the identification of disease phenotypes or treatment responsive phenotypes.

CONCLUSIONS
principal components analysis is the dominant statistical method currently employed within the field of metabolomics. principal components analysis has many merits and is particularly well used and understood by metabolomic researchers. however, the scope of principal components analysis is limited and extensions  are not possible, due to the lack of an underlying statistical model. as metabolomic research becomes more prevalent and data intensive, the development of methods which retain the familiar characteristics of principal components analysis while having additional analytical properties is of immediate importance.

this article demonstrates how probability density based methods can be used in the analysis of data resulting from metabolomics studies. probabilistic principal components analysis , and its equivalence with traditional pca, is introduced in  <cit> . thus ppca retains the familiar and useful properties of pca, but is based on a flexible statistical model. standard statistical tools are then available for use -- in this article a model selection criterion is employed as a principled approach to selecting the number of principal components to retain. additionally, uncertainty in the model estimates is assessed and standard errors are derived through the use of the jackknife technique. this provision of standard errors further aids model interpretation as inference on important model parameters such as loadings can be performed. here, standard errors are employed to construct confidence intervals which are then used to indicate which loadings  underlie the data structure.

in this article a novel model extension for ppca, principal components and covariates analysis , is proposed. the ppcca model offers a flexible way of including informative additional information in the ppca model. in the context of metabolomics, this of particular interest, as covariates can be hugely influential on the metabolomic pro le. jointly modeling such data in conjunction with metabolomic data is essential to facilitate comprehensive data analysis and understand the true metabolic changes occurring as a result of a particular stimulus. overall, incorporating covariates in the ppca model directly models any variation due to the covariates, thus ensuring that the principal components provide a clear picture of the structure underlying the data.

the use of a mixture of ppca models as a simultaneous clustering and dimension reduction technique for metabolomic data was demonstrated successfully. this application represents a robust approach to identifying the number of groups within a dataset. it has great potential use within the metabolomics field for identifying metabotypes which are responsive to certain treatments. additionally, a mixture of probabilistic principal components and covariates analyzers is an intuitive model extension which would provide clustering, dimension reduction and covariate modeling capabilities.

overall, the present study details novel methods for analysis of metabolomics data which are freely available through a software package called metabolanalyze  <cit> ; the package provides the facility to fit a ppca model, a ppcca model or a mppca model to metabolomic data, or indeed any other suitable data set.

authors' contributions
lb was involved in the study hypothesis, data interpretation and manuscript writing. cg was involved in the study hypothesis, data interpretation and manuscript writing. ng was involved in implementation and manuscript writing. all authors read and approved the final manuscript.

authors information
nyamundanda gift is phd candidate in the phd in bioinformatics and systems biology programme in university college dublin. dr. lorraine brennan is a lecturer in nutritional biochemistry in the school of agriculture, food science and veterinary medicine, conway institute, university college dublin. dr. isobel claire gormley is a lecturer in statistics in the school of mathematical sciences, university college dublin.

supplementary material
additional file 1
statistical details of model fitting.

click here for file

 additional file 2
loadings plots and plots to aid selection of the number of influential spectral bins.

click here for file

 acknowledgements
this research was supported by the irish research council for science engineering and technology  funded phd programme in bioinformatics and systems biology in university college dublin, ireland  and by hrb ireland .
