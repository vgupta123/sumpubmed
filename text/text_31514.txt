BACKGROUND
systems biology is a field that relies on both technical and scientific innovations. the technical innovations enable new scientific questions to be asked, and these in return make further demands for advances in technology. high throughput experimentation has been the main driving force behind these advances, and has primarily encompassed measurement types.

two measurement types that have seen a vast increase in utility and volume are high-throughput sequencing  and mass spectrometry based proteomics. the dramatic increase in the volumes of data are due to changes in instrumentation. in proteomics the adoption of new techniques, principally targeted approaches and high resolution instruments, means that there is a need to capture and mine vast quantities of high resolution spectra to enable the design of new assays. in genomics the cost of hts is now at a scale where populations of genomes and transcriptomes can be captured, and this is being done in a number of projects . this paper outlines an approach to the development of visual tools that have been developed to allow for the direct mining and usage of data derived from these two technologies. development of such tools requires an understanding of both the scale of data and the typical needs of the user in any exploration. the basic approach involves providing interactive high level overviews of the data, and then allowing for the selection and drill down into smaller data sets. separate visualization tools are used at each level of data exploration and linked to enable users to quickly move between data views. the fast moving pace of research means that these tools must be put in place quickly, and so they have been built on top of a series of rapid application development technologies, and are delivered as web applications.

the example tools support two major systems biology projects, the peptideatlas  <cit>  and the cancer genome atlas   <cit> . the peptideatlas, encompasses srm  data across multiple species as well as shotgun based identifications, and the tcga is a multi-institution effort to genomically characterize ten thousand cancer genomes across  <dig> different cancers.

methods
high throughput visualization tools are required to allow for the exploration of large data sets. the data sets in question consist of thousands of genomic sequences and protein mass spectra. as these atlases are relatively new, work to provide visual mining is in its infancy, however there has been a large amount of work in visualizations in the areas of network and gene expression visualization that is being adapted and learned from.

systems biology generally requires the integrated analysis of different data types  <cit> . in systems biology the majority of information visualization has tended to focus on direct representations of networks  <cit> . this is due to the fact that networks are often used to describe the dynamics of living systems . network visualization has been studied in a large number of disciplines . the interest in networks and molecular interactions has resulted in the progression of network visualization techniques, in particular involving the portrayal of the complexities of relationship characteristics using number of edge techniques  <cit>  . additionally, the context in which parts of the network exist, either through the discovery of motifs or through semantic similarities, have been used to reduce the graph's visual complexity  <cit>  . these ideas are being applied to visualizations of systems biology networks  <cit> .

alternative metaphors for the representation of complex data have also been explored. gene expression array based experiments have provided a rich area for the development of visual tools. in particular visualization of gene expression data has extended a number of popular n-dimensional data techniques: projecting high dimensional data down to two dimensions e.g. pair-wise scatter plots  <cit>  and parallel coordinates  <cit> ; encoding aspects of the data onto intrinsic and  extrinsic properties ; and using dimension reduction techniques, which transforms the data onto a small number of dimensions . innovations have also arisen from these investigations in terms of improved representation of the data  <cit>  and the provision of specialised visualisations which present the data in a relevant context . a number of visualization suites have been developed which combine these approaches .

due to their scale and complexity the visualizations of large repositories of genomic and proteomic data do represent new challenges, however it is possible to use many of the general information visualization techniques. we provide details of the visualizations that have been used across data from both the peptideatlas and tcga to enable users to explore the large-scale, highly dimensional data.

thousands of genomes
the cancer genome atlas  will, over the next three years, generate  <dig>  patient genomic sequences across  <dig> different cancers. the goal is to provide a map of large-scale, genomic mutations, both between difference cancers  and across patients within a single cancer. using these data, maps of normal variation, disease related disruptions and disease progression can be created for further analysis. ultimately this atlas will provide a rich set of data to enable better characterization of disease sub types and the development of targeted therapies.

the data gathered by tcga includes both full and exon-only genome sequences, epigenetic and transcriptomic data, and clinical information . at the scale of thousands of patients this means that providing effective ways to visually explore this data is necessary for the development of useful analyses or in targeting areas for investigation. such exploration requires the use of visual tools specifically adapted to data exploration at multiple scales. as the aim of tcga is to provide genomic data across entire populations of patients and diseases, visual tools must enable exploration using specific knowledge  as well as providing for discovery of new information.

various analyses are being performed across this data including pathway analysis, identification of functionally significant mutations and snvs, tissue biopsy and imaging, microrna regulation, and gene dosage analysis . each of these focuses on a different set of data within the atlas. providing a generic visualization would result in a visual tool too abstract to be easily useful. instead a set of tools targeted toward the specific analysis and underlying data is necessary. in developing tools for the analysis of gene disruptions, specifically structural variation, a linked set of interactive visual mining tools is used to directly compare the underlying genomic data. each tool in the set is interactive so that genomic events can be discovered through exploration and used to find further information at each level .

the user starts from a high level parallel coordinates view  <cit>  for exploration of differences in specific gene disruptions across multiple cancers . providing this view first requires that the underlying genomic data is processed with this exploration in mind. structural variations  were detected using a number of analysis tools such as breakdancer  <cit> , and the resulting data was then processed to remove biases. coverage biases were removed using a biclustering technique, and mutual information was used to filter sets of genes that had strong dependencies . the transformed data was then visualized using an interactive parallel coordinates view to show the proportion of patients where a given gene showed structural variation in each cancer. this cross-cancer view can then be mined for genes that are interesting across one or more cancers. the list of genes found to be interesting in this high-level visualization can be used to drill further into the data. the genes found in the high level view are then used to provide a visualization that focuses on a single cancer. the circos  <cit>  based tool shown in figure  <dig> provides an interactive view of the similarities and differences across patient samples within a single cancer. the circular view enables an intuitive layout of chromosomes, while the connections between them display associations that can include various features, scores or other gene related information. this allows for the mining and integration of gene data with patient information to locate gene disruption locations in a manner that is easily understood. the final linked tool  provides a single sample level view to allow exploration of genes identified in the cross-cancer  and single cancer  tools. this zoom level focuses on a short chromosomal location  for a set of patients. the normal and cancer samples are shown side by side, allowing for the direct comparison of the structural variations within that location. at each level within the hierarchy of visual tools, the mining and filtering of the provided information enables a user to drill into the genes and samples that may provide the most information. additionally, it should be noted that a different type of visualization was used at each level. this ensured that the scale of the data being viewed could be quickly comprehended , enabling context appropriate views at each step of analysis.

millions of spectra
the peptideatlas repository contains thousands of experiments and is designed to provide a compendium of the likelihood of a given peptide being detected on mass spectrometers. the repository contains information from thousands of mass spectrometry experiments  across numerous species, tissues and disease conditions. the goals in providing this repository are both to annotate genomic information with observable peptides, and to provide an integrated view on a given proteome so it can be used as the basis of selected reaction monitoring  experiments. peptideatlas can be used to identify representative  peptides that are unique to an individual protein. mining the data it contains makes it possible to identify which transition patterns could be used to uniquely identify any set of proteins in the proteome. this allows for the design of targeted proteomic experiments, where the experimenter defines a priori which proteins they wish to detect and using the atlas, find which specific transitions should be scanned for. targeted approaches can monitor at low mass/charge  levels, and have been shown to detect protein concentrations at low copy number  <cit> . as srm can be used on complex tissues, a minimum of separation chemistry is needed. this means that experiments can more accurately detect smaller amounts of protein in complex samples.

mining this data requires the use of a number of integration strategies and information theoretic approaches to connect the peptide data with information from genomic sources , disease literature , and pathways . providing visual tools that access this integrated data allows for refinement of biomarker or transition target lists from many thousands to the tens or hundreds that are detectable. the purpose is typically to identify peptides that will be suitable biomarkers for a specific disease or disease sub type. the mining tasks are rarely initiated without prior knowledge, instead they are typically initiated either through associations with other measurement types or through the literature information.

as with the genomic visualizations, those developed for exploring the peptideatlas are linked and interactive to allow for greater detail to be provided at the appropriately zoomed level. mspecline  <cit>   is the first level mining tool which combines knowledge about human disease from medline with empirical data about the detectable human proteome from spectral libraries. it associates diseases with proteins by calculating the semantic distance, based on the co-occurrence of disease and protein terms in the medline bibliographic database, between annotated terms from a controlled biomedical vocabulary. this association allows for the exploration of relationships between human diseases and parts of the proteome that are detectable using current instrumentation. given a disease, the tool will find proteins and peptides from peptideatlas that may be associated, and display relevant literature information from medline. these associations can be visually explored, and the results directly exported to the experiment design pipeline ataqs  <cit>  or explored at subsequent levels of visualization with additional data.

the next visualization, called circatlas , provides a high level view of observed proteomic data in peptideatlas overlaid with genomic information and the concordance between multiple feature types. again using the circular view of the genome , observed peptides and protein information is overlaid on chromosomal locations. the zoomed in "track" view of a specific chromosome provides more detailed information regarding the observed data and feature annotations. finally, this data can be integrated with pathway information and tcga analytical data to provide a cytoscape  <cit>  network view  that provides clinically relevant information about possible disease biomarkers.

RESULTS
one of the important aspects of this work is that the tools must support active research, where the data sets are continually growing and often changing in scale and complexity  <cit> . this means that the requirements continually change, and this must be factored into the design of visual analytic tools and the associated software technology choice  <cit> . in most cases information visualizations require a costly investment in terms of expertise, user feedback, and developer time. such investment is beyond most research groups, who must put tools in place quickly, and so often a simple, minimal functionality approach needs to be adopted. instead of focusing on the best solution, we have found that visualizations must be used together, as each visualization has specific strengths in terms of: ability to work with different sizes of data ; portraying generic aspects of information so that they can be used with multiple data sources; ease of use and understanding; and their suitability for specific tasks .

in addition to good and scalable design, we have found that there are three important facets to the development of visualizations for large scale data repositories which allow for suitable functionality to be put in place:

 <dig>  rapid development and deployment of visualizations, which allows for the development of tools to suit specific tasks through the use of software technologies.

 <dig>  nested task specific views, which allows for the adoption of best information visualization practices without dramatically increasing development time.

 <dig>  common understandable metaphors, which allows for acceptance as intuitive understanding minimizes learning time.

rapid development and deployment of visualizations
visualization is a crucial mechanism for discovering meaningful information from research data. the high volumes, complexity and heterogeneity of the proteomics and genomic data repositories means that representations that simply mirror the data are not appropriate. as research is not a static process, but rather an ongoing dynamic investigative endeavour, the visualizations must be able to deal with highly diverse and continually changing types of information. this means that it is difficult, or in many cases impossible, to provide one visualization that suits all users and usage. instead, it is more effective to adopt a task based approach, where visualizations are provided for specific tasks . if required, common visualizations can then be developed and refined. as data production and research are a constantly moving target, any tools provided to mine the data must be developed as quickly as the data is being produced.

as creating entirely new tools is a time and developer intensive process, using rapid application development  techniques, appropriate visual analytics can be quickly provided. by adapting tools and using appropriate frameworks, rather than creating entirely new ones, it becomes practical to create views that are specific to a particular task or research direction. for the work discussed in this paper we adopted a number of standard software frameworks. a number of web-based frameworks for both data and visual development were used that enable both rapid application development and rapid data delivery. these include: the google data source libraries for simple, standard data access using sql database-like language; extjs and gxtjs  for quickly creating intuitive, web-based user interfaces; and web-based visual libraries such as protovis  <cit> . well-known and understood desktop tools such as cytoscape are also adopted and integrated where appropriate. the use of existing libraries and visual tools has enabled the development of interactive, usable tools  within days or weeks rather than months or more.

nested task specific views
while it is necessary to enable general exploration, researchers often need to explore the data from a particular starting point. our visualizations are generally dedicated to a limited set of tasks however, so providing multiple levels of linked visualization for the data becomes necessary . information visualization has advocated common workflows and scopes for the development of visualizations which are useful when accessing massive data sets. the macro and micro view  <cit>  ideas have more recently evolved into the information seeking mantra  <cit>  workflow . such a workflow offers a practical approach to the delivery of visualizations, so that they can be accessed using desktop and web based tools. however, development of initial "overview" or macro visualizations is not straightforward with large research data sets, and depends largely on the task that is going to be undertaken. for this reason we have typically provide a number of macro views which can be used to start filtering or analyzing the data.

in the peptideatlas project the visual tools provide three different "overview" starting points to explore the underlying data. using the mspecline tool  users can mine literature associations through a disease of interest  and find observed peptide spectra that are linked within the literature. these spectra can then be viewed in the context of the genome or directly viewed. using the peptideatlas circos visualization, spectra can also be searched by a gene of interest or chromosomal location  then viewed or exported to other tools. alternatively, data from other experiments  or network inference analysis can be imported into cytoscape, and then information about the suitability of the associated proteins to act as biomarkers can be overlaid .

using data from tcga analyses a separate set of macro visualizations provides users with several methods to search through the data. starting from the cancer comparator , a user can explore gene disruption rates across multiple diseases. this list of genes can then be used to mine a single cancer across multiple samples as within the genome visualization . the genome visualization can then lead further to specific samples where the gene of interest can be compared across tens of patient samples with the disruptions annotated .

common understandable metaphors
information visualization provides a means for the non-expert user to mine, interrogate and interact with information at a highly conceptual level. this conceptualization is through a mental model  of the data which is shared by both the developer of the visualization and the end-user. the majority of information visualizations are based around the idea of providing a metaphor which is easily and immediately understandable enables rich interactions with complex and diverse data sets.

in the development of the visualizations discussed above it was found that immediate understanding, typically through the use of common metaphors or positive knowledge transfer, were an important facet of the success of the visualization in portraying information. rapid understanding and user acceptance of a visualization is important, as it allows scientists to immediately understand what is being portrayed. for example, the circos plot suffers due to problems associated with the use of atypical non-rectangular based interactions, making it more difficult to use standard mouse drag based operations. however, the familiarity of the metaphor means that people are willing to accept these limitations as they understand both the visual encoding of the information, and are familiar with the layout. conversely other visualizations, such as the structural variation visualization, are less easy to immediately understand and so general acceptance is limited. in our experience, easy to understand visualizations are those that are used widely, as the researchers themselves typically seek to apply the visualization to other data sets. the metaphors that are familiar are frequently those that are popularly and generically used in visualization  or those that are commonly used in a specific domain .

CONCLUSIONS
biology is a big data science with the added complexity that there is no clear understanding as to how the data may be used. supporting the scale of data that is being generated has led to the development of a number of large scale repositories, and the need to provide visualization tools to mine this data. this paper discusses work that has been undertaken to provide such visual mining tools, and also discusses lessons that have been learned providing tools for both local researchers and the wider community.

we have developed a number of bespoke visual tools, but have preferred to adopt more commonly used designs. in this paper we have discussed a number of visualizations, including: an interactive circos viewer with context sensitive zooming provided through the track viewer, which shows genomic features and their interactions; parallel coordinates, which is used to show and analyze comparisons of genes that are disruptions in different cancers; a table based view, for exploring proteins which are associated with specific diseases and are detectable; and a gene rearrangement viewer, which shows the complexity of localized rearrangements that have been identified through anomalies in read-pairs. we have found that delivery using web technologies is preferable, both due to the low admin requirements and the diverse community using the data. these visualizations each have advantages and disadvantages. the macro views, such as circos and the parallel coordinates, are relatively easy to understand and use as they provide a high level overview. the nested views, such as the track and gene rearrangement views tend to be more specialized and therefore require some level of learning. interactivity suffers with the high level views due to the number of items being displayed, this is especially true due to the limitations in web based delivery. where possible principles of information visualization have been adopted which have been used extensively elsewhere to visualize biological data . however, practicalities due to the demands of research  means that good design always has to be weighed against rapidly delivering multiple visualizations with the required functionality.

visual exploration of the large-scale atlas data sets being produced by the peptideatlas projects and to support tcga analysis allows researchers to mine data to find new meaning and make sense at scales from single samples to entire populations. providing task specific views that allow a user to start from points of interest  allows targeted exploration of thousands of spectra and genomes. as the composition of the atlases changes, and our understanding of the biology increase, new tasks will continually arise. it is therefore important to provide the means to make the data available in a suitable manner in as small a time as possible. we have done this through the use of common visualization workflows, into which we rapidly deploy visual tools which follow common metaphors where possible to assist in understanding. rapid development of tools and task specific views allows researchers to mine large-scale data almost as quickly as it is produced. ultimately these visual tools enable new inferences, new analyses and further refinement of atlas level data.

authors' contributions
jb and sk conceived of the projects and wrote the manuscript. rk and rb developed the software for visual display. all authors read and approved the final manuscript.

