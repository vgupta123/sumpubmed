BACKGROUND
dna assembling is the problem of determining the nucleotide sequence of a genome from its substrings, called reads. since dna assembling is the first step in bioinformatics research, there are many different technologies, e.g. bac-by-bac approach  <cit> , sanger technique  <cit> , for getting reads from a genome and there are many assembly algorithms  <cit>  for solving the dna assembling problem. in recent years, there is a technology breakthrough on getting reads from genomes. while the traditional technologies produce long reads  with low coverage  and low error rate, the next generation sequencing  technologies, e.g. solexa  <cit> , illumina  <cit> , can produce short reads  with high coverage  and high error rate using much less time and cost. theoretically, we can determine the sequence of a genome in much shorter time and lower cost using the ngs technologies. however, many existing dna assembling algorithms  <cit>  were designed for traditional technologies which can handle reads with low error rate only and many new algorithms  <cit>  designed for the ngs technologies assume the input reads are error free. correcting errors in reads becomes an important problem for dna assembling  <cit> .

since the ngs technology produces reads with high coverage, a read may be sampled several times in the genome. under the assumption that an error read is unlikely to be sampled several time, sundquist et al.  <cit>  designed an algorithm called shrap which corrects the error reads by considering the number of times a read being sampled. if a read is sampled more than m times, for some predefined threshold m, it is considered as a correct read, otherwise, an error read which will not be used in the assembly step.

however, since the reads are randomly sampled from the genome, some correct reads may be sampled less  than the others, it is difficult to determine the threshold m to minimize the number of false negatives  and the number of false positives . besides, many reads with only one or two errors are wasted and will not be considered in the assembly step.

in order to consider reads with only one or two errors in the assembly step , chaisson et al.  <cit>  proposed another approach, called ecindel, to correct the errors in reads. instead of considering the number of times a read being sampled, they considered the number of times each length-k substrings, called k-tuple, being sampled. a k-tuple is treated as correct if and only if it is sampled at least m times. by reducing the value of k, a higher threshold m can be set  such that both the false positives and false negatives are small. besides, error reads can be corrected by replacing some nucleotides in the reads such that all length-k substrings in the reads are correct k-tuples. although this method seems nice, the value of k cannot be set to an arbitrary small number, e.g. when k =  <dig>  we know that all 1-tuple, 'a', 'c', 'g' and 't' are correct and we cannot use this information to correct the error reads. thus there is still a problem of how to set the optimal thresholds k and m.

wong et al.  <cit>  designed another algorithm, called srcorr, which improves ecindel by considering multiple k and m. instead of considering only one pair of thresholds k and m, several sets of correct k-tuples with different lengths are determined and used to correct the error reads. although some improvements have been made on correcting error reads, it is still difficult to set the thresholds.

in this paper,  we propose a method to calculate the probabilities of false positive and false negative for different substring lengths k and thresholds m in a data set. experimental results show that the calculated probabilities match with the real data and simulated data.  based on this calculation, we calculate the optimal m  for each substring length k. by using the optimal threshold m, the total errors can be reduced by  <dig> % and  <dig> % when compared to ecindel  <cit>  and srcorr  <cit>  respectively.

RESULTS
when the hidden genome g is known, we can count the number of true positives tp , false positives fp  and false negatives fn  for each threshold m. therefore, we can find the optimal threshold m that minimizes the total errors fp + fn.

however, when solving the dna assembling problem, the genome g is unknown. both ecindel  <cit>  and srcorr  <cit>  do not have a sound theoretical analysis on how to set the threshold m. when the number of sampled reads is large, even the incorrect k-tuples are sampled m times or more, these algorithms have many false positives. when the number of sampled reads is small, even the correct k-tuples are sampled less than m times, these algorithms have many false negatives. instead of using an arbitrary threshold m, we calculate the expected number of true positives, false positives and false negatives according to the equations described in the methods section. by considering the optimal threshold m that minimizes the expected false positives plus false negatives , we can get a set of k-tuples with the minimum expected number of errors. in this paper, we will perform experiments on both real experimental data and simulated data. the experimental results show that  the expected number true positives, false positives and false negatives match with the real data. therefore, the optimal threshold m calculated by us minimizes the total errors .  by using the optimal threshold m calculated by us, the total errors reduced by  <dig> % and  <dig> % when compared to ecindel and srcorr respectively.

experimental results on real data
we performed experiments on a real data set from the human genome. the hidden genome is a subregion of the human genome of length  <dig>  length- <dig> reads are sampled from the genome using solexa  <cit>  techniques.

figures  <dig> and  <dig> show the number of false positives and false negatives for different threshold m on this data set when the substring length k are  <dig> and  <dig> respectively. since the number of false positives decreases with m and the number of false negatives increases with m, the total errors  is a u-shape curve. the minimum point of this curve represents the optimal threshold m that minimizes the total errors. besides, the optimal m* increases when the length of the k-tuple decreases. for example, the optimal threshold m* for 15-tuples is  <dig> which is larger than the optimal threshold m* for 20-tuples . according to the equations in the method section, we can calculate the expected number of false positives and false negatives. thus, we can find the threshold m with the minimum expected number of errors. we find that the threshold m is exactly the same as the optimal threshold m*

we compared the number of false positives and false negatives of ecindel and srcorr with our algorithm. in the experiment, we used k =  <dig> which is the default parameter of srcorr. since srcorr uses a range of substring length k, when comparing the performance on k-tuples, we considered the 15-tuples of srcorr only. when comparing the performance on reads, srcorr runs with multiple k and m to correct errors on reads. tables  <dig> and  <dig> show the performance of the algorithms on 15-tuples and reads respectively.

as described in table  <dig>  ecindel produces a set of 15-tuples with  <dig> errors . by considering multiple thresholds, srcorr reduces the number of errors to  <dig>  since the converge of this dataset is high, instead of using a small threshold m , we calculated an optimal threshold m* =  <dig> and reduced the number of errors to  <dig>  therefore, the number of errors were reduced by  <dig> % and  <dig> % when compared with ecindel and srcorr respectively. with a better set of 15-tuples than ecindel and srcorr, we corrected the reads such that the total errors reduced to  <dig> when compared with ecindel and srcorr respectively. note that when considering the corrected reads, we have less false positives and less false negatives than these two algorithms.

experimental results on simulated data
in this section, we compared the performance of ecindel, srcorr and our algorithm on simulated data. the simulated data was generated as follows: we generated a length-g genome sequence g with equal occurrence probability of each nucleotide . n length- <dig> reads were sampled from g with equal probabilities. each nucleotide in each read could mutate to another nucleotide with probability perr. the probability that a nucleotide mutates to each of the other nucleotide is the same . the n length- <dig> reads  were considered as input for the algorithms. similarly with the experiments on real data, we set the default parameter k =  <dig> of srcorr when comparing the k-tuples. when comparing the performance on reads correction, srcorr runs with multiple k and m.

tables  <dig> and  <dig> show the performances of the algorithms on 15-tuples and reads respectively when g =  <dig>  n =  <dig> and perr = 4%.

since there were relatively fewer reads being sampled  in this data set, srcorr applied a small threshold m =  <dig> for the 15-tuples. as srcorr chose this threshold without much analysis, the threshold selected was too small such that there were many false positives and the total errors was  <dig> for the 15-tuples. ecindel applied a fix threshold m =  <dig> and the total errors was  <dig>  based on the optimal threshold m* =  <dig> we derived, we produced a set of 15-tuples with the fewest number of errors . similarly for the real data set, since we have derived a set of 15-tuples with less errors than ecindel and srcorr, we could correct more error reads than ecindel and srcorr . the corrected reads produced by us had less false positives and false negatives than ecindel. since srcorr applied a small threshold,  <dig> more false positive reads were introduced when compared with our algorithm.

when compared to the previous set of simulated data, we had more sampled reads in this data set . since ecindel and srcorr applied a small threshold  for determining correct 15-tuples, they had many errors . instead of using a small threshold, we arrived at an optimal threshold m* =  <dig> which cound determine the correct 15-tuples with  <dig> errors only. with a set of 15-tuples with less errors, we could correct the errors in reads better than ecindel and srcorr and produced a set of reads with  <dig> errors, much less than ecindel and srcorr , and in terms of the number of false positives and false negatives; both of them were less than ecindel's and srcorr's results.

CONCLUSIONS
we have studied the problem of correcting error reads in dna assembling. we introduced a method to calculate the probability of false positives and false negatives of the k-tuples using different thresholds m. based on this calculation, we found the optimal threshold m* that minimizes the total error . our calculation can also be extended to total errors with different weightings of fp and fn. our algorithm, which uses optimal threshold m* to correct error reads, performs better than the popular algorithms ecindel and srcorr.

in the real biological data, we might not be able to remove all the false positives and false negatives by a fixed threshold m. it is mainly because the probability of each read being sampled is not the same in real experiment. this probability depends on the patterns of the reads, the positions of the reads in the genome and the adjacent reads. a better model might be needed to determine whether a k-tuple is correct  and to correct more error reads.

