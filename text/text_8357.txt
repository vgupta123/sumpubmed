BACKGROUND
the problem of developing machine-learning tools for protein function prediction has gained considerable attention during the last years. from a machine learning perspective, this task exceeds the "standard" settings of learning problems in that a protein can be involved in several different biological processes exhibiting more than one function. this means that the objects we want to classify  might belong to several classes, a setting which is referred to as multilabel classification. from a biological perspective, the information carried in these multilabels might be relevant for extracting correlations of functional classes. when it comes to predicting the function of new proteins, it is therefore desirable to develop tools that can explicitly handle such multiple labeled objects.

in this work we present a multilabel version of a nonlinear classifier employing mercer kernels. such kernel methods have been successfully applied to a variety of biological data analysis problems. one problem of using kernels, however, is the lacking interpretability of the decision functions. in particular, it is difficult to extract further insights into the nature of a given problem from kernel mappings which represent the data in implicitly defined feature spaces. it has been proposed to address this problem by using multiple kernels together with some combination rules, where each of the kernels measures different aspects of the data. methods for learning sparse kernel combinations have the potential to extract relevant measurements for a given task. moreover, the use of multiple kernels addresses the problem of data fusion which is a challenging problem in bioinformatics where data can be represented as strings, graphs, or high dimensional expression profiles. kernels provide a suitable framework for combining such inhomogeneous data under a common matrix representation.

existing algorithms for combining kernels recast the problem as a quadratically constrained quadratic program ,  <cit> , as a semi-infinite linear program ,  <cit> , or within a sequential minimization optimization  framework,  <cit> . methods for selecting kernel parameters have also been introduced in the boosting literature, see e.g.  <cit>  or in the context of gaussian processes, see e.g.  <cit> . our method extends these approaches in two major aspects: due to the generative nature of the underlying classification model, it can learn class correlations induced by multilabeled objects and it can be used as a "building block" in hidden markov models which allow the inclusion of further categorical information, such as the joint prediction of subcellular localization classes and functional classes. we show that these extensions significantly improve the predictive performance on yeast proteins.

methods
our classifier is based on an extension of the mixture discriminant analysis  framework, which forms a link between gaussian mixture models and discriminant analysis  <cit> . the algorithm for solving multilabel classification problems emerges as a special case of this clustering approach. in the following we will briefly outline the algorithm which is composed of the "building blocks" gaussian mixture models, discriminant analysis, adaptive ridge penalties and the proper handling of multilabels.

learning gaussian mixtures by lda
the dataset is assumed to be given as a collection of n samples xi ∈ ℝd, summarized in the  matrix x. consider now a gaussian mixture model with k mixture components which share an identical covariance matrix Σ. under this model, the data log-likelihood reads

lmix=∑i=1nlog⁡),     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgsbabdaahaawcbeqaaiabd2gatjabdmgapjabdiha4baakiabg2da9maaqadabagagiibawmaei4ba8maei4zacgaleaacqwgpbqacqgh9aqpcqaixaqmaeaacqwgubgba0gaeyyeiuoakmaabmaabawaaabmaeaaiigacqwfapacdawgaawcbagaem4aasgabeaakiab=z8amjabcicaogqadiab+hha4naabaaaleaacqwgpbqaaeqaaogaei4oasdccmgae0hvd02aasbaasqaaiabdugarbqabagccqggsaalcqqhjowucqggpaqkasqaaiabdugarjabg2da9iabigdaxaqaaiabduealbqdcqghris5aagccagloagaayzkaagaeiilawiaaczcaiaaxmaadaqadaqaaiabigdaxagaayjkaiaawmcaaaaa@5b8a@

where the mixing proportions πk sum to one, and ϕ denotes a gaussian density. the classical em-algorithm,  <cit> , provides a convenient method for maximizing lmix: in the e-step one computes the assignment probabilities p of objects xi to classes ck, while in the m-step the current parameters of the gaussian modes are replaced by the maximum likelihood estimates.

linear discriminant analysis  is a time-honored classifier that  finds the correct class boundaries if the class-conditional densities are gaussians with common covariance, which is exactly the supervised version of our model . this optimality is due to the fact that for given class labels, the maximum likelihood parameters of the model  can be found by lda, see e.g.  <cit> . this result has been generalized in  <cit> , where it has been shown that in the unsupervised "clustering" case the m-step can be carried out via a weighted and augmented lda: class labels are mimicked by replicating the n observations k times, with the k-th replication having observation weights p and the "class label" k.

following  <cit> , any  lda problem can be restated as an optimal scoring problem. let the class-memberships of the n data vectors be coded as a matrix z, the -th entry of which equals one if the i-th observation belongs to class k. the point of optimal scoring is to turn categorical variables into quantitative ones: a score vector θ assigns real numbers to the k levels of the categorical response variable, i.e. to the entries in the columns of z. the simultaneous estimation of a sequence of scores θk and regression coefficients βk, k =  <dig> ...,k, constitutes the optimal scoring problem: minimize

∑k=1k||zθk−xβk||2     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaaewaqaaiabcyha8jabcyha8jabdqfaahgadiab=h7axnaabaaaleaacqwgrbwaaeqaaogaeyoei0iaemiwagfaleaacqwgrbwacqgh9aqpcqaixaqmaeaacqwglbwsa0gaeyyeiuoakiab=j7ainaabaaaleaacqwgrbwaaeqaaogaeiifawnaeiifaw3aawbaasqabeaacqaiyagmaagccawljagaaczcamaabmaabagaegomaidacagloagaayzkaaaaaa@47fb@

under the orthogonality constraint Θ⊤z⊤zΘ = ik/n, where Θ =  and ik denotes the  identity matrix. in  <cit>  an algorithm for this problem has been proposed, whose main ingredient is a multiple linear regression of the scored responses zθk against the data matrix x. the algorithm starts with Θ = ik, and the optimal scores are derived from the solution of the multiple regression problem via an eigen-decomposition. it can be shown that solutions to  must satisfy a certain orthogonality constraint which allows us to start with a  scoring matrix Θ' that is orthogonal to a k-vector of ones. in the following we will always consider this latter variant which is beneficial since it reduces the number of regressions to k -  <dig>  for simplicity in notation we will still write Θ instead of Θ'.

returning to the above weighted and augmented lda problem, it has been shown in  <cit>  that the solution for this problem can be found by the standard optimal scoring version of lda after replacing the class indicator matrix z by its "blurred" counterpart z˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgabgwgaacaaaa@2df8@. the rows of z˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgabgwgaacaaaa@2df8@ consist of the class membership probabilities estimated in the preceding e-step of the em algorithm.

adaptive ridge penalties and kernelization
in order to find sparse solutions, we take a bayesian viewpoint of the multiple regression problem  and specify a prior distribution over the regression coefficients β. following some ideas proposed in the gaussian process literature, we choose automatic relevance determination  priors,  <cit> , which consist of a product of zero-mean gaussians with inverse variances ωi:

p∝exp⁡.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgwbaccqggoaakiiwacqwfyogycqgg8bafcqwfjpwdcqggpaqkcqghdistcygglbqzcqgg4baecqggwbacdawadaqaaiabgkhitmaaqadabaaccigae4xydc3aasbaasqaaiabdmgapbqabagccqgfyogydaqhaawcbagaemyaakgabagaegomaidaaaqaaiabdmgapjabg2da9iabigdaxaqaaiabdsgakbqdcqghris5aagccaglbbgaayzxaagaeiola4iaaczcaiaaxmaadaqadaqaaiabiodazagaayjkaiaawmcaaaaa@500c@

the hyper-parameters ωi encode the "relevance" of the i-th variable in the linear regression. a method called adaptive ridge regression finds the hyper-parameters by requiring that the mean prior variance is proportional to 1/λ, cf.  <cit> : 1d∑i=1d1ωi=1λ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaaiabigdaxaqaaiabdsgakbaadaaewaqaamaalaaabagaegymaedabaaccigae8xydc3aasbaasqaaiabdmgapbqabaaaaogaeyypa0zaasaaaeaacqaixaqmaeaacqwf7oabaaaaleaacqwgpbqacqgh9aqpcqaixaqmaeaacqwgkbaza0gaeyyeiuoaaaa@3dbb@, ωi >  <dig>  where λ is a predefined regularization constant.

the balancing procedure has the effect that some hyper-parameters ωi go to infinity. as a consequence, the coefficients βi are shrinked to zero and the corresponding input variables are discarded. following  <cit>  it is numerically advantageous to introduce new variables γj,i=ωi/λβj,i
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfzowzdawgaawcbagaemoaaomaeiilawiaemyaakgabeaakiabg2da9maakaaabagae8xydc3aasbaasqaaiabdmgapbqabagccqggvawlcqwf7oabasqabagccqwfyogydawgaawcbagaemoaaomaeiilawiaemyaakgabeaaaaa@3ea1@, ci=λ/ωi
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgjbwydawgaawcbagaemyaakgabeaakiabg2da9maakaaabaaccigae83udwmaei4la8iae8xydc3aasbaasqaaiabdmgapbqabaaabeaaaaa@3692@. denoting by dc a diagonal matrix with elements ci, we have to minimize

∑k=1k−1||z˜θk−xdcγk||2+λγk⊤γks.t.c⊤c=d,ci> <dig>      
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqabeqaeaaaaeaadaaewaqaaiabcyha8jabcyha8jqbdqfaazaaiaaccmgae8hude3aasbaasqaaiabdugarbqabagccqghsislcqwgybawcqwgebardawgaawcbaacbmgae43yamgabeaakiab=n7annaabaaaleaacqwgrbwaaeqaaogaeiifawnaeiifaw3aawbaasqabeaacqaiyagmaagccqghrawkiigacqqf7oabcqwfzowzdaqhaawcbagaem4aasgabagaesy==7gaaogae83sdc2aasbaasqaaiabdugarbqabaaabagaem4aasmaeyypa0jaegymaedabagaem4saskaeyoei0iaegymaedaniabgghildaakeaaieaacqafzbwccqafuaglcqaf0badcqafuaglaeaacqgfjbwydaahaawcbeqaaiabl2==ubaakiab+ngajjabg2da9iabdsgakjabcycasaqaaiabdogajnaabaaaleaacqwgpbqaaeqaaogaeyopa4jaegimaajaeiola4caaiaaxmaacawljawaaewaaeaacqai0aanaiaawicacaglpaaaaaa@6bdd@

we now consider the case of sharing weights over j blocks containing m regression coefficients each:

c=︸m times⊤.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwfjbwycqgh9aqpdaagaaqaaiabcicaoiabdogajnaabaaaleaacqaixaqmaeqaaogaeiilawiaeiola4iaeiola4iaeiola4iaeiilawiaem4yam2aasbaasqaaiabigdaxaqabaaabagaemyba0maeeiiaaccbagae4hdaqnae4xaakmae4xba0mae4xzaumae43camhakiaawij=aiabcycasiabc6cauiabc6cauiabc6cauiabcycasmaayaaabagaem4yam2aasbaasqaaiabdqeakbqabagccqggsaalcqgguaglcqgguaglcqgguaglcqggsaalcqwgjbwydawgaawcbagaemosaoeabeaakiabcmcapawcbagaemyba0maeeiiaaiae4hdaqnae4xaakmae4xba0mae4xzaumae43camhakiaawij=amaacaaaleqabagaesy==7gaaogaeiola4iaaczcaiaaxmaadaqadaqaaiabiwda1agaayjkaiaawmcaaaaa@6600@

note that for given weights c, eq.  defines a standard ridge-regression problem in the transformed data x˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgybawgaacaaaa@2df4@ = xdc. it is well-known in the kernel literature that the solution vectors γ^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiiwacuwfzowzgaqcaaaa@2e6b@k lie in the span of these input data, i.e. γ^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiiwacuwfzowzgaqcaaaa@2e6b@k = x˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgybawgaacaaaa@2df4@⊤αk, which means that the data enter the model only in form of the gram matrix  x˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgybawgaacaaaa@2df4@x˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgybawgaacaaaa@2df4@⊤. since we have assumed that a weight ci is shared over a whole block of m features, we can decompose this kernel as a weighted sum of j individual kernels:

k:=x˜x˜⊤=∑j=1jcj2x˜x˜⊤=:∑j=1jcj2kj.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwglbwscqgg6agocqgh9aqpcuwgybawgaacaiqbdifayzaaiawaawbaasqabeaacqwi9=vbaagccqgh9aqpdaaewaqaaiabdogajnaadaaaleaacqwgqbgaaeaacqaiyagmaagccuwgybawgaacamaabaaaleaacqggoaakcqwgqbgacqggpaqkaeqaaogafmiwaglbagaadaqhaawcbagaeiikagiaemoaaomaeiykakcabagaesy==7gaaaqaaiabdqgaqjabg2da9iabigdaxaqaaiabdqeakbqdcqghris5aogaeyypa0jaeiooaozaaabmaeaacqwgjbwydaqhaawcbagaemoaaogabagaegomaidaaogaem4sas0aasbaasqaaiabdqgaqbqabaaabagaemoaaomaeyypa0jaegymaedabagaemosaoeaniabgghildgccqgguaglcawljagaaczcamaabmaabagaegonaydacagloagaayzkaaaaaa@6082@

with x˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgybawgaacaaaa@2df4@ denoting a  sub-matrix of x˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgybawgaacaaaa@2df4@ consisting of one block of m input features. with the above expression we have arrived at the desired framework for learning sparse combinations of kernel matrices: the kernel matrices kj in  which have been formally introduced by partitioning an initial feature set into j feature blocks can be substituted by arbitrary kernels fulfilling the positive-semidefiniteness condition of valid dot product matrices. on the technical side, we have to minimize the "kernelized" version of eq. 

∑k=1k−1||z˜θk−αk||2+λαk⊤αk     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaaewaqaaiabcyha8jabcyha8jqbdqfaazaaiaaccmgae8hude3aasbaasqaaiabdugarbqabagccqghsislcqggoaakdaaewaqaaiabdogajnaadaaaleaacqwgqbgaaeaacqaiyagmaagccqwglbwsdawgaawcbagaemoaaogabeaaaeaacqwgqbgacqgh9aqpcqaixaqmaeaacqwgkbgsa0gaeyyeiuoakiabcmcapiab=f7ahnaabaaaleaacqwgrbwaaeqaaogaeiifawnaeiifaw3aawbaasqabeaacqaiyagmaagccqghrawkiigacqgf7oabcqwfxoqydaqhaawcbagaem4aasgabagaesy==7gaaogaeiikagyaaabmaeaacqwgjbwydaqhaawcbagaemoaaogabagaegomaidaaogaem4sas0aasbaasqaaiabdqgaqbqabaaabagaemoaaomaeyypa0jaegymaedabagaemosaoeaniabgghildgccqggpaqkcqwfxoqydawgaawcbagaem4aasgabeaakiaaxmaacawljawaaewaaeaacqai3awnaiaawicacaglpaaaasqaaiabdugarjabg2da9iabigdaxaqaaiabduealjabgkhitiabigdaxaqdcqghris5aaaa@71e2@

subject to c⊤c=∑j=1jcj2=d
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwfjbwydaahaawcbeqaaiabl2==ubaakiab=ngajjabg2da9maaqadabagaem4yam2aa0baasqaaiabdqgaqbqaaiabikdayaaakiabg2da9iabdsgakbwcbagaemoaaomaeyypa0jaegymaedabagaemosaoeaniabgghildaaaa@4024@, ci >  <dig>  the minimizing vectors α^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwfxoqygaqcaaaa@2e62@k, k =  <dig> ...,k -  <dig> can be found simultaneously in a very efficient way by employing block conjugate gradient methods  <cit> . the optimal weights c are found iteratively by a fixed-point algorithm similar to that proposed in  <cit> :

new=j∑k=1k−1cj2α^k⊤kjα^k∑k=1k−1∑l=1jcl2α^k⊤klα^k.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqggoaakcqwgjbwydaqhaawcbagaemoaaogabagaegomaidaaogaeiykakyaasbaasqaagqaaiab=5gaujab=vgaljab=dha3bqabagccqgh9aqpcqwgkbgsdawcaaqaamaaqadabagaem4yam2aa0baasqaaiabdqgaqbqaaiabikdayaaaiiwakiqb+f7ahzaajawaa0baasqaaiabdugarbqaaiabl2==ubaaaeaacqwgrbwacqgh9aqpcqaixaqmaeaacqwglbwscqghsislcqaixaqma0gaeyyeiuoakiabduealnaabaaaleaacqwgqbgaaeqaaogaf4xsdembakaadawgaawcbagaem4aasgabeaaaoqaamaaqadabawaaabmaeaacqwgjbwydaqhaawcbagaemibawgabagaegomaidaaogaf4xsdembakaadaqhaawcbagaem4aasgabagaesy==7gaaogaem4sas0aasbaasqaaiabdygasbqabagccugfxoqygaqcamaabaaaleaacqwgrbwaaeqaaaqaaiabdygasjabg2da9iabigdaxaqaaiabdqeakbqdcqghris5aawcbagaem4aasmaeyypa0jaegymaedabagaem4saskaeyoei0iaegymaedaniabgghildaaaogaeiola4iaaczcaiaaxmaadaqadaqaaiabiida4agaayjkaiaawmcaaaaa@7490@

practically, if during the iterations a component cj becomes small compared to a predefined accuracy constant, cj is set to zero, and in all regression problems the j-th kernel vanishes.

the algorithm proceeds with iterated computations of kernel weights c and the expansion coefficients αk, k =  <dig> ..k -  <dig>  we initialize the model with cj =  <dig> ∀j. in our experiments, the initialization did not critically influence the final result, as long as the initial cj's are non-zero and j <n. theoretical uniqueness results, however, are difficult to derive. for the special case without weight sharing  the above method is equivalent to the lasso model of ℓ1-penalized regression  for which a unique solution always exists if the dimensionality does not exceed the number of samples, d ≤ n. if d exceeds n , there might exist different solutions which, however, share the same globally optimal value of the functional . the experimentally observed insensitivity to different initializations is probably due to the weights-sharing constraints that shrink the number of different cj's from d to j. a theoretical analysis of the uniqueness of solutions, however, will be subject of future work.

multilabel classification
in multilabel classification problems, an object xi can belong to more than one class, i.e. it might come with a set of labels yi. we treat these multilabels in a probabilistic way by assigning to each observation a set of class-membership probabilities. these probabilities might be given explicitly by the supervisor. if such information is not available, they might be estimated uniformly as 1/|yi| for classes included in the label set yi, and zero otherwise. after encoding these probabilities in the "blurred" response matrix z˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgabgwgaacaaaa@2df8@ , we run one optimization step  described above.

kernel discriminant analysis is a generative classifier which implicitly models the classes as gaussians in the kernel feature space. the effect of multilabels on the classifier during the training phase can be understood intuitively as follows. if there are many objects in the training set which belong to both the classes ci and cj, the respective class centroids μi, μj will be shrunken towards the averaged value 1/2·. in this way, the classifier can learn the correlation of class labels and favor the co-prediction of class i and j.

for discriminant analysis it is straightforward to compute for each object a vector of assignment probabilities to the individual classes ck, see e.g.  <cit> . in a traditional two-class scenario we would typically assign an object to class c <dig> if the corresponding membership probability exceeds 1/ <dig>  in multilabel scenarios, however, an object can belong to different classes so that we have to find a suitable way of thresholding the output probabilities. in analogy to the classical two-class case, we propose to sort the assignment probabilities in decreasing order and assign an object xi to the first k classes in this order such that

∑j=1kpsorted≥τ,     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaaewaqaaiabdchawnaacaaaleqabaacbagae83camnae83ba8mae8ncainae8hdaqnae8xzaumae8hzaqgaaogaeiikagiaem4qam0aasbaasqaaiabdqgaqbqabagccqgg8bafiewacqgf4baedawgaawcbagaemyaakgabeaakiabcmcapiabgwmizigaciab9r8a0jabcycasiaaxmaacawljawaaewaaeaacqai5aqoaiaawicacaglpaaaasqaaiabdqgaqjabg2da9iabigdaxaqaaiabdugarbqdcqghris5aaaa@4e6b@

where τ is a predefined threshold . by varying τ one can record the usual precision-recall curves, cf. figure  <dig> for an example.

algorithm  <dig> multilabel kernel learning via adr regression

training: /* we start with one e-step */

compute "blurred"  response matrix z˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgabgwgaacaaaa@2df8@ encoding the membership probabilities.

/* now follows one single m-step */

compute initial  scoring matrix Θ <dig>  see  <cit>  for details;

initialize ci =  <dig>  ∀ i =  <dig> ...,j;

repeat

compute αk, k =  <dig> ...,k -  <dig> as the solution of the linear systems ;

recompute the kernel weights c, see ;

until ||c - c|| <ε

compute fitted values and projection matrix for the discriminant analysis subspace, see  <cit> ;

prediction:

compute projected test object x˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacuwf4baegaacaaaa@2e3c@* and mahalanobis distances to class centroids;

compute class membership probabilities p and extract multilabels according to eq..

model selection
for the purpose of model selection, we assume that we are given a set of kernel matrices over multi-labeled objects. we further assume that the rule of deriving "fractional labels" from the multilabels is given. in the absence of further prior knowledge , we assume that the fractional labels are derived by averaging over all classes to which an object is assigned. under these assumptions, the model contains only two free parameters, namely the regularization constant λ in eq.  and the threshold τ for predicting multilabels in eq. . in our experiments, the former is estimated via cross-validation on the training sets. a unique multilabel-threshold can also be estimated by cross-validation, but in this work we report the full precision-recall-f1-curves obtained by varying this threshold, see figure  <dig> 

functional classes and subcellular localization
in several classification tasks in bioinformatics, more than one classification scheme is available to assign the data to certain groups. proteins, for instance, can be classified not only according to their function, but also according to their subcellular localization. for the yeast genome, such a localization-based scheme is available from cygd, see table  <dig>  if for some proteins their subcellular localization can be predicted with high reliability, and if we find high correlations between the corresponding localization classes and certain functional classes, we can potentially exploit this prior knowledge to increase the performance of the functional classification.

we combine both classification schemes by way of a hidden markov model . this choice was guided by the observation that even the standard k-class discriminant analysis model can be viewed as a simple hmm consisting of k emitting nodes and a silent begin and end node, see the left panel of figure  <dig>  the k emitting nodes represent the values of the hidden variable ℱ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfxeiraaa@3787@ . the gaussian emission probabilities are derived from the classifier. the transition probabilities are estimated by the empirical frequencies of class membership in a training set.

a second classification scheme is included in the automaton by adding another layer with l emitting nodes. these additional nodes correspond to a gaussian mixture model with l components that in our case correspond to subcellular localization classes, see the right panel of figure  <dig>  the emitting nodes in the first layer represent the values of the hidden variable ℒ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfsectaaa@376e@ , whereas the nodes in the second layer represent the values of the second hidden variable ℱ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfxeiraaa@3787@ . there are now two "observed data" variables x
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfxepwaaa@384f@ <dig>  x
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbamrthrhal1wy0l2yhvtyaehbnfgdovwbhrxajfwnaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabawaaegaeaaakeaaimaacqwfxepwaaa@384f@ <dig> that are assumed conditionally independent given the states of the two hidden variables. this independence assumption might be justified by the use of sparse kernel selection rules in each layer which typically induce nearly orthogonal feature spaces. the emission probabilities p and p are learned separately in the two layers. as in the former case, the individual transition probabilities are estimated by empirical frequencies on the training set. for predicting multilabels in the second layer, we compute the posterior probabilities via the forward-backward algorithm. the number of labels to be assigned is again found by thresholding the sum of ordered probabilities, cf. eq. . varying this threshold yields precision-recall curves as depicted in figure  <dig> 

locality due to pairwise kernel classifiers
the method introduced above finds a "global" set of kernels for the full multi-class problem. in some applications, however, it might be desirable to further investigate the discriminative power of kernels in a more class-specific or "local" setting. this can be achieved by an alternative approach to multi-class discriminant analysis based on the pairwise coupling scheme in  <cit> . the main idea is to find a k-class discriminant rule  by training all k/ <dig> possible two-class classifiers and coupling the obtained conditional membership probabilities rij := prob to a consistent k-class assignment probability. in other words, we want to find probabilities p <dig> ...,pk such that the quantities sij:=pip1+pj
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgzbwcdawgaawcbagaemyaakmaemoaaogabeaakiabcqda6iabg2da9maalaaabagaemicaa3aasbaasqaaiabdmgapbqabaaakeaacqwgwbacdawgaawcbagaegymaedabeaakiabgucariabdchawnaabaaaleaacqwgqbgaaeqaaaaaaaa@3c78@ are as close as possible to the estimated rij.

the probabilities p <dig> ...,pk are finally found be minimizing the kl-divergence between rij and sij. this pairwise coupling approach can also be adapted for multiple class labels by way of "fractional" class assignments in the training step. if we find many samples in the training set which belong to both the classes i and j, multi-label discriminant analysis will effectively shrink the respective class centroids μi and μj towards their common mean while keeping the covariances constant, a procedure which will favor the co-prediction of classes i and j. in our experiments, we use this pairwise approach to lda as a "building block" in the two-layered hmm for simultaneous prediction of subcellular localization and functional class. the advantage of this pairwise method over the "global" approach is that in each of the k/ <dig> subproblems a task-specific subset of kernels is learned. concerning the computational workload, the pairwise approach is very similar to the "global" method, since the increase of classifiers to be learned is compensated by the smaller sample size in the individual two-class problems.

RESULTS
on the top-level hierarchy, the functional catalog provided by the mips comprehensive yeast genome database   <cit>  assigns roughly  <dig> yeast proteins to several functional classes listed in table  <dig>  note that this classification scheme corresponds to an old version of the mips functional catalog, whereas the newest version, funcat  <dig> , further splits some of these  <dig> classes. to allow a better comparison with previous results reported in  <cit> , however, we still use the older labeling scheme.

kernel representation
one of the major advantages of our method is its capability of automatically extracting relevant data sources out of a large collection of kernels presented by the user. the user can, thus, collect as much information sources as possible and let the algorithm decide which to choose. following this idea, we represent the yeast proteins by previously used kernels that extract information on different levels, like mrna expression, protein sequences and protein-protein interactions. moreover, we enrich this set of "basis" kernels by variants thereof resulting from nonlinear feature space mappings. we further investigated additional kernels from several publicly available microarray datasets  <cit> .

the "basis" kernels introduced in  <cit>  consist of  two kernels which analyze the domain structure: kpfdom and an enriched variant kpf_exp;  three diffusion kernels on interaction graphs: kmpi, kmgi , ktap ;  two kernels derived from cell cycle gene expression measurements: kexp_d  and kexp_g ;  a string alignment kernel ksw. from each of these  <dig> kernels we derive  <dig> additional gaussian rbf variants by computing squared euclidean distances dij2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqhaawcbagaemyaakmaemoaaogabagaegomaidaaaaa@3194@ between pairs of objects  and deriving new kernels under this nonlinear feature space transform as kij=exp⁡
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwglbwsdaqhaawcbagaemyaakmaemoaaogabagaeiikagiaemibawmaeiykakcaaogaeyypa0jagiyzaumaeiieagnaeiicaanaeiikagiaeyoei0cccigae83wdm3aasbaasqaaiabdygasbqabagccqwgebardaqhaawcbagaemyaakmaemoaaogabagaegomaidaaogaeiykakcaaa@43f9@ for the three rbf variants l =  <dig>   <dig>   <dig> 

multilabels
since a protein can have several functions, each protein comes with a set y of functional class labels. let |y| denote the cardinality of this set, i.e. the number of classes a certain protein is assigned to. for running algorithm  <dig> in the methods section below we have to translate the label sets yi, i =  <dig> ...,n into membership probabilities which form the entries of the "blurred" n × k indicator matrix z˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgabgwgaacaaaa@2df8@, where n is the number of proteins and k the number of classes. since no further information is available, for the i-th protein we set z˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgabgwgaacaaaa@2df8@ik = 1/|yi|, if the k-th class is a member of the label set yi, and zero otherwise. in the following these membership probabilities will also be called "fractional labels".

performance evaluation
in  <cit> , all  <dig> classes were trained separately in a one-against-all manner, where a gene is treated as a member of a certain class whenever it has a positive label for that class . the performance of these classifiers has been evaluated in terms of area under the roc curves . our method, on the contrary, respects the multilabel structure of the problem by explicitly exploiting co-occurrences of class labels. it uses fractional labels z˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgabgwgaacaaaa@2df8@ik, and the output is a probability vector for all classes. thus, even in the optimal case, our classifier will assign a score of 1/|yi| to a correct class. since the test set contains genes with different cardinalities of label sets, the classifier scores reside on different scales and it will be impossible to find a common threshold when computing a roc curve.

to overcome this problem, we use two different measures: for each class ck, auc0/ <dig> measures the area under the roc curve only on the subset of genes which either do not belong to class ck, or which exclusively belong to ck. for this subset  we can directly compute a roc curve, since there are no scaling problems. the measure aucweighted, on the other hand, uses all test genes and rescales both the fractional label z˜
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgabgwgaacaaaa@2df8@ik for class ck and the corresponding probabilistic classifier score p for the i-th protein by the label set cardinality, z˜′ik=z˜ik⋅|yi|⇒z˜′ik∈{ <dig> }
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgabgwgaacgaqbamaabaaaleaacqwgpbqacqwgrbwaaeqaaogaeyypa0jafmowaolbagaadawgaawcbagaemyaakmaem4aasgabeaakiabgwsixlabcyha8jabdmfaznaabaaaleaacqwgpbqaaeqaaogaeiifawnaeyo0h4tafmowaolbaggbauaadawgaawcbagaemyaakmaem4aasgabeaakiabgigiolabcuha7jabicdawiabcycasiabigdaxiabc2ha9baa@4c31@.

the improvement obtained by using the enlarged kernel set becomes more obvious when computing the f1-measure which is the harmonic mean of precision and recall. the latter are similar to but different from the axes of roc curves which encode fallout and recall. precision is the probability that a predicted category is a true category, whereas fallout is the probability that a true absence of a category was labeled a false positive presence. since the definite absence of a certain protein function can be hardly validated experimentally, the estimated fallout rate will strongly depend on the actual status of experimental coverage. the precision measure, on the other hand, does not so severely suffer from this problem, since the number of present categories might be estimated more reliably even with a small number of carefully designed experiments.

to compute the f <dig> statistics for a given threshold τ in eq. , we select for each gene the k most probable multilabels such that the sum of the k largest membership probabilities exceeds τ. we then compute precision and recall up to the rank k and combine both to the f1-measure. variation of τ yields a complete precision-recall-f1-curve. with the enlarged kernel set the maximum f <dig> value increases significantly, as can be seen in the two leftmost boxplots in figure  <dig> 

functional classes and subcellular localization
for the yeast genome mips cygd provides a classification scheme with respect to subcellular localization of the proteins, see table  <dig>  we combine both the functional and the localization-based scheme by way of the hidden markov model  described in the methods section below. the corresponding automaton model is depicted in figure  <dig>  the first layer contains  <dig> emitting nodes corresponding to the top-level localization classes in table  <dig>  the transition probabilities are estimated from a training set by counting the occurrences of paths in the model. in order to highlight the essential graph structure, only the transitions with probability above  <dig>  are shown. note that several localization nodes have dominant transitions to only one or two functional nodes, see e.g. node "750"  which has a strong prior for class "04" , the pair "730"  and "03" , or "745"  which strongly votes for functional class "07" .

in order to evaluate the possible advantages of the combined localization-function classifier, we again conducted a cross-validation experiment in which both sets of labels were predicted. according to the results summarized in the left panel of figure  <dig>  the inclusion of the prediction step for the subcellular localization of a protein indeed improves the prediction of its function.

towards a "local" model: pairwise kernel weights
while all previous models find a common set of kernels for all classes, the pairwise coupling approach described in the methods section couples pairwise classifiers which find individual kernel weights that are optimal for the "local" problem of separating only two classes. these pairwise classifiers can again learn class correlations induced by objects with multiple labels.

the rightmost boxplot in the left panel of figure  <dig> shows that this "local" model significantly improves the predictive power of the hmm-based classifier. the right panel depicts the evolution of precision, recall and f <dig> under variation of the threshold τ in eq. .

CONCLUSIONS
while kernel-based classifiers have been successfully applied to a variety of prediction tasks, their main drawback is the lacking interpretability of the decision functions. one attempt to overcome this shortcoming is to find a weighted combination of multiple kernels, each of which represents a different type of measurement. the idea is that sparse kernel combinations allow the user to identify the relevant influence factors for a given task. in this work, the problem of learning such sparse kernel combinations has been addressed by reformulating classification as an indicator regression problem using adaptive ridge penalties. while the standard adaptive ridge model presented in  <cit>  selects individual input features, our extensions concerning weight sharing and kernelization lead to a nonlinear model that finds sparse combinations of kernel matrices. a probabilistic treatment of multiple labels allows us to apply the classifier to tasks in which the input objects can belong to more than one category. the effect of multilabels can be intuitively understood as shrinking the centroids of classes which share many multilabeled objects towards their average centroid, thus favoring co-prediction of these classes.

the method has been applied to the problem of predicting the function of yeast proteins which defines a classical multilabel setting. from the experiments we conclude that our model compares favorably to the approach in  <cit> . two aspects seem to be of particular importance: on the modeling side, our approach directly exploits the multilabel structure of the problem, rather than ignoring class correlations.

concerning the computational aspects, the efficiency of the method allows us to easily enlarge the set of kernels: as long as one single matrix can be hold in the main memory, the algorithm is highly efficient. for yeast proteins, the use of additional kernels has e.g. lead to the insight that genetic interactions are highly discriminative for functional predictions.

aiming at a still higher classification rate and at more detailed information about the relevance of kernels, we have introduced two further modifications: extending the multilabel classifier to a two-layer hidden markov model  allows us to combine two different labeling schemes. multilabel prediction in the hmm naturally translates to reconstructing multiple paths through a graph. it could be shown that the prediction of the subcellular localization of a protein in the first layer helps to identify its functional class in the second layer, the reason for this improvement being the strong correlation of nodes in both layers. localization in the transport vesicles, for instance, gives a strong prior for having a role in the functional class cellular transport & transport mechanism.

the second modification concerns the transition from a single "global" prediction model to several "local" models which focus on the separation of pairs of classes only. the estimated pairwise membership probabilities are coupled to a consistent set of assignment probabilities over all classes. using the pairwise classifiers as "building blocks" in the hmm offers the advantage of increased adaptiveness, since the kernel weighting can focus on the individual requirements for separating one class from another. this approach leads not only to a significantly increased classification performance, but it also gives a much more detailed picture on the importance of different data sources for predicting protein function.

authors' contributions
both authors developed methods, performed comparisons of methods, and wrote the manuscript.

