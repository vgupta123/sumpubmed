BACKGROUND
many proteins contain regions which do not adopt a stable tertiary structure in their native state. these regions have been identified by various terminologies in the literature and names include disorder regions  <cit> , intrinsic disorder  <cit> , intrinsically disordered regions   <cit>  and intrinsically unstructured proteins   <cit> . this disorder or lack of structure may be limited to a particular region or regions of a protein chain or may extend throughout the entire protein. disorder can also be transitory in nature and linked to a certain state of a protein such as bound or unbound .

protein disordered regions are of particular interest due to their involvement in signalling pathways, transcription and translation  <cit> . their inherit flexibility make it possible for a protein to bind to many partners and make them attractive targets for drug development. several methodologies have been proposed for disorder-based rational drug design  and some peptides have already been designed which block interactions between structured and unstructured partners  <cit> . as a result, methods are needed to accurately predict protein disorder and aid in the search for new drug targets.

recent estimates indicate that there are over  <dig> protein disorder predictors  <cit> . a number of comprehensive reviews on disorder predictors exist, outlining methodology and availability  <cit> . generally speaking, existing methods for the prediction of protein disorder can be coarsely categorized as propensity-based, machine learning based, contact-based or a meta-method  <cit> . propensity-based predictors work on the premise that certain types of amino acid residues are more likely to be found in the core of an ordered region than a disordered region. likewise, there are particular residues which appear to be over represented in disordered regions. a statistical analysis of known ordered and disordered proteins allows for the creation of disorder propensities which can be used to predict disorder  <cit> . this approach is fast and simple but does not make use of the data in an optimized way. predictors based on machine learning, such as neural networks  <cit>  or support vector machines  <cit> , also make use of experimental data on ordered and disordered residues but do so via sophisticated learning algorithms which allow for more than sequence data as input. high dimensional functions are fit to the input features through training and then used to predict residue disorder. this does allow for optimized use of the experimental data but results in a prediction approach that is based on a complex function. it is often difficult to understand how the function depends on its input and this approach lacks an intuitive rationale as to how the prediction is made. methods based on residue-residue contacts attempt to determine if sufficient interactions take place to pull the protein chain into a stable conformation. residue-residue contact data may come in the form of predicted packing density or predicted residue-residue contacts  <cit> . meta predictors, or meta methods, are combinations of the aforementioned methods and are constructed by combining several predictors. this can be done by a simple averaging of the output from each method or in a performance weighted manner. this usually results in a slight improvement in performance  <cit>  but the approach may not be practical on a genomic scale if it depends on too many disorder predictors.

here we present a new sequenced-based predictor of protein disorder using boosted ensembles of deep networks . to the best of our knowledge this is the first use of deep networks for disorder prediction. by using cuda and graphical processing units we were able to create very large, deep networks to predict disordered regions. we also combined this novel approach with another sequence based disorder predictor to create a small, meta predictor. the meta predictor provides a boost in performance with a negligible increase in prediction time. to evaluate our methods, we compared them to a number of other disorder predictors on a common benchmark dataset as well as in the recent round of the critical assessment of techniques for protein structure prediction  experiment. the results of this evaluation show that our novel approach compares competitively with many state-of the-art disorder predictors. this indicates that boosted ensembles of deep networks can be used to predict protein disorder regions.

methods
datasets
the principle dataset used for training was disorder <dig>  a set of  <dig> proteins originally constructed for the development of dispro  <cit>  and later predisorder  <cit> . it consists of proteins which are more than  <dig> residues in length and contain at least one disordered region  <dig> residues or longer in length. it is comprised of  <dig> disordered residues and  <dig> ordered residues . additional datasets used for evaluation include casp <dig> and casp <dig>  respectively comprised of  <dig> and  <dig> proteins and used during the casp <dig> and casp <dig> competitions. the casp <dig> dataset consists of  <dig> ordered residues and  <dig> disordered residues  and the casp <dig> dataset contains  <dig> ordered residues and  <dig> disordered residues . all the accession dates for the proteins in disorder <dig> predate march  <dig> and well before the casp <dig> and casp <dig> competitions which took place during the years of  <dig> and  <dig>  the distribution of the lengths of the proteins comprising these datasets is shown in figure  <dig>  figures  <dig> and  <dig> represent the distribution of the lengths of the disorder regions in the datasets. for the casp <dig> and casp <dig> datasets, the protein sequences and experimentally determined order/disorder state were obtained from the official casp website  <cit> . residues that did not receive a disorder/order classification by the casp assessors  where not considered to be disordered in our assessment. the dataset disorder <dig> is available also online and available for download  <cit> .

restricted boltzmann machine and deep networks
conceptually deep networks  are similar to neural networks but contain more layers and trained in a slightly different manner. one way to train dns is using a layer by layer unsupervised approach. here, the idea is to first learn a good model or representation of the data irrespective of the label of each data point. this process allows one to first learn relationships that might exist in data. after theses relationships are learned, a supervised learning technique such as a  <dig> layer neural network can be trained on the learned, higher level representation of the data. intuitively the general idea behind such an approach is that to do effective classification it is useful to first learn the structure  of the data. relatively recent developments in training algorithms for dns has lead to their successful use in a number of areas such as image recognition  <cit> , speech recognition  <cit> , text classification and retrieval  <cit>  and residue-residue contact prediction  <cit> . there are a number of introductions and overviews to deep learning and deep networks in the literature including two foundational works by hinton et al.  <cit>  and an overview of training deep networks  <cit> .

the general framework used for our disorder predictor was a collection of boosted ensembles comprised of deep networks. each dn is a deep, multilayer neural network that is trained layer by layer using restricted boltzmann machines and then fine tuned using a back propagation procedure. a restricted boltzmann machine  is a two layer network with one layer termed the visible layer which takes on the values to be modeled and the other is the hidden, or latent, layer  <cit> . in its purest form, the nodes in a rbm are stochastic and binary. symmetric, weighted connections exist from every node in the visible layer to every node in the hidden layer. there are no connections within a layer and every node has a bias. in this context, the energy of a particular configuration can be defined as

 ev,h=-∑ibivi-∑jcjhj∑i,jhjviwij 

where hj and vi are the states of the jth hidden and ith visible nodes, cj and bi are biases for the jth hidden node and ith visible node, respectively. wij is the weight of the symmetric connection between the ith and jth nodes. by summing over all possible configurations of h and normalizing , it is possible to define a probability for a particular configuration of visible nodes, v.

 pv=∑he-ev,hz 

training a rbm entails adjustments in the weights and biases such that the probability assigned to training data is higher than randomly chosen configurations of the visible nodes. this is typically done using a process known as contrastive divergence  <cit> . in this work, the weights in the nth round of training were updated using the following rules:

 Δnwij=ε<vipj>data−<pi1pj1>recon−ηwijΔnai=ε<vi>data−<pi1>recon+vain−1Δnbj=ε<pj0>data−<pj1>recon+vbjn− <dig> 

more specifically, in these update rules the angle brackets represent averages which are taken over the batch. pj <dig> is the probability that the jth hidden unit will be activated and can be calculated by applying the sigmoid function to the bias for the jth hidden unit plus the sum of the products of each visible unit times the weight of the connection between the visible unit and the jth hidden unit.

 pj0=σ∑iviwij+bj 

where the σ() represents the sigmoid function. pi <dig> is the probability that the ith visible unit will be activated and calculated in a similar fashion to p. in this case, the biases for the visible units are used as well as the states of the hidden units. the state of the jth hidden unit is represented by hj and set to  <dig> with probability pj <dig> 

 pi1=σ∑jhjwij+ai 

pi <dig> is the probability that the jth hidden unit will be activated when driven by the probabilities of the reconstructed visible nodes ). it is calculated in the same manner as pj <dig> but with pi <dig> used in place of vi. the update rules also contain three additional parameters which can be tuned for the particular application. these are the learning rate , the weight cost  and momentum . the values for these parameters and the update rules were selected based on recent findings describing how to train rbms in practice  <cit> . in this work, the learning rate ɛ was set to  <dig>  for w and  <dig>  for the biases and the weight cost η was set to  <dig> . the momentum υ was initially set to  <dig>  and after  <dig> epochs of training increased to  <dig> . training for a rbm took place over  <dig> epochs using batches of  <dig> training examples. we did not attempt to optimize these parameters and the evaluation data was not consulted during training.

a principle use of rbms is as a means to initialize the weights in a dn. this is done by learning the weights at each level in a step-wise fashion. the first layer is trained using the training data and the aforementioned training procedure for a rbm. after the weights have been learned, the probabilities for activating the hidden nodes are calculated for every example in the training data. these activation probabilities are then used as the input to train another rbm. this procedure can be repeated several times to create several layers. the last layer is a single layer neural network trained using the target values and the last set of activation probabilities. finally, all of the nodes can be treated as returning real-valued, deterministic probabilities and the entire deep network can be fine tuned using the back propagation algorithm  <cit> .

to work with large models and datasets we implemented the training and prediction processes for the method using matrix operations. this allowed us to use cudamat  <cit> , a python library which provides fast matrix calculations on cuda-enabled gpus. with this implementation we were able to train very large dns  in a timely manner .

predicting disordered residues
to predict disordered residues, we trained a number of boosted ensembles of dns. the input for each dn came primarily from a fixed length window centered on the residues to be classified. for each residue in the window, structure based and sequence based values as well as statistical characterizations were used as features . the targets were the order/disorder states of the individual residues in a small window of  <dig>   <dig> or  <dig> residues in size. for the input window size, we used lengths of  <dig>   <dig> and  <dig>  in total, there were  <dig> input-target window combinations. these were  <dig> to  <dig>   <dig> to  <dig>   <dig> to  <dig>   <dig> to  <dig> and  <dig> to  <dig>  depending on the size of the input window there were between  <dig> to  <dig> input features which resulted in the dn having an architecture of -750-750-350-. each layer in the network was initialized using a rbm via the previously described process. the entire network was fine tuned using the back propagation algorithm to minimize the cross-entropy error. this was done over  <dig> epochs using batches of  <dig> training examples.

in order to create boosted ensembles, we trained a series of dns using a sample of  <dig>  training examples which came from the entire pool of training data. the training examples came from the dataset disorder <dig> and consisted of all target windows and their corresponding input window. initially, all of the training examples had an equal chance of being included in the training sample. after each round, the training pool was evaluated using the newly trained dn and the pool was reweighted based on the performance of the classifier. the probability of training examples which were at least partially misclassified was increased while the probability of selecting a properly classified example was decreased. this was done using a modified version of adaboost  <cit> . in particular, let xi represent the ith example in the training pool and yi∈ { <dig>  1} be the classes of the ith example . furthermore, let wt be the probability of selecting the ith example from the training pool in the tth round of boosting and call the dn classifier trained in round t to be mt which outputs a value between  <dig> and  <dig>  note that since the target has multiple values , the probability of selecting the training example was increased in a manner proportional to the number of misclassified residues in the target window. let β represent the number of target residues misclassified. now, after each round of boosting, wt is updated via ɛt, αt and ht in the following manner.

 hti={0ifmt< <dig> ifmt≥ <dig> ∈t=∑htxi≠yiwtiαt=121−∈t∈twt+1i=wtizt*{e−αtifhtxi=yieβ*αtifhtxi≠yi 

after the  <dig> rounds of boosting, the final output of the ensemble ) is a performance weighted average of all of the dns. it is a value between  <dig> and  <dig> and for any input xi calculated as follows:

 hxi=Σmtxi> <dig> αtΣtαt 

a caveat of our boosting procedure is that after  <dig> rounds of boosting, all of the probabilities for the examples in the training pool were reinitialized to a uniform distribution. this was done as we saw that the weights of a few challenging training examples became too large and effectively dominated the selection process. this type of phenomena has been seen elsewhere and can lead to over fitting or poor performance  <cit> . indeed, dns trained of these types of training samples did not generalize well and effectively limited boosting to a small number of rounds . thus, by reinitializing the weights after  <dig> rounds we were able to create larger ensembles.

dndisorder
the final step in the construction of our dn based disorder prediction was to combine the results from the various boosted ensembles into one prediction. each boosted ensemble consists of  <dig> predictors and there are  <dig> input-target window combinations . thus, in all there are  <dig> predictors. the per residue prediction for each boosted ensemble is made using the aforementioned approach . the final prediction is a simple average of the values produced by each boosted ensemble. this final value is the output of our method which we call the dndisorder predictor.

sequence based meta approach
in addition to dndisorder, our dn based disorder predictor, we developed a small, sequence based meta predictor. this approach which we call predndisorder is a simple average of the outputs from dndisorder and predisorder. predisorder is another fast sequence based predictor of disorder regions we developed and build upon 1d recursive neural networks  <cit> .

features used and generation
a number of sequence based features were used as input into our disorder predictor. these included values from a position specific scoring matrix , predicted solvent accessibility and secondary structure, and a few statistical characterizations. the predicted values for both solvent accessibility and secondary structure were obtained using accpro and sspro from the scratch cluster of tools  <cit> . the pssm was calculated using psi-blast  <cit>  for  <dig> iterations against a non-redundant version of the nr database filtered at 90% sequence similarity. for statistical characterizations of the amino acid residues we used the acthley factors which are five numeral values which characterize an amino acid by secondary structure, polarity, volume, codon diversity and electrostatic charge  <cit> . finally, note that all feature values were scaled to be in the interval from  <dig> to  <dig> in order to be compatible with the input layer of a rbm.

as previously mentioned, the input to a dn is a fix length window centered on the target window . for each residue in the input window we used two binary inputs for solvent accessibility , three binary inputs to encode for the secondary structure , five inputs for the acthley factors and from the pssm we obtained  <dig> value for the information score of the residue and  <dig> inputs for the likelihoods of each amino acid type at the position. note that as a window slides across the protein sequence, part of it may extend beyond the ends of the sequence. thus, there is the need for an additional binary feature which encodes whether or not the position in the window is contained in the sequence boundaries and actually corresponds to a residue. if a window position does not correspond to an actual residue then all of the residue specific features for that position are set to  <dig>  in addition to the residue specific inputs, we also used four, real value global features which were the percent of total residues predicted to be exposed, the percent of total residues predicted to be alpha helix, the percent of total residues predicted to be in a beta sheet and the relative position of the target residues . since three different sizes of input windows were used  the total number of input features ranged from  <dig> to  <dig> to  <dig> 

evaluation metrics
the output of dndisorder is a real valued number from  <dig> to  <dig> with  <dig> corresponding to an ordered residue  and  <dig> a disordered residue . given a set decision threshold residues can be classified as ordered if the output of dndisorder is less than the decision threshold or as disordered if the output is greater than the threshold. after predictions are made it is possible to determine the number of true positives , false positives , true negatives  and false negatives . true positives are residues experimentally determined to be disordered which are predicted as disordered and true negatives are residues experimentally determined to be ordered and correctly predicted as ordered. false positives and false negatives are predictions which do not correspond to the experimentally determined state. here, positive refers to disorder and so a false positive would be a residue incorrectly predicted to be disordered and a false negative would be a residue incorrectly predicted to be ordered.

the principle means used to evaluate the performance of our predictor are the area under the roc curve  and the balanced accuracy . the roc curve is a plot of sensitivity ) against the false positive rate ) across a variety of thresholds  <cit> . by calculating the area under the roc curve it is possible to measure the general performance of a classifier irrespective of the decision threshold. the balanced accuracy is the simple average of the sensitivity and specificity ) using a decision threshold of  <dig> . this evaluation metric is preferred over the accuracy given the disproportionate number of ordered residues compared to disordered residues in most datasets. in this setting, a naive classifier which classified all residues as ordered would have a very high accuracy but be useless for the task at hand. the same naive classifier would have a balanced accuracy of around 50%. in addition to the sensitivity, specificity, auc and acc, we also calculated a score  and the f-measure. all of these measures have been used extensively in the evaluation of other disorder predictors and in recent casp assessments  <cit> . the significance of balanced accuracy, sensitivity, specificity, f-measure and sw was obtained by approximating the standard error  for each value. it was accomplished by a bootstrapping procedure in which 80% of the predicted residues where sampled  <dig> times. more specifically, for a particular performance measure Θ, se = √2/1000) where Θi is the value of the measure calculated on the ith sample.

methods used for comparison
in this study we compared our methods dndisorder and predndisorder against several predictors. included in this comparison were several disorder predictors which are available publicly as servers or downloadable executables and several which participated in the casp <dig> and casp <dig> experiments. when selecting predictors from the casp experiments, we included only those methods which performed particularly well in terms of acc or auc as determined by the official casp <dig> assessment  <cit>  or our in-house evaluation pipeline when applied to the casp <dig> targets. publicly available predictors used in our assessment included iupred  <cit> , espritz  <cit> , predisorder  <cit>  and cspritz  <cit> . to generate disorder predictions, cspritz was used as a web service while iupred, espritz and predisorder were downloaded and run locally. for casp participants, we downloaded disorder predictions from the official casp website  <cit> . note that when calculating the performance measures, the decision threshold was set to  <dig>  for all methods  with the exception of espritz  and cspritz. in these two cases, we used decision thresholds of  <dig>  and  <dig>  respectively based on the accompanying documentation or output of these tools. one final caveat is that for the downloadable version of espritz , we only report the results on predictions made by running espritz when trained x-ray structures and without profile information.

RESULTS
with nearly  <dig> disorder prediction methods and not all of them freely available, thoroughly benchmarking a new approach is a challenge. the situation is further exacerbated by different evaluation sets and metrics. as a basis for our analysis and comparison among disorder predictors, we used the critical assessment of techniques for protein structure prediction  experiment. this is a bi-annual, international experiment of various protein structure prediction methods including disordered regions. over a period of approximately three months, protein sequences were released to the community and disorder predictions sent back to the prediction center. in casp <dig>  both dndisorder  and predndisorder  submitted disorder predictions to the prediction center along with approximately  <dig> other methods. in addition to the casp <dig>  we also benchmarked our novel approach against several disorder predictors on the casp <dig> dataset. the comparison was made using evaluation metrics consistent with the literature and official casp assessments  <cit> .

we will also mention that we examined the pair wise sequence similarity between our training dataset disorder <dig> and the casp <dig> and casp <dig> datasets using needle  <cit> . we found that  <dig> of the casp <dig> and  <dig> of the casp <dig> protein targets had sequence similarities between 40-60% with a protein in the training set. the remaining casp targets had sequence similarities less than 40% to proteins in the training set. to determine the impact of these relatively similar sequences, we evaluated dndisorder on subsets of the casp <dig> and casp <dig> datasets with sequence similarity to the training data of less than or equal to 40%. there was no significant difference in terms of the acc or auc on the subsets compared to an evaluation over the full casp datasets . as the inclusion of the these  <dig> targets did not affect or enhance the performance of our methods, we used the performance of dndisorder and predndisorder on the full casp <dig> and casp <dig> datasets in our benchmark.

tables  <dig> and  <dig> report the results of our methods on the casp <dig> and casp <dig> datasets. both dndisorder and predndisorder compete competitively against state-of-the-art disorder predictors, particularly in terms of acc. with respect to auc on the casp <dig> dataset, there are few methods such as prdos-cnf and biomine_dr which set themselves apart from the others while most of the predictors including predndisorder and dndisorder fall in the range  <dig> - <dig> . when ranking by acc on the casp <dig> dataset, both of our methods performed favourably with values in the range of  <dig> - <dig> , slightly behind the top performing method with an acc value of  <dig> .

results of a benchmark of dndisorder, predndisorder and a number of other disorder predictors. the evaluation was performed on  <dig> casp <dig> targets consisting of  <dig> ordered residues and  <dig> disordered residues. the standard error  is shown for each performance measure. not included in this is assessment are  <dig> residues which were marked as ‘x’ by the casp assessors. this table also contains four of the top performing methods from casp <dig> according to the official casp <dig> assessment. the predictions for these methods were downloaded from the official casp website and the group number for these methods is provided in parenthesis. all values except auc have been scaled by a factor of  <dig> 

results of a benchmark of dndisorder, predisorder, predndisorder and a number of other disorder predictors. the evaluation was performed on  <dig> casp <dig> targets consisting of  <dig> ordered residues and  <dig> disordered residues. the standard error  is shown for each performance measure. not included in this assessment are  <dig> residues which were marked as ‘x’ by the casp assessors. this table also includes a number of methods which participated in the casp <dig> experiment. the predictions for these methods were downloaded from the official casp website and the group number for these methods is provided in parenthesis. all values except auc have been scaled by a factor of  <dig> 

on both the casp <dig> and casp <dig> evaluation sets, dndisorder performed competitively against predisorder, a state-of-the-art disorder predictor as assessed in both casp <dig> and casp <dig>  <cit> . on the casp <dig> evaluation set, our meta method predndisorder slightly outperformed predisorder and our novel method dndisorder and showed a modest improvement in auc. this indicates that both predisorder and our novel approach are complementary in some respects as the combination of their respective predictions leads to a performance boost. to further investigate this point, we calculated the pearson correlation coefficient between the scores assigned to disorder predictions by both predisorder and dndisorder on the casp <dig> dataset and was found to be  <dig> . the additional time and complexity in running both methods and combining the results is negligible. we also generated roc curves for dndisorder, predisorder and predndisorder for the casp <dig> and casp <dig> datasets and these are illustrated in figures  <dig> and  <dig> 

in addition to the evaluation on the casp <dig> and casp <dig> datasets we performed a  <dig> fold cross validation test on the disorder <dig> dataset. this is to say we divided the dataset up into  <dig> folds, all containing roughly the same number of proteins. then  <dig> of the folds were used for training ensembles of boosted dn disorder predictors and then used to predict disorder of the proteins in the remaining fold. the average acc of our approach dndisorder was  <dig>  and the auc  <dig> . table  <dig> shows the results of all the performance measures for this  <dig> fold cross validation test.

results of a  <dig> fold cross validation assessment of dndisorder on the disorder <dig> dataset. the standard error  is shown for each performance measure.

benefits of boosting
to determine the effect of boosting we evaluated the performance of the method as a function of the number of rounds of boosting. figure  <dig> shows the acc and auc for an ensemble of dn predictors with an input window of  <dig> residues and a target window  <dig> residues in length. there is a clear improvement in performance with the auc starting near . <dig> and quickly rising to around . <dig> and finally reaching near . <dig> after  <dig> rounds. the average balanced accuracy also showed a steady improvement reaching . <dig> after  <dig> rounds of boosting.

limitations
dndisorder, as well as predisorder and predndisorder, make use of information derived from psi-blast. using such information has been shown to result in a modest boost in performance but incurs a significant computational cost  <cit> . the web service we have developed for dndisorder can process a protein of  <dig> residues in  <dig> to 20 minutes depending on server load. consequently, our methods are not presently applicable to studies on a genomic scale. in the future we plan to develop predictors which do not depend on sequence profiles , similar to the non psi-blast implementations of espritz which have been shown to be several orders of magnitude faster with only a marginal decrease in performance  <cit> .

CONCLUSIONS
in conclusion we have implemented a new framework for the prediction of protein disordered regions from sequence based on boosted ensembles of deep networks. in an evaluation with other state-of-the-art disorder predictor, our method dndisorder performed competitively, indicating that this approach is capable of state-of-the-art performance. dndisorder is available as a webservice at http://iris.rnet.missouri.edu/dndisorder/.

competing interests
the authors declare that they have no competing interest.

authors’ contributions
je implemented the algorithms and carried out the experiments. je and jc analyzed the data, wrote and edited the manuscript and approved it. both authors read and approved the final manuscript.

