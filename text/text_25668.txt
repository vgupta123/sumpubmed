BACKGROUND
animal coloration often plays a major role in survival and reproduction. consequently, various forms of coloration—from camouflage to mimicry—have long been used as key examples of evolution by natural selection, and have provided an important test-bed for evolutionary thinking . more recently, studies of colour patterns have made substantial progress in understanding the types of defensive coloration that exist, and the mechanisms that make them effective . progress has perhaps been most marked in the study of concealment, with camouflage one of the most common anti-predator strategies in nature  <cit> . however, predicting an animal’s detectability from its visual appearance remains challenging. this is an important problem because quantifying how well an animal avoids detection against a particular background is key to investigating a wide range of evolutionary and ecological hypotheses surrounding animal signalling and survival. moreover, it will also allow us to design effective camouflage patterns for human use: to disguise items as diverse as telephone masts, cameras, buildings and military equipment and personnel. indeed, rigorously quantifying colour patterns in general has been a topic of considerable recent interest due to both the potential human applications and the unique opportunity to revolutionise our understanding of animal signalling . regarding camouflage, identifying and assessing the effectiveness of a wild animal’s camouflage strategy is essential for understanding predator–prey interactions in any system with visually guided predators. many conspicuous displays will also be influenced by evolutionary pressure for greater camouflage, for example, displays that are aposematic at close quarters can be cryptic from further away  <cit> , and conversely, an understanding of how camouflage is achieved can illuminate the mechanisms that make conspicuous coloration effective  <cit> . quantifying an animal’s appearance relative to its background is therefore essential for investigating sexual, aposematic and camouflaged displays in a diverse range of fields.

a number of different types of camouflage have been identified, based on how they hinder detection. the most ubiquitous camouflage strategy is probably background matching, where animals match the general colour, brightness and patterns of their backgrounds  <cit> . however, one of the key features thought to facilitate detection and/or recognition is the overall outline of an object. here, high-contrast markings intersecting an animal’s outline may be used to ‘disrupt’ the viewer’s ability to discern or detect it  <cit> , a strategy called disruptive coloration. the most direct way to determine an animal’s camouflage, and how effective it is, uses often lengthy behavioural tests or survival experiments that are difficult to undertake in the wild  <cit> . considerable effort has therefore pursued computer models that can quantify how well any two visual samples match according to different processing or camouflage theories . however, these camouflage models have rarely, if ever, been directly compared under controlled conditions, nor using data on observer success in finding hidden objects. this lack of model validation means that researchers rarely know which methods they should adopt when investigating an animal’s appearance. furthermore, models that quantify visual signals and their match  with the background have the potential to greatly inform us regarding the mechanisms through which colour patterns work, and how they should be optimised for maximal success . if models that are based on specific or generalised features of visual processing fit with behavioural data, this can illuminate the possible mechanisms through which colour patterns are made effective  <cit> , and even how changes to them might improve the adaptive value of the defence. where the models are inspired by known or likely neural architecture this can even reveal likely avenues for research into the underlying structures performing the visual processing.

here we set out to address the above issues by pitting numerous contemporary models of camouflage directly against one another, using human observers to compare models to detection times. we tested seven classes of model that have been previously used for investigating background matching and disruptive coloration hypotheses. these models compare prey to their backgrounds according to three different criteria: i) luminance matching, ii) pattern matching and iii) disruptive coloration. luminance  matching is the simplest form of camouflage to measure, for example calculating the average difference in luminance between prey and their backgrounds, which is thought to be important in explaining survival in peppered moths against mismatching backgrounds  <cit> . contrast differences measure how similar the variation in luminance in the prey is to that of the prey’s background. this has been found to be important in the survival of wild ground-nesting bird clutches  <cit> . pattern matching is another important aspect of background-matching camouflage that has recently been found to predict the survival of nightjar nests  <cit> . pattern matching has been measured using a number of different methods that vary in their biological plausibility and complexity. these methods generally separate images into a series of spatial scales , then compare the information at these different scales between the prey and background. other methods search for shapes or features found in the prey that are similar to those in their backgrounds  <cit> . for an overview see table  <dig> and additional file  <dig>  the final type of camouflage measured was disruptive coloration, where contrasting markings break up an object’s outline and create false edges  <cit> . this camouflage strategy has received considerable investigation in the last decade, and has been shown to be highly successful in numerous contexts, including where stimuli have various contrast and pattern features manipulated . in spite of the clear protective benefits of artificial disruptive edges, it has proven far more difficult to measure how disruptive real prey are against their backgrounds  <cit> . recent edge disruption measures have quantified how many edges are present in the animal’s outline  <cit> , or have used the canny edge detector to measure the number of perceived edges in the prey’s outline relative to its surroundings  <cit> , or the number of outline edges relative to those in the object’s centre  <cit> . however, these measures do not take into account the direction of the perceived edges, so cannot distinguish ‘false edges’  from ‘coherent edges’ that match the angle of the animal’s outline, making the prey’s tell-tale shape easier to detect  <cit> . we therefore developed a novel edge disruption metric called ‘gabrat’ that uses biologically inspired and angle-sensitive gabor filters to measure the ratio of false edges to coherent edges around a target’s outline  . a high ratio of false edges to coherent edges should be more disruptive, making prey more difficult to detect. background complexity is also known to influence detection times  <cit> . for example, artificial moths were more difficult to find for humans and birds alike when the surrounding bark had higher levels of luminance contrast and edge orientation changes  <cit> . however, in this study we aimed to focus on metrics that investigate the interaction between target and background, rather than assess the general properties of backgrounds that affect concealment.table  <dig> descriptions of the methods used to measure prey conspicuousness


fig.  <dig> examples of prey and edge disruption measurements.  sample prey highlighted in blue against its background image. the ‘local’ region within a radius of one body-length is highlighted in red.  examples of prey generated with the disruptive algorithm  and background-matching algorithm . these prey were chosen as their gabrat values were near the upper and lower end of the distribution .  illustration of the gabrat measurement. red and yellow false colours indicate the perceived edges run orthogonal to the prey’s outline , blue false colours indicate the perceived edges are parallel to the prey’s outline . gabrat values are only measured on outline-pixels, so these values have been smoothed with a gaussian filter  to illustrate the approximate field of influence. the prey on the left has a high gabrat value of  <dig> , while the prey on the right has a low gabrat value .  canny edges are highlighted in the images. edges inside the prey are highlighted in blue, edges in the prey’s outline region are green, and edges outside the prey are red. the visrat and disrat disruption metrics are formed from the ratios of these edges.  gabor filter kernels , shown in false colour at the different angles measured




we tested how the above camouflage models predicted the performance of human ‘predators’ searching for camouflaged stimuli against natural background images on a touch screen monitor. each prey was unique, generated from its background using methods that maximised or minimised the prey’s level of edge disruption, with prey also varying in their level of background matching. we used tree-bark backgrounds as these are biologically relevant backgrounds for a wide range of camouflaged animals, and they exhibit a degree of background heterogeneity in contrast and spatial features. artificial prey and tree-bark backgrounds such as these have been used extensively for testing camouflage theories because they capture the essence of camouflage patterns effectively without the need to find and take calibrated images of large numbers of camouflaged prey  <cit> . these studies have also demonstrated that human and non-human visual systems respond similarly to these camouflage stimuli. we calculated the preys’ camouflage with a battery of different models to determine which best predicted capture times. each prey’s level of camouflage was measured between their entire background image, or with the local region within one body length to investigate whether camouflage is best predicted by local or global background matching. in addition we tested for interactions between the most effective luminance-, pattern- and disruption-based camouflage metrics to determine whether extreme luminance differences render pattern and disruption strategies ineffective. finally, we discuss the effectiveness of the most commonly used models for assessing an animal’s camouflage and what our findings reveal about the mechanisms underlying animal camouflage.

RESULTS
there were substantial differences between the abilities of different camouflage metrics to predict capture times, see fig.  <dig> for full results. camouflage experiments such as this are expected to entail a very high levels of residual variation in capture times due to the interaction between the location of the prey, the viewers’ eye movements  <cit> , and the heterogeneous nature of the backgrounds. for example, prey that appeared in the centre of the screen were captured sooner than those nearer the edges, explaining  <dig> % of model deviance in the best model, while the random effects of participant id and background image id explained  <dig>  and  <dig> % of variance respectively. the best predictor of capture times was gabratσ <dig>  which measured edge disruption and explained  <dig> % of model deviance . as an illustration of effect size, prey in the upper 10% of gabratσ <dig> values took on average  <dig>  times longer to catch than those in the lower 10% of gabratσ <dig> values . this was followed by the local sift model , and together with other gabrat sigma values , these were the only metrics that performed better than the prey treatment . the worst metric at predicting capture times was the canny edge disruption measurement disrat, explaining less than  <dig> % of model deviance , although this was likely due to its non-linear nature, see below.fig.  <dig> capture time prediction accuracy. the predictive performance of camouflage metrics tested in this study ranked from best to worst. all camouflage metrics were continuous variables using one degree of freedom in each model with the exception of treatment type, which was categorical, consuming two degrees of freedom. note that disrat and visrat performed better when fitted with a polynomial




the full model comparing the best edge disruption, pattern and luminance predictors contained gabratσ <dig>  local sift difference and mean local luminance difference metrics. following aic simplification the model retained an interaction between gabratσ <dig> and sift local that explained  <dig> % deviance, with the main effect of gabratσ <dig> explaining the majority of deviance  and sift local with  all terms were significant . the global comparisons model based on bandpass descriptive statistics performed comparatively well, explaining  <dig> % of deviance when summed across all model terms. this model contained four two-way interactions that retained all five descriptive variables . the local comparisons model using bandpass descriptive statistics performed less well, retaining just dominant spatial frequency difference as a predictor that explained  <dig> % of deviance). while background complexity measured independently of the prey was not the focus of this study, a number of metrics effectively include this information, such as the mean edge-region canny edges , and mean local bandpass energy .

gabor-based pattern-matching metrics did not vary consistently between local and global difference predictors. the bandpass-based pattern matching metrics performed better when comparing the prey to their global region than their local region with the exception of the euclidean pattern distance, which performed better locally. in contrast, luminance metrics all performed better when considering the local rather than global regions. however this is perhaps to be expected given the way the background images were normalised, and the way prey were generated from their backgrounds. nevertheless, the global patterndiff metric performed substantially better than the global mean luminance difference, which as predicted is non-significant .

given the striking difference in performance of disrat and gabrat metrics we tested how well each of them predicted prey treatment. as predicted, disruptive prey had a significantly lower disrat and higher gabratσ <dig> than background-matching prey , demonstrating that both were able to predict treatment type. when fitted with a quadratic, visrat local and disrat both fitted capture times significantly better , increasing the deviance explained by these variables to  <dig>  and  <dig> % respectively. the optimal visrat local ratio was equal to  <dig> , while the optimum disrat was  <dig> , values higher or lower resulted in shorter detection times.

discussion
the number of studies quantifying the appearance of animals to test evolutionary and ecological hypotheses is increasing rapidly with the advancement of imaging methods, computer models and models of animal vision . however, the methods developed to determine how conspicuous an animal is against its background have rarely been validated using behavioural data, let alone compared to alternative models. this is an issue that goes beyond simply developing the best techniques to quantify animal appearances; coupling visual models to performance, and determining which metrics are most effective regarding observer behaviour, can also enable predictions about the optimisation of visual signals in nature and in applied tasks. by comparing the performance of a suite of different analysis techniques we have determined the best methods for quantifying detectability from appearance.

we found that there were striking differences between the abilities of different camouflage metrics to predict the capture times of our computer-generated prey. our study broadly validates the use of the majority of camouflage models used in the literature to date, however there were important differences and exceptions, demonstrating the importance of behavioural validation of these models. the gabor edge disruption ratio  devised for this study performed substantially better than all other metrics; prey with high gabrat values were half as likely to be captured by our human ‘predators’ in a given time-frame than those with low values, demonstrating the potential for powerful evolutionary selection pressures on this metric. moreover, gabrat was over twice as effective at predicting capture times as the type of algorithm used to generate the prey, and over ten times better than fourier bandpass, hmax or luminance difference metrics. also striking was the relative failure of canny edge detection-based models  to predict capture times when treated as linear predictors . when visrat and disrat were fitted non-linearly, the optimal ratios were slightly below one in both cases, where ratios equal to one would fit with a background-matching strategy, and ratios below one are disruptive. the non-linear performance of visrat and disrat make them much more difficult to use as predictors of detectability without considerable further investigation of the optimal ratio, which may even change between study systems. the fact that the optimal visrat and disrat values were close to one suggests that they are either not measuring edge disruption effectively, or that the optimal level of disruption is very close to a background-matching strategy . disrat was, however, a good predictor of treatment type, able to determine whether prey were generated using the background matching or disruptive algorithms slightly better than gabrat. this demonstrates that the canny edge methods were not failing due to misidentification of edge artefacts on the prey’s outline. in line with our predictions based on biases in human spatial frequency detection  <cit> , gabrat was most effective with a sigma of 3 pixels. this also suggests that the canny edge metrics should have been performing optimally for the viewing distance, as they were also calculated at this scale. taken together this suggests that the angle of the perceived edges relative to the prey’s outline is essential in any model attempting to describe edge disruption, as this is the primary difference between the canny and gabrat methods that were found to behave so differently in this study. the canny-edges shown in fig. 1d demonstrate why basing metrics on the presence of edges alone is not sufficient; the disruptive prey in this example has a large number of detected edges in its outline region that mostly run at right angles to the outline.

the success of gabrat in predicting capture times is all the more striking given the comparatively small area that it measures . the local comparison zone encompassed approximately  <dig>  pixels, and the global camouflage metrics measured  <dig>  million pixels. by contrast, the gabratσ <dig> kernel has a maximum diameter of just 19 pixels, covering an area of approximately 5500 pixels. even though gabrat only takes into account  <dig> % of the available data in the scene, those data were found to be far more effective for predicting capture times than any other data we measured, supporting the notions of cott  <cit>  and thayer  <cit>  that the animal’s outline tends to give it away, and suggesting our working definition of edge disruption that takes into account the difference between perceived edges and body outline  <cit>  fits with the observed data. in addition, gabrat is one of the least computationally demanding metrics measured here, and uses biologically inspired methods. the variables required for specifying the model can be based on neurophysiology  <cit>  without the need for guessing variables and thresholds, which sets it apart from the canny or sift-based models  <cit> , or edge-intersecting patch counts  <cit> . an alternative conclusion is that the pattern and luminance-based metrics we have measured are less effective, perhaps because they fail to model real visual systems adequately, although these methods have a substantial track record in supporting hypotheses in natural systems  <cit> .

in line with webster et al.  <cit> , we found the edge-intersecting patch count was a good predictor of capture time, indeed it outperformed all pattern- and luminance-based metrics other than sift even though it is blind to the interaction between prey and their backgrounds. however, it is also a less useful metric for generalising to other systems where the edge-intersecting patches are less easily defined. for example, how should discrete patches be distinguished in real prey images, what regions around the prey’s outline should be used, and what visual processing architecture could reproduce these counts? therefore, although this metric is successful in this study where patches are defined by the prey generation algorithm, we think it an unlikely avenue for fruitful future research into edge disruption compared to metrics that more closely match known neural processing methods.

contrary to our expectations based on moreno et al.  <cit> , the best method of quantifying pattern difference was the sift model. although in line with our predictions, prey took longer to capture if they shared more features with their local background region than their global background. this result is similar to experiments demonstrating that captive blue tits cyanistes caeruleus took longer to find prey against backgrounds with higher densities of geometric shapes identical to those on the prey  <cit> . our finding suggests that the same effect holds true when natural backgrounds rather than repeated geometric shapes are used. the sift model was also the only pattern matching model that performed better than treatment type, which is perhaps surprising given treatment type was blind to the interaction between individual prey and their backgrounds. as predicted, the hmax models performed better than the fourier-based bandpass models. the hmax models that forced comparisons to be made between prey and background without allowing for orientation changes were more effective, for example demonstrating that where there were stripes on the prey these offered the most effective camouflage when they were at the same orientation as the stripes in their background at a similar spatial scale. the fourier-based global patterndiff metric performed comparatively well compared to the hmax metrics even though it is substantially less computationally demanding and less biologically accurate. the other fourier-based metrics fared less well, although when the global pattern descriptive statistics were combined into an optimal model it predicted capture times well, indeed performing better than any other hmax or fourier-based method. however, this model is not directly comparable to the others in that it was adapted to fit the observed data in this study from a large number of degrees of freedom, giving it an unfair advantage. nevertheless, this process is useful because it highlights those descriptive pattern metrics that best predicted capture times in this study, making it the most useful method for producing information on the importance of different aspects of pattern matching, such as whether spatial scale or energy measures are more important. by contrast the sift model provides the least information on what aspects of the general patterns are most important, making it less easy to infer what types of features are most important, how their importance is weighted, and whether these variables and weightings apply equally well to non-human visual systems.

our data suggest that while matching the average background luminance is important, it is substantially less important than pattern matching or edge disruption metrics. we might have expected to find that pattern and edge disruption should only be important up the point where the prey become so different in average luminance to their background that they stand out . however, the best luminance predictor  was dropped from the final model of the best predictors, suggesting that this is not the case. nor was there autocorrelation between this luminance metric and the best pattern and edge disruption metrics, demonstrating—contrary to our expectation—that prey can mismatch the luminance of their local background and still have a good sift pattern match and level of gabratσ <dig> edge disruption. prey in real-world situations could have a level of luminance mismatch with their environment beyond those achieved by our computer display, however most background matching prey would not be expected to have such a big luminance difference to their background. the interaction in the final model of best predictors between gabratσ <dig> and local sift pattern match suggest these metrics can operate in synergy to increase detection times. although, the effect size of this interaction was small compared to the abilities of gabratσ <dig> and sift to predict capture times on their own.

to our knowledge this study is the first to compare a wide range of different contemporary methods for testing levels of achromatic camouflage. we have validated the use of gabrat, a novel edge disruption metric, while the visrat and disrat metrics adopted in the literature to date for investigating edge disruption cannot be used as reliable indicators of detectability. fourier-based methods were less effective than more complex and biologically plausible methods, they were, however, the most informative for distinguishing different aspects of a pattern’s nature, and were still highly significant predictors of capture time. we would still therefore recommend their use in cases where little is known about the receiver’s visual system. hmax models, while being the most biologically plausible for quantifying pattern difference were not found to be as effective as sift for predicting capture times, indicating that the number of distinct features shared between two patterns is more important than the overall scales, angles and contrast levels. our use of tree-bark backgrounds photographed under diffuse lighting conditions may also have influenced our findings, and qualitatively different results could be possible against alternative backgrounds, and under different lighting conditions. a number of studies have demonstrated the importance of background complexity in affecting detectability  <cit> , so our findings may not hold against simple backgrounds with low levels of achromatic complexity. xiao and cuthill found that feature congestion best predicted human and bird detection of artificial moths  <cit> . this metric combines local achromatic and chromatic changes, and edge orientation changes. while our study did not consider colour differences or feature congestion explicitly, it measured a number of variables similar to those used in calculating the achromatic components of feature congestion; for example by measuring the number of canny edges locally, analysing the local pattern energy, and hmax gabor filtering, which takes into account edge orientations. while we found that all of these metrics predicted capture times in line with xiao & cuthill, they were not as effective as other methods, possibly because they do not consider the prey’s interaction with its background. future work should compare the effectiveness of these models with images of natural prey, and in wholly natural systems to establish how wide-ranging these findings are to detection times in alternative contexts and with non-human visual systems  <cit> . in addition, models should be developed that can integrate chromatic cues; experiments on colour discrimination typically involve comparisons between flat colour patches rather than the complex and varied colours encountered in natural search tasks.

CONCLUSIONS
this study demonstrates how best to measure camouflage from appearance, however the same models can also be used to measure signals that stand out from the background  <cit> . the methods tested in this study are therefore useful for researchers studying the appearance of wide-ranging signals, from sexual and aposematic displays to mimicry and camouflage in fields from evolutionary and sensory ecology to military camouflage and advertising. model validation in humans can also help to reduce the number of costly animal behaviour experiments required for testing visual hypotheses. our findings have two main evolutionary implications: first, that we would expect camouflage to be optimised by creating false edges at scales linked to the typical detection distances of the receiver, and second, that while visual systems should have evolved to overcome this weak-spot as effectively as possible, recognising the animal’s outline is still key to detection and/or recognition. likewise, signals that have evolved to maximise detection  should do the opposite, creating coherent edge cues at scales relevant to typical viewing distances.

