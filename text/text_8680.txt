BACKGROUND
modeling of quantitative structure activity relationship  of drug molecules will help to predict the molecular activities, which reduce the cost of traditional experiments, simultaneously improve the efficiency of drug molecular design  <cit> . molecular activity is determined by its structure, so structure parameters are extracted by different methods to build qsar models. many machine learning methods have been used to the modeling of qsar problems, like multiple linear regression, k-nearest neighbor  <cit> , partial least squares  <cit> , kriging  <cit> , artificial neural networks  <cit>  and support vector machines , of which svm is a state-of-arts method and achieved satisfactory results in the previous studies  <cit> .

nowadays, ensemble learning is becoming a hot topic in the machine learning and bioinformatics communities  <cit> , which has been widely used to improve the generalization performance of single learning machines. for ensemble learning, a good ensemble is one whose individuals are both accurate and make their errors on different parts of the input space  <cit> . the most popular methods for ensembles creation are bagging and boosting  <cit> . the effectiveness of such methods comes primarily from the diversity caused by re-sampling the training set. agrafiotis et al.  <cit>  compared bagging with other single learning machines on handling qsar problems and found that bagging is not always the best one. signal was proposed in  <cit> , it created an ensemble of meaningful descriptors chosen from a much larger property space which showed better performance than other methods. random forest was also used in qsar problems  <cit> . dutta et al. used  <cit>  different learning machines to make an ensemble to build qsar models, and feature selection is used to produce different subsets for different learning machines.

although the above learning methods obtained satisfactory results, but most of the previous works ignored a critical problem in the modeling of qsar that the number of positive examples often greatly fewer than that of negatives. to handle this problem, hou et al.  <cit>  discussed this problem and assigned different costs for two different classes of svm and improved the prediction results. here combing ensemble methods, we propose to use asymmetric bagging of svm to address the unbalanced problem. asymmetric bagging of svm has been used to improve relevance feedback in image retrieval  <cit> . instead of re-sampling from the whole data set, asymmetric bagging keeps the positive examples fixed and re-samples only from the negatives to make the data subset of individuals unbalanced. furthermore, we employ auc   <cit>  as the measure of predictive results, because only the measure of prediction accuracy of correction can not show the overall performance. we will analysis the experimental results in terms of auc and other several popular measures like sensitivity and specificity as well as correction. furthermore, in qsar problems, many parameters are extracted from the molecular structures as features, but some features are redundant and even irrelevant, these features will hurt the generalization performance of learning machines  <cit> . for feature selection, different methods can be categorized into the filter model, the wrapper model and the embedded model  <cit> , where the filter model is independent of the learning machine and both the embedded model and the wrapper model are depending on the learning machine, but the embedded model has lower computation complexity than the wrapper model has. different methods have been applied to qsar problems  <cit> , and shown that proper feature selection of molecular descriptor will help improve the prediction accuracy.

in order to improve the accuracy of asymmetric bagging, we will use the feature selection methods to improve the accuracy of individuals, this is motivated by the work of li and liu's work  <cit> , where they found embedded feature selection is effective to improve accuracy of bagging of svm and proposed an algorithm prifeb, which improved generalization performance of ordinary bagging. here we propose to combine prifeb with asymmetric bagging and develop a novel algorithm named prifeab to solve the prediction problem of unbalanced qsar.

RESULTS
in order to demonstrate the effect of unbalanced learning methods, we have performed the following series experiments by using support vector machine  as base classifiers.

 <dig>  svm is a baseline method, which uses a 2-norm soft margin version of svm.

 <dig>  unsvm assigns different c for different classes. the parameter of balanced_bridge is set as the value of the ratio of the number of positive examples to that of negatives which is  <dig>  in this paper.

 <dig>  bagging a commonly used ensemble method, which uses svm as base learners. the number of individuals is  <dig> 

 <dig>  unbagging is also a commonly used bagging method, which uses unsvm as base learners. there are also  <dig> individuals.

 <dig>  asbagging is asymmetric bagging which uses svm as base learners.

 <dig>  ppifeab is a bagging method, which employs feature section for asbagging to remove irrelevant and redundant features.

prediction performance
experiments are performed to investigate if asymmetric bagging and feature selection help to improve performance of bagging. support vector machines with c =  <dig>  Ïƒ = 0: <dig> are used as individual classifiers, and the number of individuals is  <dig> for all bagging methods. for unsvm, balanced_bridge is used to denote the ratio of c+ to c-, which is  <dig> . for ordinary bagging, each individual has one tenth of the training data set, while for asbagging, the size of individual data subset is twice of the positive sample in the whole data set. the 3-fold cross validation scheme is used to validate the results, experiments on each algorithm are repeated  <dig> times. we test the learning methods on individual molecular descriptors, and there are bcut, constitutional, prop and topological descriptors, which are represented by bcut, const, prop and topo respectively.

the average bacc values are shown in figure  <dig>  from which, we can obviously find that:

 unsvm does improve performance of svm.

 bagging does not reach our expectation, it does not improve performance of svm, so does unbagging, which has the similar results of bagging.

 asbagging greatly improves performance of svm, and prifeab slightly improve results of asbagging.

tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> list the results of different measures i.e. auc, bacc, sensitivity, specificity, ppv, npv, correction by using the above svm and bagging methods. we also list the ratio values of the number of features used in prifeab to the total number in table  <dig>  from tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  we can see that:

outside of parentheses represent the mean of the respective performance measure, while inside of parentheses correspond to the standard deviation across the  <dig> times of 3-fold cross validations.

outside of parentheses represent the mean of the respective performance measure, while inside of parentheses correspond to the standard deviation across the  <dig> times of 3-fold cross validations.

outside of parentheses represent the mean of the respective performance measure, while inside of parentheses correspond to the standard deviation across the  <dig> times of 3-fold cross validations.

outside of parentheses represent the mean of the respective performance measure, while inside of parentheses correspond to the standard deviation across the  <dig> times of 3-fold cross validations.

outside of parentheses represent the mean of the respective performance measure, while inside of parentheses correspond to the standard deviation across the  <dig> times of 3-fold cross validations.

outside of parentheses represent the mean of the respective performance measure, while inside of parentheses correspond to the standard deviation across the  <dig> times of 3-fold cross validations.

outside of parentheses represent the mean of the respective performance measure, while inside of parentheses correspond to the standard deviation across the  <dig> times of 3-fold cross validations.

outside of parentheses represent the mean value, while inside of parentheses correspond to the standard deviation across the  <dig> times of 3-fold cross validations.

 unsvm obtains a slight improvement of ordinary svm on three descriptors in terms of the auc and bacc measures.

 ordinary bagging fails to improve single learning methods, not only bagging but also unbagging get worse results than svm and unsvm on the measures of auc, bacc and sensitivity.

 asbagging and prifeab obtain 20% better results than svm, unsvm, bagging and unbagging on the auc measure. the sensitivity values of asbagging and prifeab increase by beyond 50% from svm, unsvm, bagging and unbagging on average.

 prifeab obtains slightly better results than asbagging on both sensitivity and specificity measures. we also observed that only few features are removed by feature selection.

 there are several cases, the learning machines fail in prediction and nearly all the examples are classified into negative, i.e. svm, bagging, unbagging on const and unsvm on topo. only asbagging and prifeab succeed in all predication.

discussions
the above results show that asbagging and prifeab perform better than the other several methods of svm, unsvm, bagging and unbagging. here we give some insights on these results:

 though single svm is not stable, and can not obtain valuable results, in this case of high skew data sets, bagging does not improve its generalization performance in terms of auc, bacc and sensitivity. bagging gets a high correction value, which is trivial, because few positive examples are predicted correctly. especially, when learning machines fail in prediction on some descriptor data sets, all the labels are predicted as negative, a high value of correction is obtained as  <dig> %, which is the ratio of negative sample to the whole sample.

 since this is a drug discovery problem, we pay more attention to positives. auc, bacc and sensitivity are more valuable than correction to measure a classifier. asymmetric bagging and prifeab improve the auc values of ordinary bagging. simultaneously, sensitivity are improved greatly, which shows asymmetric bagging is proper to solve the unbalanced drug discovery problem. asymmetric bagging wins in two aspects, one is that it make the individual data subsets balanced, the second is that it pay more attention to the positives by leaving the positives always in the data set, which makes sensitivity is higher than ordinary bagging.

 prifeab achieves slightly better results than asymmetric bagging does. feature selection using prediction risk as criteria also make prifeab win in two aspects, one is that embedded feature selection is dependent with the used learning machine, it will select features which benefit the generalization performance of individual classifiers, the second is that different features selected for different individual data subsets, which makes more diversity of bagging and improves their whole performance. the results improved by prifeab than asymmetric are slight, we consider the reason is that few features are removed. feature selection using prediction risk is dependent on svm. here, positives are few, which will hurt generalization performance of svm, and furthermore hurt effect of feature selection.

 the data set used is so skew that the ratio of positives to negatives is only  <dig> , not beyond 2%, which makes svm, unsvm and baging, unbagging disable of prediction, they fail on four out of sixteen cases and predict almost all labels to negative, even on other twelve cases, they give low sensitivity. analysis of high skew data set is still a difficult problem.

CONCLUSIONS
to address the unbalanced problem of drug discovery, we propose to apply asymmetric bagging and feature selection to the modeling of qsar of drug molecules. asymmetrical bagging of svm and a novel algorithm prifeab are compared with ordinary bagging of support vector machines on a large drug molecular activities data set, experiments show that asymmetric bagging and feature selection can improve the prediction ability of svm in terms of auc and sensitivity. since this is a drug discovery problem, the positive sample is few but important, auc and sensitivity is more proper than correction to measure generalization performance of classifiers.

this work introduces asymmetric bagging into prediction of drug activities and furthermore extends feature selection to asymmetric bagging. this work only concerns an embedded feature selection model with the prediction risk criteria, one of the future work will try more efficient and more effective feature selection methods for this task.

