BACKGROUND
the latest generation of expression microarray technology offers a snapshot of the entire transcriptome with an accuracy and resolution that was unimaginable just a few years ago. the recent improvements have come from several different directions. on the purely technological front, arrayers, scanners and hybridization chambers have improved significantly in recent years. laboratory technique has become standardized as microarrays are used more widely. improved genomic libraries have made it possible to select probes more appropriately and to annotate them with greater accuracy. advances in array processing and data analysis have made the most of available data within the limitations of current technology. several microarray technologies are currently available, and new players appear all the time, but a few formats have emerged as leaders in the field. cdna arrays paved the way more than ten years ago, and remain a widely used platform. affymetrix introduced the oligonucleotide approach to chip design and quickly became the most widely used expression array format. agilent, in cooperation with rosetta, has combined some of the best features of both of these approaches and gained wide spread acceptance as a result.

in the two-color cdna platform, mrna from two samples is reverse-transcribed to cdna, labeled with fluorescent dye, cy <dig>  or cy <dig> , and simultaneously hybridized to an array containing spots of dna sequences. ratios of fluorescence intensities provide a relative measure of expression at each spot on the array.

in single channel oligonucleotide arrays, affymetrix, short  oligonucleotides are synthesized in situ by photolithographic methods and attached to the array. each gene is represented by a set of spots on the array called a probe set. unlike cdna arrays, only a single sample can be measured on the chip at a time and two separate hybridizations are required to study differential expression.

agilent combines two-sample hybridization with the use of long  oligonucleotides. these arrays are also hybridized with two different fluorescent samples and measurements of differential expression obtained from the relative abundance of hybridized mrna.

pre-processing and normalization of microarray data, with the goal of controlling the effects of systematic error while retaining full biological variation, are critical to obtaining valid results. it is now recognized that these steps are platform specific and difficult to automate  <cit> . two color cdna microarrays and single color affymetrix oligonucleotide chips have been extensively studied and platform specific recommendations for the analysis of data from these microarray technologies are well documented  <cit> , for cdna arrays, and  <cit>  for affymetrix arrays. agilent microarrays have not enjoyed the same degree of scrutiny to date.

the goal of this study is to quantify some of the sources of error that affect measurement of expression using agilent arrays and to compare agilent's feature extraction software with pre-processing methods that have become the standard for normalization of cdna arrays. these include log transformation followed by loess normalization with or without background subtraction and often a between array scale normalization procedure. the larger goal is to define best study design and pre-processing practices for agilent arrays, and though this goal is not fully realized, we offer some early suggestions.

agilent's feature extraction algorithms were developed with the aim of reducing systematic errors that arise from labeling bias, irregular feature morphologies, mismatched sample concentrations and cross-hybridization,  <cit> . they quantify feature signals and their background, perform background subtraction, dye normalization, and calculate feature log ratios  and error estimates. the error estimates, based on an extensive error model and pixel level statistics calculated from the feature and background for each spot, are used to generate a p-value for each log ratio.

the file generated by agilent's extraction software also contains raw pixel intensity data. these intensities, either mean or median values, can easily be exported to other software, such as r  <cit> .

RESULTS
three datasets, representing three mammalian species, are used in this paper. full details and data quality statistics are described in the methods section. to avoid duplication, most results are shown only for a few arrays. however, all three experiments yielded very similar results, as have other agilent datasets prepared at this institution but not available for publication here.

human cancer cell line du-145
the first dataset uses the human cancer cell line du- <dig>  treated with two doses,  <dig> um and  <dig> um, of 5'aza-2'deoxycytidine. treated rna was labeled with cy <dig>  and untreated rna labeled with cy <dig>  on two arrays. a third array is a dye-swap hybridization of the  <dig> um dose.

murine prostate development
the second dataset is from a study of prostate development in the mouse. total rna was isolated and pooled from same-sex siblings for each of five age-matched litters. for each litter, competitive hybridization  with dye swap was performed using agilent  <dig> k mouse arrays.

canine self-self with spike-in
for the third dataset, total rna was isolated from one dog brain sample and applied to both channels on each of four agilent canine  <dig> k arrays. rna spike-ins were used from agilent's two-color rna spike-in kit following manufacturer's instructions. table  <dig> summarizes the design of the spike-in experiment used in this study and figure  <dig> shows an ma plot of one array. note that the lowest intensities in figure  <dig> are not represented by spike-in probes.

pre-processing and normalization
the pre-processing methods compared in this paper include agilent's feature extraction and simple loess normalization, as implemented in the limma bioconductor package  <cit> . loess normalization is considered without a background correction step as well as with background subtraction. these normalizations are compared using graphical presentations, fold change estimates, empirical bayes t-statistics and roc curves.

image processing was performed using agilent's feature extraction software. after image processing, the intensity of each spot is summarized by the mean or median pixel intensity of the spot, as well as a measurement of inter-pixel variability within the spot. we found the choice of mean or median pixel intensity had little impact on downstream analysis. both pre and post normalization, there were no marked differences in standard exploratory microarray plots using either measure. log <dig> fold changes from a representative dye-swap pair in the prostate development study are shown in figure  <dig>  we chose the median for further analyses.

after image analysis, the feature extraction software and the marray or limma r packages in bioconductor were used for further pre-processing. both bioconductor packages have functions for reading the data, plotting images, and within and between array normalizations. the limma package also implements linear modeling for selecting differentially expressed genes and has functions for alternative methods of background correction.

post-normalization data for the prostate development study are shown in the ma plots in figure  <dig>  loess curves calculated with positive and negative controls removed are plotted in red. the red lines give a robust profile of the mean fold change as a function of average intensity. blue highlighted points are the negative controls which should show very little hybridization and no differential expression. the ma plot of agilent normalized data exhibited large variablity at low intensities and a low intensity bias toward positive fold changes. this bias was also reflected in the negative controls. background subtracted, loess normalized ma plots showed more low intensity variablity than the same ma plots when background was not subtracted. negative control spots were more evenly scattered around the zero line than they were in the agilent normalized plot.

scatterplots of log <dig> fold changes for arrays containing biologically identical samples  are a useful visualization for comparison of normalization algorithms. the dye-swap design employed in the prostate development experiment is ideal for this comparison, allowing consideration of dye-related effects as well as other sources of technical error. figure  <dig> shows scatterplots of log <dig> fold changes calculated on pairs of dye-swapped arrays. log fold changes from the dye-swapped array were inverted to show treatment/control in both dimensions. dye-swap pearson correlations were higher without background subtraction  than with background subtraction . the agilent processed signal in these plots exhibited the largest variability.

the rationale for subtracting a measure of background from the signal intensity is the assumption that the fluorescence of a spot is the sum of signal intensity and some background noise which is due to a variety of technical factors. if background is included in the estimate of the signal for a spot, then the result is a biased estimate of the true hybridization. this is particularly prominent at low signal intensities, resulting in an underestimate of the fold changes. on the other hand, subtraction of background involves an additional estimate which will increase the variability of the signal log ratios  <cit> . this extra variability is clearly seen in both the ma plots and the dye-swap plots. boxplots of the spike-in probes from the canine self-self experiment, figure  <dig>  show the bias that results when background is not subtracted. the horizontal reference line in each plot is the target fold change for that probe. the first two plots in the top row are low intensity probes and this is where failure to subtract background has the greatest effect on fold change. as spike-in concentration and intensities increase, the effect on fold change is smaller. note that increased variability of the agilent pre-processing is seen in the lowest intensity spike-in features as well. for all methods, however, the range of observed fold changes did not include zero and the distributions were tight with few outliers.

to explore the effect of background subtraction further, we viewed our post loess normalized dye-swap data using three levels of background subtraction, a local background subtraction method, a minimal constant background subtraction and no background adjustment, figure  <dig>  the minimal constant background adjustment subtracted the minimum value of background for the entire array from each signal intensity. both pre and post-normalization, the highest dye-swap correlation was observed with no background adjustment, followed by the minimal constant adjustment and the lowest correlation was seen with local background subtraction. this was the expected result of increasing variability by using two estimates to quantify intensity, foreground minus background, as opposed to using the foreground estimate only.

ultimately, the best criteria on which to base the selection of a pre-processing method is whether it gives the correct answer. the spike-in experiment, though small, does allow us to compare observed results to expected. because the background rna is the same in both samples, only the spiked-in genes, comprising a total of  <dig> spots on each array, are present in different quantities in the two channels. roc curves were used to determine how well these spike-in probes, with expected fold changes greater than zero, could be distinguished among the set of non-spiked in probes and probes spiked-in with expected fold changes of zero. fold changes calculated after loess normalization, with or without background subtraction, gave higher specificity for differential expression than after processing with the agilent feature extraction algorithm, figure  <dig>  the area under the roc curve for the agilent processed signal was  <dig> % compared to  <dig> % for loess normalization with background subtraction and  <dig> % without background subtraction. when a moderated t-statistic, or a z-statistic in the case of agilent processing, calculated from the four arrays was used, the three methods performed equally well: agilent auc =  <dig> %, median with background subtraction auc =  <dig> %, and median without background subtraction auc =  <dig> %, figure  <dig>  the inset of figure  <dig> is the same graph where the x axis has been truncated and 1-specificity has been replaced by counts. this shows the very minor improvement using loess normalization with or without background subtraction compared to agilent normalization. the fact that the aucs are uniformly large indicates that the spike-in experiment does not include a sufficient number of low intensity probes to fully assess the performance of these methods.

it should be noted that, although loess normalization performs well in many cases, there are experiments for which it may not be appropriate. loess normalization requires the assumption that either most of the genes are not differentially expressed across the range of intensities or that there are an approximately equal number of up and down regulated genes across the intensity range  <cit> . a spike-in experiment, having a small number of spiked-in transcripts with a symmetric design, is ideal for loess normalization  <cit> . if the design of an experiment does not meet these assumptions , an alternative normalization may be needed. the recent paper by oshlack et al.  <cit>  discusses normalization for boutique arrays and presents an alternative weighted loess normalization that could be considered in this case.

we considered a between array scale normalization procedure  <cit>  but found it to add variability to the spike-in experiment used in this study. yang et al.  <cit>  have similarly observed that cross-array normalization can increase mean square error. given the importance of the spike-in experiment for the evaluation of pre-processing methods, we felt that we could not provide a fair evaluation of cross-array normalization. we believe that the additional variation in the spike-in log ratios observed after this cross-array normalization may be explained by the methodology for the spike-in experiments. specifically, spike-ins are added into the sample relatively late in the preparation of the cdna samples, and the log ratios of the spike-in genes are therefore subject to less cross-array variability than the naturally expressed transcripts of similar abundance. as a result, cross-array normalization based on naturally expressed transcripts can paradoxically add variability to the spiked-in transcripts.

dye effects
several investigators  <cit>  have observed that gene specific dye effects persist in two-color arrays after global normalization procedures. some have recommended that experiments incorporate dye-swaps, common reference designs, or other methods for controlling dye effect, while others argue that this is unnecessary,  <cit>  and  <cit> . the self-self design of our spike-in experiment permits an investigation of the number and magnitude of uncorrected dye effects.

we investigated the presence of dye effects using one-class moderated empirical bayes t-statistics from the non spiked-in probes in the self-self experiment. the observed and null distributions are compared in figure  <dig>  the null distribution was obtained by changing the sign of two of the four log ratios for each gene so that these two represent cy3/cy <dig> instead of cy5/cy <dig>  moderated t-statistics for each normalization algorithm are shown in the top row of figure  <dig>  in both background corrected normalizations, the peak of the observed curve is shifted in the positive direction, and in all three plots, the observed distributions have heavier tails than the expected. these results suggest that gene specific dye effects do remain following normalization and that background subtraction, which amplifies the noise at the low end of the intensities, may also amplify some of these dye effects. if these amplifications are more positive than negative, this would explain the shift in the observed distributions of the moderated t-statistics when background is subtracted. the moderated t-statistics for dye effect are largest after agilent processing and smallest after loess normalization without background subtraction. in the bottom row of this figure, observed distributions of the mean fold changes for each pre-processing method give an idea of the magnitude of the raw dye effects. it is encouraging that the effects tend to be quite small for the vast majority of genes, regardless of the processing method used.

an estimate of the number of genes affected by dye bias was also calculated from the self-self experiment. if a feature has no remaining dye effects after normalization, then in a self-self hybridization, the log ratio log <dig>  measured on an arbitrary array, is equally likely to be positive or negative. thus, the number of positive log ratios observed over n arrays will have a null binomial distribution with probability of success p =  <dig> .

the distribution observed in our self-self experiment, table  <dig>  has a distinctly heavier tail than does the null distribution, consistent with the previous observation that dye effects linger after normalization. in order to estimate the number of affected spots, a three component binomial mixture model was fit to the four arrays in the self-self experiment:

 π2binom+binom+π2binom.
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaaggaciab=b8awbqaaiabikdayaaacqqgibgycqqgpbqacqqgubgbcqqgvbwbcqqgtbqbcqggoaakcqwgwbaccqggsaalcqai0aancqggpaqkcqghrawkcqggoaakcqaixaqmcqghsislcqwfapaccqggpaqkcqqgibgycqqgpbqacqqgubgbcqqgvbwbcqqgtbqbcqggoaakcqaiwaamcqgguaglcqai1aqncqggsaalcqai0aancqggpaqkcqghrawkdawcaaqaaiab=b8awbqaaiabikdayaaacqqgibgycqqgpbqacqqgubgbcqqgvbwbcqqgtbqbcqggoaakcqaixaqmcqghsislcqwgwbaccqggsaalcqai0aancqggpaqkcqgguaglaaa@6093@ 

the choice of three components reflects both the symmetry observed in the data and the small number of arrays. the center component represents the null case, and its mixture coefficient  <dig> - π estimates the proportion of array features without dye effect. features with dye effect are segregated into the two flanking components. by taking advantage of the symmetry evident in the observed distribution, this model requires only two free parameters: π, representing the proportion of genes with dye effect; and p, which for our purposes is a nuisance parameter. the model was fit on a grid of parameter values, and the optimal fit was obtained when p =  <dig>  and π =  <dig> . it is of concern that more than 40% of genes exhibited a persistent dye effect, even though these effects were small. a 99% confidence interval of  for the proportion of genes with dye effect was estimated by parametric bootstrap, where the bootstrap was taken over genes rather than samples. the corresponding values of the nuisance parameter,  p, varied between  <dig>  and  <dig> . details of the algorithm are given in the methods section.

error model
measurement errors in a microarray experiment are a combination of systematic and random components that can arise at any step during the study: array manufacture, mrna preparation, hybridization, scanning, and imaging. background subtraction and dye normalization are techniques used to adjust raw data for known systematic errors, while error modeling estimates the additional variability in a spot due to random error or any unknown systematic errors.

agilent's default method for estimating within spot variability is a hybrid. under this model, the error estimate used for each spot is the larger of a universal error model and a propagated error model. for the propagated error model, estimates of random error for individual log fold changes are based on pixel level statistics and propagated through the background subtraction and normalization steps to the final log ratio. the universal error estimate is calculated using additive and multiplicative adjustment terms that have been estimated with many platform specific self-self experiments. it represents the expected error of the difference between the red and green channels  <cit> . on these arrays the universal error model produced the larger estimate of error for >  <dig> % of the probes. based on these error estimates, z-statistics and p-values are calculated for each probe.

an agilent processed ma plot from the spike-in study is shown in figure  <dig>  every log fold change  is matched by a blue point with the same a value which shows the error in the log fold change, estimated according to the agilent universal error model. when log fold changes are negative, errors are multiplied by - <dig> before plotting to better illustrate the pattern. the pattern of blue dots shows that while the error model is not a simple function of a alone, it can be fairly well approximated by such a function. the model clearly captures the large variation that characterizes low intensity genes after agilent pre-processing. when roc curves were drawn using the z-statistics that agilent provides, the area under the curve improved from the 89% observed for unstandardized log ratios to 99%. thus, the performance of the z-statistics was similar to that of fold changes calculated after loess normalization.

the estimates provided by the universal error model closely follow the general contours of the ma plot, and we believe that it captures sources of systematic variation introduced in pre-processing. it is possible that the universal error model also captures probe specific variability. to determine how well the model performs for individual probes, we considered  <dig> probes that are each represented on the agilent human  <dig> k array by  <dig> separate spots. figure  <dig> shows the three arrays from the human prostate cancer dataset. each point represents one of the  <dig> replicated sequences. the observed standard deviation of the  <dig> replicate spots is plotted on the horizontal axis against the mean agilent error model estimate for the  <dig> replicate spots, shown on the vertical axis. pearson correlation coefficients are  <dig> ,  <dig>  and  <dig>  for the three arrays. low intensity spots, those having mean single channel intensity less than  <dig>  are shown in blue. for these probes, where a values are useful predictors of error, some of the sequence specific differences in errors are captured with the agilent error model, although approximation to the actual error is poor. for higher intensity spots , where observed error is small, the universal error model also predicts low error, but these model based estimates do not show appreciable association with the estimates made from the observed data. our results are consistent with the hypothesis that the error model primarily captures systematic variation introduced in pre-processing, and there is no compelling evidence to support the alternative hypothesis that additional probe specific variability is captured by the error model. however, these results are not conclusive, and a much larger set of replicated probes would be necessary to fully characterize the variability in the error estimates and evaluate the competing hypotheses.

CONCLUSIONS
although it has become one of the leading expression array technologies, agilent arrays have not enjoyed the intense scrutiny that has led to the development of very effective processing methods for some competing platforms. the objective of this study was to evaluate sources of error and, specifically, to understand dye effects in the agilent platform with a consideration for the equilibrium between budgetary supply and data demand that is so crucial to study design decisions.

we came to prefer a simple pre-processing procedure that consists of a log transformation of the intensities followed by an intensity-dependent loess normalization applied to each array. this is consistent with the current practices for cdna arrays which were developed as a result of careful analysis of the technology and extensive experimentation  <cit> . on the matter of background subtraction, we prefer no background subtraction, but appreciate the arguments on both sides. two forces drive our preference for no background subtraction. the first, entirely philosophical, is an aversion to unnecessary data processing. the second is provided by the analyses presented here, which we believe demonstrate a perhaps slight but nonetheless clear advantage for this pre-processing method.

as figure  <dig> shows, agilent's feature extraction algorithm does result in less biased fold changes, particularly for low abundance transcripts, but a great deal of variability is introduced, especially at the low intensity end. the roc analysis makes it clear that the ability to detect differentially expressed genes suffers as a result. either of the simpler pre-processing methods is a better choice.

it is difficult to tease out the components of the feature extraction algorithm that contribute the excess variability seen in figures 2- <dig>  however, based on our analysis of background subtraction with loess normalization, it may be surmised that background subtraction plays a role here as well. another pre-processing step that may contribute to the variability is the use of a surrogate intensity value when the measured intensity of an array feature is not significantly different from background levels. this step is applied separately in the two channels so that for example, the intensity measured on the cy <dig> channel might just meet criteria, while the similar value measured in cy <dig> just misses it and is replaced by a generic surrogate value.

the question of whether or not to subtract background is a difficult one. the fold changes for low intensity genes are clearly attenuated toward zero when background is not subtracted. on the other hand, the additional background estimation error at the low intensity end might overwhelm signal, even without fold change attenuation. we believe that making present/absent calls does not help to resolve the dilemma. those transcripts that are expressed at the lowest levels can not be distinguished from noise and present/absent calls essentially foreclose on these. these transcripts are always difficult to find and one should not make it impossible to detect them.

rather than considering the relative merits and risks of bias and variance separately, we prefer to evaluate the contributions each makes to mean squared error, leaving a more accurate quantification for follow up work in the lab. the data presented here are not amenable to evaluating mean squared error reliably. however, scharpf et al.  <cit>  have used simulations in an extensive study of the issue. these simulations show that in most cases, the largest portion of mean squared error for low intensity genes is due to random background estimation error rather than the systematic shrinkage of fold changes. it was found that this is true unless there is clear correlation between background and foreground, indicating that local background is an important component of foreground intensities. in the arrays used here, a dozen or so outlying spots on each array  drive correlations of  <dig> – <dig>  between foreground and background, but in general no relationship is evident.

the noise evident in dye-swap plots and the slight advantage that roc analysis shows for non-subtracted intensities, together with scharpf's findings, lead us to vote against background subtraction. it should be noted, however, that when error was taken into account, in t-statistics or agilent's z-statistics derived using the universal error model, all methods gave near perfect results in this study. the inclusion of additional spiked-in probes at lower intensities would permit a larger and more sensitive experiment. we second the recent recommendation of tong et al.  <cit>  that additional controls, suitable for use with spiked-in rnas, be included on arrays.

several datasets, including human, mouse and dog samples, yield similar results giving confidence that results can be generalized. in addition to the specific datasets included in this publication, the conclusions reflect the consensus of a larger institutional experience.

at this time, the microarray community agrees that with fixed budgets and competing scientific needs, biological replication is more important than technical replication in expression studies. however, gene specific dye effects, unaddressed by within array normalization procedures, are potentially a significant source of error. therefore, for two color arrays, dye-swap replicates are often included in study design to control this source of error.

in our agilent spike-in study, gene specific dye biases persisted after normalization, affecting nearly half the genes. for all but a few genes, however, these effects were quite small. because many genes were involved, we suggest incorporating dye-swaps when budget and experimental design permit. often it is possible to fold a dye swap into existing biological replicates, hybridizing treatment with cy <dig> in one sample and with cy <dig> in another. because dye-swap plots after loess normalization without background subtraction show substantial agreement and because gene specific dye effects were generally small, we prefer not to spend arrays on dedicated dye-swaps.

