BACKGROUND
advances in protein mass spectrometry have have recently shown great potential for high-throughput disease classification and biomarker identification. in turn, fast and accurate detection of diseases, such as early cancer detection, can revolutionize the field of medical diagnosis. typically, serum samples are analyzed by a mass spectrometer, producing a high dimensional abundance histogram. next, informative features are extracted from the high dimensional data and are presented to a classifier. in turn, the classifier outputs a decision about the status of the patient with respect to a particular disease . recently, numerous feature selection and classification techniques have been shown to perform well on several isolated data sets. however, current literature does not contain rigorous comparative studies analyzing the merits of individual feature selection and classification algorithms across several data sets. this paper analyzes several state-of-the-art feature selection methods coupled with a very fast nearest centroid classifier. in addition, we present a novel combination of boosted feature extraction coupled with the nearest centroid classifier, which consistently outperforms all other algorithms tested in terms of classification accuracy.

mass spectrometry analysis
discovered by sir j.j. thomson in the early part of the 20th century mass spectrometry  is a technique for 'weighting' individual molecules, fragments of molecules or individual atoms that have been ionized. in a vacuum environment an ion source vaporizes and charges the sample matter, which is then deflected into a magnetic or electric field. the mass spectrometer then measures the molecular masses along with abundances and masses of fragments that are produced as a result of molecular breakdown. the fundamental measurement unit of the ms is the mass-to-change ratio . for proteomic applications, daltons  are used to measure mass, while the electric potential of a single electron is the measurement unit for charge . the spectrum is a graph of ion intensity as a function of mass-to-charge ratio and is often depicted as a histogram.

time-of-flight 
in time-of-flight  instruments, positive ions are produced by periodic bombardment of the sample with brief pulses of either electrons, secondary ions, or laser-generated photons. the ions produced by the laser are then accelerated by an electric field pulse and passed into a field-free drift tube. ideally, all ions entering the tube will have the same kinetic energies, and their velocities must therefore vary inversely with their masses, with lighter particles arriving at the detector earlier than the heavier ones. the ions therefore drift through a field-free path and are separated in space and time-of-flight  <cit> .

matrix assisted laser desorption/ionization 
by incorporating the molecules in a large excess of matrix molecules, strong intermolecular forces are reduced. the matrix molecules absorb the energy from the laser light and transfer it into excitation energy of the solid system. the effect is an instantaneous phase transition of small molecular layers of the sample into a gaseous state. thus solid  material can be easily analyzed by tof ms.

surface-enhanced laser desorption/ionization 
this method uses protein chip arrays with different selective surfaces such as cation or anion exchange surfaces, hydrophobic surfaces and metal binding surfaces. biofluids such as cell lysate, plasma or urine are applied onto the selective surface and, after washing, a subset of proteins is specifically bound. the chip is then analyzed in a  tof-ms which generates a protein spectrum of the different molecular masses present on the protein chip. this technology is therefore highly suited for research into molecular mechanisms of disease and biomarker identification.

related research
mass spectrometry  based pattern recognition is rapidly becoming a broad and fruitful research field. this section, provides details on current state of research within the field of early cancer diagnosis based on proteomic pattern recognition.

ovarian cancer studies
in  <cit> , genetic algorithms together with self-organizing maps were used to distinguish between healthy women and those afflicted with ovarian cancer. although cross-validation studies were not conducted, the approach was able to correctly classify all cancer stricken patients and 95% of healthy women, on a single test set.

using the same data sets in  <cit> , the researchers employed principle component analysis   <cit>  for dimensionality reduction and linear discriminant analysis   <cit>  coupled with a nearest centroid classifier  <cit>  for classification. for each of the train/test data splits,  <dig> cross-validation runs with re-sampling were conducted. when training sets were larger than 75% of the total sample size, perfect  accuracy was achieved on the oc-wcx2b data set. using only 50% of data for training, the performance dropped by  <dig> %. unfortunately, the probabilistic approach used in this study can leave some samples unclassified. for the oc-h <dig> data set, the system had a  <dig> % sensitivity and  <dig> % specificity when 75% of the data was used for training. however, only  <dig> % of the data samples were classified. similarly, for the oc-wcx2a data set  <dig> % sensitivity and  <dig> % specificity was attained on  <dig> % of the test data, when 75/ <dig> train/test split was used.

in  <cit> , the researchers compared two feature extraction algorithms together with several classification approaches on a maldi tof acquired data. the t-statistic, also known as the student-t test  <cit> , was used to rank features in terms of their relevance. then two feature subsets were greedily selected . support vector machines , random forests, linear/quadratic discriminant analysis , k-nearest neighbors, and bagged/boosted decision trees were subsequently used to classify the data. in addition, random forests were also used to select relevant features with previously mentioned algorithms used for classification. again  <dig> and  <dig> feature sets were selected and classification algorithms applied. when the t-statistic was used as a feature extraction technique, svm, lda and random forests classifiers obtained the top three results . on the other hand, classification improved to approximately 92% when random forests were used as both feature extractors and classifiers. similar performance was also achieved using the the nearest-neighbor algorithm, a close relative of the nearest centroid algorithm  <cit>  we will be using in this study. while the results appear promising, the authors provide little motivation as to why  <dig> and  <dig> feature sets were selected. other that the fact that lda and qda need the number of features to be less than the number of samples, the actual size of the selected feature set seems to be an arbitrary choice. in practice, determining the size of the feature set is an added burden, placed on the software developer and, ideally, should be eliminated. furthermore, testing several feature sets of various sizes and selecting the set with the best performance can lead to overfitting. with that in mind we propose to automatically select features and the size of the feature set using an internal leave-one-out cross-validation procedure  discussed in the following sections.

using the same maldi tof data set as in  <cit> , researchers in  <cit>  applied the nearest shrunken centroid approach to classify the ms samples. using only seven features their method achieved a classification error rate of approximately 23%. more recently, in  <cit> , both the ga approach and the nearest shrunken centroid approach have been found inferior to the boosting based feature selection approach. further investigation, in  <cit> , confirmed the poor performance of the nearest shrunken centroid on the ovarian cancer  and the prostate cancer  data sets.

prostate cancer studies
in  <cit> , the researchers used a decision tree algorithm to differentiate between healthy individuals and those with prostate cancer. this study used the seldi tof ms to acquire the mass spectra which corresponds to our pc-imac-cu data set. in order to select relevant features, the area under the receiver operating characteristics  curves was used to identify informative peaks which were subsequently used by the decision tree classification algorithm. the researchers did not perform cross-validation, but on a single test set the classifier achieved an 81% sensitivity and a 97% specificity, yielding a balanced accuracy  of 89%.

in  <cit> , the performance was improved on the pc-imac-cu data set by the use of boosting. as is  <cit> , the area under the curve  criteria was used to identify relevant features. for subsequent feature selection and classification, the researchers used decision stumps together with adaboost and its variant, boosted decision stump feature selection  method. a key difference between the two methods is that bdsfs selects features without replacement, whereas boosted decision stumps  allows for selection of the same feature multiple times. the bds algorithm achieved perfect accuracy on the single test set for the prostate cancer data set. however, a randomized 10-fold cross-validation procedure yielded an average sensitivity of  <dig> % and an average specificity of  <dig> %, for an overall bacc of 98%. for the bdsfs, the results were considerably worse, with a sensitivity of  <dig> % and a specificity of  <dig> %. the bds algorithm used all  <dig> features selected by the auc, and required  <dig> rounds of boosting. on the other hand, the bdsfs algorithm used just  <dig> features which were easily interpretable. the researchers informally report that other classifiers had similar classification accuracies but were more difficult to interpret. although, this is the highest reported accuracy on this data set, the bds algorithm  <cit>  required over  <dig> rounds of boosting which complicates the identification of key relevant features necessary to differentiate heathy individuals from those afflicted with prostate cancer.

in  <cit> , the same pc-imac-cu data set was analyzed using several classifiers. using a filter-based anova f-statistic to rank the preselected peaks, relevant features were selected in sets of increasing size. classification was performed with k-nearest-neighbors , linear/quadratic discriminants , and suport vector machines  using 100-fold randomized cross-validation strategy. linear svm achieved the best accuracy of 91% using just eight peaks.

in  <cit> , the researchers again used pca for dimensionality reduction and lda for classification. the pc-imac-cu data set was obtained from the authors of  <cit>  and, in the same fashion as with the ovarian cancer set, the researchers conducted a detailed study using various train/test set sizes. for each train/test data split,  <dig> cross-validation runs  were conducted. when training sets were larger than 75% of the total sample size, average accuracy of 88% was achieved . using only 50% of data for training, the performance dropped to 86%. in comparison to ovarian cancer sets, the lower accuracy suggests that this data set is much more difficult to classify correctly using the pca/lda algorithm.

in  <cit> , researchers used genetic algorithms  for feature selection and self organizing maps  for classification of prostate cancer . this approach achieved a specificity of 95% and a sensitivity of 71%, for an average accuracy of 83%. although cross-validation was carried out, the results were not presented.

in  <cit> , the aforementioned studies on prostate cancer raised the following question: why do the features and classification performance vary so drastically across studies? the results indicate that different seldi-tof approaches combined with different machine learning techniques for pattern recognition produce highly variable results in terms of relevant features and classification accuracy. furthermore, such results also indicate that the ms spectra contains a large number of features relevant to the task of discriminating heathy individuals from those afflicted with cancer. an alternative explanation, found in  <cit> , seems to suggest chemical/electronic noise and/or bias introduced during the acquisition of the ms spectra. this further motivates the need for comparative studies done on a regular basis using several mass spectrometry techniques in conjunction with a number of machine learning approaches done on several data sets.

data sets
for this study five data sets were acquired. each sample in each data set is represented as a vector of real valued features forming the spectra. each feature in turn represents the quantity  of ions with a specific m/z ratio. in essence, each sample spectrum is a histogram describing the composition of the sample bio-fluid or tissue sample. each data set is named based on the type of disease tested, oc for ovarian cancer and pc for prostate cancer, as well as the type of seldi affinity chip used to produce the mass spectra. this naming scheme was adopted from  <cit> . the following data sets were used for this study:

oc-h4
this ovarian cancer set was obtained using the h <dig> protein chip from ciphergen. it contains  <dig> diseased and  <dig> healthy samples which were manually prepared. each spectra contains  <dig>  features  in this data set.

oc-wcx2a
this ovarian cancer set obtained using the wcx <dig> protein chip. it contains the same  <dig> diseased and  <dig> healthy samples as the oc-h <dig> data set which were re-precessed using the wcx <dig> protein chip. for this data set the samples were also processed by hand. each spectra contains  <dig>  features  in this data set.

oc-wcx2b
this ovarian cancer set was also obtained using the wcx <dig> protein chip. however, a robotic instrument replaced the manual chip preparation for this data set. this data set contains  <dig> healthy and  <dig> diseased samples, all different from the two previous data sets. each spectra contains  <dig>  features  in this data set.

pc-h4
the spectra were collected using the h <dig> protein chip, which was prepared by hand. there are  <dig> total samples:  <dig> samples with benign prostate hyperplasia with psa levels greater than  <dig>   <dig> samples with no evidence of disease and psa level less than  <dig>   <dig> samples with prostate cancer with psa levels  <dig> through  <dig>  and  <dig> samples with prostate cancer with psa levels greater than  <dig>  each sample is again a histogram composed of  <dig>  features. for this set we combined samples with benign prostate hyperplasia and those with no evidence of disease into the healthy class. the rest of the samples formed the diseased class.

pc-imac-cu
the spectra were collected using the imac-cu metal binding chip, and were prepared by hand. there are  <dig> total samples:  <dig> samples with prostate cancer,  <dig> with benign prostate hyperplasia and  <dig> samples with no evidence of disease. each sample is composed of  <dig>  features. for this set we also combined samples with benign prostate hyperplasia and those with no evidence of disease into the healthy class. the rest of the samples formed the diseased class.

RESULTS
this section presents the evaluated feature selection algorithms in conjunction with the base classification technique. in addition the empirical evaluation results are presented.

centroid classification method
a fast and simple algorithm for classification is the centroid method  <cit> . this algorithm assumes that the target classes correspond to individual  clusters and uses the cluster means  to determine the class of a new sample point. a prototype pattern for class cj is defined as the arithmetic mean:



where xi's are the training samples labeled as class cj. recall that the training sample is a ms spectra represented as a multi-dimensional vector . in a similar fashion, we can obtain a prototypical vector for all the other classes. during classification, the class label of an unknown sample x is determined as:



where d is a distance function or:



where s is a similarity metric. this simple classifier will form the basis of our studies. it works with any number of features and its run-time complexity is proportional to the number of features and the complexity of the distance or similarity metric used. preliminary experiments in  <cit> , were conducted to establish which similarity/distance metric is most appropriate for the centroid classification algorithm, and the l <dig> distance metric was selected. defined by:

l <dig>  = || x - μ|| <dig>     

with ||y|| <dig> =  |y|, and y being the value of the ith feature. the value l <dig> has a linear cost in the number of features. in this study, data sets contain two classes and hence the number of calls to the distance metric is also two. therefore, the centroid classifier, at run-time, is linear in the number of features. during training, two prototypes are computed and the cost of computing each prototype is o, where n is the number of features and m is the number of training samples which belong to a given class. note that m only varies between data sets and not during training or feature selection processes. thus, we can view m as a constant and conclude that the centroid classifier has ocost in the training phase.

nearest shrunken centroid
a special purpose feature selection algorithm for the nearest centroid algorithm was developed by tibshirani et al. and presented in  <cit> . the algorithm, related to the lasso method, tries to shrink the class prototypes () towards the overall mean:



briefly, the algorithm calculates:



where , s is a vector of pooled within class variances for each feature and division is done component wise. we can now view the class centroid as:



where denotes component wise multiplication. by decreasing dj we can move the class centroid towards the overall centroid. when a component of the class centroid is equal to the corresponding component of the overall mean for all classes, the feature no longer plays a part in classification and is effectively removed. hence, as dj shrinks progressively more features are removed. to decrease dj soft thresholding is used to produce  with:



where dj is the ithcomponent of the vector dj. the shrunken centroid is then computed by replacing dj with  in equation  <dig>  in our experiments we used  <dig> different values for δ, nsc, { <dig> ,  <dig>   <dig> , ..., 10}.

we also tried  <dig> different values for δ also in the range .

filter-based feature selection
filter methods attempt to select features based on simple auxiliary criteria, such as feature correlation, to remove redundant features. in order to be tractable, such approaches decouple the feature selection process from the performance component, but may ultimately select irrelevant features as a result. in general, filter-based methods are designed for a specific type of feature. since the mass spectra is composed of continuous features, we use univariate statistical tests. instead of selecting features by invoking a classifier as in wrapper-based approaches, univariate statistics simply rank individual features. the student-t test , the kolmogorov-smirnov test   <cit>  and the p-test  <cit>  algorithms are the commonly used statistics. these 'goodness-of-fit' tests compare feature values of samples belonging to class  <dig> to feature values of samples belonging to class  <dig>  the goal is to determine if the feature values for class  <dig> come from a different distribution than those for class  <dig>  the key difference between these tests are the assumptions they make. the t-test assumes that both distributions have identical variance, and makes no assumptions as to whether the two distributions are discrete or continuous. on the other hand, the ks-test assumes that the two distributions are continuous, but makes no other assumptions.

in the case of the t-test, the null hypothesis is μ <dig> = μ <dig>  indicating that the mean of feature values for class  <dig> is the same as the mean of the feature values for class  <dig>  in the case of the ks-test, the null hypothesis is cdf = cdf, meaning that feature values from both classes have an identical cumulative distribution. both tests determine if the observed differences are statistically significant and return a score representing the probability that the null hypothesis is true. thus, features can be ranked using either of these statistics according to the significance score of each feature. in addition to the t-test and ks-test, we also use a simpler feature ranking criteria called the p-test and denoted as:



where σi is the standard deviation for class i. this can be seen as a simplified version of the student-t score that ignores sample size and ranks features solely on the basis of their mean and standard deviation. both the benefits and drawbacks of these statistical tests stem from the assumption that the features are independent. on one hand, the independence assumption makes these algorithms computationally efficient. on the other hand, the independence assumption clearly may not hold for all data sets, thereby producing suboptimal feature rankings.

in  <cit> , the researchers used the t-test to rank each feature but chose to test classification algorithms with  <dig> and  <dig> top-ranked features, without any apparent justification. the apparent focus of their research is on comparing classifiers rather than the two feature extraction methods . in contrast, we show that feature ranking coupled with greedy forward selection using internal leave-one-out cross-validation  can automatically find a feature subset of an arbitrary size that improves performance with respect to using the centroid algorithm without any feature selection.

wrapper-based feature selection
wrapper methods attempt to evaluate feature relevance within the context of a given task and avoid intractability by using greedy/heuristic search methods. in other words, the number of possible subsets is greatly restricted by the greedy selection procedure, and each candidate feature subset is evaluated using the actual performance element . thus far, a variety of greedy algorithms have been proposed to select feature sets sequentially. sequential forward  selection  methods start from an empty  set of features and at each step add  a single feature which produces the greatest increase in performance. the sfs technique, as described, is easily applicable to the ms data. on the other hand, the sbs algorithm, much like a full search over all subsets, is still computationally intractable. our informal estimates revealed that a naive application of the sbs algorithm to all five data sets, used in this study, would take approximately  <dig> years to complete on the hardware platform available to us. thus, in order to make sbs tractable, we implemented several heuristics. first, rather than searching through all features within the active set, and removing a feature that produces the greatest improvement in performance, we stop at the first feature whose removal does not degrade the overall performance as determined by the internal loocv approach. now that each loop of sbs terminates at the first candidate feature, we can re-order the features based on the probability of each feature being irrelevant and/or redundant. to do so we use the ks-test to rank and re-order all features. thus, the sbs search starts by first testing a feature deemed most likely to be irrelevant by the ks-test. the second heuristic added to the sbs algorithm involves recording the stoping position of the last iteration. in the standard sbs, each iteration of the algorithm tests all features in the active set. however, since the previously added heuristic lets sbs terminate the innermost loop at the first feature deemed unnecessary, re-testing previously examined features has less utility than looking at the uninspected features. hence, rather than re-starting the search from the beginning, each iteration of the modified sbs starts the feature search from the previous stopping position. upon reaching the end of the feature index array, the search is restarted from the beginning.

boosting
in addition to sfs and the modified sbs, we also use boosting which has been shown to perform very well on the pc-imac-cu data set in  <cit> . to determine the merit of this embedded feature selection approach, we created two versions of the boosting algorithm. the first version is a standard boosting algorithm  <cit>  that uses a weighted nearest centroid method as the base learner. as in the standard nearest centroid, the first round of boosting assigns equal weights to each sample and calculates the nearest centroid for each of the two classes. each training sample is then classified and re-weighted based on the outcome of classification. if a sample is misclassified, it receives a higher weight , whereas if the sample was correctly classified its weight is decreased. the next round of boosting creates new centroids based on the adjusted sample weights and the process repeats itself until training error becomes zero or a predefined number of boosting rounds is reached. this version of the algorithm does not perform feature selection and is used to assess the performance of the second version of boosted nearest centroid algorithm.

the second version of the algorithm extends the boosting algorithm by enabling feature selection. this version, called boosted feature extraction , is similar to sequential forward selection  in that during each round of boosting the algorithm searches over all features and selects a single best feature upon which to build the weighted nearest centroid classifier. although variants of this approach have been used in  <cit>  and  <cit> , to the best of our knowledge this is the first time the boostedfe algorithm has been coupled with the  nearest centroid classifier. the finer aspects of this algorithm are presented in the discussion section of this paper.

dimensionality reduction
feature selection algorithms attempt to select relevant features with respect to the performance task, or conversely remove redundant or irrelevant ones. in contrast, the goal of dimensionality reduction techniques is to literally transform the raw input features while preserving the global information content. in essence, the dimensionality reduction algorithms attempt to extract features capable of reconstructing the original high dimensional data, irrespective to the classification label assigned to each data point. for example, principle components analysis   <cit>  attempts to find a linear combination of principal components that preserves the variance of the data. in order to test dimensionality reduction algorithms, we have procured the q <dig> code used in  <cit> , which uses pca in conjunction with linear discriminant analysis  to classify the sample mass spectra. briefly, pca projects the ms spectra onto a low dimensional linear manifold required by the lda algorithm, which cannot use more features than training instances. in turn the lda algorithm attempts to project the data onto a hyperplane which minimizes within-class scatter, while maximizing between-class distance. once the data has been projected into the lda subspace, the nearest centroid approach is used to classify new instances. in our experiments, we test both pca/lda + nearest centroid as well as pca + nearest centroid approaches. this design is meant to assess the merit of individual components, namely pca and lda.

empirical evaluation
we conducted experiments on three ovarian and two prostate data sets, previously used in  <cit> . sets oc-h <dig>  oc-wcx2a, oc-wcx2b, and pc-h <dig> contain  <dig>  features , while the last data set pc-imac-cu contains  <dig>  features.

we used a stratified three-fold cross-validation procedure, for all experiments, whereby each data set was split into three subsets of equal size. each test fold used one of the three subsets with the remaining two subsets used for training. within the training phase an internal leave-one-out cross-validation  loop was used for for all feature selection methods . in this manner, test set performance remains unbiased by the feature selection process. for pca and pca/lda algorithms, the maximal number of principal components usable by the lda algorithm was selected and is further described in  <cit> . the results presented in figure  <dig> and tables  <dig>   <dig>   <dig>   <dig>   <dig> express performance statistics averaged over the three test folds. balanced accuracy  is taken as the arithmetic mean of sensitivity and specificity and is formally defined in the list of abbreviations section along with the rest of the performance measures. the bacc measure is related to the standard ber , where ber =  <dig> - bacc is commonly used for evaluation of feature selection algorithms  <cit> .

classification accuracy
to make our results comparable with those of qu et al. in  <cit> , we reran the boosted feature extraction algorithm using ten fold cross-validation scheme on the pc-imac-cu data set and obtained bacc of  <dig> %. more specifically our algorithm attained 100% specificity and  <dig> % sensitivity. qu et al. achieved a  <dig> % sensitivity and  <dig> % specificity averaged over ten 90/10% randomized train/test splits. however, their boosted decision stumps algorithm required  <dig> rounds of boosting to achieve such a high performance level. as a result, identification of relevant features and their significance is made difficult if not impossible. to find at least some of the relevant features within the pc-imac-cu data set, in  <cit>  the researchers employed the bdsfs algorithm which found  <dig> relevant features but had a significantly lower accuracy. in contrast, our boostedfe nearest centroid algorithm only required, on average,  <dig> boosting rounds to achieve comparable classification accuracy. to be fair, we note that the the bds and bdsfs algorithms used in  <cit>  were ran on pre-processed data, whereby  <dig> peaks were extracted by the auc procedure. hence the performance of our boostedfe algorithm is only comparable in terms of classification accuracy and the number of features selected to the bds + auc preprocessing. the quality of features in terms of biological relevance cannot be assessed using this or any of the other tested datasets due to i) biologically confounding factors introduced during sample acquisition and ii) ill-defined data preprocessing steps .

the rest of the tested algorithms did not produce consistent results. some algorithms performed well on one or two of the data sets, but not on all of them as shown in table  <dig> . in contrast, boostedfe consistently produced high quality results on all the tested data sets. in addition, boostedfe produced results with the lowest variance across the cross-validation folds as shown in tables  <dig> and  <dig> by low standard deviation scores. again, the oh-h <dig> data set is the exception, where boostedfe has a high standard deviation for the bacc score. a closer look at table  <dig> shows that the boostedfe algorithm had 100%  sensitivity but only  <dig> %  specificity.

in terms of merely increasing the classification accuracy without performing feature selection, the standard boosting algorithm improved average performance by over 11% as seen in table  <dig>  analysis of the training data revealed that in most cases boosting terminated in less than  <dig> rounds, indicating that for the five datasets used in this study, very few prototypes were needed for accurate sample classification. to see this, recall that in each round of boosting two centroids are produced, one for each class but the size of the training set ranges from  <dig> samples to  <dig> samples. hence, boosting effectively abstracted the training samples into prototypes, producing about  <dig> class prototypes for each class. unfortunately, this approach is unlikely to provide insight into the underlying biological factors, provided they exist, due to its use of the full mass spectra.

surprisingly, the sequential backward selection  performed rather poorly across all relevant aspects, such as accuracy, running times and size of selected feature subsets. even more surprising was the poor classification accuracy of t-test, nsc, and pca/lda algorithms, which appear highly accurate in publications  <cit>  and  <cit> . again, the effects of pre-processing steps need to be factored in when comparing our results and those of other studies. detailed experimental results of this study are presented in tables  <dig> and  <dig> in order to show additional performance statistics such as sensitivity, specificity and positive predictive value obtained under our experimental conditions.

for the oc-h <dig> data set, it appears that the filter-based methods, sfs, and nsc improve specificity at the cost of decreased sensitivity. in contrast, sbs and boosting based methods improve both with respect to the basic nearest centroid algorithm. this trend resurfaces again for the pc-h <dig> dataset. this time all algorithms increase specificity at the cost of decreased sensitivity. it is interesting to note that both methods were created via the ciphergen h <dig> proteinchip array and both datasets had their baseline subtracted.

feature sets
computational cost comparison
discussion
while it was expected that sbs would be the most costly algorithm, and that it would produce the largest feature subsets, what is surprising is the noticeably poor overall performance as seen from table  <dig>  it appears that the additional heuristics we have added to make the algorithm tractable, had a negative impact on the performance of sbs, or that it is simply a poor choice for feature selection in the presence of so many features. on the other hand, sfs is computationally nearly an order of magnitude cheaper than sbs, produces compact feature sets, and has the second best balanced accuracy after boostedfe. from the filter-based approaches, both the ks-test and p-test outperform the t-test in terms of both classification accuracy and running times. t-test, on the other hand, consistently produces very stable features sets as seen from table  <dig>  out of the three filter approaches tested, only the t-test appears in the surveyed literature. the p-test, has been used in  <cit>  for gene selection in dna microarrays. to the best of our knowledge we are the first to use the kolmogorov-smirnov test for feature selection in proteomics.

unexpectedly, the subspace projection methods, namely pca and pca/lda do not perform well under the outlined experimental conditions. this is clearly in contradiction to the results presented in  <cit> . in fact, table  <dig> shows that the nearest centroid classifier without feature selection outperforms pca on all but one data set. intuitively, the poor performance of pca, causes the pca/lda combination to also perform rather poorly on three of the five data sets. we should note that the randomized re-sampling testing strategy as used in  <cit>  and  <cit>  along with a number of other papers has been shown to be overly optimistic due to the correlations between test and train sets . hence, we believe that this testing methodology has a significant impact on performance. on the other hand, stratified cross-validation approaches, such as the one we have adopted in this paper, remove correlations between test sets, giving more accurate performance estimates. as a consequence, all performance statistics appear 'deflated' in comparison to results reported in previous studies. however, we believe that these, 3-fold cross-validation results, provide more realistic performance estimates and can be used to make statistically sound inferences.

nearest centroid, sfs, and boosting
the choice of nearest centroid classifier to study feature selection was not an arbitrary one. although the nearest centroid is one of the simplest classifiers found in the literature, nevertheless it is capable of classifying raw mass spectra without any feature selection. in addition, it is extremely fast and therefore allows the use of costly wrapper methods, such as sfs, sbs, and boostedfe, which may otherwise be intractable. hence, not only does the nearest centroid classifier able to provide a base-line for evaluation of feature selection algorithms, it also allows us to test a number of algorithms previously inapplicable in the domain of proteomic mass spectrometry. for the two class problems considered, the nearest centroid algorithm is linear and implicitly encodes a thresholding hyperplane separating the two classes. however, when combined with boosting the algorithm becomes capable of encoding non-linear boundaries. as mentioned previously, the use of boosting effectively abstracts the training samples into prototypes. integration of sequential forward selection  yields a further improvement. by merging weighted nearest centroid with boosting and sfs, the new algorithm is able to simultaneously select relevant features and learn a highly accurate classifier. thus boostedfe, fulfills both rolls as a feature selection and classification algorithm. by testing the nearest centroid without feature selection, sfs, boosting, and boostedfe, we can easily gauge the effect each component has on the performance of boostedfe. in fact this piece-wise analysis can easily explain why boosting outperformed boosting fe on the oc-h <dig> data set.

from table  <dig>  we can see that sfs performed worse than nofe , hence when boosting and sfs were used together the net effect actually lowered performance in comparison to boosting without feature selection. more specifically, we can see from table  <dig> that the specificity of sfs for the oc-h <dig> data set was extremely low  and was accompanied with a very high standard deviation of .

feature analysis
the aim of this paper was to profile a number of feature selection algorithms coupled with the nearest centroid classifier. our goal was to examine performance in terms of computational time, feature set sizes and, most importantly, classification accuracy. however, due to the concerns raised in  <cit>  regarding the quality of ovarian and prostate cancer data, we make no attempt to interpret the results of feature selection from a biological standpoint. furthermore, data preprocessing strategies, themselves being actively studied  <cit> , should also be examined in future investigations due to their influence on feature selection and classification results. in order to truly assess biological underpinnings of discriminative m/z values, it is imperative that datasets free from flaws, which confound biology with instrument noise, collection bias, and/or other "artifacts of sample effects"  <cit> , are used in further studies. in addition, the effectiveness of preprocessing methods can only be assessed with respect their ability to improve identification of relevant biological factors governing class discrimination.

CONCLUSIONS
mass spectrometry based disease diagnosis is an emerging field, with the potential to revolutionize early medical diagnosis. however, due to the vast amount of information captured by the high-resolution mass spectrometry techniques, the supervised training of classifiers is problematic. specifically, the many thousands of raw attributes forming the mass spectra frequently contain a large amount of redundancy, information irrelevant to a particular disease, and measurement noise. therefore, aggressive feature selection techniques are crucial for learning high-accuracy classifiers and realizing the full potential of mass spectrometry based disease diagnosis. this paper analyzed dimensionality reduction, filter, wrapper, and boosting based approaches to feature selection and compared the results to previously published state-of-the-art performance. in addition, a novel combination of nearest centroid classifier coupled with boosting based feature selection  was presented and evaluated. experimental results indicate that sequential forward selection, p-test, and ks-test perform reasonably well across the proteomic data sets we acquired. however, the aforementioned algorithms lack consistency. on the other hand, the proposed boostedfe algorithm greatly reduces the dimensionality of the data and significantly improves classification accuracy. in contrast to all other algorithms, its performance is much more consistent across all five data sets used in the experiments.

future research will investigate the extent to which the features selected by the boostedfe approach can be used in conjunction with more sophisticated classifiers, such as artificial neural networks and support vector machines. in addition, future studies should investigate whether the boostedfe + nearest centroid combination can serve as a meta-wrapper for more sophisticated classification algorithms. from a biological perspective, the significance of the selected features and their value in identifying potential biomarkers should be investigated. a prerequisite for this task is the production of datasets where biological factors are not confounded by instrumentation noise, sample acquisition bias and/or other experimental design flaws. the production of these datasets would also enable future studies to accurately assess the effectiveness of preprocessing techniques, critical for producing diagnostic tools which indeed base classification on underlying biological factors encoded within the mass spectra.

list of abbreviations
in this section we define the various measures used. respectively, tp, tn, fp, fn, stand for the number of true positive, true negative, false positive, false negative samples at classification time.

sensitivity is defined as  and is also known as recall.

specificity is defined as .

ppv  is defined as  and is also known as precision.

npv  is defined as .

bacc  is defined as  this measure defines the average of sensitivity and specificity.

% correct is defined as  and measures the overall percentage of samples correctly classified.

