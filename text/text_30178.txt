BACKGROUND
the advent of a new generation of high-throughput dna sequencing technologies, known as deep sequencing or next-generation sequencing , has opened up new experimental approaches in basic, applied and clinical research. among the applications that benefit from the enormous volume of data produced by ngs is the study of genetic diversity in heterogeneous samples. genetic diversity has significant consequences, for example, in hiv infection. the set of diverse variants of the virus is responsible for disease progression and hampers efforts to develop effective therapies  <cit> . other examples of biological systems where genetic diversity is equally important include bacterial communities  <cit>  and cancer cells  <cit> .

traditional sanger sequencing of a genetically diverse sample results in the consensus sequence of the population, in which low- frequency variants are not detected . this limitation is overcome in deep sequencing, which can be used to accurately quantify genetic diversity in a mixed sample, provided that sequencing errors are properly treated  <cit> . in many genetic analyses only a portion of the genome, e.g., a single gene, is isolated and pcr amplified. the sample is then sequenced on a ngs platform which performs hundreds of thousands of clonal sequencing reactions and produces a set of reads that is statistically representative of the diversity in the sample. the reads can be aligned to the reference sequence of the amplified region. in order to obtain a reliable estimate of the genetic variation, the confounding effects of sequencing errors must be taken into account  <cit> .

in order to correct sequencing errors and to estimate the population structure of a heterogeneous sample we developed shorah . this software corrects sequencing errors by clustering all reads that overlap the same region of the genome of length approximately equal to the read length . the consensus sequence of each cluster represents the original haplotype from which the erroneous reads were obtained. the number of reads associated with the cluster estimates the prevalence of the haplotype in the population. for many applications, this local diversity estimate is sufficient to draw important conclusions. for example, with current pyrosequencing reads of length about  <dig> bp, the protease gene of hiv, an important target of antiretroviral therapy, can be covered completely. longer haplotypes sequences can be reconstructed from adjacent, overlapping reads, and the frequencies of these global haplotypes can then be estimated.

shorah is a set of computational tools for inferring the local and global population structure from a set of reads obtained from a mixed sample in a single deep sequencing run.

we report here the main feature of the software, by giving an overview of its implementation, usage examples and a brief outline of the obtained results. we conclude by discussing the relevance of the software and possible future improvements.

implementation
the shorah package is a collection of tools to estimate genetic variations from deep sequencing data by correcting sequencing errors, assembling reads and estimating their frequencies. it is implemented in c++, python, and perl and the source code is distributed under the gpl licence. online documentation with installation and running instruction is available at https://wiki-bsse.ethz.ch/display/shorah. users can choose to run a local  or global analysis , depending on the question of interest and the available data. shorah has been tested on reads generated by the 454/roche flx and the illumina genome analyzer sequencing platforms.

full analysis with shorah includes four major steps that are performed by calling the wrapper shorah.py. these are: 1) alignment; 2) error correction ; 3) global haplotype reconstruction and 4) frequency estimation. users can also choose to run only part of the analysis, for example stopping after the local reconstruction  <cit> . the input can be either a fasta file with ngs reads plus a reference sequence, or a multiple alignment of the reads in fasta format, if users wish to use their own alignment. the program s2f.py performs a pairwise alignment of all reads to the reference sequence and then builds a multiple sequence alignment  by padding every read with gaps found in the other pairwise alignments. more sophisticated approaches can be used in this step  <cit> , but this method performs well in terms of quality and speed.

local analysis
the program dec.py considers a set of overlapping windows on the msa and passes each to the program diri_sampler. each run of diri_sampler estimates the diversity at the local level of the msa window, the width of which should be approximately the average read length. this program employs a model-based probabilistic clustering algorithm to correct errors, infer haplotypes and their frequencies, and estimate, in a bayesian fashion, the quality of the reconstruction by computing the full joint posterior probability distribution of all parameters of interest  <cit> . with this approach, neither independent estimates of the error rate nor of the number of haplotypes are needed in advance. rather, they are estimated from the data. two parameters can influence the outcome of this stochastic algorithm: the number of iterations and the hyper parameter Î±. although there is no general rule to choose them, they have to be large enough in order to guarantee a proper mixing in the sampling. in particular, one has to avoid situations in which solutions with new clusters are never proposed . nevertheless, the results hardly change over an interval of of several orders of magnitude. the online documentation contains further advice on how to tune the parameters.

in the default setting, dec.py constructs overlapping windows such that each position of the alignment is covered three times. the original reads are corrected following a majority vote based on these local reconstructions.

global analysis
the set of corrected reads is passed to the program contain in order to identify the set of unique reads of maximum length with respect to the substring relation. they are passed on to the program mm.py which implements the global reconstruction method. it employs a parsimony principle and computes a minimal set of haplotypes that explains all reads in the dataset  <cit> .

finally, the program freqest implements an expectation maximization  algorithm to estimate the frequencies of the reconstructed haplotypes by maximum likelihood  <cit> . the algorithm stops when the difference in the log-likelihood between two successive iterations is below a given threshold  or when the number of iterations exceeds a maximum . both parameters can be tuned .

short reads
platforms like illumina genome analyzer produce datasets composed of more reads that are shorter than those produced with the  <dig> platform . for such very large datasets it is more practical to produce an alignment in sam format , use the tool bam2msa.py to extract a msa for a single window and simply run diri_sampler  on this single file. global haplotype reconstruction is increasingly difficult for long genomic regions and short reads, because of the difficulty and ambiguity of phasing variation at different sites. for a given read length, the solution of the global reconstruction becomes less stable for longer genomic regions, an effect that users should take into account in designing their experiments.

computational efficiency
the alignment step consists of pairwise alignments of all reads to the reference sequence. its time complexity is proportional to the number of reads, the read length, and the length of the reference sequence.

the error correction step is run independently on each sequence window and, unlike the other components, is efficiently parallelized. the program diri_sampler implements a gibbs sampler with time complexity per iteration proportional to the number of unique reads in the msa window and to the number of clusters , which depends on the local diversity of the sample. the implementation of the gibbs sampler is optimized by exploiting the fact that the number of unique reads is usually much smaller than the total number of reads. moreover, the hamming distances between sequences, which make up the sufficient statistics of the model, are computed in time independent of the sequence length and proportional to the distance  <cit> . this approach allows for analysing windows with up to  <dig> reads with a ram usage on the order of gigabytes. the memory requirements depend mainly on the number of unique reads, or, in other words, on the expected diversity, the error rate and the width of the window. the global reconstruction program mm.py solves an instance of the maximum weight matching problem of time complexity at worst cubic in the total number of unique reads. frequency estimation with the program freqest is based on an em algorithm. its time complexity per iteration is the number of reads times the number of reconstructed haplotypes.

in practice, thanks to its parallel implementation, the error correction step was completed in less than  <dig> hours using  <dig> cpus at  <dig>  ghz for a large sample consisting of  <dig>  pyrosequencing reads . freqest completed the subsequent analysis in two days. the impact of mm.py and the wrapper shorah.py on total running time is negligible.

RESULTS
local analysis
the local reconstruction performance of shorah has been assessed using both simulated and experimental data. a heterogeneous sample was prepared by simulating reads from  <dig> different variants of the hiv- <dig> subtype b pol gene and mixing them at various proportions  <cit> . in general, the performance depends on the error rate of the sequencing process and on the frequencies and pairwise distances of the haplotypes in the sample. haplotypes with frequencies as low as  <dig> % were detected at a coverage of  <dig> and their frequency correctly estimated  <cit> .

a mixture with the same haplotypes used for the in silico experiment was prepared in vitro and the reads obtained with the roche/ <dig> titanium platform were analysed. similar results were obtained, inferring individual clones with frequencies as low as  <dig> % with perfect sequence identity and good estimate of the frequency at a coverage of  <dig>  a five-fold decrease of the error rate, from  <dig> % to  <dig> %, was achieved  <cit> .

a crucial advantage of clustering reads is the possibility to reliably detect co-occurrence of mutations when they are close enough to be captured by the same read. this information cannot be obtained with sanger sequencing, and most software for ngs data only allows the detection of snp at individual sites. shorah was successfully used to reconstruct local haplotypes of the hiv- <dig> subtype b protease  gene, an important target of anti-retroviral therapy, which can be entirely covered by a single  <dig> read  <cit> .

global analysis
global reconstruction and frequency estimation was evaluated on simulated error-free reads and real data as well  <cit> . tests with error-free reads show that the performance of the global reconstruction depends on the number of present haplotypes, their diversity and the error rate of the machine, while being less affected by the number of reads.

short read data
error correction and local haplotype inference was validated also on simulated and real illumina data  <cit> . haplotypes with frequency as low as  <dig> % were detected and their frequency correctly estimated at a coverage of  <dig> 

genetic diversity is also important in cancer and different variants in tumours samples can be detected by sequencing at very high coverage. therefore, we used shorah also in this setting. pcr amplicons were designed to cover regions of the human genome known to have a role in the development of rhenal cancer. samples from tumour dissection were sequenced on these regions at high coverage on a illumina ga . shorah identified the co-occurrence of two snps at neighbouring sites of the vhl gene in a fraction of the sampled cells. the short distance between these two snps  allowed  <dig> bp long illumina reads to detect them simultaneously.

related work
quantification of genetic diversity is one of the many possible applications of ngs technologies. for all applications, solid statistical models and efficient computational tools are prerequisites to fully exploit the produced data. different models to assess genetic diversity have been presented in the last few years  <cit> , but software implementing them is not yet available. there is a program for a related task, namely the assembly of reads from metagenomics studies  <cit> . while shorah addresses the problem of identifying the presence of close haplotypes in a relatively short region of the genome by exploiting very deep coverage, in metagenomics studies one typically faces the problem of identifying very diverse genomes  that have been mixed together.

future work
currently, the steps to reconstruct the population structure consist in 1) alignment, 2) error correction, 3) global haplotype reconstruction and 4) frequency estimation. tools based on different models are used in these steps and this separation clearly has some limitations. for example, while the error correction is performed in a full probabilistic framework, only the most likely solution is used in the following step. performing at least some of the steps in the same probabilistic model would likely be an advantage. current efforts are aimed at developing a common framework for error correction and global haplotype reconstruction. further, the software does not make full use of information from paired ends. another possible improvement of the method might exploit the co-occurrence of variants on the same pair to reconstruct global haplotypes more reliably  <cit> . currently, users can run shorah on a set of reads obtained with paired end protocol, if they want to exploit the higher coverage, but only as a  set of single reads.

CONCLUSIONS
with the availability of ngs techniques, studying genetic diversity becomes much quicker and cost-effective as compared to other techniques such as clonal sanger sequencing or allele specific pcr. also, additional information not available from these techniques can obtained, provided that the data is analysed properly. shorah reliably reconstructs the local  haplotype structure of a population by correcting sequencing errors with a bayesian inference algorithm. this approach provides the user also with an estimate of the quality of the reconstruction. further, shorah can reconstruct the global haplotypes and estimate their frequencies.

shorah has been applied to  <dig> data from hiv infected patients  <cit>  and illumina data from tumour dissections . we foresee its application in the context of viral infections, as well as in bacterial communities and in tumor samples, where the genetic heterogeneity of the sample must be assessed.

availability and requirements
shorah  is distributed under gnu gpl licence as c++, python  and perl source code and is available from its home page http://www.cbg.ethz.ch/software/shorah. it has to be compiled and run from command line, instructions to install/execute can be found at the online documentation page https://wiki-bsse.ethz.ch/display/shorah/documentation. it has been tested on linux and mac os x platforms. other requirements are biopython, gsl . the program needle from emboss is required for the alignment only. working with sam alignment requires other standard tools not included in the package  but available from the web.

authors' contributions
oz developed the error correction model, wrote the local analysis software, published the package and drafted the manuscript. ab developed several optimisation procedures and implemented them. ne and nb developed and implemented the first version of the error correction, the current versions of global haplotype reconstruction and frequency estimation. nb coordinated the project. all authors read and approved the final manuscript.

funding
this work has been supported by the swiss national science foundation under grant no. cr32i2_ <dig> 

