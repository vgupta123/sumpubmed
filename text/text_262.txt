BACKGROUND
protein function, regulation, and interactions can be learned from their structure  <cit> , which motivates development of novel methods for the prediction of the protein structure. these predictions concern various levels and aspects of the protein structure including the tertiary structure  <cit> , solvent accessibility, depth, flexibility and packing of residues  <cit> , and secondary structure  <cit> . in contrast to the tertiary structure that describes position of each of the protein's atoms, the secondary structure simplifies the protein structure to a set of spatially local folding patterns that include α-helices, β-strands and coils. the spatial distribution of these local patterns determines the overall, three-dimensional shape of proteins in which individual secondary structures interact with each other creating more complex structures such as parallel or antiparallel β-sheets, β-barrels, and others. in spite that final product is complex, protein structures can be categorized into a few structural classes depending on the amount, types and spatial distribution of the secondary structures found in their fold.

knowledge of the structural class is shown to stimulate the development of methods for identification of other structural and functional characteristics of proteins  <cit> . examples include prediction of protein unfolding rates  <cit> , characterization and prediction of folding rates  <cit> , quantification of the relation between chain lengths and folding rates of two-state proteins  <cit> , prediction of dna-binding sites  <cit> , discrimination of outer membrane proteins  <cit> , fold prediction  <cit> , secondary structure and secondary structure content prediction  <cit> , reduction of the conformation search space  <cit>  and implementation of a heuristic approach to find tertiary structure  <cit> , to name just a few. at the same time, the structural classes are known for a relatively small number of proteins. the most recent release  <dig>  of scop database  <cit>  includes  <dig>  protein domains with the annotated classes, while release  <dig> of the ncbi's refseq database  <cit>  includes  <dig> , <dig> non-redundant protein sequences. the main reason for this wide gap is unavailability of protein structure, which is used to assign the structural class, for the significant majority of the known protein sequences. to this end, an accurate and automated method for classification of sequences into the corresponding structural classes would provide assistance when the structural class in unknown for a given chain.

template-based modeling, which is successfully used to predict the tertiary structure, is based on an assumption that similar sequences  share similar structures  <cit> . prediction methods that rely on the sequence alignment  <cit>  usually perform relatively poorly when sequences with high identity are not available. more specifically, over 95% of protein chains characterized by low, 20-25%, pairwise identity, which is referred to as the twilight-zone similarity, have different structures  <cit> , which substantially reduces accuracy of the corresponding predictions. we observe that about 40% of sequences for which the tertiary structure was deposited to protein data bank   <cit>  in  <dig> share twilight-zone pairwise similarity with any sequence deposited in the pdb before  <dig>  <cit> , which motivates development of the prediction methods for these challenging chains. further motivation comes from the fact that finding similar folding patterns among the proteins characterized by low sequence identity is beneficial for the reconstruction of the tertiary structure  <cit> . researchers have observed that pairs of sequences with low identity may share similar folding patterns or overall structure  <cit>  and they can be used to predict tertiary structure  <cit> . the accurate alignment of the distant homologues  is still a challenging problem in spite of many years of research in this area  <cit> . we note that structurally similar proteins that share low sequence identity can be found based on coarse grained classifications such as the structural classes that are addressed in this work. we believe that the proposed method could find applications in the detection of remote homologues.

protein structural class
two databases which classify protein structures include scop   <cit>  and cath   <cit> . the former database relies on a manual process to classify the structures while the latter applies a combination of automated and manual procedures. the first level of the classification hierarchy in both databases is the structural class. the scop distinguishes seven classes where the four major classes, which cover almost 90% of all scop entries, are all-α, all-β, α+β and α/β. the two former classes include structures dominated by α-helices and β-strands, respectively. the two latter classes correspond to structures that include both helices and strands where in the case of the α+β class these secondary structures are segregated, whereas for α/β class the structures are interspersed. the three remaining classes include multi-domain proteins, membrane and cell surface proteins and peptides, and small proteins. the multi-domain proteins consist of several domains where each domain may belong to a different class while the small proteins have short sequences and their secondary structures do not fit the definition of the other classes. we note that in spite of the fact that membrane proteins are relatively common their coverage in scop database is relatively low as it is difficult to obtain their structure  <cit> . the scop also includes four supplementary categories, i.e., coiled coil, designed, and low resolution, proteins; and peptides, but they have limited practical implications. figure  <dig> shows representative structures for the seven classes in the scop database.

cath database defines only four classes that include mainly α, mainly β, mixed α-β, and proteins with few secondary structures. in this work we address the scop based classification as it further subdivides the mixed proteins, defines several important additional classes such as membrane and multi domain proteins, and since most of the existing structural class prediction methods are also based on this definition of the structural classes. moreover, the structural classes defined in catch are relatively easy to predict based on the secondary structure content of a protein, which in turn could be predicted using existing content prediction methods  <cit> . this is in contrast to the classification in the scop database where more complex information, such as relative amount and spatial position of the secondary structures, is used to assign classes  <cit> .

related work
the manual assignment of structural classes performed in scop is based on spatial arrangement of secondary structure segments which is inspected using the tertiary structure. we aim at building an automated method which makes the class predictions based solely on the protein sequence. prediction is typically performed in two steps: 1) the variable-length sequences are converted into a fixed-length feature vectors; 2) the feature vectors are inputted into a classification algorithm to generate the class prediction.

due to a relatively large existing body of research in this area the following review concentrates on recent methods. the reader is referred to a review by chou  <cit>  that provides further details on older methods and that motivates the development of the structural class prediction methods.

majority of the developed methods use relatively simple features such as composition vector, pseudo amino acid  composition  <cit> , composition of short polypeptides, sequence itself and other features obtained from aa sequence  <cit> . several recent methods use more advanced feature vectors  <cit>  which are based on the aa sequence and/or pssm profile computed using psi blast  <cit> . a recently explored alternative is to construct features based on the predicted secondary structure. this approach was used in scpred algorithm  <cit> , which up to date provides favorable prediction quality on datasets characterized by the twilight-zone similarity.

a wide range of classification algorithms was used to perform the predictions. they include component coupling  <cit> , neural network  <cit> , bayesian classifier  <cit> , logistic regression  <cit> , decision tree  <cit> , covariant or linear discriminant algorithm  <cit> , principal component analysis  <cit> , nearest neighbor  <cit> , rough sets  <cit>  and support vector machine   <cit> . recent works also explored more complex classification models such as ensembles  <cit> , bagging  <cit> , and boosting  <cit> . overall, we observe that svm is the most popular and the best-performing classifier for this task  <cit> .

the prediction quality of these methods varies widely depending on the datasets  <cit> . the methods which were tested on datasets with relatively high sequence identity report accuracies of close to or over 90%  <cit> . the tests on the dataset characterized by the low, twilight-zone identity show accuracies between  <dig> and 70%  <cit>  with only one approach, namely scpred, that obtains accuracies 80%  <cit> . we concentrate on the latter problems as they are more challenging and have implications in the context of the remote homology detection.

the above methods considered only the four major classes from the scop database, which was motivated by a relatively small number of proteins in the remaining classes. at the same time, recent years observed a substantial increase in the size of the scop database which doubled in size between  <dig> and  <dig>  and which currently includes over  <dig>  protein domains. even when considering a small subset of the protein domains in scop which is characterized by the twilight-zone similarity, we note that the current scop includes sufficient number of proteins for the smaller three classes to allow for the development of a prediction system.

there are only two methods that addressed prediction of the seven classes  <cit> . the first method predicts the four main classes and multiple domain, small protein, and peptide classes  <cit> . this differs from prediction targets of modas which additionally considers membrane and cell surface proteins as a part of the peptide class. this method is shown to achieve accuracy of over 90% for a low-identity dataset by using a large library of reference functional sequence motifs from the interpro database  <cit> . this resulted in the feature vector with  <dig>  features where each feature denotes occurrence of a given motif in the input sequence. although this method is characterized by good prediction quality, we note that it does not provide a web server, is difficult to implement due to the excessive number of used features, and was not redesigned in spite of the updates in the interpro database . we also note that the usage of such a large number of features results in an ill-defined problem in which the number of classification instances  is smaller than the number of features. the second, more recent method  <cit>  uses a complex representation of the protein sequence that includes pseudo aa composition, evolutionary conservation information, and physicochemical properties of aas, and the svm classifier to perform predictions. it achieves accuracy of  <dig> % for a dataset with the twilight-zone identity. we perform an empirical side-by-side comparison with this method.

although structural class predictors usually do not consider membrane and multi domain classes, such predictions could be addressed using methods designed specifically for these classes. we refer the reader to recent review articles concerning methods that are available for the prediction of membrane proteins  <cit>  and for the domain prediction  <cit> . these developments are motivated by the availability of specialized databases for the membrane  <cit>  and multi-domain proteins  <cit> . the abovementioned methods could discriminate chains in the corresponding class from all other chains, and they could be used to either pre-filter the chains or post-process results of the proposed modas method. more specifically, once a given chain is known to be a membrane protein, specialized predictors could be used to further categorize its membrane proteins type  <cit> . similarly, the predicted multi-domain proteins could be processed by the available methods to predict the domain boundaries  <cit> .

motivation and goals
all but two existing structural class predictors consider only the four major classes, while the remaining three classes are also important and their prediction should be addressed. for instance, while approximately  <dig> to 35% of the proteins encoded by an organism's genome are membrane proteins  <cit> , they are not covered in the four major classes. as mentioned above, the main reason for their under-representation in the scop database is that they are difficult to crystallize and as a result only a small number of membrane protein structures are known  <cit> . we also note that the current methods are relatively weak in the context of the sequence representation. most of the methods compute the representation directly from the sequence, only a handful of them use sequence-derived information such as multiple alignment  <cit>  and predicted secondary structure  <cit> , and there were no attempts to combine residue conservation computed from the alignment and the secondary structure. at the same time, the usage of the predicted secondary structure results in improved prediction quality for the low identity datasets  <cit> , and numerous prior studies have demonstrated that evolutionary information generated with psi blast  <cit>  is more informative than the sequence itself  <cit> . moreover, most of the existing predictors achieve good quality for datasets with high sequence similarity, while results on the datasets with the twilight-zone pairwise similarity are generally characterized by a relatively low, <70%, accuracy . at the same time, a solution that accommodates for the low sequence identity could have important applications for the tertiary structure prediction  <cit> . finally, the existing methods are fixed to a given set of classes, while a modular design would allow the user to choose how many and which classes should be considered for the prediction. the latter is a particularly attractive feature for a method that would address all  <dig> classes, i.e., the user could choose which subset of classes, including the four major classes, should be considered for a given prediction. we also observe that current methods use the same feature-based sequence representation for prediction of all classes. in the modular design a separate predictor is created for each class and the results of these predictors are combined together. this allows for the design of a specialized sequence representation for each class.

our goal is to develop a novel, modular method that predicts the seven structural classes from the protein sequences. the proposed modular approach to structural class prediction  exploits sequence and sequence-derived information to generate input for the classifier. more specifically, modas is the first to combine both the multiple sequence alignment profiles and the predicted secondary structure to generate features that are fed into a set of seven svm classifiers. our design concentrates on datasets that include sequences characterized by low, twilight-zone similarity and we aim at providing prediction quality that is competitive or better than the quality offered by the existing methods.

methods
datasets
we use total of  <dig> datasets to design and test the proposed method. we utilized version  <dig>  of the astral database  <cit> , which is a subset of the sequences from the scop database characterized by a certain similarity threshold, to derive two datasets. we selected the astral database with < 20% sequence similarity that includes  <dig> sequences where  <dig> of them belong to the all-α class,  <dig> to all-β,  <dig> to α+β  <dig> to α/β,  <dig> to multi-domain proteins,  <dig> to membrane and cell surface proteins and peptides, and  <dig> to small proteins class. we randomly divided this set into two equal size subsets, one that was used as the training set  and the second that was used as the test set . the astraltraining set was used to design the proposed method, which includes features and classifier selection and parameterization of the classifiers. the astraltest set was used to perform an independent  validation of the proposed method. both of these datasets are available at http://biomine.ece.ualberta.ca/modas/.

we also selected  <dig> widely used low sequence identity benchmark datasets to provide a comprehensive and unbiased comparison with the existing prediction methods. the d <dig> dataset includes  <dig> sequences extracted using astral version  <dig>  using 20% identity threshold which was used to test the most recent method for prediction of the  <dig> classes  <cit> . we use this dataset to perform a side-by-side comparison with the method by chen and colleagues  <cit> . the remaining  <dig> datasets are used to compare against methods that address prediction of the four major structural classes. the 25pdb dataset, which includes  <dig> sequences which share twilight-zone pairwise similarity, was taken from  <cit>  and two datasets d <dig> and d <dig> were taken from  <cit>  and  <cit>  and include  <dig> sequences with up to 40% pairwise identity and  <dig> sequences with up to 30% pairwise identity, respectively. the latter three datasets are the most commonly used benchmark sets that include low identity sequences and they allow for a side-by-side comparison with a wide selection of recent methods for the prediction of the four major structural classes.

finally, we include one larger benchmark dataset, namely d <dig>  which have been proposed in  <cit>  and which includes a set of sequences that were not filtered with respect to their similarity. we include this dataset to demonstrate the quality of the proposed method when compared with a wider range of predictors which were tested on datasets with unspecified sequence identity. we explore the distribution of the sequence identity in this dataset to compare it with the other  <dig> datasets. for each chain we compute maximal sequence identity with all remaining sequences in the dataset. we chose the maximal values since the empirical tests are based on the jackknife strategy in which all but one sequence are used to predict the class for the remaining chain. we generate pairwise sequence alignments using smith-waterman algorithm  <cit>  with gotoh's improvement  <cit>  and for each sequence we report the highest obtained score. the number of matching residues in the alignment is divided by the length of the query sequence including the gaps/insertions; a result of 100% sequence identity means that there were no gaps/insertions and that a query sequence was a substring of one of the sequences in the dataset. figure  <dig> shows the distribution of sequences in the d <dig> dataset based on the sequence identity. almost 70% of sequences from this dataset have 100% sequence identity and around 89% have identity of above 90%. this means that using the jackknife test, 89% of the tested sequences have are at least one very similar sequence in the training part of the dataset. this explains higher predictive performance on this dataset when compared with results on the remaining datasets with controlled, low sequence identity .

overall design
the input protein sequence is first processed by psi blast to obtain the position specific scoring matrix  and by psi pred  <cit>  to predict secondary structure. we selected psi pred due to its successful application in the scpred method  <cit>  and since this predictor enjoys a widespread use in prediction of a variety of related structural properties of proteins including template-based tertiary structure prediction  <cit> , and prediction of beta-turns  <cit> , residue depth  <cit> , protein fold  <cit> , and contact orders  <cit> , to name just a few. next, the sequence, the pssm and the predicted secondary structure are converted into a set of features that are fed into seven classifiers , where each classifier corresponds to one of the seven scop classes. we performed feature selection to find a suitable set of features for each structural class. we also considered several different classifier types and selected the one that provides the best prediction quality for a given class. the seven classifiers generate a probability of classification into the corresponding class and these probabilities are aggregated to predict the final outcome. the aggregation is based on a simple max operator, i.e., we predict the class that corresponds to the highest probability. although more complex aggregations could be conceived, this approach is motivated by the necessity to assure modularity of the predictor, i.e., the aggregation should work for every subset of the considered seven classes. the overall design of the proposed modas method is shown in figure  <dig> 

the design of the proposed method concerns development and selection of the features which best describe each of the classes and a classifier which provides the best predictive performance. the feature and classifier selection is based on 10-fold cross validation on the astraltraining dataset to assure that the design is independent of the other datasets and, at the same time, that it generalizes into the other datasets. the methods were written in java language and we utilized the weka workbench  <cit>  in this research.

feature vector
the three sources of data used to generate the features include the protein sequence, the pssm matrix generated with psi blast and the secondary structure predicted from the sequence using psi pred. the pssm matrices were built using the nr  dataset  <cit> , as of october  <dig>  the quality of the matrix, and consequently the quality of the proposed method, depends on the size of the dataset used. prior results demonstrate that larger number of diverse sequences in the database leads to more accurate evolutionary information, which in turn was shown to improve secondary structure predictions  <cit> . this suggests that subsequent retraining of the modas method at a later time using updated, larger nr database may potentially lead to better predictive performance. besides features that were based on counting individual aas, the aas were grouped according to their physicochemical properties including polarity , hydrophobicity, structure-preserving mutations , and their ability to be electron donors or acceptors, see table  <dig>  we also used these groupings in connection with the predicted secondary structure, i.e., amino acids were grouped based on their secondary structure and a given property. finally, we considered combining information coming from the predicted secondary structure with the multiple alignments.

the features are divided into five sets: 1) features generated directly from the sequence; 2) features computed from the pssm matrix; 3) features generated by combining information from pssm and the predicted secondary structure; 4) features obtained from the predicted secondary structure, which are based on the features utilized in the scpred method  <cit> ; and 5) novel features based on the predicted secondary structure which describe collocation of helical and strand segments.

features based on the aa sequence 
these features describe basic characteristics of the input sequence, such as length, aas composition and composition of property groups. they include:

- seqlen - the length of a sequence. 

- comp_aai = , the number of aai in the sequence  normalized by the sequence length where i =  <dig>   <dig> ...,  <dig> and aai stands for ith aa type. 

- comp_gr_grjk = , the number of aas in the sequence belonging to grjk where j ∈ {r group, electronic group, hydrophobicity group, exchange group} and k is a particular subgroup , see table  <dig>  normalized by the sequence length. 

features based on the pssm matrix 
the psi blast provides two position specific scoring matrices; one contains conservation scores of a given aa at a given position in a sequence, denoted as pssmconslm, and the other provides probability of occurrence of a given aa at given position in the sequence, denoted as pssmproblm where l =  <dig>   <dig> ..seqlen denotes the position in the sequence and m =  <dig>   <dig> ...,  <dig> denotes one of the substitution positions that correspond to the twenty aas . we normalized the conservation scores  using max-min normalization where min and max equal - <dig> and  <dig>  respectively. the pssmproblm values are already normalized by the psi blast. the matrix values were aggregated either horizontally  or vertically  to obtain a fixed length feature vector. this feature set, which quantifies evolutionary information of individual aa types and grouping of aas according to the property groups, includes the following features:

- ach_cs_{aai} = , sum of all normalized pssmconslm values , where l includes only positions of aai  and m = aai , divided by the sequence length. 

- max_cs_{aai} = , sum of maximal, over m, pssmconslm values, where l includes only positions of aai, divided by the sequence length. 

- max-ach_cs_{aai} = , sum of differences between maximal pssmconslm  and pssmconsli values where l includes only positions of aai, and i = aai , divided by the sequence length. 

- ach_prob_{aai} = , sum of all normalized pssmproblm values , where l includes only positions of aai and m = aai, divided by the sequence length. 

- max_prob_{aai} = , sum of maximal, over m, pssmproblm values, where l includes only positions of aai, divided by the sequence length. 

- max-ach_prob_{aai} = , sum of differences between maximal pssmproblm  and pssmprobli values where l includes only positions of aai, and i = aai , divided by the sequence length. 

- csseq_{aai} = , sum of normalized pssmconslm values where l =  <dig>   <dig> ..seqlen and m = aai, divided by the sequence length . 

- csseq_gr_{grjk} = , sum of normalized pssmconslm values where l =  <dig>   <dig> ..seqlen and m = grjk  divided by the sequence length. 

- ent_{aai} = , entropy of pssmproblm values, for l =  <dig>   <dig> ..seqlen and m = aai. 

- avg_prob_gr_{ grjk} = , average pssmproblm values where l =  <dig>   <dig> ..seqlen and m = grjk  divided by the sequence length. 

the ach_cs_{aai}, max_cs_{aai}, max-ach_cs_{aai}, ach_prob_{aai}, max_prob_{aai}, and max-ach_prob_{aai} features aggregate information along the sequence by the aa type. the csseq_{aai}, csseq_gr_{grjk}, ent_{aai}, and avg_prob_gr_{gri,j} aggregate the values along the columns of the pssm.

features based on the pssm matrix in combination with the predicted secondary structure 
the third feature set is analogous to the features based on the pssm matrix, but instead of aggregating the values by aa type, they are aggregated either by the type of the secondary structure predicted with psi pred or by the combination of the aa type and the predicted secondary structure. these features quantify conservation of predicted secondary structures, as well as the conservation for individual aa types and grouping of aas according to the property groups that are in a given predicted secondary structure. this feature set consists of:

- ach_cs_{aai}, max_cs_{aai}, max-ach_cs_{aai}, ach_prob_{aai}, max_prob_{aai}, and max-ach_prob_{aai} are redefined as ach_cs_{ssn}, max_cs_{ssn}, max-ach_cs_{ssn}, ach_prob_{ssn}, max_prob_{ssn}, and max-ach_prob_{ssn}, respectively, where instead of using  <dig> aai we aggregate by the predicted three state secondary structure ssn = {h, e, c}. 

- ach_cs_{aai}, max_cs_{aai}, max-ach_cs_{aai}, ach_prob_{aai}, max_prob_{aai}, and max-ach_prob_{aai} are redefined as ach_cs_{ssn}_{aaj}, max_cs_{ssn}_{aaj}, max-ach_cs_{ssn}_{aaj}, ach_prob_{ssn}_{aaj}, max_prob_{ssn}_{aaj}, and max-ach_prob_{ssn}_{aaj}, respectively, where we aggregate pssmconslm/pssmproblm values by l that corresponds to positions of aai that are predicted as ssn. 

- csseq_gr_{grjk} and avg_prob_gr_{grjk} are redefined as csseq_gr_{grjk}_ss_{ssn} and avg_prob_gr_{grjk}_ss_{ssn}, respectively, where we aggregate pssmconslm/pssmproblm values by l that corresponds to a given ssn. 

features based on the predicted secondary structure 
the fourth feature set, which was computed based on the output of psi pred, describes the content of the predicted secondary structures and distribution of the predicted secondary structures segments aggregated based on segments length and by grouping of aas according to the property groups. this set consists of:

- content_{ssn} = , the number of residues predicted as ssn where l =  <dig>   <dig> ..seqlen, divided by the sequence length. 

- content_{ssn}_gr_{grjk} = , the number of residues predicted as ssn and that belong to grjk where l =  <dig>   <dig> ..seqlen, divided by the sequence length. 

- segcount_{e,h}_l{li} = , the number of helix or strand segments which contain at least li =  <dig>   <dig>  ..  <dig> aas divided by the total number of helix and strand segments in the input protein chain. 

- segcount_c_l{li} = , the number of coils which contain at least li =  <dig>   <dig>  ..  <dig> aas divided by the number of all segments in a protein . 

- segcount_{e,h}_p{pi} = , the number of helix or strand segments which contain at least piaas where pi =  <dig> ,..,10% of the sequence length, divided by the total number of helix and strand segments in the input protein chain. 

- segcount_c_p{pi} = , the number of coil segments which contain at least pi aas where pi =  <dig> ,..,10% of the sequence length, divided by the number of all segments. 

- normsegcount_{ssn} = , the total number of ssn segments divided by the total number of all secondary structure segments in the input protein chain. 

- maxseglength_{ssn} = max len), the maximal ssn segment length. 

- normmaxseglength_{ssn} = , the maximal ssn segment length divided by the sequence length. 

- avgseglength_{ssn} = avglen), the average ssn segment length. 

- normavgseglength_{ssn} = , the average ssn segment length divided by the sequence length. 

features based on the collocation of helix and strand segments in the predicted secondary structure 
the four main structural classes are based on the content and relative spatial position of the secondary structures. the preferred way to represent these collocations of the secondary structures would be to use 3d protein structure. however, since our input is only the sequence, we approximate this information using features that quantify collocation of helices  and strands  in the predicted secondary structure. we use the predicted secondary structure to annotate helix, coil and strand segments and to compute relative position of these segments in the sequence. the following features are computed:

- hh = count, the number of helix-coil-helix motifs  divided by the total number of the secondary structure segments in a protein. 

- ee = count, the number of strand-coil-strand motifs  divided by the total number of the secondary structure segments in a protein. 

- he = count + count, the number of strand-coil-helix or helix-coil-strand motifs  divided by the total number of the secondary structure segments in a protein. 

- {hh,he,ee}_l{li} = , the number of helix-coil-helix, helix-coil-strand/strand-coil-helix, or strand-coil-strand motifs which include at least li =  <dig>   <dig>  ..,  <dig> residues in the middle coil, divided by the total number of the secondary structure segments in a protein. 

- {hh,he,ee}_p{pi} = , the number of helix-coil-helix, helix-coil-strand/strand-coil-helix, or strand-coil-strand motifs which include at least pi =  <dig>   <dig>  .., 10% of a sequence length residues in the middle coil, divided by the total number of the secondary structure segments in a protein. 

- maxhch = max), the maximal number of helices among all helix-coil-helix-coil...coil-helix motifs, i.e., the maximal number of helix segments separated only by coils. 

- maxece = max), the maximal number of strands among all strand-coil-strand-coil...coil-strand motifs, i.e., the maximal number of strand segments separated only by coils. 

- avghch = , the average number of helices in all helix-coil-helix-coil...coil-helix motifs, divided by the total number of the secondary structure segments in a protein. 

- avgece = , average number of strands in all strand-coil-strand-coil...coil-strand motifs, divided by the total number of the secondary structure segments in a protein. 

- hch_l{li} = , the number of helix-coil-helix-coil...coil-helix motifs with more than li =  <dig>   <dig>  ..,  <dig> helices, divided by the total number of the secondary structure segments 

- hch_p{pi} = , the number of helix-coil-helix-coil...coil-helix motifs with more than pi =  <dig>   <dig>  .., 10% of all helices in a protein, divided by the total number of the secondary structure segments 

- ece_l{li} = , the number of strand-coil-strand-coil...coil-strand motifs with more than li =  <dig>   <dig>  ..,  <dig> strands, divided by the total number of the secondary structure segments 

- ece_p{pi} = , the number of strand-coil-strand-coil...coil-strand motifs which more than pi =  <dig>   <dig>  .., 10% of all strands in a protein, divided by the total number of the secondary structure segments 

feature and classifiers selection
feature selection was performed to select the best subset of the considered features for each structural class. this is motivated by the fact that while the considered features are generic, the individual structural classes are likely characterized by a small and specific set of descriptors. in other words, while the features describe the sequence, conservation of residues and predicted secondary structure for every protein in the same way, the structural classes can be described by a subset of these features, i.e., for a specific class some features could be irrelevant and should be discarded to improve the efficiency of the prediction model. we considered a comprehensive set of eight feature selection methods which include four methods that select feature sets and four methods that perform feature ranking. the first group includes consistency subset selection  <cit> , wrapper-based feature selection with naïve bayes and svm classifiers  <cit> , and correlation-based feature subset selection  <cit>   methods. the latter group includes a filter-based relieff algorithm  <cit> , and three methods that perform ranking based on symmetrical uncertainty  <cit> , chi-squared  and gain ratio  criterions. the feature selection was performed based on tenfold cross validation on the astraltraining dataset. in the case of the methods that select feature sets, individual features were ranked based on the number of folds in which they were selected. for the ranking methods the feature were ranked based on as the average rank over the ten folds.

we considered four classifiers which are based on complementary model types: nonlinear kernel-based svm  <cit> , probabilistic naïve bayes  <cit> , linear logistic regression  <cit> , and instance-based k-nearest neighbor  <cit>   with k =  <dig>  the selection was also motivated by their prior successful applications in the context of the structural class predictions, i.e., naïve bayes based classifier was used in  <cit> , logistic regression in  <cit> , nearest neighbor in  <cit> , and svm in  <cit> .

the quality of the prediction was reported using several measures including overall accuracy , accuracy for each structural class , matthews's correlation coefficient  for each structural class, and generalized squared correlation . the mcc values range between - <dig> and  <dig>  where  <dig> represents random correlation, and bigger positive  values indicate better  prediction quality for a given class. since mcc works only for binary classification, we also reported gc <dig>  which is based on χ <dig> statistics. the gc <dig> values range between  <dig> and  <dig>  where  <dig> corresponds to the worst classification  and  <dig> corresponds to the perfect classification. mcc and gc <dig> are described in detail in  <cit> . during the design we selected a classifier/feature subset combination that provides the best mcc value for a given class. we used mcc since this measure, in contrast to accuracy, takes into account the unbalanced nature of the datasets, i.e., while high accuracy could be obtained for a default classification in which small class is ignored , positive mcc values assure that both small and large classes are correctly predicted.

for each structural class and each of the four considered classifiers we used the output of each of the eight feature selection methods to find the best subset of features, i.e., subset of features that provides the highest mcc value for a given classifier. for the four selection methods that generate subsets of features, we considered different subsets based on the number of folds in which a given feature was selected. in other words, for each of the four methods we generated subsets of features that were included in at least  <dig> cross validation fold, at least  <dig> folds, ...., and at least  <dig> folds . in the case of the four feature ranking methods, we started with the highest ranked features and kept adding subsequent features until the mcc values for a given classifier was increasing . finally, for each of the  <dig> feature sets we compared results of the tenfold cross validation test on the astraltraining dataset using each of the classifiers to select the setup with the highest mcc for a given structural class.

we note that although naïve bayes, logistic regression and k-nn do not require parameterization, svm is sensitive to parameterization. we used svm with linear kernel and cost parameter c set to  <dig> to find the best feature set for each structural class , and later we used two different kernels, polynomial and rbf, and different values of c to parameterize the svm for the selected feature sets. we performed a grid search  and selected the configurations that provide the highest mcc values for the tenfold cross validation on the astraltraining dataset.

our resulting design shows that the best results for all seven classes were obtained with the svm classifier. this is consistent with the successful prior use of this classifier for the prediction of the four major structural classes  <cit> . table  <dig> summarizes the selected classifiers, i.e., it lists the results of the parameterization of the svm classifier, and the feature selection methods together with the number of the selected features for each of the seven considered structural classes. we observe that usage of a variety of feature selection methods was proven beneficial since five out of eight of them were used to derive the final feature sets.

classification
once the user selects the classes that he would like to consider, the input sequence is converted into the feature space and the corresponding feature sets are passed to the classifiers for each of the selected classes. each of the classifiers returns a probability that the input sequence belongs to a given class. the prediction corresponds to the class that is associated with the highest probability. this type of aggregation allows the user to select any combination of the classes.

RESULTS
this section includes discussion of the selected feature sets, reports results of the proposed modas method on the independent test set astraltest and compares them with results provided by several competing solutions, and compares the results of the proposed and over two dozens of existing methods for the prediction of the structural classes on five benchmarking datasets including d <dig>  25pdb, d <dig>  d <dig>  and d <dig>  we emphasize that all considered datasets, except d <dig>  are characterized the twilight zone pairwise sequence similarity . we report the overall accuracy, accuracies and mcc values for each structural class, and the gc <dig> values.

discussion of the selected features
the selected features are summarized using tables  <dig> and  <dig>  the former table shows the number of selected features for each of the five feature set and for each structural class. the latter table presents details related to features computed from the predicted secondary structure focusing on different types of the secondary structures.

we observe that only a few sequence based features are used by the proposed modas method. more specifically, although the total number of features in this set includes  <dig> only between  <dig> and  <dig> of them are used by the seven classifiers. the most frequently used source of information is the pssm in combination with the predicted secondary structure. for almost all classes, including all-α, all-β, α+β, multi-domain, membrane and small proteins, over half of the features are computed using pssm. this confirms that the conservation of the residues provides higher quality information than their presence. in the case of the remaining α/β class the majority of features are based on the predicted secondary structure. we also note that a few other classes, such as all-α, α+β, multi-domain and membrane proteins, heavily utilize the information concerning the predicted secondary structure in connection with the pssm. the popularity of the features derived from the secondary structure stems from the fact that the structural classes are de facto defined based on the secondary structures.

the predictor for the all-α class uses large number of features from pssm and pssm combined with the predicted secondary structure. this shows that residue conservation is an important factor that distinguishes between all-α and other classes. we also observe that these features utilize information about both helix and strand segments, where the strand segments are likely used to indicate non all-α proteins. finally, this feature set includes  <dig> features based on the helix-coil-helix motifs that occur in virtually all proteins from this class.

most of the features for the all-β class are again based on the pssm. this feature set also includes features that quantify the amount of helix-coil-helix  and strand-coil-strand segments  and a relatively large number of coil-based features. the latter is likely due to the fact that proteins from the all-β class include relatively large number of β-sheets which incorporate larger number of coils  that connect individual strand segments that make up the β-sheet.

the α/β class incorporates a relatively large number of features that quantify the occurrence of the helix-coil-strand and strand-coil-helix motifs. this agrees with the definition of this class that incorporates structures in which helices and strands are interspersed. such spatially scattered secondary structures are likely to also alternate in the sequence.

the largest number of features was selected for the α+β class. this is likely because this class is the hardest to predict among the four major classes, e.g.,  <dig> out of  <dig> structural class prediction methods that were recently compared in  <cit>  provide the lowest prediction quality for this class when compared with the predictions for the all-α, all-β and α/β classes. most of the features utilized by the α+β classifier are based on the psmm combined with the predicted secondary structure. all of the features that exploit collocation of the helix and strand segments are based on either collocation of helix  or strand  segments. this is motivated by the definition of this class that includes protein in which secondary structures are segregated.

the multi domain proteins have structures that combine characteristics of the four major structural classes since different domains may fold into structures characteristic to different classes. this is likely the reason why this class uses relatively equal number of features coming from different sources, like the pssm and the predicted secondary structure, and why the secondary structure based features equally cover all three structure types .

the membrane proteins include long transmembrane α-helices and this is the likely the reason why the corresponding classifier makes use of  <dig> out of  <dig> features that are based on the predicted helices. as in the case of most of the other classes, features used to classify membrane proteins also heavily rely on the residue conservation.

we note that although the small protein class includes short protein chain, the feature that measures the sequence length was not selected for the corresponding classifier. this is likely since several other classes also include short chains, but their secondary structure fits the definition of a given class rather than being composed mostly of coils which is characteristic for the small proteins class. the features for this class come from different sources including the sequence, the pssm and the predicted secondary structure. we observe that helix/strand collocation based features were not selected for this class; again, this is likely since these proteins are mostly composed of coils.

we also discuss the most useful features for prediction of each of the considered seven structural classes. we select two representative features for each class and use a scatter plot of their values to explain their relation with the classes. the selection of the features is based on their correlation with the classes  and correlation with each other . the first feature was selected based on the largest values of its biserial correlation with the class labels . the remaining features were ranked based on their biserial correlation coefficients and the top ranked feature for which the pearson correlation coefficient with the first feature is smaller than  <dig>  was selected as the second feature. we also compare the scatter plots for these two features with the scatter plots when using helix and strand content to discriminate between the classes. this is motivated by the fact that some older structural class assignment methods performed the class assignment using the secondary structure content rather than the spatial arrangement of the secondary structures which comes from the tertiary structure  <cit> . figure  <dig> presents the corresponding  <dig> scatter plots.

the two representative features for the all-α class are csseq_ss_e_c  and segcount_h_l <dig> . we observe that proteins with high segcount_h_l <dig> values and proteins with low values of csseq_ss_e_c likely belong to the all-α class. this is supported by the fact that all-α proteins are characterized by significant helix content and thus they include relatively large number of long helices. the csseq_ss_e_c feature shows that all-α proteins include virtually no strands in which cys is conserved. costantini and colleagues have observed that cys has strong propensity to form strands and is more prevalent among the proteins from all-β class  <cit>  and thus proteins that include strands with conserved cys are unlikely to belong to all-α class. the right-hand-side plot in figure 4a shows that the all-α proteins are characterized, as expected, by a high content of helices and a low content of strands. at the same time, we note that some non all-α proteins  could be misclassified using this criteria, which shows that the two representative features used in the proposed method likely provide better discriminatory power.

the two features selected for the all-β class  include he  and csseq_ss_h_a . the proteins from this class have low csseq_ss_h_a and medium to low he values for chains for which csseq_ss_h_a values are close to zero. the he feature is motivated by the fact that all-β proteins include relatively large number of strands and a low number of helices and thus strand-coil-helix or helix-coil-strand motifs are less likely to occur in these proteins. the csseq_ss_h_a feature shows that the all-β class includes chains that have very few helices with conserved ala. this is supported by the work in  <cit>  which shows that ala has strong propensity to form helices and occurs relatively more frequently in proteins from the all-α class, which suggests that chains that include helices with conserved ala are unlikely to belong to the all-β class.

the proteins from the α/β class are characterized by average values of avghch  and high values of he_l <dig>  features. the he_l <dig> indicates that the proteins from this class have the helices and strands interspersed in the sequence and avghch shows that they do not include secondary structures with no consecutive helices and with many consecutive helices. the latter shows that α/β class includes proteins with helices, but they are less likely to form long helix-coil-helix-coil...coil-helix motifs.

the two representative features for the α+β class include segcount_e_l <dig>  and ece_l <dig> . protein from this class have average to high values of both features which is motivated by the observation that they have strands  and the strands and helices are segregated, i.e., that strands co-occur closely in the sequence, which results in high values of ece_l <dig>  we observe that usage of the content leads to a significant overlap between the proteins from the α/β and α+β classes, see the right-hand-side plots in figures 4c and 4d. at the same time, the proposed method uses different features for different classes, which can potentially provide better discrimination between these two classes when compared to using the content. the representative features for the α/β and α+β classes quantify the spatial relation of the helices and strands  which, in our opinion, better captures the characteristics of these two classes when compared with the content.

the scatter plot for the multi domain proteins class shows no clear trends since the number of proteins in this class is small, only  <dig> out of  <dig> in the astraltraining dataset, and since the best feature for this class has relatively small biserial correlation value of  <dig> . this is likely due to the significant overlap between this class and other classes, i.e., individual domains in these proteins belong to different structural classes. we observe that proteins from this class have relatively high value of segcount_c_p <dig>  combined with low value of segcount_e_p <dig> . this suggests that on average they include longer coil segments and a few or none longer strands when compared with other classes. we note that similar structures occur also for chains from other classes, i.e., the markers in figure 4e have only relatively light shading. we also observe that the usage of helix and strand contents results in the scatter plot with even lighter shading of the markers.

the membrane and cell surface proteins are best described using csseq_gr_r_polarcharged  and max-ach_prob_ss_h_g  features . these proteins are characterized by high values of max-ach_prob_ss_h_g, which is motivated by the inclusion of transmembrane helices  <cit>  and by frequent presence of gly in membrane proteins  <cit> . this class is also associated with medium to low values of csseq_gr_r_polarcharged, which is supported by prior research that shows that asp, arg, lys, gln, asn, glu, pro, ser, thr, gly, and his are characterized by low  propensity to form membrane regions based on the membrane propensity scale from  <cit> ; in other words, the existence of the conserved residues of the above type suggests that the corresponding chain is less likely to be associated with the membrane regions in the protein chain.

lastly, the high values of comp_c  together with above average values of comp_gr_e_neutral  features are shown to be associated with the small proteins class. the former agrees with the strand and helix content scatter plot  that shows that small proteins usually include only a few helix and strand structures. according to costantini and coworkers gly, his, and ser are shown to be among the amino acids with high propensity to form coils  <cit> , which is a likely reason why comp_gr_e_neutral feature was selected.

results for the independent test set astraltest
the proposed prediction system was trained using the astraltraining dataset and tested using the astraltest database. a summary presented in table  <dig> shows results for three configurations of the proposed modas method that include prediction of the four major classes, six classes that exclude the small proteins class, and prediction of all seven classes. for each setup we use only the instances from the selected classes to perform the test.

# of
the results show that the accuracy is around 83% for the prediction of the four major classes and close to 80% when considering the  <dig> classes. this moderate drop in accuracy is attributed to the predictions for the multi-domain proteins class which obtains the lowest accuracies. we note that positive mcc values indicate that the proposed model provides predictions that are always better than random. most importantly, in spite of the twilight zone similarity between training and testing sequences we observe that the proposed method is characterized by good performance for all classes except the multi-domain proteins class, which is supported by the mcc and gc <dig> values of above  <dig>  and  <dig> , respectively. the all-α class is the easiest to predict. the corresponding predictions for all three configurations are characterized by accuracy of above 91% and mcc of  <dig>  or higher. the predictions for the α/β and all-β classes have similar quality with accuracies ranging between  <dig> and 85% and mcc between  <dig>  and  <dig> . the predictions of the small proteins class are also characterized by a relatively high accuracy and mcc. we observe that inclusion of this class, see the results for the  <dig> and  <dig> classes in table  <dig>  results in a slight drop in the quality of the prediction of the all-α, all-β, and α+β classes. this suggests existence of an overlap between these classes and the small proteins class. the relatively poor scores for the multi-domain proteins class are likely due to the small size of this class and since proteins from this class consist of domains that likely belong to different structural classes. although the accuracy of the prediction of the membrane proteins is at 58%, we emphasize that relatively high mcc value of  <dig>  indicates that the proposed method performs well for this class. the results for this class should be considered successful given that this class is significantly underrepresented in the datasets, i.e., membrane proteins account for only  <dig> % of proteins in both the astraltraining and the astraltest databases.

we also compare the results obtained by the proposed modas method on the astratest dataset with the results of two recent representative methods that were designed to work with low identity sequences, scpred  <cit>  and scec  <cit> . both of these methods use svm to perform predictions and they are shown to provide favorable prediction quality with compared with other existing structural class predictors . scpred is the only existing method that uses predicted secondary structure to predict the structural classes and scec uses psmm to compute the predictions. these two methods predict only the four major classes and thus we compare the performance considering only these classes. we removed sequences from the three minor classes and sequences with less than  <dig> residues from the training and test sets since scec cannot provide predictions for such short chains. the scpred algorithm was trained both on the original 25pdb dataset as it was done by the authors of this method  <cit> , and we also retrained this method using astraltraining dataset. in the case of the scec algorithm we used the corresponding web server to perform predictions. we assumed that the user of the modas system may not know how many classes should be considered in the test and thus we included the results when prediction was made for only the  <dig> major classes, the  <dig> classes , and all  <dig> classes. the results are presented in table  <dig> 

the modas method was used to make predictions for all  <dig>   <dig> , and the  <dig> major classes. the scec method was trained on the astraltraining with the proteins from the  <dig> major classes  and on the 25pdb dataset based on results in  <cit> . the scec predictions were generated using the web server at http://biomine.ece.ualberta.ca/structural_class/scec.html. bold font indicates the best results.

the modas method is shown to provide favorable quality for the prediction of the  <dig> classes. the quality of the results generated by the proposed method is slightly lower when using predictions that consider more classes, but the overall accuracy and gc <dig> are still higher than the quality provided by both competing solutions even when using the model that predicts all  <dig> classes. the accuracy improvements of the best modas model that predicts  <dig> classes over the best results from other methods equal  <dig> %,  <dig> %,  <dig> %, and  <dig> % for the all-α, all-β, α/β, and α+β classes, respectively. this translates into  <dig> / =  <dig> / <dig>  = 13%,  <dig> / <dig>  = 21%,  <dig> / <dig>  = 10%, and  <dig> / <dig>  = 20% error rate reductions, respectively, when compared with the error produced by the best performing competing method. the corresponding error rate reduction for the overall accuracy equals  <dig> / = 19%. the most encouraging improvements that are measured using mcc concern α/β and α+β classes where the modas method is better by at least  <dig>  when compared with the best existing method. we also observe that scpred performs slightly better when trained on the bigger astraltraining dataset. the scec provides the lowest ranked predictions among the considered methods.

comparison with the existing structural class predictors
the side-to-side comparison with recently proposed structural class prediction methods is based on the tests on three popular benchmarking datasets, 25pdb, d <dig> and d <dig>  which are characterized by the low sequence identity. these sets were used to test methods that predict the  <dig> major classes and thus the proposed modas method is also setup to predict these  <dig> classes. we also use the d <dig> dataset to compare with the most recent structural class predictor that considers the  <dig> classes  <cit> . following the prior works in this area we use jackknife test to measure the performance. the selection of this test strategy is motivated by the work in  <cit>  which shows that jackknife is deemed the most objective as it always yields a unique result for a given dataset and that this test is increasingly used to examine the accuracy of various predictors. in this test all but one sequence are used to train the proposed classification system  and the remaining sequence is used to perform the test; this process is repeated to use each sequence from the dataset once as the test sequence.

the results were obtained using jackknife test. the methods are ordered by their average accuracies which coincide with the gc <dig> scores. best results are shown in bold and "---" indicates results that were not reported by the original authors and which cannot be duplicated.

results shown in tables  <dig> and  <dig>  which concern jackknife tests on the d <dig> and d <dig> datasets, respectively, are consistent with the results on the 25pdb dataset. the modas method outperforms all competing methods as measured by the overall accuracy. the only method that provides similar prediction quality is again scpred. results show that accuracy provided by modas is better than the accuracy of scpred by  <dig> % and  <dig> % on the d <dig> and d <dig> datasets, respectively. the proposed method provides substantial improvements over scpred for the prediction of the α+β class. the scec predictor, which utilizes pssm generated with psi blast as its input, provides the third best results on both of these datasets. this demonstrates that evolutionary information provides a better source of information for the prediction of the structural class when compared with the sequence of the input protein that is used as an input by all lower ranked methods. we note that the size of the dataset used to build pssm would likely impacts the prediction quality, as it was demonstrated for the secondary structure predictions  <cit> . larger size of the dataset may induce better prediction performance, which could explain a portion of the improvements of the modas method that was trained using relatively recent version of the nr database, when compared with other predictors, including scec and scpred, which used smaller datasets. we could not provide mcc and gc <dig> values for results on these two datasets  since they were not provided by the authors of the existing methods.

the results were obtained using jackknife test. the methods are ordered by their average accuracies. best results are shown in bold and "---" indicates results that were not reported by the original authors and which cannot be duplicated.

the results were obtained using jackknife test. the methods are ordered by their average accuracies. best results are shown in bold.

we also compare modas with methods that were tested on datasets with unspecified sequence identity between the test and the training sequences. the results of the jackknife test on the d <dig> dataset are presented in table  <dig>  the proposed method again achieves the highest accuracy  among all competing methods that were tested on this dataset. we observe that the lowest accuracy for this dataset is around 89%. the accuracy of  <dig> % obtained by the third best scec method demonstrates that it is easier to obtain high predictive performance on this protein set when compared with the datasets with lower sequence identity, i.e., scec achieves 63-67% accuracy for the low-similarity datasets. based on the observations from a recent study by kurgan and homaeian  <cit> , the high levels of accuracy are most likely due to relatively high pairwise sequence similarity of the d <dig> dataset, see datasets section. on the other hand, the differences between the accuracy on the low and the high-similarity datasets for the scpred and modas methods are smaller than for the scec. this is most likely since these methods were designed using low sequence identity datasets.

the results were obtained using jackknife test. the methods are ordered by their average accuracies. best results are shown in bold.

the results were obtained using jackknife test. "---" indicates results that were not reported by the original authors and which cannot be duplicated.

the high quality of the results provided by scec and scpred supports our choice to use the evolutionary information encoded in pssm and the predicted secondary structure as inputs for the proposed modas method. the above results demonstrate that modas consistently, over multiple datasets, outperforms competing approaches and that it is capable of providing high quality predictions for both the  <dig> major classes and the  <dig> classes.

CONCLUSIONS
this work addresses lack of structural class predictors that consider seven structural classes, as defined in scop, and which are characterized by high prediction quality when applied to problems that involve query sequences that share twilight-zone similarity with the sequences used to develop the prediction model. this is motivated by the fact that prediction for the low-similarity sequences has applications in the detection of the remote homologues.

we propose a prediction method that applies svm classifier on a set of features that are computed from the input protein sequence. our design incorporates novel features that utilize sequence-derived information that includes pssm computed with psi blast and secondary structure predicted with psi pred. we performed a comprehensive feature selection and classifier selection and parameterization procedure to optimize the quality of the predictions. the proposed method is the first to provide modular design in which a separate classifier is created for each class.

an extensive empirical evaluation of the proposed modas method that includes tests on  <dig> twilight-zone and  <dig> high-similarity datasets and comparison with over two dozens of modern existing structural class predictors shows that modas achieves the best overall accuracies for predictions of both the  <dig> major structural classes  and the  <dig> classes . modas is shown to achieve accuracy of over 80% and gc <dig> scores of over  <dig> . the main advantages of the proposed method include  the high quality of the predictions for problems involving low sequence similarity datasets;  availability of predictions for  <dig> structural classes ; and  modularity which allows the user to select any subsets of the  <dig> classes that will be considered as the possible outcomes for the query sequence. in particular, we observe that modas provides accurate predictions for the membrane and cell surface proteins, which is an important class that is not considered by the majority of the existing predictors. the improved quality stems from the usage of the two important sequence-derived sources of information, the predicted secondary structure and the evolutionary information, and the development of novel features that express collocation of the secondary structure segments in the protein sequence and that combine evolutionary and secondary structure information. the results also suggest that the information extracted from the secondary structure that is predicted along the protein chain can be successfully used to predict structural classes that are defined based on the spatial arrangement of the secondary structures.

a web server that implements the modas method is available at http://biomine.ece.ualberta.ca/modas/. this server limits the number of input sequences to  <dig>  in the case of the larger sequence sets, the interested user is asked to contact the corresponding author. the web server was trained on the  <dig>  version of the astral database with less than 20% sequence similarity .

abbreviations
: three dimensional; : amino acid; : class, architecture, topology and homologous superfamily; : correlation-based feature subset selection; : generalized squared correlation; : k-nearest neighbor; : matthews's correlation coefficient; : modular approach to structural class prediction; : position specific scoring matrix; : structural classification of proteins; : support vector machine.

authors' contributions
mjm contributed to the conception of the proposed method, designed and implemented the feature sets and the classifiers, performed the tests, implemented the web server, contributed to the evaluation and interpretation of the results, and wrote the manuscript. lk contributed to the conception of the proposed method and the design of the feature sets and the classifier, helped in performing the tests, contributed to the evaluation and interpretation of the results, and wrote the manuscript. both authors have read and approved the final version of the manuscript.
