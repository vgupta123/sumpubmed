BACKGROUND
inferring gene orders and gene content of ancestral genomes has a wide range of applications. high-level rearrangement events such as inversions, transpositions that change gene order are important because they are "rare genomic events"  <cit>  and can be used to estimate ancestral genomes and infer the number of steps that evolve one genome into another.

the fundamental question in building phylogenies is how far apart two species are from each other. hannenhalli and penvzner  <cit>  provided the first polynomial algorithm for computing inversion distance, which can be used as a representation of the evolution distance between species. then by using a direct optimization approach based on median calculation, which is to optimize each ancestral node in terms of its three or more immediate neighbors, the phylogeny and ancestral genomes could be reconstructed. current popular methods  all use this approach and can infer ancestral gene orders with high accuracy. however these methods are extremely slow especially when the rearrangement rate is high. a new method based on the maximum likelihood approach is recently proposed  <cit> . in this method, the probabilities of all possible ancestral gene order are calculated based on the present species' gene order data and the ancestral gene orders are reconstructed to maximize the overall probability. this method is much faster than the direct optimization methods under high evolution rearrangement event rate, but the accuracy of the reconstructed gene order is lower than that of the direct optimization methods.

in this paper, we propose a mixture method to enhance the inference of ancestral gene orders. we first use the maximum likelihood method to reconstruct an initial ancestral genome. then we randomly select a number of gene adjacencies from the ancestral genome and fix them to run the median calculation. by analyzing the results of each median calculation, we try to infer the correct gene adjacencies generated by the maximum likelihood method. finally we fix these adjacencies and perform median calculation to get the ancestral genome. we have conducted extensive experiments on simulated datasets and the results show that this mixture approach is more accurate than the maximum likelihood method. also our method is much faster than solely using the median calculation when the rearrangement rate is high. using this hybrid approach, for those datasets that are previously too difficult for existing methods, we will be able to analyze them within a reasonable time frame with very high accuracy.

genome rearrangements
given a set s of n genes { <dig>   <dig>  ⋯, n}, a genome can be represented by an ordering of these genes. a gene with a positive orientation is written as i, otherwise it is written as a -i. a genome can be linear or circular. a linear genome is a permutation on the gene set, while a circular genome can be represented in the same way under the implicit assumption that the permutation closes back on itself. let g be the genome with signed ordering of  <dig>   <dig>  ⋯, n. an inversion  between indices i and j , transforms g to a new genome with ordering

  <dig> ,⋯,i- <dig> -j,-,⋯,-i,j+ <dig> ⋯,n 

there are some additional events for multiple-chromosome genomes, such as translocation , fission  and fusion .

we use a breakpoint graph  <cit>  to represent the permutation with respect to the identity permutation. given a genome with permutation π, let the breakpoint graph that corresponds to it be m . the vertex set v of m  is the collection of {2i -  <dig>  2i}, and i is any distinct gene of permutation π. two genes i and j are said to be adjacent in genome g if i is immediately followed by j and can be represented by an edge . if i and -j are adjacent, then it can be represented by an edge . the edge set e of m  consists of all the adjacencies in π. for example, for a circular unichromosomal genome g <dig> =  <dig> ,  <dig>  - <dig>   <dig>  the vertex set is v = { <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> } and the edge set is e = {, , , , }, as shown in figure  <dig>  the breakpoint graph extends naturally to a multiple breakpoint graph , representing a set of three or more genomes.

inferring ancestral gene order by median calculation
one popular approach to reconstruct ancestral genome is through direct optimization and the task is to seek the tree with the minimum number of events, which is in spirit similar to the maximum parsimony approach used in sequence phylogeny. the core of these methods is to solve the median problem defined as follows: given a set of n genomes with permutations {πi}1≤i ≤ n and a distance measure d, find a permutation πm that minimizes median score defined as ∑i=1nd.

widely used methods like grappa  <cit>  and mgr  <cit>   both use median calculation to infer phylogenies and ancestral gene orders. for example, grappa examines every possible tree topology and reports the one with the minimum events. for a given tree, it iteratively updates the gene order on each ancestral node in terms of its three neighbors through median calculation until the whole score of the tree reaches minimum. at this point the gene order obtained on each internal node corresponds to the inferred ancestral genome data and the total number of events is simply the summation of pairwise distances along all edges.

the median problem is known to be np-hard  <cit>  under most rearrangement distances proposed so far. recently xu proposed the concept of adequate subgraph  <cit>  and applied this theory to create the asmedian solver, which is the fastest median solver to date. the multiple breakpoint graph  is used to model the median problem and an adequate subgraph is defined as a subgraph that has number of cycles larger or equal to 3m/ <dig> . adequate subgraphs allow a decomposition of an mbg into smaller, more easily solved graphs. the median solution is the combination of the median solutions for the adequate subgraphs and the remaining mbg, so we can shrink the known median edges of the adequate subgraph and reduces the original mbg size.

by searching the existing adequate subgraphs up to a certain size and incorporate them into an exhaustive algorithm for the median problem, the asmedian solver could significantly reduce the computation time of median calculation, though it still runs very slowly with high gene rearrangement rate. given n the number of genes in the dataset and r the expected number of events per edge, when the ratio of r/n is larger than 25%, all direct optimization methods have great difficulty in finishing the analysis even after months of computation. as asmedian  use a branch-and-bound approach, its performance relies on how to quickly prune branches. if we can fix some adjacencies before performing median calculation, the search space will be further decreased and the problem would be solved much faster.

reconstruct the ancestral genomes based on maximum likelihood
another new approach to reconstruct the ancestral genome is based on maximum likelihood. ma proposed a probabilistic framework for inferring ancestral genomic orders with rearrangements including inversion, translocation and fusion  <cit> . when the phylogeny tree is given, the tree is re-rooted so that the ancestral node to be inferred becomes the root of the new tree. in this way, all the leave nodes can be taken into account for reconstructing the target ancestral node.

because the gene order of any ancestral node is unknown, the probabilities of all possible gene adjacencies within the genome need to be calculated. since each internal node of the phylogeny tree has two children and we assume that both children evolved from the parent node, the posterior probability of any gene adjacency within the parent node can be computed when treating the gene orders of the children as observed data. the calculation can be performed recursively from the bottom leaves level to the top level until the root is reached. at this point the probabilities of all the possible gene adjacencies within the target ancestral genome are computed. finally an approximate algorithm is used to pick up and connect the adjacencies together to maximize the overall probability, and these selected adjacencies form the final inferred ancestral genome.

methods
we propose a framework that combines both maximum likelihood and direct optimization approaches to infer ancestral gene orders. the maximum likelihood method runs much faster with high genome rearrangement rate compared with the direct optimization methods, but the latter can infer more accurate gene orders. to speed up the latter, we need to reduce the computation time for the median calculation as it is clearly the bottleneck of the computation. our method tries to pick up the correctly inferred gene order from the maximum likelihood method and fix these adjacencies in the median calculation. as a result, the median search space can be significantly reduced and the median calculation is accomplished much faster while the accuracy of the inferred gene order is also improved.

reduce median search space by fixing adjacencies
we use xu's asmedian solver which is based on the adequate subgraph theory  <cit> . in order to speed up the median calculation, we have to further reduce the median search space. given permutation {πi}1≤i≤n, let the mbg that corresponds to it be m . if we assume an adjacency  should be present in the final median solution, we could force {i, j} to become an adequate subgraph of m . the procedure is as follows: for each permutation π, if edge  is already present in the breakpoint graph, we do nothing. otherwise let ,  be the edges corresponding to vertices i and j. we then remove both edges ,  and create two new edges , . figure  <dig> shows the procedure stated above. three permutations are represented by thin lines, double lines and thick lines respectively. for the permutation of thick lines, edge  already exists, so nothing needs to be done. for the other two permutations, edge  is not present. for the permutation represented by double line, we remove the original edges  and  which are incident to vertices i, j and add an edge  with double line. for the permutation represented by thin line, we remove the original edges  and  which are incident to vertices i, j and add an edge  with thin line. according to the definition of adequate subgraph, vertices {i, j} become an adequate subgraph because three edges are incident to these two vertices  <cit> .

if we can fix a number of adjacencies, an equal number of adequate subgraphs can be created. thus we can shrink these subgraphs from the original multiple break point graph, and the original median problem will be greatly reduced. if the adjacencies that we try to fix are the actual adjacencies present in the final solution, this fixing procedure will not affect the accuracy of the median solution.

infer the correct adjacencies
based on our observation and simulation, only a part of the adjacencies inferred by the maximum likelihood method are the correct adjacencies that are present in the ancestral genome. it will be desired if we can optimally pick out all the correct adjacencies and fix them for the median computation. however, it is very difficult to find out all the correct adjacencies inferred by the current maximum likelihood method as only a small percentage of adjacencies are reported with high probability while others usually have similar small probability which cannot be used to judge whether an adjacency is correct or not. our method to identify the correct adjacencies is a randomized approach. first, we will keep adjacencies that have higher probabilities as "correct". then we will randomly select a number of adjacencies from those with lower probability and fix them in the median computation using asmedian. these randomly selected adjacencies may contain both correct and incorrect adjacencies, but the correct adjacencies are more likely to lead the inference of other correct adjacencies within the ancestral genome after the median computation. thus after the median is found, we examine the non-fixed adjacencies in the "ancestral genome" generated by the median solver and record the ones that also appear in the "ancestral genome" generated by the maximum likelihood method. we repeat the procedure many times. finally for each adjacency generated by the maximum likelihood method, we get the number of times it appears in the resulted median genomes. the more it appears, the more likely it is a correct adjacency.

the number of randomly selected adjacencies should not be too large. too many fixed adjacencies may contain too many incorrect adjacencies to pollute the median calculation and leads to bad results. on the other hand, the number of fixed adjacencies should not be too small, otherwise the time consumption of median calculation is unacceptable since the search space is still too large. in our practice, we randomly choose 70-75% of the adjacencies and fix them in the median computations. we also conducted extensive testing to verify that 70-75% is indeed a good choice and the results are shown later.

when more than three genomes are involved and a phylogeny is given, we will first infer part of the correct adjacencies for each ancestral genome that corresponds to an internal node. we will then use our modified median solver to iteratively update the gene order of these internal nodes with those fixed adjacencies. the tree edge length is the genomic distance between the nodes and the score of the tree is defined as the sum of all the tree edge. the iteration stops when the score of the tree dose not change, suggesting that convergence is reached. at this time, gene orders contained in the internal nodes are the final ancestral genomes to be reported.

RESULTS
experimental methods
we use simulated data to assess the quality of our method as we know the "true" ancestors in simulations. in our experiments, we use two models to generate the data. first we generate model tree topologies from the uniformly distributed binary trees, each with  <dig> leaves indicating  <dig> genomes. we use different evolutionary rates, of 20%, 24% ... 36% expected rearrangement rates per tree edge, with 50% relative flucturation. we also use the model proposed by lin and moret  <cit>  to generate another set of random trees with  <dig> leaves. the tree diameters range from  <dig>  to  <dig> . the lin and moret's tree model has more unbalanced edge lengths and is suggested as more biologically realistic.

once the simulated genomes are generated, we use our mixture method to infer the ancestral gene order data. because we are handling genomes with equal gene contents, the accuracy can be measured with the number of correctly inferred adjacencies  divided by the total number of adjacencies for each genome.

number of replicates required
as discussed before, we use maximum likelihood method to reconstruct the initial ancestral genome and then randomly select a number of gene adjacencies from the initial ancestral gene order data and fix them for median computation. this procedure is repeated until each adjacency has been selected for enough times. finally we record the number of occurrences of each gene adjacency that are present in the median results  and sort these adjacencies according to the number of occurrences. if the number of repeats is large enough, the sorting of the gene adjacencies will remain almost unchanged.

according to our test, when the number of replicates is over  <dig>  the sorting of the gene adjacencies comes to a steady state and there is little change with more repeated tests are conducted. since we will pick a certain percentage of adjacencies with higher numbers of occurrences, the very small change in adjacency sorting will hardly affect the result.

adjacency selection percentage
when we sort the gene adjacencies by the number of their occurrences in the median calculation result, it is clear that the more an adjacency appears, the more it is likely to be a correct adjacency. thus we can just pick the adjacencies with large numbers of occurrences and fix them in the median calculation. however, we need to determine the threshold of how many adjacencies should be fixed. we define the selection percentage as the number of selected adjacencies divided by the total number of adjacencies. from our experience, we found that the percentage cannot be lower than 65%, otherwise the median calculation are still very slow as the search space is not small enough.

to determine the selection percentage, we use various percentages, from 65% to 85%, to select the gene adjacencies and fix them to reconstruct ancestral genomes through median computation. we compare the average accuracy of the reconstructed gene order under different selection percentages. table  <dig> shows the average accuracy of reconstructed gene order under different selection percentages, using uniform trees with various evolutionary rates per edge, while table  <dig> shows the results of lin and moret's tree model with various tree diameters.

from these tables, we can see that when the rearrangement rate or the tree diameter is relatively small , the gene accuracy reaches maximum values when the selection percentage is 70%. when the rearrangment rate or the tree diameter grows larger, it would be better to use 75% as the selection percentage. this is because with larger rearrangement rates, some correct adjacencies inferred by the maximum likelihood method are hard to be inferred by the median calculation, so we should use a larger selection percentage to include more adjacencies inferred by the maximum likelihood method.

accuracy on closely related genomes
first we compare the accuracy of the ancestral genome reconstructed by our method with the result from both ma's maximum likelihood approach and the pure asmedian solver. because asmedian solver is very time consuming, we only perform tests under lower rearrangement rates and diameters. we perform simulation tests using the two types of tree models as stated above with various rearrangement rates and tree diameters. for each configuration we run  <dig> data sets and the results are averaged. table  <dig> shows the comparison under uniform distribution binary trees with average rearrangement rates ranging form 12% to 20% and table  <dig> shows the comparison under lin's random trees with diameters ranging form  <dig>  to  <dig> . as mentioned before, we can see that the median method produces more accurate results than those of the maximum likelihood method under all situations. also we notice that our mixture method could produce a similar or even slightly better result than solely using asmedian. these tables also show that the randomized approach could effectively infer correct adjacencies from the maximum likelihood method and leads to equivalent accuracy of the best median solver.

accuracy on distant genomes
asmedian solver cannot finish datasets with higher rates or diameters, thus for distant genomes, we only compare the accuracy of the ancestral genome reconstructed by our method with the result of ma's maximum likelihood approach. to see the effectiveness of our new approach, we also compare the optimal solution when all correct adjacencies in the maximum likelihood results can be told, which is impossible in real data analysis but is trivial in simulation as the truth is known. this optimal solution is also the upper bound that our mixture framework can achieve.

in conclusion, our method that randomly picks adjacencies from the maximum likelihood method could achieve accuracy that is comparable to the optimal result by picking all the correct adjacencies. for distant genomes, the original median-based parsimony methods cannot give result in reasonable time and we can expect that the result of parsimony method will not exceed the optimal result achieved by fixing part of the correct adjacencies. as a result, we assume that our method could achieve similar accuracy for distant genomes compared with the parsimony methods. also the result shows that, although the accuracy decreases when genomes are more distant from each other, our method are still more accurate than the original maximum likelihood method. however, for very distant genomes , the maximum likelihood method can only produce result with very low accuracy, resulting not enough correct adjacencies for our mixture method. if we keep the original selection percentage , the accuracy of the inferred ancestral genome becomes very low. on the other hand, if we use lower selection percentages, it will be very difficult for the median calculation to finish in reasonable time as not so many adjacencies are fixed. thus inferring the ancestral genome for very distant genomes is still a challenging problem for our method.

time consumption
we also compare the time consumption of our mixture method and the original parsimony method. the time cost of our mixture method includes the time spent on the maximum likelihood method and the time for median calculations with multiple fixed adjacencies. as stated above, operations for calculating median with randomly selected adjacencies are independent from each other, so they can be performed in parallel. the test is performed on an server with  <dig> ghz,  <dig> core cpus. average running time of the two methods is reported in seconds.

CONCLUSIONS
ancestral gene order data can be inferred by either median computations or the maximum likelihood method. median calculation can give a more accurate result but is extremely time consuming under high rearrangement rates. maximum likelihood method runs much faster but the inferred gene order is not as accurate as the result of median method. we propose a mixture method to infer the ancestral gene order based on the above two approaches to improve both time cost and accuracy. the main idea of our method is to pick the correct gene adjacencies through a randomized procedure and reduce the median computation cost by fixing these correct gene adjacencies. experiments show that our mixture method produces more accurate ancestral genomes than the maximum likelihood method while the computation time is far less than that of pure median method.

competing interests
the authors declare that they have no competing interests.

authors' contributions
yz and jt contributed to the development and implementation of the algorithms, and fh were in charge of conducting simulations.

