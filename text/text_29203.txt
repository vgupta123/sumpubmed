BACKGROUND
modeling the protein folding process is crucial in understanding not only how proteins fold and function, but also how they misfold triggering many devastating diseases .

knowledge of the stability, folding, kinetics, and detailed mechanics of the folding process may help provide insight into how and why the protein misfolds. since the process is difficult to experimentally observe, computational methods are critical.

traditional computational approaches for generating folding trajectories such as molecular dynamics  <cit> , monte carlo methods  <cit> , and simulated annealing  <cit>  provide a single, detailed, high-quality folding pathway at a large computational expense. as such, they cannot be practically used to study global properties of the folding landscape or to produce multiple folding pathways. the use of massive computational resources, such as tens of thousands of pcs in the folding@home project  <cit>  have helped improve the time overhead involved but still are unable to handle very large proteins. statistical mechanical models have been applied to compute statistics related to the folding landscape  <cit> . while computationally more efficient, they do not produce individual pathway trajectories and are limited to studying global averages of the folding landscape.

robotics-based motion planning techniques, including the probabilistic roadmap method , have been successfully applied to protein folding . they construct a roadmap, or model, of the folding landscape by sampling conformations and connecting neighboring ones together with feasible transitions using a simple local planner. they can generate multiple folding pathways efficiently  enabling the study of both individual folding trajectories and global landscape properties.

while promising, making good choices for each of the algorithmic steps remains difficult. machine learning approaches have been used to dynamically decide which approach to take for generating samples and connecting them together. these approaches generally learn globally and can perform well in homogeneous spaces or partitioned spaces where each partition is homogeneous  <cit> . preliminary work applied connection learning to protein folding simulations  <cit> , but with no way to ensure a good partitioning of the landscape, the results were only comparable to methods with no learning involved.

we present local adaptive neighbor connection  that localizes learning to within the vicinity of the current conformation being connected. when choosing a connection method , we first dynamically determine a neighborhood around the conformation under consideration. then, the performance history within this neighborhood is used to bias learning. our method adapts both over time and to local regions without any prior knowledge about the methods involved. this approach has been successfully used in robotics  <cit> , and here we adapt it to protein folding.

we compare anc-local’s performance to three distance-based connection methods and to global learning over  <dig> proteins of varying secondary structure makeup with 52– <dig> residues. we examine both the time to build roadmaps and the resulting trajectory quality. we further look at the local planner success rate to understand performance changes between methods. our results confirm that learning is necessary, as no individual method is the best choice for all proteins. we also show that anc-local generates better quality trajectories in comparable time than the best connection method for each individual input and outperforms global learning.

we next describe some preliminaries and related work in further detail, including experimental protein dynamics, the protein model used, prms for protein folding, and several key components such as candidate neighbor selection methods and distance metrics. we also discuss existing machine learning techniques for prms and for protein motion and analysis.

experimental protein dynamics
there have been several advances in experimental techniques to study protein dynamics and motion including circular dichroism, fluorescence experiments, hydrogen exchange and pulse labeling, nmr spectroscopy, and time-resolved x-ray crystallography. we briefly discuss each in turn.

circular dichroism  is a spectroscopic technique used to investigate the structure and conformational changes of proteins  <cit> . by informing on binding and folding properties, cd provides information about the protein’s biological functions. the cd signal occurs when chromophores in an asymmetrical environment interact with polarized light. in the case of proteins, the main chromophores are the peptide bonds as they absorb polarized light in the far-uv wavelength region .

fluorescence spectroscopy analyzes the emission of fluorophores in the protein as the protein undergoes conformational change  <cit> , such as during folding or upon binding. these fluorophores act as indicators of the state of the local environment, e.g., how structured the portion of the protein is near the fluorophore. as almost all proteins have natural fluorophores , fluorescence spectroscopy has broad applicability.

hydrogen exchange mass spectrometry and pulse labeling can investigate protein folding by identifying which parts of the structure are most exposed or most protected  <cit> . from this data, one can infer which portions of the protein fold first and which are last to form, up to the millisecond timescale.

nmr spectroscopy, another experimental tool often used to study protein dynamics, is a technique used to determine a compound’s unique structure. it identifies the carbon-hydrogen framework of an organic compound and has been used to study side-chain motion and backbone motion  <cit> . see  <cit>  for a recent review of current techniques.

x-ray crystallography obtains a three dimensional molecular structure from a crystal  <cit> . a purified sample at high concentration is crystallized and the resulting crystals are exposed to an x-ray beam. this produces a pattern of diffraction spots. the intensities of these spots can be used to determine the structure factors from which an electron density map can be calculated.

while experimental methods can probe some fine-grained details of protein motion, they are time intensive and limit the time scales they can access. in addition, experimental methods may not be able to be applied to all proteins, e.g., some proteins naturally precipitate out and cannot be analyzed. simulations, instead, affords the opportunity to study such proteins and others much faster  with computational resources which will potentially save both time and money.

protein model
proteins are sequences of amino acids, or residues. we model the protein as a linkage where only the ϕ and ψ torsional angles are flexible, a standard modeling assumption  <cit> . a potential energy function models the many interactions that affect the protein’s behavior  <cit> . this function helps quantify how energetically feasible a given conformation is.

in this work, we employ a coarse-grained potential function  <cit>  which help define some characteristics of our modeling and they state that- if the atoms are too close to each other , the conformation is unfeasible; otherwise, the energy is calculated by: 
  <dig> utot=∑constraintskd{1/2−dc}+ehp 

where kd is  <dig> kj/mol, di is the length on the ith constraint, ehp is the hydrophobic interaction, and d0=dc=2Å as in  <cit> . the coarse grain model has been shown to produce qualitatively similar results as all-atoms models faster  <cit> .

prm for protein folding
the probabilistic roadmap method   <cit>  is a robotics motion planning algorithm that first randomly samples robot  conformations, retains valid ones, and then connects neighboring samples together with feasible motions . to apply prms to proteins, the robot is replaced with a protein model and collision detection computations are replaced with potential energy calculations .

sampling
protein conformations, or samples, are randomly generated with bias around the native state, the functional and most energetically stable state. samples are iteratively perturbed, starting from the native state, and retained if energetically feasible by the following probability: 
  <dig> p=1ife<eminemax−eemax−eminifemin<e≤emax0ife>emax 

where emin is the energy of the open chain and emax is  <dig> emin. we use rigidity analysis to focus perturbations on flexible portions as detailed in  <cit> .

connection
once a set of samples is created, they must be connected together with feasible transitions to form a roadmap, or model of the folding landscape. connecting all possible pairs of samples is computationally unfeasible, and it has been shown that only connecting the k-closest neighbors results in a roadmap of comparable quality  <cit> .

given a pair of samples, we compute a transition between them by a straight-line interpolation of all the ϕ and ψ torsional angles. straight-line local planning involves the fewest number of intermediates to check for validity and has been shown to be a sufficient measure of transition probability; i.e., it can accurately predict secondary structure formation order  <cit> .

we assign an edge weight to reflect the energetic feasibility of the transition as ∑i=0n−1−log where pi is the probability to transit from intermediate conformation ci to ci+ <dig> based on their energy difference Δei=e−e: 
  <dig> pi=e−ΔeiktifΔei>01ifΔei≤ <dig> 

where k is the boltzmann constant and t is the temperature. this allows the most energetically feasible paths to be extracted by standard shortest path algorithms.

validation by secondary structure formation order
proteins are composed of secondary structure elements . experimental methods, such as hydrogen exchange mass spectrometry and pulse labeling, can investigate protein folding by identifying which parts of the structure are most exposed or most protected  <cit> . from this data, one can infer the secondary structure formation order.

in  <cit> , we compared the secondary structure formation order of folding pathways extracted from our maps to experimental results  <cit>  by clustering paths together if they have the same formation ordering. we return a stable roadmap when the distribution of secondary structure formation orderings along the folding pathways in the graph stabilizes, i.e., the percentage of pathways following a given ordering does not vary between successive graphs by more than  <dig> %. as our roadmaps contain multiple pathways, we estimate the probability of a particular secondary structure formation order occurring by the percentage of roadmap pathways that contain that particular formation order. the roadmap corroborates experimental data when the dominant formation order  is in agreement.

candidate neighbor selection methods
recall that only neighboring  samples are attempted for connection because it is unfeasible to attempt all possible connections. typically, conformations that are more similar are more energetically feasible to connect.

there have been a number of methods proposed for locating candidate neighbors for connection. the most common is the k-closest method which returns the k closest neighbors to a sample using a distance metric. this can be implemented in a brute force manner taking o-time per node, totaling o-time for connection. a similar approach is the r-closest method which returns all neighbors within a radius r of the node as determined by some distance metric.

other methods use data structures to more efficiently compute nearest neighbors. metric trees  <cit>  organize the nodes in a spatial hierarchical manner by iteratively dividing the set into two equal subsets resulting in a tree with o depth. however, as the dataset dimensionality increases, their performance decreases  <cit> . kd-trees  <cit>  extend the intuitive binary tree into a d-dimensional data structure which provides a good model for problems with high dimensionality. however, a separate data structure needs to be stored and updated.

approximate neighbor finding methods address the running time issue by instead returning a set of approximate k-closest neighbors. these include spill trees  <cit> , mpnn  <cit> , and distance-based projection onto euclidean space  <cit> . these methods usually provide a bound on the approximation error.

in this paper, we work with proteins with a higher dimensionality  than approximate methods can handle. note, however, that there is nothing inherent in our approach that precludes the use of approximate methods.

distance metrics
the distance metric plays an important role in determining the best connections to attempt. it is a function δ that computes some “distance” between two conformations a=〈a <dig> a <dig> …,ad〉 and b=〈b <dig> b <dig> …,bd〉, i.e., δ→ℝ, where d is the dimension of a conformations. here, a1… and b1… are the ϕ and ψ torsional angles for each protein conformation. a good distance metric generally predicts how likely it is that a pair of nodes can be successfully connected. their success is dependent on the nature of the problem studied. we use the following set of distance metrics commonly used for motion planning:

euclidean distance metric
the euclidean distance metric captures the amount of physical movement  that conformation a would undertake to move to a conformation b. this distance is computed by measuring the difference in the ϕ and ψ angle pairs of the two conformations: 
  <dig> δeucl=ϕ1a−ϕ1b2+ψ1a−ψ1b2+...+ϕna−ϕnb2+ψna−ψnb22n. 

cluster rigidity distance metric
rigidity analysis  <cit>  computes which parts of a structure are rigid and flexible based on the constraints present. it may be used to define a rigidity map r, which marks residue pairs i,j if they are in the same rigid cluster.

rigidity maps provide a convenient way to define a rigidity distance metric, between two conformations a and b where n is the number of residues: 
  <dig> δrig=∑0≤i<j≤2n≠rb). 

more details may be found in  <cit> .

root mean square distance metric
the protein model has  <dig> atoms for each amino acid. thus, a protein with n amino acids will have 6n atoms. denoting the coordinates of these atoms as x <dig> to x6n, the root mean square distance  between conformations a and b is: 
  <dig> δrmsd=x1a−x1b2+x2a−x2b2+...+x6na−x6nb26n. 

least rmsd  is the minimum rmsd over all rigid body superpositions of a and b.

machine learning for protein analysis and motion
machine learning algorithms have been employed to predict protein folds, estimate folding rates, and study folding motions. we highlight a few relevant techniques here.

protein fold recognition
protein fold recognition involves identifying the correct structural fold from among a set of known template protein structures for a given protein sequence. fold recognition is essential for template-based protein structure modeling. the fold recognition problem is defined as a binary classification problem of predicting whether or not the unknown fold of the input protein is similar to an already known template from a protein structure library.

rf-fold uses random forests, a highly scalable classification method, to recognize protein folds  <cit> . a random forest is composed of many decision trees that are each trained on datasets of target-template protein pairs. rf-fold recognition rate is comparable to the best performance in fold recognition at the family, superfamily, and fold levels.

dn-fold is another fold recognition technique, but it uses a deep learning neural network as a basis for learning  <cit> . a deep learning network has many more layers than a typical neural network. in addition, they may be trained through unsupervised learning. deep learning was applied to fold prediction by restating the problem as predicting if a given target-template pair belonged to the same fold. they showed that dn-fold achieved comparable performance over a wide variety of methods at all three fold levels.

folding rate prediction
in addition to predicting the fold of a protein, it is useful to estimate its folding rate. this is important when studying properties such as stability and classifying kinetics. characteristics of the protein structure, such as contact order and total contact distance, affect the folding rate. however, the precise relationship between these characteristics and the rate are unknown. a back-propagation neural network was used to quantify this relationship  <cit> . their results showed that correlations exist between these properties and the folding rate with relative errors for predicted results lower than competing methods.

simulating protein motion trajectory
machine learning has also been applied to studying protein folding trajectories. in  <cit>  they use unsupervised learning to cluster similar states and basins present in the folding landscape. they then use this clustering to construct an exploration bias to speed up molecular dynamics simulations. specifically, the exploration bias guides the next basin to jump to in the simulation while ensuring that the entire conformation space is explored. they provide simulation results for an alanine trajectory.

machine learning for prms
many techniques use machine learning to improve prm performance. in this section we briefly highlight some of these methods.

learning sampling methods
in feature sensitive motion planning  <cit> , the planning space is recursively subdivided and machine learning is used to characterize the resulting partitions and select an appropriate prm variant to use in each. a key strength of this approach is its ability to map workspace/c-space topologies for planners to work in. however, it does not adapt planner choices over time.

hybridprm  <cit>  uses reinforcement learning to adaptively select the appropriate sampling method over time. it does so by maintaining a selection probability for each method and updates these probabilities based on the method’s past performance. while learning adapts over time, it is global. it does not perform well when the planning space is heterogeneous, as is the case for most protein folding landscapes.

resampl  <cit>  is similar in spirit to feature sensitive motion planning, but it dynamically generates local regions to plan in. instead of using supervised learning, it uses local region information  to make decisions about how and where to sample, and which samples to connect together.

while the classification of a region may change over time as it is explored, it’s placement does not. thus it cannot adequately adapt if the initial region placement or resolution is not sufficient.

learning connection methods
prior work  <cit>  adaptively selects the appropriate connection method to use over time. as the roadmap is built, it records the performance of several connection methods and with this history, decides which to employ by maintaining a selection probability for each. the main weakness of adaptive neighbor connection  is that it bases its decisions on the performance of connection methods over the entire planning space. this is problematic in protein landscapes that are naturally heterogeneous. therefore, to obtain better results, it became necessary to first partition the space into smaller  regions. this puts greater burden on the user, particularly as the dimensionality of the problem increases. while anc-global was applied to proteins, its performance was limited and so a local learning approach is needed.

learning from trajectories
some methods have been proposed to learn from previous experience. for example, the lightning framework  <cit>  executes two components in parallel: a traditional planning from scratch approach and an approach that extracts and repairs paths from a path history library. it uses the result of the fastest component as the final solution and then adds it to the path history library for future use. note that as the size of the library grows, it becomes impractical to add additional paths to it.

apprenticeship learning  <cit>  also uses existing trajectories to plan motion, but instead aims to learn good trade-offs between different cost functions that describe properties of the trajectories. it learns these trade-offs via inverse reinforcement learning the premise is to learn from a small set of demonstration trajectories instead of a large path library.

methods
our learning framework is a machine learning reinforcement learning method that stems from multi-armed bandit problem algorithms  <cit> . in the multi-armed bandit problem, the goal is to find the arm  with the highest expected payoff during a gambling game of cards as soon as possible and then keep gambling using that best arm. each selected arm is associated with a reward, and the gambler’s objective is to maximize his cumulative expected earnings during the game duration. to do this, the gambler needs to acquire information about arms  while simultaneously optimizing immediate rewards .

we apply this to selecting which connection method to use for a given protein sample/conformation by redefining the reward and cost functions of choosing a connection method. as in the multi-armed bandit problem, we aim to maximize connection success while also exploring other methods that may perform well later on in the connection process.

the local learning approach
in local adaptive neighbor connection , learning is localized to within the vicinity of the current conformation being connected. when choosing a connection method, the current conformation’s neighborhood is dynamically determined. this neighborhood is defined as the set of nearest neighbors given by some distance metric.

we use the performance history of only those connection attempts within this neighborhood to bias learning. thus, our method adapts both spatially and temporarily, and no prior knowledge about the connection method involved is needed. this approach has been introduced for robotic motion planning  <cit> , and here we adapt it to simulate the folding process.

for proteins, we measure performance as a function of the edge weights in the roadmap and the time needed to construct a stable roadmap. we want to balance both compute time and trajectory quality where quality may be inferred from the edge weights . performance is measured only from the dynamically determined neighborhood so learning is continuous and localized.

examplefig.  <dig> example energy landscape and roadmap. two connection methods are used to build a roadmap on the protein’s energy landscape: c
m
a  and c
m
b . when connecting a new conformation q , it is important to learn from local information not global, as c
m
b is more locally successful even though a majority of the edges are from c
m
a




method details
algorithm  <dig> describes the anc-local algorithm as introduced in  <cit> . we initialize all the methods m to the uniform probability and determine the local learning region as defined by the set of nearest neighbors using nflocal in d, where d is a tuple containing the connection method, reward, and cost. for each determined neighbor, we update the probability using the updateprobability function in algorithm  <dig> and make a connection based on the chosen connection method cm.





the updateprobabilty function  is used to continually calculate and update the probabilities of the connection methods. this is where performance is monitored and reinforcement learning takes place.

potential energy computations take up a large portion of the total computation time and thus are a good measure of cost. here, we calculate the cost as the number of potential energy calls incurred by the connection method.

anc-local maintains a weight for each connection method similar to hybrid prm  <cit>  but reconstructed to handle potential energy calculations. these weights keep track of the past performance of each connection method. anc-local initializes each weight wi to  <dig>  based on the weights, anc-local computes in a step-wise manner a probability pi∗ for cmi without considering the cost:

  <dig> pi∗=wi∑j=1mwj+γ1m,i= <dig> ,...,m, 

where wi is the weight of cmi in step t, t is the current connection attempts made, and γ is a fixed constant. the probability pi∗ is a weighted sum of the relative weight of cmi and the uniform distribution. this ensures that each connection method has some chance of being selected.

let xi be the reward for the cmi that was selected: 
  <dig> xi=α+−minyimaxyi−minyi) 

where yi = current edge weight, minyi = minimum edge weight recorded during the current step, maxyi = maximum edge weight recorded during the current step, and α = a constant value used to normalize the reward. all other rewards for that time step are  <dig>  the reward is thus a function of the edge quality  and the local planner’s success.

to update the weights, we first take into account an adjusted reward that is not dependent on the cost accrued: 
  <dig> xi∗=xi/pi∗,i= <dig> ,...m. 

then we update the weights for all the connection methods: 
  <dig> wi=wiexpγxi∗m,i= <dig> ,...m. 

the new weight is the current weight multiplied by a factor that depends on the reward received. the exponential factors enable the weights to adapt quickly.

we now include the cost in the selection probability: 
  <dig> pi=pi∗ci∑j=1mpi∗cj,i= <dig> ,...k. 

where ci is the average cost of attempting to connect i.

RESULTS
in this section, we investigate the performance of anc-local , anc-global , and individual connection methods to model the folding landscape of  <dig> proteins. individual connection methods are k-closest neighbor selection using either cluster, euclidean, or lrmsd distance metric. anc-global and anc-local use these methods as their learning set.

we first establish each method’s ability  to validate against experimental data when available. we then look into the local planner success rate in the context of each strategy. we examine the quality of the resulting folding pathways and the time required by each individual method and look at the cumulative performance of these metrics. we show how anc-local’s learning decisions corroborate with the individual connection method performance outside of the learning framework. in addition, we compare anc-local’s learning performance against anc-global’s learning approach.

experimental setup
we study  <dig> proteins  with 52– <dig> residues. this set contains α, β, and mixed proteins that were also studied by  <cit>  and many have experimentally determined secondary structure formation orders  <cit> . the protein structures were obtained from the protein data bank  <cit> .


for all experiments, we generate conformations using iterative sampling based on rigidity analysis  <cit> . for all connection methods, we use a straight line local planner and attempt to connect to the  <dig> nearest neighbors. for anc-local, we set nflocal to be the  <dig> nearest neighbors based on euclidean distance. this resulted in the best performance in preliminary experiments. we stop construction once we have a stable roadmap.

metrics are computed as follows: 
secondary structure formation order: we compare, when available, the secondary structure formation order predicted by each method to experimental data. we examine shortest paths from all unfolded states to the native state. . we then compare the dominant ordering  to the ordering given by experimental data.

pathway quality: we define folding pathway quality as the weight of each edge  multiplied by the dominance of that edge . this metric is important because it identifies how many edges with low energies are present and how frequently they are used.

having low quality values in our results indicate a better performing connection methods.



validation by secondary structure formation ordertable  <dig> validation of secondary structure formation order to experimental data when available. proteins are ordered by protein length as in table 1


n
n
n


when experimental data was not available, all methods produced the same ordering for  <dig> proteins and different orderings for  <dig> proteins . upon examination of the  <dig> proteins that methods disagree on, we find that anc-local, anc-global, and cluster are always in agreement and euclidean and lrmsd are always in agreement. additionally, disagreements only occur at the end of the pathway; all methods agree on the order of the first elements to form. specifically, all methods find that the central α-helix forms first in 2spz and disagree on the relative ordering of the two terminal α-helices. similarly, all methods find that β-strands  <dig>   <dig>   <dig>   <dig>   <dig> form first  and disagree on the relative ordering of the three α-helices and the remaining β-strand for 1buj.

local planner success rate
recall that a connection method comprises both the distance metric used to identify neighbors to connect and a local planner  that computes a set of intermediate conformations, evaluates their energetic viability, and adds an edge between the two neighbors if such trajectory is feasible. the local planner success rate is a good indicator of the performance of the whole connection process. we measure the local planner success rate as the number of connections made out of the number of connections attempted.
fig.  <dig> local planner success rate for each method over all proteins studied. the local planner success rate of anc-local is greater than all the other methods for  <dig> of the  <dig> proteins studied and comparable for  <dig> of the proteins. note that entries are ordered by the local planner success rate in the context of anc-local



quality, time, and the tradeoff between them
qualityfig.  <dig> roadmap quality for each method over all proteins studied. no single individual connection method performs best across all proteins. anc-local produces the best quality roadmaps for  <dig> of the  <dig> proteins studied. note that entries are ordered by anc-local performance



it is not surprising then that learning methods outperform the best individual connection methods much of the time: anc-global  produces lower weighted pathways than cluster, euclidean, and lrmsd for  <dig> of the  <dig> proteins, and anc-local  for  <dig> of the  <dig> proteins. notice, however, that the type of learning is important. anc-local with its local learning is much more successful than anc-global with its global approach. anc-global outperforms anc-local for only  <dig> protein in the set  and even then the performance is only marginally better while anc-local outperforms anc-global by a large margin for many of the proteins. in fact, anc-local is the best approach for  <dig> out of the  <dig> proteins studied. note that the best performing method in the other  <dig> proteins is not the same : lrmsd produces lower weight pathways for  <dig> proteins , euclidean for  <dig> , and anc-global for  <dig> .

additionally, in  <dig> of the  <dig> proteins where anc-local produces the best quality, it produces significantly better quality than the other methods for  <dig> of the  <dig>  we see an improvement of anc-local over anc-global in terms of quality for  <dig> of the  <dig> proteins studied. of the  <dig> remaining proteins  where anc-global performs better, anc-local performance is comparable.

timefig.  <dig> time for each method over all proteins studied. anc-local performs as well as or better than the best performing method for  <dig> out of  <dig> proteins studied. note that entries are ordered by protein length



to further understand the scalability of these approaches, we plot the time to build a stable roadmap as a function of protein length for both anc-local and its fastest competitor, euclidean. each point in fig.  <dig> corresponds to the time taken for a protein of that length. figure  <dig> also plots a linear regression for each data set. there is a roughly linear relationship between length and running time .
fig.  <dig> time as a function of protein length. anc-local and its fastest competitor, euclidean, display a roughly linear relationship between time and protein length



note that while we see some overhead for learning , other methods may not produce pathways of high quality. for example, acyl-co enzyme  is a protein where only anc-local produced the correct secondary structure formation order as seen in experiment . it is also the furthest outlier above the regression line . while more time is consumed constructing a stable roadmap for this protein, it is time well-spent as it produces the correct secondary structure formation order while others do not.

quality vs. time
finally, we look at each method’s cumulative performance to examine how these two metrics interplay. figure  <dig> shows the ordered ranking of each connection method, anc-global, and anc-local across all  <dig> proteins. for each protein, we assign a rank from  <dig> to  <dig>  to each method for quality and time. the cumulative performance for each method is the average of these rankings.
fig.  <dig> cumulative performance of each method over all proteins studied. methods are ranked from  <dig>  to  <dig> . entries are ordered by cumulative quality ranking. anc-local performs better than the other methods across the entire protein set in terms of quality and second best in terms of time



anc-local performs better than the other connection methods across the entire protein set in terms of quality and second best in terms of time. lrmsd, as expected, is the slowest. while anc-local is not the fastest overall , it does produce the best quality. anc-local is the only method that is able to adapt locally to varying energy landscapes and thus yields higher quality roadmaps. anc-global is the second best in terms of quality but third in terms of time. anc-local outperforms anc-global.
fig.  <dig> quality as a function of protein length. anc-local outperforms and its fastest competitor, euclidean, in terms of quality irrespective of protein length



inspection of anc-local learning choicesfig.  <dig> connection method usage percentage in anc-local across all proteins studied. entries ordered by euclidean usage



for many proteins, anc-local favors a single connection method, but for some , it favors  <dig> connection methods, and for  <dig> proteins , it selects equally among all connection methods. when it favors a subset of the connection methods, it selects the best individual method in both time and quality for  <dig> proteins, the best individual method in time only for  <dig> proteins, and the best individual method in quality only for  <dig> proteins.

CONCLUSIONS
in this work, we present anc-local, an algorithm that uses local learning to select appropriate connection methods in the context of prm roadmap construction for protein folding. our method monitors the performance and cost of various methods within the local neighborhood of the connecting conformation and adjusts their selection probabilities accordingly.

we have demonstrated a clear need for learning  and showed that local learning is superior to global learning . we also showed that our method produces a higher local planner success rate indicating that wise choices in how to use the costly local planner greatly impact performance. in many cases, anc-local produces significantly higher quality results than the other methods. anc-local removes the burden of deciding which method to use, leverages the strengths of the individual input methods, and it is extendable to include other future connection methods.

declarations
publication of this article was funded by nsf awards cns- <dig>  ccf- <dig>  ccf- <dig>  ccf- <dig>  iis- <dig>  iis- <dig>  efri- <dig>  ri- <dig>  by nih nci r <dig> ca090301- <dig>  and by the schlumberger faculty for the future fellowship. this research used resources of the national energy research scientific computing center, which is supported by the office of science of the u.s. department of energy under contract no. de-ac02-05ch <dig> 

this article has been published as part of bmc systems biology vol  <dig> suppl  <dig> 2016: selected articles from the ieee international conference on bioinformatics and biomedicine 2015: systems biology. the full contents of the supplement are available online at http://bmcsystbiol.biomedcentral.com/articles/supplements/volume-10-supplement- <dig> 

authors’ contributions
ce, st and na conceived this study. ce implemented algorithms and experiments and performed all calculations and analysis. st and na aided in the interpretation of data. ce drafted the manuscript with the aid of st and na. all authors have read and approved this manuscript.

competing interests
the authors declare that they have no competing interests.
