BACKGROUND
nuclear organization is of fundamental importance to gene regulation. recently, proximity ligation assays have greatly enhanced our understanding of chromatin organization and its relationship to gene expression  <cit> . here we focus on hi-c, a powerful genome-wide chromosome conformation capture variant, which detects genome-wide chromatin interactions  <cit> . in hi-c, chromatin is cross-linked and dna is fragmented using restriction enzymes, the interacting fragments are ligated forming hybrids that are then sequenced and mapped back to the genome. hi-c is a very powerful technique that has led to important discoveries regarding the organizational principles of the genome. more specifically, hi-c has revealed that the mammalian genome is organized in active and repressed areas   <cit>  that are further divided in “meta-tads”  <cit> , tads  <cit>  and sub-tads  <cit> . tads consist evolutionarily conserved, megabase-scale, non-overlapping areas with increased frequency of intra-domain compared to inter-domain chromatin interactions  <cit> . despite the fact that hi-c is very powerful, it is known to be prone to systematic biases . moreover, as the sequencing costs plummet allowing for increased hi-c resolution, hi-c poses formidable challenges to computational analysis in terms of data storage, memory usage and processing speed. thus, various tools have been recently developed to mitigate biases in hi-c data and make hi-c analysis faster and more efficient in terms of resource usage. hic-box  <cit> , hiclib  <cit>  and hic-pro  <cit>  perform various hi-c analysis tasks, such as alignment and binning of hi-c sequencing reads into hi-c contact matrices, noise reduction and detection of specific dna-dna interactions. hi-corrector  <cit>  has been developed for noise reduction of hi-c data, allowing parallelization and effective memory management, whereas hi-cpipe  <cit>  offers parallelization options and includes steps for alignment, filtering, quality control, detection of specific interactions and visualization of contact matrices. other tools that allow parallelization are hifive  <cit> , homer  <cit>  and hic-pro  <cit> . allele-specific hi-c contact maps can be generated using hic-pro and hicup  <cit>  . tadbit can be used to map raw reads, create interaction matrices, normalize and correct the matrices, call topological domains and build three-dimensional  models based on the hi-c matrices  <cit> . hicdat performs binning, matrix normalization, integration of other data  and visualization  <cit> . hippie offers similar functionality with hicdat and allows detection of specific enhancer-promoter interactions  <cit> . other tools mainly focus on visualization of hi-c data . despite the recent boom in the development of computational methods for hi-c analysis, most of these tools only focus on certain aspects of the analysis, thus failing to encompass the entire hi-c data analysis workflow. more importantly, these tools or pipelines are not easily extensible, and, for any given hi-c task, they do not allow the integration of multiple alternative tools  whose performance could then be qualitatively or quantitatively compared. available tools do not support comprehensive reporting of the parameters used for each task and they do not enable reproducible computational analysis which is an imperative requirement in the era of big data  <cit> , especially given the complexity of hi-c analyses. the recently released hifive is an exception as it offers a galaxy interface  <cit> . however, use of galaxy  <cit>  can become problematic for data-heavy analyses, especially when the remote galaxy server is used.

to facilitate comprehensive processing, reproducibility, parameter exploration and benchmarking of hi-c data analyses, we introduce hic-bench, a data flow platform which is extensible and allows the integration of different task-specific tools. current and future tools related to hi-c analysis can be easily incorporated into hic-bench by implementing simple wrapper scripts. hic-bench covers all current aspects of a standard hi-c analysis workflow, including read mapping, filtering, quality control, binning, noise correction and identification of specific interactions . moreover, it integrates multiple alternative tools for performing each task , while at the same time allowing simultaneous exploration of different parameter settings that are propagated from one task to all subsequent tasks in the pipeline. hic-bench also generates a variety of quality assessment plots and offers other visualization options, such as generating genome browser tracks as well as snapshots using hicplotter. we have built this platform with reproducibility in mind, as all tools, versions and parameter settings are recorded throughout the analysis. hic-bench is released as open-source software and the source code is available on github and zenodo . our team provides installation and usage support.table  <dig> comparison of hic-bench with published hi-c analysis or visualization tools

hic-bench is a comprehensive and feature-rich hi-c analysis pipeline that performs various hi-c tasks by combining our newly-developed tools with existing tools




implementation
the hic-bench workflow
hic-bench is a comprehensive computational pipeline for hi-c sequencing data analysis. it covers all aspects of hi-c data analysis, ranging from alignment of raw reads to boundary-score calculation, tad calling, boundary detection, annotation of specific interactions and enrichment analysis. thus, hic-bench consists the most complete computational hi-c analysis pipeline to date . importantly, every step of the pipeline includes summary statistics  and direct comparative visualization of the results. this feature is essential for quality control and facilitates troubleshooting. the hic-bench workflow  starts with the alignment of hi-c sequencing reads and ends with the annotation and enrichment of specific interactions. more specifically, in the first step, the raw reads  are aligned to the reference genome using bowtie <dig>  <cit>  . the aligned reads are further filtered in order to determine those hi-c read pairs that will be used for downstream analysis . a detailed statistics report showing the numbers and percentages of reads assigned to the different categories is automatically generated in the next step . the reads that satisfy the filtering criteria are used for the creation of hi-c contact matrices . these contact matrices can either be directly visualized in the washu epigenome browser  <cit>  as hi-c tracks , or further processed using three alternative matrix correction methods:  matrix scaling ,  iterative correction   <cit>  and  hicnorm   <cit> . as quality control, plots of the average number of hi-c interactions as a function of the distance between the interacting loci are automatically generated in the next step . the hi-c matrices, before and after matrix correction, are used as inputs in various subsequent pipeline tasks. first, they are directly compared in terms of pearson or spearman correlation  in order to estimate the similarity between hi-c samples. second, they are used for the calculation of boundary scores , identification of topological domains  and comparison of boundaries . third, high-resolution hi-c matrices are used for detection and annotation of specific chromatin interactions , enrichment analysis in transcription factors, chromatin marks or other segmented data  and visualization of chromatin interactions in certain genomic loci of interest . we should note here that hic-bench is totally extensible and customizable as new tools can be easily integrated into the hic-bench workflow . in addition to the multiple alternative tools that can be used to perform certain tasks, hic-bench allows simultaneous exploration of different parameter settings that are propagated from one task to all subsequent tasks in the pipeline . for example, after contact matrices are generated and corrected using alternative methods, hic-bench proceeds with tad calling using all computed matrices as inputs . this unique feature enables the design and execution of complex benchmark studies that may include combinations of multiple tool/parameter choices in each step. hic-bench focuses on the reproducibility of the analysis by keeping records of the source code, tool versions and parameter settings, and it is the only hic-analysis pipeline that allows combinatorial parameter exploration facilitating benchmarking of hi-c analyses.
fig.  <dig> hic-bench workflow. raw reads  are aligned and then filtered . filtered reads are used for the creation of hi-c track files  that can be directly uploaded to the washu epigenome browser  <cit> . a report with a statistics summary of filtered hi-c reads, is also automatically generated . raw hi-c matrices  are normalized using  scaling ,  iterative correction   <cit>  or  hicnorm   <cit> . a report with the plots of the normalized hi-c counts as function of the distance between the interacting partners  is automatically generated for all methods. the resulting matrices are compared across all samples in terms of pearson and spearman correlation . boundary scores are calculated and the corresponding report with the principal component analysis  is automatically generated . domains are identified using various tad calling algorithms  followed by comparison of tad boundaries . a report with the statistics of boundary comparison is also automatically generated. hi-c visualization of user-defined genomic regions is performed using hicplotter   <cit> . specific chromatin interactions  are detected and annotated . finally, enrichment of top interactions in certain chromatin marks, transcription factors etc. provided by the user, is automatically calculated 


fig.  <dig> 
a computational trails. each combination of tools and parameter settings can be imagined as a unique computational “trail” that is executed simultaneously with all the other possible trails to create a collection of output objects. as an example, one of these possible trails is presented in red. the raw reads were aligned, filtered and then binned in 40 kb resolution matrices. our own naïve matrix scaling method was then used for matrix correction and domains were called using topdom  <cit> . b hic-bench pipeline task architecture. all pipeline tasks are performed by a single r script, “pipeline-master-explorer.r”. this script generates output objects based on all combinations of input objects and parameter scripts while taking into account the split variable, group variable and tuple settings. the output objects are stored in the corresponding “results” directory. as an example, domain calling for imr <dig> is presented. the filtered reads of the imr <dig> hi-c sample  are used as input. the pipeline-master-explorer script tests if tad calling with these settings has been performed and if not it calls the domain calling wrapper script  with the corresponding parameters . after the task is complete, the output is stored in the corresponding “results” directory




the hic-bench toolkit
hic-bench performs various tasks of hi-c analysis ranging from read alignment to annotation of specific interactions and visualization. we have developed two new tools, gtools-hic and hic-matrix, to execute the multiple tasks in the hic-bench pipeline, but we have also integrated existing tools to allow comparative and complementary analyses and facilitate benchmarking. more specifically, the alignment task is performed either with bowtie <dig>  <cit>  or with the “align” function of gtools-hic, our newest addition to genomictools  <cit> . likewise, filtering, creation of hi-c tracks and generation of hi-c contact matrices are performed using the functions “filter”, “bin/convert” and “matrix” of gtools-hic respectively. for advanced users, we have implemented a series of novel features for these common hi-c analysis tasks. for example, the operation “matrix” of gtools-hic allows generation of arbitrary chimeric hi-c contact matrices, a feature particularly useful for the study of the effect of chromosomal translocations on chromatin interactions. another example is the generation of distance-restricted matrices  in order to save storage space and reduce memory usage at fine resolutions. for matrix correction we use either published algorithms   <cit> , hicnorm  <cit> ) or our “naïve scaling” method where we divide the hi-c counts by  the total number of  reads, and  the “effective length”  <cit>  of each genomic bin. we also integrated published tad callers like di  <cit> , armatus  <cit> , topdom  <cit> , insulation index   <cit>  and our own tad calling method  implemented as the “domains” operation in hic-matrix. additionally, the “domains” operation produces genome-wide boundary scores using multiple methods and allowing flexibility in choosing parameters. boundaries are simply defined as local maxima of the boundary scores. for the detection of specific interactions, we introduce the “loops” function of hic-matrix, while genomictools is used for annotation of these interactions with gene names, chip-seq and other user-defined data. finally, we implemented a wrapper for hicplotter, taking advantage of its advanced visualization features in order to allow the user to quickly generate snapshots of areas of interest in batch. the hic-bench toolkit is summarized in table  <dig>  all the tools we developed appear in bold. further information on the toolkit is provided in the user manual found online and in the supplemental information section.table  <dig> the hic-bench toolkit

gtools-hic
gtools-hic
gtools-hic
hic-matrix
hic-matrix
genomic-tools
the hic-bench toolkit consists mostly of newly-developed tools  but we have also incorporated existing tools to allow comparisons and benchmarking




main concepts and pipeline architecture
we built our platform based on principles outlined in scientific workflow systems such as kepler  <cit> , taverna  <cit>  and vistrails  <cit> . the main idea behind our platform is the ability to track data provenance  <cit> , the origin of the data, computational tasks, tool versions and parameter settings used in order to generate a certain output  from a given input . thus, our pipeline ensures reproducibility which is a particularly important feature for such a complex computational task. in addition, hic-bench enables combinatorial analysis and parameter exploration by implementing the idea of computational “trails”: a unique combination of inputs, tools and parameter values can be imagined as a unique  trail that is followed simultaneously with all the other possible trails in order to generate a collection of output objects . our platform consists of three main components:  data,  code and  pipelines. these components are organized in respective directories in our local repository, and synchronized with a remote github repository for public access. the data directory is used to store data that would be used by any analysis, for example genome-related data, such as dna sequences and indices , gene annotations and, in general, any type of data that is required for the analysis. the code directory is used to store scripts, source code and executables. more details about the directory structure can be found in the user manual. finally, the “pipelines” directory is used to store the structure of each pipeline. here, we will focus on our hi-c pipeline, but we have also implemented a chip-seq pipeline, which is very useful for integrating ctcf and histone modification chip-seq data with hi-c data. the structure of the pipeline is presented to the user as a numbered list of directories, each one corresponding to one operation  of the pipeline. as shown in fig.  <dig>  our hi-c pipeline currently consists of several tasks starting with alignment and reaching completion with the identification and annotation of specific dna-dna interactions and annotations with chip-seq and other genome-wide data . we will examine these tasks in detail in the results section of this manuscript.

parameter exploration, input and output objects
in conventional computational pipelines, several computational tasks  are executed on their required inputs. however, in existing genomics pipelines, each task generates a single result object  which is then used by downstream tasks. to allow full parameter  exploration, we introduce instead a data flow model, where every task may accommodate an arbitrary number of output objects. downstream tasks will then operate on all computed objects generated by the tasks they depend on. pipeline tasks are implemented as shown in the diagram of fig. 2b. first, input objects are filtered according to user-specified criteria . then, pipeline-master-explorer  generates the commands that create all desired output objects. in principle, all combinations of input objects with all parameter settings will be created, subject to user-defined filtering criteria. in the interest of extensibility, new pipeline tasks can be conveniently implemented using a single-line pipeline-master-explorer command , provided that wrapper scripts for each task  have been properly set up. in the simplest scenario, any task in our pipeline will generate computational objects for each combination of parameter file and input objects obtained from upstream tasks. for example, suppose the aligned reads from  <dig> hi-c datasets are filtered using three different parameter settings, and that we need to create contact matrices at four resolutions . then, the number of output objects  will be  <dig> . although many computational scenarios can be realized by this simple one-to-one mapping of input–output objects, more complex scenarios are frequently encountered, as described in the next section.

filtering, splitting and grouping input objects into new output objects
oftentimes, a simple one-to-one mapping of input objects to output objects is not desirable. for this reason, we introduce the concepts of filtering, splitting and grouping of input objects which are used to modify the behavior of pipeline-master-explorer . filtering is required when some input objects are not relevant for a given task, e.g., tad calling is not performed on 1 mb-resolution contact matrices, and specific dna-dna interactions are not meaningful for resolutions greater than 10–20 kb. splitting is necessary in some cases: for example, we split the input objects by genome assembly  when comparing contact matrices or domains across samples, since only matrices or domains from the same genome assembly can be compared directly. in our platform, the user is allowed to split a collection of input objects by any variable contained in the sample sheet , thus allowing user-defined splits of the data, such as by cell type or treatment. complementary to the splitting concept, grouping permits the aggregation of a collection of input objects  into a single output object. for example, the user may want to create genome browser tracks or contact matrices of combined technical and/or biological replicates, or group all input objects  together in tasks such as principal component analysis  or alignment/filtering statistics.

combinatorial objects
even after introducing the concepts described above, more complex scenarios are possible as some tasks require the input of pairs  of objects. this feature has also been implemented in our pipeline  and is currently used in the compare-matrices and compare-boundaries tasks. however, it should be utilized wisely  because it may lead to a combinatorial “explosion” of output objects.

parameter scripts
the design of our platform is motivated by the need to facilitate the use of different parameter settings for each pipeline task. for this reason, we have implemented wrapper scripts for each tool/method used in each task. for example, we have implemented a wrapper script for alignment, filtering, correcting contact matrices using ic or hicnorm , tad calling using armatus  <cit> , topdom  <cit> , di  <cit>  and insulation index   <cit>  . the main motivation is to hide most of the complexity inside the wrapper script and allow the user to modify the parameters using a simple but flexible parameter script. unlike static parameter files, parameter scripts allow for dynamic calculation of parameters based on certain input variables . within this framework, by adding and/or modifying simple parameter scripts, the user can explore the effect of different parameters  on the task directly affected by these parameters, and  on all dependent downstream tasks. additionally, these parameter scripts serve as a record of parameters and tool versions that were used to produce the results, facilitating analysis reproducibility as well as documentation in scientific reports and manuscripts.

results stored as computational trails
all the concepts described above have been implemented in a single r script named pipeline-master-explorer. this script maintains a database of input-output objects for each task, stored in a hidden directory under results . it also creates a “run” script which is executed in order to generate all the desired results. all results are stored in the results directory in a tree structure that reveals the computational trail for each object . therefore, the user can easily infer how each object was created, including what inputs and what parameters were used.

initiating a new reproducible analysis
in the interest of data analysis reproducibility, any new analysis requires creating a copy of the code and pipeline structure into a desired location, effectively creating a branch. this way, any changes in the code repository will not affect the analysis and conversely, the user can customize the code according to the requirements of each project without modifying the code repository. copying of the code and initiating a new analysis is done simply by invoking the script “pipeline-new-analysis.tcsh” as described in the user manual.

pipeline tasks
a pipeline consists of a number of  ordered tasks that can be described by a directed acyclic graph which defines all dependencies. hic-bench implements a total of  <dig> tasks as shown in the workflow of fig.  <dig>  in the analysis directory structure, each task is assigned its own subdirectory found inside the pipeline directory starting from the top level. this directory includes a symbolic link to the inputs of the analysis , a link to the code, a directory  containing links to all dependencies, a directory containing parameter scripts  and a “run” script which can be used to generate all the results of this task. the “run” scripts of each task are executed in the specified order by the master “run” script located at the top level .

input data and the sample sheet
before performing any analysis, a computational pipeline needs input data. all input data for our pipeline tasks are stored in their own “inputs” directory accessible at the top level  and via symbolic links from within the directories assigned to each task to allow easy access to the corresponding input data. a “readme” file explains how to organize the input data inside the inputs directory . briefly, the fastq subdirectory is used to store all fastq files, organized into one subdirectory per sample. then, the sample sheet needs to be generated. this can be done automatically using the “create-sample-sheet.tcsh” script, but the user can also manually modify and expand the sample sheet with features beyond what is required. currently required features are the sample name , fastq files , genome assembly version  and restriction enzyme name . adding more features, such as different group names , allows the user to perform more sophisticated downstream analyses, such as grouping replicates for generating genome browser tracks, or splitting samples by genome assembly to compare boundaries .

executing the pipeline
the entire pipeline can be executed automatically by the “pipeline-execute.tcsh” script, as shown below: code/code.main/pipeline‐execute<projectname><usere‐mailaddress> 


where < project name > will be substituted by the name of the project and < user e-mail address > by the preferred e-mail address of the person who runs the analysis in order to be notified upon completion. the “pipeline-execute.tcsh” script essentially executes the “run” script for each task . at the completion of every task, the log files of all finished jobs are inspected for error messages. if error messages are found, the pipeline aborts with an error message.

timestamping
besides creating the “run” script used to generate all results, the “pipeline-master-explorer.r” script, also checks whether existing output objects are up-to-date when compared to their dependencies . currently, the pipelines are set up so that out-of-date objects are not deleted and recomputed automatically, but only presented to the user as a warning. the user can then choose to delete them manually and re-compute. the reason for this is to protect the user against accidentally repeating computationally demanding tasks  without given first the chance to review why certain objects may be out-of-date. from a more philosophical point of view, and in the interest of keeping a record of all computations , the user may never want to modify parameter files or the code for a given project, but instead only add new parameter files. then, no object will be out-of-date, and only new objects will need to be recomputed every time.

alignment and filtering
paired-end reads were mapped to the reference genome  using bowtie <dig>  <cit> . reads with low mapping quality  were discarded. local alignments of input read pairs were performed as they consist of chimeric reads between two  interacting fragments. this approach yielded a high percentage of mappable reads  for all datasets . mapped read pairs were subsequently filtered for known artifacts of the hi-c protocol such as self-ligation, mapping too far from the enzyme’s known cutting sites etc. more specifically, reads mapping in multiple locations on the reference genome , double-sided reads that mapped to the same enzyme fragment , reads whose 5’-end mapped too far  from the enzyme cutting site, reads with only one mappable end  and unmapped reads , were discarded. read pairs that corresponded to regions that were very close  in linear distance on the genome as well as duplicate read pairs  were also discarded. in additional file 4: figure s <dig>  we show detailed paired-end read statistics for the hi-c datasets used in this study. we include the read numbers  and their corresponding percentages . eventually, approximately 10–50% of paired-reads passed all filtering criteria and were used for downstream analysis . the statistics report is automatically generated for all input samples. the tools and parameter settings used for the alignment and filtering tasks are fully customizable and can be defined in the corresponding parameter files.

contact matrix generation, normalization and correction
the read-pairs that passed the filtering task were used to create hi-c contact matrices for all samples. the elements of each contact matrix correspond to pairs of genomic “bins”. the value in each matrix element is the number of read pairs aligning to the corresponding genomic regions. in this study, we used various resolutions, ranging from fine  to coarse . the resulting matrices either remained unprocessed , or they were processed using different correction methods including hicnorm  <cit> , iterative correction   <cit>  as well as “naïve scaling”. in additional file 5: figure s <dig>  we present the average hi-c count as a function of the distance between the interacting fragments, separately for each hi-c matrix for not corrected  and ic-corrected matrices.

comparison of contact matrices
our pipeline allows direct comparison and visualization of the generated hi-c contact matrices. more specifically, using our hic-matrix tool, all pairwise pearson and spearman correlations were automatically calculated for each  input sample,  resolution, and  matrix correction method. the corresponding correlograms were automatically generated using the corrgram r package  <cit> . a representative example is shown in additional file 6: figure s <dig>  the correlograms summarizing the pairwise pearson correlations for all samples used in this study are presented before and after matrix correction using the iterative correction algorithm. these plots are very useful because the user can quickly assess the similarity between technical and biological replicates as well as differences between various cell types. as shown before , iterative correction improves the correlation between enzymes at the expense of a decreased correlation between samples prepared using the same enzyme.

boundary scores
topological domains  are defined as genomic neighborhoods of highly interacting chromatin, with relatively more infrequent inter-domain interactions  <cit> . topological domains are demarcated by boundaries, i.e., genomic regions bound by insulators thus hampering dna contacts across adjacent domains. for each genomic position, in a given resolution , we define a “boundary score” to quantify the insulation strength of this position. the higher the boundary score, the higher the insulation strength and the probability that this region actually acts as a boundary between adjacent domains. the idea of boundary scores is further illustrated in additional file 7: figure s <dig>  where two adjacent tads are shown. the upstream tad on the left  is separated from the downstream tad on the right  by a boundary . we define two parameters, the distance from the diagonal of the hi-c contact matrix to be excluded from the boundary score calculation   and the maximum distance from the diagonal to be considered . the corresponding parameter values can be selected by the user. for this analysis, we used δ =  <dig> and d = 2 mb as suggested before  <cit> . in addition to the published directionality index  <cit> , we defined and computed the “inter”, “intra-max” and “ratio” scores, defined as follows: inter=meanx 
 intramax=maxmeanl,meanr 
 ratio=intramax/inter 


principal component analysis  of boundary scores across samples in this study, before and after matrix correction, shows that biological replicates tend to cluster together, either in the case of filtered or corrected  matrices .

topological domains
topologically-associated domains  are increasingly recognized as an important feature of genome organization  <cit> . despite the importance of tads in genome organization, very few hi-c pipelines have integrated tad calling . in hic-bench, we have integrated tad calling as a pipeline task and we demonstrate this integration using different tad callers:  armatus  <cit> ,  topdom  <cit> ,  di  <cit> ,  insulation index   <cit>  and  our own hic-matrix . our pipeline makes it straightforward to plug in additional tad callers, by installing these tools and setting up the corresponding wrapper scripts. hic-bench also facilitates the direct comparison of tads across samples by automatically calculating the number of tad boundaries and all the pairwise overlaps of tad boundaries across all inputs, generating the corresponding graphs . we define boundary overlap as the ratio of the intersection of boundaries between two replicates  over the union of boundaries in these two replicates, as shown in the equation below: boundary_overlap=a∩b/a∪b 


for the boundary overlap calculation, we extended each boundary by 40 kb on both sides . the fact that hic-bench allows simultaneous exploration of all parameter settings for all installed tad-calling algorithms, greatly facilitates parameter exploration, optimization as well as assessment of algorithm effectiveness. pairwise comparison of boundaries  across samples is shown in fig.  <dig> and additional file 9: figure s <dig> fig.  <dig> comparison of topological domain calling methods subject to hi-c contact matrix preprocessing by simple filtering or iterative correction . the methods were assessed in terms of boundary overlap between replicates , change  in mean boundary overlap after matrix correction , change  in standard deviation of mean overlap across replicates after matrix correction  and number of identified topological domains per cell type . the different colors correspond to the different callers. gradients of the same color are used for the different values of the same parameter, ranging from low  to high  values. the tad callers along with the corresponding parameter settings are presented in the legend. for this analysis all available read pairs were used




visualization
in our pipeline, we also take advantage of the great visualization capabilities offered by the recently released hicplotter  <cit> , in order to allow the user to visualize hi-c contact matrices along with tads  for multiple genomic regions of interest. the user can also add binding profiles in bedgraph format for factors , boundary scores, histone marks of interest  etc. an example is shown in additional file 10: figure s <dig>  where an area of the contact matrix of human embryonic stem cells   is presented along with the corresponding tads , various boundary scores, the ctcf binding profile and annotations of selected genomic elements, before and after matrix correction . the integration of hicplotter in our pipeline, allows the user to easily create publication-quality figures for multiple areas of interest simultaneously.

specific interactions, annotations and enrichments
the plummeting costs of next-generation sequencing have resulted in a dramatic increase in the resolution achieved in hi-c experiments. while the original hi-c study reported interaction matrices of 1 mb resolution  <cit> , recently 1 kb resolution was reported  <cit> . thus, the characterization and annotation of specific genomic interactions from hi-c data is an important feature of a modern hi-c analysis pipeline. hic-bench generates a table of the interacting loci based on parameters defined by the user. these parameters include the resolution, the lowest number of read pairs required per interacting area as well as the minimum distance between the interacting partners. the resulting table contains the coordinates of the interacting loci, the raw count of interactions between them, the number of interactions after “scaling” and the number of interactions between the partners after distance normalization . this table is further annotated with the gene names or the factors  and histone modification marks  that overlap with the interacting loci. the user can provide bed files with the features of interest to be used for annotation. as an example, the enrichment of chromatin marks in the top  <dig>  chromatin interactions in the h <dig> and imr <dig> samples is presented in additional file 11: figure s <dig> 

software requirements
the main software requirements are: bowtie <dig> aligner  <cit> , python  , r   <cit>  and various r packages . more details on the versions of the packages can be found in the user manual ). in addition, installation of mirnylib python library  <cit>  is required for matrix balancing based on ic . the pipeline has been tested on a high-performance computing cluster based on sun grid engine . the operating system used was redhat linux gnu .

RESULTS
we used hic-bench to analyze several published hi-c datasets and the results of our analysis are presented below. additionally, we performed a comprehensive benchmark of existing and new tad callers exploring different matrix correction methods, parameter settings and sequencing depths. our results can be reproduced by re-running the corresponding pipeline snapshot available upon request as a single compressed archive file .

comprehensive reanalysis of available hi-c datasets using hic-bench
our platform is designed to facilitate and streamline the analysis of a large number of available hi-c datasets in batch. thus, we collected and comprehensively analyzed multiple hi-c samples from three large studies  <cit> . from the first study we analyzed imr <dig>  samples, from the second we analyzed hi-c samples from lymphoblastoid cells , human lung fibroblasts ), erythroleukemia cells , chronic myelogenous leukemia  cells  and keratinocytes , and from the third one, we analyzed samples from human embryonic stem cells  and all the embryonic stem-cell derived lineages mentioned, including mesendoderm, mesenchymal stem cells, neural progenitor cells and trophectoderm cells. all datasets yielded at least  <dig> million usable intra-chromosomal read pairs in at least two biological replicates. we performed extensive quality control on all datasets, calculating the read counts and percentages per classification category , the attenuation of hi-c signal over genomic distance , the correlation of hi-c matrices before and after matrix correction , the similarity of boundary scores  and all pairwise boundary overlaps across samples . in addition, we performed a comprehensive benchmarking of our own and published tad callers, across all reanalyzed hi-c datasets. the results of our benchmark are presented in the following sections.

iterative correction of hi-c contact matrices improves reproducibility of tad boundaries
iterative correction has been shown to correct for known biases in hi-c  <cit> . thus, we hypothesized that ic may increase the reproducibility of tad calling. we performed a comprehensive analysis calculating the tad boundary overlaps for biological replicates of the same sample , using different tad callers and different main parameter values for each tad caller . after comparing tad boundary overlaps between filtered  and ic-corrected matrices, we observed an improvement in the boundary overlap when corrected matrices were used, irrespective of tad caller and parameter settings . the only exception was di. careful examination of the overlaps per sample revealed that ic introduced outliers only in the case of di . we hypothesize that ic may occasionally negatively affect the computation of the directionality index, especially because its calculation depends on a smaller number of bins  compared to the rest of the methods . in addition to the increase in the mean value of boundary overlap upon correction, we observed that the standard deviation of boundary overlaps among replicates decreased  . while this seems to be the trend for almost all tad caller/parameter value combinations, the effect of correction in variance is more profound in certain cases  than others. it is also worth mentioning that increased size of the insulation window , the resolution parameter γ  or the distance d  may result in certain cases in increased boundary overlap , but this is not generalizable . interestingly, increased tad boundary overlap does not necessarily mean increased consistency in the number of predicted tads across sample types, as would be expected since tads are largely invariant across cell types  <cit> . for example, the tad calling algorithm which is based on insulation score , predicted similar tad overlaps and similar tad numbers for different insulation windows , whereas armatus performed well in terms of tad boundary reproducibility  but the corresponding predicted tad numbers varied considerably . this may be partly due to the nature of the armatus algorithm, as it has been built to reveal multiple levels of chromatin organization . we conclude that while iterative correction improves the reproducibility of tad boundary detection across replicates, the number of predicted tads should be also taken into account when selecting tad calling method for downstream analysis. the method of choice should be the one that is robust in terms of both reproducibility and number of predicted tads.

increased sequencing depth improves the reproducibility of tad boundaries
after selecting the parameter setting that optimized tad boundary overlap between biological replicates of the same sample per tad caller, we also investigated the effect of sequencing depth on the reproducibility of tad boundary detection. since some of the input samples were limited to only  <dig> million usable intra-chromosomal hi-c read pairs, we resampled  <dig> million,  <dig> million and  <dig> million read pairs from each sample and evaluated the effect of increasing sequencing depth on tad boundary reproducibility. the results are depicted in fig. 4a. we noticed that increased sequencing depth resulted in increased tad boundary overlap, regardless of the tad calling algorithm used . as far as the tad numbers are concerned, increased sequencing depth decreased tad number variability for certain callers  but not in all cases  . in many cases, increased sequencing depth, decreased the variance of tad boundary overlap among replicates . in summary, based on this benchmark, we recommend that hi-c samples should be sufficiently sequenced as sequencing depth seems to affect tad calling reproducibility.fig.  <dig> comparison of topological domain calling methods for different preprocessing method and sequencing depth. tad calling methods were assessed in terms of boundary overlap between replicates , number of identified topological domains  and boundary overlap across replicates upon increasing sequencing depth  for different matrix preprocessing  and different sequencing depths . for tad calling, only the optimal caller/parameter value pairs are shown . the boxplot and line colors correspond to the different tad callers




CONCLUSIONS
recently, several computational tools and pipelines have been developed for hi-c analysis. some focus on matrix correction, others on detection of specific chromatin interactions and their differences across conditions and others on visualization of these interactions. however, very few of these tools offer a complete hi-c analysis , addressing tasks which range from alignment to interaction annotation. hic-bench is a comprehensive hi-c analysis pipeline with the ability to process many samples in parallel, record and visualize the results in each task, thus facilitating troubleshooting and further analyses. it integrates both existing tools but also new tools that we developed to perform certain hi-c analysis tasks. in addition, hic-bench focuses on parameter exploration, reproducibility and extensibility. all parameter settings used in each pipeline task are automatically recorded, while future tools can be easily added using the supplied wrapper template. more importantly, hic-bench is the only hi-c pipeline so far that allows extensive parameter exploration, thus facilitating direct comparison of the results obtained by different tools, methods and parameters. this unique feature helps users test the robustness of the analysis, optimize the parameter settings and eventually obtain reliable and biologically meaningful results. to demonstrate the usefulness of hic-bench, we performed a comprehensive benchmark of popular and newly-introduced tad callers, varying the matrix preprocessing , the sequencing depth, and the value of the main parameter of each tad caller, which is usually the window used for the calculation of directionality index or insulation score. we found that the matrix correction has a positive effect on the boundary overlap between replicates and that increased sequencing depth leads to higher boundary overlap.

in conclusion, hic-bench is an easy-to-use framework for systematic, comprehensive, integrative and reproducible analysis of hi-c datasets. we expect that use of our platform will facilitate current analyses and enable scientists to further develop and test interesting hypotheses in the field of chromatin organization and epigenetics.

additional files

additional file 1: hic-bench manual. 


additional file 2: table s <dig>  hic-bench task implementation. the table summarizes how the pipeline tasks are implemented, which are the requirements for their execution and how they are handled by the pipeline-master-explorer script. the first column lists all the tasks performed by the pipeline ranging from alignment to annotation. the second column lists the input directory required for each task while the third one lists the parameter files. certain tasks depend on the reference genome , thus the genome assembly acts as split variable . in some tasks, replicates can be grouped using the group variable . pairwise comparisons between replicates or samples are also allowed using tuples . the last column lists the full pipeline-master-explorer command for each pipeline task. 


additional file 3: table s <dig>  hic-bench input-output objects. the table summarizes the inputs and outputs of the tad-calling task using three different methods with parameter values stored in the params files . the first column describes the tree structure of the input directories that are essentially the different hi-c matrices for each sample, before  and after matrix correction using different methods . the second column lists all the different parameter scripts and the third column corresponds to the tree structure of the generated output objects. 


additional file 4: figure s <dig>  hi-c reads filtering statistics. number  and percentage  of the various read categories identified during filtering for all datasets used in the study. mappable reads were over 95% in all samples. duplicate , non-uniquely mappable , single-end mappable  and unmapped reads  were discarded. self-ligation products  and reads mapping too far  from restriction sites or too close to one another  were also discarded. only double-sided uniquely mappable cis  and trans  read pairs were used for downstream analysis. the x axis represents either the raw read number  or the percentage of reads  falling within each of the categories described in the legend. the y axis represents the samples. 


additional file 5: figure s <dig>  matrix statistics. normalized hi-c counts are presented as a function of the distance between the interacting partners for all samples and correction methods. the hi-c samples analyzed were gm <dig> , hescs  , mesenchymal cells , mesendoderm , neural progenitors , trophectoderm , imr <dig> , k <dig> , kbm <dig>  and nhek . the matrices were either unprocessed   or corrected using ic . the y axis represents the normalized count of hi-c interactions and the x axis the distance between the interacting partners in kilobases. 


additional file 6: figure s <dig>  pairwise pearson correlation of hi-c matrices. correlograms summarizing all pairwise pearson correlations for all hi-c samples used in this study: raw  matrices  and matrices after iterative correction . dark red indicates strong positive correlation and dark blue strong negative. the resolution of the matrices is 40 kb. 


additional file 7: figure s <dig>  boundary score calculation. two adjacent topological domains  are depicted. the left domain  is separated from the right domain  by a boundary . the areas of more-frequent intra-domain interactions are in red. the area of less-frequent cross-domain  interactions is x. we also introduce parameter d which is the maximum distance from the diagonal to be considered for the calculation of boundary scores . 


additional file 8: figure s <dig>  principal component analysis of boundary scores. boundary scores were calculated using ratio score, for all samples either before   or after iterative correction  . 


additional file 9: figure s <dig>  pairwise overlaps of tad boundaries. the pairwise overlaps of tad boundaries are shown for all samples of this study, after calling boundaries using hicratio . before tad calling, the hi-c matrices were either unprocessed  or corrected using iterative correction  . 


additional file 10: figure s <dig>  visualization of tads and certain areas of interest. hic-bench integrates hicplotter  <cit>  and it offers the ability to easily prepare publication-quality figures. we present the area surrounding nanog, a gene of particular importance for the maintenance of pluripotency. the hi-c matrix corresponding to the chr12:3940389– <dig> genomic region is presented for h <dig> cells, before and after matrix correction. the matrix is also rotated 45° to facilitate tad visualization. various boundary scores  are shown as individual tracks along with ctcf binding. the location of nanog is presented as a blue line. 


additional file 11: figure s <dig>  enrichment of chromatin interactions in human fibroblasts  and embryonic stem cells . the enrichment of certain chromatin marks and ctcf in the top  <dig>  chromatin interactions in the imr <dig> and h <dig> samples is shown. deep blue and larger circle size indicate higher enrichment. 




abbreviations
didirectionality index

ic or iceiterative correction

pcaprincipal component analysis

tadtopological domain or topologically associating domain

