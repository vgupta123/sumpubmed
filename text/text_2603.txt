BACKGROUND
quantitative gene expression experiments provide key information for elucidating biological pathways and understanding diseases. many methods have been developed over time, from hybridization based, northern blotting, real-time polymerase chain reaction, high throughput microarrays and serial analysis of gene expression , up to modern synthesis based sequencing methods . microarrays are a popular technology for large-scale measurement of gene expression. substantial public repositories have been set up to capture the wealth of information on gene expression generated by researchers world-wide: the national center for biotechnology information's gene expression omnibus   <cit> , the european bioinformatics institute's arrayexpress  <cit>  and the dna data bank of japan's center for information biology gene expression database   <cit> . there are, however, several essential pieces of information that are needed to make these measurements useful for anyone other than the original researcher. first, a clear description of the process is necessary to make results reproducible: this includes description of the sample preparation, microarray platform and reporter  sequences. the second essential piece of information is a description of the data processing methods, from the raw image to the expression level. both steps have been subject to rigorous standardization efforts in the past  <cit> . while it is still challenging to compare measurements, another essential piece of information has received far less attention: clinical data describing the origin and characteristics of the samples, which is often sparse, inconsistent or simply absent.

due to the public availability of large microarray repositories and reasonable standardization of measurement values, many 'meta-analyses' and meta-analysis systems emerged  <cit> . these approaches usually merged the lists of differentially expressed genes obtained from previous studies. while this led to emphasis and validation of former results, new insights were less frequent. in order to derive genuinely new results from previous experiments, utilization of raw measurement values is almost inevitable.

however, analysing the raw data is challenging, as it requires  consistent and high quality probe annotations for all involved platforms,  appropriate cross-platform normalization methods, and  detailed sample annotations. only few were able to report success with this approach  <cit> . while mapping reporter sequences to genes no longer poses a problem, with high quality gene transcripts and several updated re-mappings at hand, normalization is more difficult, as many different factors come into play  <cit> . furthermore, sample annotations in popular large collections are hardly structured or consistent across studies and often lack important details.

we addressed the problem of annotating gene expression samples with a consistent set of variables on a large number of existing studies. while this seems to be a daunting task, there were sufficient similarities between samples to make it possible to annotate many samples at once. moreover, we hypothesized that it was possible to obtain high quality annotations by non-expert individuals who received proper training.

previous manual or automatic annotation attempts relied or tried to rely on domain experts to do the annotation  <cit> , which was costly and time consuming. however, it has also been shown previously that using students for annotation is a worthwhile alternative to employing experts  <cit>  and that the students themselves quickly become experts for annotating a certain disease. we were confident that students could become adept for the task of annotating a specific set of variables in a specific field.

in the past, several groups proposed annotation tools for microarray experiments  <cit> . these approaches tried to either automatically standardize the existing information or create collaborative platforms together with controlled vocabularies or ontologies to manually create consistent and reusable sample annotations. while there have been some successful cases of automatic term normalization in a smaller scale  <cit> , attempts to automatically curate geo, the largest public repository, were of limited success, especially when expecting detailed clinical annotation  <cit> . the large diversity of information and unguided annotation currently present in large repositories call for taking the best ideas from these previous approaches and combining them into a new tool.

RESULTS
our database currently contains the  <dig> most popular platforms from geo,  <dig>  studies, and  <dig>  samples. we also imported a total of  <dig>  billion raw measurement values that can be used for new analyses with the help of a cross-platform probe annotation and a cross-platform normalization tool.

we were able to efficiently annotate more than  <dig>  samples. more than half of these samples have been redundantly annotated by at least two different annotators. within four weeks of work of one full time and one part time annotator and a following five weeks of work with four full time annotators, a total of almost half a million variable assignments were made. on average, every sample received  <dig> annotations.

the most frequently available variables were 'tissue' with  <dig>  assignments, followed by 'disease state' with  <dig>  annotations, 'sample type' with  <dig>  annotations, and 'cell line' with  <dig> . information about genetic modification is also readily available in  <dig>  annotations. other frequently available variables were 'treatment', 'time series', 'gender', 'patient age', 'lymphatic spread', 'estrogen receptor status' and 'tumor type'.

CONCLUSIONS
many possibilities of further processing are within reach with a method at hand that facilitates an efficient and rich annotation of existing gene expression data. one obvious use case is the regrouping of existing samples to perform new virtual studies. another possible extension is the employment of this or a similar system directly at the public repositories so the uploaded data is annotated by the original submitter.

the potential benefit of these detailed annotations is clear: while finding a suitable set of samples across studies was virtually impossible using existing unstructured information alone, we are now able to easily find and compare, for example,  <dig>  breast cancer samples with  <dig> normal breast samples, or  <dig> er+ versus  <dig> er- samples. figure  <dig> shows a screenshot from the annotation explorer interface that uses the annotated and normalized samples to compare gene expression of brca <dig> across several annotated disease states. in a future version, the annotated variables will be made available in an ontology browser to allow for more powerful searching, by expanding the concepts' associations.

we have shown the feasibility of collecting publicly available information about previous microarray experiments and subjecting them to a consistent and efficient annotation. with the current speed and quality of annotation and a total of ten full-time annotators we project the time to annotate all of geo  with two redundant annotations per sample to be  <dig> weeks. in the future, we plan to assemble new large datasets and perform new differential expression analyses, avoiding the high cost of sample collection, preparation and hybridization by exploiting existing data.

