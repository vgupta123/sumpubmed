BACKGROUND
microarray has become an important tool for identifying genes that discriminate sample classes because of its power of monitoring the expression levels of thousands of genes in a single experiment. finding discriminative genes with microarray data is actually the feature selection problem in classification theory. from the machine learning point of view, it is critical since the microarray datasets usually contain a small number of experiments  and a large number of genes  in each experiment. the selected highly discriminative genes after filtering out those non-representative genes which may dilute the pattern in classification computation can be further studied for the investigation on the biological mechanisms that are responsible for class distinction.

a number of efforts have been put in searching effective gene selection methods . due to the small-sample size and high-dimension properties of the tissue classification problem, it is not difficult to find a small feature subset that can perfectly discriminate all the samples  <cit> . in fact, theoretical study in  <cit>  showed that even for the non-informative, randomly generated dataset, the expected size of a feature subset that can linearly discriminate all the n samples is just / <dig>  in microarray data analysis, there can be a large number of highly discriminative subsets containing only a couple of genes; and each individual gene in such a subset is not necessarily highly discriminative. for example, we observed by exhaustive search that there are as many as  <dig>  perfect 3-gene subsets for classification with the weighted voting method proposed by golub et al and with their proposed training-test split  <cit> ; and these gene subsets cover  <dig>  genes . this observation suggests that a method of finding a highly discriminative compact gene subset is not enough. the variability of the subsets found by such a method likely hinders the discovery of real interaction among the genes given that the method is usually sensitive to both the choice of samples and noise in the microarray data.

the fundamental limit and challenges mentioned above motivates us to design more robust methods by taking into account the expression similarity information among genes. in this paper, we identify a series of discriminative gene clusters by running clustering and feature selection processes iteratively, where the centroids of the clusters are used to form predictors. this work also shows that the predictor constructed in this way is more stable and less sensitive to the choice of training samples. because biological functions are usually resulted from collective behavior and coordinated expression of a group of genes rather than that of an individual gene, genes grouped according to their co-expression pattern may be more powerful in revealing gene regulation mechanisms.

our approach to generate discriminative gene clusters is a combination of supervised and unsupervised technique in recent years, jornsten and yu  <cit>  and dettling and buhlmann  <cit>  proposed similar combination approaches. however, there are major differences between their methods and our method. we use a multivariate approach for cluster selection, while dettling and buhlmann  <cit>  employed a univariate approach, which assumes the independence of the contribution of clusters to classification. although such hypothesis reduces computational complexity for large datasets, the accuracy is compromised since the complex biological interaction among gene clusters is not properly reflected. we exploit a multivariate approach in the content of gene expression analysis since it accounts for the joint contribution of clusters to classification.

our method differs from  <cit>  in the following two aspects: although both works adopt multivariate approach, first of all, in their information-based approach, clustering and cluster selection are done simultaneously, resulting in a set of clusters optimizing the minimum description length. in comparison, our computation-oriented approach is a refining process where clustering and cluster selection are performed alternatively in each iteration step with better and better results. secondly, the clusters generated with jornsten and yu's approach include both active and inactive ones. here, active clusters are those whose centroids are relevant to classification, and inactive ones are not. our method is essentially a backward approach  <cit> . it iteratively eliminates the less active clusters and re-clusters the remaining genes in the active clusters, reducing the negative influence of non-discriminative clusters on the classification.

our program outputs a series of cluster sets that have increasing discrimination power for training samples without losing prediction power on the test samples, as indicated in our experimental results. it achieved similar or better prediction accuracy than the known methods aforementioned for most of the tested datasets in our validation process. more importantly, our test shows that the centroids of the output clusters using different sets of training samples are stable and consistently achieve significant proximity to the global optimal gene clusters obtained by using all the samples. another advantage of our method is that it provides researchers with flexibility to decide which cluster set should be chosen for their purpose.

RESULTS
we implemented the algorithm  as matlab functions. it runs on a pc with the windows operating system. the svm program written by gavin cawley was downloaded from the website . in this section, we present the detailed test results on both simulated and leukemia aml/all datasets  <cit> . we also have tested our method on other real datasets and compared the performance of our algorithm with those reported in the previous literature. the details of the performance measures are described in the method section.

simulated datasets
we generated  <dig> simulated binary classification datasets using a simple stochastic model. each simulated dataset contains  <dig> samples evenly split into two classes. both training and test samples contain  <dig> samples in each class.

each dataset contains of  <dig> genes evenly divided into four gene clusters. two of the four clusters are relevant to classification and these two discriminative clusters c <dig> and c <dig> contribute to classification independently. their centroids x and x are generated according to the sample class labels. each component of x in a position is generated according to normal distributions n or n depending on whether the corresponding sample is in class  <dig> or class - <dig>  while each component of x generated according to n if the sample is in class  <dig> and n otherwise. similarly, the centroids of the non-discriminative clusters c <dig> and c <dig> are generated according to the normal distribution n and n regardless of the samples' class. for each i =  <dig>   <dig>   <dig>   <dig>  the expression values of a gene in the cluster ci are generated according to the multivariate normal distribution n,di4), where di = minj ≠ id, x).

we run our algorithm with the input gene set s contains all the  <dig> genes for each of the  <dig> simulated datasets. the performance results are summarized in fig.  <dig>  we observed that the classification performance of the generated clusters keeps increasing as the iteration process goes. the average classification accuracy θtest of these tests jumps from  <dig>  up to  <dig>  ; and the classification accuracy θtrain on training samples goes up from  <dig>  to  <dig>  .

we also observed that more and more truly discriminative genes are identified in the active clusters as the algorithm proceeds. since the genes in the discriminative clusters are known in each simulated dataset, we computed the ratio psim=|si∩||si| of the truly discriminative genes over all the genes in si for each iteration i. the active clusters output psim , just before the algorithm terminates is about  <dig>  . recall that, at each iteration i, the algorithm generates κ =  <dig> active gene clusters since the number of training samples nr =  <dig> for each simulated dataset. we found that at each iteration i, the centroids of two active clusters are very close to x and x, the centriods of the discriminative clusters in the model. this is reflected by the indistinguishably small p-value ρs  calculated based on d¯. here d¯ is the 'average' euclidean distance of centroids between an active cluster in ai and its closest cluster in Δ' = {c <dig>  c2}.

in the same time, the centroids of active clusters become more and more distinguishable from each other, increasingly close to the average pairwise distance of all  <dig> genes, and such trend can also be reflected by the increasing p-value ρs  from  <dig>  up to  <dig>  , calculated based on δ^, the average euclidean distance between the centroids of active clusters in ai. meanwhile, the silhouette width ω¯ of active clusters in ai increases from  <dig>  to  <dig> .

leukemia dataset
leukemia aml/all dataset  <cit>  contains the expression values of  <dig>  human genes in  <dig> acute lymphoblastic leukemia  and  <dig> acute myeloid leukemia  tissue samples. after performing the threshold filtering and logarithmic transformation procedure, we obtained a reduced dataset with only  <dig>  genes.

here, we validate our algorithm by using three-fold cross validation. in each run, we randomly selected two third of the samples as the training samples and the rest as the testing samples. the samples of different classes are kept proportional in the training and test samples. the resulting dataset was further normalized by rescaling the variance of expression values of each gene to  <dig> in the training samples, and then applying the same rescaling factor to the expression values of that gene in the test samples.

we conducted the three-fold cross validation for  <dig> times. to reduce computational cost, we restrict our algorithm on small portions of discriminative genes. in each run, the algorithm starts with the input gene set s consisting of the  <dig> genes  that are highly correlated with the training samples' classification in terms of the correlation metric proposed in  <cit> .

fig.  <dig> summarizes the values of the different performance indicators. the average classification accuracy θtrain on the training samples ranges from  <dig>  up to  <dig> ; and the average classification accuracy θtest on the test samples increase slightly from  <dig>  to  <dig>  . these results show that the centroids of the clusters generated in different iteration steps discriminate the training samples better and better without significant decrease of its generalization ability.

for the evaluation of our algorithm, we searched for perfect 3-gene subsets, which can be used to perfectly classify all  <dig> samples using the weighted voting classifier trained on all the samples. this search resulted in  <dig>  perfect subsets. we selected  <dig>  genes gi  with highest occurrence frequency to form the cluster set Δ′ <dig> = {{gi}| <dig> ≤ i ≤ 48} for comparison with the clusters generated by our algorithm.

we also evaluate our algorithm using another cluster set Δ′ <dig>  the final set of active clusters generated by our algorithm with s' as the input gene subset and with all the  <dig> samples as the training samples, where s' is the set of the  <dig> genes  that are highly correlated with the aml/all classes in terms of the correlation metric proposed in  <cit> .

probably because of the selection sensitivity of the correlation metric of  <cit>  resulting from small sample size, the gene sets that are selected according to different training-test splits do not have many genes in common. in all the  <dig> validation experiments, only  <dig> genes appearing in every input gene set s. this number is quite small compared with  <dig> , the number of the genes appearing in some input gene sets . by contrast, the centroids of clusters in the set ai generated in each of iterations of our algorithm in different runs are significantly similar to the selected discriminative genes in Δ′ <dig> and Δ′ <dig> at most iteration steps. this is reflected by the very small p-values ρs computed based on d¯ and d¯, which range from  <dig>  × 10- <dig> to  <dig>  × 10- <dig>  and from  <dig>  × 10- <dig> to  <dig>  × 10- <dig>  respectively. the above observation strongly suggests the stability associated with discriminative clusters rather than with individual discriminative genes. such stability is one of the main advantages of our method.

we further studied the biological function of genes in the active clusters using gene ontology , focusing on the biological processes located at the fifth level of the go hierarchy. for the set of all genes from active clusters in ai, we find its enriched biological processes by calculating the hyper-geometric p-value, then convert the p-value into a log score s by s = -log <dig> . table  <dig> gives the top four biological processes that are most significantly enriched in the active clusters in the final iteration, in terms of the score averaged from the  <dig> validation experiments. all four processes are frequently associated with leukemia. in addition, we inspected the change of proportion of the genes of the four processes in the active clusters during refinement iterations. the proportions are also averaged over the  <dig> validation experiments. fig  <dig> shows that when the active clusters contain less than two third genes in input gene set s, the average gene proportions of all four processes monotonically increase until the last iteration. such convergence strongly suggests that our method can indeed refine clusters into biologically meaningful ones. interestingly, processes of inflammatory response and response to wounding showed very similar convergence patterns. in fact, these two processes are closely related. the same holds for biological processes of regulation of catalytic activity and positive regulation of metabolic process.

the performance analysis on other real datasets
besides the above dataset, we also tested our algorithm on seven other datasets. the descriptions of these datasets are listed in additional file  <dig>  altogether, we derive  <dig> classification studies from the  <dig> datasets.

we preprocessed each dataset by applying filtering and logarithm transformation if necessary. for each classification study, we run our algorithm  <dig> times by choosing random training-test splits in the same way as the leukemia dataset described in the last subsection. the performance of our method is summarized in table  <dig>  in the table, there are two columns for each performance measure, indicating the average values of the corresponding measures at the first and last iteration step of our algorithm. because the exhaustive search of the most frequent globally optimal genes for constructing Δ′ <dig> is time-consuming, we only compare the active clusters with Δ′ <dig> constructed as follows: 1) we apply our algorithm on all samples in the dataset and 2) use the active clusters of the last iteration as Δ′ <dig> 

                              test
the classification accuracy θtest on the test samples shows that among  <dig> of  <dig> classification studies, the prediction performance of active clusters in ai increases slightly from the start to the end of each execution, which are highlighted in the table. the value of θtest for the remaining three studies  decrease slightly. the above observations indicate that for all datasets we tested, there is no significant decrease in the generalization ability of the active clusters in ai obtained in each iteration step. the classification performance θtrain on the training samples increases in all of the  <dig> studies, which indicates that the separation of the training samples improves for all studies.

all the  <dig> input gene sets s vary a lot in different runs for each study. there are only  <dig> % to  <dig> % of all the genes appearing in all the  <dig> input gene sets s, while at least  <dig> % to  <dig> % genes appear in some input gene sets. by contrast, the centroids of clusters in ai generated by our algorithm at each iteration step i are stably close to the optimal centroids of clusters in Δ′ <dig> as reflected by the p-values ρs ranging from  <dig>  × 10- <dig> to  <dig>  × 10- <dig> at the first iteration step and those ranging from  <dig>  × 10- <dig> to  <dig>  × 10- <dig> in the last iteration step. the consistent closeness of the clusters generated in different repeats can also be reflected in the standard deviation of ρs, which are limited from  <dig>  to  <dig>  times of the absolute values of ρs in the first iteration step and  <dig>  to  <dig>  times at the last iteration step.

during the generation process, the p-values ρall of average pairwise distance δ^ among centroids of clusters in ai keeps increasing for all  <dig> studies , and the average silhouette width of active clusters ω¯ keeps increasing for all the  <dig> studies . this indicates that the clusters in ai are more and more distinct in general.

in summary, our test shows that on real microarray datasets, our algorithm is able to generate clusters that separate the training samples with increasing prediction accuracy and closeness to known optimal clusters. such discriminative cluster refinement is consistent with what we have observed on simulated datasets.

comparing the classification performance to other studies
in this section, we compare the cross validation performance of our method with previous works reported in  <cit> . for the purpose of comparison, we converted the classification performance from the classification accuracy θtest into the error rate. table  <dig> summarizes the comparison of our algorithm  with others by the cross validation error rates. it is difficult to make direct comparisons with other approaches in the literature, because the specific data sets or data preparation are not always available. however, the performances our method is in general comparable to others. in the comparison, the dlbcl and carcinoma datasets are validated using leave-one-out validation; and the remaining datasets are validated using three-fold cross validation.

dettling and buhlmann  <cit>  reported the error rate of their algorithm for different datasets. they employed nearest neighbors and aggregated trees as the classifiers in their three-fold cross validation test. for the leukemia aml/all dataset, our algorithm seems to achieve a slightly lower error rate than theirs. in the colon and prostate datasets, the error rate of our algorithm lies between that of theirs. for the breast dataset, the error rate is significantly higher than that of dettling and buhlmann's. however, we obtained the performance using all the original  <dig> samples. the error rate in each test ranges from  <dig>  to  <dig> . according to  <cit> , at least  <dig> out of the  <dig> samples are inherently erroneous. in comparison, dettling and buhlmann  <cit>  used the  <dig> good samples selected by  <cit> , and the error rate ranges from  <dig>  to  <dig> . the  <dig> samples used in dettling and buhlmann  <cit>  consists none of the above  <dig> erroneous samples. thus, we believe that the performances of ours and dettling and buhlmann's are still comparable for the breast dataset.

for the dlbcl dataset, the leave-one-out performance of shipp et al.  <cit>  is in our performance range. for carcinoma dataset, jaeger et al.  <cit>  achieved perfect leave-one-out performance, and our best performance can match theirs. for the colon dataset, both ours and dettling and buhlmann's error rate are higher than jornsten and yu's.

we also test the performance of the multiple-class version of our method against other methods. for the leukemia three-class dataset, our method is comparable to jornsten and yu's method. however, for the srbct multi class dataset, our algorithm seems had a slightly higher error rate than that of dettling and buhlmann's.

CONCLUSIONS
due to the small-sample-high-dimension nature of the microarray dataset, it is not difficult to find highly discriminative gene subsets of small size. however, if a gene selection process is unstable with the choice of training samples, the biological significance of the resulting gene subsets is often not guaranteed. in this paper, instead of finding individual discriminative genes or gene subsets, we propose a novel backward approach for generating a series of highly discriminative gene clusters. compared to selection of individual discriminative genes, genes grouped in these clusters are more stable when subject to change of training samples. therefore they could provide more convincing support to gene interactions that are associated with the sample classes. in future, we will work with biologists to study the biomedical implication of these clusters.

regarding to the classification performance, the gene clusters produced by our approach can generally achieve good cross validation performance compared to the existing methods for most of datasets we tested. more importantly, our test experiments show that regardless of the choice of training samples, the centroids of the clusters generated are stable and significantly close to the known optimal gene clusters found using all the samples. all these indicate that our approach is promising. however, the current version of our algorithm is time-consuming. in future, the computational efficiency will be investigated. on the other hand, we used k-means algorithm, a typical partitioning based clustering method to seek a certain number of clusters that minimize the sum of squared distances between each gene and its centroid. the drawback for k-means is the subjective specification of input parameters such as the number of clusters and initial centroid locations. for unknown microarray datasets, such information is unavailable. furthermore, different input parameters may result in significantly different clustering results. k-means can only converge to local optima, rather than the global optimum. in order to address these problems associated with k-means clustering. we plan to apply a novel clustering method based on random matrix theory   <cit>  which is completely objective and do not require the specification of cluster number and initial centroid locations. rmt method avoids being trapped into local optima. furthermore, most previous clustering methods including k-means and hierarchical clustering partition members into non-overlapping groups. the rmt method allows the same genes in multiple groups to reflect the fact that a single gene may contribute to multiple biological pathways.

in order to test the discriminative power of a certain gene cluster, additional criteria established by statistical analysis should also be conducted to identify and remove inactive cluster. for example, gene expression pattern observed in the active clusters should be less likely to appear in the control set. chi square test might be used to test the significance. some data normalization technique may be considered in the preprocessing step to improve the data quality. furthermore, more suitable backward feature selection method needs to be exploited so that the gene clustering and cluster selection processes can be integrated better. our approach provides a flexible framework that allows us to test the performance of various computing modules in a various ways of combinations.

