BACKGROUND
due to the exponential growth of the biomedical literature, text mining tools have become crucial to process all available information contained in literature databases such as pubmed. text mining can offer automatically generated summaries to the expert user who needs to retrieve all knowledge on a certain topic or stay up-to-date with recent findings. the level of detail of the extracted information ranges from simple binary interactions, such as protein-protein interactions  <cit>  or gene-disease associations  <cit> , to a more complex event representation  <cit> . all these relations typically involve one or multiple genes or gene products .

ggps are represented by gene symbols or synonyms and can be linked to database identifiers. for instance, esr- <dig> refers to entrez gene id  <dig>  similarly, the full term human esr- <dig> gene can be linked to the same id. however, a complex noun phrase should not always be resolved to the embedded gene symbol. for example, the phrase esr- <dig> inhibitor refers to an entirely different molecular entity.

understanding complex noun phrases with embedded gene symbols is thus crucial for a correct interpretation of text mining results  <cit> . such non-causal relations between a noun phrase and the embedded gene symbol are being referred to as entity relations  <cit> , or, in previous work, static relations  <cit> . this type of relationship may also occur between two different noun phrases within one sentence. typically, such relations hold between two molecular entities without necessary implication of causality or change. entity relation types include equivalence, locus, protein-component, member-collection and subunit-complex.

the rel supporting task  <cit>  of the bionlp shared task  of  <dig>  <cit>  was focused on extracting entity relations, contributing to the general goal of the st to support more fine-grained text predictions. furthermore, by formally defining these relations, a text mining module is able to establish semantic links between various molecular entities found in text .

a more detailed explanation of the entity relations and the corresponding datasets is provided in the next section. additionally, we describe two machine learning frameworks applied to the prediction of such relations. the turku event extraction system  provides a largely unified extraction approach for all bionlp st' <dig> sub-challenges, with relatively minor adaptations specifically for the rel task. the ghent text mining  framework on the other hand, contains several novel rel-specific modules, including the deduction and application of semantic similarities between domain terms, measured using latent semantic analysis and a manually annotated corpus.

further, we show how feature selection techniques, in combination with the getm framework, can be used to analyse and visualise the most discriminative patterns in the data in a structured fashion, offering valuable insights into the classification challenge. finally, we analyse the performance and strengths of both frameworks on different datasets, analysing the contribution of both the term detection and the relation extraction modules. we conclude the paper by discussing the usage of entity relations for large-scale integrative data mining tools.

data
entity relations are non-causal relations between a ggp symbol  and a domain term. domain terms are usually general words denoting biomolecular concepts such as promoter or complex, occasionally such concepts have a specific name such as nf-kappab. a few examples of entity relations are depicted in table  <dig> 

examples of entity relation types, including both embedded and non-embedded cases. ggps are in italic and domain terms are delimited by square brackets.

there are two related corpora publicly available with annotations for entity relations: the data of the bionlp st' <dig> and the more extensive genia relation corpus. the characteristics of these two corpora are summarized in table  <dig> and  <dig>  the st data is divided into three distinct datasets: training , development  and test data   <cit> . the training set of the genia relation corpus corresponds to the training set of the st data, and the genia relation test data corresponds to the st development data. in both corpora, valid entity relations involve exactly one ggp and one domain term and both occur within a single sentence. gold standard relations are provided for the training and development set, allowing the application of machine learning algorithms to produce predictions for the test set.

number of positive instances of the various types in the entity relation corpora. st refers to the bionlp' <dig> shared task data, while genia refers to the genia relation corpus. the latter corpus is further divided into embedded  and non-embedded  cases. datasets sufficiently large for classification analysis are in bold.

characteristics of the two different entity relations corpora. the number of relation types only includes those that were used in the classification experiments.

in the st data, two types of entity relations are defined. a subunit-complex relation holds between a protein complex and its subunits, while a protein-component relation is less specific and involves a ggp and its components, such as protein domains or gene promoters. the genia relation corpus contains several additional types, including equivalence and member-collection, which expresses a relationship between e.g. a gene family and its members. this corpus is further divided into 'embedded' and 'non-embedded' relations, the first being relations occurring within a noun phrase  <cit> , and the latter containing broader relations between nominals  <cit> . all these datasets are available at the genia project webpage: http://www-tsujii.is.s.u-tokyo.ac.jp/genia.

methods
a supervised learning framework is a perfect match to deal with entity relations, as statistical properties can be drawn from the grammatical and lexical structures in the training data, providing a way to generate plausible hypotheses on unseen data. in this section we describe two machine learning frameworks developed for the extraction of entity relations: the winning system of the st  <dig>  implemented by turku university  <cit> , and the system that achieved second place, by ghent university  <cit> .

tees
the turku event extraction system  is a generalized biomedical relation extraction tool based on a unified, extensible graph representation, where word entities constitute the nodes, and edges between them define the relations . the system consists of a pipeline of three main components based on support vector machines   <cit> . first, entity nodes are predicted for each word token in a sentence. then, argument edges are predicted for each pair of such nodes. finally, the resulting graph is 'pulled apart' by classifying subgraphs as actual events/relations or not. these main steps can be followed by several post-processing steps, such as prediction of speculation and negation, or conversion to the st file format. for a detailed description of the general system, see  <cit> .

parsing
tees relies heavily on syntactic dependency parses, represented as graphs of word token nodes and dependency edges. the system uses the charniak-johnson parser  <cit>  with david mc-closky's biomodel  <cit>  trained on the genia corpus  <cit>  and unlabeled pubmed articles. the parse trees produced by the charniak-johnson parser are further processed with the stanford conversion tool  <cit> , creating a dependency parse  <cit> . the parse is the main source of features for all the svm classification steps.

term detection
the term detection component classifies each word token in the sentence as being a domain term or not. multi-token terms are always represented by a single token, their syntactic head. for term detection, features are mostly based on dependency paths, generated up to a depth of three and centered on the candidate token. word tokens have many attributes that are used as features, such as part-of-speech tags, dependency types, the word itself and its stem using the porter stemmer  <cit> . all examples are classified with the svmmulticlass software, using a linear kernel  <cit> .

the term detection module is optimized in isolation, but this optimal f-score may not always be best for overall system performance. a recall boosting step multiplies the negative class weight with a set value, trading precision for increased recall. this results in more entities being available for the edge detection step, and a higher final f-score.

edge detection
after the prediction of domain terms, the edge detection component predicts argument edges between the given ggp names and the predicted terms. one potential edge candidate is generated for each ggp-term pair in a sentence, and these are classified as subunit-complex, protein-component, member-collection, equivalence or negative. for edge detection, features are largely based on the shortest connecting path of dependencies between the two nodes of respectively the ggp and the domain term. all examples are classified with the same svm software as for term detection. since entity relations are pairwise, no further processing is required, and the resulting graph can be directly converted to the st format.

getm
the getm framework is based on a previously introduced event extraction system  <cit>  which was significantly extended with rel-specific modules. it first calculates semantic similarities between domain terms. these similarities are used to construct generalized feature vectors that represent the semantic and grammatical information contained in the training sentences. the rich feature vectors are then subjected to feature selection and subsequently used for training a binary svm for each entity relation type. finally, for each selected sentence and each ggp occurrence, a suitable domain term is selected within a certain search window. the flowchart of the getm framework is depicted in figure  <dig> 

in contrast to tees, the getm framework, with its rel-specific modules, has not been published yet . in the next sections, we therefor describe the getm rel-specific modules in a bit more detail.

semantic analysis
to fully understand the relationship between a ggp and a domain term, it is necessary to account for the usage of synonyms and lexical variants in human language. to capture this textual variation, two strategies were implemented for creating semantic lexicons, grouping similar words together. the first method takes advantage of manual annotations of semantic categories in  <dig> articles. the second method relies on statistical properties of nearly  <dig>  articles.

the genia term corpus contains manual annotations of various domain terms such as promoters, complexes and other biological entities  <cit> . these annotations are used to link certain lexical patterns to semantic categories, such as dna-domain-or-region and protein-family-or-group. the genia term corpus consists of the same  <dig> abstracts as the combined training and development st data, and is therefore a very suitable additional data source.

in addition to using the genia term corpus, semantic spaces were calculated to deduce the underlying similarities between domain terms. a semantic space can be defined as a mathematical representation of a text corpus, containing high-dimensional vectors that capture the context in which certain words are used. similar vectors then represent semantically similar words. by applying semantic spaces in this study, we aim at finding clusters of closely related biomolecular concepts, such as complex and heterodimer.

in a first step, a large-scale corpus is collected containing  <dig>  pubmed articles concerning the topic of the genia corpus: human transcription factor blood cells. next, all words are transformed to their lowercase variants and the porter stemming algorithm is used for generalization purposes  <cit> .

the actual semantic spaces are then built with the open-source s-space package  <cit> . this package contains implementations of several semantic algorithms that have been extensively documented, tested and validated. we have experimented with latent semantic analysis   <cit> , random indexing  <cit> , hal  <cit>  and coals  <cit> . by running these semantic algorithms on the nearly  <dig> thousand articles, we obtain datasets of terms linked to their semantic vectors. in a final step, these semantic vectors are clustered into meaningful groups. clustering was performed using the markov cluster algorithm  <cit>  with the cosine similarity measure.

to assess the best fitting semantic algorithm for this specific classification task, a score heuristic s was implemented to evaluate the resulting clusters:

  s=unknown×hg×reliability 

  reliability=knownknown+unknown 

some terms in the clusters can be assigned a gold classification label by looking at the training portion of the genia relation corpus. for example, the domain term "complex" is always associated with a subunit-complex relation. the number of such gold labels in each cluster is represented by known and the homogeneity of a cluster  expresses the internal agreement between these labels. the homogeneity hg is multiplied by the number of unlabeled test terms  to assess the predictive value of the cluster. the reliability  of the cluster further expresses the percentage of known labels versus predicted ones. clusters with relatively more known labels are deemed to be more reliable, unless the labels are highly contradicting, which would result in less homogeneity and thus a lower score. the final score metric s calculates one score for each cluster, and a clustering result is scored as the sum of all clusters.

evaluation using the score heuristic s clearly indicates that the semantic algorithms random indexing and hal produce less useful results than lsa and coals. after manual inspection of the clusters, lsa was chosen as the preferred method to produce semantic vectors. figure  <dig> depicts some of the resulting clusters.

machine learning module
the machine learning component of the framework identifies entity relations by analysing lexical and grammatical patterns in sentences containing ggps.

to capture the lexical information for each sentence containing at least one ggp, bag-of-word  features are derived. in addition, n-grams, containing n consecutive words, are extracted from the sentence. all lexical information in the feature vectors is stemmed with the porter algorithm and generalized by blinding the ggp symbol with protx and all other co-occurring ggp symbols with exprotx, while at the same time keeping the content of these symbols as separate features. furthermore, terms occurring in the semantic lexicons  are blinded with the corresponding cluster number or category for additional, generalized features.

for extracting grammatical patterns, the same parsing techniques are employed as used by tees. additionally, the semantic mappings are applied to the patterns derived from dependency graphs, and one additional generalization is obtained by substituting words for their part-of-speech tags. a few example features are depicted in figure  <dig>  with generalized features in italic.

to reduce the dimensionality of the resulting datasets, ensemble feature selection is performed with linear svms as described in  <cit> . this feature selection methodology has been shown to enhance performance of text mining tools in a supervised learning setting and can also serve to gain a better insight into the task at hand. hence, feature clouds have been automatically generated and manually analysed to improve the feature generation module. for example, figure  <dig> depicts the feature cloud of the most informative feature patterns when predicting embedded protein-component relations for the genia relation dataset. features indicating positive examples  include words of the semantic class 'protein-domain-or-region' and the lexical pattern  'human protx promoter'. negative features  include the 2-gram 'protx subunit' and the semantic class 'protein-complex', which would in turn be a positive hint for the subunit-complex type. notably, there are almost no syntactic features in the top most informative features. this is a property inherent to the prediction of the embedded class of entity relations, for which the close lexical context of the ggp is the most determining factor. in contrast, the non-embedded types do rely more on the syntactic structure of the sentence.

the final feature vectors are classified using an svm with a radial basis function  as kernel. the rbf kernel has been evaluated to perform best in this framework which employs several binary, type-specific classifiers in parallel  <cit> . an optimal parameter setting  for this kernel was obtained by 5-fold cross-validation on the training data.

term detection
in the getm framework, sentences are selected for classification if they contain at least one ggp. when the sentence is classified as containing a certain type of entity relation, it is necessary to also identify the exact domain term that is related to the ggp. to this end, a pattern matching algorithm was designed that applies a rule-based search algorithm within a given window  around the gene symbol. the search algorithm employs dictionaries obtained from the training data in combination with information from the semantic lexicons.

RESULTS
in this section we first present the official st' <dig> results. we then analyse these results and the underlying frameworks in more detail by benchmarking on the genia relation corpus. a hybrid framework is created and validated on the  st test set. finally, we experiment with further combinations of the frameworks and achieve either high-precision or high-recall results.

official results of the st'11
performance on the st' <dig> test set, measured by precision, recall and their harmonic mean, the f-score . the first few rows indicate the official results of the tees  and getm frameworks. next, the performance of the hybrid  system is shown. finally, the two last rows report on the performance of creating the intersection and the union of tees and the hybrid system.

a performance gap of  <dig> percentage points is measured between the best and second system, and another discrepancy of  <dig>  percentage points between the second and third system. the relatively high performance of tees is remarkable, as this system has not been developed specifically for the detection of entity relations, but rather is able to generalize quite well to different text mining challenges. in contrast, the getm framework contains specific algorithms designed for the entity relations classification task such as the creation of the semantic lexicons. in this study, we aim at elucidating the performance discrepancy between the first two systems, by analysing whether most errors originate from the term recognition step or from the relation extraction module.

analysis on the genia relation corpus
to analyse the performance discrepancy between tees and the getm framework, a number of analyses were performed on the genia relation corpus. the genia relation corpus was chosen for two main reasons. first, its scope is broader and the annotations cover several additional types of entity relations compared to the st data . secondly, the availability of gold standard domain annotations in the genia relation corpus allow for benchmarking the relation extraction module in isolation. this also means that the results obtained here are not directly comparable to the results on the st data, because the latter corpus does not include gold domain terms.

for the new experiments, tees has remained unchanged, while the feature generation module of the getm framework has been modified slightly to benefit from the specific properties of the genia relation corpus, optimizing feature sets for the different entity relation types . due to the available gold standard terms, the getm framework now classifies sentences with exactly one ggp and one term, rather than just sentences containing one ggp. consequently, additional features describing the lexical and semantic content of the domain terms are added to the feature vectors.

the classification parameters of tees have been optimized on the st development corpus, which corresponds to the genia relation test set. for the getm framework, the best feature set was selected after several analyses on the same data. these settings result in slightly optimistic performance values for both systems, when benchmarking on the genia relation data. however, the resulting overfitting only accounts for a few percentage points in f-score, and because these analyses are used for comparison between tees and getm, this is not considered to be a problem. this is even more the case because the hidden st test set is the de facto standard for benchmarking and comparing different systems. the results on this dataset are described in the next section.

for the classification experiments on the genia relation corpus, separate runs were performed for 'embedded' and 'non-embedded' relations. the performance results are depicted in table  <dig>  from this table, we learn that both frameworks perform almost equally well, with a small advantage for tees. the huge discrepancy, as observed in the official results, has disappeared. this can be explained by the availability of the gold standard domain terms, but may also be due to the added relation types. for example, getm performs worse than tees for the subunit-complex relation type in both the st and genia evaluations, but performs better for the equivalence type, which is not included in the st evaluation. to further isolate and analyse the influence of the term detection module specifically, we will create a hybrid framework and evaluate it on the st corpus in the next section.

performance on the genia relation corpus for embedded  and non-embedded  relation types, measured by precision, recall and their harmonic mean, the f-score.

another important result emerging from the analysis on the genia relation corpus, is the performance discrepancy between embedded and non-embedded types. global performance reaches around 93-94% f-score for the embedded cases, while the non-embedded relations are predicted with an f-score of 74-76%. the embedded cases are indeed less grammatically complex than the non-embedded ones. interestingly, they do represent an important sub-challenge of entity relations. when combining text mining results with public databases, automatically tagged ggp symbols need to be resolved to the correct record in the database. ggp symbols are often extracted by named entity recognition software such as banner  <cit> , which applies statistical models for the recognition of ggp symbols in text, and might sometimes tag a whole noun phrase rather than just the embedded ggp name. embedded relation types formally describe the relationship between e.g. esr- <dig> and esr- <dig> promoter, thus providing an automatic way of dealing with these strings and enabling a meaningful integration between text and database records. even taking the previously described effects of overfitting into account, embedded relations can still be predicted with an f-score above 90%.

the recent release of the evex dataset, containing biomolecular event predictions for millions of pubmed articles  <cit> , provides an interesting opportunity to overlay these entity relations with event predictions. in this setting, entity relations could provide hubs between events concerning similar molecular entities, they could improve on the level of detail provided by the events  <cit>  and finally they would be useful for large-scale normalization and integration with external databases such as entrez gene from ncbi  <cit>  or uniprot  <cit> .

combining the two frameworks
to test the hypothesis that the getm framework lags behind because of its term detection module, a hybrid framework was created by combining the term detection module of tees with the getm relation detection module. this framework is tested on the official st test data and it performs almost equally well as the original submission by tees . this result clearly shows the huge impact of the term detection module on the final results, as the relation extraction modules perform almost equally well. apparently, the svm-based term detection module of tees performs much better than the rule-based approach implemented in the getm framework, resulting in a much higher global performance result on the st data.

even though the performance of tees and the hybrid framework are very similar, there is still a considerable variability in the underlying predictions, as the relation extraction component differs significantly. consequently, we can further experiment with ensemble methods to combine both systems. considering we only have access to two high-performing systems, the options for creating combinations are limited.

first, the intersection of the two systems was created. comparing two relations across the different frameworks is straightforward because they use the same ggp occurrences  and the same domain terms . the results are shown in table  <dig>  obviously, an intersection could never improve on recall compared to the original submission by tees, but we do find a precision increase of  <dig>  and  <dig>  percentage points for protein-component and subunit-component respectively. the resulting f-score is  <dig>  percentage points higher. while this is marginally higher than the original submission with tees, the difference is not statistically significant, and this new framework is more complex as it needs to train two different classifiers. finally, it is important to note that any machine learning framework can in theory be tuned to achieve either high recall or high precision by applying the well-known precision-recall trade-off  <cit> .

the union of tees and the hybrid system was subsequently constructed aiming at higher recall rates while still benefiting from the relatively high precision rates of both systems. however, this approach seems to include many irrelevant false positives . recall rises with  <dig>  and  <dig>  percentage points for protein-component and subunit-component respectively, but f-score drops with  <dig>  percentage points compared to the original submission with tees.

CONCLUSIONS
we have presented the two best frameworks for the bionlp st' <dig> rel challenge, discussing the application of machine learning techniques, semantic spaces and feature selection for this task. we further analysed the performance discrepancy of  <dig> percentage points between these two frameworks as observed on the st data. benchmarking on a related and more extensive dataset has guided the construction of a hybrid framework which combines the tees term recognition module with the getm relation detection module. from these experiments, it became clear that the term detection module has a much higher impact than the relation extraction module on the final performance, and future development efforts in this field should thus focus more on accurate detection of the domain terms.

the extraction of entity relations from text has several interesting applications, such as retrieval and semantic labeling of various molecular concepts within and across articles. additionally, the prediction of entity relations can also have an impact on the prediction of biomolecular events from text. finally, knowledge on entity relations is a crucial step for the normalization of biological entities automatically extracted from text with named entity recognition software. we have shown that we can predict the class of embedded entity relations, necessary for such normalization efforts, extremely well .

the results obtained in this study will enable us in the near future to annotate semantic relations between molecular entities in the entire scientific literature available through pubmed, exploiting these relations for further refinements and improvements of large-scale text mining efforts.

competing interests
the authors declare that they have no competing interests.

authors' contributions
jb and svl have designed and implemented the tees and getm frameworks respectively. ta has contributed to getm with algorithms and analysis for clustering and feature selection. svl, ta and jb have drafted the manuscript. bdb, ts and yvdp have participated in the design and coordination of the study. all authors have read and approved the final manuscript.

