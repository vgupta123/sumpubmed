BACKGROUND
increasing complexity of analysis and scientific workflow paradigm
the advent of high-throughput sequencing technologies - accompanied with the recent advances in open source software tools, open access data sources, and cloud computing platforms - has enabled the genomics community to develop and use sophisticated application workflows. such workflows start with voluminous raw sequences and end with detailed structural, functional, and evolutionary results. the workflows involve the use of multiple software tools and data resources in a staged fashion, with the output of one tool being passed as input to the next. as one example, a personalized medicine workflow  <cit>  based on next generation sequencing  technology can start with short dna sequences  of an individual human genome and end with a diagnostic and prognostic report, or potentially even with a treatment plan if clinical data were available. this workflow involves the use of multiple software tools to assess the quality of the reads, to map them to a reference human genome, to identify the sequence variations, to query databases for the sake of associating variations to diseases, and to check for novel variants. as another example, consider a workflow in the area of metagenomics  <cit> . such workflow can start with a large collection of sequenced reads and end up with determination of the existing micro-organisms in the environmental sample and an estimation of their relative abundance. this workflow also involves different tasks and software tools, such as those used for assessing the quality of the reads, assembling them into longer dna segments, querying them against different databases, and conducting phylogenetic and taxonomical analyses.

to simplify the design and execution of complex bioinformatics workflows, especially those that use multiple software tools and data resources, a number of scientific workflow systems have been developed over the past decade. examples include taverna  <cit> , kepler  <cit> , triana  <cit> , galaxy  <cit> , conveyor  <cit>  pegasus  <cit> , pegasys  <cit> , gene pattern  <cit> , discovery net  <cit> , and omii-bpel  <cit> ; see  <cit>  for a survey and comparison of some of these tools.

all such workflow systems typically adopt an abstract representation of a workflow in the form of a directed graph, where nodes represent tasks to be executed and edges represent either data flow or execution dependencies between different tasks. based on this abstraction, and through a visual front-end, the user can intuitively build and modify complex applications with little or no programming expertise. the workflow system maps the edges and nodes in the graph to real data and software components. the workflow engine  executes the software components either locally on the user machine or remotely at distributed locations. the engine takes care of data transfer between the nodes and can also exploit the use of high performance computing architectures, if available, so that independent tasks run in parallel. this makes the application scientist focus on the logic of their applications and no longer worry about the technical details of invoking the software components or use of distributed computing resources.

within the bioinformatics community, two workflow systems have gained increasing popularity, as reflected by their large and growing user communities. these are galaxy  <cit>  and taverna  <cit> . both systems are efficient, open source, and satisfy to a great extent the requirements of the bioinformatics community. taverna has been developed primarily to simplify the development of workflows that access and use analyses tasks deployed as remote web and grid services. it comes with an associated directory of popular remote bioinformatics services and provides an environment that coordinates their invocation and execution. galaxy has been developed primarily to facilitate the execution of software tools on local  infrastructure while still simplifying access to data held on remote biological resources. its installation includes a large library of tools and pre-made scripts for data processing. both systems are extensible, allowing their users to integrate new services and tools easily. each system offers log files to capture the history of experiment details. furthermore, both systems provide web-portals allowing users to share and publish their workflows: these are the myexperiment portal for taverna  <cit>  and the public pages for galaxy  <cit> . the features of both systems are continuously being updated by their development teams and their user communities are active in developing and sharing new application workflows.

however, since both taverna and galaxy have been developed with different use cases and execution environments in mind, each system tends to be suited to different styles of bioinformatics applications. the key differences between the two systems can be categorized into three major classes:

 <dig>  execution environment and system design: taverna is oriented towards using web-services for invoking remote applications, while galaxy is oriented towards efficient execution on a local infrastructures.

 <dig>  model of computation: taverna includes control constructs such as conditionals and iterations, and data constructs that can handle  lists  using a number of pre-defined operations. these constructs are not directly available in galaxy, which puts a limitation on the types of workflows that can be executed on galaxy.

 <dig>  workflow description language: taverna uses the xml-based language scufl for describing the workflows, while galaxy expresses workflows in its own language using json format.

these differences lead to two major consequences: first, some tasks can be implemented easily on one system but would be difficult to implement on the other without considerable programming effort. second, a  workflow developed on one system cannot be imported and re-used by the other easily , which limits sharing of workflows between their communities and leads to duplication of development efforts.

our contribution
in this paper, we present tavaxy, a pattern-based workflow system that can integrate the use and execution of taverna and galaxy workflows in a single environment. the focus of tavaxy is facilitating the efficient execution of sequence analysis tasks on high performance computing infrastructures and cloud computing systems. tavaxy builds on the features of taverna or galaxy providing the following benefits:

· single entry point: tavaxy is a standalone pattern-based workflow system providing an extensible set of patterns, and allows easy integration with other workflow systems. it provides a single environment to open, edit, and execute its own workflows as well as integrate native taverna and galaxy whole- or sub-workflows, thus enabling users to compose hybrid workflows. figure  <dig> summarizes the different integration use cases at run-time and design-time levels in tavaxy. 

· transparent use of local and remote resources: for most programs, tavaxy allows its user to choose whether a task should run on local or remote computational resources. furthermore, if a taverna workflow is imported , tavaxy offers users an option to replace calls to remote web services automatically with calls to corresponding tools that run on a local computing infrastructure, or vice versa. . changing the default mode of invocation in either taverna or galaxy requires programming knowledge, and it is difficult to achieve by the non-programming scientist.

· simplified control and data constructs: tavaxy supports a set of advanced control constructs  and data constructs  and allows their execution on the local or remote computational infrastructures. the use of these constructs, which are referred to as “patterns” in tavaxy, facilitates the design of workflows and enables further parallelization, where the data items passed to a node can be processed in parallel. the user of tavaxy has the extra advantages of 1) adding these constructs to imported galaxy workflows, and 2) using these constructs on the local infrastructures; features that are available only in taverna and only for remote tools.

beyond these integration issues, tavaxy provides the following additional features that facilitate authoring and execution of workflows:

· enhanced usability: tavaxy uses flowchart-like elements to represent control and data constructs. the workflow nodes are annotated with icons to reflect if they are executed locally or remotely. the tool parameters can be defined either at the design- or run-time of the workflow. the data patterns offered in tavaxy further facilitate the composition of workflows, making them more compact, and enable exploitation of local high performance computing infrastructure without any additional effort. furthermore, each user has an account associated with its data and each workflow is further associated with its history as well as previously used datasets within the user account.

· modularity: tavaxy is modular; it separates the workflow composition and management modules from the workflow engine. its workflow engine is a standalone application accepting both workflow definitions and data as input. this feature, as will be made clear later in the manuscript, is of crucial importance for implementing control constructs and for supporting cloud computing.

· high performance computing infrastructure support: tavaxy can readily run on a computer cluster, once a job scheduler system  and a distributed file system  are installed. the execution of parallel tasks is handled automatically by the workflow engine, hiding all invocation details.

· cloud computing support: tavaxy is cloud computing friendly, enabling users to scale-up their computational infrastructure on a pay-as-you go basis, with reduced configuration efforts. through a simple interface within the tavaxy environment, a user who has a cloud computing account  can easily instantiate the whole system on the cloud, or alternatively use a mixed mode where his local version of the system can delegate the execution of a sub-workflow or a single task to a tavaxy cloud instance.

in the remaining part of this section, we will review basic concepts of workflow interoperability and workflow patterns that contributed to the design and development of tavaxy.

related technical work
workflow interoperability
our approach described in this paper goes beyond the run-time “black-box” invocation of one system from the other, which was used in the work of  <cit>  to enable interoperability between galaxy and taverna. to highlight the difference, the workflow management coalition, wfmc,  <cit>  defines eight models, or approaches, for achieving interoperability between workflow systems. these models can be grouped broadly into two major categories: 1) run-time interoperability, where one system invokes the other system through apis. 2) design-time interoperability, where the two systems are based on a) the same model of computation ; or b) the same languages , or c) the same execution environment . these three design-time issues are discussed in detail in the paper of elmroth et al.  <cit> .

as discussed earlier, both taverna and galaxy have different models of computation and different languages. in this paper, we use ideas from the workflow interoperability literature and introduce the concept of patterns to integrate and execute taverna and galaxy workflows in tavaxy at both run-time and design-time levels..

workflow patterns
workflow patterns are a set of constructs that model a  requirement ; the description of these constructs is an integral part of the pattern definition. workflow patterns, despite being less formal than workflow languages, have become increasingly popular due to their practical relevance in comparing and understanding the features of different workflow languages. as originally introduced in  <cit> , workflow patterns were used to characterize business workflows and were categorized into four types: control flowdata flowresource and operational, and exception handling patterns. we note that the concept of patterns is in general applicable to scientific workflows. in  <cit> , we used this concept for the first time to demonstrate the feasibility of achieving interoperability between taverna and galaxy. our work in this paper extends this demonstrative work by providing a larger set of the patterns, and also by providing a complete implementation of them within a functional and usable system.

implementation
tavaxy model of computation and language
tavaxy workflows are directed acyclic graphs , where nodes represent computational tools and edges correspond to data flows or dependencies between them. the workflow patterns defined and used in tavaxy have special meanings in this dag, as will be explained in detail later in the pattern implementation subsection. the tavaxy engine is based on a data flow model of execution  <cit> , in which each node  can start computation only once all its input data are available. the tavaxy workflow engine is responsible for keeping track of the status of each node and for invoking its execution when the predecessor nodes have finished execution and when its input data is completely available. when executing on a single processor, where tasks are executed sequentially, the order of task invocation can be determined in advance. this is achieved by traversing the dag and scheduling a node  only if all the its predecessor nodes are already scheduled. as such, this scheduling is referred to as a static scheduling  <cit> . when executing on multiple processors, independent ready tasks can be executed in parallel. in this case, the tavaxy engine keeps looking for ready tasks and launches them concurrently. on multi-core machines, the engine uses multi-threading to handle the execution of concurrent tasks. on a computer cluster, it passes the concurrent tasks to a job-scheduler, which in turn distributes them for execution on different cluster nodes. the default job-scheduler used in tavaxy is pbs torque, and it is set-up over a shared file system  to guarantee availability of data for all cluster nodes.

a tavaxy workflow is defined and stored in tscufl format, which is similar in flavor to the taverna scufl format. however, there are two main differences between the two formats: 1) a node’s parameters are represented in tscufl by default as attributes of the respective tool, whereas they are considered as input items in scufl. 2) the workflow patterns  are explicitly specified in tscufl but implicitly defined in scufl.

integrating galaxy and taverna workflows in tavaxy
tavaxy provides an easy-to-use environment allowing the execution of tavaxy workflows that integrate taverna and galaxy workflows as sub-workflows. such integration can be achieved at both design-time and run-time:

for run-time integration, tavaxy can execute both galaxy and taverna  workflows ‘as is’, with no modification. for galaxy workflows, this is straightforward, because the tavaxy engine is compatible with the galaxy engine and follows the same model of computation. for taverna workflows, tavaxy can execute a taverna  workflow by invoking the taverna engine through a command line interface that takes both the taverna  workflow file and its data as input. the tavaxy mapper component assures the correct data transfer between the taverna engine and other nodes. this is achieved by setting source and destination directories and input/output file names in appropriate manners.

for design-time integration, tavaxy imports and manipulates workflows written in either galaxy or taverna formats. tavaxy can import a galaxy workflow file to its environment, allowing its modification and execution. the engineering work for this step includes translation of the json objects of the galaxy workflow to the tscufl format of tavaxy. for taverna workflows, the implementation addresses the differences in the model of computation and workflow languages. specifically, the workflow engine of tavaxy is a data-flow oriented one, with no explicit specification of control constructs, while the taverna engine supports both data- and control-flow constructs.

the taverna workflow language is scufl/t2flow but that of tavaxy is tscufl. to overcome these differences, we use the concept of workflow patterns to 1) execute  the execution of taverna control and data constructs on the data-driven workflow engine of tavaxy; and 2) to provide a pragmatic solution to language translation where a taverna  workflow is decomposed into a set of patterns that are then re-written in tavaxy format. the following section introduces the tavaxy workflow patterns and their implementation.

workflow patterns: definitions and implementation
we divide the tavaxy workflow patterns into two groups: control patterns and data patterns. in the remainder of this subsection, we define these patterns and their implementation on the tavaxy data-flow engine.

control patterns
control patterns specify execution dependencies between tasks. for most control patterns, data flow is still required and is defined as part of the control pattern specification itself. the following are the key control patterns used in tavaxy:

 <dig>  sequence: in this pattern, task b runs after the termination of task a, as shown in figure  <dig>  the data produced by a is subsequently processed by b and moves over an edge whose start is an output port at a and whose destination is an input port at b. the concept of ports makes it possible to select which pieces of data produced by a are passed to b. desired execution dependencies involving no data can be achieved on the tavaxy data flow engine by a special token  from a to b. the current engine of tavaxy does not support streaming, and the tasks are stateless, according to the discussion of ludäscher et al.  <cit> .

 <dig>  synchronous merge: a task is invoked only if all its predecessor tasks are executed; figure  <dig> depicts this pattern with three tasks a, b, and c, where task a and b should be completed before c. this pattern also specifies that task c takes two inputs  and the data flowing from a and b to c goes to different input ports.

 <dig>   synchronous fork: figure  <dig> depicts this pattern with three tasks a, b, and c. tasks b and c run after the execution of a. the data output from a flows according to one of two schemes, as specified by the user through settings of ports: 1) one copy of the data output of a is passed to b and another one to c. 2) different data output items of a are passed to b and c. the tasks b and c can always run in parallel, because their input set is already available and they are independent.

 <dig>  multi-choice fork: this pattern includes the use of an if-else construct to execute a task if a condition is satisfied. this condition is defined by the user through an implementation of a Ψ function. figure  <dig> shows an example, where either b or c is executed, depending on the Ψ function, whose domain may include the input data coming from a. note that the input data to b and c, which can come from any other node including a, is not specified in the figure. because this pattern specifies run-time execution dependencies, it is not directly defined over a data-flow engine. therefore, we implemented this pattern on the tavaxy engine by creating a special node containing a program that implements the switch function. the engine executes this node as a usual task. the program for switch pattern takes the following as input: 1) the multi-choice condition, and 2) the data to be passed to the next tasks. it then checks the condition and passes a success signal to the branch satisfying the condition and passes fail signal to the branch violating that condition. the success and fail signals are special tokens recognized by tavaxy nodes.

 <dig>  iteration: this pattern specifies repetition of a workflow task. in figure  <dig>  the execution of node b, which could be a sub-workflow, is repeated many times. the number of iterations can be either fixed or dependent on the data produced at each step. in each iteration, an output of task b can replace the corresponding input. for example, a parameter file can be passed to b and at each iteration this parameter file is modified and passed again to b. node c, which represents any node that uses the output of b, is invoked only after the iteration pattern terminates. the iteration pattern is represented by a special node in tavaxy and the associated program that implements it takes the following items as input: 1) the task  that iterates, 2) its parameters, 3) the termination criteria , and 4) information about feedback data. the iteration is implemented as a do-while loop, where the tasks in the body of the loop are encapsulated as a sub-workflow. tavaxy is invoked recursively to execute this sub-workflow in each iteration. the output of the iteration pattern is specified by the user and is passed to the next task upon termination. the loop iterations are in general stateless; but the user can modify the included sub-workflow to keep state information.

advanced data patterns and types
 <dig>   lists: in this pattern, the input to a node is a list of n items. the program associated with the node is invoked independently n times on each of the list items. figure  <dig> shows an example where a list  is passed to a. the output is also a list , ..., a). note that if the list option is not specified in the node, then the respective program is invoked once and the input list is handled as a single object, as in the sequence pattern. for example, a program for primer design would consider a multi fasta file as a list and is invoked multiple times on each item , while an alignment program would consider the sequences of the multi-fasta file as a single object to build a multiple sequence alignment. in tavaxy, it is possible to process the list items in parallel, without extra programming effort. furthermore, a list can be a list of lists defined in a recursive manner, so as to support a nested collection of items, according to the notion of  <cit> . the jobs corresponding to the processing of every list item are stateless, according to the discussion of ludäscher et al.  <cit> . however, the script implementing the list keeps track of the running jobs, and reports an error message if any job failed.

over this list data type, we define a set of operators that can be used by the main program associated with the node.

· dot product: given two lists a and b, n ≤ m as input, a dot product operation produces the n tuples  which are processed independently by the respective program, see figure  <dig>   this operation can be extended to multiple lists.

· cross product: given two lists a and b, n < m as input, a cross product operation produces the set of  tuples {|,i∈,j∈}, which are processed independently by the respective program. this option can be used, for example, for comparing two protein sets  to each other to identify orthologs. if a = b, then we compare the set of proteins to themselves to identify paralogs.

the list operations are implemented by a generic tool-wrapper of tavaxy. as we will explain later in the sub-section describing the architecture of tavaxy, this wrapper is what is invoked by the workflow engine, and it is the one that invokes the program to be executed. the wrapper pre-processes the input and can make parallel invocations on different list items if tavaxy is executing on a multiprocessor machine. the data collect pattern  can then be used to combine the results back in list format.

 <dig>  data select: consider figure  <dig> with the three tasks a, b, and c. the data select pattern takes as input 1) output data from a and b, denoted by a→ and b→, respectively. it takes also an implementation of a function Ψ that operates on properties of a→ or b→. without loss of generality, the output of this pattern is a→, if Ψ) is true, otherwise it is b→. the output of the pattern can be passed to another node c. this pattern is implemented in a similar way to the multi-choice pattern, where it specifies selection of certain data flow.

 <dig>  data collect : this pattern, which is depicted in figure  <dig>  specifies that the data outputs of a and b are collected  together in a list; i.e., the output is . note that a→ or b→ could be a list of objects as well, which leads to creation of nested collections. this pattern is implemented in a similar way to the data select pattern, where data items are collected.

tavaxy architecture
workflow authoring tool and language
the tavaxy workflow authoring module  is a web-based drag-and-drop editor that builds on the look and feel of galaxy with two key modifications. first, it supports a user-defined set of workflow patterns that are similar to those used in a traditional flowchart. second, it allows users to tag which workflow nodes execute on the local infrastructure and which execute using remote resources. for each node, there is a form that can be used to set the node’s parameters. furthermore, each node has a specific port that can accept a parameters file that can be used to over-write parameter values set through the web-interface. the use of a parameters file allows changing of the value of parameters at run time. figure  <dig>  shows the tavaxy authoring module and highlights some of its key features.

workflow mapper
the workflow mapper performs the following set of tasks:

· the mapper parses the input tscufl file and checks its syntax. it translates the galaxy json format and tavernascufl format to the tavaxytscufl format. depending on user choices, it can replace remote taverna calls with calls to corresponding local tools. the nodes that are still executed remotely by the taverna engine will be encapsulated as a sub-workflow. each sub-workflow is then associated with a tavaxy node that invokes the taverna engine so as to execute the corresponding sub-workflow. the mapper sets the names of the sub-workflow input and output files in an appropriate manner so that the data correctly flows between the nodes. additional file  <dig>  contains the re-writing rules for translating scufl to tscufl formats, including control constructs and replacement of remote services with local tools.

· the mapper optimizes the execution of a workflow by identifying the tasks that will be executed by the taverna engine and aggregating them into maximal external sub-workflows.. a sub-workflow is called external if it includes only taverna nodes and it is maximal if no extra external nodes can be added to it. the mapper determines the maximal external sub-workflows using a simple graph-growing algorithm, where we start with a sub-graph composed of a single taverna node and keep adding external nodes to this sub-graph provided that there are edges connecting the new nodes to the sub-graph and no cycles are introduced. to find the next maximal external sub-workflow, we move to the next non-processed external node. after sub-workflow identification, the mapper encapsulates each maximal external sub-workflow in a new node and adjusts the input and output ports in an appropriate manner. accordingly, the taverna engine is invoked only once for each maximal external sub-workflow, which avoids the overhead of multiple taverna calls. note that taverna uses multi-threading to handle execution of independent tasks, including remote invocations. hence, the use of maximal external sub-workflows with remote calls entails no loss in efficiency.

workflow engine
the tavaxy engine is based on the data flow model of execution discussed earlier in this section. it is written in python, based on some galaxy functions to save development time. the tavaxy engine  is standalone and not tightly coupled with the web-interface and database-interface; i.e., it can be invoked programatically or using a command line interface. furthermore, it can invoke itself in a recursive manner, which enables the implementation of different patterns and integration of heterogeneous workflows. by building on some of core features of galaxy engine, the tavaxy engine can be regarded as an extended and engineered version of that of galaxy. the taverna engine is invoked as any program  to achieve run time interoperability with taverna workflows and to use it in invocation of remote services.

all local tools in tavaxy are wrapped within a generic wrapper that is invoked by the engine.

this wrapper is responsible for the following:

· the wrapper decides whether the associated tool is executed or not, depending on reception of a special token . the special token can correspond either to 1) execution dependency or 2) “do-not-execute” or “fail” signal from the preceding node, as in the case of the multi-choice pattern. in the former case, the wrapper executes the respective computational tool, while in the latter case, it will not invoke the tool and further passes the token to the output ports.

· it handles the list patterns by determining the list items, executing list operations, and invoking the respective program  on list items.

· it uses cloud computing apis to execute tasks on cloud computing platforms. the use of cloud computing is discussed below in more detail.

workflow pattern database
the workflow pattern database stores the definition and implementation of the workflow patterns used in tavaxy. it also stores how the nodes associated with these patterns are rendered on the workflow authoring client. this pattern database is extensible by the user, who can define new patterns according to the rules of the tavaxy system.

use of cloud computing
as briefly mentioned before in the introduction, we provide three modes for using cloud computing: 1) whole system instantiation, 2) sub-workflow instantiation, and 3) tool  instantiation. to further simplify the use of the first mode, we installed an instance of tavaxy  on an amazon aws virtual machine and deposited a public image of it at the amazon web-site. a user who has an amazon account can directly start the image and use it. based on amazon apis, this image can establish a computer cluster upon its activation. the user can specifically define the type of nodes  and their number. the amazon s <dig> storage is used as a shared storage for the computer cluster. we developed several interface functions that manage data transfer among the compute nodes and the shared storage of the cluster at run time. figure  <dig> shows a screen shot of the tavaxy interface page, where the user can configure the cluster and storage.

in the second mode, the user already has a tavaxy version installed on his local machines  and delegates the execution of one or multiple sub-workflows to be executed on the cloud. to support this scenario, a lightweight version of tavaxy has been deposited at the amazon platform as a virtual machine image. from a simple user interface in the local tavaxy, the user can start and configure a cloud cluster using the prepared tavaxy image.

at run-time, the local version of tavaxy communicates with the cloud counterpart, using a simple asynchronous protocol , to send the sub-workflow, execute it, and retrieve the results. the input and output data related to such a sub-workflow flow according to one of two scenarios:

 <dig>  the input data is sent to the mounted disk of the main cloud machine along with the workflow to be executed. after processing, the output is sent back to the local tavaxy. after termination of the machine, the input and result data are lost, unless they are moved by the user to a persistent storage. this scenario is useful in case no computer cluster is needed.

 <dig>  the input data is sent to a shared volume in the persistent s <dig> storage , where the compute nodes of the computer cluster can easily access it. because reads and writes to s <dig> require the use of amazon apis, we developed special scripts to facilitate this access between the local tavaxy and s <dig> on one side and between the compute nodes and s <dig> on the other side. after execution of the sub-workflow, a copy of the output is maintained on the s <dig> and another copy is sent to the local tavaxy to complete the execution of the main workflow.

the third mode is a special case of the second mode, where the user can delegate the execution of only a single task to the cloud. for this mode, we also use a simple protocol to manage the data transfer and remote execution of the task on the cloud. figure  <dig> shows the architecture of the cloud version of tavaxy and the data flows among its components.

RESULTS
accessing tavaxy
there are different ways to access and use the tavaxy system from its main home page:

 <dig>  downloadable version: the whole tavaxy system, with all features described in this manuscript, can be downloaded for local use. the bioinformatics packages are provided in a separate compressed folder, because we assume that some users already have installed the packages of interest on their local systems and just need the tavaxy system. the packages currently include about  <dig> open source tools, coming from emboss  <cit> , samtools  <cit> , fastx  <cit> , ncbi blast toolkit  <cit> , and other individual sequence analysis programs. addition of extra tools is explained in the tavaxy manual.

 <dig>  web-based access: we offer a traditional web-based interface to a tavaxy instance for running small and moderate size jobs. for large scale jobs, we recommend the use of cloud version.

 <dig>  cloud-computing based access: in this mode, each user creates a tavaxy instance with the hardware configuration of choice on the aws cloud. the interesting feature in this model is that multiple users may have multiple tavaxy systems, each with different configuration . the tavaxy instances on the cloud already include the  <dig> tools currently tested. they also include a number of databases to be used with the cloud machines, such as the ncbi  and swissprot databases.

pre-imported workflows
at the time of preparing this manuscript , the taverna repository myexperiment contained  <dig> workflows in scufl  format and  <dig> workflows in t2flow format . by manual inspection, we found that  <dig> workflows  are related to the sequence analysis domain, which is the main focus of this version of tavaxy. to help the community, we already imported all these workflows into the tavaxy environment, and arranged them in a special web-accessible repository for public use. we also provided the user with optimized versions of the sequence analysis workflows, where many of the web-services are replaced with local invocations of the corresponding local tools distributed with tavaxy. we also imported all public galaxy workflows from the galaxy public pages and added them to this repository. the workflows imported from both the taverna and galaxy repositories are included in the tavaxy system, and will be kept up-to-date on its web-site. these workflows can serve as “design patterns” that can can be used to speed up workflow development cycle, when developing more complex workflows.

experiments overview
in the following sub-sections, we introduce two case studies that demonstrate the key features of tavaxy. in the first case study, we demonstrate 1) how taverna, galaxy, and tavaxy sub-workflows can be integrated in a single tavaxy workflow, highlighting both the integration capabilities and use of workflow patterns; and 2) the optimization steps included before the execution of imported workflows and their effects on the performance of the system. in the second case study, we demonstrate 1) the use of tavaxy for a metagenomics workflow based on ngs data; 2) the advantages of using advanced data patterns in facilitating the workflow design and supporting parallel execution; 3) the speed-up achieved by using local hpc infrastructure; and finally 4) the efficient and cost-saving use of cloud computing.

case study i: composing heterogeneous sub-workflows on tavaxy
searching the myexperiment repository, there is already an existing taverna implementation for most of the desired workflow, deposited under the name “workflow_for_protein_sequence_analysis”  <cit> , and figure  <dig> shows its implementation as it appears in the taverna authoring module. the missing functionality in this taverna workflow are the two parts highlighted in figure  <dig>  including the parts for translating the dna sequence into protein sequence and the one for muscle-consensus. in the original taverna implementation, the software tools blast, clustalw, and phylogeny plotting are invoked through web-service interfaces. the other intermediate steps are executed by built-in taverna programs.

we downloaded the taverna workflow and imported it into tavaxy; figure  <dig> shows the same workflow in the tavaxy environment. at this step, the user may choose to execute this workflow as it is from tavaxy, or may choose to optimize the execution of the workflow and/or customize it by adding further tasks. for example, for this workflow, the user can replace web-services with equivalent locally installed tools through a simple user interface. the workflow mapper carries out this replacement and can, according to user choices, coalesce the remaining taverna tasks into maximal sub-workflows, as described earlier in the tavaxy implementation section. in this example, we decided that the clustalw and the phylogeny analysis parts of the workflow run on the local infrastructure, while the blast part still runs remotely. figure  <dig> shows the optimized version of this workflow, where the maximal taverna sub-workflows are computed. the functionality of this workflow can be augmented with further tasks. first, we re-used a native galaxy  workflow that computes multiple alignment using the muscle program and computes the consensus sequence. second, we added a tavaxy sub-workflow, in which the dna sequences are translated into protein sequences, instead of ignoring processing them. to link the translated sequences to the other parts of the workflow for further analysis, the data merge pattern is used to pass the protein sequences. these extra parts are highlighted in figures  <dig> and  <dig> 

measuring the performance
we conducted an experiment to evaluate the overhead associated with invoking the tavern engine to execute remote tasks, before and after the optimization step. we used the original taverna workflow and its imported version , with the list of input protein ids  being empty. we measured the running time of this workflow with respect to three different execution scenarios. in the first scenario, the original taverna workflow was executed on taverna, where the tasks are executed remotely. in the second scenario, the workflow was executed after replacing the remote tools with equivalent local ones . in the third scenario, the workflow was executed after conducting the optimization step to reduce the number of invocations of the local taverna engine.

for this experiment, we used the example protein sequence distributed with the taverna workflow on myexperiment. we also used another set of proteins used by kerk et al.  <cit>  to update the protein phosphatase database with novel members. the basic idea of their work is to use a set of representative human proteins from different phosphatase classes to identify homologs from different genomes. it is worth mentioning that the workflow at hand automates most of the manual steps conducted in the study of kerk et al.  <cit> . hence, it can be used to systematically and automatically revisit the protein phosphatase repertoire.

the average running times in minutes for different protein sequences and for different execution scenarios of the protein homology workflow. the last protein, example seq., is the example protein distributed with the tavernaworkflow. the other proteins are from the study of  <cit> .

despite the differences in the design of the taverna, galaxy, and tavaxy engines, we performed an extra experiment to compare their performance. we used the sub-workflow in this case study, including the blast and clustalw calls, as a test workflow. this sub-workflow is highlighted in figure  <dig> and denoted as ‘core workflow’. for taverna, we used the local installation of the programs and we wrote special shell scripts to run them on the local infrastructure.  the results of this experiment, which are shown in table  <dig>  indicates that the performance of the three systems is very similar. we note a little overhead when using galaxy and tavaxy, because the engines of both systems are designed for multiple users, while the taverna engine is desktop based serving a single user. we also note that the tavaxy engine, as expected, is a little slower than that of galaxy. this can be attributed to the overhead associated with the extra wrapper module developed for handling the patterns and cloud functionalities. note that these overheads are proportional to the workflow size, and would be negligible for large datasets.

the average running times  of the workflow involving blast and clustalwfor the protein sequences in table  <dig>  the whole workflow runs on local infrastructure. the queries are performed against the swissprot and ncbi refseq databases.

case study 2: a metagenomics workflow
in the original implementation of this workflow on galaxy  <cit> , and as depicted in the schematic representation of figure  <dig>  we can identify two issues: first, the input reads are passed to megablast is a single multi-fasta file which implies sequential processing of the queries against the database. second, there are two nodes for megablast: one to consider the ncbi_wgs database and the other to consider the ncbi_nt database. to query more databases in galaxy, additional nodes should be manually added; this will yield a bulky workflow for a large number of databases. in tavaxy, we can enhance the design and execution of this workflow with respect to these two issues.

for the first issue, we use the tavaxy list pattern in association with megablast so that the input multi-fasta file is handled as a list of items. this will immediately lead to parallelization of this step. a list item could be a single fasta sequence or a block of multiple fasta sequences. we recommend that the input reads are divided into a list of n blocks, each of size k sequences. the parameter k is set by the user and it should be proportional to the number of processors available.  are separated by a special user-defined symbol.) when the workflow with the list pattern is executed, multiple versions of megablast will be invoked to handle these blocks in parallel.

for the second issue, concerning the simple integration of more databases, we will use only just one megablast node and create a list of input databases. this list is passed as input to the megablast node. to ensure that each read is queried against all given databases, we use the cross product operation defined over the list of databases and the list of input sequences. for m databases and n blocks, we have n × m invocations of megablast, which can be handled in parallel without extra coding effort.

measuring the performance
we tested the performance of the enhanced metagenomics workflow on a computer cluster using two datasets. the first was the dataset used by huson et al.  <cit> , constituting a metagenomic survey of the sargasso sea  <cit> . this dataset, which represents four environmental samples, is composed of  <dig>  sanger reads, where  <dig>  come from sample  <dig> and another  <dig>  come from samples 2– <dig>  the second dataset is the windshield data set of  <cit> , which is composed of two collections of  <dig> flx reads. these reads came from the dna of the organic matter on the windshield of a moving vehicle that visited two geographic locations . we used the reads of the left part of the windshield experienced both trips. the number of reads are  <dig>  and  <dig>  for trips a and b, respectively. for megablast, we used the ncbi_htgs, ncbi_nt, and ncbi_env datasets.

the average running times in minutes for varying numbers of processors  on the local infrastructure.

use of cloud computing
we used the cloud computing features of tavaxy on the sub-workflow level to execute the metagenomics workflow. the purpose is to test the use of cloud computing in terms of execution time and cost of computation. here, we focused on the sub-workflow mode of using cloud computing, because it demonstrates the case. we decided to run the sub-workflow involving megablast with the list pattern on the cloud because it is the most compute-intensive part in this workflow. from the tavaxy interface, we established a computer cluster on the aws cloud. each node includes a copy of the databases needed by megablast. the shared s <dig> cloud storage is attached to the cluster to maintain the output and intermediate results. for this experiment, we used amazon instances of type “extra large”, with  <dig> cores , 15 gb ram, and  <dig>  gb storage. the establishment of the cluster with the storage took a few minutes from the machine images.

the average running times in minutes for a computer cluster on the cloud. the number in bracket is the computation cost in us dollars for the us-east site with $ <dig>  per hour . 

CONCLUSIONS
in this paper we introduced tavaxy, a stand-alone pattern-based workflow system that can also integrate the use of taverna and galaxy workflows in a single environment, enabling their modification and execution. the tavaxy integration approach is based on the use of hierarchical workflows and workflow patterns. tavaxy also supports the use of local high-performance computing and the use of cloud computing. the focus of the current version of tavaxyis on simplifying the development of sequence analysis applications, and we demonstrated its features and advantages using two sequence analysis case studies. future versions of the system will support further applications in transcriptomics and proteomics.

we also introduced a set of advanced data patterns that simplify the composition of a variety of sequence analysis tasks and simplify the use of parallel computing resources for executing them. in future work, we will extend the available patterns to support more complex sequence analysis tasks, as well as other application domains. tavaxyis currently shipped with its own repository of pre-imported tavernaand galaxy workflows to facilitate their immediate use. this repository can be regarded as a set of “design patterns” that can help in speeding up composition of more complex workflows.

in the current version of tavaxy, we have set up the system for use on a traditional computer cluster on the aws cloud. we have not yet investigated other hpc options, such as the amazon elastic mapreduce or the use of gpus.

in future versions of tavaxy we will investigate the use of these options to support efficient execution at the sub-workflow and task levels. we will also investigate the use of other cloud computing platforms.

finally, we believe that one of the key advantages of tavaxy is that it provides a solution that consolidates the use of remote web-services, cloud computing, and local computing infrastructures. in our model, the use of remote web-services is limited to only those shared tools that cannot be made locally available, the use of a local infrastructure supports the execution of affordable tasks, and the use of cloud computing provides a scalable solution to compute- and data-intensive tasks.

availability and requirements
 <dig> . <dig> . project name:tavaxy.

 <dig> . <dig> . project home page: http://www.tavaxy.org.

 <dig> . <dig> . operating system: linux.

 <dig> . <dig> . programming language: python, c, java script, jsf

 <dig> . <dig> . other requirements: compatible with the browsers firefox, chrome, safari, and opera. see the manual for more details.

 <dig> . <dig> . license: free for academics. authorization license needed for commercial usage .

 <dig> . <dig> . any restrictions to use by non-academics: no restrictions.

competing interest
the authors declare no conflict of interest.

authors’ contributions
ma led the tavaxy project. ma and mg contributed to theoretical developments of the architecture and workflow patterns which form the basis of tavaxy. sa and ma developed and tested the software and implemented the workflows. all authors wrote and approved the manuscript.

supplementary material
additional file 1
re-writing rules for translating scufl to tscufl. a pdf file describing the re-writing rules for translating a tavernaworkflow in scufl format into tavaxy workflow in tscufl format.

click here for file

 additional file  <dig> 
paper figures in original size. compressed folder containing the paper figures in original size for better visualization.

click here for file

 acknowledgment
we thank the galaxy team for making the code of their system available under the open source license, which helped us re-use a number of galaxy components within our system. we also thank the taverna team for making their system available under open source license and for their valuable feedback on the manuscript. we thank peter tonellato and dennis wall from harvard medical school, as well as the amazon team, for providing us with aws compute hours. we thank mohamed elkalioby from nile university for his support in establishing the cloud computing infrastructure. we thank sondos seif from nile university for helping us in software engineering tasks.
