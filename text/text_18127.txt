BACKGROUND
gene expression data are complex, noisy, and subject to inter- and intra-laboratory variability  <cit> . moreover, because tens of thousands of measurements are made in a typical experiment, the likelihood of false positives  is high. one way to address these issues is to increase replicates in the experiments. however this is generally cost prohibitive. therefore, quality control of gene expression experiments with limited sample size is important for identification of true degs. although the completion of the microarray quality control  project provides a framework to assess microarray technologies, others have pointed out that it does not sufficiently address inter- and intra-platform comparability and reproducibility  <cit> .

even with reliable gene expression data, statistical analysis of microarray experiments remains challenging to some degree. jeffery and coworkers found a large discrepancy between gene lists generated by  <dig> different feature selection methods, including significance analysis of microarrays , analysis of variance , empirical bayes, and t-statistics  <cit> . several studies have focused on finding robust methods for identification of degs  <cit> . however, as more methods become available, it is increasingly difficult to determine which method is most appropriate for a given experiment. hence, it is necessary to objectively compare and evaluate different gene selection methods  <cit> , which result in different number of degs and different false discovery rate  estimates  <cit> .

fdr is determined by several factors such as proportion of degs, gene expression variability, and sample size  <cit> . controlling for fdr can be too stringent, resulting in a large number of false negatives  <cit> . therefore, determination of an appropriate threshold is critical for effectively identifying truly differentially expressed genes, while minimizing both false positives and false negatives. a recent study, using a cross validation approach showed that optimal selection of fdr threshold could provide good performance on model selection and prediction  <cit> . although many researchers have made considerable progress in improving fdr estimation and control  <cit> , as well as other significance criteria  <cit> , the instability resulted from high level of noise in microarray gene expression experiments cannot be completely eliminated. there is therefore a great need to make meaningful statistical significance and fdr thresholds by incorporating biological function.

recently, chuchana et al. integrated gene pathway information into microarray data to determine the threshold for identification of degs  <cit> . by comparing a few biological parameters such as total number of networks and common genes among pathways, they determined the statistical threshold by the amount of biological information obtained from the degs  <cit> . this study seems to be the first attempt to objectively determine the threshold of degs based on biological function. however, there are several limitations of this study. first, the method relied on ingenuity pathway analysis which may be biased toward well studied genes and limited by human curation. second, the threshold selection is iteratively defined. finally, the approach is manual, which is not realistic for large scale genome-wide applications.

a number of groups have developed computational methods to measure functional similarities among genes using annotation in gene ontology and other curated databases  <cit> . for example, chabalier et al., showed that each gene can be represented as a vector which contains a set of go terms  <cit> . each term was assigned a different weight according to the number of genes annotated by this term and the total number of annotated genes in the collection. thus, go-based similarity of gene pairs was calculated using a vector space model. other studies not only focused on using go annotations to calculate gene-gene functional similarities but also to determine the functional coherence of a gene set. recently, richards et al utilized the topological properties of a go-based graph to estimate the functional coherence of gene sets  <cit> . they developed a set of metrics by considering both the enrichment of go terms and their semantic relationships. this method was shown to be robust in identifying coherent gene sets compared with random sets obtained from microarray datasets.

previously, we developed a method which utilizes latent semantic indexing , a variant of the vector space model of information retrieval, to determine the functional relationships between genes from medline abstracts  <cit> . this method was shown to be robust and accurate in identifying both explicit and implicit gene relationships using a hand curated set of genes. more recently, we applied this approach to determine the functional cohesion of gene sets using the biomedical literature  <cit> . we showed that the lsi derived gene set cohesion was consistent across > <dig> go categories. we also showed that this literature based method could be used to compare the cohesion of gene sets obtained from microarray experiments  <cit> . subsequently, we applied this method to evaluate various microarray normalization procedures  <cit> . in the present study, we aimed to develop and test a robust literature-based method for evaluating the overall quality, as determined by functional cohesion, of microarray experiments. in addition, we describe a novel method to use literature derived functional cohesion to determine the threshold for expression p-value and fdr cutoffs in microarray analysis.

methods
gene-document collection and similarity matrix generation
all titles and abstracts of the medline citations cross-referenced in the mouse, rat and human entrez gene entries as of  <dig> were concatenated to construct gene-documents and gene-gene similarity scores were calculated by lsi, as previously described  <cit> . briefly, a term-by-gene matrix was created for mouse and human genes where the entries of the matrix were the log-entropy of terms in the document collection. then, a truncated singular value decomposition  of that matrix was performed to create a lower dimension  matrix. genes were then represented as vectors in the reduced rank matrix and the similarity between genes was calculated by the cosine of the vector angles. gene-to-gene similarity was calculated using the first  <dig> factors, which has good performance for large document collections  <cit> .

calculation of literature-based functional significance 
this study is an extension of our previous work on gene-set cohesion analysis  <cit> . briefly, we showed that lsi derived gene-gene relationships can be used effectively to calculate a literature cohesion p-value . lpv is derived by using fisher's exact test to determine if the number of literature relationships above a pre-calculated threshold in a given gene set is significantly different from that which is expected by chance. in many cases, the size of the differentially expressed gene set can be very large. computationally it is not feasible to calculate one lpv for a very large gene set. also, it is difficult to compare lpvs if the gene sets are vastly different in size. therefore, we defined a new metric called literature cohesion index  of randomly sampled subsets of  <dig> genes from the pool of degs. lci is the fraction of the sampled subsets that have an lpv <  <dig> . then, the overall literature-based functional significance  of the entire deg set is determined by a fisher's exact test comparing the lci to that expected by chance  via a permutation test procedure . in forming the 2-by- <dig> table, average counts from the multiple permutations are rounded to the nearest integers.

literature aided statistical significance threshold 
now suppose a differential expression p-value  is computed for each probe  by a proper statistical test. a statistical significance threshold  can be determined by considering the relationship between the epv and the lci for a given deg set. first, a grid of epv cutoffs is specified such as  <dig> ,  <dig>   <dig> ,  <dig> , ⋯,  <dig>  to generate a deg set at each cutoff value. next, the lci is calculated for each deg set using the sub-sampling procedure as described above. apart from some random fluctuations, the lci value is typically a decreasing function of the epv threshold and assumes an l shape , implying that the lci partitions the epv thresholds  into two subpopulations: one with good lci  and one with poor lci. the epv threshold at the boundary of the two subpopulations , can be used as a statistical significance cutoff for selecting degs. the bend point can be determined by moving a two-piece linear fit to the l-shaped curve from left to right. the lasst algorithm is as follows:

 specify an increasing sequence of epv statistical significance thresholds α <dig>  ⋯, αm and generate deg sets at these specified significance levels.

 for each deg set generated in , estimate the lci using the sub-sampling procedure described above, to obtain pairs , i =  <dig>   <dig>  ⋯, m.

 choose an integer m <dig>  and perform two-piece linear fits to the curve as follows: for k = m <dig>  m0+ <dig>  ⋯, m-m <dig>  fit a straight line by lease square to the points , j =  <dig>   <dig>  ⋯, k  to obtain intercept and slope β^0kl, β^1kl. similarly fit a straight line to the points , j = k+ <dig>   <dig>  ⋯, m  to obtain intercept and slope β^0kr, β^1kr. compute vk=+.

 let k* be the first local maxima of vk , that is, k*=min{j:vj≥vj+1}.

 take the k*th entry on the α sequence specified in  as the epv significance cutoff.

microarray data analysis
to test the performance of our approach, we randomly chose three publicly available microarray datasets from gene expression omnibus : 1) interleukin- <dig> responsive  genes  <cit> ; 2) pgc-1beta related  genes  <cit> ; 3) endothelin- <dig> responsive  genes  <cit> . to be able to compare across these datasets, we focused only on experiments using the affymetrix mouse 430- <dig> platform. all datasets  were imported into genespring gx  <dig> and processed using mas <dig> summarization and quantile normalization. probes with all absent calls were removed from subsequent analysis. as discussed earlier, the content and literature cohesion of a deg set can largely depend on the statistical test. for this reason, four popular statistical tests including empirical bayes approach  <cit> , student t-test, welch t-test and mann-whitney test were performed to identify degs with a statistical significance level  <dig> .

RESULTS
comparison of various statistical tests using lbfs
the goal of our study was to develop a literature based method to objectively evaluate the biological significance of differentially expressed genes produced by various statistical methods applied to gene expression experiments. previously, we developed a method and web-tool called gene-set cohesion analysis tool  which determines the functional cohesion of gene sets using latent semantic analysis of medline abstracts  <cit> . however, this method was applicable only to small gene sets and could not be used to compare gene-sets with varying sizes. here, we have extended this functional cohesion method to determine the biological significance of larger gene sets, which are typically found in microarray studies. to accomplish this, we first calculate the literature cohesion index  of degs produced . literature based functional significance  is then calculated by comparing the lci of the original labeled experiment and a permuted experiment . importantly, we found that lbfs values varied greatly between different statistical tests for a given dataset . for example, the empirical bayes method produced the most functionally significant degs for pgc-1beta dataset, but not the other two datasets. in contrast, the welch t-test generated the most functionally significant degs for the il <dig> dataset. both pgc-1beta and il <dig> experiments showed significant  lbfs values with multiple statistical tests, whereas none of the tests on et <dig> dataset produced degs with significant lbfs . these results suggest that the pgc-1beta and il <dig> experiments likely produced biologically relevant degs compared with the et <dig> experiments. the lack of biological significance for et <dig> degs may be due to poor data quality or lack of knowledge in the literature that functionally connects these degs. however, the latter may not be the case as the percentage of genes with abstracts was 68-84% for all datasets and statistical tests .

for comparison the literature cohesion index  which is used to calculate lbfs is displayed for each experiment.

determination of epv threshold using lasst
in the above analysis, degs were selected using an arbitrary statistical threshold of p< <dig> , as is the case for many published expression studies. however, in reality, there is no biological reason why this threshold is selected for experiments. once the appropriate statistical test was chosen by application of lbfs above, we tested if literature cohesion could be applied to determine the epv cutoff. we developed another method called literature aided statistical significance threshold  which determines the epv by a two-piece linear fit of the lci curves as a function of epv as described in methods. lasst was applied to p-values produced by empirical bayes for pgc-1beta experiment and welch t-test for the il <dig> and et <dig> experiments. degs were produced at each point on a grid of unequally-spaced statistical significance levels . in computing the lci, the lpv level was set to  <dig> , and the size of the gene subsets from the deg pool was set to  <dig> in the sub-sampling procedure as described in methods. the lci of a deg set was plotted against various α levels of the epv . interestingly, application of lasst determined an epv significance threshold of  <dig>   for pgc-1beta dataset and  <dig>   for il <dig> dataset. none of the deg sets from the et <dig> experiment had appreciable lci, which remained consistently low across the α levels . thus, an epv threshold could not be determined using the lci approach for et <dig> dataset. these results are consistent with what we observed above .

while computing lcis in the above analysis, the lpv threshold was set at  <dig> . we wondered if different lpv thresholds would affect lasst results. therefore, we calculated lci at different lpv thresholds such as  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig>  and  <dig> . we found that the shape of the lci curves were similar with respect to epv values , indicating that lasst is not sensitive to different reasonably conservative lpv thresholds.

we next compared the lasst results with several popular multiple hypothesis testing correction procedures along with the unadjusted p-value threshold of  <dig>  in a student t-test . for the il <dig> experiment, storey's q-value method at  <dig>  identified the highest number of degs. in stark contrast, only  <dig> gene was selected by any of the four fdr correction methods for the pgc-1beta experiment and  <dig> genes were selected for the et <dig> experiment. importantly, application of lasst selected  <dig> genes at a p-value threshold of  <dig>   and  <dig> genes at a p-value threshold of  <dig>   for il <dig> and pgc-1beta experiments, respectively. these results suggest that perhaps more biologically relevant degs can be selected with lower fdr values.

discussion
although microarray technology has become common and affordable, analysis and interpretation of microarray data remains challenging. experimental design and quality of the data can severely affect the results and conclusions drawn from a microarray experiment. using our approach, we found that some datasets  produced more functionally cohesive gene sets than others . there can be many biological or technological reasons for the lack of cohesion in any microarray dataset. for instance, it is possible that the experimental perturbation  simply did not alter mrna expression levels in that system as hypothesized. it is also possible that the data are noisy due to technical or biological variations, which result in false differential expression. although our method will not identify the causes of this variation, it can help in assessment of the overall quality of the experiment and provide feedback to the investigators in order to adjust the experimental procedures. for example, after observing a low lbfs value, the investigator may choose to remove outlier samples or add more replicates into the study design.

it is important to note that a low cohesion value could be due to a lack of information in the biomedical literature. in other words, it is possible that the microarray experiment has uncovered new gene associations which have not been previously reported in the literature. this issue would affect any method that relies on human curated databases or natural language processing of biomedical literature. however, our lsi method presents a unique advantage over other approaches because it extracts both explicit and implicit gene associations, based on weighted term usage patterns in the literature. consequently, gene associations are ranked based on their conceptual relationships and not specific interactions documented in the literature. thus, we posit that lsi is particularly suited for analysis of discovery oriented genomic studies which are geared toward identifying new gene associations. further work is necessary to be able to determine exactly how  a subset of functionally cohesive genes are related to one another in the lsi model.

a major challenge in microarray analysis involves selection of the appropriate statistical tests, which have different assumptions about the data distribution and result in different deg sets. for instance, parametric methods are based on the assumption that the observations adhere to a normal distribution. the assumption of normality is rarely satisfied in microarray data even after normalization. nonparametric methods are distribution free and do not make any assumptions of the population from which the samples are drawn. however, nonparametric tests lack statistical power with small samples, which is often the case in microarray studies. in this study, we found that although mann-whitney nonparametric test identified the largest number of degs for pgc-1beta experiment, the degs were not functionally significant . also, we found that some tests were selectively better for some experiments. for example, the empirical bayes method produced the best results for the pgc-1beta experiment, while the welch t-test produced the best results for the il <dig> experiment. taken together, we demonstrate that our method allows an objective and literature based method to evaluate the appropriateness of different statistical tests for a given experiment.

several groups have developed methods to assess functional cohesion or refine feature selection by incorporating biological information from either the primary literature or curated databases  <cit> . to our knowledge, a literature based approach to evaluate the overall quality of microarray experiments has not been reported. although we did not extensively compare our approach with these methods, we performed a preliminary comparison with a well known gene set enrichment analysis  method  <cit> . gsea calculates the enrichment p-value for biological pathways in curated databases for a given set of degs. presumably, if a microarray experiment is biologically significant, then higher number of relevant pathways should be enriched. indeed, we found that gsea identified  <dig>   <dig> and  <dig> enriched pathway gene sets with fdr < <dig>  for pgc-1beta, il <dig> and et1experiments, respectively. these results correlated well with our lbfs findings which showed that degs obtained from pgc-1beta and il <dig> were more functionally significant than et <dig>  however, gsea identified a substantial number of enriched pathways for et <dig>  one issue is that gsea only focuses on gene subsets and not the entire deg list. thus, it does not evaluate the overall cohesion or functional significance of the deg list. in addition, since gsea relies on human curated databases such as go and kegg, it is susceptible to curation biases, which favor well-known genes and pathways and contain limited information on other genes.

assuming that microarray experiment is of high quality and an appropriate statistical test has been selected for a microarray experiment, selection of the expression p-value cutoff still remains arbitrary for nearly all published studies. in our work, we found a positive correlation between literature cohesion index and epv . based on the distribution of lci with respect to epv, we devised a method  which empirically determined the epv cutoff value. not surprisingly, we found that different epv cutoffs should be used for the different microarray experiments that we examined. indeed, we found that application of lasst resulted in a smaller p-value threshold and substantially smaller number of degs for both il <dig> and pgc-1beta experiments. therefore, lasst enables researchers to narrow their gene lists and focus on biologically important genes for further experimentation.

finally, another major challenge for microarray analysis is the propensity for high false discovery rate  caused by multiple hypothesis testing. correction of multiple hypothesis testing including family wise error rate  are often too stringent which may lead to a large number of false negatives. as with epv cutoff concerns above, setting the fdr threshold at levels  <dig> ,  <dig> , or  <dig>  does not have any biological meaning  <cit> . for instance, no false positive error correction method produced adequate degs for pcg-1beta and et <dig> experiments. however, our analysis showed that pgc-1beta dataset was biologically very cohesive . this suggests that applying fdr correction to this dataset would produce a very large number of false negatives. another important finding of our study is that the false positive error correction procedures appear to be sensitive to deg size. for instance, using student t-test il <dig> dataset consisted of  <dig> degs with a p-value < <dig> , whereas the storey fdr method produced  <dig> at q< <dig> . however, our literature based analysis revealed that the il <dig> dataset produced less biologically cohesive degs than the pgc-1beta dataset, which showed only  <dig> gene with q< <dig> . in the future, it will be important to expand these preliminary observations to a larger of set of microarray experiments and to determine the precise relationships between false positive correction methods and biological significance.

CONCLUSIONS
in this study, we developed a robust methodology to evaluate the overall quality of microarray experiments, to compare the appropriateness of different statistical methods, and to determine the expression p-value thresholds using functional information in the biomedical literature. using our approach, we showed that the quality, as measured by the biological cohesion of degs, can vary greatly between microarray experiments. in addition, we demonstrate that the choice of statistical test should be carefully considered because different tests produce different degs with varying degrees of biological significance. importantly, we also demonstrated that procedures that control false positive rates are often too conservative and favor larger deg sets without considering biological significance. the methods developed herein can better facilitate analysis and interpretation of microarray experiments. moreover, these methods provide a biological metric to filter the vast amount of publicly available microarray experiments for subsequent meta-analysis and systems biology research.

abbreviations
anova: analysis of variance; degs: differentially expressed genes; epv: expression p-value; et1: endothelin- <dig> responsive; fdr: false discovery rate; gca: gene-set cohesion analysis; gcat: gene-set cohesion analysis tool; geo: gene expression omnibus; il2: interleukin- <dig> responsive; lasst: literature aided statistical significance thresholds; lbfs: literature-based functional significance; lci: literature cohesion index; lpv: literature cohesion p-value; lsi: latent semantic indexing; maqc: microarray quality control; pgc-1beta: pgc-1beta related; sam: significance analysis of microarrays; svd: singular value decomposition;

competing interests
the authors declare that they have no competing interests.

authors' contributions
l. xu developed the algorithm, carried out the data analyses, performed all of the evaluation and wrote the manuscript. c. cheng developed the literature aided statistical significance thresholds method and wrote part of the manuscript. e.o. george provided statistical supervision of the study. r. homayouni conceived, co-developed the methods, supervised the study and wrote the manuscript.

supplementary material
additional file 1
number of de genes  and percentage of having abstracts that generated from different tests for pgc-1beta, il <dig> and et <dig> datasets.

click here for file

 acknowledgements
we thank dr. kevin heinrich  for providing the gene-gene association data. this work was supported by the assisi foundation of memphis and the university of memphis bioinformatics program.

this article has been published as part of bmc genomics volume  <dig> supplement  <dig>  2012: proceedings of the international conference on intelligent biology and medicine : genomics. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcgenomics/supplements/13/s <dig> 
