BACKGROUND
with the advent of high-throughput techniques, such as dna microarrays and mass spectrometry, transcriptomic and proteomic studies are generating an abundance of high-dimensional biomedical data. the analysis of such data presents significant analytical and computational challenges, and increasingly data mining techniques are being applied to these data with promising results  <cit> . a typical task in such analysis, for example, entails the learning of a mathematical model from gene expression or protein expression data that predicts well a phenotype, such as disease or health. in data mining, such a task is called classification and the model that is learned is termed a classifier. the variable that is predicted is called the target variable , which in statistical terminology is referred to as the response or the dependent variable. the features used in the prediction are called the predictor variables , which are referred to as the covariates or the independent variables in statistical terminology.

a large number of data mining methods have been developed for classification; several of these methods are unable to use continuous data and require discrete data  <cit> . for example, most rule learning methods that induce sets of if-then rules and several of the popular methods that learn bayesian networks require data that are discrete. some methods that accept continuous data, as for example methods that learn classification trees, discretize the data internally during learning. other methods, such as the na√Øve bayes classifier, that accept both continuous and discrete data, may perform better with discrete data  <cit> . a variety of discretization methods have been developed for converting continuous data to discrete data  <cit> , and one that is commonly used is fayyad and irani's  discretization method  <cit> .

in this paper, we present an efficient bayesian discretization method and evaluate its performance on several high-dimensional transcriptomic and proteomic datasets, and we compare its performance to that of the fi discretization method. the remainder of this paper is structured as follows. the next section provides some background on discretization and briefly reviews the fi discretization method. the results section describes the efficient bayesian discretization  method and gives the results of an evaluation of ebd and fi on biomedical transcriptomic and proteomic datasets. the final section discusses the results and draws conclusions.

discretization
numerical variables may be continuous or discrete. a continuous variable is one which takes an infinite number of possible values within a range or an interval. a discrete variable is one which takes a countable number of distinct values. a discrete variable may take few values or a large number of values. discretization is a process that transforms a variable, either discrete or continuous, such that it takes a fewer number of values by creating a set of contiguous intervals  that spans the range of the variable's values. the set of intervals or the set of cut points produced by a discretization method is called a discretization.

discretization has several advantages. it broadens the range of classification algorithms that can be applied to datasets since some algorithms cannot handle continuous attributes. in addition to being a necessary pre-processing step for classification methods that require discrete data, discretization has been shown to increase the accuracy of some classifiers, increase the speed of classification methods especially on high-dimensional data, and provide better human interpretability of models such as if-then rule sets  <cit> . the impact of discretization on the performance of classifiers is not only due to the conversion of continuous values to discrete ones, but also due to filtering of the predictor variables  <cit> . variables that are discretized to a single interval are effectively filtered out and discarded by classification methods since they are not predictive of the target variable. due to redundancy and noise in the predictor variables in high-dimensional transcriptomic and proteomic data, such filtering of variables has the potential to improve classification performance. even classification methods like support vector machines and random forests that handle continuous variables directly and are robust to high dimensionality of the data may benefit from discretization  <cit> . the main disadvantage of discretization is the loss of information entailed in the process that has the potential to reduce performance of classifiers if the information loss is relevant for classification. however, this theoretical concern may or may not be a practical one, depending on the particular machine-learning situation.

discretization methods can be classified as unsupervised or supervised. unsupervised methods do not use any information about the target variable in the discretization process while supervised methods do. examples of unsupervised methods include the equal-width method, which partitions the range of variable's values into a user-specified number of intervals and the equal-frequency method, which partitions the range of variable's values into a user-specified fraction of instances per interval. compared to unsupervised methods, supervised methods tend to be more sophisticated and typically yield classifiers that have superior performance  <cit> . most supervised discretization methods consist of a score to measure the goodness of a set of intervals , and a search method to locate a good-scoring set of intervals in the space of possible discretizations. the commonly used fi method is an example of a supervised method.

a second way to categorize discretization methods is as univariate versus multivariate methods. univariate methods discretize a continuous-valued variable independently of all other predictor variables in the data, while multivariate methods take into consideration the possible interactions of the variable being discretized with the other predictor variables. multivariate methods are rarely used in practice since they are computationally more expensive than univariate methods and have been developed for specialized applications  <cit> . the fi discretization method is a typical example of a univariate method.

we now introduce terminology that will be useful for describing discretization. let d be a dataset of n instances consisting of the list , , ..., , ..., ) that is sorted in ascending order of xk, where xk is a real value of the predictor variable x and zk is the associated integer value of the target variable z. for example, suppose that the predictor variable represents the expression level of a gene that takes real values in the range  <dig> to  <dig>  and the target variable represents the phenotype that takes the values: healthy or diseased . then, an example dataset d is , , , , , ). let sa, b be a list of the first elements of d, starting at the ath pair in d and ending at the bth pair. thus, for the above example, s <dig>   <dig> = . for brevity, we denote by s the list s <dig>  n. let tb be a set that represents a discretization of s <dig>  b. for the above example of d, a possible 2-interval discretization is t <dig> = {s <dig>   <dig>  s <dig>  6} = {, }. equivalently, this 2-interval discretization denotes a cut point between  <dig>  and  <dig> , and typically the mid-point is chosen, which is  <dig>  in this example. thus, all values below  <dig>  are considered as a single discrete value and all values equal or greater than  <dig>  are considered another discrete value. for brevity, we denote by t a discretization tn of s.

fayyad and irani's  discretization method
fayyad and irani's discretization method is a univariate supervised method that is widely used and has been cited over  <dig> times according to google scholar <dig>  the fi method consists of i) a score that is the entropy of the target variable induced by the discretization of the predictor variable, and ii) a greedy search method that recursively discretizes each partition at a cutpoint that minimizes the joint entropy of the two resulting subintervals until a stopping criterion based on the minimum description length  is met.

for a list sa, b derived from a predictor variable x and a target variable z that takes j values, the entropy ent is defined as:   

where, p is the proportion of instances in sa, b where the target takes the value zj. the entropy of z can be interpreted as a measure of its uncertainty or disorder. let a cutpoint c split the list sa, b into the lists sa, c and sc +  <dig>  b to create a 2-interval discretization {sa, c, sc +  <dig>  b}. the entropy ent induced by c is given by:   

where, |sa, b| is the number of instances in sa, b, |sa, c| is the number of instances in sa, c, and |sc +  <dig>  b| is the number of instances in sc +  <dig>  b. the fi method selects the cut point c from all possible cut points that minimizes ent and then recursively selects a cut point in each of the newly created intervals in a similar fashion. as partitioning always decreases the entropy of the resulting discretization, the process of introducing cut points is terminated by a mdl-based stopping criterion. intuitively, minimizing the entropy results in intervals where each interval has a preponderance of one value for the target.

overall, the fi method is very efficient and runs in o time, where n is the number of instances in the dataset. however, since it uses a greedy search method, it does not examine all possible discretizations and hence is not guaranteed to discover the optimal discretization, that is, the discretization with the minimum entropy.

minimum optimal description length  discretization method
to our knowledge, the closest prior work to the ebd algorithm, which is introduced in this paper, is the modl algorithm that was developed by boulle  <cit> . modl is a univariate, supervised, discretization algorithm. both modl and ebd use dynamic programming to search over discretization models that are scored using a bayesian measure. ebd differs from modl in two important ways. first, modl assumes uniform prior probabilities over the discretization, whereas ebd allows an informative specification of both structure and parameter priors, as discussed in the next section. thus, although ebd can be used with uniform prior probabilities as a special case, it is not required to do so. if we have background knowledge or beliefs that may influence the discretization process, ebd provides a way to incorporate them into the discretization process.

second, the modl optimal discretization algorithm has a run time that is o, whereas the ebd optimal discretization algorithm has a run time of o, where n is the number of instances in the dataset. in essence, ebd uses a more efficient form of dynamic programming, than does modl. their difference in computational time complexity can have significant practical consequences in terms of which datasets are feasible to use. a dataset with, for example,  <dig>  instances might be practical to use in performing discretization using ebd, but not using modl.

while heuristic versions of modl have been described  <cit> , which give up optimality guarantees in order to improve computational efficiency, and heuristic versions of ebd could be developed that further decrease its time complexity as well, the focus of the current paper is on optimal discretization.

in the next section, we introduce the ebd algorithm and then describe an evaluation of it on a set of bioinformatics datasets.

RESULTS
an efficient bayesian discretization method
we now introduce a new supervised univariate discretization method called efficient bayesian discretization . ebd consists of i) a bayesian score to evaluate discretizations, and ii) a dynamic programming search method to locate the optimal discretization in the space of possible discretizations. the dynamic programming method examines all possible discretizations and hence is guaranteed to discover the optimal discretization, that is, the discretization with the highest bayesian score.

bayesian score
we first describe a discretization model and define its parameters. as before, let x and z denote the predictor and target variables, respectively, let d be a dataset of n instances consisting of the list , , ..., , ..., ), as described above, and let s denote a list of the first elements of d. a discretization model m is defined as:  

where, w is the number of intervals in the discretization, t is a discretization of s, and Œò is defined as follows. for a specified interval i, the distribution of the target variable p is modeled as a multinomial distribution with the parameters {Œ∏i  <dig> Œ∏i <dig> ...,Œ∏ij,...,Œ∏ij} where j indexes the distinct values of z. considering all the intervals, Œò = {Œ∏ij} over  <dig> ‚â§ i ‚â§ i and  <dig> ‚â§ j ‚â§ j and Œò specifies all the multinomial distributions for all the intervals in m. given data d, ebd computes a bayesian score for all possible discretizations of s and selects the one with the highest score.

we now derive the bayesian score used by ebd to evaluate a discretization model m. the posterior probability p of m is given by bayes rule as follows:   

where p is the prior probability of m, p is the marginal likelihood of the data d given m, and p is the probability of the data. since p is the same for all discretizations, the bayesian score evaluates only the numerator on the right hand side of equation  <dig> as follows:   

the marginal likelihood p in equation  <dig> is derived using the following equation:   

where Œò are the parameters of the multinomial distributions as defined above. equation  <dig> has a closed-form solution under the following assumptions:  the values of the target variable were generated according to i.i.d. sampling from p, which is modeled with a multinomial distribution,  the distribution p is modeled as being independent of the distribution p for all values of i and h such that i ‚â† h,  for all values i, prior belief about the distribution p is modeled with a dirichlet distribution with hyperparameters Œ±ij, and  there are no missing data. the closed-form solution to the marginal likelihood is given by the following expression  <cit> :   

where Œì is the gamma function, ni is the number of instances in the interval i, nij is the number of instances in the interval wi that have target-value j, Œ±ij are the hyperparameters in a dirichlet distribution which define the prior probability over the Œ∏ij parameters, and . the hyperparameters can be viewed as prior counts, as for example from a previous  dataset of instances in the interval i that belong to the value j. for the experiments described in this paper, we set all the Œ±ij to  <dig>  which can be shown to imply that a priori we assume all possible distributions of p to be equally likely, for each interval i. <dig> if all Œ±ij =  <dig>  then all Œ±i = j. with these values for the hyperparameters, and using the fact that Œì = !, equation  <dig> becomes the following:   

the term p in equation  <dig> specifies the prior probability on the number of intervals and the location of the cut points in the discretization model m; we call these the structure priors. the structure priors may be chosen to penalize complex discretization models with many intervals to prevent overfitting. in addition to the structure priors, the marginal likelihood p includes a specification of the prior probabilities on the multinomial distribution of the target variable in each interval; we call these the parameter priors. in equation  <dig>  the alphas specify the parameter priors.

the prior probability p is modeled as follows. let xk denote a real value of the predictor variable, as described above, and zk denote the associated integer value of the target variable. let prior be the prior probability of there being at least one cut point between xk and xk +  <dig>  in the methods section, we describe the use of a poisson distribution with mean Œª to implement prior, where Œª is a structure prior parameter. consider the prior probability for an interval i that represents the sequence  in a discretization model m. in general, we assume that the prior probability for interval i is independent of the prior probabilities for the other intervals in m. the prior probability for interval i in terms of the prior function is defined as follows:   

expression  <dig> gives the prior probability that no cut points are present between any consecutive pairs of values of x in the sequence  and at least one cut point is present between the values  and . using the above notation and assumptions, and substituting equations  <dig> and  <dig> into equation  <dig>  we obtain the specialized ebd score:   

the above score assumes that the n values of x in the dataset d are all distinct. however, the implementation described below easily relaxes that assumption.

dynamic programming search
the ebd method finds the discretization that maximizes the score given in equation  <dig> using dynamic programming to search the space of possible discretizations. the pseudocode for the ebd search method is given in figure  <dig>  it is globally optimal in that it is guaranteed to find the discretization with the highest score. additional details about the search method used by ebd and its time complexity are provided in the methods section.

the number of possible discretizations for a predictor variable x in a dataset with n instances is 2n- <dig>  and this number is typically too large for each discretization to be evaluated in a brute force manner. the ebd method addresses this problem by the use of dynamic programming that at every stage uses previously computed optimal solutions to subproblems. the use of dynamic programming reduces considerably the number of possible discretizations that have to be evaluated explicitly without sacrificing the ability to identify the optimal discretization.

an example of the application of the ebd method on the example dataset d = , , , , , ) is given in figure  <dig>  although there are  <dig> =  <dig> possible discretizations for a dataset of six instances, as in this example, ebd explicitly evaluates only  <dig> of them in determining the highest scoring discretization.

as described in the methods section, the ebd algorithm runs in o time, where n is the number of instances of a predictor x. although ebd is slower than fi, it is still feasible to apply ebd to high-dimensional data with a large number of variables.

evaluation of the efficient bayesian discretization  method
we evaluated the ebd method and compared its performance to the fi method on  <dig> biomedical datasets  using five measures: accuracy, area under the receiver operating characteristic curve , robustness, stability, and the mean number of intervals per variable . the last three measures evaluate the discretized predictors directly while the first two measures evaluate the performance of classifiers that are learned from the discretized predictors. we performed this comparison using the fi method, because it is so commonly used  in practice and  as a standard algorithmic benchmark for discretization methods.

in the type column, t denotes transcriptomic and p denotes proteomic. in the p/d column, p denotes prognostic and d denotes diagnostic. #t is the number of values of the target variable and #n is the number of instances in the dataset. #v is the number of predictor variables. m is the proportion of the data that has the majority target value.

for computing the evaluation measures we performed  <dig> √ó  <dig> cross-validation . for a pair of training and test folds, we learned a discretization model for each variable  for the training fold only and applied the intervals from the model to both the training and test folds to generate the discretized variables. for the experiments, we set Œª, which is user specified parameter introduced in figure  <dig> and in equation  <dig>  to be  <dig> . the parameter Œª is the expected number of cut points in the discretization of the variables in the domain. our previous experience with discretizing some of the datasets used in the experiments with fi indicated that the majority of the variables in these datasets have  <dig> or  <dig> intervals . we chose Œª to be  <dig>  as the average of  <dig> and  <dig> cut points.

we used two classifiers in our experiments, namely, c <dig>  and na√Øve bayes . c <dig>  is a popular tree classifier that accepts both continuous and discrete predictors and has the advantage that the classifier can be interpreted as a set of rules. the nb classifier is simple, efficient, robust, and accepts both continuous and discrete predictors. it assumes that the predictors are conditionally independent of each other given the target value. given an instance, it applies bayes theorem to compute the probability distribution over the target values. this classifier is very effective when the independence assumptions hold in the domain; however, even if these assumptions are violated, the classification performance is often excellent, even when compared to more sophisticated classifiers  <cit> .

accuracy is a widely used measure of predictive performance . the mean accuracies for ebd and fi for c <dig>  and nb are given in table  <dig>  ebd has higher mean accuracy on  <dig> datasets for each of c <dig>  and nb, respectively. fi has higher mean accuracy on  <dig> datasets and  <dig> datasets for c <dig>  and nb, respectively. ebd and fi have the same mean accuracy on  <dig> datasets and  <dig> datasets for c <dig>  and nb, respectively. overall, ebd shows an increase in accuracy of  <dig> % and  <dig> % for c <dig>  and nb, respectively. this increased performance is statistically significant at the 5% significance level on the wilcoxon signed rank test for both c <dig>  and nb.

accuracies for ebd and fi discretization methods are obtained from the application of c <dig>  and nb classifiers to the discretized variables. the mean and the standard error of the mean  for the accuracy for each dataset is obtained by  <dig> √ó  <dig> cross-validation. for each dataset, the higher accuracy is shown in bold font and equal accuracies are underlined.

the auc is a measure of the discriminative performance of a classifier that accounts for datasets that have a highly skewed distribution over the target variable . the mean aucs for ebd and fi for c <dig>  and nb are given in table  <dig>  for c <dig> , ebd has higher mean auc on  <dig> datasets, fi has higher mean auc on  <dig> datasets, and both discretization methods have the same mean auc on  <dig> datasets. for nb, ebd has higher mean auc than fi on  <dig> datasets, lower mean auc on  <dig> datasets, and the same mean auc on two datasets. overall, ebd shows an improvement in auc of  <dig> % and  <dig> % for c <dig>  and nb, respectively, and both increases in auc are statistically significant at the 5% level on the wilcoxon signed rank test.

aucs for ebd and fi discretization methods are obtained from the application of c <dig>  and nb classifiers to the discretized variables. the mean and the standard error of the mean  for the auc for each dataset is obtained by  <dig> √ó  <dig> cross-validation. for each dataset, the higher auc is shown in bold font and equal aucs are underlined.

robustness is the ratio of the accuracy on the test dataset to that on the training dataset expressed as a percentage . the mean robustness for ebd and fi for c <dig>  and nb are given in table  <dig>  for c <dig> , ebd has higher mean robustness on  <dig> datasets, fi has higher mean robustness on  <dig> datasets, and both have equivalent mean robustness on three datasets. for nb, ebd has better performance than fi on  <dig> datasets, worse performance on  <dig> datasets, and similar performance on two datasets. overall, ebd shows a small decrease in mean robustness of  <dig> % and  <dig> % for c <dig>  and nb, respectively, that are not statistically significant at the 5% level on the wilcoxon signed rank test.

the mean and the standard error of the mean  for robustness for each dataset is obtained by  <dig> √ó  <dig> cross-validation. for each dataset, the higher robustness value is shown in bold font and equal robustness values are underlined.

stability quantifies how different training datasets affect the variables being selected . the mean stabilities for ebd and fi are given in table  <dig>  overall, ebd has higher stability than fi, but only at an overall average of  <dig> , which nevertheless is statistically significant at the 5% significance level on the wilcoxon signed rank test.

the mean stability for each dataset is obtained by  <dig> √ó  <dig> cross-validation. for each dataset, the higher stability value is shown in bold font and equal stability values are underlined.

the mean fraction of predictor variables discretized to one interval , the mean number of intervals for predictor variables discretized to more than one interval , and the mean number of intervals for all predictor variables for each dataset is obtained by 10-fold cross-validation done ten times. for each dataset, the higher value is shown in bold font and equal values are underlined.

the results of the statistical comparison of the ebd and fi discretization methods using the wilcoxon paired samples signed rank test are given in table  <dig>  as shown in the table, the accuracy and auc of c <dig>  and nb classifiers were statistically significantly better at the 5% level when the predictor variables were discretized using ebd over fi. ebd was statistically significantly more stable to the variability of the datasets than fi. however, ebd was less robust, though not statistically significantly so, than fi and produced slightly more complex discretizations than fi.

in the first column the range of a measure is given in square brackets where n is the number of instances in the dataset. in the last column the number on top in the last column is the z statistic and the number at the bottom is the corresponding p-value. on all performance measures, except for the mean number of intervals per predictor, the z statistic is positive when ebd performs better than fi. the two-tailed p-values of  <dig>  or smaller are in bold, indicating that ebd performed statistically significantly better at that level.

running times
we conducted the experiments on an amd x <dig>  <dig> +  <dig>  ghz personal computer with 2gb of ram that was running windows xp. for the  <dig> datasets included in our study, on average to discretize all the predictor variables in a dataset, ebd took  <dig> seconds per training fold while fi took  <dig> seconds per training fold.

discussion
we have developed an efficient bayesian discretization method that uses a bayesian score to evaluate a discretization and employs dynamic programming to efficiently search and identify the optimal discretization. we evaluated the performance of ebd on several measures and compared it to the performance of fi. table  <dig> shows the number of wins, draws and losses when comparing ebd to fi on accuracy, auc, stability and robustness. on both accuracy and auc, which are measures of discrimination performance, ebd demonstrated statistically significant improvement over fi. ebd was more stable than fi, which indicates that ebd is less sensitive to the variability of the training datasets. fi was moderately better in terms of robustness, but not statistically significantly so. on average, ebd produced slightly more intervals per predictor variable, as well as a greater proportion of predictors that had more than one interval. thus, ebd produced slightly more complex discretizations than fi.

number of wins, draws and losses on accuracy, auc, robustness and stability for ebd and fi.

a distinctive feature of ebd is that it allows the specification of parameter and structure priors. although we used non-informative parameter priors in the evaluation reported here, ebd readily supports the use of informative prior probabilities, which enables users to specify background knowledge that can influence how a predictor variable is discretized. the alpha parameters in equation  <dig> are the parameter priors. suppose there are two similar biomedical datasets a and b containing the same variables, but different populations of individuals, and we are interested in discretizing the variables. the data in a could provide information for defining the parameter priors in equation  <dig> before its application to the data in b. there is a significant amount of flexibility in defining this mapping for using data in a similar  biomedical dataset to influence the discretization of another dataset. the lambda parameter in equation  <dig>  allows the user to provide a structure prior. this is where prior knowledge might be particularly helpful by specifying  the expected number of cut points per predictor variable. although we have presented a structure prior that is based on a poisson distribution, the ebd algorithm can be readily adapted to use other distributions. in doing so, the main assumption is that a structure prior of an interval can be composed as a product of the structure priors of its subintervals.

the running times show that although ebd runs slower than fi, it is sufficiently fast to be applicable to real-world, high-dimensional datasets. overall, our results indicate that ebd is easy to implement and is sufficiently fast to be practical. thus, we believe ebd is an effective discretization method that can be useful when applied to high-dimensional biomedical data.

we note that ebd and fi differ in both in the score used for evaluating candidate discretizations and in the search method employed. as a result, the differences in performance of the two methods may be due to the score, the search method, or a combination of the two. a version of fi could be developed that uses dynamic programming to minimize its cost function, namely entropy, in a manner directly parallel to the ebd algorithm that we introduce in this paper. such a comparison, however, is beyond the scope of the current paper. moreover, since the fi method was developed and is implemented widely using greedy search, we compared ebd to it rather than to a modified version of fi using dynamic programming search. it would be interesting in future research to evaluate the performance of a dynamic programming version of fi.

CONCLUSIONS
high-dimensional biomedical data obtained from transcriptomic and proteomic studies are often pre-processed for analysis that may include the discretization of continuous variables. although discretization of continuous variables may result in loss of information, discretization offers several advantages. it broadens the range of data mining methods that can be applied, can reduce the time taken for the data mining methods to run, and can improve the predictive performance of some data mining methods. in addition, the thresholds and intervals produced by discretization have the potential to assist the investigator in selecting biologically meaningful intervals. for example, the intervals selected by discretization for a transcriptomic variable provide a starting point for defining normal, over-, and under-expression for the corresponding gene.

the fi discretization method is a popular discretization method that is used in a wide range of domains. while it is computationally efficient, it is not guaranteed to find the optimal discretization for a predictor variable. we have developed a bayesian discretization method called ebd that is guaranteed to find the optimal discretization  and is also sufficiently computationally efficient to be applicable to high-dimensional biomedical data.

