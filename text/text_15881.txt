BACKGROUND
proteomics studies are widely used in the biomedical research as an investigation tool to gain understanding of biological processes under specific conditions. proteomics gives a detailed picture of the presence, integrity and/or modification of the whole mixture of proteins extracted by a source. for medical purposes, proteomics offers a diagnostic perspective for the early detection of pathologies as well as for the choice of the most effective therapy. in fact, samples as serum, plasma, and other kinds of extracts contain proteins for which the covalent structure may be modified by specific pathological states, which may induce or prevent processes as glycation, phosphorylation, methylation, or any other addition of a molecular group to the protein. as a more general case, a whole protein could be expressed or not under pathological conditions. in all these cases, the proteomics pattern analyzed by mass spectrometry techniques can evidence differences due to the pathology. similarly, comparative proteomics can be exploited to evaluate the effects of a specific therapy.

among the thousands of proteins and peptides present in a serum sample, which represent its proteome, few key signals may be significant markers of the pathological state, and their search within the proteome represents a still open field of research. the detection of the markers and their full characterization has a number of advantages, including the opportunity of using them for diagnostics uses and the improvement of the knowledge about the pathological effects at molecular level, required to develop new drugs and therapies.

mass spectrometry  <cit>  is the elective technique to characterize the proteome and its modification. the mass spectrum represents a molecular profile of the sample under analysis, obtained with increasing precision and automation techniques. despite the large number of signals obtained in the proteome analysis, molecular modifications can be detected and markers of pathological states can be identified. maldi-tof  is a common technology used in mass spectrometry, and seldi-tof  is also used as a modified form of maldi-tof. according to these techniques, proteins are co-crystallized with uv-absorbing compounds, then a uv laser beam is used to vaporize the crystals, and ionized proteins are then accelerated in an electric field. the analysis is then completed by the tof analyzer. differences in the two technologies, which reside mainly in the sample preparation, make seldi-tof more reliable for biomarkers discovery and other proteomic studies in biomedicine.

data produced by mass spectrometry are spectra, typically reported as vectors of data, describing the intensity of signals due to biomolecules with specific mass-to-charge  ratio values. given the high dimensionality of spectra, given their different length and since they are often affected by errors and noise, preprocessing techniques are mandatory before any data analysis. after preprocessing , several statistical and artificial intelligence based technologies could be used for mining these data.

a very important contribution of the application of mass spectrometry techniques to the classification of ovarian cancer is reported in  <cit>  where the authors suggest a stochastic search method for selecting a subset of feature which best separates between healthy and pathological cases; the classification is based on a clustering approach using self-organizing maps and the authors show that the proposed method is able to classify all cancer cases and 95% of healthy women. the same methodology has been also successfully applied to high resolution spectrometry in  <cit>  where the authors obtain a 100% sensitivity and specificity of classification over a random split  of the data. the feature selection approach of both papers follows an optimization procedure having as objective function the discrimination ability of the adopted classifier over the subset of selected features. from the geometrical point of view it can be described as the selection of a random projection of data onto a subspace where the selected patterns are best separated. however, one should consider that in this kind of problems we have a small set of data  in a very high dimensional space . therefore, there is the risk that this good separation into the subspace could just be due to a random effect depending on the sparseness of the data  <cit> . in order to avoid this risk, large scale cross validation should be applied for the correct evaluation of the prediction accuracy  <cit> .

another important issue to be addressed in the selection of features for classification is the way of performing feature selection and cross validation together. in particular, if the feature selection step is external to the cross validation procedure, as for example in  <cit> , i.e when the feature selection is done by using all the data and the performance evaluation by cross validation is performed just for the classification phase, then the obtained results may be severely biased due to the so called selection bias effect. an interesting experiment is reported in  <cit>  where the selection bias effect produces perfect classification even for completely fake datasets. a more proper approach should validate by cross validation both classification algorithms and feature selection, and this can be easily done by leaving the test samples out of the dataset before undergoing feature selection  <cit> . one of the main contribution of the present paper is the validation of the dataset of  <cit>  with a large scale cross validation study and the adoption of an unsupervised feature extraction showing that it is possible to classify the dataset with a very high accuracy without the selection bias effect.

the dataset adopted in the present work was also used in the paper  <cit>  where the authors developed a preprocessing based on the kolmogorov-smirnov test, restriction of coefficient of variation and wavelet analysis. the classification step is then performed, as in our case, with support vector machines. their method achieves an average sensitivity of  <dig> %  and an average specificity of  <dig> %  in  <dig> independent k-fold cross-validations; here we have a better sensitivity and specificity as reported in the results.

a study about the classification methods for ovarian cancer detection is reported in  <cit>  where the authors compared two feature extraction algorithms together with several classification approaches on a maldi tof dataset. the t-statistic was used to rank features in terms of their relevance. then two feature subsets were greedily selected . support vector machines, random forests, linear/quadratic discriminant analysis , k-nearest neighbors, and bagged/boosted decision trees were subsequently used to classify the data. in addition, random forests were also used to select relevant features with previously mentioned algorithms used for classification. when the t-statistic was used as a feature extraction technique, support vector machines, lda and random forests classifiers obtained the top three results . on the other hand, classification improved to approximately 92% when random forests were used as both feature extractors and classifiers. while these results appear promising, the authors provide little motivation as to why  <dig> and  <dig> feature sets were selected. here we do not fix a priori the number of features letting the algorithm select automatically the number of features as function of the percentage of energy to be preserved in the pca and the number of peaks in the analyzed average spectrum as reported below .

the pca dimensionality reduction approach was also used in  <cit>  with the dataset of  <cit>  coupled with a nearest centroid classifier for classification. when training sets were larger than 75% of the total sample size, perfect  accuracy was achieved on the oc-wcx2b data set. the author performed cross validation after feature selection, and as explained before, this results could be influenced by the selection bias effect. using only 50% of data for training, the performance dropped by  <dig> %. unfortunately, the probabilistic approach used in the study can leave some samples unclassified. for the oc-h <dig> data set, the system had a  <dig> % sensitivity and  <dig> % specificity when 75% of the data was used for training with only  <dig> % of the data samples classified.

RESULTS
the proposed feature extraction and classification method has been tested on a dataset available from the national cancer institute of the u.s. national institute of health consisting of  <dig> cancer samples and  <dig> control samples. each sample is an high resolution spectrum with about  <dig> points and m/z ranging from  <dig> to  <dig>  some results on these data have been published in  <cit>  and  <cit>  and are useful for comparison with the method proposed here.

in figure  <dig> the overall process used to test our solution is shown. after having independently corrected the baseline and re-sampled each spectrum, we started k-fold cross validation . as it is well known, in k-fold cross validation the data set is randomly divided in k sets; of the k sets, a set is retained as the validation data for testing the model, and the remaining k -  <dig> subsamples are used as training data. the cross-validation process is repeated k times, with each of the k subsets of samples used exactly once as the validation data. the k results from the folds are then averaged to produce a single estimation. using the training set we derive the normalization parameters that are used to normalize both the training and the test sets. the normalized training data set is then used for feature extraction  obtaining the m/z's of the peaks that best describe  each spectrum; these m/z's are then used to synthetically represent both the training spectra and the test spectra. then, the training set is used to obtain pca directions ; these directions are of course used to project both the training and the test sets. last, the training set is used to train our svm while the test set is clearly used to test the correct classification rate.

in order to get the results, all the classification experiments and estimated classification rates are averaged over  <dig> runs of the whole process. the main results are reported in figure 2: it contains the distribution of the accuracy of classification over each k-fold run on the test set and repeated  <dig> times. the classification accuracies on these runs clusters around the average with a gaussian shape having average  <dig>  and standard deviation  <dig>  * 10- <dig>  as we can notice, the classification accuracy is almost stable over the runs even if each run could eventually extract different peak sets to perform the classification. moreover, we obtain a 100% accuracy in some of the runs, just as in  <cit> , but without using peak selection based on the classification accuracy.

an interesting problem to be investigated is the feature stability of the feature extraction phase. indeed, since for a correct cross validation procedure, the feature selection must be performed inside each fold, it is possible that different folds lead to different feature sets. therefore the question whether there is a set of stable features which are maintained in different folds and runs arises. we run the whole feature extraction  <dig> times using 10-folds and, at each iteration, we compute the intersection of the selected feature positions with the previous set. the final result being the set of selected peaks which are shared among all the  <dig> runs.

we observe that the number of features rapidly decreases towards  <dig>  and that the algorithm tends to select repeatedly these core peaks. a further interesting observation concerns the fact that these stable features contain most of the peaks used by  <cit>  in their work as reported in table  <dig>  in particular, the results in the table show that the proposed method selects as stable features most of the values used by them and all the recurring m/z. since our data are subject to a re-sampling step before feature selection, the matching is measured up to a small approximation error.

the values in bold represent the features recurring between the various models reported by the authors. as it can be seen from the table, the proposed method selects as stable features most of the values and all the recurring m/z. since our data are subject to a re-sampling step before feature selection, the matching is measured up to a small approximation error.

since we are facing a binary classification problem, it is significant to plot the false positive rate  versus the true positive rates  obtaining the receiver operating characteristics  curve. in order to obtain a roc curve where all the samples are represented, we used a merging strategy of the test sets generated inside each fold. in this way we can plot a curve where each sample belongs to the test at least once. moreover, since each run of the cross validation produces a curve, we used a vertical averaging approach to combine all the graphs  <cit> . this means that each curve is treated as a function r such that r = tpr and assuming as averaged roc curve  = e  thus obtaining figure  <dig>  the graph in figure  <dig> shows a curve representing the diagnostic ability of the classifier. in particular the area under the curve  of a binary classifier can be interpreted as the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. in our case, for the curve of figure  <dig>  we obtained a high auc of  <dig> .

finally, if we want to consider the specificity and sensitivity of the classification method we obtained the results reported in table  <dig> as compared to that obtained in  <cit> . the table reports the averaged sensitivity and specificity over the  <dig> run of the 10-fold cross validation.

system tuning
as detailed in the section methods, there are some basic parameters which can influence the whole performance of our approach:

• the maximum scale of signal smoothing, σ

• the peak averaging window size, ,

• the amount of energy, in percentage, retained in the principal component analysis, e

• width of the kernel functions for the svm classifier, γ;

it is important to evaluate the influence of the parameters and how the performance of the classification depends on them. we ran a large set of experiments for this purpose having:

• σ varying in the interval  using a step of  <dig> ;

• ws varying in the interval ;

• e varying in the interval  using a step of  <dig> ;

• γ varying in the interval .

for each parameter configuration, k-fold  cross validation has been used to test the generalization performance. each test has been repeated  <dig> times, so that each set has been tested  <dig> times: in this phase, the mean correct classification value has been used as quality measure. figure  <dig> and figure  <dig> show the results we have obtained. in particular in the first figure we plot the various curves, representing the accuracy of classification in terms of σ, each curve in the plot is obtained by fixing γ and the various plots correspond to ws =  <dig>   <dig>   <dig> and e =  <dig> ,  <dig>   <dig>   <dig> . the various curves follow almost similar shapes and there is not a well defined maximum; rather the optimum is obtained over a quite large interval of the considered values. the same is true if we consider the accuracy in terms of the svm width γ as reported in figure  <dig> 

CONCLUSIONS
the paper presented the results obtained by applying a feature extraction procedure for mass spectra classification based on a scale-space analysis of the data. the features were then used to train a statistical classifier to discriminate between normal and cancer samples. in order to compare our results with state of the art methods, we adopted a public available dataset already analyzed by other researchers. we obtained an average accuracy of  <dig>  of correct classification,  <dig> % sensitivity and  <dig> % specificity over a large cross validation experiment designed to be free of the selection bias effect: this improves previously known results  <cit> . we also analyzed how stable the feature selection methods is and showed that over a large set of runs the method tends to select the same set of features.

another advantage of the adopted method consists in the use of the multi-scale properties of the spectra rather than a procedure based on the discrimination ability of the selected features. for the considered problem, with a high dimensional space and a small number of data points, optimal separating surfaces based on projection can be the results of chance rather than a subset of significant features. finally, we used the discrimination accuracy as figure of merit just to compute the optimal parameters of the feature selection step.

