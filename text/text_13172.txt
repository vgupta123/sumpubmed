BACKGROUND
the vector autoregressive regression  model is an approach to describe the interaction of variables through time in a complex multivariate system. it is very popular in economics  <cit>  but with few exceptions  <cit>  it has not been widely used in systems biology, where it could be employed to model genetic networks or metabolic interactions. one possible reason for this is that while the statistical properties of the var model are well explored  <cit> , its estimation from sparse data and subsequent model selection is very challenging due to the large number of parameters involved  <cit> .

in this paper we develop a procedure for effectively learning the var model from small sample genomic data. in particular, we describe a novel model selection procedure for learning causal var networks from time course data with only a few time points, and no or little replication. this procedure is based on regularized estimation of var coefficients, followed by subsequent simultaneous significance testing of the corresponding partial correlation coefficients.

once the var model has been learned from the data, it allows to elucidate possible underlying causal mechanisms by inspecting the granger causality graph implied by the non-zero var coefficients.

the remainder of the paper is organized as follows. in the next section we first give the definition of a vector autoregressive process and recall the standard estimation. subsequently, we describe our approach to regularized inference and to network model selection. this is followed by computer simulations comparing a variety of alternative approaches. finally, we analyze data from an arabidopsis thaliana expression time course experiment.

methods
vector autoregressive model
we consider vector-valued time series data x = ,...,xp). each component of this row vector corresponds to a variable of interest, e.g., the expression level of a specific gene or the concentration of some metabolite in dependence of time. the vector autoregressive model specifies that the value of x is a linear combination of those of earlier time points, plus noise,

x=c+∑i=1mxbi+εi.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwf4baecqggoaakcqwg0badcqggpaqkcqgh9aqpcqwfjbwycqghrawkdaaewbqaaiab=hha4jabcicaoiabdsha0jabgkhitiabdmgapjabdyeamjabcmcapiab=jeacnaabaaaleaacqwgpbqaaeqaaogaey4kascccmgae4xtdu2aasbaasqaaiabdmgapbqabagccqgguaglcawljagaaczcaiabcicaoiabigdaxiabcmcapawcbagaemyaakmaeyypa0jaegymaedabagaemyba0ganiabgghildaaaa@4eff@

in this formula m is the order of the var process, l the time lag, and c a  <dig> × p vector of means. the errors ∈i are assumed to have zero mean and a p × p positive definite covariance matrix Σ. the matrices bi with dimension p × p represent the dynamical structure and thus contain the information relevant for reading off the causal relationships.

the autoregressive model has the form of a standard regression problem. therefore, estimation of the matrices bi is straightforward. a special case considered in this paper is when both m and l are set to  <dig>  then the above equation reduces to the var process

x = c + xb + ε.     

we now denote the centered matrices of observations corresponding to x and x by xf  and xp , respectively, i.e. xp=
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwfybawdawgaawcbagaemicaahabeaakiabg2da9maadmaabaqbaeqabmqaaaqaaiab=hha4jabcicaoiabigdaxiabcmcapaqaaiablacilbqaaiab=hha4jabcicaoiabd6gaujabgkhitiabigdaxiabcmcapaaaaiaawufacagldbaaaaa@3e35@ and xf=
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwfybawdawgaawcbagaemozaygabeaakiabg2da9maadmaabaqbaeqabmqaaaqaaiab=hha4jabcicaoiabikdayiabcmcapaqaaiablacilbqaaiab=hha4jabcicaoiabd6gaujabcmcapaaaaiaawufacagldbaaaaa@3c46@. in this notation the ordinary least squares  estimate can be written as

b^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacuwfcbgqgaqcaaaa@2dd1@ols = - <dig> xp t xf.     

this is also the maximum likelihood  estimate assuming the normal distribution. the coefficients of higher-order var models may be obtained in a corresponding fashion  <cit> .

small sample estimation using james-stein-type shrinkage
genomic time course data contain only few time points  and often little or no replication – hence the above restriction on var models with unit lag. furthermore, it is known that for small sample size the least squares and maximum likelihood methods lead to statistically inefficient estimators. therefore, application of the var model to genomics data requires some form of regularization. for instance, a full bayesian approach could be used. however, for the var model the choice of a suitable prior is difficult  <cit> .

here, as a both computationally and statistically efficient alternative, we propose to apply james-stein-type shrinkage, a method related to empirical bayes  <cit> . this procedure has the advantage that it is computationally as simple as ols, yet still produces efficient estimates for small samples.

following  <cit>  we now review how an unconstrained covariance matrix may be estimated using shrinkage. in the next section we then show how this result may be used to obtain shrinkage estimates of var coefficients.

assuming centered data x for p variables  the unbiased empirical estimator of the covariance matrix is s=1n−1xtx
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwftbwucqgh9aqpcawla8+aasaaaeaacqaixaqmaeaacqwgubgbcqghsislcqaixaqmaagae8hwag1aawbaasqabeaacqwgubavaagccqwfybawaaa@3885@. for small number of observations s is known to be inefficient and also ill-conditioned  for n <p. a more efficient estimator may be furnished by shrinking the empirical correlations rij towards zero and the empirical variances vi against their median. this leads to the following expressions for the components of a shrinkage estimate s*:

skl∗=rkl∗vk∗vl∗     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgzbwcdaqhaawcbagaem4aasmaemibawgabagaey4fiocaaogaeyypa0jaemocai3aa0baasqaaiabdugarjabdygasbqaaiabgehiqaaakmaakaaabagaemoday3aa0baasqaaiabdugarbqaaiabgehiqaaakiabdaha2naadaaaleaacqwgsbabaeaacqghxiikaaaabeaakiaaxmaacawljawaaewaaeaacqai0aanaiaawicacaglpaaaaaa@4423@

with

rkl∗=rkl
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgybgcdaqhaawcbagaem4aasmaemibawgabagaey4fiocaaogaeyypa0jaeiikagiaegymaejaeyoei0cccigaf83udwmbakaadaqhaawcbagaegymaedabagaey4fiocaaogaeiykakiaemocai3aasbaasqaaiabdugarjabdygasbqabagccawljawaaewaaeaacqai1aqnaiaawicacaglpaaaaaa@41fb@

vk∗=λ^2∗vmedian+vk     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwg2bgddaqhaawcbagaem4aasgabagaey4fiocaaogaeyypa0dccigaf83udwmbakaadaqhaawcbagaegomaidabagaey4fiocaaogaemoday3aasbaasqaaiabd2gatjabdwgaljabdsgakjabdmgapjabdggahjabd6gaubqabagccqghrawkcqggoaakcqaixaqmcqghsislcuwf7oabgaqcamaadaaaleaacqaiyagmaeaacqghxiikaagccqggpaqkcqwg2bgddawgaawcbagaem4aasgabeaakiaaxmaacawljawaaewaaeaacqai2agnaiaawicacaglpaaaaaa@4e65@

and

λ^1∗=min⁡∑k≠lrkl2)     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwf7oabgaqcamaadaaaleaacqaixaqmaeaacqghxiikaagccqgh9aqpcyggtbqbcqggpbqacqggubgbcqggoaakcqaixaqmcqggsaaldawcaaqaamaaqababawaaecaaeaaieaacqgfwbgvcqgfhbqycqgfybgcaiaawkwaaiabcicaoiabdkhaynaabaaaleaacqwgrbwacqwgsbabaeqaaogaeiykakcaleaacqwgrbwacqghgjsucqwgsbabaeqaniabgghildaakeaadaaeqaqaaiabdkhaynaadaaaleaacqwgrbwacqwgsbabaeaacqaiyagmaaaabagaem4aasmaeyiyikraemibawgabeqdcqghris5aaaakiabcmcapiaaxmaacawljawaaewaaeaacqai3awnaiaawicacaglpaaaaaa@59f7@

λ^2∗=min⁡∑k=1p2).     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwf7oabgaqcamaadaaaleaacqaiyagmaeaacqghxiikaagccqgh9aqpcyggtbqbcqggpbqacqggubgbcqggoaakcqaixaqmcqggsaaldawcaaqaamaaqadabawaaecaaeaaieaacqgfwbgvcqgfhbqycqgfybgcaiaawkwaaiabcicaoiabdaha2naabaaaleaacqwgrbwaaeqaaogaeiykakcaleaacqwgrbwacqgh9aqpcqaixaqmaeaacqwgwbaca0gaeyyeiuoaaoqaamaaqadabagaeiikagiaemoday3aasbaasqaaiabdugarbqabagccqghsislcqwg2bgddawgaawcbagaemyba0maemyzaumaemizaqmaemyaakmaemyyaemaemoba4gabeaakiabcmcapmaacaaaleqabagaegomaidaaaqaaiabdugarjabg2da9iabigdaxaqaaiabdchawbqdcqghris5aaaakiabcmcapiabc6cauiaaxmaacawljawaaewaaeaacqai4aaoaiaawicacaglpaaaaaa@656b@

the particular choice of the shrinkage intensities λ^1∗
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwf7oabgaqcamaadaaaleaacqaixaqmaeaacqghxiikaaaaaa@3083@ and λ^2∗
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacuwf7oabgaqcamaadaaaleaacqaiyagmaeaacqghxiikaaaaaa@3085@ is aimed at minimizing the overall mean squared error.

shrinkage estimation of var coefficients
small sample shrinkage estimates of var regression coefficients may be obtained by appropriately substituting the empirical by the shrinkage covariance. more specifically, we need to proceed as follows:

 <dig>  we combine the centered observations xp and xf into a joint matrix Φ = . note that f contains twice as many columns as either xp or xf.

 <dig>  next, we consider the  multiple of the empirical covariance matrix, s = ΦtΦ, noting that s contains the two submatrices s <dig> = xp t xp and s <dig> = xp txf. this allows to write the ols estimate of var coefficients as b^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacuwfcbgqgaqcaaaa@2dd1@ols = -1s <dig> 

 <dig>  we replace the empirical covariance matrix s by a shrinkage estimate.

 <dig>  from s* we determine the submatrices s1∗
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwftbwudaqhaawcbagaegymaedabagaey4fiocaaaaa@2fef@ and s2∗
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwftbwudaqhaawcbagaegomaidabagaey4fiocaaaaa@2ff1@ which in turn allow to compute the estimates

b^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacuwfcbgqgaqcaaaa@2dd1@shrink = -1s2∗
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwftbwudaqhaawcbagaegomaidabagaey4fiocaaaaa@2ff1@.

by decomposing s* using the svd or cholesky algorithm it is possible to reconstruct pseudodata matrices xf∗
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwfybawdaqhaawcbagaemozaygabagaey4fiocaaaaa@305e@ and xp∗
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacqwfybawdaqhaawcbagaemicaahabagaey4fiocaaaaa@3072@. the above algorithm may be interpreted as ols or normal-distribution ml based on these pseudodata.

var network model selection
the network representing potential directed causal influences is given by the non-zero entries in the matrix of var coefficient. for an extensive discussion of the meaning and interpretation of the implied granger -causality we refer to  <cit> .

as b^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacuwfcbgqgaqcaaaa@2dd1@shrink is an estimate it is unlikely that any of its components are exactly zero. therefore, we need to statistically test whether the entries of b^
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiewacuwfcbgqgaqcaaaa@2dd1@shrink are vanishing. however, instead of inspecting regression coefficients directly, it is preferably to test the corresponding partial correlation coefficients: this facilitates small-sample testing and additionally allows to accommodate for dependencies among the estimated coefficients  <cit> .

specifically, consider in the var model the multiple regression that connects the first variable x <dig> at time point t +  <dig> with all variables x <dig> ...,xp at the previous time point t,

x1=c+βk1xk+∑j= <dig> j≠kpβj1xj+ error.     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwg4baedawgaawcbagaegymaedabeaakiabcicaoiabdsha0jabgucariabigdaxiabcmcapiabg2da9iabdogajjabgucarggaciab=j7ainaadaaaleaacqwgrbwaaeaacqaixaqmaagccqwg4baedawgaawcbagaem4aasgabeaakiabcicaoiabdsha0jabcmcapiabgucarmaaqahabagae8nsdi2aa0baasqaaiabdqgaqbqaaiabigdaxaaakiabdiha4naabaaaleaacqwgqbgaaeqaaogaeiikagiaemidaqnaeiykakiaey4kasiaagpavdwcbagaemoaaomaeyypa0jaegymaejaeiilawiaemoaaomaeyiyikraem4aasgabagaemicaahaniabgghildacbagccqgflbqzcqgfybgccqgfybgccqgfvbwbcqgfybgccqgguaglcawljagaaczcamaabmaabagaegyoakdacagloagaayzkaaaaaa@6781@

if in this equation the roles of xk and x <dig> are reversed,

xk=c+β1kx1+∑j= <dig> j≠kpβ1jxj+ error,     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwg4baedawgaawcbagaem4aasgabeaakiabcicaoiabdsha0jabcmcapiabg2da9iabdogajjabgucarggaciab=j7ainaadaaaleaacqaixaqmaeaacqwgrbwaaagccqwg4baedawgaawcbagaegymaedabeaakiabcicaoiabdsha0jabgucariabigdaxiabcmcapiabgucarmaaqahabagae8nsdi2aa0baasqaaiabigdaxaqaaiabdqgaqbaakiabdiha4naabaaaleaacqwgqbgaaeqaaogaeiikagiaemidaqnaeiykakiaey4kasiaagpavdwcbagaemoaaomaeyypa0jaegymaejaeiilawiaemoaaomaeyiyikraem4aasgabagaemicaahaniabgghildacbagccqgflbqzcqgfybgccqgfybgccqgfvbwbcqgfybgccqgfsaalcawljagaaczcamaabmaabagaegymaejaegimaadacagloagaayzkaaaaaa@6857@

the partial correlation between the two variables is the geometric mean of the corresponding regression coefficients, times their sign, i.e. β1kβk1sgn⁡
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadagcaaqaaggaciab=j7ainaadaaaleaacqaixaqmaeaacqwgrbwaaagccqwfyogydaqhaawcbagaem4aasgabagaegymaedaaaqabagccyggzbwccqggnbwzcqggubgbcqggoaakcqwfyogydaqhaawcbagaegymaedabagaem4aasgaaogaeiykakcaaa@3f0a@ <cit> .

once the partial correlations in the var model are computed, we use the "local fdr" approach of  <cit>  to identify significant partial correlations, similar to the network model selection for graphical gaussian models  of  <cit> . note that unlike in a ggm the edges in a var network are directed.

we point out that recently two papers have appeared describing related strategies for var model selection. as in our algorithm the strategies pursued in both  <cit>  and  <cit>  also consist in choosing the var network by selecting the appropriate underlying partial correlations. however, the key advantage of our variant of var network search is that it is specifically designed to meet small sample requirements, by using shrinkage estimators of regression coefficients and partial correlation, and due to the adaptive nature  of the "local fdr" algorithm  <cit> .

RESULTS
simulation study
in a comparative simulation study we investigated the power of diverse approaches to recovering the true var network. we simulated var data of different sample size, with n varying between  <dig> and  <dig>  for  <dig> randomly generated true networks with  <dig> edges and p =  <dig> nodes. the  <dig> nonzero regression coefficients were drawn uniformly from the intervals  and .

in addition to the shrinkage procedure we estimated regression coefficients by ordinary least squares  and by ridge regression . all these three regression strategies were applied in conjunction with the above var model selection based on partial correlations, with a cutoff value for the "local fdr" statistic set at  <dig>  – the recommendation of  <cit> . as a fourth method we employed l <dig> regression  <cit>   to estimate var regression coefficients. note that in the latter instance there is no need for additional model selection, as the lasso method combines shrinkage and model selection and automatically sets many regression coefficients identically to zero.

in the simulations we ran ols only for n >  <dig>  as for small sample size the corresponding empirical covariance matrix is singular and consequently the ols regression is ill-posed. the penalty for the lasso regression was chosen as in  <cit> . the regularization parameter in rr was determined by generalized cross validation  <cit> . unfortunately, even gcv turned out to be computationally expensive, so that for rr we conducted only  <dig> repetitions, rather than the  <dig> considered for the other methods.

the results of the simulations are summarized in figure  <dig>  the left box shows the positive predictive value, or true discovery rate of the four methods. this is the proportion of correctly identified edges in relation to all significant edges. our proposed shrinkage algorithm is the only method achieving around 80% positive predictive value regardless of the sample size. note that this is exactly the theoretically expected value, given the specified "local fdr" cutoff of  <dig> . in contrast, the rr and lasso methods perform remarkably poor at small sample size, with much lower true discovery rates. for medium to large sample size the ols estimation dominates rr, lasso and the shrinkage approach. this is easily explained by the fact that ols has no parameters to optimize and that it is asymptotically optimal. however, it is bothering that for both the rr and the ols approach the false discovery rate appears not to be properly controlled. finally, for large sample size the stein-type estimator appears to be prone to overshrinking, which leads to an increase of false positives.

the relative performance of the four approaches to var estimation can be further explained by considering the relative amount of true and false positive edges . the shrinkage method generally produces very few false positives. in contrast, the rr and lasso methods lead to a large number of false edges, especially for small sample size. this is particularly pronounced for the lasso regression, as can be seen in the differently scaled inlay plot contained in the right box of figure  <dig>  indicating that the penalty applied in the l <dig> regression may not be sufficient in this situation. in terms of the number of correctly identified edges the rr and shrinkage approach are the two top performing methods. however, even though rr finds a considerable number of true edges even at very small sample size, this has little impact on its true discovery rate because of the high number of false positives.

in summary, the simulation results suggest to apply for small sample size the james-stein-type shrinkage procedure, and for n >p the traditional ols approach.

analysis of a microarray time course data set
for further illustration we applied the var shrinkage approach to a real world data example. specifically, we reanalyzed expression time series resulting from an experiment investigating the impact of the diurnal cycle on the starch metabolism of arabidopsis thaliana  <cit> .

we downloaded the calibrated signal intensities for  <dig>  probes and  <dig> time points for each of the two biological replicates from experiment no.  <dig> of the nascarrays repository  <cit> . after log-transforming the data we filtered out all genes containing missing values and whose maximum signal intensity value was lower than  <dig> on a log-base  <dig> scale. subsequently, we applied the periodicity test of  <cit>  to identify the probes associated with the day-night cycle. as a result, we obtained a subset of  <dig> genes that we further analyzed with the var approach.

we note that a tacit assumption of the var model is that time points are equidistant – see eq.  <dig>  this is not the case for the arabidopsis thaliana data which were measured at  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig> hours after the start of the experiment. however, as the intensity of the biological reactions is likely to be higher at the change points from light to dark periods , one may argue that assuming equidistant measurements is justifiable at least in terms of equal relative reaction rate.

a further implication of the var model  is that dependencies among genes are essentially linear. this can easily be checked by inspecting the pairwise scatter plots of the calibrated expression levels. for the  <dig> considered arabidopsis thaliana genes we verified that the linearity assumption of the var model is indeed satisfied.

subsequently, we estimated from the replicate time series of the  <dig> preselected genes the regularized regression coefficients and the corresponding partial correlations, and identified the significant edges of the var causal graph as described above. we found a total number of  <dig>   <dig> significant edges connecting  <dig> nodes. in figure  <dig> we show for reasons of clarity only the subnetwork containing the  <dig> most significant edges, which connect  <dig> nodes. note that this graph exhibits a clear "hub" connectivity structure , which is particularly striking for the node  <dig> but also for nodes  <dig>   <dig>   <dig> and a few other genes .

as the var network contains directed edges it is possible to distinguish genes that have mostly outgoing arcs, which could be indicative for key regulatory genes, from those with mostly ingoing arcs. in the graph of figure  <dig> node  <dig>  an ap <dig> transcription factor, and node  <dig>  a gene involved in dna-directed rna polymerase, belong to the former category, whereas for instance node  <dig>  a structural constituent of ribosome, seems to be part of the latter. node  <dig> is another hub in the var network, which according to the annotation of  <cit>  encodes a protein of unknown function. another interesting aspect of the var network is the web of highly connected genes  which we hypothesize to constitute some form of a functional module.

finally, we note that the var network visualizes influences of the genes over time, hence a var graph can also include directed loops and even genes that act upon themselves. in contrast, the ggm graphs discussed in  <cit>  visualize the partial correlation with no time lag involved. for comparison, we display the ggm graph for the arabidopsis thaliana data in figure  <dig>  as expected, both graphs share the same structure : if one node influences another in the next timepoint with a constant regression coefficient , they also tend to be significantly partially correlated in the same time point . however, using a ggm it is not possible to infer the causal structure of the network.

CONCLUSIONS
we have presented a novel algorithm for learning var causal networks. this is based on james-stein-type shrinkage estimation of covariances between different time points of the conducted experiment, that in turn leads to improved estimates of the var regression coefficients. subsequent var model selection is conducted by using "local fdr" multiple testing on the corresponding partial correlations.

we have shown that this approach is well suited for the small sample sizes encountered in genomics. in addition, the approach is computationally very efficient, as no computer intensive sampling or optimization is needed: the inference of the directed network for the arabidopsis thaliana data with  <dig>   <dig> potentially directed edges takes about one minute on a standard desktop computer. while we have illustrated the approach by analyzing a microarray expression data set, it is by no means restricted to this kind of data – we expect that our var network approach performs equally well for similar high dimensional time series data from metabolomic or proteomic experiments.

the current algorithm employs a fixed "one step ahead" time lag. one strategy to generalization to arbitrary time lags may be to consider functional data – see, e.g.,  <cit> . this would have the additional benefit to suitable deal with non-equally spaced measurements, a common characteristic of many biological experiments.

authors' contributions
both authors participated in the development of the methodology and wrote the manuscript. r.o. carried out all analyzes and simulations. all authors approved of the final version of the manuscript.

supplementary material
additional file 1
this pdf file contains a multi-page table containing the annotation data  for the  <dig> genes displayed in the var network of figure  <dig> 

click here for file

 acknowledgements
we thank papapit ingkasuwan for pointing us to the arabidopsis thaliana data set, and the referees for their valuable comments. this work was supported by the deutsche forschungsgemeinschaft  via an emmy-noether research grant to k.s.

this article has been published as part of bmc bioinformatics volume  <dig>  supplement  <dig>  2007: probabilistic modeling and machine learning in structural and systems biology. the full contents of the supplement are available online at .

figures and tables
