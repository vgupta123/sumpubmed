BACKGROUND
multiple genome alignment is an important problem in bioinformatics. it is used in comparative studies to help find new genomic features such as genes and regulatory elements. current multiple genome alignment programs  <cit>  use progressive alignment  <cit> , with a phylogenetic tree as a reference.

the primary operation of progressive alignment is the alignment of two multiple alignments. most genome aligners have two main phases: anchoring and aligning between the anchors. here, we focus on the algorithms used to align between anchors. many popular alignment algorithms for dna use the sum-of-pairs heuristic, where the score of a multiple alignment is the sum of the induced pairwise alignment scores. however, just  <cit>  has shown that finding the optimal alignment of two multiple alignments under the sum-of-pairs heuristic is np-hard. of course, since the problem is important, numerous heuristic algorithms  <cit>  exist for alignment of alignment under sum of pairs.

the biological meaning of the sum-of-pairs of pairs heuristic is not obvious. additionally, many heuristic algorithms are complicated and slow, compared to pairwise alignment algorithms  <cit> . an alternative approach to the strategy of aligning alignments under the sum-of-pairs heuristic is to first infer ancestral sequences for each alignment and then align the two ancestral sequences. in addition to being fast, this method has a clear biological basis that takes into account the evolution implied by the underlying tree.

bray and pachter use this approach to align alignments in mavid  <cit> . mavid uses maximum likelihood to infer ancestral sequences for alignments. in our previous work  <cit> , we infer ancestral sequences for anchoring multiple alignments, but not for aligning alignments. instead of maximum likelihood, we use parsimony to infer ancestral sequences, but we also allow these sequences to keep some ambiguity. the idea behind allowing ambiguity is to retain as much information about the underlying multiple alignment as possible. a natural question is whether aligning such ambiguous ancestral sequences leads to better alignments than aligning unambiguous ancestral sequences.

in this study, we explore this idea as well as other aspects of aligning alignments by ancestral sequence inference. we compare four ancestral alignment methods with two sum-of-pairs alignment algorithms. we infer ancestral sequences using parsimony and maximum likelihood, and study the effect of allowing ambiguity in these sequences. since we are interested in the performance of these methods under optimal conditions, we use data generated by a very simple evolution simulation. for aligning full alignments with the sum-of-pairs heuristic, we use clustalw  <cit>  and a newer heuristic by ma, wang, and zhang  <cit> .

we find that alignment algorithms based on the sum-of-pairs heuristic are more accurate than all of our methods based on ancestral sequence alignment. however for alignment of inferred ancestral sequences, parsimony outperforms maximum likelihood in this application. using maximum likelihood to infer ancestral sequences results in final alignment accuracies that are more unpredictable. also, computing log-odds for ancestral sequences inferred with maximum likelihood is far more computationally intensive than computing log-odds scores for ancestral sequences inferred with parsimony. finally, we find that allowing ancestral sequences to have ambiguity does not result in more accurate final alignments.

RESULTS
to determine whether using ambiguous symbols in ancestral sequences inference improves multiple alignment, we have performed experiments on simulated sequences. we propose five hypotheses, explain our experimental method, and finally discuss results and give conclusions.

our hypotheses
the first hypothesis is that by using ancestral sequences with ambiguity, we obtain more accurate multiple alignments. ambiguous symbols may allow us to retain more information about the underlying multiple alignments, which may make it easier to identify matching positions. combined with an appropriate log-odds scoring system, this extra information may allow for more accurate alignment of ancestral sequences, and by extension, for more accurate multiple alignments.

our second hypothesis is that alignment of ancestral sequences is more sensitive to gap open costs than alignment of alignments using the sum-of-pairs heuristic. when aligning ancestral sequences, existing gaps in the underlying alignment are not considered when inserting a new gap, so the first position of a new gap always costs the gap open cost. incorrect gap penalties may cause too many gaps to be inserted between ancestral sequences. during progressive alignment, errors at each step propagate leading to an incorrect final alignment. in contrast, when aligning alignments using the sum-of-pairs heuristic, the cost of adding a new gap depends on all underlying gaps as well as the gap open cost. an incorrect gap open cost affects the cost of gaps less and new gaps may still be correctly inserted based on the structure of the existing gaps. 

our third hypothesis is that the function used to estimate the gap open cost during progressive alignment is important to alignment accuracy when aligning ancestral sequences. when aligning ancestral sequences, the frequencies of gaps in the ancestral sequences depends on the amount of mutation between the sequences. therefore, it is important to modify the gap open cost based on the distance between the ancestral sequences being aligned.

our fourth hypothesis is that we expect that aligning alignments using the sum-of-pairs heuristics gives more accurate multiple alignments than aligning inferred ancestral sequences. there are two reasons for this. first, as stated in hypothesis two, using an incorrect gap open cost affects the sum-of-pairs heuristic less than it affects the alignment of ancestral sequences. since choosing the correct gap open cost can be difficult in practice, we expect that aligning alignments using the sum-of-pairs heuristic results in a more accurate final alignment because it is less sensitive to this parameter. additionally, the ancestral sequences we infer are not completely accurate, which compounds the errors made in the process of progressive alignment. thus, the much slower run times of algorithms based on the sum-of-pairs heuristic are acceptable.

finally, we expect that the maximum likelihood methods result in better multiple alignments than the parsimony methods. unlike parsimony, maximum likelihood uses the edge distances on the phylogenetic tree. thus we expect maximum likelihood to better infer ancestral sequences.

experimental data
we use synthetic data in order to have correct alignments to test our methods against. additionally, by generating our own data we ensure that the data is generated from the same model of evolution that is required for the alignment algorithm. therefore, we consider the performance of the algorithms on this data to be the the best possible for algorithms of their type.

phylogenetic trees
despite our use of synthetic data, we want our data to mimic the basic properties of real biological sequence. thus we generate random trees that resemble real trees, and assign mutation rates based on analysis of real sequences.

specifically, we are interested in algorithm performance on two different types of random trees: trees with a period of heavy growth followed by a period of no growth, and trees with a period of light growth followed by a period of heavy growth. the first type of tree, which we refer to as early growth, is similar to the tree of placental mammals from eizirik, murphy, and o'brien  <cit> . the second type of tree, which we refer to as late growth, is similar to the type of tree formed in a coalescent process of neutral mutation and speciation. we generate two sets of twenty phylogenetic trees, one set for each tree type. each individual tree has eight taxa. we have chosen to limit our trees to trees with four taxa in each subtree of the root, both for ease of programming and to keep the time required to compute the log-odds scores for maximum likelihood reasonable.

we generate these in two steps. first, we generate one large tree of each type using a random birth-death process implemented in phyl-o-gen v <dig>   <cit> . to generate the early growth tree we start with a pure birth process with a birth rate of  <dig>  events per million years. after we produce  <dig> lineages we switch to a birth-death process with both the birth and death rate set to  <dig>  per million years. this episode lasts seven times as long as the first episode. we sample  <dig> lineages from the result, and this becomes our final tree, in figure  <dig>  we chose these values such that the tree resembles the tree of placental mammals both in topology and time scale. to generate the late growth tree we start with a pure birth process with a birth rate of  <dig>  speciation events per million years. after eight speciation events, we switch to a birth rate of  <dig>  speciation events per million years. we continue the process until we obtain  <dig> lineages. again, we obtain a final tree by sampling  <dig> lineages from the result of the random process. this tree is shown in figure  <dig>  here we choose values such that the tree has the same time scale and taxa as the early growth tree, but with a different topology.

from each final tree, using the method of kearney, munro, and phillips  <cit> , we randomly sample subtrees of eight taxa subject to the constraint that the amount of simulated time from the root to each taxon is between  <dig> to  <dig> million years. we then filter the resulting trees and only keep trees where the the left and right children have four descendants each. examples of two of these trees are in figure  <dig> 

random sequences
for each of the eight taxon trees in our two data sets, we generate twenty random sequence sets by simulating evolution over the tree. we use a program written by us, but similar to rose  <cit> , to generate our sequences. for a given input tree, our program starts with an initial random sequence and mutates that sequence into new sequences down the tree. the program simulates jukes/cantor mutation events  <cit>  as well as geometrically distributed insertion and deletion events with a mean length of  <dig> . at the end, we have a set of sequences suitable as input to a multiple alignment program, and we have the original multiple alignment of the sequences.

since we wanted our mutations, insertions, and deletions to be as close to real sequences as possible, we calibrated our simulator with parameters estimated through analysis of homologous human and baboon sequences. this gives us two sets of  <dig> random input sequences, one for each set of trees.

we chose the cftr region in human and baboon for this parameter estimation, so that we could ensure that our alignment is mostly correct. we obtained human and baboon sequences with repeats masked out from the nisc comparative vertebrate sequencing project  <cit> . we aligned a  <dig> kb region from each sequence and trimmed the ends of the alignments to obtain a final good alignment. from this final alignment, we measure the number of mutations as well as the length and number of gaps. assuming that humans and baboons diverged approximately  <dig> million years ago ,  <cit> , and using the equation

pr = 3/ <dig> ,     

where t is the time in millions of years  <cit> , we estimate the mutation rate α to be  <dig>  × 10- <dig> mutations per site per million years. we observe a rate of  <dig>  × 10- <dig> insertions or deletions per site. assuming that insertions and deletions are equally likely and that we have only one possible insertion or deletion at a particular site, we find that pr = pr =  <dig>  × 10- <dig> events per site per million years. additionally, we create another two sets of random input sequences using the same sets of trees, but with double the mutation rate on each tree branch.

experimental methods
we use the same insertion and deletion rates from our evolution simulator to compute the log-odds scores for the maximum likelihood methods. for a mean gap length of  <dig> , we compute the optimal gap extension penalty to be  <dig>  by standard methods  <cit> . when aligning alignments using the standard sum-of-pairs heuristic, any single gap cost is wrong for many pairs. therefore, we should ideally use a different gap cost for each pair. however, as this increases the time complexity of ma, wang, and zhang's algorithm, we instead use a single, unscaled gap cost for all pairwise alignments.

we test our first hypothesis by running all ancestral alignment methods and sum-of-pairs methods on all data sets using the optimal gap extension costs and a base gap open cost of  <dig>  we use the expected gap open cost estimation function since later tests show it to be better than the max estimation function. to test our second hypothesis, we expand the previous test by exploring gap costs of  <dig> and  <dig>  we test our third hypothesis by using two different scaling methods for each of the ancestral alignment methods. our fourth and fifth hypothesis are also answered by the above three tests.

measuring success
we take the fraction of correct columns in an alignment to be the measure the alignment's accuracy. a correct column is one which contains the exact same nucleotides, from the same positions in the sequences, as a column in the correct alignment; a column with the same bases as a correct column, but from different positions, is incorrect. for a given data set and algorithm, we use the mean alignment accuracy of all  <dig> sequence data sets as a measure of the algorithm's accuracy.

we also considered an accuracy measure based on the pairwise alignments induced by the multiple alignment. in this, we compute the alignment accuracy of all induced pairwise alignments as in the previous paragraph and take the mean of these.

discussion
before we discuss our results, we compare the two accuracy measures: column accuracy and mean pairwise column  accuracy. tables  <dig> and  <dig> show that both measures have the same trends. additionally, figure  <dig> shows that the column accuracies and the mpc accuracies have a roughly linear relationship; the mpc accuracies are strictly higher than the column accuracies. therefore, we do not give mpc accuracies for the experimental results in tables  <dig> and  <dig>  we now describe our experimental results with respect to our five hypotheses.

it is not clear that including ambiguity in ancestral sequences improves alignment. looking at tables  <dig> and  <dig>  we see that on more than half the data sets, the ambiguous versions of parsimony and maximum likelihood have lower mean accuracies than their unambiguous counterparts. additionally, ambiguous methods are less consistent in their scores, as evidenced by the larger standard deviations in the results for the ambiguous methods.

our experiment confirmed our hypothesis that the gap cost scaling function is very important to the resulting alignment accuracies. when changing from scaling based on the largest value in the ancestral sequence scoring matrix to the expected cost for related positions, we see a significant increase in alignment accuracy on all data sets and all methods. see table  <dig> for results.

in tables  <dig>   <dig>  and  <dig>  we see that both ma, wang, and zhang's algorithm and clustalw perform consistently better than the ancestral alignment methods. also, ma et al.'s algorithm performs better than clustalw in most cases. however, the gap open cost affects ma et al.'s algorithm and clustalw differently. depending on the choice of gap open cost, clustalw may perform better than ma et al.'s algorithm. looking at all examined gap costs, ma et al.'s algorithm achieves the highest alignment accuracy on each data set.

surprisingly, the maximum likelihood methods performed worse than parsimony methods in the context of ancestral alignment. in tables  <dig>   <dig>  and  <dig>  unambiguous maximum likelihood always performs worse than unambiguous parsimony. in some cases, such as on the late growth data set in table  <dig>  maximum likelihood performs drastically worse obtaining a mean alignment accuracy of  <dig> % versus  <dig> %. looking at the mean alignment accuracy alone, it is not clear that ambiguous maximum likelihood is worse than ambiguous parsimony. however, on the late growth data set ambiguous parsimony does much worse than ambiguous parsimony. on other data sets, it performs similarly. therefore, we conclude that ambiguous parsimony is more reliable than ambiguous maximum likelihood, if not more accurate.

CONCLUSIONS
we have tested four ancestral alignment methods as well as two sum-of-pairs alignment methods on simulated data. the data mimics evolution on two types of evolutionary trees: trees with a period of rapid growth followed by a period of slow growth, and trees with a period of slow growth followed by a period of rapid growth. the four ancestral alignment methods we have tested are unambiguous parsimony, ambiguous parsimony, unambiguous maximum likelihood, and ambiguous maximum likelihood. the sum-of-pairs alignment methods we have tested are the clustalw  <cit>  algorithm and the algorithm of ma, wang, and zhang  <cit> .

we have found that, contrary to our hypotheses, allowing ambiguity in ancestral sequences does not lead to better alignments. when we use ambiguous ancestral sequences, we find that the multiple alignment is more sensitive to our choice in gap costs than to the form of ancestral sequence chosen. reinforcing this conclusion, we find that the gap open cost scaling function is also extremely important to obtaining good scores when aligning ancestral sequences. finally, to our surprise, using maximum likelihood to infer ancestral sequences resulting in less accurate alignments than using parsimony. the reason for this is that the maximum likelihood method is far more sensitive to the underlying data and therefore resulting in alignments accuracies that have a large amount of variation. also, on the data set generated from the tree that has a small amount of growth followed by a large amount of growth, the maximum likelihood based methods did particularly poorly compared to the parsimony based methods.

finally, both the sum-of-pairs approaches did better than all the ancestral alignment methods, as expected. additionally, we found that ma, wang, and zhang's algorithm  <cit>  outperformed clustalw  <cit>  by a small amount.

