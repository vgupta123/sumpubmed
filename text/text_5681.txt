BACKGROUND
when performing gene-expression analysis for inference of relationships between genes and conditions/phenotypes, one typically must analyze a small number of samples, each composed of expression values from tens of thousands of genes. in this setting the observed data is x ∈ ℝp×n, where each column corresponds to one of n samples, quantifying the associated gene-expression values for all p genes under investigation. we typically must address the "large p, small n" problem  <cit> , in which often n ≪ p. therefore, to yield reliable inference, one must impose strong restrictions on the form of the model.

when developing regression and classification models for gene-expression data, a widely employed assumption  is that the model parameters are sparse, implying that only a small subset of the genes are important for prediction. if only a small set of genes  are responsible for differences in disease groups, then reliable inference may often be performed even when n ≪ p. example approaches that have taken this viewpoint are lasso  <cit> , the elastic net  <cit> , and related bayesian approaches  <cit> . in fact, sparse regression and classification algorithms are widely used in many statistics and machine-learning applications, beyond gene analysis  <cit> .

an important research direction for gene-expression analysis, and many other applications, involves the use of factor models  <cit> . to address the "large p, small n" problem, sparseness is again imposed, now typically on the factor loadings. specifically, in an unsupervised setting the data are assumed to satisfy

  x=as+e 

where a ∈ ℝp×r, s ∈ ℝr×n and e ∈ ℝp × n; if covariates are available they may also be considered in the model  <cit> , with none assumed here. note that here and henceforth we assume that the gene-expression data are centered in advance of the analysis; otherwise, there should be an intercept added to the model. considering the jth sample, xj , corresponding to the jth column of x, the model states that xj = asj + ej , where sj and ej are the jth columns of s and e, respectively.

the columns of a represent the factor "loadings", and rows of s are often called factors. to address the fact that n ≪ p, researchers have typically imposed a sparseness constraint on the columns of a  <cit> , with the idea that each column of a should ideally  correspond to a biological "pathway", which should be defined by a relatively small number of correlated genes. within bayesian formalisms, the sparse columns of a are typically imposed via spike-slab-like priors  <cit> ,  <cit> , or alternatively via shrinkage  priors. several non-bayesian approaches have also been introduced, including sparse-pca  <cit>  and the related penalized matrix decomposition   <cit> .

a problem that is receiving increased attention in factor-analysis-based approaches is a means of defining an appropriate number of factors . the non-bayesian approaches are often sequential, and one may infer r by monitoring the error ||e||f as a function of iteration number  <cit> ,  <cit> . in many previous bayesian approaches r has just been set  <cit> , and presumably many non-biologically-relevant factor loadings are inferred. a computationally expensive reverse-jump mcmc approach has been developed  <cit> , with computational efficiency improved in  <cit>  while also considering a default robust prior specification. perhaps the most widely employed approach  <cit>  for choosing r is the bayesian information criteria . a disadvantage is that conditioning on a fixed choice of the number of factors ignores uncertainty and the bic is not well justified in hierarchical models, as the number of parameters is unclear.

there has been recent interest in applying nonparametric bayesian methods  <cit> ,  <cit>  to infer r , based on the observed data x. an example of recent research in this direction employs the indian buffet process   <cit> ,  <cit> . in this paper we also consider the beta process , recognizing that the bp and ibp are closely linked  <cit> ,  <cit> .

for data sets with very large p , computational efficiency is of major practical importance. in previous use of nonparametric bayesian methods to this problem, a gibbs sampler has typically been employed  <cit> . the bp-based formulation admits a relatively simple variational bayesian   <cit>  approximation to the posterior, which is considerably faster than gibbs sampling. an advantage of a vb analysis, in addition to speed, is that convergence is readily monitored . we perform a comparison of the difference in inferred model parameters, based on vb and gibbs analysis.

the specific data on which the models are employed correspond to gene-expression data from recent viral challenge studies. specifically, after receiving institutional review board  approvals from duke university, we performed three separate challenge studies, in which individuals were inoculated with respiratory syncytial virus , rhino virus, and influenza. informed consent was used in all studies. using blood samples collected sequentially over time, we have access to gene-expression data at pre-inoculation, just after inoculation, and at many additional time points up to the point of full symptoms . using these data, we may investigate time-evolving factor scores of samples, to examine how the response to the virus evolves with time. of particular importance is an examination of the factors of importance for individuals who became symptomatic relative to those who did not. in the factor analysis we consider data individually for each of the three viruses , as well as for all three viruses in a single analysis . results are generated based on nonparametric bayesian approaches to factor analysis, employing the beta process, the indian buffet process, and a related sparseness-constrained pseudo-svd construction . we also make comparisons to the non-bayesian penalized matrix decomposition   <cit> .

ii. 
RESULTS
a. brief summary of models
we first provide a brief intuitive explanation of the workings of the different bayesian models considered. these models are built around the indian buffet process   <cit> , so named for the following reason. in the factor model of , the columns of a represent factor loadings in which the gene-expression values for sample j are expressed: xi = asj + ej . one construction of the ibp constitutes a set of candidate columns of a, and these are termed "dishes" at an indian "buffet". each of the n samples {xj}j =  <dig> n correspond to "customers" at the buffet; each customer selects a subset of dishes from the buffet . the ibp is constructed such that the more a particular dish  is used by a subset of customers {xj}j =  <dig> n, the more probable it is that it will be used by other customers. thus, the ibp imposes the idea that many of the samples {xj}j =  <dig> n will utilize the same subset of columns of a, but each sample may also utilize idiosyncratic factor loadings, representing unique characteristics of particular samples. the ibp construction does not impose a total number of factors for the data {xj}j =  <dig> n, with this number inferred by the analysis. thus, the ibp is a natural bayesian method for inferring the number of factors appropriate for representing all observed data {xj}j =  <dig> n. a convenient means of implementing the ibp employs the beta process   <cit> .

there are multiple ways in which one may utilize the ibp/bp within the factor model, with three such methods considered here:  the bp is applied to the factor scores s ,  the ibp is employed on the factor loadings a  <cit>  , and  a bp-like construction is employed to implement a bayesian construction of a singular-value decomposition of x . to realize the approximate posterior density function for the parameters of these models, we have considered both mcmc and vb computational methods. the specifics of the bp, ibp and pseudo-svd methods, as well as computational details, are provided in section iv.

b. synthesized data
the first validation example we considered was taken from  <cit> . in this example the gene-factor connectivity matrix of an e-coli network is employed to generate a synthetic dataset having  <dig> samples of  <dig> genes and  <dig> underlying factors. the data had additive white gaussian noise with a signal-to-noise-ratio of  <dig>  for this very small-scale example we considered all three bayesian methods ; in each case we considered both mcmc and vb methods for inferring the posterior density function. we also considered the non-bayesian pmd and sparse-pca  <cit> ,  <cit> . all methods performed well in uncovering the proper number of factors, and in capturing the proper genes associated with each factor. for brevity we do not provide further details on this example. while it is worthy of consideration because it was considered in related published research  <cit> , its small-scale nature  makes it less relevant for the large-scale real application we consider below. therefore, in the next synthetic example we consider a much larger-scale problem, and consequently for that problem we were unable to test against the ibp method.

the synthetic data were generated as follows. a total of p =  <dig>   <dig> features  are employed, and the expression value for these p genes was constituted using five factors  plus a noise term e ). for each of the five factors, a unique set of  <dig> genes were selected and were given a factor-loading value of one. in addition, ten more genes were selected, with these shared among all five factors . thus, a total of  <dig> genes contributed non-zero loadings to at least one of the five factors. for all other genes the factor-loading contribution was set to zero. the above construction defines the sparse matrix a in . the components of s ∈ ℝr×n, for n =  <dig> samples, are drawn i.i.d. from 
n . the elements of the noise matrix e are drawn i.i.d. from n. the data x were then utilized within the various factor-analysis models, with the data-generation process repeated  <dig> independent times , with mean and standard-deviation results presented on the inferred model parameters , based on the  <dig> runs.

we consider a range of noise variances 1/α <dig> to constitute e, to address model performance as a function of the signal-to-noise ratio . as one definition of snr, one may consider the average energy contributed from a non-zero gene to a particular factor, relative to the energy in the noise contribution for that gene, from e. based on the fact that the non-zero components of a have unit amplitude, and the components of s are drawn from n , on average  the energy contributed by a non-zero gene to a particular factor is one. the average noise energy contributed to each gene is 1/α <dig>  hence, the ratio of these two quantities, α <dig>  may be considered as a measure of snr. other measures of snr may be defined with respect to this model, each of which will be defined in terms of α <dig> 

in figure  <dig> are presented the average number of inferred factors and the associated standard deviation on this number, for the bp and pseudo-svd models. we also compare to the sparse-pca model in  <cit> . the integer k represents the truncation level in the models, defining the maximum number of columns of a considered for analysis, from which r ≤ k columns are inferred as needed to represent the data x. this is discussed in detail in section iv. in these examples the models were each truncated to k =  <dig> factors. consequently, when  <dig> factors are used, the models have effectively failed, since the true number of factors is  <dig> and  <dig> is the maximum allowed within the model, given the truncation level under consideration. the mcmc results are based upon  <dig> burn-in iterations and  <dig> collection iterations . results are shown as a function of the standard deviation of the noise, 1/α <dig>  the sparse-pca model works well up to the point that the noise variance equals the amplitude of the non-zero values in a , while most of the bayesian methods infer the proper number of factors to a higher level of noise.

in figure  <dig> we examine how meaningful the inferred factor loadings are. specifically, recall that the data are based upon  <dig> unique genes that contribute to the factor loadings. based on the inferred factor loadings, we rank the genes based upon their strength in the loadings. we then rank the genes from  <dig> to  <dig>  based on the above strength, and examine the percentage of the top  <dig> inferred genes are consistent with truth. considering figure  <dig>  all of the bayesian methods perform well in this task, up to a noise standard deviation of approximately  <dig> , while sparse-pca performs degrades quickly beyond standard deviations of one . note that we also consider the bayesian factor analysis model in  <cit> ; we did not consider this method in figure  <dig> because it does not have a mechanism for estimating r-we simply set r = k in this analysis, using the same k =  <dig> as employed for the other bayesian methods. in  <cit>  the authors only considered an mcmc implementation, where here we consider both mcmc and vb inference for this model; further, here we have employed a student-t prior on the components of the factor loading matrix a, where in  <cit>  a spike-slab prior was employed.

concerning sparse-pca  <cit>  , every effort was made to optimize the model parameters for this task. our experience is that, while sparse-pca and pmd  <cit>  are very fast algorithms, and generally quite effective, they are not as robust to noise as the bayesian methods . it is possible that the sparse-pca and pmd results could be improved further if the model parameters are optimized separately for each noise level . however, the model parameters were fixed for all noise variances considered  or influenza a; these three challenges were performed separately, with no overlap in the subjects. all exposures were approved by the duke university institutional review board and conducted according to the declaration of helsinki. the three challenges are briefly summarized here, with further details provided in  <cit> .

human rhinovirus cohort
we recruited  <dig> healthy volunteers via advertisement to participate in the rhinovirus challenge study through an active screening protocol at the university of virginia . on the day of inoculation,  <dig> tcid <dig> gmp rhinovirus  was inoculated intranasally. subjects were admitted to the quarantine facility for  <dig> hours following rhinovirus inoculation and remained in the facility for  <dig> hours following inoculation. blood was sampled into paxgene™blood collection tubes at pre-determined intervals post inoculation. nasal lavage samples were obtained from each subject daily for rhinovirus titers to accurately gauge the success and timing of the rhinovirus inoculation. following the 48th hour post inoculation, subjects were released from quarantine and returned for three consecutive mornings for sample acquisition and symptom score ascertainment.

human rsv cohort
a healthy volunteer intranasal challenge with rsv a was performed in a manner similar to the rhinovirus intranasal challenge. the rsv challenge was performed at ret-roscreen virology, ltd  using  <dig> pre-screened volunteers who provided informed consent. on the day of inoculation, a dose of  <dig> tcid <dig> respiratory syncytial virus  manufactured and processed under current good manufacturing practices  by meridian life sciences, inc.  was inoculated intranasally per standard methods. blood and nasal lavage collection methods were similar to the rhinovirus cohort, but continued throughout the duration of the quarantine. due to the longer incubation period of rsv a, subjects were not released from quarantine until after the 165th hour and were negative by rapid rsv antigen detection .

influenza cohort
a healthy volunteer intranasal challenge with influenza a/wisconsin/67/ <dig>  was performed at retroscreen virology, ltd , using  <dig> pre-screened volunteers who provided informed consent. on the day of inoculation, a dose of  <dig> tcid <dig> influenza a manufactured and processed under current good manufacturing practices  by bayer life sciences, vienna, austria was inoculated intranasally per standard methods at a varying dose  with four to five subjects receiving each dose. due to the longer incubation period of influenza as compared to rhinovirus, subjects were not released from quarantine until after the 216th hour. blood and nasal lavage collection continued throughout the duration of the quarantine. all subjects received oral oseltamivir   <dig> mg by mouth twice daily prophylaxis at day  <dig> following inoculation. all patients were negative by rapid antigen detection  at time of discharge.

for each viral challenge, subjects had samples taken  <dig> hours prior to inoculation with virus , immediately prior to inoculation  and at set intervals following challenge. for the rhinovirus challenge, peripheral blood was taken at baseline, then at  <dig> hour intervals for the first  <dig> hours, then  <dig> hour intervals for the next  <dig> hours, then  <dig> hour intervals for the next  <dig> hours, and then  <dig> hour intervals for the remaining  <dig> days of the study. for the rsv and influenza challenges, peripheral blood was taken at baseline, then at  <dig> hour intervals for the initial  <dig> hours, and then  <dig> hours for the remaining  <dig> days of the study. all results presented here are based on gene-expression data from blood samples. for the rsv and rhino virus cases not all blood samples were converted to gene expression values, as a cost-saving measure. hence, for these two cases the gene expression data are not sampled as finely in time as are the influenza data.

in the statistical analysis, the matrix x in  has columns that correspond to the n samples; n = nsnt, with ns representing the number of subjects and nt the number of sample time points. we do not impose a prior on the time-dependence of the factors scores, and uncover this time dependence via the inferred posterior distribution of factor scores s.

d. analysis of influenza data
the gene-expression data consisted of over p =  <dig>   <dig> genes, and consequently we found that the ibp approach developed in  <cit>  was computationally intractable. we found that the vb and mcmc results were generally in good agreement for this real data, and therefore the two very distinct computational tools served to cross-validate each other. the vb and mcmc computations also required similar cpu time ; while the vb analysis required far fewer iterations to converge, each iteration is significantly more expensive than that associated with the gibbs sampler.

for brevity, we here focus exclusively on mcmc solutions when considering bayesian analysis. results are presented using the bp and pseudo-svd methods, as well as via pmd  <cit>  . we note that the design of each the experiments involves samples from the same subjects observed at multiple time points . therefore, the assumption within the models that the samples at different times are statistically independent may warrant reconsidering in future studies. this subject has been considered in related work  <cit> , although that research assumes a known factor structure and gaussian latent factors.

we first consider results based on the bp as applied to the factor scores. in these results we set k =  <dig> , and inferred approximately r =  <dig> important factors ; although only approximately r =  <dig> factors are used, we show the factor scores for all k =  <dig> possible factors such that the sparseness of the unused factors is evident, as inferred via the posterior. the results in figure  <dig> correspond to one example  collection sample from the gibbs sampler; factor  <dig>  which is most closely tied to the symptomatic/asymptomatic response, is employed by all data, while other factors are used more idiosyncratically .

at each time point, there are data from  <dig> subjects . the horizontal axis in figure  <dig> corresponds to a sequence of groups of data, proceeding in time from inoculation, with generally  <dig> samples per time point . the blue points correspond to samples of individuals who eventually became symptomatic, and the red points to asymptomatic individuals.

the vertical axis in these plots corresponds to the factor score associated with the respective sample. we observe in figure  <dig> that factor  <dig>  provides a clear discriminator of those who will become symptomatic, particularly as time proceeds .

having introduced the form of the data presentation, we now present results using the pseudo-svd method and pmd; for the pseudo-svd method we again show one  sample from the gibbs collection samples, while for pmd the results are the single solution. in figures  <dig> and  <dig> we present results, respectively, for the bayesian pseudo-svd model and for pmd  <cit> . for the bayesian methods we again set k =  <dig>  both methods uncover a relatively small  number of relevant factors.

note that in each case there appears to be one factor that clearly distinguishes symptomatic vs. asymptomatic, particularly as time increases. upon examining the important genes in each of these factors, one recognizes a high level of overlap . further discussion of the associated genes and their biological significance is provided in  <cit> .

e. pan-viral factors
we now consider a "pan-viral" analysis, in which data from all three viruses are analyzed jointly. for further conciseness, for this example we only present results for the bp applied to the factor scores; similar results were obtained with the bayesian pseudo-svd framework and by pmd.

since three viruses are now considered jointly, we have increased k to k =  <dig> in this example, and now approximately  <dig> factors were inferred . considering figure  <dig>  we note that factor  <dig> provides good discrimination between the symptomatic  and asymptomatic  samples, with this factor examined more closely in figure  <dig>  this same factor is able to distinguish the samples of each virus, at sufficient time after inoculation . factor  <dig> in figure  <dig> also appears to provide separation between symptomatic and asymptomatic samples; however, this is manifested because it contains two genes that are highly discriminative , with most of the other genes in factor  <dig> not discriminative. when addressing biological significance in  <cit> , the focus is on factor  <dig> in figure  <dig>  as it contains numerous discriminative genes. in these figures we are again showing one  sample from the gibbs collection.

it is also of interest to consider factors  <dig> and  <dig> in figure  <dig>  each of the samples from the individual viruses is offset by a distinct amplitude, almost entirely independent of whether the sample was symptomatic or asymptomatic. this phenomenon associated with factors  <dig> and  <dig> in figure  <dig> is attributed to challenge-study-dependent offsets in the data , which account for different normalizations of the data between the three distinct viral challenges. this underscores that not all factors have biological significance, with some a consequence of the peculiarities of gene-expression data . the other factor-analysis methods  produced very similar normalization-related factors.

in figure  <dig> are depicted the important genes associated with the discriminative pan-viral factor  <dig> in figure  <dig>  it is a subject of further research, but based on the data analyzed thus far, it appears the fa model applied to gene-expression data cannot distinguish well between the different viruses. however, we have applied fa jointly to our pan-virus data and to bacterial data available from related but distinct studies  <cit> . from that analysis we are able to distinguish between viral-based phenotypes and bacteria-based phenotypes; this is discussed in greater detail in  <cit> .

we have here identified many genes that are inferred to be connected with the viruses under study. it has been observed, by the medical doctors on our research team, that the inferred genes are closely aligned with relevant known pathways, with this discussed in detail in  <cit> .

iii. 
CONCLUSIONS
we have examined two distinct but related objectives. first, in the context of bayesian factor analysis, we have examined three ways of inferring an appropriate number of factors. each of these methods is based on a different means of leveraging the utility of the beta process, and the closely related indian buffet process . in the context of such models, we have examined inference based on variational bayesian analysis, and based on a gibbs sampler. we have also compared these bayesian approaches to state-of-the-art non-bayesian factor models.

the second contribution of this paper is the introduction of a new set of gene-expression data, from three time-evolving viral challenge studies. these data allow one to examine the time-evolution of rhino virus, rsv and influenza-a. in addition to the gene-expression data, we have also recorded clinical symptom scores, to which the gene-expression analysis may be compared. with the limited space available here, we have presented results on the influenza data alone, and for all three viruses together .

based on this study, we may make the following observations. for the number of gibbs iterations deemed necessary, the vb and mcmc inference approaches required comparable computation time . although vb requires far fewer iterations , each vb iteration is significantly more expensive than that associated with mcmc. the advantage of using these two very distinct computational methods on the models considered is that they serve to cross-validate each other .

of the three methods of inferring the number of factors, the ibp applied to the factor loadings works well for small-scale problems, but it is computationally intractable for the large-scale viral data considered here. applying the beta process to the factor scores, or to the singular values of a pseudo-svd construction, yields reliable and high-quality results.

it is not our purpose to provide a detailed  discourse on the relative merits of bayesian and non-bayesian approaches. however, we observed that the non-bayesian penalized matrix decomposition  yielded very high-quality results, as long as the model parameters were set carefully via cross-validation; very similar phenomenon was observed for the closely related sparse-pca. both pmd and sparse-pca infer an appropriate number of factors, but one must very carefully set the stop criterion. since pmd and sparse-pca are much faster than the bayesian approaches, perhaps a good compromise is to use the output of these models to initialize the gibbs sampler in a bayesian solution .

concerning the viral data, it was observed that all methods were able to infer a factor that was capable of distinguishing those subjects who would become symptomatic from those who would not. it was possible to infer a "pan-viral" factor, that was discriminative for all viruses considered.

the evolution of the factor scores tracked well the recorded clinical symptom scores. further, for the discriminative factor, there was a good association between the genes inferred as important and the associated biology  <cit>  .

iv. methods
a. basic sparse factor model
recall the factor model in ; r defines the number of factors responsible for the data x, and it is not known in general, and must be inferred. within the analysis we will consider k factors , with k set to a value anticipated to be large relative to r. we then infer the number of columns of a needed to represent the observed data x, with this number used as an estimate of r. since we will be performing a bayesian analysis, we will infer a posterior density function on r. henceforth we assume a has k columns, with the understanding that we wish to infer the r <k columns that are actually needed to represent the data.

let ak represent the kth column of a, for k =  <dig>  . . . , k, and ej and sj represent respectively the jth columns of e and s . within the imposed prior, vectors ej and sj are generated as sj ~ n, and ej~n) ; ik is the identity matrix and the precisions  are all drawn i.i.d. from a gamma prior.

one may consider many alternative means of defining sparseness on the ak, with the choice often dictated by convenience; we discuss two such methods here. in one approach  <cit>  one may employ a spike-slab prior:

  alk~wlkδ0+n , wlk~ beta  , αk~ gamma  

where  are selected as to strongly favor wlk →  <dig>  δ <dig> is a distribution concentrated at zero, and l =  <dig>  . . . , p. the advantage of  is that sparseness is imposed explicitly .

an alternative to  is to employ a student-t prior  <cit> , implemented via the hierarchical construction

  alk~n , alk~ gamma   

but now with  selected as to constitute a student-t sharply peaked about zero. one may employ a similar construction to impose a double-exponential  sparseness-promoting prior  <cit> .

b. beta process for inferring number of factors
the beta process  was first developed by hjort for survival data  <cit> , and more recently it has found many other applications and extensions  <cit> . we here seek to provide a simple discussion of how this construction may be of interest in inferring an appropriate number of factors in factor modeling  <cit> . our goal is to use the bp construction, which is closely related to the indian buffet process   <cit> , to infer the number of factors r based on the observed data x.

consider a measure drawn h ~ bp and constructed as

  h=∑k=1kπkδak  , πk~beta/k), αk~h <dig> 

we seek to link our construction explicitly to the factor model, and therefore ak is the kth candidate factor loading , and h <dig> is defined by the construction in  or , depending upon which model is used. the expression πk represents the probability that ak is used to represent any particular data sample, defined by the columns of x. the expression δak represents a unit point measure concentrated at ak.

the bp is closely linked with a bernoulli process bep  <cit> . specifically, for the jth column of x, we perform a draw from the bernoulli process

  bj=∑k=1kzkjδak, zkj∈{ <dig> }, zkj~bernou11i    ,   j= <dig>  . . .  , n 

where the h in bep is drawn h ~ bp, as defined in . as discussed further below, if zkj =  <dig> then ak is used as a factor loading to represent xj, the jth column of x; if zkj =  <dig>  ak is not used to represent xj. in other words, bj is a sum of point measures , and the binary variables zkj denote whether specific δak are employed within bj. more details on such constructions may be found in  <cit> .

to make a connection to the introductory comments in section ii-a, and to relate the model to the ibp  <cit> , we consider the above construction in the limit k → ∞. further, we marginalize  out the probabilities  used to constitute the bp draw h; we retain the k candidate factor loadings {ak}k =  <dig> k used to define a, as drawn from the bp. recall that xj represents the jth data sample . we assume that the data samples  select from among "dishes" at a "buffet", with the dishes defined by {ak}k =  <dig> k . data sample x <dig> enters the buffet first, and selects the first ν <dig> dishes a <dig>  . . . , aν <dig> , where ν <dig> is a random variable drawn from possion. therefore, the first column of s has the first ν <dig> elements as non-zero, with the remaining elements in that column set to zero. the second "customer" x <dig> then enters the buffet, and selects from among the first ν <dig> dishes; the probability that x <dig> selects ak, for each of k ∈ { <dig>  . . . , ν1}, is 1/; i.e., zk <dig> ~ bernoulli), for k ∈ { <dig>  . . . , ν1}. customer x <dig> also selects ν <dig> new dishes {aν1+ <dig> . . . , aν1+ν <dig> }, with ν <dig> ~ possion). hence, zk <dig> =  <dig> for k ∈ {ν <dig> +  <dig>  . . . , ν <dig> + ν2}, and unless stated explicitly otherwise, all other components of zjare zero. this process continues sequentially, with each xj entering the buffet in ascending order of j. sample xj , with j ∈ { <dig>  . . . , n} selects dishes as follows. let cj−1=∑j=1j−1vj represent the cumulative number of dishes selected off the buffet, among the previous customers {x <dig>  . . . , xj−1}. further, let mj− <dig> k ≥  <dig> represent the total number of times dish ak has been selected by previous customers {x <dig>  . . . , xj−1}, for k ∈ { <dig>  . . . , cj−1}. then xj selects dish ak, k ∈ { <dig>  . . . , cj−1}, with probability mj− <dig> k/; i.e., zk,j~bernoulli for k ∈ { <dig>  . . . , cj−1}. note that the more "popular" ak among the previous j −  <dig> customers , the more probable it is that it will be selected by xj . additionally, xj selects new dishes ak for k ∈ {cj− <dig> +  <dig>  . . . , cj− <dig> + νj}, where vj~poisson. therefore we have zk, j =  <dig> for k ∈ {cj− <dig> + <dig>  . . . , cj− <dig> + νj}. thus, each new customer selects from among the dishes  already selected by at least one previous customer, and the more "popular" one of these dishes is, the more probable it is that the new customer will select it. further, a new customer will also select additional dishes  not selected by any of the previous customers. however, note that as j increases, the draws vj~poisson are likely to be decreasing in size, since αβ+j− <dig> is getting smaller with increasing j. therefore, although k → ∞, a finite subset of the candidate dishes  {ak}k =  <dig> k will be used among the n customers, defined by the columns of x, thereby imposing sparseness in the use of factor loadings. this model is also fully exchangeable, in that the order of the columns of x may be permuted, with no change in the properties of the prior  <cit> . the model imposes that many of the n samples will share the same set of factors, but the model is flexible enough to allow idiosyncratic  factor usage.

in practice k is finite, and therefore it is also if interest to consider the properties of this prior for finite k. for finite k, one may show that the number of non-zero components of zj is drawn from binomial)), and therefore one may set α and β to impose prior belief on the number of factors that will be important. the expected number of non-zero components in zj is αk/.

to complete the model specifications, note that ak from the beta-bernoulli construction above defines the kth column of the factor-loading matrix a. the factor-score matrix s utilizes the binary vectors zj = t defined in , for j ∈ { <dig>  . . . , n}. specifically, we define the jth column of s as sj=s^j∘zj , with s^j~n . the vector product s^j∘zj selects a subset of the components in s^j , setting the rest to zero, and therefore the columns of s are sparse.

c. sparse factor modeling with bp/ibp placed on factor loadings
in the above discussion the beta-bernoulli and ibp processes were presented for a specific construction of the factor-analysis model, with the goal of making the connection to the model explicit and hence clearer. however, there are alternative ways of utilizing the ibp for design of factor models. specifically, rather than using the binary vectors to construct s, as above, they may alternatively be used to define a, with factor scores designed as in traditional factor models. this approach was considered in  <cit> , using an indian buffet process  construction . a limitation of this approach is that one must perform p draws from the ibp to construct a, and typically p is very large for the gene-expression problems of interest. when presenting results in section ii-b, we discuss our experience with this model on small-scale problems, although this approach was found computationally intractable for the motivating virus studies considered in section ii-d.

d. constructing pseudo singular values
the final bayesian construction considered for inferring r is closely related to the non-bayesian sparse-pca  <cit>  and penalized matrix decomposition   <cit>  models. we generate the vectors {ak}k =  <dig> k as before, using a sparseness-promoting prior like that discussed in section iv-a. further, the factor scores ξk for factor loading k is drawn ξk ~ n , for k= <dig>  …, k;ξkt constitutes the kth row of s, and we consider k such rows, for large k . finally, the vector of pseudo singular values λ =  is generated

  λ = z∘wzk ~ bernou11i , k= <dig>  …, kπk ~ beta) , k= <dig>  …, kw ~n 

the matrix product as in  is now constituted as ∑k=1kλkakξkt . the non-zero components of λ select the columns of a used across all columns of x. as discussed in section iv, the number of non-zero components of λ is drawn binomial)), and the posterior on the number of such components provides desired information on the number of factors r. note that this construction is like the beta-bernoulli process discussed above, in that it utilizes πk ~ beta) and the bernoulli distribution; however, it only draws the binary vector z once, and therefore there is not the idea of multiple "customers", as in the two ibp-related formulations discussed above.

e. computational issues, model quality and hyper-parameter settings
the mcmc results presented here correspond to using  <dig> collection samples, after a burn-in of  <dig> iterations. however, with  <dig> burn-in iterations and  <dig> collection samples, the average results of the factor scores and factor loadings are almost identical to those found with  <dig>  for all mcmc results, we employed a singular value decomposition  of the data matrix to initialize the factor loading and factor score matrix in the fa model, as well as the right-and left-singular matrix in the matrix decomposition model. for each iteration of the gibbs sampler a particular number of factors r are employed, and based upon all collection samples one may infer an approximate posterior distribution for r. running on a typical modern pc, the computation times are summarized in table  <dig> for the different models, as applied to the influenza data .

to be explicit, we provide detailed hyper-parameter settings for the model in -; the other models are set similarly. specifically, α =  <dig>  β =  <dig>  c =  <dig>  and d = g = h = e = f = 10− <dig>  these parameters were not optimized, and were set in the same manner for all experiments. although the pmd model is a non-bayesian method, it also has parameter settings that must be addressed carefully; two hyper-parameters need adjusting: the sparseness threshold and the stop condition  <cit> . in all pmd experiments, we set the sparseness threshold as  <dig>  and the pmd iterations were terminated when the reconstruction error was smaller than 5%.

all calculations were performed on pcs with intel pentium dual e <dig> processors and  <dig>  gb memory, and all software was written in matlab. for the large-scale analysis performed on the real data discussed above, mcmc required approximately  <dig> hours of cpu, while vb required  <dig> hours .

authors' contributions
the following authors performed the statistical analysis: bc, mc, jp, ah, jl, dd and lc. the following authors executed the three viral challenge studies, and performed all biological interpretation of the ressults: az, cw and gsg. all authors read and contributed to writing this paper.

appendix: gibbs and variational bayesian analysis
we here provide a concise summary of the inference methods applied to one of the bayesian fa models discussed above, with this representative of the analysis applied to the rest. specifically, we consider the model discussed in section iv-b, in which the bp is applied within the factor-score matrix. the complete model may be expressed as

  xi ~n, diag) 

  zki ~ bernou11i 

  πk ~ beta/k) 

  ajk ~n 

  si ~n 

  γjk~gamma 

  ψj~gamma 

  δ~gamma 

where i =  <dig>  . . . , n, j =  <dig>  . . . , p and k =  <dig>  . . . , k.

gibbs sampler
the full likelihood of the model is

 p=∏i=1nn), diag−1)n×∏j=1p∏k=1kngamma×∏i=1n∏k=1kbernou11ibetak)×∏j=1pgamma×gamma 

the sequential update equations of the gibbs sampler are as follows.

• sample each entry of the binary matrix, zki. the probability of zki =  <dig> is expressed as

 p∝ln−12akski2−2aktdiagxi−kski). 

• sample πk from p = beta where α′=∑i=1nzki+αk and β′=n+βk−∑i=1nzki.

• sample each entry of factor loading matrix, ajk from p = n  where Σjk=− <dig>  μjk=Σjk , and xji−k=xji−∑l= <dig> l≠kkajlzlisli.

• sample each column of factor score matrix, si, from p = n  where λi=− <dig>  ξi=λidiagxi, and z˜i= with the k-dimensional vector,zi, repeated p times,  <dig> ≤ i ≤ n.

• sample ψj from p=gamma where gj′=g+n <dig> and hj′=h+12∑i=1n‖2).

• sample γjk from p = gamma  where c' = c+1/ <dig> and d′=d+12ajk <dig> 

• sample δ from p = gamma  where e' = e + nk/ <dig> and f′=f+12∑i=1n in the above equations expressions of the form p represent the probability of γjk conditioned on all other parameters.

variational bayesian inference
we seek a distribution q to approximate the exact posterior p, where in Θ ≡{a,s,z,α,π,ψ,γ,δ} our objective is to optimize the parameters Γ in the approximation q. toward that end, consider the variational expression

  f˜=∫dΘqlnqpp=−lnp+kl||p) 

note that the term p is a constant with respect to Γ, and therefore f˜ is maximized when the kullback-leibler divergence kl||p) is minimized. however, we cannot explicitly compute the kl divergence, since p is unknown. however, the denominator term in f˜ may be computed, since pp = pp, and the prior p and likelihood function p are available. to make computation of f˜ tractable, we assume q has a factorized form q = Πiqi. with appropriate choice of qi, the variational expression f˜ may be evaluated analytically. the update equations are as follows.

• for zki we have q=bemou11i where ρki′ is the probability of zki =  <dig>  we consider the following two conditions:

discussion below, the symbol < • > represents the expectation of the argument.

lnq∝ζ1=〈lnπk〉−lnq∝ζ2=〈ln〉  where xi−k=xi−∑l= <dig> l≠kkalzlisli, 〈lnπk〉=Ψ−Ψk+n) ,〈ln〉=Ψk+n −∑i=1n〈zki〉)−Ψk+n), Ψ=∂∂xlnΓ and Γ1=∫0∞dττx−1e−τ . therefore, we can calculate ρki′=exp)exp+exp) . above, and in the discussion below, the symbol < • > represents the expectation of the argument.

• for πk we have q=beta where αk′=∑i=1n〈zki〉+αk and βk′=n+βk−∑i=1n〈zki〉.

• for ajk we have q=n with Σjk=− <dig> and  μjk=Σjk=n , with λi=− <dig> and ξi=λidiagxi) , where z˜i= is a k-dimensional vector of all zi repeated p times. in order to exactly calculate the expectation,  b=〈diag〉 

, we have to consider it as two parts. specifically, the off-diagonal elements of b are diag , and the diagonal elements, bkk=〈ψj〉)〈zki〉 , since 〈zki2〉=〈zki〉 and 〈ajk2〉=〈ajk〉2+Σjk , where  <dig> ≤ k ≤ k,  <dig> ≤ j ≤ p and  <dig> ≤ i ≤ n.

• for ψj we have q=gamma , where gj′=g+n <dig> hj′=h+12∑i=1n〈2〉.

• for γjk we have q=gamma , with cjk′=c+1/2  and djk′=d+12〈ajk2〉.

• for δ we have q gamma , where e' = e + kn/ <dig> and f′=f+12∑i=1n〈sitsi〉 .

