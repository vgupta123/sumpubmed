BACKGROUND
ribonucleic acid  is an important molecule that performs a wide range of functions in biological systems, such as synthesizing proteins, catalyzing reactions, splicing introns and regulating cellular activities. the function of an rna molecule generally can be derived from its secondary structure. currently, the only completely accurate method of determining the folded structure of an rna molecule is by x-ray crystallography and nuclear magnetic resonance , however, those methods are time consuming and very expensive. therefore, computational methods have been widely used in the field of rna secondary structures prediction, such as thermodynamic energy minimization methods, homologous comparative sequences, stochastic context-free grammar methods  and genetic algorithm and so on. among which the most popular structure prediction algorithm is the minimum free energy  method  <cit> . it was presented in  <dig> by m. zuker and has been implemented by three famous programs: mfold  <cit> , rnafold  <cit>  and rnaalifold  <cit>  .

both mfold and rnafold implement the zuker algorithm for computing minimal free energy  structures by folding a single sequence and employ the same thermodynamic parameters  <cit> . the time complexity is o and the spatial complexity is o by limiting the length of interior loops, where n is the sequence length. rnaalifold  <cit>  implements an extension of the zuker algorithm for computing a consensus structure from rna alignments. the algorithm computes an averaged energy matrix and a covariation score matrix, augmented with penalties for inconsistent sequences. the algorithm requires extreme computational resources o in time, and o in space, where n is the sequence length and m is the number of sequences in the alignment  <cit> .

free energy minimization is the most common method for rna secondary prediction. however, this method typically suffers two drawbacks. the first one is the limitation of structure prediction accuracy. the reason is that the thermodynamic rules are incomplete and the current model itself is an estimate of the real physics of rna folding  <cit> . in practice, benchmarks of prediction accuracy on single rna sequence show that current rna folding programs get about 50–70% of base pairs correct on average  <cit> . the second one is the extreme demand for computational resources. the cost is intolerable with the growth in rna database.

there are two kinds of parallel processing solutions based on mfe method at present, but both of them only considered the application of single sequence folding, mfold and rnafold, which are based on zuke algorithm. high performance parallel computers with shared or distributed memory, such as smp multiprocessor  <cit>  or cluster systems  <cit>  are widely used to accelerate zuker algorithm  <cit> . the main idea is to partition the matrix in a regular fashion and to distribute tasks to multiple processors.

unfortunately, the simple coarse-grain zone blocking method  results in severe load imbalance because the size of the computation for each element is closely related with its position in the matrix. in  <cit> , the authors presented some parallel implementations of the zuker algorithm and methods for load balance. however, they did not consider communication delays, which account for 50% of the execution time for a sequence length  <dig>  due to fine grain data transfer. they achieve a 19× speedup on a 32-processor system, dawning 4000a  <cit> , and 8× on a cluster with  <dig> opteron processors running at  <dig>  ghz, each with  <dig> gb memory  <cit> . the other solution to accelerate the zuker algorithm is using multi-core architecture. based on the ibm cyclops <dig> simulator, g.m. tan et al.  <cit>  presented a parallel zuker algorithm. they report a 30× speedup on  <dig> cores for an rna sequence length of  <dig>  parallel efficiency is greatly limited by complicated data dependency and tight synchronization. thus, efficiently executing the mfe algorithm on a general-purpose computer or a multi-core architecture becomes very awkward.

recently, the use of field programmable gate-array  coprocessors has become a promising approach for accelerating bioinformatics applications. the computational capability of fpgas is increasing rapidly. the top level fpga chip from xilinx virtex <dig> series contains  <dig> slices and  <dig> k bits storage. the reconfigurability of fpga chips also enables algorithms to be implemented with different computing structures on the same hardware platform.

accelerating the mfe algorithm on fpga chips is a challenging task. first, the non-uniform multi-dimensional data dependences with variable dependence distance make it difficult to find a well-behaved task assignment for load balance. second, the irregular spatial locality with a great deal of small granularity access operations make it difficult to optimize memory scheduling for efficient external access. third, multiple copies of free energy parameters for parallel processing consume a large amount of on-chip memory and memory ports limiting the physical scale of parallel processing. finally, the limited on-chip memory cannot hold all o matrices, resulting in long-latency matrix loads from external dynamic random access memory . so far, the algorithm accelerator based on mfe method is still under research. g.m. tan et al.  <cit>  introduced a fine-grained parallelization of the zuker algorithm which considers only the interior loop calculation rather than the whole algorithm. a recent paper, arpith jacob et al.  <cit> , implemented the simplest rna folding algorithm, nussinov algorithm  <cit> , on a virtex-ii  <dig> fpga, but only input sequences with  <dig> ~  <dig> bases can be predicted. both presented results from simulation only.

in this paper, we propose a systolic array structure including one master pe and multiple slave pes for fine grain hardware rnaalifold algorithm implementation on fpga. we optimize the nested loop structure and reorganize the computation order by analyzing the data dependency in the original rnaalifold algorithm and improve the spatial locality in folding process. for load balance, we partition tasks by columns and assign tasks to pes. we aggressively exploit data reuse schemes to minimize the need for loading energy matrices from external memory. specifically, we add a cache to buffer a triangular sliding window of one of the matrices, most of which will be used in computing the next element in the column. we also transfer local elements directly to the next adjoining pe. in our design, only the master pe loads energy matrices from external dram. the remaining slave pes simply wait for data from the previous pe. we also propose several methods that collectively reduce the storage requirements of the energy parameter tables by 80%-fitting curves with piecewise linear function, replacing scattered points with register constants and compressing the address space while shortening data length. the whole array structure is carefully pipelined in order to overlap multiple pe's column computations, master pe's load operations and multiple pe's write-back operations as much as possible. we implemented an rnaalifold algorithm accelerator with  <dig> processing elements on a single fpga chip. the experimental results show a factor of 12× speedup over the viennarna- <dig> . <dig> software for a group of aligned rna sequences with 2981-residue each running on a pc platform with pentium  <dig>  <dig>  ghz cpu. moreover, the power consumption is only about 1/ <dig> of general-purpose microprocessor.

overview of the rnaalifold algorithm
brief introduction
the rnaalifold algorithm predicts a consensus secondary structure from a group of aligned rna sequences by calculating an averaged minimum free energy for the alignment, incorporating covariance information into the energy model  <cit> . the essential idea of rnaalifold algorithm is still the thermodynamic energy minimization theory, which was first presented by m. zuker in  <dig>  it uses a "nearest neighbor" model and empirical estimates of thermodynamic parameters for neighboring interactions and loop entropies to score structures  <cit> . the data input of rnaalifold is the result of multiple sequence alignment and the common secondary structure  will be generated through below three processing phases.

calculating co-variance bonus
the function of this stage is calculating the co-variance bonus for each pair depending on compensatory or consistent mutations. then it uses the bonus to judge if the residues located on the two positions can consist a base-pair and direct the energy filling process.

filling energy matrices
suppose s <cit> , s <cit> , ..., s is a group of aligned rna sequences as shown in figure  <dig>  r1r <dig> ..ri...rj...rn is an rna sequence, which consists of four nucleotides: a, c, g, u and a blank inserted by multiple sequence alignment tool, where i and j represent the nucleotides' location in rna sequence, k is the id of current input sequence and n is the sequence length.

the core of energy matrices filling stage in rnaalifold is a triple cycle operation as shown in figure  <dig>  the two control variables, i and j in surrounding loops, moves alone the horizontal axis to pass through every place and search for the potential base-pairs. the inner loop, control variable k, moves down and implements the free energy accumulation of substructure located on  position in different energy matrices. the value of v equals the minimum value among the four energy parameters that four fundamental substructures corresponding respectively, which stands for the energy of an consensus optimal structure of the common subsequence ri...rj.

the vector w holds the minimal free energy for certain structures of common subsequences. the element, w, is the energy of a consensus optimal structure of the common subsequence r1r <dig> ..rj. once the longest fragment, the complete sequence, is considered, the lowest conformational free energy is calculated then the filling step ends and w stands for the energy of the most energetically stable structure of the aligned rna sequences. the calculation of w depends on its left elements from w to w and the jth column in matrix v, v .

as for one of the input rna sequence, s, the energy computing for each base-pair  involves four triangular matrices: v, vbi, vm, wm and three energy functions: es, eh, el. the recurrence relations as shown in figure  <dig> 

v  is the energy of the optimal structure of the subsequence riri+1⋯rj where rirj comprises a base pair. vbi is the energy of the subsequence from ri through rj where rirj closes a bulge or an internal loop. vm is the energy of the subsequence from ri through rj where rirj closes a multi-branched loop. wm  is the energy of the subsequence from ri through rj that constitutes part of a multi-branched loop structure. es, eh and el are free energy functions, which are used to compute the energy of stacked pair, hairpin loop and internal loop respectively. in software folding solution, these free energy functions are calculated by looking up tables of the standard free energy parameters, which are detected by experimental method. the storage requirement of those tables is about  <dig> m byte.

backtracking
when the corresponding energy matrices of all input sequences have been filled out, the free energy for optimal consensus structure is known, which is stored in the element w, but the structure is unknown. the phase of backtracking is performed to determine the structure leading to the lowest free energy, using the free energies calculated in the filling step to revivify the exact structure. experiments show that the energy matrices filling step consumes more than 99% of the total execution time. thus, computing energy matrices quickly is critical.

characteristics of rnaalifold
we make five observations about the characteristics of the rnaalifold algorithm. these observations suggest details of the parallel implementation.

observation  <dig>  the computation size of each element in an energy matrix is variable and closely related with its position
considering the most time-consuming calculation of matrix v for each input sequence, which is an upper triangle matrix as described in formula . the computation size  for each element, c, is closely related with the indices i and j, as shown in formula .

  c=+{30× <dig> j−i>30;× <dig> j−i≤ <dig> 

the computation size of jth column v, c, is the sum of c in the jth column:

  c=j⋅2+{s,3≤j≤30;s+30×31× <dig> j>30; 

where, s =  <dig> +  <dig> +  <dig> + ⋯ +  <dig> ×  × . the difference in computation size between column j and column j +  <dig> is Δc:

  Δc=j+{j× <dig> ≤j≤30;30× <dig> j>30; 

we can find that the computation size gradually increases with the matrix location moving up from bottom to top in the same column and increases with the location moving right in the same row. specifically, the workload of v is the heaviest one, it depends on the entire row and column elements of wm matrix and the bottom-left triangle region of v with the maximum size  <dig> ×  <dig> ×  <dig>  this workload imbalance suggests a cyclic column allocation scheme, in which each processing element  is assigned one column of matrix v. each pe processes its column from bottom to top.

observation  <dig>  parallel computation of v requires multiple copies of the free energy parameters
the calculation of each element of matrix v involves looking up parameter tables to get the values of the free energy functions eh, es and el, obtained from experimental methods. the tables are addressed by pairs of rna residues. to calculate v, we first find the residue pair indexed by i and j in the rna sequence, then lookup the tables to obtain the energy values. the number of query operations in rnaalifold for computing matrix v is o, n is the rna sequence size. for parallel computing, centralized tables will become the performance bottleneck. we have to distribute the parameter table to each pe so that energy values can be read without memory conflict. but the storage requirement of entire free energy parameter tables at 37°c is more than  <dig> kbyte. in addition to other storage requirements for data buffers, the total storage requirement will greatly exceed the capacity of the current largest fpga chip if the number of pes is over  <dig>  as a result, for rnaalifold, the storage factor has a major effect on the scalability of parallel processing.

we figured out several efficient compression approaches to reduce the storage overhead of the free energy parameters. first, we partition the loop destabilizing energies into segmented linear functions. the transformation from query operations to arithmetic operations reduces the requirements for memory ports and storage capacity. the linear functions are simple, require little logic, and have no impact on accuracy. second, we represent a few scattered points using registers. some parameters in free energy tables are scattered too widely to be fitted by simple linear functions. instead of using block ram to hold these parameters, we assign these parameters to registers. third, we compress the address space and shorten the data length. interior loop energy data occupies more than 80% of the total parameter tables, when four nucleotides, a, c, g, u are encoded directly. instead, we encode the six base pairs, au, cg, cu, gc, ua and uc using  <dig> bits. the table address length reduces from 16-bit occupying  <dig> k entries to 14-bit for  <dig> k entries. the storage requirement is compressed by 75%. finally, we transform the raw data of free energy parameter tables from signed decimal fraction into complementary integer reducing data width from 16-bit to 8-bit without affecting accuracy. with the above schemes, the storage requirement of free energy parameter tables drops by 80%. as a result, more processing elements can be fitted in fpga chip.

observation  <dig>  we can use a sliding window to reuse data within a column
from formula , we observe the element vbi, which is used to calculate v , depends on the elements located in the bottom-left triangle region of v . as shown in figure  <dig>  the region is a triangle window with the maximum size of  <dig> ×  <dig> due to the limitation of the interior loop length. in figure  <dig>  the blue triangle window in matrix v, called window a, contains the necessary data for computing the element a of vbi. with the computation traveling upward in the same column from a, a <dig>  to a <dig>  the triangle window moves from bottom to top.

we observe that only one row of elements is updated when the window slides from a to a <dig>  the other elements remain unchanged. we use a local buffer of  <dig> elements  and prefetch  <dig> elements of v into it before calculating each element of vbi, saving  <dig> elements. this greatly reduces the memory bandwidth requirements for loading elements of v from external dram.

observation  <dig>  we can use a sliding window to reuse data between adjoining columns
the triangle windows in adjoining columns exhibit an overlapped area which can be exploited for additional data reuse. assuming the four elements a, b, c and d located in four adjoining columns in figure  <dig>  we arrange four pes, pe <dig>  pe <dig>  pe <dig> and pe <dig> for parallel computing, respectively. in figure  <dig>  we observe that triangle b contains two parts, one is a sub-triangle residing in triangle a completely, and the other is a column of a with a maximum size of  <dig> elements which becomes available before element a is computed. the same overlapped area can be found between b and c, c and d. this observation implies that except that the first column pe <dig> has to hold the entire triangle window, the other column pes have to wait only for the elements transferred from the previous pe. a similar scenario is found in the computation of wm. as a result, by transferring data between the adjoining columns processing, we can greatly reduce the memory bandwidth requirements for loading elements of v and wm from external dram.

observation  <dig>  we can reorganize the computation order in parameter accumulating among multiple energy matrices to improve the spatial locality
in order to recover the consensus structure of the common subsequences from position i to j in a group of aligned rna sequences, the sum of energy value of all fragments located in the same position  must be figured out. in the original rnaalifold algorithm, the sum-of-energy is calculated by adding up the energy of the same position in all input sequences one by one ). according the recurrence relations ~ and the observations  <dig> and  <dig>  the parameter v of each sequence corresponding is depend on its energy matrices v and wm.

the jumping of current energy computing and accumulation operand among the multiple energy matrices will cause the high-frequency data exchanging  between fpga and off-chip memory. as a result the poor spatial locality and low efficiency of external memory access will become the performance bottleneck in fpga implementation.

to address the problem, we improved the nest relationship of the triple cycle operation in original algorithm. in the original algorithm as shown in figure  <dig>  only the current computing result, the four energy components on position  k, es' k, vbi k, vm k), can be reused.

however, in the optimized algorithm, the elements located in the triangle region of v  and current rowcolumn of wm  can be reused for computing the next elements v  and v . as a result, we can eliminate the high-frequency data exchanging between fpga and off-chip memory and improve the access efficiency of external memory.

methods
system architecture
our rna folding computation platform consists of an algorithm accelerator and a host processor. the accelerator receives a group of aligned rna sequences with a co-variance bonus matrix, executes the energy matrices filling and backtracking phases, and reports the consensus rna secondary structure represented in base pairs back to the host for display. the structure is shown in figure  <dig> 

the accelerator engine comprises one fpga chip, two sdram modules, and one i/o channel to the host pc. two sdram dimms store the energy matrices of each sequence for energy accumulating and backtracking, and are connected to the fpga pad directly. the host interface channel is responsible for transferring initial rna data, co-variance bonus and final results between the accelerator and the host. the core of the rnaalifold algorithm accelerator is composed of a pe array controller, a pe array, a shared memory module, an energy matrices superposition module and a trace-back module. the pe array controller is responsible for assigning column tasks to the pe array and switching from the filling phase to the backtracking phase. the shared memory module contains a v cache, which holds the triangular sliding window and a wm column buffer, which stores the current p column elements for writing back to sdram, where p is the number of pes. the pe array performs the free energy calculation in parallel. the array consists of a series of pe modules, in which the first one, pe <dig> is the master and the others are slaves. each pe is augmented with a local memory to store a copy of the current rna sequence and a register for the current column elements of matrix wm. the registers between adjoining pes, called the trans regs, are used for delivering reusable data including wm row/column elements and the bottom-left elements of v. the energy matrices superposition module is responsible for energy accumulating and generating the energy matrices  for backtracking.

master-slave pe array algorithm
free energy computing  is the kernel in the rnaalifold algorithm. as implied by observation  <dig>  the upper-triangular shaped energy matrices are partitioned into columns. each pe holds one column cyclically in turn. every group of p contiguous columns forms a section as shown in figure  <dig>  figure  <dig> describes the parallel rnaalifold algorithm in a single-program multiple-data  style.

performance measures
the execution time of our parallel rnaalifold algorithm can be predicted in cycles due to the tight synchronization of the systolic array structure. the total execution time  is the sum of matrices filling time , matrices accumulating time  and the time for trace-back . moreover, the matrices filling time  equals the energy computation time  plus the external memory access time . assuming p is the number of pes, n is the length of the aligned rna sequence, m is the number of input sequences and s = ⌊np⌋, we have energy computation time  in 

  tc=14p⋅Δt1⋅ 

the time for accessing external memory  is tm = n⋅2·Δt <dig>  thus, the filling time for all energy matrices  is

  tf=m⋅p⋅Δt14⋅+12⋅m⋅n⋅⋅Δt <dig> 

the matrices accumulation time  is ⋅n22·Δt <dig> and the time for trace-back  is tt = 3n <dig>  where Δt <dig> is the time for each sum operation, which is one cycle in our implementation, Δt <dig> is the average access overhead for storing an element to external memory, which is  <dig> cycles in the worst case. as a result, the total execution time  is

  t=mp4⋅+15mn⋅2+⋅n22+3⋅n <dig> 

according to the formula  and , we can theoretically analyze the parallel efficiency  of our accelerator, where α is

  α=3n2smp+180mn 

in the general case that m =  <dig>  p <  <dig> and n >  <dig>  the parameter α is always less than  <dig> . as a result, the parallel efficiency can reach more than 90%, showing good parallelism.

RESULTS
experiment environment
we implement the rnaalifold algorithm accelerator in an fpga testbed. the testbed is composed of one fpga chip, stratixii ep2s130c <dig> from altera, two  <dig> gb sdram modules, mt16lsdt12864a from micron and a usb <dig>  interface to the host computer. the folding software, rnaalifold , runs on a desktop computer with intel pentium <dig>  <dig>  ghz cpu and  <dig> gb memory at level o <dig> compiler optimization. both the accelerator and software use the same free energy parameters, rna free energies at 37°c, version  <dig>  downloaded from m. zuker's homepage. we also measure software execution time on amd and xeon platforms to verify the acceleration of our approach.

verifying correctness
we verify the correctness of our implementation in three steps. first, we ensure the correctness of the optimized algorithm by comparing the software folding result. second, we implement hardware rnaalifold algorithm and verify the function correctness of the hardware using software emulation with modelsim se  <dig>  h. then, we run the search in our testbed to compare the base-paring results with the ones produced by software to verify the correctness of the folding result generated by our accelerator. we fold six group of rna sequences  with the size from  <dig> bps to  <dig> bps by using hardware and software solutions respectively. the experimental results show that the folding results of our accelerator agree with the software version.

resource usage
besides implementing the accelerator on altera fpga chips, we place different numbers of pes on xilinx fpga chips to evaluate the resource usages as well. as shown in the last row of table  <dig>  in altera fpga chips, one pe consumes  <dig> aluts and  <dig> k bits of memory. it consumes  <dig> slices and  <dig> slices in xilinx xc4v and xc5v fpga chips respectively. at most  <dig> pes fit on ep2s130c <dig> because the storage requirement consumes almost all of the memory resources. on xc5vlx <dig>  the latest fpga from xilinx, we can fit  <dig> pes. all implementation can reach a clock frequency of over  <dig> mhz.

scalability
to explore the scalability of the proposed accelerator architecture, we folded six group of sequences on our accelerator. as shown in figure  <dig>  the execution time of input rna sequences with varying length from 83-base to 2981-base on multiple pes includes computation time, trace-back time and the time for sending sequence query and taking results back for display. because execution time greatly increases with the increase in sequence length ranging from  <dig> s for  <dig> bases to  <dig> s for  <dig> bases, we show the execution time in different figures. considering the longest sequence with  <dig> bases, the execution time is shortened sharply from  <dig> s for one pe to  <dig> s for  <dig> pes, a factor of  <dig>  speedup. performance with  <dig> pes is estimated according to the formula . figure  <dig> shows the speedup with different sequence lengths ranging from  <dig> to  <dig> exhibit similar linear features due to the scalable parallel structure in the accelerator.

performance compared to viennarna
the original loops in rnaalifold are unrolled and the calculation order in energy matrices filling stage is reorganized in our fine-grained parallel algorithm. however, the time complexity is undiversified since the number of add and compare operations are not change. in the original algorithm, only two triangle matrices  are stored for each sequence in energy matrices filling stage. in optimized algorithm, the other four substructure matrices  are also stored for energy accumulation. thus, the parallel algorithm needs triple storage requirement compared to the original rnaalifold software. in order to reduce the bandwidth requirements for external memory access, the six triangle matrices are compressed into two matrices by using component combined strategy. furthermore, the data reusing in pe array is well exploited with the spatial locality improving. therefore, the increasing of storage requirements for off-chip memory will not become the bottleneck in fpga implementation. taking pentium  <dig> as the base, we compare the execution time and speedup among  <dig> different platforms, including three general-purpose computers and our algorithm accelerator. despite the variation in cpu type, clock frequency, main memory capacity, cache capacity and software versions, the three general-purpose computers exhibit similar performance. as shown in table  <dig>  athlon shows a little advantage over pentium and xeon, achieving at most  <dig> × speedup. however, the fpga accelerator exhibits significant speedup ranging from  <dig>  to  <dig> .

the soft/hardware experimental environment of different platforms are listed as follows. pentium:  <dig>  g cpu,  <dig>  gb memory,  <dig> kb cache, linux  <dig> . <dig>  gcc  <dig> .0; xeon:  <dig>  g cpu,  <dig>  gb memory,  <dig> kb cache, linux  <dig> . <dig>  gcc  <dig> .3; athlon:  <dig>  g cpu,  <dig>  gb memory,  <dig> kb cache, linux  <dig> . <dig>  gcc  <dig> .1;

power consumption compared to general-purpose microprocessor
fpga accelerator is also energy-efficient relative to general-purpose computers. our rnaalifold algorithm accelerator with  <dig> pes only consumes  <dig>  w as simulated by xilinx ise  <dig>  xpower tool. general-purpose microprocessors consume between  <dig> w to  <dig> w on average  <cit> . furthermore, considering the low clock frequency of  <dig> mhz in fpga chips, we believe the application-specific fine-grained schemes implemented in our accelerator provides significant advantage over the general-purpose schemes.

CONCLUSIONS
the minimum free energy  method plays an important role in the area of rna secondary structures prediction. many parallel implementations on general purpose multiprocessors exhibit parallel efficiency of no more than 50%. in this paper, we explore the use of fpgas to accelerate the rnaalifold algorithm based on mfe method.

after carefully studying the characteristics of the algorithm, we make five observations to direct our design. we optimize the nested loop structure in original rnaalifold and reorganize the computation order to improve the spatial locality. we propose task assignment in cyclic column order to achieve load balance. we introduce two data reuse schemes that use a sliding window cache and transfer registers between adjoining pes. we also presented several methods to reduce the storage requirement for holding multiple copies of energy parameter tables. the experimental evaluation demonstrates that the performance of our algorithm accelerator is scalable with multiple pes and that the fpga accelerator outperforms general-purpose computers with a speedup of more than 12× on  <dig> pes.

competing interests
the authors declare that they have no competing interests.

authors' contributions
fei xia carried out the fine-grained parallel rnaalifold algorithm, participated in the characteristics analysis of the rnaalifold algorithm and drafted the manuscript. yong dou conceived of the study, and participated in its design and helped to draft the manuscript. xingming zhou and xuejun yang participated in the discussion of the study and performed the performance evaluation. jiaqing xu participated in the sequence alignment and the analysis of original rnaalifold algorithm. yang zhang participated in hardware implementation and correctness verification and power consumption analysis. all authors read and approved the final manuscript.

