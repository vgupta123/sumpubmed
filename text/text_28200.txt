BACKGROUND
the huge number of gene expression values generated by microarray technology leads to very complex datasets, and many cohorts have the imbalanced classes problem . these complexities raise the challenge of how to identify the biomarkers that are strongly associated with the disease and that can be used to distinguish classes of patients. hence, feature selection is a critical technique in the field of bioinformatics
 <cit>  and it has been used in various domains for large and complex data, such as gene expression datasets.

gene expression datasets are typically noisy and often consist of a limited number of observations  relative to the large number of gene expression values . in practical applications, datasets often exist in an unbalanced form. that is, at least one of the classes constitutes only a small minority of the data. for example, the following well-known and publicly available microarray datasets are imbalanced: malignant pleural mesothelioma  and lung adenocarcinoma  gene expression dataset with a 17% class imbalanced ; acute lymphoblastic leukaemia  and acute myeloblastic leukaemia  dataset with a 32% class imbalanced . for problems such as these, the practical classification interest usually leans towards correct classification of the minor class. generally, most of the classifiers used to select features suffer from the imbalanced classes and many have poor performance because they are biased to the large samples and pay less attention to the rare class. consequently, unsatisfactory classification performance results and most of the rare class features are not recognized. these characteristics result in difficulties in working with standard machine learning techniques, which must be modified to deal with the complexities of gene expression data and to build an effective feature selection algorithm. these characteristics also adversely affect the analysis of microarray datasets that have received significant attention in the field of cancer diagnosis and treatment.

acute lymphoblastic leukaemia  is the most common childhood malignancy
 <cit> . it is a type of cancer that affects the blood and bone marrow. the causes of all are still unknown, but are thought to most likely result from mutations of genes
 <cit> . nowadays, all is diagnosed by a full blood count and a bone marrow biopsy. based on these examinations, an all patient’s risk of relapse and appropriate treatment are identified. most children achieve an initial remission, yet approximately 20% of children with all suffer a relapse
 <cit> . this relapse problem, where the cancer recurs, is considered as one of the major obstacles to curing all patients. one reason for relapse is incorrect therapy due to mis-classification of risk factors of all patients
 <cit> . consequently, accurate risk assessment of patients is crucial for successful treatment.

with microarray technology, it is becoming more feasible to look at the problem from a genetic point of view and to perform genetic-based risk assessment for each patient. however, too many features or genes in a dataset adversely affect similarity measurement and classification performance, because many of these genes are irrelevant to specific traits of interest
 <cit> . consequently, biologists need to identify a small number of informative genes that can be used as biomarkers for the disease in order to understand gene expression in cells and to facilitate diagnosis and treatment of patients. to achieve this, a real childhood leukaemia gene expression dataset collected from the children’s hospital at westmead is provided for this project that aims to identify biomarkers that are strongly associated with the risk of relapse of patients with the eventual aim of supporting clinicians and biologists in diagnosis and treatment of all patients. the dataset is composed of  <dig> patients and each patient has more than twenty two thousand gene expression values. patients are classified into three categories based on the cancer’s risk type: standard, medium and high risk. the majority of  <dig> patients are classified as a medium risk,  <dig> patients are classified as a standard risk and the minority of  <dig> patients are classified as high risk. this imbalanced classes problem adversely affects the classification performance in the feature selection process, because it can result in a trivial classifier that classifies all patients as the majority class. therefore, ignoring this critical data characteristic may result in very poor feature selection.

the random forest algorithm was developed by breiman
 <cit> , and is known as one of the most robust classification algorithms developed to date. it is an ensemble classifier consisting of many decision trees. many classification trees are grown during training. a training set is created for each tree by random sampling with replacement from the original dataset. during the construction of each tree, about one-third of the cases are left out of the selection and this becomes the out-of-bag cases that are used as a test set. the classification performance of the test set is evaluated based on the out-of-bag error rates.

random forest has been used extensively in the biomedical domain
 <cit>  because it is well suited for microarray data. features will not be deleted based on one decision or one tree, but many trees will decide and confirm elimination of features. another positive characteristic of random forest is that it is applicable to very high dimensional data with a low number of observations, a large amount of noise and high correlated variables. moreover, random forest is less prone to over-fitting and can handle the problem of imbalanced classes. all these characteristics make the random forest classifier an appropriate choice for gene expression datasets.

this paper addresses the problem of gene selection in the case of imbalanced datasets. several authors have previously used random forest for gene selection but they haven’t addressed that complex problem  and they did not take advantage of random forest in dealing with imbalanced classes. diaz-uriarte and alvarez de andres
 <cit>  explored the potential of random forest for attribute selection and proposed a method for gene selection using the out-of-bag error rates. the authors thoroughly examined the effects of changes in the parameters of random forest specifically, mtry, ntree and nodesize. however, the authors did not address the problem of imbalanced classes and how the parameters cutoff and sampsize can handle that problem. archer and kimes
 <cit>  performed a similar evaluation of the random forest classifier and achieved feature selection using variable importance measures obtained by random forest, but they did not address the problem of imbalanced classes. moorthy et al
 <cit>  also use random forest for gene selection based on the out-of-bag-error rates. the only difference is that
 <cit>  aims to obtain the biggest subset of genes with the lowest error rates. they have performed experiments to see whether the classification performance of the larger subset of genes outperformed the smaller subset of genes. these experiments also have been performed in this paper, but with the consideration of the classification performance effects on the imbalanced classes. overall, these proposed methods
 <cit> - <cit>  might not be the appropriate choice for our purposes as we have to select genes from imbalanced data. this paper also considers the problem of over-fitting, which must be addressed in any machine learning algorithm that is dealing with datasets having a low number of samples compared to a very high number of attributes. finally, and as in
 <cit> , the last issue addressed in this paper is the evaluation of the selected genes to determine whether they are stable and appear in multiple executions, or selected only once.

this paper proposes a feature selection method called balanced iterative random forest  to select genes that are relevant to a specific trait of interest from gene expression datasets. this work is different to the previous approaches because it enhances the gene selection process of imbalanced data by tuning the parameters cutoff and sampsize of the random forest classifier.

methods
balanced iterative random forest for feature selection
this paper introduces a new method for feature selection based on random forest called balanced iterative random forest . balanced iterative random forest is an embedded feature selector that follows a backward elimination approach. the base learning algorithm is random forest, which is involved in the process of determining what features are removed at each step. the algorithm starts with the entire set of features in the dataset. at every iteration, the number of the attributes is reduced by removing those attributes that have zero importance value. after discarding those genes, a new random forest is built with the selected set of genes that yields the smallest out-of-bag  error rate.

this algorithm is mainly tested on the real childhood leukaemia gene expression dataset collected from the children’s hospital at westmead. all specimens, as well as the associated comprehensive patient clinical data, used to generate the microarray dataset upon which we developed the birf algorithm, were made available to the chief investigators with the approval of and according to the guidelines established by the children’s hospital at westmead’s human research ethics committee and tumour bank committee and is compliant with the declaration of helsinki.

the r package randomforest is used in this paper. the two main parameters of random forest are mtry, the number of input variables randomly chosen at each split and ntree, the number of trees in the forest. these two parameters are set to their default values
. two other parameters are very important in this algorithm due to the problem of imbalanced classes and ignoring them may result in poor feature selection. the two parameters are cutoff, a vector weight for each class, and sampsize, the number of cases to be drawn to grow each tree. these two parameters are carefully tuned in order to achieve a successful feature selection process that able to recognize features in the minority classes and not ignoring them.

similar to standard classifiers, random forest also has the problem of learning from extremely imbalanced class datasets. however, random forest has the capacity to mitigate this problem, and two solutions are applied on the birf to alleviate it: balanced sample and cost sensitive learning. the balanced sample solution is based on the parameter sampsize, which aims to induce random forest to build trees from a balanced bootstrap sample, which is a bootstrap sample that is drawn from the minority class with the same number of samples from the majority class. in the case of imbalanced data, there is a high probability that random forest will build a tree from a bootstrap sample that contains only a few samples from the minority class, resulting in poor performance for predicting the minority class.

the second solution aims to apply a cost sensitive learning technique through the parameter cutoff in order to make random forest more suitable for learning extremely imbalanced data. cost sensitive learning assigns a high cost for mis-classification of the minority class and minimisation of the cost of the major class. as random forest generates votes to classify the input case, cost weights are applied on those votes in order to make the calculation of the votes as proportion, rather than whole. this solution aims to balance the distribution of classes without altering the semantics of the dataset or by down-sampling or over-sampling the dataset.

algorithm of balanced iterative random forest
a balanced iterative random forest algorithm is proposed to select the most relevant genes for the disease and can be used in the classification and prediction process. due to the large size of gene expression datasets, and in order to have a fast feature selection process, it was not practical to run birf algorithm on all genes of the dataset because it takes too long. consequently, we split the data, by the number of genes, randomly into different datasets only in the first iteration of the algorithm. this splitting of the dataset is optionally in the birf algorithm. by splitting the dataset, birf will run fast, but random forest may lose some global correlation in the first iteration however, it will be able to include it in the rest of the algorithm. without splitting, the birf algorithm takes too long to run, but it is able to include the global correlation in all iterations.

the birf algorithm  is run on each dataset to select the informative genes. the selected genes from each dataset are then combined to form a new gene expression dataset with fewer attributes. with this obtained dataset, birf then begins an iterative attribute elimination process and without losing the global correlation. it is presented below as algorithm  <dig>   

validation of over-fitting
over-fitting occurs in statistics and machine learning algorithms especially when these algorithms are dealing with complex datasets, such as gene expression datasets 
 <cit> . we also established in the background that one of the characteristics of random forest is that it is less prone to over-fitting. nevertheless, to further support the process of feature selection, additional experiments are performed to ensure that there is no over-fitting in the gene selection process. early-stopping
 <cit> , <cit>  is used here to avoid over-fitting by stopping the elimination of genes once over-fitting starts to happen. this is achieved by splitting the training set into a new training set and a validation set, which is used in the genes selection process to decide when to stop. in each iteration, after removing the irrelevant genes from the new training set, the same genes are eliminated from the validation set and classification performance is evaluated on the validation set . once the classification error rate of the validation dataset starts to increase after reaching a minimum value, it is assumed that the new training set is over-trained and that the algorithm should stop at this stage.

validation of the selected genes
the decision about how many attributes to use during the feature selection process is critical and has two effects. selecting too many attributes from the original dataset makes it difficult to analyse these genes in terms of their effect on the disease. on the other hand, in order to build a generalizable classifier or gene-based similarity measurement model, it is important to incorporate as much information as possible. therefore, it is possible to make a principled decision by testing the effect of the selected number of attributes on the classification performance to know whether more genes provide new information or not.

although the error rate of the validation dataset with the selected genes may reach a minimum value and provides a good classification performance, these selected genes may still require further exploration to determine whether they are globally informative or if they are just selected by chance and may be only predictive to that particular dataset. in order to support the gene selection process and to distinguish between predictive attributes and those that only appear to be predictive, this paper proposes a methodology to decide what genes best describe the original dataset. the methodology repeats experiments, training the birf algorithm several times and reduces the training dataset into several subsets . the resultant attributes in each subset are then compared to see what attributes are selected in multiple executions, and which attributes are only selected once. the assumption is that the attributes that appear in multiple subsets are more informative than attributes that appear in a single subset. the subset that contains the most common attributes with the minimum error rates on the validation dataset is the one that best describes the original dataset.

RESULTS
several experiments are performed on the balanced iterative random forest algorithm in order to demonstrate the validity of the proposed algorithm, to evaluate the algorithm on different datasets and to compare our achieved results to other algorithms by using the same datasets.

datasets
the experiments are performed on a childhood leukaemia gene expression dataset that has been collected from the children’s hospital at westmead. this dataset is also available in the public domain and can be explored through the oncogenomics section of the paediatric oncology branch at the national cancer institute nih, usa . the dataset was normalized by the distance weighted discrimination  algorithm
 <cit> . the entire childhood leukaemia gene expression dataset is composed of  <dig> patients with expression values for  <dig>  genes. however, stratified random sampling is applied on the gene expression dataset and it is divided into training and test datasets. the training dataset is composed of  <dig> patients who are classified as follows: 

• standard risk 

• medium risk 

• high risk 

the test dataset is composed of  <dig> patients and they are classified as follows: 

• standard risk 

• medium risk 

• high risk 

three other publicly available microarray datasets: nci  <dig>  colon cancer and lung cancer datasets have been used in this paper for evaluation of birf. these datasets are characterized by a relatively small number of samples with a high dimensional space. for the two datasets , the same training and test data reported in the previous studies are used in these experiments, without changing the sample sizes, so that the obtained results can be objectively compared with earlier published results. however, a stratified random sampling is applied on the nci  <dig> dataset and it is divided into training and test datasets. 

• nci  <dig> dataset is a well-studied publicly available microarray benchmark collected by ross et al
 <cit>  and is produced using affymetrix hg-u133a chips. the data we used is the same as the data used in
 <cit> . the dataset consists of  <dig> samples that are classified into eight categories. each sample is measured over  <dig>  gene expression values .

• colon dataset is a publicly available microarray dataset that was obtained with an affymetrix oligonucleotide microarray
 <cit> . the colon dataset contains  <dig> samples, with each sample containing the expression values for  <dig> genes. each sample indicates whether or not it came from a tumour biopsy. this dataset is used in many different research papers on feature selection of gene expression datasets
 <cit> . the dataset is quite noisy but the real challenge is the shape of the data matrix where the dimensionality of the feature space is very high compared to the number of cases. it is important to avoid over-fitting in this dataset. although the number of cases is very low, the dataset is split into two: a training dataset and a test dataset composed of  <dig> and  <dig> samples, respectively .

• lung cancer dataset is also used in the experiments and it was generated with an affymetrix oligonucleotide microarrays and normalized by z-score
 <cit> . each sample it indicates whether it came from a malignant pleural mesothelioma  or adenocarcinoma . there are  <dig> tissue samples  that have already been broken into training and testing samples. the training dataset contains  <dig> of samples,  <dig> mpm and  <dig> adca. the remaining  <dig> samples are used for testing. each sample is described by  <dig> genes. similar to the colon dataset, the lung cancer dataset is also noisy but with more samples and genes. these samples are broken into two datasets: a training dataset and test dataset composed of  <dig> and  <dig> samples, respectively .

experiments on childhood leukaemia dataset
balanced iterative random forest is validated with the childhood leukaemia gene expression dataset collected from the children’s hospital at westmead. it is important to note that the main purpose of these experiments is to find a subset of genes most closely correlated with the leukaemia risk type distinction. also, it is important to incorporate as much data as possible without including so much data that it may result in losing interesting separations between patients. the set of informative genes to be used in the prediction of risk type was chosen to be the  <dig> genes  selected at a lowest error rate  <dig> . in order to validate the results obtained from this experiment, the test dataset is processed by random forest to view the classification performance of the selected genes. table
 <dig> shows the confusion matrix for classification of the test dataset and demonstrates that the classifier generalised reasonably well.

validation of results in terms of over-fitting
it is important to be able to validate results and prove that they are not due to over-fitting of the training data. the early-stopping is used here and iterative random forest is run again on the training dataset to select the relevant biomarkers. simultaneously, the validation dataset is involved in this process to ensure that the training process does not over-train. the training process of the iterative random forest on the childhood leukaemia dataset is shown in figure
 <dig> 

as can be seen from the graph and based on the training dataset, the out-of-bag error decreases as the number of irrelevant features and noise is eliminated at each iteration. after several iterations, the out-of-bag error becomes stable in a range between  <dig>  and  <dig> . with respect to the validation dataset, the error rates also decreases as the number of irrelevant features from the training dataset are eliminated at each iteration. the error rates of the validation dataset consistently decreases in the first eight iterations. after the eighth iteration, the error rates of the validation dataset increases again and becomes unstable for several iterations. the training stops at the eighth iteration when the lowest error rates are achieved for the validation dataset . it is important to note that there is no over-training of the dataset in the first eight iterations and that the number of features is greater than  <dig>  after the ninth iteration, the error rates of the validation dataset starts to increase again after reaching the minimum.

analysis of selected genes
to further evaluate the attribute-selection process, experiments with the balanced iterative random forest algorithm are repeated three times. the resultant attribute lists from each repetition are then compared to the attributes obtained from the initial experiment where  <dig> genes have been selected. the goal is to see whether the  <dig> selected attributes appear in the three resultant attribute lists, or not. it is interesting to note that 80% of the top  <dig> genes consistently appear in the three lists, and the top  <dig> genes remain near the top in the other three lists. sixty four percent of the top  <dig> selected genes from each list are the same. this supports the fact that the top selected genes are globally predictive and have not been selected by chance. moreover, it also indicates that the feature selection process was not over-trained.

classification performance of the three resultant attribute lists are also compared to see whether the list that contains the most common attributes provides good separation between the patients. the error rates of the of the three lists are  <dig> ,  <dig>  and  <dig> , respectively. it can be clearly seen from this analysis that the dataset with the selected  <dig> genes  contains the most common attributes. it provides the minimum error rates  and is the best for describing the original dataset.

experiments on the three public microarray datasets
one of the most important aspects of any experiment is validating the algorithm. validation is achieved by applying the proposed algorithm on three publicly available microarray datasets. if the algorithm performs well then the feature selection process has been completed correctly.

balanced iterative random forest is initially validated on the nci  <dig> dataset. the same procedure, that is early stopping, is applied on the nci  <dig> dataset in order to validate the results in terms of over-fitting. the training process of the iterative random forest on the nci <dig> dataset is shown in figure
 <dig>  the graph shows that the out-of-bag error rates of the training dataset decreases until they reach the minimum reduction of genes, which is realized at  <dig> genes with the lowest out-of-beg error rates  <dig> . with respect to the validation set, the minimum error rates of  <dig>  are achieved at  <dig> genes. the error rates are then increased again to reach  <dig>  at  <dig> genes. the number of the selected genes from this process is  <dig> with a  <dig>  error rates.

balanced iterative random forest is also validated on the lung cancer dataset
 <cit> . the minimal o error rates of zero is achieved at  <dig> features, which are selected as the most important features for classification. this result is also validated in order to ensure that the feature selection process has not over-fitted to that training dataset. with the selected  <dig> features, 97% accuracy have been achieved on the test dataset with only one patient is wrongly classified.

the same procedure is applied to the colon dataset
 <cit> . nineteen features are selected as the most important features in classification with a minimal error rates of zero. an accuracy of 96% has been achieved for the test data where only one patient is wrongly classified. these results suggest that birf works well for several gene expression datasets.

comparison with other state-of-the-art algorithms
in the previous section, we performed experiments on two different public gene expression datasets that have been analysed by researchers using various gene selection methods. we compare the classification performance of the variable selection approaches used by the following two classifiers: 

• support vector machines : svm are considered as one of the best performers for a number of classification tasks ranging from text to microarray data
 <cit> . the goal of svm is to find the optimal hyperplane that separates the classes. this hyperplane separates the classes into two categories. in case the target data has more than two categories, several approaches have been proposed, but the one used here is one-versus-one  svm
 <cit>  as implemented in
 <cit> . more introductions and description of svm can be found in
 <cit> .

• naive bayes : nb is a simple probabilistic classifier based on the so-called bayesian theorem. the goal of nb is to calculate the probability for a given case in order to assign it to a certain class. naive bayes assumes that the features constituting the case contribute independently for a given class. naive bayes is used for predicting mirna genes
 <cit> , emotion recognition
 <cit>  and gene selection
 <cit> .

we report the results achieved by support vector machine-recursive feature elimination , multiple svm-rfe , random forest  based backward elimination procedure
 <cit>  and naive bayes  as shown in table
 <dig>  we have used the standard error rates, that is, subtracting the predicted samples from the actual samples and then dividing it by the actual samples. the error rates in birf are calculated using the independent sub-sample method. however, in
 <cit> , the authors calculate the error rates using leave-one-out cross validation, and in
 <cit> , the authors use the bootstrap sample. the best performance on the colon dataset is achieved at 96% obtained by birf. the accuracy of svm-rfe, msvm-rfe, rf and nb is  <dig> %
 <cit> ,  <dig> %
 <cit> , 87%
 <cit>  and 87%
 <cit> , respectively. with respect to the lung cancer dataset, 81% and 88% accuracies are reported  using the bagging and boosting methods
 <cit>  where 97% is achieved by birf and standard random forest. the results were not available  for rf and nb in the lung cancer dataset because no results were provided in the references
 <cit> .

comparison of birf, rf and msvm-rfe on the childhood leukaemia dataset
we have compared the performance of birf to msvm-rfe and rf gene selection methods to show the predictive performance of birf, particularly on the childhood leukaemia dataset. multiple svm-rfe is a widely used gene selection method that involves iteratively fitting svm classification models by eliminating the genes with the low impact on classification in order to produce a small subset of genes that provides the best classification model. one-versus-one  svm
 <cit>  is used in these experiments for a multi-class dataset. on the other hand, random forest based backward elimination procedure involves iteratively fitting the random forest model. at each iteration, genes with the smallest importance value are removed and a new random forest model is built with less number of genes and smallest out-of-bag error rates.

the childhood leukaemia dataset is used here to compare the gene selection performance of birf to msvm-rfe. at each number of selected genes, svm and random forest models are built on the training dataset with the selected genes using the leave-one-out method to compute the accuracy of classifiers. table
 <dig> shows the performance prediction of the two classifiers at different numbers of selected genes. as can be seen from the table, the accuracies of the two classifiers increased as the number of the selected genes decreased until the two classifiers achieved the maximum accuracies. at that stage, both classifiers’ accuracies decreased as the number of genes decreased. the highest accuracy of birf is achieved at  <dig> . with that accuracy, all the patients in the two minor classes  are predicted correctly, while  <dig> % were predicted from the majority class . the highest accuracy achieved by msvm-rfe is  <dig> . however, the accuracies of the high, medium and standard risk patients are  <dig> %, 100% and  <dig> %, respectively. from this comparison we can conclude that birf outperforms msvm-rfe, especially in predicting the patients in the minor class.

the two built models  that provide better prediction are then used on the independent test data  to assess the accuracies of the classifiers with the selected genes. the auc  of the random forest model built on the  <dig> birf-selected biomarkers is  <dig> . however, the svm model built on the  <dig> msvm-rfe-selected biomarkers has an auc of  <dig> .

random forest  based backward elimination procedure
 <cit>  is also applied to the childhood leukaemia gene expression dataset. this method completely failed to predict patients in the minority classes without handling the problem of imbalanced classes. this result suggests that standard random forest has to be modified in order to consider the problem of a cohort existing in an imbalanced form.

CONCLUSIONS
this paper proposes a method called balanced iterative random forest to select features from imbalanced gene expression datasets. feature selection as one of the most important processes in the field of microarray data has been considered carefully in this paper. this paper shows that the feature selection process is undertaken in an intelligent way, especially the way in which the imbalanced classes and over-fitting problems are handled, and when the selected genes are evaluated by reducing the dataset into several subsets of varying sizes to see whether the selected genes are stable and appear in the multiple subsets. it is unrealistic to assume that the attribute-selection algorithm, in this case the balanced iterative random forest algorithm, will be able to pinpoint what attributes can describe the risk type of the patient and identify all of the biologically significant attributes with such a large and complex dataset. nevertheless, the attribute selection process is undertaken carefully by validating the results, and it produces a small subset containing the most informative genes. this result was validated and supported through two different experiments: over-fitting validation and analysis of the selected genes. the experiments demonstrated that the classifier did not over-fit the training dataset. also, the analysis of attributes to distinguish between predictive attributes and those that only appear to be predictive  showed that most of these attributes appeared in multiple repeats of the algorithm runs. however, birf algorithm has a limitations that is random forest will not be able to get global correlation due to the splitting of the dataset but this is optional and can be avoided if you don’t want to run birf fast or you have a powerful machine. another limitation is tuning the parameter cutoff which is responsible handling the imbalanced classes problem. balanced iterative random forest is also applied to three other microarray datasets: nci  <dig>  colon cancer and lung cancer datasets. overall, birf resulted in classifiers comparable or superior in accuracy to svm-rfe, msvm-rfe, rf and naive bayes on the colon and lung datasets.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
aa and pjk conceived the algorithm and designed the experiments. mg proposed comparing the algorithm to the other state-of-art methods. aa performed experiments and analysed the data. drc worked on the biological analysis of the selected genes. all authors read and approved the paper.

supplementary material
additional file 1
this additional file shows the information about the selected genes which validated through gene ontology.

click here for file

 acknowledgements
the authors thank the children’s hospital at westmead for providing the childhood leukaemia dataset and for giving the biological point of view for the selected genes by presenting them to biologists whose have the key molecular factors of genes or biomarkers behind the disease.
