BACKGROUND
genomics and systems biology rely on vast amounts of organized data in order to use the sophisticated bioinformatics tools that allow modeling, analysis and interpretation of biological processes like gene regulation, variation in expression profiles and metabolic pathways  <cit> . gene regulation, for example, involves complex interactions between genes, transcriptions factors , proteins and metabolites that can be visualized as networks of inducing and repressing interactions controlling gene expression. those interactions triggering gene expression and protein synthesis may control cell development and adaptability to environmental changes. most of the end result of this kind of biological research materializes in textual publication in peer-reviewed journals, but as such is not directly amenable to computational treatment. databases compiling this information have been developed for some organisms . however, the manual curation involved in ensuring the reliability of this data is a resource- and labor-intensive endeavor. the ever-expanding literature  can literally overwhelm the ability of researchers to make sense of this flood of information. thousands of papers must be selected and retrieved from repositories such as nih's pubmed and medline, and carefully reviewed by experts in order to extract the facts needed by the research community. the last ten years have seen a proliferation of natural language processing  techniques to aid in dealing with the explosive growth of useful data. several good overviews  <cit>  of the state-of-the-art in text-mining have been published recently, and some of the techniques have reached an acceptable level of maturity that allows them to finally perform reasonably well. information extraction  is one of the computational methods that has been used successfully before in other non-scientific fields  <cit> . in contrast to information retrieval, which provides a ranked list of relevant documents, ie not only finds which sources of information are relevant, but also automatically populates databases with information that fulfills the user's needs. compilation of the extremely complex networks of biochemical interactions that control developmental and functional processes in cells is a prime example of the usefulness of these technologies for biological research, as evidenced, for instance, in the various systems competing in the biocreative evaluations  <cit> , or in earlier examples, the pastaweb  <cit> , genies  <cit> , ihop  <cit>  and biorat  <cit>  systems, all of them correlating biological networks to the literature that describe them.

in this work, we show how, and to what extent, a state-of-the-art nlp system can aid the manual curation of transcriptional regulation in escherichia coli, and how both approaches  can complement each other and enrich curation. although we don't claim any significant technical advance in the current state-of-the-art in text mining per se, we present a much-needed evaluation of the impact of human language technologies  on annotation and data-gathering using as a gold standard a biological model  instead of an annotated corpus. we also suggest a methodology to merge these methodologies into the conventional curatorial workflow.

RESULTS
for the overall architecture of the ie system, we adapted a rule-based pipeline first described by saric et al. for the string-ie system  <cit> . the pipeline is described in detail in the methods section. we chose a language-engineering approach since in general we were concerned more with accuracy  than with coverage , and wanted to be fairly sure about the regulatory interactions we would be extracting. rule-based systems are more accurate for well-defined tasks , while statistically-based approaches, although less exact, are more robust and tolerant to noisy data and errors.

in order to test the capabilities of the extraction system reported here, various collections of documents were downloaded; involving both abstracts from pubmed or full text articles available from the various journal's websites or from subscription services. we employed different strategies for the triage process, which will be described in the corpus subsection. in this work we used fairly standard search procedures, either gathering the lists of references from pre-curated databases, or doing various keyword searches using the nih entrez facilities.

the database we used to evaluate the output of our ie system, regulondb,  <cit>  is the primary source of curation of original literature with experimental knowledge about the elements and interactions of the network of transcriptional regulation in e. coli k- <dig>  regulondb can be considered a computational model of mechanisms of transcriptional regulation in this organism, and contributes data to the ecocyc database. both contain mechanistic information about operon organization and their decomposition in transcription units, promoters and their sigma type, binding sites of specific transcriptional regulators, etc. the usual curation process starts by searching for articles that contain information about transcriptional regulation , using a set of pertinent keywords in the pubmed database. to select only the most relevant articles, a team of biologists-curators reads the abstracts of these papers. other papers not originally retrieved with our search strategies are suggested by external researchers and are incorporated into the curation cycle. finally, the data annotated by reading the articles is added to both regulondb and ecocyc databases. although this curation process yields reliable data reflecting what is there in the literature, it is a long-term and manpower-intensive effort in which errors can occur at different stages, and recovery of omitted data is very difficult. furthermore, with high throughput technologies, the amount of published interactions with some type of experimental evidence , will likely scale up in the near future. this motivates us to get prepared for new curation strategies combining automatic nlp processes with human curation.

corpora
for our evaluation of curation of e. coli regulatory networks we collected different sets of abstracts and full papers that we knew contained, to varying degrees, valuable information on the subject of transcriptional regulation. some sets were based on manual curation efforts from related databases , while others resulted from carefully-crafted search strategies using the ncbi pubmed facilities, so that those manually-selected ones constitute a reliable baseline of positive examples of information-rich documents. the different corpora used in our study are summarized in table  <dig>  ranked  according to their putative relevance to the domain of transcriptional regulation in e. coli k- <dig>  the corpora include both full-text papers and abstracts, and range from around  <dig>  abstract document sets from regulondb curator's keyword searches  or the ecocyc  <cit>  database  to  <dig> full-text papers  carefully selected by regulondb curators as containing information on the regulatory network. one of the corpora  was compiled by saric et al.  <cit>  by searching the full pubmed for abstracts that mentioned e. coli and sentences that contained at least two gene names, a strategy that although could find many relevant articles but would nonetheless introduce a lot of noise due to the nature of the bacteria as a model organism used in many biological contexts. the different selection criteria of the diverse datasets analyzed reflect different purposes, and a comparison of the results obtained using each one, as a textual source, would be of obvious interest for text miners and curators alike. figure  <dig> illustrates in a venn diagram how each of the six corpora might overlap and complement each other. the dots represent papers putatively relevant for transcriptional regulation in our bacteria, and the different selection criteria  result in diverse document sets, which can contain groups of the same documents as well as other papers also relevant. thus, corpus rn with curator-reviewed references to the regulatory network constitutes the most relevant textual source, while corpora st and ea contain less pertinent, more diverse information. a search with a wider net cast, for example, in the st corpus will contain many more papers than the ea one, or the rs one , but the quality and density of relevant information will supposedly be higher in the human-selected ra or rp corpus. full-text papers are logically richer in information than abstracts, since the latter constitute only brief outlines of the main claims of papers and lack the concrete detailed data that mining efforts strive to extract from documents. one of the purposes of using such a diverse array of corpora was to see if the added effort needed to use full-text papers for text mining was worth it, as well as getting some insight on which search strategy to locate relevant documents was more valuable.

description of the different full text and abstract corpora used for extraction of regulatory interactions. the document sets are based on pubmed searches and on reference lists from database curation efforts.

a markup language for mining bacterial regulatory networks
the system's output is an xml file with a format we have called regulatory network mining markup language , which allowed representation, in a user-friendly way, of the basic data relevant to information extraction of genetic regulation, both from the perspective of a biologist's interests  and from the nlp specialist's needs . interactions are here defined as a tf-target gene relationship, independently of the number of binding sites of a tf upstream of a particular promoter. the following example shows some of the main features of a typical entry in rnm2l from our tests, depicting sentence  as a source.

 in contrast, acnb expression is activated by crp and repressed by arca, frur and fis from pacnb.

in the "interaction" node, the label from shows the internal name of the final extraction grammar rule that justified retrieval of that sentence, while source refers to the collection where it was found, and pmid refers to the identification number of the paper where this particular sentence was retrieved from:

<interaction id="596" from="anaph+ev_act_expr_xr" ri_function="repressor" source="regulonabs">

<regulator genprotid="eck120011345" org="ecoli" type="nxpg"> arca </regulator>

<regulated genprotid="eck120002193" org="ecoli" type="nxpg"> acnb </regulated>

<evidence verb="repressed"/>

<sentence pmid="9421904"> in contrast, acnb expression is activated by crp and repressed by arca, frur and fis from pacnb. </sentence>

</interaction>

thus, we identify two entities, one the regulator and the other the regulated one, with unique id numbers for each, as well as organism and semantic types . note that this entry represents only one of the four different one-to-one interactions that can be extracted from the sentence. we devised rnm2l because: a) system biology markup language  <cit>  was considered too cumbersome for the more limited task of representing networks with discrete states, and b) because a more specialized tagging scheme would allow better comparison between networks extracted from different sources or with different systems or methodologies, while at the same time being useful both for nlp optimization and for biological interpretation. in the supplementary material site we provide a defining dtd schema for rnm2l.

evaluation criteria for the network extraction task
to evaluate the extracted regulatory networks we implemented a benchmarking tool that checked each interaction against the regulondb database. we assume the database is a reliable gold standard of the final desired output, in contrast to the more customary evaluation methodologies that use a set of manually-annotated sentences from which a database of relevant information can be elicited. since our database does not include the actual sentences from which the facts have been extracted, our automatic evaluation does not have available to it the linguistic expression of the facts. this methodology involves, of course, some assumptions about the completeness and exhaustiveness of the regulondb database. since e. coli k- <dig> is one of the best understood model organisms in the literature, and the regulondb is carefully compiled and monitored, we can nevertheless rely on it to be a good baseline for evaluation.

those interactions that had a regulator gene/protein not found in regulondb  were filtered out, but overall we kept interactions where the gene/protein was identified as a mutant or coming from another  organism. our evaluation considered two cases, a) where the interaction has both the regulator and the regulated genes/proteins correctly identified but their type of interaction might be undetermined, or b) where, besides a correct identification of the involved entities, the nature of their interaction was also correctly identified .

to test how previously-known information on the network could support the extraction effort, we created two different networks from each corpus: one where we artificially inserted interactions derived from previous knowledge about operons, heterodimers and the like, and another one where we assumed we didn't have a priori information about those multi-entity objects; for example, from the sentence: "transcription of the e. coli melab operon is regulated by the melr protein" the fact that mela and melb are regulated by the melr protein could be extracted. the non-enriched networks, although less complete, would be more realistic with regard to the true capabilities of the extraction system, when applied to other less studied organisms for which we don't have as much genomic information as for the e. coli proteobacteria.

we calculated precision, recall and f-measure of the extracted samples by considering regulondb as the ultimate instance of the bacterial network. we previously subtracted from it all computational predictions, since they would conceivably not be present in experimental papers. recall  was estimated using the entries in our reference regulondb database, and considering as a single one the cases where there was both activation and repression annotated. this is not an orthodox measure of exhaustiveness in information extraction evaluations, but since we were not using annotated corpora it was the closest we would get to knowing how many extractable phrases were extant in our corpus, assuming all were there and available to the system. because of these assumptions, the actual performance of the system could realistically be significantly better than what this fully-automatic verification might lead to believe. for precision , we calculated two values: one  where we used all the interactions retrieved , and another one  in which we only considered the subset of all interactions where the regulator gene/protein was cataloged in the reference database. as in other aspects of these evaluation, we did this in order to understand what would happen in cases where we are dealing with other organisms for which we don't have all the information that we have available for e. coli, for instance, where we don't have an assumption of a completely curated network or not all transcription factors are known.

in addition to the networks extracted from the different document collections described earlier in the corpora section, we also integrated into a single network all unique interactions retrieved from all the text-mining sources , in order to have a single sample that would be as exhaustive as possible, regardless of whether the initial search strategy was fine-grained  or coarse-grained as in the  <dig>  abstract corpus from string-ie keyword searches. our final metrics for various datasets are shown in table  <dig>  and the network files, customized processing modules and supplementary material can be found at the regulondb website  <cit> .

an asterisk  next to source name indicates that no multiple-unit objects  were added; the regulondb "dual" interactions , are counted here as two distinct interactions. as represents a file containing all compiled interactions found in all textual sources used in this work, and constitutes the sum of all non-redundant interactions extracted from the full-text and abstract documents.

 <dig>  unique, non-repeated interactions found for each file

 <dig>  interactions that match regulondb interaction pairs, but whose function  was not determined by the system.

 <dig>  interactions that match regulondb entries, and also match repressor/activator function

 <dig>  overall matches , including both interactions with complete information as well as under specified ones

 <dig>  % of total interactions in file which are correct: /

 <dig>  recall: percentage of regulondb's  <dig> interactions that is represented by correct interactions in file: *100)/3108

 <dig>  precision 1: number of overall matches /unique interactions in file 

 <dig>  precision 2: number of overall matches /interactions in file that contain a regulondb-annotated regulator

 <dig>  f-measure 

discussion
our usual curatorial workflow  leaves out between  <dig> and 50% of all papers retrieved from keyword searches . figure  <dig> shows how many abstracts were retrieved with our search strategies each year, and how many of them were actually manually curated. one of the motivations of this work was to estimate if our triage and curation efforts were retrieving all the potential information on the subject of transcriptional regulation on e. coli that was available in pubmed-based literature, and how to better explore for curation the full extent of the literature obtained in our keyword searches.

a biologist reviewed a random sampling of the most comprehensive set of interactions, which our system was able to extract, but that were not found to be in regulondb. this exercise allowed us to explore information that: a) was not retrieved during the manual curation process, but should have been, b) was processed incorrectly or c) although basically correct, was not completely relevant to our current purposes . from a random sample of  <dig> interactions, we found  <dig> that represented relevant information that was not present in our reference database, but that merited either a closer look at the sources or further analysis to establish if it should be incorporated into regulondb. there were multiple reasons for this information being missing, among them: 1) the source papers had not been retrieved for curation or had not been curated yet, or 2) the genes or tfs were mentioned with unusual synonyms, ids, references or terms, which made their manual curation difficult, 3) or the evidence presented either was deemed insufficient by curators or was presented with high level of hedging . if we extrapolate this figure to the  <dig>  interactions from the comprehensive non-enriched  <dig>  interactions set that could not be matched with the relevant  <dig>  regulondb entries, around  <dig> new interactions could be added to the database after manual review. regulondb curators will curate this automatically-generated network to see if they can integrate the data into the database, but also will search for other pertinent information, such as site and distances for gene and promoters.

in order to test the linguistic processing of the system, we did a manual review of a random sample of  <dig> interactions extracted, and we established  <dig> of them as having a basically correct semantic interpretation of the sentences, and  <dig> of them as being biologically correct to the point of including the right activation or repression function, for a 84% overall precision. the network that was gathered from all sources allowed us to obtain 45% of the entire human-curated regulondb network, while a more limited 700-plus selection of network-related papers  accounted for 33% of that total. the "artificial" addition of multiple-entity objects like operons and two-component protein systems from regulondb  increased the size of the global network by  <dig> percentile points . in most datasets the increase was less significant, and we believe that as a whole the value of the information added with previous domain knowledge was not overly important.

in a more extensive evaluation simulating full curation of nlp-generated networks, a domain expert reviewed  <dig> interactions that a) were obtained from processing the  <dig>  abstracts retrieved using the regulondb search strategies and b) were not found in the regulondb database. again, we wanted to test if the extraction system could find relevant information that was missed at the triage or curation stages . we found  <dig> interactions that could be added to regulondb, while in the rest there was either an error in the inference made from the text, or even if the inference was correct the data could not be added to regulondb because of various reasons, among them: the regulation was not transcriptional, did not correspond to e. coli, there was an error in the gene/protein identification module, etc. in a few of the interactions the data seemed correct but the complete papers were unavailable to check them fully before adding them to the database. the  <dig>  % of total interactions that was either immediately useful or seemed correct pending further analysis constitutes a reasonable addition to the curated network that seems to be worth the review effort by the curatorial team . in any retrieval system, the small "tail" of valuable missed information is always harder to come by than obvious cases, and improvement on recall and precision is hard. it is clear that a fully automatic processing of these sentences would require complex artificial intelligence inference engines customized for biological interpretation. data that was not included in the regulondb network we tested against  could nonetheless provide important relationships associated with the annotation process. by doing a manual sweep of computer-generated curation, new or relevant information can be garnered that complements, expands or confirms human annotation.

an interesting issue to explore is why our ie system didn't obtain a more complete network from these papers. in other words, what are the limitations of an extraction system such as the one implemented for this task? first, there is the issue of the availability of full-text papers, which contain orders of magnitude more information than abstracts. from around  <dig>  regulondb pubmed ids as of june  <dig>  we were able to obtain just  <dig>  . even with those we actually collected, we also had to deal with incorrect conversion from pdf format, inconsistencies in term usage, etc. another problem encountered is that not all the information is consistently presented in an explicit manner. sometimes tables, graphics and illustrations provide what human curators need to generate relevant information, for example, by using some kind of domain inference in ways that our system was not designed to do. the sources used were also decisive factors in how well the system was able to generate a useful network. in order to compare the triage techniques employed in regulondb, we estimated what could be termed the "informational density" of the different corpora. we correlated the total size of the network obtained from these sources, the number of distinct documents and the raw size of each one, as shown in table  <dig>  this comparison allowed us to compare more accurately the quality and quantity of the network information obtained with each of the document sets. one of the ratios we estimated was the percentage of all interactions obtained that were found in regulondb , while other measures the average size of each document in the corpus , and the last one show how many regulondb interactions were obtained per document . the density of regulondb-related information shows that in a set of abstracts with fewer overall interactions than a full-text one with a similar number of documents, the relevant information can be more densely-packed, although we can expect to retrieve a smaller quantity of information. the interesting comparisons here are between documents gathered with different search strategies, and resulting in different numbers of documents. regulondb searches  and string-ie abstract searches  show a similar global network size, but quite different number of relevant interactions per document, while the curated ecocyc paper set  compares fairly well with rs. these numbers by themselves are not an explicit guideline of how to obtain a corpus that will maximize the potential information retrieved in text mining, but they do allow some degree of insight into diverse sources where document numbers and sizes as well as relevance are very different. until the automatic paper selection issue is satisfactorily solved, high-throughput information extraction techniques can help lessen the impact of this specific problem on total results, since the technology can process equally well a lesser number of more informational papers than a much more extensive set of less-relevant papers, and still retrieve a significant amount of useful interactions.

a comparison of the degree of informativeness with regard to transcriptional regulation in e. coli k- <dig> in various corpora, as established from the number of regulondb-attested interactions they contain; the table includes total number of documents and interactions , percentage and number of all interactions found in regulondb , average size of each document in the corpus , ordered by number of regulondb interactions per document .

CONCLUSIONS
how much valuable information is lost in papers that are filtered-out in the initial triage? is a tightly-focused search-and-selection strategy better than casting a wider net? what is preferable when curating large-scale biological networks, exhaustiveness or precision in the data? can automatic and manual annotation complement each other so we don't have to expect a trade-off with these two parameters? in this work we have addressed some of these issues by evaluating the use of nlp-based information extraction techniques for enhancing curation of a database of regulatory interactions. saric et al.  <cit> , for instance, presented a regulatory extraction system designed for inter-species versatility, but their evaluations focused more on the computational aspect of their system and not sufficiently on the implementation of such a system within a curatorial workflow. rodriguez-esteban et al.  <cit>  evaluated the results of the geneways pathway information extraction system using machine-learning techniques to simulate the decision-making process of curators when reviewing the output of an ie system, thus framing the question as a classification problem . one conspicuous difference between their evaluation approach and ours is that we used regulondb as a gold standard, while they relied on manually-reviewed training-testing data, albeit with measures of inter-annotator agreement that ensures certain objectivity. karamanis et al.  <cit>  reported the use of nlp-based tools to assist curation of the fruit fly database, but their evaluations were based on the average time employed by curators to fill their forms and are thus not really comparable with our own methodologies for testing overall curation, where completion time is not as critical as annotation accuracy.

by going over automatically-curated information from an extensive spectrum of sources, human users can not only verify information already curated by more traditional means but can also encounter information that either escaped manual curation or can help contextualize previously captured information. an accurate pre-annotation of non-curated text is also possible with automatic processing, which can suggest, as well, better search heuristics for finding relevant literature. we believe that a parallel manual/automatic curation process, as shown in figure  <dig>  can achieve results that represent the best of both worlds. this two-pronged approach can ensure broad coverage of the data without significant loss of precision, since a human expert will still vet all information added to the reference database. at present we are evaluating the integration of nlp-aided tools to enrich and enhance regulondb curation. nlp tools will require a different type of curation, devoted initially to revising the false positives and negatives, in order to detect what failed in each case, thus providing feedback to the system developers in addition to the actual gathering of biological information, but not replacing manual curation.

besides some of the challenges already mentioned in the previous section for a fully-automatic curation system, some of the system's shortcomings have to do with mundane reasons, such as incomplete named-entity dictionaries, imperfect format conversion, word tokenization, reporter-gene occurrences, etc. incremental enhancement in these areas is time-consuming, but it is also perfectly feasible and does not represent especially difficult technical hurdles. a more complex challenge for automated curation is the interpretation of tables and figures, which help authors present multiple interactions or their evidence in a manner well suited for human readers, but enormously difficult for computer processing.

in this work we have put forward an xml-based markup language  to represent text-mined regulatory networks in a way that is useful both to biologists and to nlp specialists alike. we have also suggested using a combination of manual and automatic literature curation to achieve precision in the annotation without sacrificing completeness. our experiences with automatic and semi-automatic annotation have shown that natural language processing can be an extremely useful tool for curatorial efforts, although it is still far from being able to do full text interpretation or locate all the information that a trained human expert can. an important conclusion we have drawn is that since information extraction systems can as easily handle large amounts of documents as a more limited datasets, using extensive search strategies combined with more focused ones can provide both a satisfactory coverage of the potential information, as well as good precision, without too many false positives to review. a rule-based system will not obtain all the network interactions that a human curator team can, but will not overwhelm them with non-relevant instances, unlike a statistical-based one that can potentially be more comprehensive but also more noisy. methods for filtering interactions that are based on, for example, the number of sentences attesting the same annotation or on the presence of known transcription factors, are being explored by our group and others  <cit> , but the amount of interactions and the time needed to review them by curators in order to find overlooked or novel information, is still an interesting open question. manual curation of the output of automatic processes is a good way to complement a more detailed review of the literature, either for validating the results of what has been already annotated, or for discovering facts and information that might have been overlooked at the triage or curation stages. by combining the exacting precision of human readers with the tireless abilities of information extraction systems to rapidly cover a lot of ground with reasonable accuracy, we believe that genomic data on other organisms less studied than e. coli can be obtained for the high-throughput methods of systems biology and genomics.

