BACKGROUND
protein-ligand binding is essential for important physiological processes, such as cellular signaling, respiration, metabolism, defense against antigens, neuronal excitation and inhibition, hormone regulation, protein translation, etc., and so plays a fundamental role in drug design. to develop a new drug, first, a critical protein is identified in the pathway of a disease of interest. then, small drug-like molecules called ligands are found or designed that will bind to the target protein, modulate its activity, and thus provide therapeutic benefit to the patient. the strength of binding of these drug-like molecules to the target protein is referred to as binding affinity and is commonly characterized using the dissociation constant between the ligand and its target macromolecule. in vitro determination of binding affinity is a time consuming and laborious task, especially for a large number of ligands. due to prohibitive costs and delays associated with experimental drug discovery, pharmaceutical and biotechnology companies rely on virtual screening using computational molecular docking  <cit> . typically, this involves docking tens of thousands to millions of ligand candidates into a target protein receptor's binding site and using a suitable scoring function  to evaluate the binding affinity of each candidate to identify the top candidates as drug leads, and then to perform lead optimization  <cit> ; it is also used for target identification  <cit> . relative ranking of large number of ligands can also be predicted using the calculated binding affinities. besides drug discovery, the bioactive molecules thus identified can be used as chemical probes to investigate the biochemical role of a target of interest  <cit> . molecular docking also has applications in many structural bioinformatics problems, such as protein structure  <cit>  and function prediction  <cit> . it has become attractive because of the ever-increasing number of available receptor protein structures and putative ligand drug candidates in publicly-accessible databases, such as the protein data bank   <cit> , pdbbind  <cit> , cambridge structural database   <cit> , and corporate repositories.

in this work, we will build scoring functions based on an ensemble of neural networks to accurately and quickly predict protein-ligand binding affinity.

related work
existing scoring functions employed in commercial and free molecular docking software fall in one of three main categories: force-field-based  <cit> , empirical  <cit> , or knowledge-based  <cit>  sfs. many comparative studies have found that these types of sfs are not accurate enough for reliable and successful molecular docking and virtual screening. a recent study examined a total of  <dig> popular scoring functions in their ability to reproduce experimental binding affinities of  <dig> protein-ligand complexes that encompass  <dig> different protein families  <cit> . although these sfs are employed in mainstream commercial and academic molecular docking tools, the best performing sf achieved only mediocre accuracy of less than  <dig>  in terms of pearson's correlation between its predictions and measured binding affinities . these findings are in agreement with an earlier work by wang et al. in which a related benchmark and scoring functions were examined  <cit> . several of the evaluated sfs were empirical models derived via fitting linear regression equations to training data, but none were based on nonlinear modeling approaches. therefore, we recently proposed random forests , boosted regression trees , support vector machines , k-nearest neighbors , and multivariate adaptive regression splines  nonlinear scoring functions and compared their ligand scoring and ranking performances against the sixteen conventional sfs considered by cheng et al. on the same benchmark test sets  <cit> . our ml sfs, especially rf and brt that are based on an ensemble of decision trees, have shown substantial improvement in binding affinity prediction accuracy over all the sixteen traditional scoring models.

artificial neural networks  have been previously used in computational drug development, but they have mostly been applied in qsar modeling problems or in predicting the biological activity of ligands  against a target protein  <cit> . their application in predicting binding affinity has been very rare and only reported in small scale experiments in which just a handful of protein-ligand complexes were used for training and validation  <cit> . neural networks' poor generalization performance for higher dimensional data is perhaps the main reason for their limited use in scoring protein-ligand complexes in commercial docking tools. in this work, we propose novel sfs based on an ensemble of neural networks to predict binding affinity of protein-ligand complexes characterized using large and diverse number of descriptors. we train and test our models on hundreds of high-quality protein-ligand complexes and compare their accuracies against conventional and state-of-the-art scoring functions. we show that our nn sfs are resilient to overfitting and generalize well even when predicting bas of complexes characterized by a large number of features.

key contributions
conventional empirical sfs rest on the hypothesis that a linear regression function is sufficiently capable of modeling protein-ligand binding affinity  <cit> . instead of assuming a predetermined theoretical function that models the unknown relationship between different energetic terms and binding affinity, an accurate nonparametric machine-learning method inspired from statistical learning theory is introduced in this work. we utilize a variety of features to build sfs bgn-score and bsn-score by combining a large number of diverse neural networks using bagging and boosting ensemble techniques, respectively. we show that bgn-score and bsn-score have scoring powers of  <dig>  and  <dig>  , respectively, compared to  <dig>  for the best conventional sf for a benchmark test set--this is a significant improvement in predictive power. in addition to this substantial 25% improvement, these ensemble nn sfs are also at least 19%  more accurate than sfs based on a single neural network. we also compare our proposed models to sfs based on random forests  <cit> . we found that our ensemble nn sfs surpass rf sfs --the rf sfs we compare against are better than the rf sfs presented in the past in  <cit>  because of the use of greater variety and number of features. although nn and rf ensemble approaches are competitive with each other, the significance of nn ensemble sfs introduced in this work is two-fold. first, they represent a way to overcome the overfitting limitations of single neural network models that have been used traditionally in drug-discovery applications  <cit> . second, neural networks have the ability to approximate any underlying function smoothly  <cit>  in contrast to decision trees that model functions with step changes across decision boundaries  <cit> .

we seek to advance structure-based drug design by designing sfs that significantly improve upon the protein-ligand binding affinity prediction accuracy of conventional sfs. our approach is to couple the modeling power of ensemble learning algorithms with training datasets comprising hundreds of protein-ligand complexes with known high-resolution 3d crystal structures and experimentally-determined binding affinities and a variety of features characterizing the complexes. we will compare the predictive accuracies of bgn-score, bsn-score, single nn sf, rf sf, and existing conventional sfs of all three types, force-field, empirical, and knowledge-based, on diverse and homogeneous sets of protein families.

the remainder of the paper is organized as follows. the next section describes the protein-ligand complex database used for the comparative assessment of sfs, the physicochemical features extracted to characterize the complexes, the training and test datasets used, and the proposed and conventional sfs that we study. next, we present results comparing the scoring powers of various sfs on diverse and homogeneous test sets of protein families. finally, we summarize these results and conclude our work.

materials and methods
protein-ligand complex database
we used the same complex database that cheng et al. used as a benchmark in their recent comparative assessment of sixteen popular sfs  <cit> . they obtained a refined set containing high-quality 3d structures of  <dig> protein-ligand complexes from the  <dig> version of pdbbind  <cit> . from this set, the curators of pdbbind built a test set that encompasses  <dig> different protein families, each of which binds to three different ligands to form a set of  <dig> unique protein-ligand complexes. this is called the core set and is mainly intended to be used for benchmarking docking and scoring systems. in order to be consistent with the comparative framework used to assess sfs in  <cit> , we too consider the  <dig> version of pdbbind. we use the core set as a test set in this work and denote it by cr. a primary training set, denoted by pr, was formed by removing all cr complexes from the total  <dig> complexes in the refined set of pdbbind. as a result, pr contains  <dig> complexes that are completely disjoint from cr complexes.

protein-ligand complex characterization
for each protein-ligand complex, we extracted physicochemical features used in the empirical sfs x-score  <cit>   and affiscore  <cit>   and calculated by gold  <cit>  , and geometrical features used in the ml sf rf-score  <cit>  . the software packages that calculate x-score, affiscore , and rf-score features were available to us in an open-source form from their authors and a full list of these features is provided in the appendix of  <cit> . the gold docking suite provides a utility that calculates a set of general descriptors for both molecules as separate entities and in a complex form. the full set of these features can be easily accessed and calculated via the descriptors menu in gold. by considering all fifteen combinations of these four types of features , we generated  <dig> versions of the pr and cr data sets, which we distinguish by using apropriate subscripts identifying the features used. for instance, prxr denotes the version of pr comprising the set of features x ∪ r  and experimentally-determined binding affinity data for complexes in the pr dataset.

artificial neural networks
computational methodologies inspired by networks of biological neurons, artificial neural networks , are employed in this work. neural networks  have been applied in several drug design applications for both regression and classification problems  <cit> . our ensemble approaches are based on feed-forward-back-propagation  neural networks implemented in the r language package nnet  <cit> . neural networks we fit using nnet are composed of an input layer that contains neurons corresponding to features extracted for complexes, an arbitrary number of neurons  in the hidden layer, and an output neuron for the output layer. these neurons are interconnected via weighted links as shown in figure  <dig>  the outputs of the input neurons are directed to all the neurons in the hidden layer. the outputs of the hidden layer neurons are also directed forward to the output neuron. the output of a network is calculated at its output neuron according to the formula:

  y^=f=o))), 

where xp∈ℜ|p| is a feature vector representing a protein-ligand complex characterized by a feature set p, f is the function that maps it to binding affinity y^, o is the activation function of the output neuron  = u), h +  <dig> is the total number of hidden neurons, s is the activation function for the hidden-layer neurons  = eu/), wh,o refers to the weights associated with the links connecting the hidden to the output layer, wi,h represents the weights of input-to-hidden layer links, and xi is the ith feature characterizing the protein-ligand complex. it should be noted that the weight variables w <dig> h in the wi,h set of weights serve as bias parameters and they are associated with an internal input variable x <dig> whose value is always fixed at one . we similarly followed the same approach to absorb the bias parameter w <dig> o into the hidden layer set of weights wh,o by making the sigmoid function in equation  <dig> output the value one  = 1) when h =  <dig>  this topology is shown in figure  <dig> where the value  <dig> is fed directly to the top neurons of the network's input and hidden layers. the network weights wi,h and wh,o are optimized such that they minimize the fitting criterion e defined as:

  e= ∑n=1n2+λ∑∀i,jwi,j <dig>  

where n is the number of protein-ligand complexes in the training data, yn and y^n are the measured and predicted binding affinities of the nth complex, respectively, and λ is a regularization parameter. the parameter λ is also known as the weight decay and it guards against weights converging to large values. introducing the weight decay parameter avoids the scenario of saturation at the output of the hidden-layer neurons. we scaled the input features to the range  <cit>  to effectively optimize the weights when regularization is considered. the accuracy of the network is maximized by performing thousands of randomized training rounds  while imposing the regularization constraint on the weights.

limitations of ann models and our approach to tackling them
although multi-layer ann models can theoretically approximate any nonlinear continuous function, their application in drug-discovery related problems has always been complicated by several challenges. bioinformatics and cheminformatics data are typically high-dimensional. since ann models cannot handle large number of features efficiently, a pre-processing step prior to fitting the data using an ann model is usually necessary. feature subset selection using evolutionary algorithms or dimension reduction using, say, principal component analysis , is commonly applied to overcome this problem. however, valuable experimental information may be discarded when only a small subset of features is selected to build a prediction model. the dimensionality-reduction approach is also complicated by the fact that the underlying data distribution is unknown and hence making the right choice of which dimensionality-reduction technique to apply is a tricky problem in itself. in addition to these pre-processing issues, training ann models is also a challenging task because their weights can not be guaranteed to converge to optimal values. this causes nn models to suffer from high variance errors which translate to unreliable sfs.

the aforementioned problems can be avoided and state-of-the-art performance can be achieved by combining predictions of hundreds of diverse and nonlinear nn models. we propose here ensemble methods based on anns. the ensemble itself is trained on all the features, although each network in the ensemble is fitted to only a subset of the features. this approach relieves us from carrying out feature subset selection or dimensionality reduction prior to training. in fact, the performance of the ensemble can even be improved by describing the data with more relevant features. moreover, it is no longer critical to tune the weights of each network in the ensemble to optimal values as it is the case for a single nn model. suboptimal weight tuning of individual networks could contribute to decreasing the inter-correlation between them, which translates to a diverse ensemble and therefore a more accurate model  <cit> .

our proposed nn ensemble models are inspired from the random forests  <cit>  and boosted regression trees  <cit>  techniques in the formation of the ensembles. so far, the focus in ensemble learning has been more or less biased toward using decision trees as base learners in forming ensembles. choosing trees as base learners is mainly due to their high flexibility and variance . high variance decreases inter-correlation between trees and therefore increases the overall ensemble model's accuracy. instead of using decision trees as base learners, we employ here multi-layered perceptron  anns. ann shares several characteristics with prediction trees. they are nonparametric, nonlinear, and have high variance. moreover, both techniques are very fast in prediction. anns such as mlp, however, have the ability of modeling any arbitrary boundary smoothly while decision trees can only learn rectangular-shaped boundaries. decision trees are typically pruned after training to avoid overfitting, whereas ann uses regularization while the network weights are optimized during learning. we next describe our two new ensemble nn models.

bgn-score: ensemble neural networks through bagging
bootstrap aggregation, or bagging for short, is a popular approach to construct an ensemble learning model. as the name implies and as indicated in the third step of algorithm  <dig>  the ensemble is composed of neural networks that are fitted to bootstrap samples from the training data. to further increase the diversity of the ensemble and decrease its training time, the inputs to each network l are a random subset  of the total p features extracted for every protein-ligand complex . feature sampling has proven effective in building tree-based ensemble algorithms such as random forests  <cit> . when the task is to predict the binding affinity of a new protein-ligand complex, the output is the aggregated average of the predictions of the comprising individual networks as shown in algorithm  <dig> and depicted in figure  <dig>  this mechanism can be formally expressed as:

  y^=f=1l ∑l=1lfl=1l ∑l=1ly^l, 

where xp∈ℜ|p| is a feature vector representing a protein-ligand complex characterized by a feature set p, f is the function that maps it to binding affinity y^∈ℜ, xpl∈ℜ|pl| is the same complex but characterized by a random subset pl of features , l is the number of networks in the ensemble, and y^l is the prediction of each network l in the ensemble which is calculated at the output neuron according to equation  <dig>  the final bagging-based ensemble sf is referred to as bgn-score.

bsn-score: ensemble neural networks through boosting
boosting is an ensemble machine-learning technique based on a stage-wise fitting of base learners. the technique attempts to minimize the overall loss by boosting the complexes having highest predicted errors, i.e., by fitting nns to  residuals made by previous networks in the ensemble model. there are several different implementations of the boosting concept in the literature. the differences mainly arise from the employed loss functions and treatment of most erroneous predictions. our proposed nn boosting algorithm in this work is a modified version of the boosting strategy developed by cao et al.  <cit>  and friedman  <cit>  in that we perform random feature subset sampling. this approach builds a stage-wise model as listed in algorithm  <dig> and shown in figure  <dig>  the algorithm starts by fitting the first network to all training complexes. a small fraction  of the first network's predictions is used to calculate the first iteration of residuals y1res as shown in step  <dig> of algorithm  <dig>  step  <dig> also shows that the network f <dig> is the first term in the boosting additive model. in each subsequent stage l, a network is trained on a bootstrap sample of the training complexes described by a random subset pl of features . the values of the dependent variable of the training data for the network l are the current residuals corresponding to the sampled protein ligand complexes. the residuals for a network at each stage are the differences between previous stage residuals and a small fraction of its predictions. this fraction is controlled by the shrinkage parameter ν <  <dig> to avoid any overfitting. network generation continues as long as the number of networks does not exceed a predefined limit l. each network joins the ensemble with a shrunk version of itself. in our experiments, we fixed the shrinkage parameter to  <dig>  which gave the lowest out-of-sample error. we refer to this boosting-based ensemble sf as bsn-score.

neural networks and random forests scoring functions
in order to investigate the effectiveness of ensemble nn sfs in comparison to traditional nn models and ensemble decision-tree models, we trained and tested bgn-score, bsn-score, a single neural network sf referred to as snn-score, and a random forests  sf on the pr and cr datasets, respectively, characterized by all fifteen combinations of the x, a, r, and g features discussed above. for a fair comparison of their potential, the parameters of these sfs were tuned in a consistent manner to optimize the mean-squared prediction errors on validation complexes sampled without replacement from the training set and independent of the test data. out-of-bag instances were used as validation complexes for bgn-score and rf, while a ten-fold cross-validation was conducted for bsn-score and snn-score sfs. out-of-bag  refers to complexes that are not sampled from the training set when bootstrap sets are drawn to fit individual nns in bgn-score models or decision trees in rf--on average, about 34% of the training set complexes are left out  when bootstrap sets are drawn. the parameters that are tuned and their optimized values are as follows.  l: the number of base learners  was set to  <dig>   |p|: the size of the feature subset p randomly selected from the overall set of features p while constructing each neural network in ensemble nn sfs or the size of the randomly-selected feature subset used at each node of a decision tree to perform a binary split on the "best" feature in rf sf. this was set to  <dig> for ensemble sfs, except in the case where ensemble sfs are fitted to the  <dig> x-score features when it was set to  <dig>  the number of input neurons for snn-score is set to one more than the number of features used to characterize training and test complexes. all nn sfs have one output neuron per network that produces the binding affinity score.  h + 1: the number of hidden-layer neurons in nn sfs was set to  <dig>   a total of  <dig> training epochs and a decay value  of  <dig>  were used to optimize the weights for each network in the ensemble and single nn sfs. the training process of a network is terminated earlier if the fitting criterion defined in equation  <dig> falls below  <dig>  before the maximum number of training epochs is completed. this threshold is the default value for the abstol parameter in the nnet package that we use.  ν: the shrinkage parameter for bsn-score models was set to  <dig> .  the weights of each network were randomly initialized in the range . the bounds of this uniform distribution are the default values for the rang parameter in the nnet package.

algorithm  <dig> algorithm for building bgn-score: an ensemble nn sf using bagging

1: input: training data d = {xp, y}, where xp={x1p,…,xnp}, y = {y <dig> ...,yn}, and n is the number of training complexes.

2: for l =  <dig> to l do

3:   draw a bootstrap sample xlp from xp.

4:   describe the complexes in the bootstrap sample xlp using a random subset pl of features: xlpl.

5:   from y, draw the measured binding affinities of the complexes in the sample xlp : yl.

6:   construct a new training set:dl={xlpl,yl}.

7:   learn the current binding affinities by training an ffbp nn model fl on dl.

8: end for

9: the final prediction of a protein-ligand complex xp is: y^=f=1l ∑l=1lfl=1l ∑l=1ly^l

algorithm  <dig> algorithm for building bsn-score: an ensemble nn sf using boosting

1: input: training data d = {xp, y}, where xp={x1p,…,xnp}, y = {y <dig> ...,yn}, and n is the number of training complexes.

2: construct {d1={xp <dig> y} from xp by selecting a random subset p <dig> of features.

3: train an ffbp nn model f <dig> on d <dig> and use it to predict bas  of the complexes xp <dig> . calculate the residuals: y1res=y-vy^ <dig> 

4: for l =  <dig> to l do

5:   draw a bootstrap sample xlp from xp.

6:   describe the complexes in the bootstrap sample xlp using a random subset pl of features: xlpl.

7:   from yl-1res, draw the residuals corresponding to the complexes in the sample xlp:yl-1res*.

8:   construct a new training set: dl={xlpl,yl-1res*}.

9:   learn the current residuals by training an ffbp nn model fl on dl.

10:   calculate the predictions y^ <dig> of the nn model fl on all xpl training complexes in the original training set d.

11:   update the residuals: y1res=yl-1res-vy^l

12: end for

13: the final prediction of a protein-ligand complex xp is: y^=f= ∑l=1lvfl= ∑l=1lvy^l

we distinguish the various nn models we built from each other using the notation nn model::tools used to calculate features. for instance, bsn-score::xa implies that the sf is a boosted ensemble neural networks model that is trained and tested on complex sets described by xa features. for brevity, for each of snn-score, bgn-score, bsn-score, and rf models, we report results only for the feature combination  that yields the best performance on the validation complexes sampled without replacement from the training data and independent of the test set.

scoring functions under comparative assessment
we compare the scoring performance of our proposed nn models to those for sixteen scoring functions used in popular molecular docking software. the scoring accuracies of these sixteen sfs were computed by cheng et al. in a recent study on the same benchmark we consider. the functions are listed in table  <dig> which includes five sfs implemented in discovery studio, five sfs in sybyl, three sfs in gold, one in glide, and two standalone sfs. nine of these sfs are empirical, four are knowledge-based, and the remaining three are based on force fields.

 <dig> sybyl's implementation of pmf

 <dig> gold's implementation of chemscore

 <dig> gold's implementation of g-score

some of the scoring functions have several options or versions, these include drugscore, ligscore, ludi, plp, and x-score. for conciseness, we only select the version that has the highest scoring accuracy on the pdbbind benchmark that was considered by cheng et al.  <cit> . our nn model selection, however, was based on the validation complexes sampled without replacement from the training data which is independent of the test set. therefore, the gap in performance between our proposed sfs and the conventional models we report in the following sections could in fact be even bigger if model/version selection of conventional sfs was done based on their performance on independent validation sets instead of the test set cr.

RESULTS
evaluation of scoring functions
scoring power of sfs quantifies their ability to accurately predict protein-ligand binding affinity or reproduce it for complexes with known experimental ba data. the similarity between the predicted and measured bas are calculated using pearson's  and spearman's  correlation coefficients, the standard deviation  of errors, and the root-mean square-error . pearson's correlation coefficient measures the linear relationship between two variables as follows:

 rp=∑i=1n∑i=1n2∑i=1n <dig>  

where n is the number of complexes and y^i and yi are the predicted and measured binding affinities of the ith complex, respectively. the average values of the predicted and experimentally measured affinities for all complexes are y^¯ and y¯, respectively. spearman's correlation coefficient is used to evaluate the correlation between the predicted and measured bas in terms of their ranks and it is defined as follows:

 rs=1−6∑i=1ndi2n, 

where di is the difference in ranks of the predicted and measured affinities of the ith complex.

the sf that achieves the highest correlation coefficient  for some dataset is considered more accurate than its counterparts that realize smaller rp and/or rs values . another measure of scoring power we report here is the standard deviation  of errors between predicted and measured bas . to calculate this statistic for a given sf, a linear model that correlates predicted scores y^ to the measured ones y is first evaluated: y=β0+β1y^, where β <dig> and β <dig> are the intercept and the slope of the model, respectively. the sd statistic can then be computed as follows  <cit> :

 sd=∑i=1n)2n− <dig>  

the root-mean square-error  of the predicted scores is calculated as:

 rmse=∑i=1n2n. 

sfs that yield smaller sd and rmse values usually realize higher rp and rs values, and therefore have higher scoring power than models with large sd and rmse statistics.

ensemble neural networks vs. other approaches on a diverse test set
we trained three neural network sfs  and an rf scoring model on the primary training set pr and evaluated their scoring performance on an independent test set of  <dig> diverse protein-ligand complexes from  <dig> different protein families.

n
1
rp
2
rs
3
sd
4
 <dig> number of complexes in cr with positive  binding scores using this sf  <cit> .

 <dig> rp is the pearson correlation coefficient between predicted and measured ba values of complexes in cr.

 <dig> rs is the spearman correlation coefficient between predicted and measured ba values of complexes in cr.

 <dig> sd is the standard deviation of errors between predicted and measured ba values of complexes in cr based on equation  <dig> in  <cit> .

 <dig> rmse is the root-mean-square of errors between predicted and measured ba values of the test complexes in cr. test rmse is not available for conventional sfs except for x-score::hmscore that we have re-constructed.

 <dig> rmse is the root-mean-square of errors between predicted and measured ba values of out-of-sample complexes in the training set pr. training rmse is not available for conventional sfs except for x-score::hmscore that we have re-constructed.

there are two main reasons for the superior performance of ensemble sfs. first, more numerous and varied features more fully characterize protein-ligand interactions. thus we find that bsn-score and bgn-score sfs employing all four features types considered  are more accurate than the same sfs employing fewer features. second, and more important, the learning model of ensemble sfs is nonlinear and flexible and can exploit a large number of features while being resilient to overfitting. thus we find that snn-score::x  is more accurate compared to the versions of snn-score employing one of a, r, or g features only as well as snn-score::xarg  because single neural network models overfit the training complexes when characterized by a large number of features. we attempted to decrease the effect of overfitting by conducting feature reduction using pca which helped increase the performance of snn-score::xarg to rp =  <dig> . however, the predictions of snn-score::xarg are still substantially less accurate than those of bsn-score::xarg and bgn-score::xarg even though the first  <dig> principle components we used to calculate the  <dig> new features explain more than  <dig>  of the total variance in the raw xarg features. further, the significance of the ensemble modeling approach can be gauged from the fact that even with a single type of feature, bsn-score::a and bgn-score::a yield accuracies of rp =  <dig>  and  <dig> , respectively, which are within ∼ 4% of the accuracies of bsn-score::xarg and bgn-score::xarg.

ensemble neural networks vs. other approaches on homogeneous test sets
it has been observed that around 92% of existing drug targets are similar to proteins already present in the protein data bank, which is the primary source of our training and validation complexes  <cit> . based on this finding and the similar overlap relationship between training and test set proteins in the previous experiment, we believe that the scoring performance of the sfs listed in table  <dig> should be expected in typical molecular docking and virtual screening campaigns. for each protein family in that experiment's test set, there is at least one protein family in the training set of our proposed nn and rf sfs, but the two sets share no protein-ligand complexes when these pairs of compounds are considered as whole biological units. we describe here a more stringent experiment to assess the generalization of the nn and rf sfs when they are applied to score ligands for novel drug targets. in this experiment, we evaluate the ba predictive accuracy of the nn sfs on four protein families not present in their training set. these protein families are the most frequent in our data and include  <dig> hiv protease,  <dig> trypsin,  <dig> carbonic anhydrase, and  <dig> thrombin complexes. a test set for each of these protein families was constructed by sampling all complexes formed by that protein from the training  and the test  sets. the training complexes corresponding to each of these four test sets are the remaining protein-ligand pairs in pr. for each protein family, we fitted the proposed nn and rf models to the corresponding independent training complexes and validated them on the test set complexes that are formed between that type of protein and a unique set of co-crystallized ligands. the prediction accuracy of our proposed models and the top four conventional scoring functions on complexes formed by the four protein types are shown in table  <dig> 

rp
1
rs
2
sd
3
rp
1
rs
2
sd
3
rp
1
rs
2
sd
3
rp
1
rs
2
sd
3
 <dig> rp is the pearson correlation coefficient between predicted and measured ba values of complexes in this protein-family-specific test set.

 <dig> rs is the spearman correlation coefficient between predicted and measured ba values of complexes in this protein-family-specific test set.

 <dig> sd is the standard deviation of errors between predicted and measured ba values of complexes in this protein-family-specific test set.

 <dig> rmse is the root-mean-square of errors between predicted and measured ba values of the test complexes in in this protein-family-specific test set. test rmse is not available for conventional sfs except for x-score that we have re-constructed. training rmse is not reported in this table because the values are very similar to rmsetrain in table  <dig> due to the overlap between the training data sets of the two experiments.

 <dig> this indicates whether the test set complexes are disjoint from  or overlap with  the training set complexes for nn and rf models. any overlap between the training and test data of the conventional sfs is unknown  to us.

examining the upper portion of the table for the four families where the test and training sets are disjoint for the nn and rf sfs, we notice that the predictive accuracy of all sfs varies from poor to good depending on the protein family. all sfs have failed to reproduce the experimental binding affinities for the ligands that bind to hiv protease proteins. the highest pearson's correlation value between predicted and true bas is less than  <dig> , which is the case for the scoring function x-score::hpscore. improper characterization of enthalpic and entropic forces for hiv protease complexes could be the main reason for these erroneous predictions  <cit> . the significant conformational changes observed during binding as well as the lack of similar proteins in the training set could also result in such inaccurate estimations for ba. the scoring accuracy on the other three protein families is substantially better. the binding affinities for ligands bound to trypsin were predicted with an accuracy of at least rp =  <dig> . discovery studio's empirical sf plp <dig> shows the highest accuracy on the carbonic anhydrase dataset with a linear correlation value of  <dig> . the most accurate models on the thrombin test set are the nn sfs and rf with rp values of  <dig>  and better, followed by the conventional scoring functions.

it can be observed that the sf based on a single nn, snn-score, performs relatively poorly overall, except in one case. in some of these test sets, a few conventional sfs perform better than the ensemble nn sfs. this behavior can be attributed to the possibility of some overlap between the training complexes of the conventional approaches and the four family-specific test sets. as discussed earlier, the protein families of training and test complexes for the nn and rf models do not overlap and they are completely disjoint. when we retrain ensemble nn and rf sfs on the original training set , which overlaps with the family-specific test sets, and assess their scoring power on the four homogeneous test sets, we notice that the predictions of the proposed sfs and rf are near perfect as listed in the lower portion of table  <dig> 

the results listed in tables  <dig> and  <dig> show the performance of the proposed and conventional sfs on target proteins when they are partially or fully encountered in their training sets, or completely novel for them. therefore, we believe that these results are very useful in estimating the accuracy of our scoring models given the number of solved structures of the drug target with other ligands and the availability of their binding data.

CONCLUSIONS
our experiments have shown that the proposed neural networks sfs, bsn-score and bgn-score, achieved the best results in reproducing experimental binding affinity for large and diverse number of protein-ligand complexes. we further found that ensemble models based on nns surpass sfs based on the decision-tree ensemble technique random forests. sfs that were trained on a single neural network, which have traditionally been used in drug-discovery applications, showed linear correlation  of only  <dig>  between observed and predicted binding affinities. on the other hand, bsn-score and bgn-score along with rf-score far outperform the best of existing conventional knowledge-based, force-field-based, and empirical sfs  and those based on a single neural network. the accuracies of ensemble nn sfs are even higher when they predict binding affinities for protein-ligand complexes that are related to their training sets. the high predictive accuracy of ensemble sfs bsn-score and bgn-score is due to the following three factors:  the low bias error of the highly-nonlinear neural network base learners,  the low variance error achieved using bagging and boosting, and  the employed diverse set of features we extract for protein-ligand complexes. we aim to improve the scoring powers of bsn-score and bgn-score even further in the future. we will periodically update the training data to include larger number of complexes with more protein families and ligands. we will analyze the effect of including more training complexes on the gain in predictive accuracy of nn sfs. we will also systemically examine their improvement patterns upon scoring ligands of specific protein families when complexes formed by those families have varying degrees of presence in the training data. furthermore, we will develop new tools to extract a diverse and large number of physiochemical descriptors about the protein, the ligand, and the complex as a whole. we believe the suggested enhancement approaches will make the nn sfs even more useful for accurate molecular docking and scoring.

competing interests
the authors declare that they have no competing interests.

authors' contributions
devised the comparison techniques and experiments: n.m. and h.a. implemented the techniques and carried out the experiments: h.a. analyzed the results: n.m. and h.a. wrote the paper and revised it: h.a. and n.m.

