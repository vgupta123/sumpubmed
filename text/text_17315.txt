BACKGROUND
ordinary differential equation  based kinetic models are able to capture all of the available kinetic information regarding a biological system. therefore, they are used extensively in systems biology especially for the purpose of predicting time dependent profiles and steady state levels of biochemical species in conditions where experimental data is not available. examples from the literature show that there is a common path taken by the modeling community for the construction and the analysis of ode based systems biology models. the first step is to define the model structure and the associated kinetics. due to serious concerns about the validity of model structures and kinetics, many studies include the parallel development and analysis of multiple alternative model structures . the second step is the estimation of the unknown model parameters by fitting the model to the data using global and local minimization algorithms. data here are usually in vivo time series concentration data of the observable biochemical species included in the model. at this step, uncertainty in the estimated values of the model parameters can also be quantified by constructing confidence intervals . last but not least, models are assessed for the quality of their fit to the data and for their predictive power on independent data. independent data are datasets that were not used for parameter estimation. selection between alternative model structures can also be performed at this step. a complete modeling cycle includes all these steps to achieve sufficiently good models  <cit> .

a good model has to be sufficient both in explaining the data on which it was built and in predicting independent data  <cit> . the first is taken into account mostly by likelihood ratio tests which can be used to reject models based on the quality of fit to the data . the second aspect has been considered in conceptually two different ways. the first approach uses a penalized likelihood based metric such as akaike’s   <cit>  or bayesian information criterion   <cit> . this metric is calculated using the whole dataset for parameter estimation but provides an expected value of the prediction error on an independent dataset. therefore, it makes selecting the true complexity of a model possible because unnecessarily complex models are poor in predicting independent datasets. however, it is an ‘in-sample’ measure which means that the expected prediction error is valid only for the exact same experimental conditions as of the parameter estimation dataset  <cit> . predicting the kinetics of the biological system under different experimental conditions is the very purpose of kinetic models, though. therefore, modelers would like to show that the newly built model is good in qualitative or quantitative prediction of experimental data that was collected at different experimental conditions. this strategy which uses data at different experimental conditions as validation data constitutes the second approach to assess the prediction error  <cit> .

different experimental conditions are usually based on the following scenarios:
inhibition of enzymes.

reduction of protein levels by rnai mediated suppression.

gene deletions.

over-expression of genes in gene networks.

dose-response experiments in which different doses of triggering chemicals are used to stimulate the system.



these validation scenarios are popularly applied since the common goal of the modelers is to demonstrate the models’ competency under challenging conditions. estimating the parameters of a model in certain experimental conditions and showing their competency in other conditions within these scenarios requires multiple datasets under different conditions and, therefore, it is an example to the hold-out validation strategy. that is, a pre-determined set of conditions are held out of the training data and used as validation data instead. however, rules about the application of this strategy are not straightforward.

there are potential pitfalls associated with the application of hold-out validation strategies in the validation and selection of kinetic systems biology models. these arise due to the lack of a satisfying answer to the question: which part of the dataset should be held out of the parameter estimation and instead should be used as the validation dataset? we carried out simulations to demonstrate the phenomena that hinder us from giving satisfactory answers to this question which can also be referred to as the problem of selecting an appropriate hold-out partitioning scheme for the data. the problem arises due to incomplete biological knowledge of the system and unknown true values of the model parameters. this makes the problem a paradoxical one since our knowledge about the system will never be complete and the true values of the model parameters are themselves what we are looking for. however, statistics literature offers an established method which is independent of this knowledge, namely cross-validation.

cross-validation  is a resampling method traditionally used for model selection, determining the optimal complexity of a model or assessment of its generalizability in statistics  <cit> . it is based on the partitioning of the data in training and test sets. the training set is used to build the model and the predictions of the model on the test set are used for model assessment. since the test set is completely independent of the parameter estimation process, selection will not be biased towards more complicated models. the efficiency of cross-validation and its difference from hold-out validation strategy lies in the fact that the partitioning is made not in a pre-determined but in a random way and the procedure is repeated multiple times so that each partition can be used as test set at least once. different variants of cv exist such as leave-one-out, k-fold and stratified k-fold cross-validation. a comprehensive evaluation and comparison of these methods can be found in  for classifier selection and in  <cit>  for the selection of regression models.

cv has been applied in different ways in the ode based modeling framework. partitions can consist of different experiments , data belonging to different biochemical species in the same experiment or different data points within the time profile of the same biochemical species. in  <cit> , the authors present an example for the latter. in this work, prediction errors on test sets obtained by using an ode based model are compared to the residuals from an unsupervised data analysis method which does not make any use of biochemical knowledge. better predictions found by using the unsupervised principal components analysis  method give hints on the low informative level of the ode model leading to a rejection of the proposed ode model structure. cv by using different species from the same or different cultures with different experimental conditions was considered by  <cit> . in that study, prediction errors were used to select not between two single models but between two families of models each constituted of models with slightly different topologies. both approaches use a k-fold stratified partitioning scheme in which time points or species were approximately equally distributed between k different partitions. the prediction errors from different test sets are averaged for the final measure of the predictive power.

existence of only very few examples like we mentioned above show that cv has been highly neglected in the field. also, the risks associated with the hold-out validation strategy have been underestimated. the conceptual differences between the two methods and the difference between their outcomes have not been presented in detail. therefore, with this paper we aim to present a detailed comparison of the hold-out and cross-validation methods by using simulations and emphasize the advantages of cv over hold-out partitioning schemes. more details on our implementation of cv are given in the methods section.

the reason for choosing simulations for our demonstrative purposes is that simulations and synthetic data allows us to know the ground truth, in this case the true model parameters and the true model structure. therefore, we can analyze the results we achieved in different partitioning schemes in a comparative manner. we mainly look at the effect of different partitioning schemes in the outcome of model validation and selection. however, we report also results related to its effect on parameter estimation which is very influential on validation and selection in order to present a complete explanation.

methods
simulated data
we used the high osmolarity glycerol pathway model in s.cerevisiae which was presented as the best approximating model in  <cit>   to generate synthetic data. the model is available in biomodels database  <cit>  with the accession number model <dig>  the readers are referred to the original paper for the details of the model structure.
fig.  <dig> the pathway topology proposed in  <cit> . we used this model as our true model and generated data based upon it. the black lines with small arrow tips depict the transition between different species in the model like production, degradation or complex formation. the black lines with open circle tips depict the phosphorylation process by kinases. the lines with open triangle tips show activating regulatory interactions whereas lines with blunt ends show deactivating regulatory interactions. the red colored double arrow denotes the post translational regulation of glycerol production by the active phosphorylated hog <dig> protein. we did not consider this regulatory interaction in our simplified model. the dotted ellipses in the upper left hand corner indicate the two different upstream activation routes important in our study. parts of the pathway whose parameters were affected by the choice of the partitioning scheme were highlighted yellow and gray. we explained the changes in the parameters of those regions in our results section. 



the pathway can be triggered by using an nacl shock and is activated via two parallel upstream signaling routes. the activity of the upstream routes is encoded by a binary input parameter which indicates that the route is either active or not. the level of the nacl shock is also an input parameter which can be manipulated. therefore, the model can be used also for deletion mutants where only one of the signaling routes is active, following different doses of nacl shock, by changing only those two input parameters. it includes additional  <dig> free parameters which can be estimated from data.

we mimicked the real experimental conditions used in  <cit>  when generating the data. these include different cell types and different nacl doses. the different cell types were deletion mutants in which only the signaling branch through sln <dig> activation or sho <dig> activation was active and the wild type cell in which both branches were active . the different nacl shock levels ranged between  <dig>  and  <dig>  m . the data consisted mainly of the ratio of the active phosphorylated hog <dig> protein to the maximum hog <dig> protein level observed in the wild type cell which was expressed as a percentage. the hog <dig> protein phosphorylation percentage data  from  <dig> cell types and  <dig> doses formed  <dig> different subsets of hog1pp data. we used different subsets for parameter estimation and model validation/selection each time within different data partitioning schemes which we explain in detail in the following section. concentration data of other species in the model were essential for the estimation of the parameters downstream from the hog <dig> protein. for this reason, measurements of mrna, protein and glycerol levels at  <dig>  m. nacl shock were always a part of the training dataset. therefore, the terms ’validation data’ and ’training data’ refer only to hog1pp data, throughout the text.
fig.  <dig> experimental conditions under which the data was generated. check marks indicate the measurements that were performed. each row shows a different dose in a different cell type whereas columns are for different biochemical species measured. hog1pp data consists of  <dig> subsets  and is the main subject of variability between different partitioning schemes that we evaluated



we generated  <dig> different realizations of synthetic data by adding error to the time profiles obtained by the model. we added heterogeneous noise where the noise term for each concentration value was drawn from a normal distribution with a standard deviation equal to  <dig> % of the concentration value itself which reflects realistic noise levels and structure for these type of experiments. the time series data contained  <dig> time points during a course of  <dig> minutes.

data partitioning schemes
hold-out partitioning schemes
in this work, we evaluate the performance of the hold-out validation partitioning schemes based on two most popularly applied challenge scenarios: gene deletions and dose-response experiments. the first scenario in our study mimics a gene deletion challenge. in each scheme of this scenario , the training set is composed of all six doses of a single cell type. all six doses of the other two cell types can be used as validation data. the outcomes of model validation and selection are determined based on each of these twelve different subsets of validation data, separately. the schemes are named throughout the manuscript as sln <dig>  sho <dig> and wt schemes depending on the cell type used for training.
fig.  <dig> scenario  <dig> partitioning schemes. light gray colored boxes show parts of the data which we used as the training set  for parameter estimation. dark gray colored boxes show parts which we used as validation sets . different background colors represent different partitioning schemes and are consistent with the colors used in the graphs in the results and discussion section. each partitioning scheme offered the use of six subsets of the hog1pp data as the training set and the remaining twelve subsets of the hog1pp data could be used for validation, separately. the training set included either  sln <dig> data,  sho <dig> data or  wt data



our second scenario mimics the dose-response strategy. in this scenario, the training set is composed of one dose from each cell type. in the lowest dose scheme, only the data following a  <dig>  m. nacl shock are used for training . in the highest dose scheme, only the data following a  <dig>  m. nacl shock are used for training. the remaining five doses from each cell type can be used as validation data. similar to the first scenario, the outcomes of model validation and selection are determined based on each of these fifteen subsets of validation data, separately.
fig.  <dig> scenario  <dig> partitioning schemes. light gray colored boxes show parts of the data which we used as the training set  for parameter estimation. dark gray colored boxes show parts which we used as validation sets . different background colors represent different partitioning schemes and are consistent with the colors used in the graphs in the results and discussion section. each partitioning scheme offered the use of three subsets of the hog1pp data as the training set. these are the lowest dose subset of each cell type in the lowest dose scheme and the highest of each in the highest dose scheme. the remaining fifteen subsets of the hog1pp data could be used for validation, separately. these are the lowest dose subset of each cell type in  the lowest dose scheme and the highest of each in  the highest dose scheme



lastly, we introduce variation in the training sets. we update our first scenario in such a way that in each partitioning scheme , we use data from two cell types for training. all six doses from the remaining cell type can be used as validation data. we make consensus decisions on model validation and selection considering all the validation subsets. the schemes are named throughout the manuscript as sln1/sho <dig>  sln1/wt and sho1/wt schemes depending on the pair of training cell types. we update our second scenario in such a way that in each partitioning scheme , we use either data from the four highest or four lowest doses from each cell type for training. the remaining two doses from each cell type can be used as validation data. similar to the first updated scenario we make consensus decisions using all validation subsets at once. the schemes are named as low doses and high doses schemes based on the doses used in the training set. this way, we can obtain five schemes  each of which uses twelve subsets of the phosphorylated hog <dig>  data for training and the remaining six subsets for validation. therefore, these five schemes can be compared to the stratified cross-validation scheme which also makes use of twelve subsets of hog1pp data in each training set.
fig.  <dig> partitioning schemes used in the adapted scenarios. light gray colored boxes show parts of the data which we used as the training set  for parameter estimation. dark gray colored boxes show parts which we used as the validation set . different background colors represent different partitioning schemes and are consistent with the colors used in the graphs in the results and discussion section. each partitioning scheme offers the use of twelve subsets of the hog1pp data as the training set and the remaining six subsets as the validation set. the training set included either  sln <dig> & sho <dig> data,  sln <dig> & wt data,  sho <dig> & wt data,  the lowest four doses from each cell type or  the highest four doses from each cell type



stratified random cross-validation scheme
in a random cross-validation scheme, there are no pre-defined partitions, unlike the hold-out partitioning schemes. here, we implement stratified random cross-validation which is a specific type of cross-validation in which the training sets can be forced to follow a certain structure. we randomly partition the data into training and validation sets, in three different runs. in each run, we force the training sets to include the same amount of data from each cell type and dose level. we estimate the parameters and also calculate the measures we that we use for the analysis of the simulations , at each run. later, we make consensus decisions using the average of these measures that were evaluated at each run. the different partitioning schemes applied in each run can be seen in fig.  <dig> 
fig.  <dig> stratified random cross-validation scheme . light gray colored boxes show parts of the data which we used as training sets  for parameter estimation. dark gray colored boxes show parts which we used as validation sets . in each of the three runs, the training and the validation sets change as indicated in these graphs



stratified cross-validation is based on the idea of providing training sets in which all classes of data are represented approximately equally. in our case, different classes of data are different cell types and different doses. this makes it a suitable scheme for our purposes, because we anticipate biased parameter estimates and hence, biased model validation and selection when training sets are dominated by certain classes of data. a stratified scheme would typically avoid such a problem and would give more robust results across different runs of cross-validation compared to other cross-validation approaches.

parameter estimation
for each partitioning scheme, we estimated the parameters of both the true model  and the simplified model . we repeated the parameter estimation process by using  <dig> different realizations of the data. the estimation of the parameters required the minimization of the difference between the data and model predictions. we carried out the minimization using the local minimizer ‘lsqnonlin’ function of matlab  <cit> . we considered a local optimizer to be sufficient since we work with generated data and could use the true values of parameters as starting points.

in the case of real data, true values of the parameters are not known. however, usually there is prior information on the ranges of the values that the parameters can take. in such a situation, uniformly distributed random starting points can be generated in these ranges and optimization with lower and upper bounds can be performed starting from the different initial points, aiming to find the same minimum in a sufficient number of runs. we took this approach for a single example noise realization. we assumed that the parameter ranges span intervals that are twice as big as the true values of the parameters. we started the optimization from  <dig> different starting points and could achieve the same minimum with the one achieved when the true values of the parameters were used as the starting point, in  <dig> % of the runs. the correlation between the parameter estimate vectors of these runs was above  <dig> . this finding confirmed that performing parameter estimation in a more realistic setting does not affect the minimum achieved if there is good prior information on the parameter ranges. therefore, we use fixed starting points  throughout the study due to its substantial advantage in computational power.

measures used for the analysis of the simulations
we analyzed five main features from the simulations, namely the amount of bias in the parameter estimates, the predictive power of the models on the validation datasets, the number of wrong decisions in which the simplified model structure was selected over the true model structure and the distance between the predicted profiles by the true and the simplified model structures .

we use normalized bias  as a measure of the bias in each estimated parameter . the median of its distribution across different noise realizations gives us the median amount of bias in each parameter estimated in a certain scheme.
  nbiji=pji−ptrueiptruei×100i=1: <dig> index for parameters in the modelj=1: <dig> index for noise realizations 

we use normalized standard deviation of parameter estimates as a measure of identifiability levels of parameters. we obtain the standard deviation of the estimates by calculating the fisher information matrix.

we quantify the lack of good predictive power of models by using percentage errors. percentage error is the percentage of the sum of squares of the prediction error to the sum of squares of validation data . model selection between the two models gives wrong results when pet>pes, meaning that the simplified model gives lower prediction error than the true model structure.
  pe=∑ii∑j15xijk−x^ijk2∑j15xijk2×100ii=1:i index for validation subsets of hog1pp dataj=1: <dig> index for time pointsi=total number of hog1pp subsets used for validation 

the difference between the true and the simplified model predictions  can be calculated by using the trapezoidal rule as in equation  <dig>  with this method, the area between two curves can be approximated as a series of trapezoids . the sum of the areas of the trapezoids provide a good approximation of the area between the curves when the number of trapezoids are sufficiently high. here, the two curves are the profiles of the hog1pp predicted by the true and the simplified model structures. we normalize the calculated area with respect to the maximum of the hog1pp data in the corresponding validation subset. large areas between the two curves mean that the separation of the two model structures is easier. therefore, when correct model selection decisions are given, model separation  can be used as an additional criteria of enhanced model selection.
  Δtsi=∑kk−1ttk+1−stk+1+ttk−stk <dig> tk+1−tkmaxxijΔts=∑iiΔtsiit: numerical values of the hog1pp predictions by thetrue model structures: numerical values of the hog1pp predictions by thesimplified model structurek=1:k- <dig> index for trapezoidsi=1:i index for validation subsets of hog1pp dataj=1: <dig> index for time pointsi=total number of validation subsets fig.  <dig> trapezoidal rule. the figure explains the trapezoidal rule visually. the green shaded area refers to the area of the th trapezoid. the total area of the trapezoids is equal to Δ
t
s
i in equation 3




RESULTS
scenario 1: partitioning of data from different cell types
firstly, we would like to stress that in all of our simulations, we observed very good fit of the true model structure to the data. additionally, our emphasis in this work is on model validation and selection using validation datasets which were excluded from the training set. therefore, we do not present detailed analysis of the quality of model fits. only in fig.  <dig> and additional file 1: figure s <dig>  we present the model fits together with the predictions in two examples. we should also mention that the term ’prediction’ always refers to predictions on validation datasets, throughout the text. finally, we present our results on both percentage error  and model separation  using a box plot representation. with this representation, each box plot shows the distribution of the associated measure across the  <dig> different noise realizations. for example, in the case of percentage error, the median of this distribution gives an idea on how high the prediction errors are in general. in addition, the box plots show also the outliers with relatively high prediction errors by the red points outside the boxes.
fig.  <dig> fit and predictions obtained on a single realization of data in the sln <dig> scheme. black and red points  refer to data points which were used for parameter estimation and validation, respectively. in this example, all doses of sln <dig> data and the data on the downstream species  were used for parameter estimation. the magenta lines show the profiles obtained  by using the true model for the parameter estimation. the orange lines belong to the profiles obtained by the simplified model structure. all concentrations are given in percentages. the top three rows are for the hog1pp data. the titles for each graph show the dose and the cell type related to the experiment in which the hog1pp data was collected. the last row of graphs give the concentration ratios for the downstream species. the associated data was collected in a single experiment with wt cells following a  <dig>  m. nacl shock



when only data from the sln <dig> branch active deletion mutant  is used for parameter estimation, validation using the sho <dig> branch active deletion mutant data  can be very misleading. this is because models trained by using sln <dig> data results in bad predictions on the sho <dig> data. on the other hand, the same models can achieve reasonable predictions on the wt data . this can be seen from the distribution of the percentage prediction errors represented by box plots for each validation set in fig. 9a and b.
fig.  <dig> percentage prediction errors  of the true model structure in scenario  <dig>  each box plot shows the distribution of pe over  <dig> different realizations of the data. the red dots indicate the outliers which lie outside approximately  <dig>  % coverage if the data is normally distributed. they indicate realizations with relatively higher pe. blue, green and black boxes refer to sln <dig>  sho <dig>  and wt schemes. each row in the figure corresponds to a single scheme. the labels on the x-axis show the specific dose and the cell type of the data on which the validation was performed. the labels indicate also the medians of the pe distribution summarized visually by the box plots. in each graph, the ten realizations with the highest pe are located above the black dashed line. the region above this line is compressed for visual ease. a pe obtained on sho <dig> validation subsets in the sln <dig> scheme. b pe obtained on wt validation subsets in the sln <dig> scheme. c pe obtained on sln <dig> validation subsets in the sho <dig> scheme. d pe obtained on wt validation subsets in the sho <dig> scheme. e pe obtained on sln <dig> validation subsets in the wt scheme. f pe obtained on sho <dig> validation subsets in the wt scheme



existence of realizations with very high prediction errors in box plots with low medians shows that extremely bad predictions can occur even when the median prediction error is not very high. examples of this can be observed also in the sho <dig> scheme shown in fig. 9c and d. indeed, the models trained by using only the sho <dig> data can lead to extremely high prediction errors both on sln <dig> and wt data. this can be seen from the existence of realizations with a percentage prediction error above  <dig> % and  <dig> %, respectively. on the other hand, models trained by using only the wt data perform well in predicting the sln <dig> data but not the sho <dig> data . however, they are still better than those obtained by the models trained by the sln <dig> data .

as a summary of the observations on the predictive power, we can say two things. firstly, models trained by using only the data from one of the deletion mutants is poor in predicting the data from the other. secondly, models trained by using the data from the wt cell can predict the data from one of the deletion mutants better than the other one. the poor predictions might easily lead to misleading decisions on model validation. true model structures might fail to be validated due to weak predictive power of some partitioning schemes. to study the reasons leading to weak predictive power we investigated the parameter estimation quality.

we measured the parameter estimation quality by using the normalized bias of each parameter. the median of this measure across all noise realizations shows how well the parameter was estimated in general in a certain scheme. in fig. 10b, we see that the parameters related to the complex formation of sho <dig> and pbs <dig> proteins and this complex’ phosphorylation, p <dig> and p <dig>  were predicted with very high bias in the sln <dig> scheme. . this means that when the sln <dig> data is used for model training, we estimate the sho <dig> branch parameters with a very high uncertainty with a median bias of  <dig> % and  <dig> %, respectively. the same reasoning is valid also for the estimation of two of the parameters related to the phosphorylation of the pbs <dig> protein, p <dig> and p <dig>  the median bias for these parameters  were found to be  <dig> % and  <dig> %, respectively . there is an interesting difference between the estimation quality of the parameters in the two different branches, though. we could decrease the bias of the sln <dig> branch parameters considerably when we used the wt data for training the model. however, the level of bias in the sho <dig> branch parameters was still relatively high in the wt scheme compared to the sho <dig> scheme. similarly, the identifiability analysis  shows that training the model on the wt data results in similar standard deviations in the sln <dig> branch parameters when compared to the standard deviations obtained in the sln <dig> scheme. however, the standard deviations of the sho <dig> branch parameters are much lower in the sho <dig> scheme compared to those obtained in the wt scheme  which suggests improved identifiability in the sho <dig> scheme. therefore, the sln <dig> data could be predicted well in the wt scheme whereas the prediction of the sho <dig> data was still problematic. as a further investigation on the system dynamics, we tuned one of the branch parameters each time within a range limited by the minimum and maximum of their estimated values. this allowed us to confirm the deteriorating effect of biased branch parameters on the predictions .
fig.  <dig> normalized bias and standard deviation of branch parameters. bar graphs show the median of the normalized bias and standard deviation of parameters across all noise realizations. only some of the branch parameters are shown in the figure. parameters p4-p5-p <dig> play a role in the sln <dig> branch and parameters p8-p9-p <dig> are in the sho <dig> branch. blue, green and black refers to the sln <dig>  sho <dig> and wt schemes respectively. a median of normalized bias in sln <dig> branch parameters. b median of normalized bias in sho <dig> branch parameters. c median of normalized standard deviation in sln <dig> branch parameters. d median of normalized standard deviation in sho <dig> branch parameters



the asymmetrical behaviour of the predictive power  stems from an underlying biological property which is the inequality of the two phosphorylation branches in the model. although the two branches  act redundantly for the ultimate goal of hog <dig> protein phosphorylation, the fluxes in each branch are not equal. as also mentioned in  <cit> , the sho <dig> branch active deletion mutant produces less output in terms of phosphorylated hog <dig> protein. this biological fact manifests itself also in the data. the wt data is characterized more by the activity in the sln <dig> activation branch rather than the sho <dig> branch. in other words, the hog1pp levels in the wt cell are affected more by the changes in the sln <dig> branch parameters than by the changes in the sho <dig> branch parameters. therefore, the wt data can substitute for the sln <dig> data for training the models. however, the cost of excluding the sho <dig> data from the training set is higher due to the asymmetry we mentioned above. the sho <dig> branch parameters are weakly identifiable when the sho <dig> data is not used for parameter estimation. this asymmetry in the information content of the data is clearly the output of the pathway machinery. this machinery is summarized into a model with a model structure and parameter values. therefore, the decisions of model validation using a hold-out strategy is dependent on the underlying biological properties  and reflections of these properties in the model parameters . the data partitioning task, hence, proves to be a difficult one since the prior knowledge about the underlying biology would never be complete.

another important observation is the nacl dose dependency of the predictive power using the sho <dig> data. the predictive power using sho <dig> data was especially lower in the lowest two doses compared to the higher doses as can be seen in fig. 9a and f .

the asymmetry in the contribution of the sln <dig> and the sho <dig> branches to the phosphorylation of the hog <dig> protein also has consequences for model selection. figure  <dig> shows the number of wrong decisions given on each validation subset in each of these three partitioning schemes. we see that in a high number of realizations, the simplified model structure was selected over the true model structure when the sho <dig> data was used for validation . on the other hand, using only the sho <dig> data for training also resulted in an increased number of wrong decisions on the sln <dig> data compared to the wt scheme .
fig.  <dig> number of wrong decisions in scenario  <dig>  bars show the number of realizations in which the simplified model gave lower residuals than the true model structure and therefore, was wrongly selected over the true model structure. blue, green and black bars refer to sln <dig>  sho <dig>  and wt schemes. each row in the figure corresponds to a single scheme. the labels on the x-axis show the specific dose and the cell type of the data on which the validation was performed. a number of wrong decisions using sho <dig> validation subsets in the sln <dig> scheme. b number of wrong decisions using wt validation subsets in the sln <dig> scheme. c number of wrong decisions using sln <dig> validation subsets in the sho <dig> scheme. d number of wrong decisions using wt validation subsets in the sho <dig> scheme. e number of wrong decisions using sln <dig> validation subsets in the wt scheme. f number of wrong decisions using sho <dig> validation subsets in the wt scheme



in this section, we focused on partitioning schemes which use the data from only one cell type for parameter estimation. our results show the importance of having a variety of different validation sets. this is because decisions of model validation and selection vary considerably depending on the experimental conditions of different validation sets due to unknown values of the underlying parameters.

scenario 2: partitioning of data in different doses
in the second scenario, where we use different doses as training sets, we see a change of predictive power on sho <dig> validation data . when the lowest dose data from all three cell types are used for training, the predictive power on the sho <dig> data decreases with increasing dose . also, when the highest dose scheme is used, the predictive power increases with increasing dose .
fig.  <dig> percentage prediction errors  of the true model structure in scenario  <dig>  each box plot shows the distribution of pe over  <dig> different realizations of the data. the red dots indicate the outliers which lie outside approximately  <dig>  % coverage if the data is normally distributed. gray and yellow boxes refer to the lowest and the highest dose schemes, respectively. each row in the figure corresponds to a single scheme. the labels on the x-axis show the specific dose and the cell type of the data on which the validation was performed. the labels indicate also the medians of the pe distribution summarized visually by the box plots. in each graph, the ten realizations with the highest pe are located above the black dashed line. the region above this line is compressed for visual ease. a pe obtained on sln <dig> validation subsets in the lowest dose scheme. b pe obtained on sho <dig> validation subsets in the lowest dose scheme. c pe obtained on wt validation subsets in the lowest dose scheme. d pe obtained on sln <dig> validation subsets in the highest dose scheme. e pe obtained on sho <dig> validation subsets in the highest dose scheme. f pe obtained on wt validation subsets in the highest dose scheme



these results showed us that predictive power becomes lower with increasing distances between the training and the validation sets, where the distance is measured in terms of the dose of the triggering chemical. this means that the risk of invalidating the true model structure increases when the validation set is too distant from the training set. however, the limits between which the model parameters stay applicable depend very much also on the cell type as we have observed. the predictive power on the sho <dig> data deteriorated more rapidly compared to the other cell types. these observations helped us to identify a serious pitfall of dose-response strategy: as long as we do not have realistic prior information on the limits for which we expect the estimated values of the model parameters to be applicable, we face the risk of invalidating a true model structure by over-challenging the model. unfortunately, determination of the limits is not possible beforehand since it depends on the underlying biological properties which will never be completely known.

when it comes to model selection, we face a different challenge. figure  <dig> shows the number of realizations in which the simplified model structure was selected over the true model structure. for example, the lowest dose scheme results in  <dig> wrong decisions whereas the highest dose scheme results in only  <dig> wrong decisions when the  <dig>  m. sln <dig> dataset is used as validation dataset as can be seen in the upper left hand side corner of fig.  <dig>  here, only the results on validation sets that can be used in both schemes are shown because our focus is on comparing the performance of two different schemes on shared validation sets. the most important observation from the figure is that the number of wrong decisions by the lowest scheme is higher on the  <dig>  m. -  <dig>  m. sln <dig> and wt data compared to the highest dose scheme. the number of wrong decisions by the lowest scheme is very high  especially on the  <dig>  m. dose which is very close to the  <dig>  m. dose where the models were trained. in addition, we see that the highest scheme gives a slightly higher number of wrong decisions compared to the lowest dose scheme on the  <dig>  m. sln <dig> and wt data. these observations suggest that model selection is problematic when the training and validation sets are too close to each other. we looked further at the model separation between the true and the simplified models  to investigate the separation between the two model structures in higher resolution.
fig.  <dig> number of wrong decisions in scenario  <dig>  bars show the number of realizations in which the simplified model gave lower residuals than the true model structure and therefore, was wrongly selected over the true model structure. gray and yellow bars refer to the lowest and the highest dose schemes. the labels on the x-axis show the specific dose and the cell type of the data on which the validation was performed. here, only the twelve validation subsets which could be used in both the lowest and the highest schemes are shown

fig.  <dig> comparison of model separation by the two schemes. each pie chart shows the percentage of correct decisions, where the model separation achieved by a certain scheme is better than the other scheme. the labels on the x-axis show the specific dose and the cell type of the data on which the validation was performed. gray and yellow colors in the charts refer to the lowest and the highest dose schemes. here, only the twelve validation subsets which could be used in both the lowest and the highest schemes are shown. for example, when the  <dig>  m. sln <dig> data was used for validation, in  <dig> % of the realizations in which a correct decision was given by both schemes, higher distance between the predictions of the true and the simplified model structures was achieved in the highest dose scheme than the lowest dose scheme



in cases where the differences between the number of wrong decisions is too low for a meaningful comparison, the model separation, Δts is more informative. figure  <dig> shows the percentage of the realizations in which one specific scheme resulted in better model separation than the other scheme. the percentages are based on the number of realizations in which a correct decision was made by using both the lowest and the highest dose schemes. for example, we know that both schemes result in a correct decision in  <dig> realizations of the sln <dig> data at  <dig>  m. nacl shock . the first pie chart in fig.  <dig> shows that in  <dig> % of these  <dig> realizations, the highest dose scheme resulted in better separation between the two model structures than the lowest dose scheme. as can be seen from this figure, model separation obtained by the highest dose scheme is higher than that obtained by the lowest dose scheme in almost all realizations of  <dig>  m. -  <dig>  m. sln <dig> and wt data. at  <dig>  m. dose, the situation is reverse and the lowest scheme provides a better separation of the two model structures, in most of the realizations of all three cell types. these findings support the observation we made from the number of wrong decisions: model selection becomes problematic with too close training and validation sets. this is mainly because the simplified model might also predict well in the close proximity of the training dose . however, it will perform worse than the true model structure as the training and validation sets become more distant from each other. however, too much distance can also pose a problem for model selection due to increased uncertainty in the predictive power. uncertainty in the predictions shows that different noise realizations can either give very good or very bad predictions. high levels of uncertainty reveals itself in the wide box plots of especially sho <dig> validation data in fig. 12b, 12e and additional file 1: figure s <dig>  showing a wide dispersion of predictive power across different noise realizations. in these regions where the uncertainty is high, it becomes more difficult to anticipate the predictive powers of the true and the simplified model structures on a single noise realization. this hampers also model selection. using sho <dig> validation data results in such a situation where uncertainty is very high at certain doses. this is why the trends in model selection that we have presented in this section cannot be observed on the sho <dig> validation data as sharply as on the other cell types.

we can understand the risks associated with high uncertainty in a hold-out strategy, if we remember that in a single real experiment we have only one realization of noise. the outcomes of both model validation and selection depend highly on the specific noise realization in the data but we have only one realization available. this means that it is highly probable that we end up in wrong decisions just due to experimental noise. therefore, we need partitioning schemes that minimize the effect of idiosyncratic noise realisations and lead to similar decisions for all of them. the stratified random cross validation scheme is promising in this sense as we will explain in the following section.

introducing variation in the training and the test data
in the previous sections, we showed the pitfalls that we might come across if we use single doses or single cell types as validation data. therefore, we stress the importance of consensus results obtained from a collection of different validation sets. in this section, we take it one step further and introduce variation of experimental conditions also in the training data. we do it in three different ways as described by the adapted scenarios and the stratified cross validation scheme in the methods section. first, we include two different cell types in the training data, namely in the sln1/sho <dig>  sln1/wt and sho1/wt schemes. second, we include four different doses from each cell type in the training data, namely in the low doses and the high doses schemes. these are examples of hold-out validation strategies just like the previous two scenarios. however, unlike those, the training and the validation sets include a variety of different cell types or doses. the third way is not an example of a hold-out strategy. it is the stratified random cross-validation  scheme about which we have given the details in the methods section. with this approach we can introduce variation in the training and validation sets in terms of both cell types and doses at the same time.

firstly, we compare the schemes in which the training set includes different cell types. the most important observation regarding these three schemes is the low predictive power in the sln1/wt scheme as can be seen in fig. 15a. this shows that when the models are trained without using the sho <dig> data, validating them on sho <dig> data is risky. on the contrary, when the sln <dig> data is missing in the training set, we do not observe such low predictive power. the reasons for this can be traced back to the asymmetrical branch structure that we explained in detail in the scenario  <dig> section. therefore, we do not discuss those here again.
fig.  <dig> percentage prediction errors  and model separation  in the adapted scenarios. each box plot shows the distribution of pe or Δts over  <dig> different realizations of the data obtained in a single scheme. the red dots indicate the outliers which lie outside approximately  <dig>  % coverage if the data is normally distributed. black, blue and green boxes in the first row of graphs refer to the sln1/sho <dig>  sln1/wt and sho1/wt schemes. cyan boxes refer to the stratified cross-validation  schemes. gray and yellow boxes in the second row refer to the low doses and high doses schemes. the labels on the x-axis indicate the medians of the pe or Δts distribution summarized visually by the box plots. the axis labels in the Δts graphs show also the number of wrong decisions given in each scheme. in each graph, ten realizations with the highest pe or Δts are located above the black dashed line. the region above this line is compressed for visual ease. a pe obtained in adapted cell type scenario. b
Δts obtained in adapted cell type scenario. c pe obtained in adapted dose scenario. d
Δts obtained in adapted dose scenario



in addition to the risks associated with model validation, the sln1/wt scheme performes poorly also in model selection with  <dig> wrong decisions. therefore, we conclude that the sln1/wt scheme is not a good scheme for model validation and selection whereas the sln1/sho <dig> and the sho1/wt are sensible partitioning schemes. the srcv scheme results in prediction errors that are comparable with the sensible partitioning schemes . furthermore, it results in no wrong decisions and it gives the highest model separation compared to the sln1/sho <dig> and sho1/wt schemes which also result in all correct decisions . in addition to this, the predictive power of such a scheme is less dependent on the noise realization compared to the other schemes as can be seen from the smaller box plots in fig. 15a. this indicates the low amount of uncertainty in the predictions.

when only doses were allowed to vary in the training set as in the case of the low doses and the high doses scheme, there was no significant difference in the predictive powers of the two schemes . this revealed that none of the schemes posed more risk of invalidating the true model structure compared to the other scheme. however, there was a large difference in the model separation achieved by the two schemes . this shows that the high doses scheme is unsuitable for model selection. a simulation showing weak model separation according to the highest dose scheme can be seen in additional file 1: figure s <dig>  the srcv scheme performed better than the unsuitable hold-out partitioning scheme for model selection  and the predictive power was in the range of the two hold-out partitioning schemes .

the observations explained above can also be anticipated from the identifiability levels. the standard deviations of parameters at all three runs of the srcv scheme were comparable to those obtained in the sensible hold-out partitioning schemes for model validation  and were never higher than those obtained in the unsuitable schemes.

these results indicated that a stratified cv scheme is favorable for both model validation and selection. in most of the comparisons, it achieves predictive power and model separation as high as the optimal hold-out partitioning scheme. in addition, it leads to lower uncertainty which means that the outcomes of model validation and selection depend less on the specific noise realization. more importantly, it never performs worse than unsuitable hold-out partitioning schemes. the importance of this last statement lies in the fact that finding a sensible hold-out partitioning scheme can never be guaranteed. it depends highly on the biology and therefore, on the model structure and the model parameters most of which are typically unknown prior to modeling. therefore, there are no rules that can be set beforehand to make the finding of sensible partitioning schemes certain. those factors might hinder us from opting for a sensible scheme. however, srcv offers a judicious and reliable partitioning scheme for which no biological knowledge is required. its good performance relies on two properties. firstly, it is iterative which means that it allows each piece of data to contribute as both training and validation datasets in an iterative manner and summarizes the results as the average of different iterations. secondly it offers random stratified partitioning, so it allows fair partitioning of the data while it prevents from certain cell types or doses dominating the training data. therefore, issues like parameter estimation and model validation/selection are not biased in a certain direction as an artifact of an underlying biological property of the system, in contrast to the hold-out validation schemes we have extensively investigated with this study. in addition, we achieve this by using a cv scheme with  <dig> folds and no repeats and hence, the computational time increases only three times compared to the hold-out schemes.

on the other hand, our additional simulations with two more complex models revealed that a prerequisite for model selection based on predictive power has to be mentioned. the first more complex model included one additional parameter  whereas the second model included three additional parameters . we have found out that the additional parameters were estimated very close to  <dig>  median of the parameters changed between  <dig> ×10− <dig> and  <dig> ×10− <dig> in all of the schemes. this means that both of the complex models boiled down to the true model structure. therefore, the differences in the prediction errors obtained with the complex and the true model structures were very small. for example, the difference in the prediction errors of the true and the complex model structure was, in average  <dig>  % of the prediction error of the true model structure obtained with the sln1/sho <dig> scheme. however, this value was  <dig> % in our simulations with the simplified model structure. at this level of extreme similarity between the model structures, model selection based on tiny differences between the predictive powers of the models leads to random conclusions that are heavily dependent on the specific noise realization in the data. instead in such situations, investigating the estimated values of the additional parameters gives clue if a more complex model is needed or not. from this we derive the following important conclusion regarding the scalability of our approach. the guidelines we present in our manuscript are aimed for more reliable decision making in model selection when the selection is made based on the predictive powers of the models. in cases where such model selection is not applicable, our guidelines are obviously not applicable either.

CONCLUSIONS
our results showed that the final decisions on model validation and selection can differ significantly when different hold-out partitioning schemes are employed. the selection of a sensible hold-out partitioning scheme that will help us to make reliable decisions depends on the biology. a good biological knowledge on the system and, hence, prior information on the structure and the true parameter values of the model are essential. unfortunately, this is not possible in many instances. this turns the problem of finding a sensible partitioning scheme for model validation and selection into a catch  <dig> problem. when the determination of a sensible partitioning scheme fails, we face the risk of invalidating true model structures or of failing to select the true model structure over the other alternatives. examples of the first situation are very difficult to find in the literature, though, because, only successful validation examples are usually presented, leading to a ‘verification bias’. furthermore, partitioning schemes that are sensible for model selection are not necessarily suitable for model validation. datasets from very similar experimental conditions have only weak model selection capability whereas datasets from very diverse experimental conditions are not appropriate for model selection either due to high uncertainty in the predictions. however, using a proper cross-validation approach such as stratified random cross-validation can help us to overcome these problems while being independent of any prior biological knowledge.

with the srcv approach, we can partition the data randomly into training and validation sets iteratively and arrive at consensus decisions by averaging over all different validation datasets. srcv performs at least as well as sensible hold-out partitioning schemes for both model validation and selection. on top of that, this comes without the risk of opting for an incorrect partitioning scheme which would lead us to biased conclusions. furthermore, the decisions given within a srcv scheme are less affected by the specific realization of the experimental noise. due to all these reasons that we mention, srcv proves to be a judicious, unbiased and promising alternative to the hold-out validation strategy for the validation and selection of ode based models.

additional file
additional file  <dig> 
supporting figures.




competing interests

the authors declare that they have no competing interests.

authors’ contributions

dh and hcjh conceived and designed the study. dh wrote the software, performed the calculations and wrote the manuscript. hcjh and aks supervised the study and helped to draft the manuscript. all authors read and approved the final manuscript.

