BACKGROUND
massively parallel sequencing has expanded the genomics era by dramatically reducing the cost and time of large-scale dna sequencing. although whole genome sequencing may soon become routine, in terms of cost, time, and labor, it is often more practical to target specific regions of interest in the genome. targeted genomic enrichment , also known as targeted sequence capture, allows efficient isolation of genomic regions prior to massively parallel sequencing  <cit> . briefly, dna libraries are hybridized with dna or rna oligonucleotides complementary to regions of interest , and these bait-library complexes are pulled out of solution after the hybridization to generate an enriched library for sequencing. this method has generally been used to target exonic and splice-site sequences of the human genome, as ~ 85% of known mutations reside in these regions  <cit> .

tge has been used to discover mutations in sets of genes associated with specific diseases  <cit>  or, in an unbiased way, by targeting the whole exome  <cit> . a trade-off exists between the number of base-pairs targeted for sequencing and the throughput of sequencing with respect to cost and time. however, in all types of tge, there is increased efficiency in uncovering disease-causing mutations when compared to whole genome sequencing.

the increase in sequencer output has well outpaced our ability to efficiently use the sequence generated. while it is clear that saturating levels of sequencing coverage are required for the lowest false positive and false negative rates in tge experiments  <cit> , it is also clear that there is a diminishing return after the threshold coverage level for variant detection is exceeded, and, in fact, over-coverage can introduce errors  <cit> . pooling samples is an attractive option to maximize sequencer output; however, due to sequencer error rate, reliable differentiation of true positives from false positives is generally difficult  <cit>  unless specialized software is employed  <cit> . to avoid this difficulty, molecular barcodes  can be ligated to sheared dna fragments prior to pooling in a process called multiplexing. because the purpose of tge is to focus on relatively small genomic regions of interest, multiplexing can be used to maximize sequencer output.

when applied to tge, multiplexing can be performed prior to capture  or after capture . the first protocols using molecular barcodes employed a post-capture approach  <cit>  and post-capture multiplexing has since become the standard method for tge. several studies, however, have shown that the pre-capture approach is also feasible and that this method offers three important advantages over post-capture multiplexing: 1) decreased cost as the capture step is generally the most costly step in the tge protocol; 2) reduced hands-on time as samples are pooled earlier in the protocol; and, 3) reduced cross-contamination risk by the earlier addition barcodes  <cit> .

however, the limits of pre-capture multiplexing have not been thoroughly tested. of two pilot studies showing the feasibility of pre-capture multiplexing one used  <dig> samples and single pools of  <dig> or  <dig> samples pooled pre-capture  <cit> , while another used  <dig> samples with single pools of either  <dig> or  <dig> samples pooled pre-capture  <cit> . both of these studies included only  <dig> pool at each pooling size and so a detailed comparison of the effects of increasing pool size and a comparison to standard post-capture pooling was not possible. in another study, pre-capture pools of  <dig>   <dig>  and  <dig> were evaluated and the authors went on to use  <dig> pools of  <dig> samples to sequence  <dig> samples  <cit> . and finally, in another study the authors pooled  <dig> samples pre-capture, however, only a single pool was used and micro-array based tge was used  <cit> . all of these studies showed the feasibility of pre-capture pooling for solution-phase tge, however we sought to study the effects of pre-capture pooling on tge in a systematic fashion in different pool sizes on a large number of samples.

in general, post-capture multiplexing is used for tge whereas pre-capture multiplexing is not. in this study, we sought to address several questions germane to multiplexing during tge experiments, including the effect of inter-sample competition for capture baits during hybridization, the impact on capture efficiency, and the downstream effects on overall sequence quality as measured by read mapping and duplicate reads. to address these questions, we compared standard post-capture multiplexing to pre-capture multiplexing using two pool sizes  to study a large set of samples . we demonstrate the significant advantages of pre-capture multiplexing in cost and time reductions while at the same time maintaining our minimum threshold for accurate variant detection.

RESULTS
we compared standard post-capture multiplexing with pre-capture multiplexing for tge of a relatively small genomic region of  <dig>  bp that comprises all known non-syndromic deafness and usher syndrome genes as described previously  <cit> . we performed tge on a set of  <dig> dna samples:  <dig> unknowns,  <dig> positive controls with  <dig> sanger-sequence-verified deafness-causing mutations, and  <dig> negative controls. the samples were captured in two ways for comparison, as shown in figure  <dig>    <dig> samples were prepared using standard post-capture multiplexing tge and run in a single illumina hiseq lane  and  pools of  <dig> or  <dig>  samples were pre-capture multiplexed with barcodes added during ligation and sequenced in groups of  <dig> in a single illumina hiseq lane . tge for post-capture samples was performed with the agilent sureselect xt version  <dig> protocol while tge for pre-capture samples was performed with the agilent sureselect xt version  <dig>  a protocol optimized for pre-capture pooling with regards to hybridization, amplification and capture wash conditions . a single sample in the post-capture  <dig> set failed sequencing qc and was excluded from further analysis.

total reads and read mapping
the total number of sequencing reads was significantly lower in pre-capture lanes as compared with the post-capture lane , however, the two pre-capture lanes were not significantly different.  <dig> %,  <dig> %, and  <dig> % of sequencing reads mapped on average for post-capture, pre-capture  <dig>  and pre-capture  <dig>  respectively . the percent of mapped reads was significantly lower for pre-capture lanes , however in all cases, greater than 90% of reads mapped, constituting a high-quality sequencing run in our experience.

capture efficiency and duplicate reads
we define capture efficiency as the percent of all mapped reads that overlap the targeted regions. in our experience using sureselect tge, capture efficiency ranges from ~40% for small target regions  to up to ~80% for large target regions  when using standard post-capture multiplexing . on average, the capture efficiency for post-capture samples was  <dig> % . this was significantly higher  than the average for both pre-capture  <dig> samples  and pre-capture  <dig> samples . the difference between pre-capture  <dig> and pre-capture  <dig> was also significantly different . the difference in average duplicate reads and average optical duplicate reads also varied significantly between all three methods  as shown in figure 2c, and was  <dig> % versus  <dig> % ,  <dig> % versus  <dig> % , and  <dig> % versus  <dig> % , respectively.

coverage performance
depth of coverage represents the number of sequencing reads aligned over a sequenced base pair. on average, all methods of multiplexing showed a 1x coverage  that was not significantly different: an average of  <dig> %,  <dig> %, and  <dig> % for post-capture, pre-capture  <dig>  and pre-capture  <dig>  respectively. 10x depth of coverage  was greater than 94% with all methods of multiplexing, but lowest for pre-capture  <dig> . the average depth of coverage for pre-capture  <dig> samples was significantly lower at 1x, 10x, and 20x than both post-capture and pre-capture  <dig> . average depth of coverage  was significantly reduced for pre-capture  <dig>  and pre-capture  <dig>  samples when compared with post-capture samples  .

we found a difference in coverage distribution among multiplexing methods . the distribution of coverage for post-capture multiplexing was most broadly distributed with increasingly sharply peaked distribution seen for pre-capture  <dig> and pre-capture  <dig> multiplexing. we identified no systematic differences in coverage of targeted regions that could not be accounted for by differences in total numbers of reads when comparing different methods of multiplexing.

in order to obtain a similar depth of coverage as post-capture samples when performing pre-capture  <dig> multiplexing, fewer pools of  <dig> can be sequenced in a single lane as a compensation for reduced capture efficiency. to test this hypothesis, we randomly sub-sampled sequencing reads from the pre-capture  <dig> lane to simulate a total of  <dig>   <dig>  and  <dig> pools of  <dig> samples  per lane . we validated this simulation technique by simulating data for  <dig> pools of  <dig> and found results were not significantly different when compared to actual data: average depth of coverage was 125x and 136x for simulated and actual data, respectively; coverage at 1x, 10x, and 20x was  <dig> %,  <dig> %, and  <dig> % for simulated data and  <dig> %,  <dig> %, and  <dig> % for actual data. our simulations for fewer number of pools per lane showed that coverage levels approached post-capture averages when reduced by a single pool: coverage for  <dig> samples in  <dig> pools of  <dig>  was  <dig> % at 1x,  <dig> % at 10x and  <dig> % at 20x . when the number of pools was further reduced, results surpassed post-capture averages.

variant detection
we used  <dig> unique dna samples in three sets of  <dig> captures each. in each set of samples,  <dig> samples were from persons with presumed genetic hearing loss of an unknown cause and the results from these samples will be reported elsewhere.  <dig> samples in each set were either positive controls carrying deafness-causing mutations verified by sanger sequencing or negative controls . the exact composition of samples per set was different between post-capture and pre-capture sets due to sample limitations, but there was a similar composition of types of control mutations in each set. the post-capture positive control samples included  <dig> mutations:  <dig> small deletions,  <dig> large deletions,  <dig> missense mutations ,  <dig> mitochondrial mutations, and  <dig> splice site mutations . the two pre-capture multiplexing lanes contained the same  <dig> positive control mutations:  <dig> small deletions,  <dig> large deletions,  <dig> missense mutations ,  <dig> mitochondrial mutations, and  <dig> splice site mutations .

the average number of variants identified within the targeted regions of interest for post-capture, pre-capture  <dig>  and pre-capture  <dig> samples was  <dig>   <dig>  and  <dig> variants, respectively. when normalized by total number of sequencing reads per sample, there was no significant difference between methods . 100% of variants were identified in all three lanes using our variant calling and annotation pipeline  with the exception of the sample from post-capture lane that failed at sequencing . no pathogenic variants were identified in the negative controls. we examined allelic balance for heterozygous positive control variants and found no significant difference between post-capture, pre-capture  <dig>  or pre-capture  <dig> samples .

we performed a systematic analysis to examine the possibility of artifacts or sequencing errors associated with molecular barcoding that may lead to erroneous sample assignment  and misdiagnosis. we searched within each lane for positive control variants present in the un-annotated  variant call format  files for any of the samples present in any other sample in that lane even at low observation rates. we did not find any evidence of sample-switching due to aberrant barcode identification in any lanes, although we did find that a single individual was an incidental carrier for a disease-causing mutation in the myo7a gene , as this variant was found in all three lanes and confirmed with sanger sequencing.

advantages of pre-capture multiplexing
pre-capture multiplexing was associated with reduced capture efficiency, which in turn reduced average depth of coverage but did not negatively impact variant detection. significant savings in cost and time were associated with pre-capture multiplexing. costs for xtv <dig> kits are 15% lower than xtv <dig>  there are other costs that are associated with library preparation including consumables which are difficult to quantify, but can be estimated as 1/16th or 1/12th of the cost of post-capture tge. as an example, each spri-bead purification during the protocol costs approximately $ <dig> /purification. using pre-capture  <dig> multiplexing, samples are pooled for three of these purifications compared to post-capture tge. for  <dig> samples, the cost simply for purification of each of these samples individually three times  is $ <dig> . <dig>  pre-capture  <dig> multiplexing would reduce the number of purifications required to  <dig> samples three times  and therefore the cost is $ <dig>  or a cost reduction of  <dig> %. the same calculation yields a cost of $ <dig>  for pre-capture  <dig> multiplexing and a corresponding cost reduction of  <dig> %. similar reductions in cost can be assumed for other reagents and consumables.

in our hands, tge requires  <dig> hours for pre-hybridization steps and  <dig> hours for post-hybridization steps. post-capture multiplexing introduces barcodes at the final amplification step and pooling is completed immediately prior to sequencing, which does not reduce hands-on time. pre-capture multiplexing occurs prior to the hybridization and therefore 12x or 16x as many samples can be hybridized and captured in the same amount of time. therefore, though difficult to quantify, hands-on time is significantly reduced with pre-capture multiplexing.

discussion and 
CONCLUSIONS
massively parallel sequencing and tge enable rapid and efficient sequencing of hundreds of thousands or millions of base pairs of the human genome simultaneously. while prices have decreased drastically, efficiency and cost-effectiveness are still important considerations, especially when large numbers of samples are analyzed. in this study we provide the first systematic comparison of post- and pre-capture multiplexing on a large sample set.

we used a large number of positive control mutations to validate this capture method, and in the process uncovered a variety of genomic variants including small and large deletions, and single nucleotide variants in control patients. we found no evidence of barcode switching or erroneous barcode assignment. in addition, the allelic balance, which is important for making heterozygous calls, was not affected by pre-capture pooling.

as expected, our data show that multiplexing samples prior to hybridization and capture reduces capture efficiency. importantly, we noted a decrease in duplicate reads, which may partially offset this loss in efficiency. we used simulations to show that reduced efficiency can be compensated for by modifying the experimental design . we hypothesize that this effect is due to competition for complementary rna baits among multiple genomes, specifically because the effect was more pronounced with the higher pool size . in addition, we found a difference in coverage distribution when comparing multiplexing methods , with the distribution of coverage became less broad with pre-capture pooling. we believe this effect also reflects competition for baits with regions most deeply covered becoming distributed amongst multiple genomes when pre-capture multiplexing is used. finally, we noted a significant reduction in total number of reads and percent of reads mapping. because only  <dig> sequencing lanes were compared, it was not possible to determine whether this effect reflected inter-lane variability or the effects of pre-capture multiplexing.

costs are an important factor in any experimental design. kit costs for pre-capture multiplexing are 15% lower. however, greater cost savings are found in the reduction in consumables and ancillary reagents used when pre-capture multiplexing is employed. in our example, we show that costs of purifications alone are reduced by ~94% for pre-capture  <dig> multiplexing and ~92% for pre-capture  <dig> multiplexing. the most significant reduction when using pre-capture multiplexing lies in reduced hands-on time. although this is difficult to quantify, 12x or 16x as many samples can be processed after pooling occurs when pre-capture multiplexing is used. therefore, due to the greatest reduction in costs and hands-on time, as well as a lack of detrimental effects on quality of tge, pre-capture  <dig> represents the most effective and efficient method for tge of an experiment with a similar target size as shown here . when designing their own experiments, investigators can estimate the optimum number of samples to pool pre-capture and sequence per lane based on the effects on capture efficiency described here.

here we show for the first time the effect of pre-capture multiplexing during tge on a large set of samples. we noted a specific effect on capture efficiency during pre-capture multiplexing and we hypothesize that this effect is due to multiple genomes competing for hybridization with complementary rna. however, pre-capture multiplexing provided significant cost-savings and time reductions, resulted in no barcode mis-identification, and could reliably identify several classes of genetic variation. in summary, pre-capture multiplexing increases the efficiency of tge and massively parallel sequencing to identify genomic variants.

