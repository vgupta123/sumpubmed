BACKGROUND
high throughput technologies, such as microarrays or single nucleotide polymorphisms  are seen as a great potential to gain new insights into cell biology, biological pathways or to assess population genetic structure. microarray technique has been mostly used to further delineate cancers subgroups or to identify candidate genes for cancer prognosis and therapeutic targeting. to this aim, various classification techniques have been applied to analyze and understand gene expression data resulting from dna microarrays . genome wide association studies using snps aim to identify genetic variants related to complex traits. thousands of snps are genotyped for a small number of phenotypes with genomic information, and clustering methods such as bayesian cluster analysis and multidimensional scaling were previously applied to infer about population structure  <cit> .

variable selection
as these high throughput data are characterized by thousands of variables  and a small number of samples , they often imply a high degree of multicollinearity, and, as a result, lead to severely ill-conditioned problems. in a supervised classification framework, one solution is to reduce the dimensionality of the data either by performing feature selection, or by introducing artificial variables that summarize most of the information. for this purpose, many approaches have been proposed in the microarray literature. commonly used statistical tests such as t- or f-tests are often sensitive to highly correlated variables, which might be neglected in the variable selection. these tests may also discard variables that would be useful to distinguish classes that are difficult to classify  <cit> . machine learning approaches, such as classification and regression trees , support vector machines  do not necessarily require variable selection for predictives purposes. however, in the case of highly dimensional data sets, the results are often difficult to interpret given the large number of variables. to circumvent this problem, several authors developed wrapper and embedded approaches for microarray data: random forests , recursive feature elimination , nearest shrunken centroids , and more recently optimal feature weighting . other approaches were also used for exploratory purposes and to give more insight into biological studies. this is the case of linear discriminant analysis , principal component analysis , partial least squares regression , to explain most of the variance/covariance structure of the data using linear combinations of the original variables. lda has often been shown to produce the best classification results. however, it has numerical limitations. in particular, for large data sets with too many correlated predictors, lda uses too many parameters that are estimated with a high variance. there is therefore a need to either regularize lda or introduce sparsity in lda to obtain a parsimonious model. another limitation of the approaches cited above is the lack of interpretability when dealing with a large number of variables.

numerous sparse versions have therefore been proposed for feature selection purpose. they adapt well known ideas in the regression context by introducing penalties in the model. for example, a l <dig> norm penalty leads to ridge regression  <cit>  to regularize non invertible singular matrices. in particular, penalties of type l <dig> norm, also called lasso  <cit> , or l <dig> norm, were also proposed to perform feature selection, as well as a combination of l <dig> and l <dig> penalties  <cit> . these penalties  were applied to the variable weight vectors in order to select the relevant variables in pca  <cit>  and more recently in canonical correlation analysis  <cit>  and in pls  <cit> .  <cit>  also proposed a penalized version of the pls for binary classification problems. recently,  <cit>  extended the spls from  <cit>  for multiclass classification problems and demonstrated that both splsda and spls with an incorporated generalized framework  improved classification accuracy compared to classical pls  <cit> .

multiclass problems
in this study, we specifically focus on multiclass classification problems. multiclass problems are commonly encountered in microarray studies, and have recently given rise to several contributions in the literature  <cit>  and more recently  <cit> . extending binary classification approaches to multiclass problems is not a trivial task. some approaches can naturally extend to multiclass problems, this is the case of cart or lda. other approaches require the decomposition of the multiclass problem into several binary problems, or the definition of multiclass objective functions. this is the case, for example, of svm one-vs.-one, one-vs.-rest or multiclass svm.

sparse pls-da
we introduce a sparse version of the pls for discrimination purposes  which is a natural extension to the spls proposed by  <cit> . although pls is principally designed for regression problems, it performs well for classification problems  <cit> . using this exploratory approach in a supervised classification context enables to check the generalization properties of the model and be assured that the selected variables can help predicting the outcome status of the patients. it is also important to check the stability of the selection, as proposed by  <cit> . we show that spls-da has very satisfying predictive performances and is well able to select informative variables. in contrary to the two-stages approach recently proposed by  <cit> , spls-da performs variable selection and classification in a one step procedure. we also give a strong focus to graphical representations to aid the interpretation of the results. we show that the computational efficiency of spls-da, combined with graphical outputs clearly give spls-da a strong advantage to the other types of one step procedure variable selection approaches in the multiclass case.

outline of the paper
we will first discuss the number of dimensions to choose in spls-da, and compare its classification performance with multivariate projection-based approaches: variants of slda  <cit> , variants of splsda and with sgpls from  <cit> ; and with five multiclass wrapper approaches  on four public multiclass microarray data sets and one public snp data set. all approaches perform internal variable selection and are compared based on their generalization performance and their computational time. we discuss the stability of the variable selection performed with spls-da and the biological relevancy of the selected genes. unlike the other projection-based sparse approaches tested, we show that spls-da proposes valuable graphical outputs, also available from our r package mixomics, to guide the interpretation of the results  <cit> .

RESULTS
in this section, we compare our proposed spls-da approach with other sparse exploratory approaches such as two sparse linear discriminant analyses  proposed by  <cit> , and three other versions of sparse pls from  <cit> . we also include in our comparisons several wrapper multiclass classification approaches. comparisons are made on four public cancer microarray data sets and on one snp data set. all these approaches perform variable selection in a supervised classification setting, i.e. we are looking for the genes/snps which can help classifying the different sample classes.

we first discuss the choice of the number of dimensions h to choose with spls-da, the classification performance obtained with the tested approaches and the computational time required for the exploratory approaches. we then perform a stability analysis with spls-da that can help tuning the number of variables to select and we illustrate some useful graphical outputs resulting from the by-products of spls-da. we finally assess the biological relevancy of the list of genes obtained on one data set.

data sets
leukemia
the 3-class leukemia version  <cit>  with  <dig> genes compares the lymphocytes b and t in all  and the aml class . the classes aml-b and aml-t are known to be biologically very similar, which adds some complexity in the data set.

srbct
the small round blue-cell tumor data of childhood  includes  <dig> different types of tumors with  <dig>   <dig>   <dig> and  <dig> microarrays per class and  <dig> genes.

brain
the brain data set compares  <dig> embryonal tumors  <cit>  with  <dig> gene expression. classes  <dig>   <dig> and  <dig> count  <dig> microarrays each, the remaining classes  <dig> and  <dig> 

gcm
the multiple tumor data set initially compared  <dig> tumors  <cit>  and  <dig> gene expressions. we used the normalized data set from  <cit>  with  <dig> types of tumor. the data set contains  <dig> samples coming from different tumor types: breast , central nervous system , colon , leukemia , lung , lymphoma , melanoma , mesotheolima , pancreas , renal  and uterus .

snp data
the snp data set considered in our study is a subsample of the data set studied by  <cit>  in the context of the human genome diversity project, which was initiated for the purpose of assessing worldwide genetic diversity in human. the original data set of  <cit>  included the genotypes at  <dig>  single-nucleotide polymorphisms  of  <dig> individuals from a worldwide sample of  <dig> populations. in order to work on a smaller sample size data set with still a large number of classes or populations  and with a high complexity classification, we chose to keep only the african populations: bantu kenya, bantu south africa, biaka pygmy, mandenka, mbuty pygmy, san and yoruba. we filtered the snps with a minor allele frequency>  <dig> . for computational reasons, in particular to run the evaluation procedures using the wrapper methods, we randomly sampled  <dig>  snps amongst the ones of the original dataset. the aim of this preliminary analysis is to show that spls-da is well able to give satisfying results on biallelic discrete ordinal data  compared to the other approaches.

choosing the number of spls-da dimensions
in the case of lda or sparse lda , it is of convention to choose the number of discriminant vectors h ≤ min, where p is the total number of variables and k is the number of classes. the p-dimensional data will be projected onto a h-dimensional space spanned by the first h discriminant vectors, also called dimensions in the case of spls.

to check if the same applies to spls-da, we have plotted the mean classification error rate  for each spls-da dimension . we can observe that the estimated error rate is stabilized after the first k -  <dig> dimensions for any number of selected variables for the microarray data sets. for the snp data set, h should be set to k -  <dig>  the latter result is surprising, but can be explained by the high similarity between two of the classes: the bantu kenya and banty south africa populations, as illustrated later in the text.

therefore, according to these graphics, reducing the subspace to the first k -  <dig>  dimensions is sufficient to explain the covariance structure of the microarray  data. in the following, we only record the classification error rate obtained after k -  <dig>  deflation steps have been performed with spls-da - this also applies to the tested variants of spls from  <cit> .

comparisons with other multiclass classification approaches
we compared the classification performance obtained with state-of-the-art classification approaches: rfe  <cit> , nsc  <cit>  and rf  <cit> , as well as a recently proposed approach: ofw  <cit>  that has been implemented with two types of classifiers, cart or svm and has also been extended to the multiclass case  <cit> . these wrapper approaches include an internal variable selection procedure to perform variable selection.

we compared the classification performance of spls-da to slda variants proposed by  <cit>  based on a pooled centroids formulation of the lda predictor function. the authors introduced feature selection by using correlation adjusted t-scores to deal with highly dimensional problems. two shrinkage approaches were proposed, with the classical lda  as well as with the diagonal discriminant analysis . the reader can refer to  <cit>  for more details and the associated r package sda.

finally, we included the results obtained with  <dig> other versions of sparse pls proposed by  <cit> . the splsda formulation is very similar to what we propose in spls-da, except that the variable selection and the classification is performed in two stages - whereas the prediction step in spls-da is directly obtained from the by-products of the spls - see section methods. the authors in  <cit>  therefore proposed to apply different classifiers once the variable selection is performed: linear discriminant analysis  or a logistic regression . the authors also proposed a one-stage approach sgpls by incorporating spls into a generalized linear model framework for a better sensitivity for multiclass classification. these approaches are implemented in the r package spls.

computational time in seconds on a intel core   <dig> duo cpu  <dig>  ghz machine with  <dig> gb of ram to run the approaches on the training data for a chosen number of selected variables .

the approaches are ranked by their performances.

details about the analysis
the aim of this section is to compare the classification performance of different types of variable selection approaches that may require some parameters to tune. we performed  <dig> fold cross-validation and averaged the obtained classification error rate accross  <dig> repetitions, and this for different variable selection sizes .

the wrapper approaches were run with the default parameters or the parameters proposed by the authors  <cit> . the sdda and slda approaches are actually two-stages approaches as variables need to be ranked first before slda/dda can be applied, but they do not require any other input parameter than the number of variables to select. spls-da, splsda-log/lda and sgpls require as input the number of pls dimensions as discussed above. in addition, while spls-da requires the number of variables to select on each dimension as an input parameter, splsda-log/lda and sgpls require to tune the η parameter that varies between  <dig> and  <dig> - the closer to  <dig> the smaller variable selection size, so that it matched the variable selection sizes with the other approaches. splsda-log/lda are performed in two steps: one step for variable selection with spls and one step for classification.

complexity of the data sets
all data sets differ in their complexity. for example, the 4-class srbct data set is known to be easy to classify  <cit>  and most approaches - except nsc, have similar good performances. analogously, the gcm data set that contains numerous classes  gives similar overall classification error rates for all approaches. the brain and leukemia data sets with  <dig> and  <dig> classes respectively seem to increase in complexity, and, therefore, lead to more accentuated discrepancies between the different approaches. the snp data set is more complex due to the discrete ordinal nature of the data , a high number of populations  that have similar characteristics - some of them, for instance bantu kenya and bantu south africa, are closely related. consequently, it can be expected that a large number of snp may be needed to discriminate at best the different populations. this is what we observe, but, nonetheless, most approaches  perform well, in particular nsc.

computational efficiency
we only recorded the computational time of the exploratory approaches sdda, slda, splsda-log, splsda-lda, sgpls and spls-da as the wrapper approaches are computationally very greedy . some computation time could not be recorded as a r memory allocation problem was encountered .

the fastest approach is sdda . this approach is not necessarily the one that performs the best, but is certainly the most efficient on large data sets. spls-da is the second fastest one. the splsda approaches were efficient on srbct but otherwise performed third, while sgpls computation time was similar to splsda except for large multiclass data set such as gcm.

wrapper approaches
amongst the wrapper approaches, rfe gave the best results for a very small selection of variables in most cases. the performance of rfe then dramatically decreased when the number of selected variables becomes large. this is due to the the backward elimination strategy adopted in the approach: the original variables are progressively discarded until only the 'dominant' mostly uncorrelated variables remain. rf seemed to give the second best performance for a larger number of variables. ofw-cart also performed well, as it aggregates cart classifiers, whereas ofw-svm performed rather poorly. this latter result might be due to the use of the one-vs-one multiclass svm. nsc seemed affected by a too large number of variables, but performed surprisingly well on the snp data.

sdda/slda
both variants gave similar results, but we could observe some differences in the gcm data set. in fact,  <cit>  advised to apply sdda for extremely high-dimensional data, but when a difference was observed between the two approaches , it seemed that slda performs the best. however, in terms of computational efficiency, sdda was the most efficient.

splsda-log/splsda-lda
splsda-lda gave better results than splsda-log except for srbct where both variants performed similarly. on leukemia, brain and snp, splsda-lda had a similar performance to spls-da but only when the selection size became larger.

sgpls
sgpls performed similarly to spls-da on srbct and gave similar performance to spls-da on leukemia when the selection size was large. however, it performed poorly in brain where the number of classes becomes large and very unbalanced. sgpls could not be run on gcm data as while tuning the η parameter, the smallest variable selection size we could obtain was  <dig>  which did not make sgpls comparable to the other approaches. on the snp data sgpls encountered r memory allocation issues.

spls-da
spls-da gave similar results to sdda and slda in the less complex data sets srbct and gcm. the performance obtained on brain was quite poor, but results were very competitive in leukemia for a number of selected genes varying between  <dig> and  <dig>  note that the number of selected variables is the total number of variables selected accross the k -  <dig> chosen dimensions . in overall, spls-da gave better results than the wrapper approaches, and remained very competitive to the other exploratory approaches. one winning advantage of spls-da is the graphical outputs that it can provide , as well as its computational efficiency.

stability analysis of spls-da
it is useful to assess how stable the variable selection is when the training set is perturbed, as recently proposed by  <cit> . for instance, the idea of bolasso  <cit>  is to randomize the training set by drawing boostrap samples or drawing n/ <dig> samples in the training set, where n is the total number of samples. the variable selection algorithm is then applied on each subsample with a fixed number of variables to select and the variables that are selected are then recorded  <cit> . proposed to keep in the selection only the variables that were selected in all subsamples, whereas  <cit>  proposed to compute a relative selection frequency and keep the most stable variables in the selection.

we chose to illustrate the latter option as we believe that the stability frequency, or probability, gives a better understanding of the number of stable discriminative variables that are selected in spls-da. the highly correlated variables will get a higher probability of being selected in each subsample, while the noisy variables will have a probability close to zero. this stability measure can also guide the user in the number of variables to choose on each spls-da dimension. once the number of variables to select has been chosen for the first dimension, the stability analysis should be run for the second dimension and so on. note that  <cit>  proposed an additional perturbation by introducing random weights in the lasso coefficients, called random lasso. this latter approach could not, however, be directly applied in the spls-da algorithm due to its iterative nature.

we also noticed that once we reached too many dimensions , then the frequencies of all variables dropped, which clearly showed that spls-da could not distinguish between discriminative variables and noisy variables any more .

data visualization with spls-da
representing the samples and the variables
data interpretation is crucial for a better understanding of highly dimensional biological data sets. data visualization is one of the clear advantage of projection-based methods, such a principal component analysis , the original pls-da or spls-da, compared to the other tested approaches . the decomposition of the data matrix into loading vectors and latent variables provide valuable graphical outputs to easily visualize the results. for example, the latent variables can be used to represent the similarities and dissimilarities between the samples: figure  <dig> illustrates the difference in the sample representation between classical pls-da  and spls-da  for the brain data set. variable selection for highly dimensional data sets can be beneficial to remove the noise and improve the samples clustering. a 3d graphical representation can be found in additional file  <dig> with spls-da. figures  <dig>   <dig> and  <dig> compare the sample representation on the snp data set using pca , classical pls-da and spls-da on several principal components or pls dimensions. on the full data set, pca is able to discriminate the african hunter gatherers populations san, mbuti and biaka from the  <dig> other populations that are very similar . this is a fact that was previously observed  <cit>  and it indicates a good quality of the data. with pca however, the differentiation between the  <dig> populations mandeka, yoruba, bantu south africa and bantu kenya is not very clear, even for further dimensions . on the contrary to pca, pls-da  and spls-da  are able to discriminate further these  <dig> populations on dimensions  <dig> and  <dig>  in particular, the mandeka population is well differentiated on dimension  <dig>  and so is the yaruba population on dimension  <dig>  in terms of sample representation and in contrary to what was obtained with the brain data set , the difference between pls-da and spls-da is not striking on this particular data set. this is probably because the snp variables, although containing redundant information, are all informative and mostly not noisy. this also explains the good population clusters obtained with pca . however, the variable selection performed in spls-da has two advantages: firstly it reduces the size of the data set for further investigation and analyses; secondly, since each pls dimension focuses on the differentiation of some particular populations  and since spls selects an associated subset of variables on each on these dimensions, each of these subsets of variables is well able to differentiate these particular populations. this variable selection therefore gives more insight into the data . figure  <dig> illustrates the weights in absolute value of the sparse loading vectors for each spls-da dimension in the brain data set. only the genes with a non-zero weight are considered in the spls-da analysis and were included in the gene selection . generally, the sparse loading vectors are orthogonal to each other, which permits to uniquely select genes across all dimensions. the latent variables can also be used to compute pairwise correlations between the genes to visualize them on correlation circles and better understand the correlation between the genes on each dimension ). note that this type of output is commonly used for canonical correlation analysis.

on the contrary, the pooled centroid formulation used in sdda and slda do not provide such latent variables, and, therefore, lack of such useful outputs. the same can be said about the wrapper approaches, which often have a much higher computational cost than the sparse exploratory approaches applied in this study.

brain data set: biological interpretation
comparisons between the gene lists
the ultimate aim when performing variable selection is to investigate whether the selected genes  have a biological meaning. we saw for example that some of the tested approaches gave similar performances, even though they select different variables.

we therefore compared the lists of  <dig> genes selected with the different approaches on the brain data set. note that the selection size has to be large enough to extract known biological information from manually curated databases.

unsurprisingly, given the variety of approaches used, there were not many genes in common: there were between  <dig> and  <dig> genes shared between spls-da, sdda, slda and splda - sdda and slda shared the most important number of genes . the gene selection from sgpls grandly differed from the other multivariate approaches . this may explain why the performance of sgpls was pretty poor compared to the other approaches on the brain data set. rf seemed to be the approach that selected the highest number of genes in common with all approaches except with nsc . a fact to be expected was that there were very few commonly selected genes between the exploratory approaches and the wrapper approaches .

we then investigated further the biological meaning of the selected genes. this analysis was performed with the genego software  <cit>  that outputs process networks, gene ontology processes as well as the list of diseases potentially linked with the selected genes.

it was interesting to see that in all these gene lists , between  <dig> to  <dig> genes were linked to networks involved in neurogenesis, apoptosis, as well as dna damage  and neurophysiological processes . most of the lists that were selected with the wrapper approaches generated interesting gene ontology processes, such as degeneration of neurons , synaptic transmission or generation of neurons . on the contrary, the sparse exploratory approaches seemed to pinpoint potential biomarkers linked with relevant diseases: central nervous system and brain tumor , sturge weber syndrome, angiomatosis, brain stem , neurocutaneous syndrome , neurologic manifestations and cognition disorders .

this preliminary analysis shows that the different approaches are able to select relevant genes linked to the biological study and are able to select complementary information. this was also the conclusion drawn in  <cit> .

further biological interpretation with the spls-da list
using the genego software, known biological networks were generated from the list of genes selected with spls-da -  <dig> genes in total for the first two dimensions. for example, the network represented in figure  <dig> is based on  <dig> of these selected genes , which are involved in biological functions such as cell differentiation, cellular developmental process and central nervous system development. these genes are organised around two transcription factors, esr <dig> and sp <dig>  sp <dig> can activate or repress transcription in response to physiological and pathological stimuli and regulates the expression of a large number of genes involved in a variety of processes such as cell growth, apoptosis, differentiation and immune responses.

interestingly, all  <dig> genes present in the network were also found to be highly correlated to the spls-da dimensions  <dig> and  <dig> . this latter result suggests a. that the first  dimension of spls-da seems to focus on the sp <dig>  network and b. that the genes selected with spls-da are of biological relevance . further investigation would be required to give more insight into the spls-da gene selection.

description of the genes or proteins encoded by the genes selected by spls-da and present in the known genego biological network.

CONCLUSIONS
in this article, we showed that spls could be naturally extended to spls-da for discrimination purposes by coding the response matrix y with dummy variables. spls-da often gave similar classification performance to competitive sparse lda approaches in multiclass problems. undoubtedly, the sparse approaches that we tested are extremely competitive to the wrapper methods, which are often considered as black boxes with no intuitive tuning parameters . the preliminary biological analysis showed that some tested approaches brought relevant biological information. the pls-based approaches such as the spls-da approach that we propose have a well established framework for class prediction. the computational efficiency of spls-da as well as the valuable graphical outputs that provide easier interpretation of the results make spls-da a great alternative to other types of variable selection techniques in a supervised classification framework. we also showed that a stability analysis could guide the parameter tunings of spls-da. on the brain data set, we showed that spls-da selected relevant genes that shed more light on the biological study. for these reasons, we believe that spls-da provides an interesting and worthwhile alternative for feature selection in multiclass problems.

