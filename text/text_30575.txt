BACKGROUND
the depth and breadth of curated knowledge in molecular biomedicine is staggering. the  <dig> nucleic acids research peer-reviewed compilation of molecular biomedical databases lists  <dig>  databases  <cit> , many of which hold millions of detailed records about biomedically significant entities. much contemporary biomedical research depends on broad and unbiased assays at genomic scale. interpretation of the results of such assays, which generally implicate hundreds or even thousands of relevant gene products  in the context of what is already known is particularly challenging, and existing approaches are clearly inadequate to address this challenge and others. the aggregate consequences of this failure to capitalize on existing knowledge for the interpretation of genome-scale experimental results is a substantial reduction in the efficiency of the biomedical research enterprise writ large, delaying the development of both key insights and new therapies. the ability to query many independent biological databases using a common, community-driven semantic model would facilitate deeper integration and more effective utilization of these diverse and rapidly growing resources.

attempts to access and integrate data from multiple public biomedical databases are often plagued with issues stemming from heterogeneous database schemas, idiosyncratic file formats, data redundancy, numerous independent identifiers, and differing curation standards and practices. while researchers’ decisions about which databases and which data to use should be based on their task and on biomedical criteria, they are often based instead on logistical criteria such as the underlying database representations, the ease or difficulty of accessing the data, and the ability to integrate a given data set with others. researchers need an environment not only in which these data are readily accessible but also where they can ask queries that are biological in nature and unencumbered from the underlying shape or format of the data.

goble and stevens  <cit>  have written of several serious issues in need of addressing for biomedical data integration, including the need for shared identities and semantics, the need to use existing standards where available, and balancing data collection with data use; they state that these problems have led to a current “loose federation of bio-nations”. while work to integrate various data is progressing, good and wilkinson point out that we are seeing “‘semantic creep’—timid, piecemeal and ad hoc adoption of parts of standards”  <cit> . linking data across resources is necessary for building integrated systems; however, linking the data without understanding the semantics of those links merely generates more data  <cit> . furthermore, any data integration must be able to support multiple modes of reasoning that can deal with the fact that integrated data are likely to have noise and errors  <cit> .

while many domains have developed standard file formats for the consistent sharing of data, these formats are generally domain- or task- specific, making them difficult to integrate with one another. other existing non-ontologically grounded approaches to data integration include maintaining cross references that point to related identifiers and records in other sources but often conflate semantics, e.g., by linking a protein record to a gene record . other linked data approaches typically have weak semantic abstractions that do not map to a single common biomedical model and do not unambiguously assert which biomedical entities are semantically identical across data sources. this is a serious hindrance, as goble and stevens have posited: ′the failure to address identity will be the most likely obstacle that will stop mashups, or any other technology or strategy, becoming an effective integration mechanism”  <cit> . ontology-based approaches to data integration thus far have been either small or focused on specific domains or tasks. larger semantic integrations have not provided declarative representations of mappings, or use non-standard semantic models. despite decades of effort, the goal of integrating diverse data into a common biomedical model remains elusive.

we have put together a set of five methods, some novel and some that build on prior work of others, that overcome problems commonly encountered when attempting to semantically integrate information from multiple data sources, when applied collectively these methods resolve seven key problems.  varying identifiers used across data sources to refer to the same concepts;  differing file formats using different lexicalizations and interpretations of identifiers;  conflation of informational entities  with biomedical concepts ;  the use of varying non-ontologically grounded semantic models;  errors and inconsistency among source data;  instability of identifiers and uris over time in integrated resources; and  difficulty in tracing and reporting provenance of integrated data.

we demonstrate these solutions by presenting kabob , a system that integrates  <dig> sources of biomedical data using  <dig> prominent open biomedical ontologies  as a foundation and vocabulary for modeling, thus facilitating interaction with the wealth of existing data and tools that already rely on these obos. in kabob, identity across data sources is maintained through the generation of a single biomedical entity for each set of data-source-specific identifiers each referring to the entity. these entities, along with the obo concepts, function as the building blocks for the common biomedical representations, which can be simultaneously modeled and thus queried at multiple levels of abstraction. kabob maintains a clear distinction between source data and biomedical concepts and represents both explicitly. users need only understand the common obo-based representations to interact with data from all of the integrated sources that have been mapped into the biomedical representation, rather than having to know each relevant source’s specific modeling and the similarities and differences among each data model; however, for data that have not been mapped to biomedical concepts, the source data are also available for querying over a common informational metamodel. kabob uses declaratively represented forward-chaining rules to map from the source data to biomedical concepts, and the explicit representation of both the source data and the rules together provide transparent and computable provenance for every concept and assertion. by resolving many of the issues that routinely plague biomedical researchers intending to work with data from multiple data sources simultaneously, kabob provides a platform for ongoing data integration and development and for formal reasoning over a wealth of integrated biomedical data.

methods
biomedical data sources tend to use various idiosyncratic data models that often do not integrate well with each other, and no common model exists across them. consequently, there is no immediately straightforward way to combine their data. to build kabob we tackle the problem of mapping data source contents to a common biomedical model incrementally. first we explicitly represent the contents of the data sources as informational constructs , then apply declarative rules to create representations of the biomedical entities they denote, grounded in a common model that is based on the obos. this division of information entities and biomedical concepts is one of the fundamental ideas underlying kabob. it greatly simplifies the manner in which biomedical representations are created and edited, as well as provides several additional advantages: for example, functioning as provenance. furthermore it easily allows multiple representations of the source data to be generated, for example at differing levels of granularity or generalization, all still coherent with the overall model. since each step in kabob construction is separate and incremental they can be developed and debugged independently, by teams of developers with differing skill sets. for example, the kabob approach separates computational systems design decisions  from ontological decisions related to biomedical model representation, processes that are intermingled in other approaches.

we use five methodological steps to solve major problems commonly encountered in semantic data integration.  data source records are explicitly represented using a common informational model.  references to identifiers in these records are canonicalized.  identifier mappings across data sources are used to derive sets of ids that are intended to refer to the same biomedical concept, and  these identifier sets and a corresponding biomedical entity for each are explicitly represented. finally,  forward-chaining rules are used to produce representations that are grounded in common biomedical models in the form of prominent obos that build upon the unified biomedical entities. this modeling is done in such a way as to avoid conflicts in the event of inconsistent underlying data. each step is discussed individually, in the context of applying them collectively to produce kabob.

kabob has three major subdivisions of representation:  an imported collection of prominent obos that serve as the representational foundation for the rest of the knowledge base;  the representation of source database records, schemas, and identifiers, modeled as instances of information content entities , collectively referred to as the ice content of the knowledge base,; and  the representation of biomedical concepts such as biological processes and interactions, diseases and phenotypes, and genes, gene products, and other types of biological sequences , collectively referred to as the bio content of the knowledge base.

figure  <dig> depicts how kabob is constructed. kabob, initially an empty triplestore, is built up incrementally. first, ontologies are downloaded and then loaded directly into the triplestore. database source files are downloaded and converted to rdf; the resultant rdf triples are loaded into the ice section of kabob. forward-chaining rules  generate ice identifiers for each of the biomedical concepts in the ontologies. . the second set of forward-chaining rules generates identity links between ice identifiers, specifically, assertions of skos:exactmatch links between identifiers denoting the same biomedical concepts. the next step instantiates an id set in the ice side of kabob corresponding to each unique biomedical concept. each biomedical concept is then explicitly represented in the bio section; for example, a gene entity on the bio side is created for each such set of gene ids. . more forward-chaining rules are then used to create  other biomedical concepts and assertions referred to within the data source records, e.g., interaction events with protein participants from protein-protein interaction database records, processes with participating entities from gene ontology annotations, and links from drugs to genes or gene products from drug-related data sources. when the rules have finished running and their output has been loaded into the bio side, kabob is ready to be queried and used. each of the steps required to build an instance of a kabob knowledge base is discussed in greater detail in the following subsections. a detailed list of the steps used to build kabob is provided in additional file  <dig> figure  <dig> kabob construction. depicts the incremental construction of kabob. labeled arrows represent processes that flow from inputs to outputs. construction starts with downloading files and flows through translating them into rdf and then iteratively querying and producing more rdf. steps marked with ** involve multiple sets of rules being run and their output loaded in sequence.



sources
the initial release of kabob has been designed to include a wide variety of biomedical data from a number of prominent publicly available sources. these data range from attributes of core biomedical entities  to interactions between these entities  to biological functions attributed to the entities . kabob is designed to be extensible; as such, the list of data sources should not be considered exhaustive or limiting. new data sources are being added as needed to accomplish specific reasoning and querying tasks.

kabob currently imports the following  <dig> ontologies:basic formal ontology   <cit> 

brenda tissue / enzyme source   <cit> 

chemical entities of biological interest   <cit> 

cell type ontology   <cit> 

gene ontology including biological process, molecular function, and cellular component   <cit> 

information artifact ontology   <cit> 

protein-protein interaction ontology   <cit> 

mammalian phenotype ontology   <cit> 

ncbi taxonomy  <cit> 

ontology for biomedical investigation   <cit> 

protein modification   <cit> 

protein ontology   <cit> 

relation ontology   <cit> 

sequence ontology   <cit> 



kabob currently imports data from the following  <dig> data sources:database of interacting proteins   <cit> 

drugbank  <cit> 

genetic association database   <cit> 

uniprot gene ontology annotation   <cit> 

hugo gene nomenclature committee   <cit> 

homologene  <cit> 

human protein reference database   <cit> 

interpro  <cit> 

irefweb  <cit> 

mouse genome informatics   <cit> 

mirbase  <cit> 

ncbi gene  <cit> 

online mendelian inheritance in man   <cit> 

pharmgkb  <cit> 

reactome  <cit> 

rat genome database   <cit> 

transfac  <cit> 

uniprot  <cit> 



the utility of kabob is predicated not only on the knowledge it contains but also by how up-to-date this knowledge is. we have eased this knowledge acquisition process by automatically downloading individual data sources from their corresponding locations on the internet and constructing file parsers that can detect changes in data source file formats and report back if the parsers need to be updated. logs are kept for every download recording the date and location of each source file, and logs are generated for every file parse recording warnings and errors that need to be addressed. the entire process from download to final knowledge base creation can be accomplished in under 2 days, allowing kabob to be updated at the same frequency as the major data sources it depends on. changes to the format of the data sources can require modifications to the file parsers, which is relatively straightforward, but does require some time, typically about a day. efficient storage of and access to historic copies of all ice data is being investigated .

database record representation
all database content is directly modeled as information content entities  as defined in the information artifact ontology, one of the open biomedical ontologies  <cit> . an initial model for the database record representation is discussed in detail in a previous publication  <cit> . briefly, each database, schema, record, field, and field value is modeled as an ice. the obo:has_part relation is used to connect record ices to their corresponding field value ices and database schema ices to component field ices, while the kiao:has_template relation is used to link record ices to their corresponding schema ices and field value ices to corresponding field ices. the simplicity and generality of this record representation permits its use toward the many different data sources being incorporated into kabob, regardless of the underlying file format . an example of this representation can be seen in the ice panel  of figure  <dig>  this figure in part depicts two simplified gene ontology annotation records , which are each connected to two field values, one a uniprot id and one a go id. note that since both source records use the same go id, these two record ices use the same field value ice instance ; each such reuse of a field value ice instance keeps the three triples required to define it from being redundantly represented in the knowledge base. at the scale of the complete knowledge base, this reuse of field values results in a large reduction in the total number of triples required to represent the ices. for the ncbi gene info file this amounts to an approximate 44% reduction in the number of required triples. each field value is linked to an associated template that indicates the field for which it serves as a value , since fieldvalue <dig> and vieldvalue <dig> are values of the same field, they point to the same field ice instance. each field is connected to its corresponding value.figure  <dig> example ice records and corresponding bio concepts. depicts an excerpt of the knowledge representation in kabob. ovals are used to depict instances, and rectangles classes. single line arrows represent triples and point from their subject to their object and are labeled with their property. the iao:denotes links that cross from the ice to the bio side are emphasized with dashed arrows. the double arrows are shorthand for representing an owl:restriction on the given property with some values from the object value. this figure depicts two go annotation records that are then converted to biomedical concepts using the same rule . additionally sets of gene identifiers are also depicted that denote their corresponding gene concept. on the bio side the relations between genes, proteins, and gene or gene product aggregate classes are also shown. other than the records and their field values, generated by the file parsers, all other links are the output of applying rules.



the record representation used by kabob differs from the previously published representation of bada et al.  <cit>  by being more record-centric. we now allow records to share structure with other records, for example to share field value instances such as in the aforementioned example, greatly reducing the number of triples required. a minor difference between the current and former representations is that record ice instances are now linked via obo:has_part to field value ice instances in kabob, whereas field value ice instances were linked via obo:part_of to record ice instances in the former representation of data source records.

the use of the sha- <dig> hash enables the ice uris to be deterministically generated and reused whenever a field with the identical value is encountered without the need to keep track of field values previously encountered during construction. this also provides consistency across kabob instance builds, which aids in checking for differences and debugging. for example, when specifying the uri for an ncbi gene database taxonomy field with value  <dig>  our optimization uses the sha- <dig> hash function  <cit>  over the field value  and incorporates the resultant hash value into the uri, e.g., http://kabob.ucdenver.edu/iao/eg/f_entrezgeneinfofiledata_taxonid_0buksty0wb-d665ttwobpzrc1xi
.

canonicalization of identifiers in records
while we attempt to generate records that are as faithful to the source representation as possible, we do modify the records by transforming identifier strings to canonicalized uris. for example, the ncbi gene identifier for the human atp5d gene is rendered in source records as “eg513”, “eg_513”, “eg:513”, “513” , etc., all of which would be canonicalized to the same uri for that ncbi gene identifier, i.e., http://kabob.ucdenver.edu/iao/eg/eg_513_ice
.

forward-chaining rules
assertions in kabob are generated using a series of declaratively represented forward-chaining rules. these rules take information that is variably encoded among the records of the disparate source databases and create rdf assertions uniformly represented in terms of prominent obos. there are several batches of forward-chaining rules, as depicted in figure  <dig>  and the triples generated by the rules are saved in compressed n-triple files and then loaded into the kabob triplestore.

the following is an example rule that corresponds to the example in figure  <dig>  where some of the symbol names have been simplified for readability. . this rule queries record ices imported from the gene ontology annotation database  <cit>  to retrieve uniprot id ices and go biological process id ices. it uses the triples linking id ices to biomedical concepts  to retrieve the corresponding protein class for the protein id and the corresponding biological process for the go id using the following graph pattern.

the rule then uses the following template  to construct additional triples.

these five triples specify a new process class, formally defined as a subclass of the given go process with a restriction that the given protein is a participant in the process. this new class captures the “all-some” semantics of a go biological process annotation, describing the subclass of the biological process such that for each instance of that subclass there exists some instance of the specified protein that participates in that process  <cit> . it uses existing known values for ?protein and ?goprocess retrieved as bindings by the body of the forward-chaining rule, and it creates new uris for the restriction ?participant and the dynamically generated ?newprocess class. figure  <dig> shows an example of the input and output to such a rule. the rule would be run for record <dig> and record <dig>  it would get the protein id value of the uniprot id field, and the go id value of the go id field in each of these records, then get the denoted concepts  by following the dotted lines into the bio representation . then, new classes of biomedical concepts would be dynamically generated  and connections made to existing biomedical concepts. in this case, each would be made a subclass of the go process specified in the original go annotations  and a subclass of a restriction specifying the given protein as a participant in the process.

the rules are represented using a domain-specific language written using clojure s-expressions. it is an extension of the pattern language provided in the open-source kr clojure library  <cit> . the rules are applied using a straightforward implementation of a forward chainer in clojure. the rules could also be serialized to other formats. swrl  <cit>  is an obvious potential target; however, swrl rules cannot have unbound variables in the head, thus blocking reification, which is needed for many  rules. the rules could be realized as sparql construct queries as well. this avenue has not been explored in great detail, as sparql  <dig>  was still in its infancy and access to the functions necessary to reify new entities was extremely limited at the time of the start of the kabob project. this can be reinvestigated as future work, along with providing rif   <cit>  export and import of rules. a complete example of a rule and more discussion is provided in additional file  <dig> 

identifiers and identifier sets
since different data sources use different identifiers to refer to the same given concept, aggregating data across sources requires managing sets of identifiers that are intended to refer to the same concepts. in kabob, identifier sets are built incrementally. first, mappings between identifiers mentioned in the underlying data sources are extracted and explicitly represented. then, unified identifier sets are derived from all relevant extracted mappings. a corresponding biomedical entity is created for each identifier set, e.g., a protein for a given set of protein ids that refer to this protein. we do this work in a series of stages, allowing the output of each stage to be used on its own as well as enabling the results to be easily inspected and debugged, as identifier mappings in individual data sources can be idiosyncratic and occasionally incorrect. these data further serves as provenance for how the biomedical entities are created.

identifier mapping idiosyncrasies can arise from information in data sources being represented at varying levels of abstractions and granularity, requiring additional effort to understand and disambiguate the identifier mappings that they provide. for example, data sources, such as ncbi gene info  <cit> , often use a field called “database cross-reference”  that may have mappings to different types of things, e.g., related drugs, diseases, pathways. care must be taken when navigating these fields to prevent links from being constructed between identifiers that are not actually semantically exact matches, e.g., between drugs and diseases, or between genes and proteins. in order to be able to sift through the semantic ambiguities in cross-referencing fields, a step was introduced to help identify the cross-referencing intent. identifying the intent of a cross-reference field requires knowing something about what a given identifier will ultimately refer to; however, at this stage in kabob construction entities are not fully represented. this circular logic was overcome by bootstrapping the kabob construction process with simple type information of what will be the ultimate resulting entities. rules were created to generate this information and break the cycle.

these entity typing rules connect ids from data sources to corresponding biomedical classes ; for example, all ncbi gene ids are asserted to refer to types of dna  using kiao:denotessubclassof, a macrorelation for the property chain of iao:denotes and rdfs:subclassof. . after these denotessubclassof assertions are created, a second set of ice-to-ice rules use these assertions to extract only those mappings between ice ids from the various data sources  that denote semantically identical entities. for example, so as to extract only the mapped ids stored in the dbxref field of a given ncbi gene info record that are denotationally semantically equivalent , the executed rules pick out only those ids that kiao:denotessubclassof so:dna, as the entities denoted by the ids that do not satisfy this criterion are not dna sequences and are therefore unlikely to be the same entity denoted by the ncbi gene id. filtering mapped ids by making use of our explicit linkages of id types to types of biomedical concepts is a pragmatic solution that is both effective and efficient. the output of these identifier-mapping rules are skos:exactmatch links between id ices.

great care is taken to not link things across type. for example, even though the protein denoted by the identifier uniprot:p <dig>  is coded by the gene denoted by the identifier hgnc: <dig> , they are not linked with an skos:exactmatch relation, because they do not refer to precisely the same concept, in that the former denotes a class of proteins and the latter a class of genes. on the other hand, a skos:exactmatch link is created between hgnc: <dig> and eg: <dig>  as they denote the same gene. . the kiao:denotessubclassof assertions are essential to sorting this out. a simplified example of this type of representation can be seen in figure 2: on the ice side are pairs of gene identifiers that refer to the same genes on the bio side, i.e., hgnc_837_ice and eg_513_ice, which both denote gene <dig>  and hgnc_9604_ice and eg_5742_ice, which both denote gene <dig>  note that the gene identifiers denoting the same genes are linked to each other via the skos:exactmatch relation; however, there is no asserted relationship between corresponding protein identifiers and gene identifiers on the ice side , only a relation between the denoted protein  and corresponding gene  on the bio side.

after the skos:exactmatch links are created, their transitive closure is computed using the union-find algorithm  <cit> . the union-find algorithm is an efficient method for building a collection of disjoint  sets given a list of pairs of members that are in the same set. it incrementally builds and merges sets of connected components as it streams through the list of pairs. an explicit identifier set is then created for each set of ice identifiers with a uri based on an sha- <dig> hash of the sorted members of the set. the use of a hashing function in this manner allows the identifiers to be computed consistently over time, which ensures that two kabob instances computed from the same sources produce the same identifiers. . the computation of this transitive closure, along with constructing the initial ice rdf, are the only parts of kabob construction not performed using the forward-chaining rule system. it is possible to compute this transitive closure with forward-chaining rules; however, the union-find algorithm is extremely efficient, with a running time of o, compared to potentially needing multiple passes with forward chaining to compute.

as an example of computing and creating a set of identifiers each denoting the same biomedical concept, we start with the following three triples, which specify mappings between ids denoting the same gene.

after passing through the union-find algorithm the four identifiers in these triples are grouped into a single identifier set. the following four triples  specify this set, which is given the uri kiao:kabob-id-set-qn-3e2r15nyu8wune-a2bxb_nz, based on the sha- <dig> hash of its members. the set is comprised of four identifiers whose membership is the set is asserted via the kro:hasmember relation:

representation of biomedical concepts
more rules are used to build up biomedical representations. first, a biomedical concept is created for each identifier set, e.g., a gene class corresponding to the set of id ices denoting the given gene. layers of biomedical sequence abstractions  are added, e.g., aggregate gene-or-gene-product classes and gene-or-gene-product-or-variant classes. connections are made between corresponding genes and gene products, e.g., representing that a given gene serves as the indirect template for a given protein, and connecting that protein to the corresponding gene-or-gene-product aggregate classes.

the last set of rules continue to convert information variably represented in the ice records into unified biomedical representations in terms of relevant obo classes. generally, each kabob assertion derived from a source database requires one rule and results in at least one dynamically generated subclass to model the assertion in owl. a rough estimate is one rule per field in the source records; however, some fields require multiple rules. for example, the gene type field in the ncbi gene database requires one rule per possible sequence type  to accurately model the value of the field, as these values are curated by ncbi using a small custom controlled vocabulary, each term of which is uniquely mapped to a sequence ontology class. on the other hand, extracting other types of assertions requires looking at multiple fields in one rule. for example, extracting a drug-gene association assertion from pharmgkb requires the examination of four fields, as pharmgkb uses two fields to specify the identifiers of the interacting entities and two more fields to specify the types of these entities .

as an example of a dynamically rule-generated biomedical concept extended from existing obo concepts, the following nine triples  represent the interaction  between the drug isoflurane  and the gene atp5d  , derived from the drugbank database  <cit>  :

the first block of triples models the owl restriction class  representing the class of all things in which the gene atp5d or one of its products or variants participates. the second analogously models a restriction class  of all things in which isoflurane participates. the third block formally defines a class  as the subclass of both of these restriction classes and an obo interaction class, i.e., the subclass of interactions such that for each of its members, there exists some isoflurane that is a participant, and there exists some atp5d or one of its products or variants that is a participant.

a large amount of redundant generation of semantically equivalent owl restriction classes is avoided through the use of assertions that generate unique hashes for the restrictions. for example, the uri for the restriction kbio:r_i4hmhklpj3plud__6t2qucrtfwk is generated from a sha- <dig> hash of the property and object values of the triples used to define it. reusing owl fragments like this significantly reduces the number of triples and in doing so reduces the load on reasoners that will eventually operate over kabob.

currently there are  <dig> rules. depending on the complexity of the rule, and if other rules that look at similar source records or that have similar output representations exist, a new rule can be written in anywhere from minutes  to an hour or so .

implementation
command-line build scripts for installations in both allegrograph   <cit>   and virtuoso   <cit>   are provided in the open-source release. we run these scripts via a hudson server in order to monitor performance and output; however, they could be run just as easily without hudson. while we have targeted allegrograph and virtuoso, there is nothing specific about kabob to either. to query the triplestore, the rule engine and identifier-set creation code uses the open-source kr clojure library, which can talk to any triplestore that speaks sesame or jena. at most, one small function for establishing the connection to the source triplestore would have to be extended, but more than likely speaking to a different server requires only changing the parameters for server location and authentication. the build scripts would have to be extended for additional triplestores to provide a command-line call for loading a directory of rdf files into the triple store. porting the scripts from allegrograph to virtuoso was done in about a day. the command-line scripts are written in bash, and all other code is java or clojure , both configured with maven, allowing the code to be run anywhere the jvm can. we have done all of our work on a custom-built machine with  <dig> cores, 96 gb ram, 2 tb of ssd drives lvm raid- <dig> , and  <dig>  tb of spinning disk raid , running fedora v <dig> 

RESULTS
two fully functional versions of kabob have been built, along with one partial version. one fully functional version has been built using only human source data and another using human data plus data for seven major eukaryotic model organisms : mus musculus , rattus norvegicus , drosophila melanogaster , saccharomyces cerevisiae , caenorhabditis elegans , danio rerio , and arabidopsis thaliana .  ideally, a version of kabob would be built using data for every organism included in the data sources; however, that is out of reach of our current hardware/software configuration . for the all-organism version, only the ice records have been produced. for the two fully functional versions, all kabob steps are performed, from initial ice record representation to running of all of the bio concept generating rules. each version can be built in approximately two days.

data is parsed from a total of  <dig> different files from  <dig> data sources. table  <dig> shows the numbers of triples and compressed file sizes for the three versions of kabob and for these versions in total as well as for the three primary triple subsets of imported obos, ices of original source data, and rule-generated data. all versions of kabob use the same  <dig> ontology files, which amount to  <dig> , <dig> triples. the triple subsets of imported obo content and rdfized original source data are the two primary sources of data to kabob and are shown in the two parallel paths on the left side of figure  <dig>  the rule-generated data comprise all other triples in kabob and includes all of the rdf files depicted under the kabob block in figure  <dig>  table  <dig> shows the number of identifier sets in each version of kabob, along with the number of triples required to represent those sets, and the size of those triples in compressed n-triple format.table  <dig> 
size of kabob


imported obos
ice records
generated 
kabob total

subset
# triples
size .owl 
# triples
size .nt.gzip 
# triples
size .nt.gzip 
# triples
size 
lists the size of the various collection of rdf generated in the kabob build process, recorded in number of triples and size on disk. the first three major columns include the imported obos, the ice records , and the generated triples . the fourth column is the sum of the first three. the rows represent subsets of the kabob data based on organisms included. the subsets are human-only, human plus seven major model organisms , and the final row is for all organisms combined. due to the scale of the data in the final subset this data is currently incomplete.
number of entities / id sets



subset
total rdf .nt.gz file size
list the number of entities or id sets in each subset of kabob. each id set is the collection of identifiers from multiple data sources that are intended to denote the same biomedical concept. number of id sets, number of triples, and size on disk is reported.



the ability to answer complex queries that target the common obo-based biomedical model is demonstrated by a series of example queries. an extended use case is discussed in the “following up on gsea results; a case study for using kabob” section, and several other examples are provided in additional file  <dig> 

this project has also produced or substantially expanded three open-source software libraries. first, this project was the primary motivation for the already released kr library for working with rdf and sparql in clojure  <cit> , which has been downloaded more than  <dig> times. in addition, timed with the publication of this paper is the release of two more libraries of code. the first is a java-based project consisting of the file parsers for all of the data sources serving as input into kabob and code to convert the parsed files into rdf. the second project released in conjunction with this paper is the kabob-specific code itself. this code includes the scripts for building kabob, as well as the id set merging code and all of the declarative rules.

in addition to building an integrated system that increases the value of the data from the underlying sources, we are also able to detect potential errors in the data sources and report them to their curators. errors can be detected by querying kabob for assertions that should not exist, e.g., a protein asserted to exist in two disjoint organismal taxa. in this way, we were able to find a mapping in dip that erroneously equated a mouse protein to the homologous rat protein, an error that was replicated in the irefweb aggregation. during the identifier-merging step, we have also looked for collapses of what should be multiple entities into single entities. this has revealed errors in our own code, such as bugs in the id canonicalization inadvertently truncating ids and causing them to collide, as well as revealing bad mappings in the underlying data sources, including finding over  <dig> cases in drugbank where multiple drugbank records mapped to the same external identifier. pipelined approaches to using these data sources would be susceptible to propagating such errors, as they would blindly hop through whatever mappings are being used to the target id, potentially producing erroneous mappings. see additional files  <dig> and  <dig> for a more detailed discussion of queries used to find errors, and a more complete explanation of the drugbank example.

discussion
in this section we discuss the solutions to data integration problems that our methods overcome, and how they are manifested in kabob. we also provide a detailed example of querying kabob using sparql  <dig>  and conclude with a discussion of limitations and future work.

solutions to data integration problems
the methods and knowledge representations that we have developed to integrate multiple data sources into a unified knowledge base provide solutions to seven semantic data integration problems. these solutions include:  distinct representations of data and biomedical concepts;  common biomedical representations;  identity resolution across data sources;  consistency despite potential errors or contradictions in source data;  the ability to represent and query data using multiple different levels of abstraction or granularity simultaneously;  stable and reusable uris; and  traceable provenance.

distinct informational entities and biomedical concepts
previous work integrating large quantities of biomedical data either use source-specific modeling and thus are actually disjoint, or do not include the portions of the source data that have not yet been represented in the common biomedical model . another common problem when dealing with biomedical data is overloading the meaning of common identifiers . kabob resolves these problems by clearly modeling both biomedical concepts  and informational entities referring to these concepts  and maintaining an explicit distinction between the two categories. for example, kabob has distinct, explicitly represented concepts for a uniprot identifier, the corresponding uniprot record, and the class of protein that the identifier denotes and the record describes.

all of the aforementioned informational entities of the source databases are represented as information content entities on the ice side of kabob. biomedical representations are derived from these ice data and represented on the bio side of kabob. . the only links that cross the demarcation between ice and bio representations are relations that indicate that a given informational entity on the ice side “is about” some biomedical concept on the bio side. there are three relations from the iao that are used to make these connections: iao:denotes is the most frequently used one, indicating that a given ice exists for the sole purpose of identifying a given bio concept ; iao:mentions is a weaker relation stating some part of the ice denotes the bio concept ; and iao:is_about is the parent relation of the two, encompassing a more general sense of aboutness. the vast majority of these ice-to-bio links are iao:denotes assertions that are generated to link identifiers to their denoted concepts. rules can also assert an iao:mentions link between a record from which information was retrieved to the bio class that was dynamically created in order to represent that the class was based on information in that record. these links across the ice-bio divide serve as a primary source of identifying provenance data for biomedical concepts.

the clear separation and explicit representation of source data and biomedical concepts provide several distinct advantages. the availability of a common biomedical model allows queries to be written in terms of biomedical concepts, not in terms of information artifacts or database structures with which the user must become familiar, and queries over the biomedical modeling will not have to change if new source information is mapped to existing biomedical concepts and assertions. source information that has not yet been mapped to biomedical-concept-based representations can still be queried given that it is first represented as information content entities on the ice side. furthermore, explicitly representing the source data makes it available as provenance for the biomedical representations, enabling queries of source evidence for given biomedical assertions .

ontology-based representations
kabob is modeled using representations from existing prominent biomedical ontologies created and maintained by core developers with significant community input. we rely on prominent open biomedical ontologies  as a framework. the species-specific biological sequences and their information from source databases that comprise kabob are placed within this framework. when necessary we extend existing ontological classes with dynamically generated composite classes . these composite classes are formally defined in terms of explicitly represented obo classes, carefully maintaining the owl “all-some” quantification among related classes. the large majority of kabob assertions rely on relations from the relation ontology   <cit> , those used in the obo cross-products effort  <cit> , and natural extensions of the latter. all of the ice representations in kabob are types of information content entities that we have modeled as extensions of the information artifact ontology   <cit> .

our use of ontological concepts and relations already explicitly represented along with the dynamically generated composite concepts formally defined in terms of these existing concepts allows us to precisely capture biomedical knowledge, including content of source databases, to arbitrary levels of complexity. using and extending prominent obos as a framework enables sophisticated reasoning over the content of kabob. most straightforwardly, a plethora of deductive inferences can be made based on the ontologies’ fundamental taxonomic hierarchies, non-taxonomic linkages among their classes, and the formal definitions of the active obo cross-product efforts. this approach also opens the door for research into inductive and abductive reasoning methodologies. furthermore, reliance on these obos facilitates interaction with the enormous amount of data annotated with them and with other resources making use of them. this in turn makes it easier to model and absorb source data into kabob as well as easier for users familiar with the community ontologies to interact with kabob and to formulate queries and understand their output. the representations in kabob are biased to be event-centric, as they are easier to represent with all-some restriction semantics. however, entity-centric representations are created for simplicity in some cases, e.g., assertions linking proteins to the genes that code for them . event-centric representations could be translated into entity-centric classes and assertions; however, to conform to all-some assertional semantics, this requires a somewhat more roundabout representation so that only the entity’s potential to participate in a particular event is represented as opposed to stating that all instances of a given entity necessarily participate in it.

identity resolved across data sources
in order to integrate data from multiple data sources it is essential to understand which identifiers across the sources fundamentally refer to the same things. this is complicated by the fact that data sources often use their own source-specific identifiers to avoid external dependencies that could cause problems in their curation efforts. fortunately, mappings are often provided across data sources. sometimes these mappings provide one-to-one mappings specifying identity; however, they are often provided as more convoluted sets of “related” identifiers. great care must be taken when processing these mappings .

in kabob, each identifier in a set, all of which denote the same biomedical concept, is directly linked to this single shared biomedical concept. we chose this approach as opposed to the alternative of modeling assertions from each data source individually on the bio side of kabob and then connecting the bio entities using owl:sameas assertions. the alternative is more opaque and would be difficult to navigate for rdf approaches that do not make inferences over owl:sameas assertions. even for some systems that handle owl:sameas the alternative approach dramatically increases the number of triples and could result in intractability. our method of first generating an identifier set and then linking to a single corresponding biomedical concept alleviates these problems. the identifier sets are also useful in and of themselves when querying the data source records without having to travel in and out of the biomedical representations.

by resolving identity across data sources, kabob alleviates one of the most critical  <cit> , time-consuming, and redundant steps  <cit>  in integrating data from multiple sources. systems that do not do this require users to maintain mappings across sources in every query, dramatically increasing the complexity of the query and creating ongoing maintenance problems. in kabob a set of trusted high-quality mappings is applied first, and then the unified entities are used to aggregate data from multiple data sources. since the mappings are all managed and extracted using explicitly represented sets of declarative rules, it is easy to produce or maintain alternative mappings or to recompute the unified entities using a different set of trusted sources in the event a user believes that a different set of sources should be used as the basis for the mappings. this can be done by either adding rules or deactivating existing rules during kabob generation. the subsequent steps to building kabob would then proceed as normal, connecting representations to the alternate entities, and existing queries targeting the biomedical representations could still be issued without the need to be altered. resolving the identities of data-source-specific identifiers, aggregating them into sets, and linking them to biomedical concepts they denote in common is the essential foundation that enables the querying of kabob in terms of the shared biomedical modeling rather than having to perform queries in terms of  data-source-specific representations.

tolerance for inconsistent source data
kabob tolerates inconsistencies in assertions among disparate data source records on both the ice and bio sides of the knowledge base. on the ice side of kabob, source data records are modeled as independent informational entities . conflicting assertions would each be independently represented as different records. this captures the fact that one of the assertions was made in its corresponding record and the other assertion in the other record, and these two assertions may be conflicting or not if integrated. note that our methodology also alleviates the issue of assertional inconsistencies within a given data source. while the semantics of conflicting assertions within most of these data sources is ambiguous, these assertions are modeled the same way as if from separate data sources, with one assertion within one record and the other assertion in the other record, all explicitly and clearly represented as informational entities.

conflicting assertions extracted from conflicting source records can also be modeled on the bio side without generating an inconsistency. this is enabled by generating dynamic subclasses for every assertion being modeled in the bio portion of the knowledge base. this minimizes the places at which two assertions can conflict since each new assertion from a source database is represented as a subclass. as such each assertion is essentially in its own “world”, all contained in the kabob open world. these sets of subclasses can then be aggregated in multiple ways as discussed in the following section, and demonstrated in the subsequent example query. since there is no guarantee that data from multiple sources, or even within one source, is necessarily consistent  this is something that data integration systems, especially those using formal logics  must necessarily address. failure to do so could result in an inconsistent  or erroneous knowledge base. by modeling representations with an extensive amount of subclassing we create an environment where the inevitable inconsistent or erroneous assertion will not ultimately result in an inconsistent knowledge base. by following this precedent when new sources are added this environment is maintained, creating a stable environment for the ongoing integration of data.

representation at multiple levels of abstraction
for some tasks biologists care greatly about distinctions between corresponding biological sequences, e.g., genes versus gene products, reference sequences versus variants, species-specific sequences versus homologs; for other tasks, the distinctions are unimportant and so corresponding sequence types can be aggregated into more collective types. kabob provides a flexible knowledge model capably of representing the full spectrum of sequence type abstractions. representations such as the collective class of genes, gene products, and variants can provide this freedom and are needed when a particular source curates a given type of data at a high level of generality. for example, with regard to a drug-gene interaction assertion in a source database, while it is possible that the given drug directly interacts with the specified gene , it is much more likely that it binds to one of its products, though which product  may not be specified at this level of curation; furthermore, it may not be specified whether the interacting entity is a reference sequence or a variant. . we can properly model such a curated interaction by making use of a sequence abstraction class, asserting that there is an interaction between the drug and the gene or one of its products, either in the form of a reference sequence or a variant.

certain abstractions are necessary for the current modeling being done in kabob and have already been explicitly modeled and used; for example, for every species-specific gene class, we have also created a corresponding abstracted class of the gene and its gene products, which subsumes the gene class and all of its products. other abstractions could easily be created and layered on additively; for example, for species-specific gene classes, we plan on creating corresponding abstracted homology classes, i.e., the class of a given species-specific gene and all of its homologs. this would allow consistent modeling of situations in which the precise identity of the homolog  is ambiguous in the source information. user-defined abstractions could be layered on as well without affecting the underlying data. these abstractions can be formally defined as union classes in owl, and the rule engine can be used to generate all of the specific classes . since these abstractions are formally defined using ontologies and generated using rules that can be traced all the way back to the records used to trigger the rules, it is completely unambiguous what is meant by any given abstraction. no implicit assumptions are made about what is being modeled. this is not true for systems that provide weaker definitions of the abstractions they employ.

in addition to sequence-based abstractions kabob also supports varying granularity in process-based representations as well. multiple rules can be applied to the same underlying data to generate representations at multiple layers of abstraction. for example, it is possible to look at a pathway database that associates proteins with a pathway and initially model that as a pathway which obo:has_participant those proteins; this would model the pathway at a high level of granularity. if that pathway database also modeled the interactions that make up the pathway or the processes that inhibit or enable other processes, subsequent rules could be written to extract and model that information at finer levels of granularity. all of these rules and resultant triples can exist simultaneously and without conflict.

representing biological sequences and processes at multiple levels of abstraction enables users to choose a level of specificity or generality for a given query; in fact, different parts of a query may be specified to operate at different levels of abstraction. further, it requires queries to be explicit when moving between levels of abstraction and thus highlights where a query might be making non-deductive steps. this rightfully places decisions on where to make inferential leaps or how to leverage abstractions on the users and tools that need these inferences and abstractions. kabob makes no preferential commitment to any one level of abstraction or granularity and allows data and queries to exist at multiples levels simultaneously.

stable and reusable uris
when generating assertions in kabob, many new entities must be explicitly represented, including informational entities such as field values and records as well as biomedical concepts such as proteins and interactions. owl also requires the generation of anonymous identifiers such as those for dynamically generated owl:restriction classes that are commonly modeled using rdf blank nodes. in order to provide consistency and stability over time, uris for these entities are deterministically constructed using sha- <dig> hashes of the values that functionally define these entities; for example, a specific field value of a specific field is deterministically defined by the field name and its value. such hashing provides stable, unique, and consistent uris for these identifiers, and by using a one-way hash function we avoid encoding data into the uris, thus complying with the well-established guideline for uris to be devoid of implicit meaning. the stability of the uris over time supports debugging by consistently regenerating content the same way every time, and future work will leverage these uris to monitor changes in data sources over time.

these hashed uris further enable significant savings in terms of the number of triples by allowing representation to be shared in both ice and bio content. this is most notable in the sharing of field values and owl restrictions; however, it also allows different databases that refer to the same biomedical concept, e.g., a protein-protein interaction, to point to the same class. these connections are made without having to look up an existing uri or even know if there is an existing one, as identical uris will be minted every time. not having to remember or look up potentially existing uris enables efficient parallelization as well.

although a hashing collision is theoretically possible with this approach, sha- <dig> should provide more than enough space  to avoid it empirically. the best known theoretical attack on sha- <dig> requires  <dig>  hashes to identify a collision  <cit> . this is further mitigated by the fact that the hashed uris have other components as well, e.g., source-specific namespaces. a collision would have to happen within one data source or one type of thing represented in the bio content, making an extremely unlikely event even more unlikely. there is no foreseeable need to cryptographically secure kabob uris; however, if such a need arose, sha- <dig> or sha- <dig> hashes could be used instead at the cost of longer uris.

traceable provenance
provenance can be tracked in two primary ways in kabob. most directly, concept-level provenance can be assessed via the iao links that connect ices to bio concepts, e.g., an iao:denotes link between a protein id ice and a protein, or between a protein-protein interaction record ice and the corresponding interaction concept in the bio part of kabob. thus, the informational source of any bio concept is directly accessible by simply querying for the triple linking the given ice to its denoted biomedical concept.

the second method for acquiring provenance is by running rules “backwards”. every triple in kabob is from one of four sources: owl ontologies, rdf built from the data sources, rdf output from id set generation, or the output of rules that use the other available triples. every triple can thus have its provenance dynamically re-derived as a set of source records  and rules that created it. note that triples in the bio part of kabob can actually have multiple sources of provenance. for example, it is possible that two different rules and two different sources of drug-gene interaction information lead to the same set of triples in the biomedical representations. . in addition to this provenance, records themselves often have data-source-specific provenance, e.g., the pubmed id of the publication serving as the evidential basis for the creation of a given data record. these provenance data are mirrored in the record ices and are readily accessible for querying. future work could include writing rules to extract and represent this type of provenance more explicitly and consistently by generating ice-based links between the pubmed ids and corresponding records. by directly asserting concept-level provenance assertions and by declaratively representing the transformations that generate assertions, provenance in kabob is completely transparent and unambiguous. this is in contrast to other existing systems that bury these transforms and the representational choices they make in procedural code , which is then generally opaque to users and reasoning systems. the approach taken by kabob affords a more direct ability to inspect and update provenance and resolve potential modeling errors when they are uncovered. errors resulting from bad source data can also be traced back to their origins, some of which we have reported to their curators.

following up on gsea results; a case study for using kabob
one common type of biomedical experiment is to run a high-throughput microarray analysis and compare transcription levels in case and control groups to get a list of differentially expressed genes. researchers will then often report the results of gsea   <cit>  on that list. frequently this is where the analysis ends; however there are numerous follow-up questions the researchers and readers likely have. kabob can be used to answer these questions.

for example, choi et al.  <cit>  were interested in changes in mitochondria in mice bred for high and low fear. they showed that the genes differentially expressed in the prefrontal cortex  were enriched for the process of oxidative phosphorylation. the authors conclude their discussion by stating that “a better understanding of the genes associated with the mitochondrial function in the pfc may provide an opportunity to identify a novel drug target for the treatment of mood and anxiety disorders.” natural follow-up questions to these results are: “which genes/gene products in humans, are localized to mitochondria, involved in oxidative phosphorylation, and are targets of drugs? and what are these drugs?” these questions can be readily asked and answered with kabob. for example, the following sparql  <dig>  query retrieves drugs that bind to gene products that are localized to mitochondria and are involved in oxidative phosphorylation:

the query is broken into five major lettered blocks, a-e. section a queries for the restriction of all things that have been found to localize in mitochondria, finds the corresponding localization events, and retrieves the gene products found to have localized there. for a given gene product, section b retrieves the corresponding gene-or-gene-product aggregate class, and section c queries for which of these gene-or-gene-product classes pertain to humans. section d then returns to the gene-or-gene-product aggregate class and first retrieves all of its subclasses, including itself. note that to effectively use the aggregate classes for every biomedical assertion one must first query for all the subclasses of the aggregate. this allows for all of the subclasses to be queried as an aggregate; if instead a variable was reused across clauses, that query would effectively be asking for one specific gene product known to satisfy all conditions . section d continues by querying for all things in which these gene products participate, and then selecting only those that are a subclass of oxidative phosphorylation. section e again gets a new subclass of the aggregate abstraction, then retrieves all events in which it has been found to participate, finds other participants of these events, and then makes sure that the second participant is realizing the role of  a drug in that interaction. finally, the query retrieves the names of the resulting drugs.

this query highlights some of the benefits of the kabob representations discussed in earlier sections. sequence abstractions are utilized, as seen in section b of the query. the extensive use of subclassing is also evident throughout the query. for example, section a is looking for the subclass of localization, that is the subclass of all things that result in localization to the mitochondria, and then queries for the other parent classes that provide information about what is being localized. this query also highlights the fact that the query is asked only in terms of biomedical concepts , with no informational entities  ever referenced.

with only pharmgkb integrated as a source of drugs and drug targets, the query above returns the two drugs adefovir dipivoxil and tenofovir. by extending the select statement to include the variable ?gorgporv  the query is modified to return gene-drug interacting pairs, instead of just the drugs. executing this query shows that both drugs interact with the ak <dig> gene or its products.

after writing three rules to also integrate drugbank into kabob using the same common biological representation, the query can be run without changes and the results are extended to  <dig> genes that collectively interact with  <dig> different drugs. these eight genes include two from the pfc gene list in choi et al. , two more that are mentioned in another gene list in choi et al.  and four that appear in neither gene list .

looking up all the drugs that interact with the pfc gene list produces  <dig> potential compounds. running the query above produces a far more targeted list of  <dig> compounds. the five in bold do not occur in the list of  <dig> and thus are unique to this approach; this is due to the fact that kabob can identify other potentially relevant genes that were not in the experimental results that are drug targets, in this case genes involved in oxidative phosphorylation that have been found to localize in mitochondria that were not in the experimental pfc gene list.

-3-anilino-5--5-methyl- <dig> -oxazolidine- <dig> -dione

1-acetyl-2-carboxypiperidine

2-hexyloxy-6-hydroxymethyl-tetrahydro-pyran- <dig> ,5-triol

2-nonyl-4-hydroxyquinoline n-oxide

5-heptyl-6-hydroxy- <dig> -benzothiazole- <dig> -dione

5-n-undecyl-6-hydroxy- <dig> -dioxobenzothiazole

adefovir dipivoxil

aurovertin b

bis-5’-pentaphosphate

cholic acid

desflurane

enflurane

famoxadone

halothane

isoflurane

methoxyflurane

methyl -2-pyrimidin-4-yl]oxy}phenyl)-3-methoxyacrylate

methyl -3-methoxy-2-{2-phenyl}acrylate

myxothiazol

n-formylmethionine

n1-octahydro-pyrrolo pyrimidine

piceatannol

quercetin

sevoflurane

tenofovir

ubiquinone-2

the purpose of this example is not to demonstrate novel biomedical findings but instead to illustrate how a complex query expressed exclusively in terms of biomedical concepts, without having to know underlying database schemas, can be formulated and issued against our integrated knowledge base. however, even a brief search of pubmed with the results from this example shows biomedical relevance of the results of the example query. for example, research has been conducted looking at the effects of isoflurane and desflurane on mitochondrial function and cognition  <cit> .

current limitations and future work
the primary limitations of the kabob methods and knowledge model are related to issues of scale. these limitations include the size of the original data source files and the number of records they contain, limits on the number of triples a given machine or piece of software can store and query, and limits on the ability to query for triples that are entailed but not explicitly represented. fortunately all of these issues are major and active areas of research and development in the semantic web community and by product vendors, so in addition to our attempts to mitigate them, it is likely that they will continue to be externally addressed.

storage of the record rdf for the all-organism version of kabob takes approximately 140 gb per version. updating kabob monthly and preserving legacy data would require nearly 2 tb of space per year. part of our ongoing research is developing a methodology to require keeping only differences to historical data instead of complete copies, hopefully greatly reducing storage requirements. this is potentially feasible due to our use of a common underlying representation for records from all source databases and sha- <dig> hashing to consistently name uris that we are generating, which collectively provide a high level of consistency across versions and over time.

early steps in the kabob build process are possible using the full  data set; however, more complicated queries later in the build process were taking inordinate amounts of time on our hardware/software combination and so have been temporarily placed on hold while we work on moving to a larger machine. triplestores tend to scale as a function of memory, and we believe an all-organism kabob can be computed on a larger-memory machine. our collaborators at franz have run some of our test queries successfully on a 384 gb ram machine . machines with 500 gb or even  <dig>  gb are becoming more common at supercomputing facilities, such as those being built at the university of colorado, and cloud-based environments such as amazon’s ec <dig> currently rent access to 244 gb machines. we will also continue to explore other potentially large but bounded subsets of the data, e.g., all eukaryotes. additionally, triplestores are continuing to improve, and the hardware needed to run them is decreasing in cost. we are operating on the order of  <dig> billion triples with hardware costing less than us$ <dig> . distributed triplestores are an ongoing area of research, and experimental systems have been successfully fielded on  <dig> billion triples already.

kabob satisfies the data modeling desiderata for integration put forward by . these papers also discuss what is required to have a more open and federated system of data access, but this is beyond the scope of our work. common models for identity, semantics, and provenance are prerequisites for such a federated system, and these are some of the problems kabob addresses. the knowledge representations in kabob lay the foundation for how to integrate data using the obos such that data from disparate source databases are interoperable, and this work should be equally applicable to any future attempts at federation. future work could potentially enable multiple independent end-points participating in a federated kabob.

reasoning at scale can also be problematic. kabob representations are predominantly currently modeled in owl-el  <cit> , which can be computed in polynomial time, though we have yet to attempt to run a classifier over billions of triples. . there is also ongoing work by triplestore providers to perform inference and materialization faster and for more complicated inferences  <cit> . thus far we have taken advantage of sparql  <dig>  property paths to reason through transitive properties ; the performance of doing so varies by triplestore. with some tuning to query ordering, allegrograph can navigate these paths in satisfactory time, with the slowest queries running on the order of minutes to tens of minutes on our hardware. alternately, transitive properties could be materialized into the triplestore in order to get better performance. given the way virtuoso currently implements property paths, such a step would be necessary to query with multiple paths. not all inferences can be accessed with property paths alone, e.g., entailments from transitive properties or subclass hierarchies nested in owl restrictions. we are interested in using owl-el reasoners such as elk  <cit>  to attempt to make these queries tractable. other alternatives include using datalog forward chainers to materialize the necessary triples, such as the allegro graph materializer  <cit>  or rdfox  <cit> .

querying owl using only sparql can be tedious, but this is also improving with time. sparql  <dig>  provides significant improvements over  <dig>  via property paths. other apis for querying triplestores and interacting with owl are also being developed by the community. we are interested in developing other apis for common tasks and queries, or extending our existing domain-specific languages to alleviate some of the strain. the rule language for kabob, for example, makes available several macros to significantly reduce the number of boilerplate triples humans have to produce when interacting with record ices.

as reasoning and query capabilities continue to improve we are also interested in developing tools for maintaining and monitoring the quality of kabob. this will include research into systems that query for potentially erroneous assertions in kabob, trace their provenance, and report on the collective set of rules and source records used to create the incorrect assertions. prior to a specific resolution for these problems being provided by human intervention these assertions could be blocked from generation in subsequent builds of kabob or filtered from an existing kabob instance.

related work
related research on the semantic integration of data sources falls into two primary classes: automated integration investigated by the computer science community , and the more manual but domain-specific research conducted by the bioinformatics community.

automated database integration
the problem of integrating multiple databases has a history almost as old as relational databases themselves; doan and halevy  <cit>  review this work and provide links to older reviews on this topic. research in the database and artificial intelligence  communities on automated data integration falls into two main categories: schema matching and data matching. schema matching focuses on mapping the schema for one database to another, e.g., figuring out that the “surname” column of one database is the same as the “last name” column of another. data matching is performed using actual field values, e.g., finding that a record in database a has the value “livingston” and database b also has the value “livingston” for a given record, and so on for the other values of the records.

work on automated matching is complicated by the fact that schemas often model data at different level of abstraction and there is not necessarily a one-to-one mapping across databases. for example, one schema might have separate fields for street address, city, and state, while another schema might represent all of that data in an integrated address field. these complex matches can be difficult to identify, as increasing the number of fields that can be combined along with the ways in which they can be combined can result in a combinatorial explosion. when it comes to tuple matching there is the question of whether or not two nearly identical tuples represent duplicate or different data. there has also been growing evidence that there may not be one universally correct match, but rather that mappings are application-dependent  <cit> . domain ontologies are also being used as a backbone for mapping multiple database schemas together  <cit> . kabob maps representations to ontologies allowing for multiple levels of abstraction to be represented simultaneously. these mappings are also currently produced manually, allowing knowledge engineers to use all available background knowledge  to generate the matches to the obos.

in the development of kabob we have opted for a manually built rule-based approach to schema matching, converting the matched data into a common model grounded in prominent biomedical ontologies. while our approach is potentially slower than automated approaches, there are a finite number of rules to be produced. future work could involve exploring the use of automated mapping techniques or human-computer hybrid techniques to accelerate the process. however, the primary bottleneck is still creating the target knowledge representation for a new class of biomedical information; after a new such representation is constructed, a second source of the same type of information can be more easily built, copying from the first.

doan and halevy  <cit>  state the core problem of semantic integration is identifying if any two elements refer to the same real-world concept. in kabob we explicitly model these mappings in terms of identifier sets and other iao:denotes links, as well as tracking the provenance of other concepts through declarative rules. provenance and explanation of matches is not always provided by automated database integration systems, although it is also being researched by the database community  <cit> .

biomedical database integration
interchange languages, such as biopax  <cit> , gaf <dig>  <cit> , and psi mif  <cit>  provide a common way to represent data and are playing a growing role in the biomedical data ecosystem for sharing data. these data formats have done much to increase the level of data sharing in the community; however they are generally domain- or task-specific. larger database integration projects also exist, such as biomart  <cit>  which provides the ability to query across multiple biomedical data sources. while this provides a common interface, queries are still required to know which sources they wish to query and how the data is organized in those individual sources. much work has gone into producing database integrations with various configurations and goals; louie et al.  <cit>  provide a review-level discussion of several existing biomedical data integration systems. the remainder of our discussion will focus on ontology-based integration of biomedical data, as this is most related to kabob.

semantic integration and querying of multiple sources of biomedical data dates back at least to tambis  <cit> , which attempted to answer queries without the user having to know which data sources were necessary. more recently, there has been a push to integrate biomedical data with the semantic web  <cit> . this includes work in building canonicalized and stable uris  <cit>  as well as work on publishing and disseminating data and recording their provenance  <cit> .

bio2rdf  <cit>  is the most prominent project for providing access to rdf versions of many existing biomedical data sources. it provides canonical identifiers, and although it does not attempt to explicitly assert sets of identifiers that point to the same entities, it does provide access to identifier mappings; in this way it is primarily analogous to the ice portions of kabob. additionally, bio2rdf does not attempt to align representations to a common biomedical model, thus making querying across data sources more difficult and convoluted in that query writers must navigate all relevant component database schemas themselves. bio2rdf generates representations using source-specific models and is primarily focused on facilitating “mash-ups”. given this focus, bio2rdf has weaker constraints on its representation and sometimes conflates things that should be semantically distinct. for example, the uri for glutathione in bio2rdf  is explicitly asserted as rdf:typehttp://bio2rdf.org/drugbank_vocabulary:drug and implicitly as foaf:document . these representational inconsistencies create a weak foundation if not real problems for research that wishes to build upon it for semantic reasoning purposes. while it may be possible to build kabob on top of something like bio2rdf data, this remains an open question and entails risks. we believe our ice representations provide a maximally general, and representationally sound, foundation for kabob.

subsequent work points out that “querying bio2rdf remains difficult due to the lack of uniformity in the representation of bio2rdf datasets”  <cit> . their work attempts to resolve this problem by aligning the source-specific schemas of bio2rdf to the semanticscience integrated ontology . they acknowledge that the sio is limited in its coverage and needs to be extended, thus introducing a significant additional ontology development problem. the manual mappings done by callahan et al.  <cit>  require one-to-one relations between the source classes and properties in the bio2rdf ontologies to those in the sio, which they acknowledge are often imprecise matches to high-level concepts.

kabob makes very explicit the differences between database entities and biomedical concepts, unlike bio2rdf. its shared biomedical representations are grounded in prominent, actively maintained, large-scale open biomedical ontologies. compared to the sio they have much greater coverage, greater consensus among the community, and are already used directly by many prominent curated databases, greatly simplifying the mapping problem. kabob also performs the translation from implicit database content to explicit biomedical representations using declaratively represented forward-chaining rules that are capable of dynamically constructing new entities. in contrast to approaches that only align ontology terms, this allows mismatches between the abstractions implicitly used for curation in the databases and those in the obos to be more easily overcome, while still recording provenance. when a rule  fails , it is also easily detected as the output of the rule will produce zero triples, unlike mappings provided in hand-coded ontology files. neurocommons  <cit>  is another ontology-based knowledge base aggregating biomedical information from a range of sources. precise identifiers for records that maintain the distinction between entities such as genes and gene records have been carefully created, and they are used for provenance of biomedical assertions, but, unlike kabob, the content of the records is not directly modeled. record content that has not been mapped to a common biomedical model in neurocommons is computationally inaccessible, and the mappings to the model are performed via procedural code. in kabob, record data is available for computational systems in the ice portion of the knowledge base even if rules that map these data to biomedical representations have yet to be written or run. furthermore, kabob’s declaratively represented rules can also function as provenance for concepts in the common biomedical model.

there have been several other approaches to semantic integration of biomedical data. early work by ruttenberg et al.  <cit>  discussed the need for uniformly structured data across domains in order to advance translational research. they further discuss how the semantic web might provide a platform for such a taks. biogateway  <cit>  was an approach that aggregated a large quantity of data using a mixture of obos and custom ontologies. like most earlier work it did not make a distinction between ice and bio content as does kabob nor does is integrate overlapping content. the work by marshall et al.  <cit>  also has similar goals to kabob; however it stops short in many key areas, leaving as opportunities for future development problems that kabob resolves, such as how to represent and integrate concepts representing information artifacts and how can they be used to provide provenance. it does solve other problems in ways that kabob has also adopted, such as normalizing the identifiers in source records. more recently work by hoehndorf et al.  <cit>  demonstrates how integrating data from multiple sources using owl can support very complex querying. they integrate several types of data that kabob also provides; however their model forces the use of certain abstractions, for example, it is gene-centric mapping all data to genes, as opposed to supporting multiple parallel abstractions as does kabob. their mappings are also done without making an ice-bio distinction and preserving the information entities as provenance. finally they use equivalent class axioms to merge entities in the biomedical representations, while kabob resolves these mappings at an earlier stage with identifier mappings.

other systems for indexing and querying federated sources of biomedical data have also been developed. resourceindex  <cit>  indexes data sources and their records for search by using the ncbo annotator  <cit>  to identify the ontology concepts mentioned in them. resourceindex enriches its annotations using intra-ontology information, such as the transitivity of the subclass hierarchy; and inter-ontology information, such as mappings between ontologies. nif  <cit>  is a neuroscience-specific environment for indexing and querying web pages, publications, and even databases. nif provides the ability to use ontologies to query across data sources and even into databases by providing an environment where data providers can register their data and map it to the common ontologies, and where users can issues queries that are translated into ontology concepts and federated to all participating resources. queries can be expanded to use a neighborhood of related ontology terms. systems like resourceindex and nif provide incredible querying power and specificity to users. however, these tools do not model the relationships between concepts present in records or documents, and they do not link or aggregate data across sources.

existing commercial work on biomedical data integration includes ingenuity pathway analysis  <cit> , which identifies relevant molecular networks by integrating gene expression data, gene annotations, and manually curated data from literature. other tools such as bioxm  <cit>  provide a platform for data integration using a custom knowledge model and provide access for querying of these data. in addition to its reliance on large, prominent biomedical ontologies developed with significant community input for the structuring of its integrated data, kabob notably differs from these in being freely licensed.

CONCLUSIONS
we presented five processes that when collectively applied provide solutions to seven key semantic data integration issues. we applied these processes to  <dig> large biomedical data sources to produce kabob , an integrated knowledge base of biomedical data representationally based in prominent, actively maintained open biomedical ontologies, thus enabling queries of the underlying data in terms of biomedical concepts  rather than features of source-specific data schemas or file formats. in kabob, identity is resolved through the representation of biomedical concepts that are referred to by sets of identifiers, making no preferential commitments to any identifier space. declaratively represented forward-chaining rules take information that is variably represented in disparate independent underlying database models and generate representations in a common ontology-based biomedical model but also leave the underlying source data available for querying and provenance. these rules also function to track provenance and allow all transformations to be inspected and evaluated. common biomedical abstractions are used to take into account the ambiguity of modeling within source data and to reflect this ambiguity in queries of these data. kabob resolves many of the issues that routinely plague biomedical researchers intending to work with data from multiple data sources and provides a platform for ongoing data integration and development and for formal reasoning over a wealth of integrated biomedical data.

availability of supporting data
we provide and maintain the open source code that achieves the steps described above, from downloading source files to running the rules. usage agreements of some data sources prohibit redistribution so we cannot redistribute the complete set of triples. we welcome inquiries about specific collaborations. we intend to develop tools to expose parameterized questions for more general consumption as well.

datasources library: https://github.com/ucdenver-ccp/datasource

kr library: https://github.com/drlivingston/kr

kabob library: https://github.com/drlivingston/kabob

additional files
additional file 1: appendix a. current kabob build procedure. appendix a describes the steps and their sequence used to build kabob in more detail than is depicted in figure  <dig> 

additional file 2: appendix b. example rule. appendix b provides an example rule in the kr rule language and a corresponding discussion. this is the actual rule used to convert from go biological process annotations to biomedical concepts that is discussed in the paper and depicted in part in figure  <dig> 

additional file 3: appendix c. example queries. appendix c provides several examples of querying kabob’s integrated data using the common obo-based biomedical model.

additional file 4: appendix d. error detection. appendix d provides more detailed examples of error detection queries with kabob.

additional file 5: appendix e. drugbank identifier mapping errors. appendix e provides more detail to one of the examples discussed in appendix d.



competing interests

the authors declare that they have no competing interests.

authors’ contributions

kl was the primary architect of kabob and lead the team in its development. he was responsible for building the rule engine, writing rules, and the overall integration and final construction, which included developing the kr library, and the kabob library being open sourced with this work. mb is the lead ontologist for kabob and was primarily responsible for designing the knowledge representations used to integrate all of the data, collaborated in the design of kabob, and contributed to the writing of rules. wb was primarily responsible for the data acquisition and rdf-ization, which included developing the file parser library being open sourced with this work, additionally he contributed to writing rules. lh was involved in the conception, overall design, and planning of the project. all authors contributed to the writing of this manuscript. all authors read and approved the final manuscript.

