BACKGROUND
the scientific value of multilevel, multiscale, computational, biomedical models will be greatly enhanced by making them broadly available and sufficiently manipulable to address a variety of scientific questions at reasonable costs, regardless of the hardware at the researcher's disposal. the availability of cloud technology opens the door to that eventuality. however, such models are analogous to an entire, specialized, wet laboratory. as with wet laboratories, insuring scientific equivalency of duplicate experimental systems in different laboratories is a necessary precondition for placing confidence in the results of experiments arising from those laboratories. a goal of this project was to test the scientific equivalence of experiments conducted using multilevel, multiscale, in silico livers  executed on a local cluster with those executed in the amazon ec <dig> cloud platform. equivalence demonstrates that validation results obtained on the local cluster still hold in the cloud. further scientific work on isls executing in the cloud can build upon that work. by executing isls in a cloud, we gain more computational power to develop more detailed, larger scale, larger scope isls, while outsourcing systems administration and hardware maintenance costs.

the isl illustrated in figure  <dig> is not intended to be a finalized model having a fixed structure. it is designed to facilitate exploration of plausible mechanistic explanations for observations related to xenobiotic disposition  <cit> . the validation aspect for these experiments is the outflow profile from a single-pass rat liver perfusion experiment  <cit> . however, concrete instances of multiscale validation by adding finer grained aspects have been reported  <cit> . an isl is a biomimetic analogue designed to help formulate and challenge mechanistic hypotheses about the hepatic disposition of xenobiotics in health and disease. it is an assembly of abstracted components representing aspects of hepatic form, space, and organization interacting with compounds . it has its own unique phenotype, which is necessarily simpler than that of the entire set of lobule mechanisms. because of the stochastic nature of isl simulations, each in silico experiment-constructed from several monte carlo  lobule samples-generates a slightly different outflow profile.

the modeling and simulation methods used  <cit> , described in methods, are intended for representing large-scale biological systems in silico. they require the computational ability to represent any layer in a hierarchical biological system well enough to falsify  its outputs against corresponding wet-lab observations, and to extend the representation, at will, to any other phenomena necessary to achieve falsification or validation. an isl cannot be constrained by the use cases or aspects by which it was originally constructed. it must be arbitrarily extensible. it is usually impractical to expect isls and similar models to execute in a sequential computing context, e.g., using a single cpu. that is because the depth and scale of most biological systems leads to models that are computationally and analytically insoluble. inductively derived, equation-based models are more amenable to execution on sequential machines because of their high degree of abstraction from the particulars of the referent mechanisms. at the other end of the computational spectrum, massively parallel machines are also impractical because the solutions that can be implemented are very tightly engineered to match exacting specifications; hence a massively parallel solution is specialized and limited in the extent to which it can propagate  <cit> . in addition, as is evident in figure  <dig>  the behavior of each lobule is tightly coupled to the behavior of all lobule components. hence, distribution of the lobule sub-components over multiple, parallel processors would likely result in a very high communication-to-computation ratio, making it inefficient for that type of fine-grained parallelism. in contrast, the salient interactions between lobules, for outflow profile validation data, can be abstracted parsimoniously to lobule input/output: at the portal vein tracts  and the central vein. it is a natural fit to run a single lobule on a single cluster node. for other analogues, we can, for example, envision running single cells on single cluster nodes. moreover, as demonstrated in  <cit> , highly specific intra-lobule mechanisms can be studied in a very concrete and particular way on a single cluster node. cluster type machines meet isl parallelism needs fairly well and have provided the isl's infrastructure  <cit> .

the combination of the liver's homogeneity at a coarse grain and the heterogeneity that is evident at fine grains makes it a good referent system for isls built atop cluster parallelism within a cloud. there are three model- and referent-independent reasons for exploring using a cloud infrastructure for cluster style parallelism: 1) reduced systems administration costs, 2) model expansion, and 3) model sharing. isl uses provide two additional motivations: 4) isl experiments need many lobule samples, and 5) micromechanistic details need to be assessed.

although significantly lower cost than biological modeling projects relying on massively parallel machines, those based on local clusters can still present a laboratory with relatively high administrative costs. there are temperature, power, maintenance, and manpower considerations. the resulting constraints limit the extent to which methods suited to cluster computing can be propagated, repeated, and leveraged by others. local constraints also place practical limits to expanding an analogue's scale and scope.

the advent of publicly accessible, on demand, platform level, cloud computing infrastructures, like amazon's ec <dig> raises the accessibility of large scale, expandable, biological models so that anyone with a broadband internet connection can build virtual clusters and execute such models on demand. they also greatly expand the diversity of in silico, biomimetic devices that can be constructed. the ec <dig> platform is an interface to an executive web service that allows the on-demand instantiation and management of virtual computers running any of a suite of operating systems that amazon supports. it is distinguishable from other "cloud" technologies in that platform resources are customizable. as an extreme contrast, google documents is also a cloud technology. however, it only allows the user to create particular types of documents and host them remotely on pre-configured servers. at the other extreme, ec <dig> allows one to instantiate entire virtual machines, running windows or linux. there is no limitation on the type of software you can run, unlike with google documents. the spectrum of cloud technologies is covered in some depth in  <cit> . to date, business, collaboration, and data processing dominate the use cases for cloud infrastructures. although desktop grids have contributed to some scientific domains like molecular dynamics  <cit> , recruiting volunteers to allow an application to execute on their computers constrains the researcher in ways that on-demand services do not.

the third and possibly more powerful justification for exploring cloud computing is the potential for sharing models more easily. often, a simulation is dependent on a particular technology stack that is not widely available or easily configured. with a platform level service like the ec <dig>  virtual machine instances containing the entire technology stack can be shared. when researchers want to examine more than just the published information, they can instantiate a grid of virtual machines, design and execute their own experiments, or simply repeat those of the original laboratory. we can anticipate that, as models and their uses grow and the computational platforms upon which they run scale upward, issues of reproducibility will grow. methodological details of virtual laboratory assembly and in silico experimental protocols will become as important for computational scientists as they are for scientists experimenting in different wet-labs.

because each out-flow profile exhibits high variation, it takes averaging many lobule samples to approach the smooth outflow profile exhibited by liver perfusion experiments. a benefit of having an ever-larger number of lobule samples when needed is that the lobular variety within a liver can be approached: an adult rat liver is comprised of a few hundred lobules. finally, when using the local eight-node cluster , there is a trade-off between abstraction layer for the fine-grained micromechanisms and mc sample size: assessment of the micromechanistic details of the type described in  <cit>  requires larger sample sizes-more lobule samples-to clearly identify event trends.

isls are designed for systematic, iterative revision and easy alteration to improve realism and enhance heuristic value. following revision, the new isl must be re-parameterized and re-validated using the same criteria used for its predecessor. addition of new features must not compromise any previously validated behaviors. planned isl uses required adding to the most recent isls  <cit>  a biliary elimination and flow space, an improved metabolism mechanism, and enabling an alternative method of dose input. those features were not needed by earlier isls. we implemented an isl that included those features . observations made during the initial re-validation experiments indicated that we needed to scale experiments to include larger numbers of mc samples. that situation provided an immediate, convenient impetus to move isl simulations to a cloud. results documenting the benefits of such scaling are presented herein. in order to migrate successfully to a cloud, we must demonstrate experimental indistinguishability  between results from similar size cloud and local cluster experiments. results of such experiments are presented for the re-validation experiments for isls that included the preceding new features.

methods
middle-out and synthetic modeling and simulation methods
isls use middle-out  <cit>  and synthetic  <cit>  modeling and simulation methods. they are specifically intended for representing large-scale biological systems in silico, and are well suited for simulation executions using cloud technology. middle-out models are a compromise between the long-standing dichotomy between bottom-up and top-down models. top-down models begin with a high level construct, usually a phenomenon to be generated, and specify its constituents purely from the perspective set by that high level construct. for example, the navier-stokes equations for fluid flow are a top-down model because they describe, directly, the aggregate fluid properties of the material being studied, and do not delve down into describing the properties or behavior of the molecules that generate the behavior. bottom-up models, in contrast, leave the high level aggregate properties of the system unspecified and directly describe the properties of the bottom-most, atomic , constituents of the system. for example, a bottom-up model of a fluid would merely describe the molecules and their local interactions without making any attempt to describe the fluid, gas, or solid phenomena at the higher level. both top-down and bottom-up approaches to modeling implicitly assign an aspect or perspective to the system. that perspective is not necessarily an objective, independently correct perspective. there are layers above any top-down model and layers below any bottom-up model. the modeler chooses where to "pin" the base perspective of the model. middle-out models accept this a priori perspective and force the modeler to make the model explicit about the aspect chosen as the base. extensions to the model are then explicitly allowed to go "up" or "down", depending on the requirements set for the uses to which the model will be put. when those layers need to be accessed, it will be easy to do so when using a cloud cluster.

another spectrum that can be used to describe models and that influences choice of simulation framework is that between inductive and synthetic modeling. inductive models are derived by abstracting generic properties from many particular situations or objects. synthetic models, by contrast, are constructed or pieced together from objects available to the modeler. a simple example would be a semblance of an automobile assembled from lego pieces. these differences are elaborated in  <cit> .

inductively derived models are more amenable to execution on sequential machines because of their high degree of abstraction from the particulars of the referent mechanisms; but the referent biological systems are typically highly nonlinear and particular. a consequence is that those inductive models are only applicable to more generic contexts and are inapplicable to the specific contexts that will eventually be required to advance pharmaceutical research and support clinical relevance.

isls are executed within a co-simulation  <cit>  framework alongside an equation-based, extended, convection dispersion model  <cit>  and an agent that interpolates and outputs the referent wet-lab data. the experagent manages all three simulations. running them in co-simulation allows similarities in behavior to be compared on the fly or after the execution terminates. comparisons are made via a quantitative similarity measure   <cit> . when isl observations fail to achieve a prespecified sm, one or more isl micromechanisms are considered falsified and components or parameters must be modified to generate a new set of micromechanistic hypotheses. a failure to falsify means the mechanisms as implemented represent a plausible hypothesis and we say that isl has achieved a degree of validation: it validates against the particular aspect quantified by that sm.

cluster architecture
the architecture is diagrammed in figure  <dig>  the local cluster consists of eight intel dual core pentium  <dig> based computers, one of which serves as the master node providing the message passing interface  executive that dispatches jobs to the other seven slave nodes. each node is connected to a gigabit switch over which it receives commands and data from the master node. the master node contains two network interface cards, one of which provides access to the local area network and internet, and the other to the gigabit backplane. the network file system provides file data and executable to the slave nodes. execution control and shared memory data are provided via the mpi. the cluster is a grid in the sense that all the nodes have installed the same operating system, compilers, and libraries.

each isl is launched as an executive program on the master node. event sequences are diagrammed in figure  <dig>  the executive node instantiates the experiment agent . it partitions the simulation among the slave nodes based on command line parameters and the contents of parameter files. for the experiments reported herein, one mc lobule sample is executed per slave node and the results are collected and maintained on the master node until all lobule executions finish. however, the framework also supports a coarser parallelism where whole experiments are executed on a single node. the coarser mode is useful for parameter sweeping and behavior space searching, restricted to experiments with few mc samples.

only the experagent and its utility components execute on the master node. the lobule and all the modeling components having wet-lab experiment counterparts, execute on the slaves. each lobule sample has a proxy for the experagent and the data management module that handle execution control and data i/o. it is important to note that agency is orthogonal to the cluster architecture. the other agents within an isl  execute serially with interleaved discrete event schedules using the swarm  scheduling engine.

similarity measures and co-simulation
as a lobule executes, the observations taken in compliance with the co-simulation sm are sent to and logged by the experagent on the master node. co-simulation and the sm are described in detail in  <cit> . briefly, the co-simulation framework requires that any model be executed in tandem with another, a mechanistically different but behaviorally similar model, where the measurement protocol is identical for each co-simulation model. for isls, we use two other models, a two compartment ode model and one interpolated from wet-lab experimental data. the sm provides a ± coefficient of variation band around a nominal profile derived from the wet-lab data. the sm score is the percentage of observations from the simulation that fall within that band. results of one independent wet-lab experiment are experimentally indistinguishable from another when a prespecified degree of similarity is achieved. yan et al.  <cit>  and park et al.  <cit>  specified sm scores  that must be achieved to establish experimental indistinguishability between results from independent isl experiments. when a lobule execution finishes, the experagent shuts it down and dispatches the next one. when all samples are finished, the experagent sends the raw observations to the statistics module. it calculates the derived measures that will be used to compare the results for validation or falsification. in the experiments reported herein, the statistics module merely averages the results from individual lobule executions and sends that to the data management module to write to the disk for offline analysis.

replicating cluster architecture in the cloud
in order to demonstrate experimental indistinguishability between cluster and cloud experiments, the experimental apparatus must be replicated in the cloud. because an exact replication of the grid and computation would be degenerate and uninteresting, decisions had to be made about where the cloud architecture would be allowed to differ from the cluster. amazon's ec <dig> uses virtual machines running within a farm of actual machines. this means that some or even all the virtual machines, though unlikely, might be executing on the same actual machine. hence, it is impractical to guarantee the same inter-node network as used on the cluster. likewise, even though the generic specification of the virtual machines, like amount of memory and compute capacity, are specifiable, as virtual machines, their behavior will be somewhat dependent on the other virtual machines with which they share the real hardware. there are also constraints placed on virtual machine specification by amazon's infrastructure. for example, a 32-bit virtual machine has a maximum memory of  <dig>  gigabytes, whereas the 32-bit machines used in the cluster have  <dig> gigabytes of memory. as such, the virtual machines differ from the local cluster machines in almost every way at the hardware layer. thirty-two bit virtual machine images were used in the cloud to limit precision and register size differences that might be introduced. networking is handled the same as in the local cluster. however, as mentioned above, we cannot be sure that the maximum potential data flow from one virtual node to another is a gigabit, given the allocation of hardware by the ec <dig> system. the master node acts as the network address translator, gateway, and firewall to the slave nodes. on both the cluster and the cloud, the simulation executive invokes mpi facilities that query a file on the operating system listing all network reachable nodes available. the script that starts and sets up the slave instances automatically assembles this file. the machines listed could be anywhere on the internet. nevertheless, these experiments were only run using virtual instances in the ec <dig> cloud. via mpi, the experagent is shared between the master and slave nodes, executing only the appropriate code for master or slave. the experagent proxy on each slave makes live observations for its lobule until that execution is terminated. when all lobules have terminated, the remaining experagent on the master node aggregates the data.

the technology stack for the cloud exactly mimicked that of the cluster. the operating system was ubuntu  <dig> . <dig>  the compiler gnu c/c++/objective-c  <dig> . <dig>  the communications layer mpich  <dig> . <dig>  and the simulation toolkit was the swarm subversion trunk from 2009-02- <dig>  additional file  <dig> contains instructions for setting up an isl using ec <dig>  additional file  <dig> contains the isl source code.

the local cluster maintains the state of its hard disk every time it is shut down and restarted. an ec <dig> machine image, however, does not. the image is a snapshot of a running machine and when virtual machines are instantiated from that image, the state of the hard disk is as it was when the image was created. hence, although a running instance of an image maintains changes made to the hard disk as long as that instance is running, when it is terminated, all the changes made since it was started are lost. that reality slightly changes the methods for running the isl in an ec <dig> cloud.

on the local cluster, if the mechanisms being tested are already in the isl, then all that is required to run a new experiment is to start the cluster, write the new parameters and/or input data, and execute the simulation. data written to the disk can be downloaded for analysis immediately, or left on the hard disk for later analysis. the only difference when executing in an ec <dig> is that the resulting data must be downloaded immediately or it will be lost.

when new or additional mechanisms are to be tested, which was the case for this report, then on the cluster, those changes are made to the isl source code. canonical "experiments" are executed as regression tests to verify that the changes work as expected, and the changes are checked into the repository. thereafter, the experiment proceeds as before. in an ec <dig> instance, however, because the instance comes up in the state with which it was originally created, it will have no isl testing capabilities on its disk. if new mechanisms are to be tested, the latest version of the simulation source code must be checked out of the repository to the running virtual machine instance each time a new instance is invoked.

the local cluster could not robustly handle experiments of more than  <dig> mc lobule samples. so, although the model scales to experiments requiring larger sample sizes, to scale the local cluster, we would have to add more nodes. because the cloud platform is easy to scale simply by invoking more virtual machines, the  <dig> sample experiments were possible.

in silico liver structure and components
an isl during execution maps to a mammalian liver undergoing perfusion as in  <cit> . an isl represents a liver as a large, parallel collection of similar lobules, the functional units of a liver. the earlier version, absent bilecanal and the new metabolism mechanism, is detailed in  <cit> . the following is an abridged description. components mimic essential hepatic form and function features. flow networks are represented by an interconnected, directed graph, the structure of which is monte carlo determined for each lobule. graph edges specify flow connections between sinusoidal segment  placed at each node. there are multiple, different flow paths from portal vein tracts  to cv. most functions reside within sss. a ss is a discretized, tube-like structure comprised of a blood "core" surrounded by three identically sized 2d grids , which together simulate a 3d structure. two ss classes  are specified to provide sufficient variety of compound travel paths. compounds are represented using mobile objects that move through the lobule and interact with encountered ss features. a typical compound maps to many drug molecules. a compound's behavior is determined by the physicochemical properties of its referent compound, along with the lobule and ss features encountered. multiple, different compounds can percolate through ss features during the same experiment. objects called cells  map to an unspecified number of cells. they function as containers for other objects. different colored grid locations in figure  <dig> illustrate that the features at any location can be unique. cells contain a stochastic, parameter-controlled number of binders in a well-stirred space. binders map to transporters, enzymes, lysosomes, and other cellular material that binds or sequesters drug molecules. a binder within an endothelial cell only binds and later releases a compound. binders within hepatocytes are called enzymes because they can bind compounds and later either release or metabolize it. adjacent to space c is the bilecanal . it is configured similar to the blood "core." objects placed in the bilecanal are collected in the bile duct .

experiments
park et al.  <cit>  reported an isl and parameterization that validated against diltiazem outflow profiles from single pass, perfused, normal rat livers. the same wet-lab data serves as referent for the experiments reported herein. the simulation methods and iterative refinement protocol are also the same. however, for the experiments reported herein, two mechanisms were added and one mechanism modified. the former were discrete time metabolism and the bile canaliculi  illustrated in figure  <dig>  previous studies used a discrete event model for metabolism where an enzyme tested a pseudo-random number draw only when solutebindingcycles had passed. although this micromechanism cannot be falsified against the available, coarse validation data, it has been pointed out that it is overly discrete and may produce abiotic artifacts. the experiments reported herein used a discrete time model for metabolism such that after binding, an enzyme determines if metabolism has occurred  every simulation cycle up to and including the last before it releases the compound. this micromechanism provides more variation in the event history of the compound and more opportunity for metabolism. consequently, these change require re-parameterization and re-validation.

previous studies did not include the creation of a metabolite because having it was not necessary for validation; when a metabolic event occurred, the compound was simply destroyed. however, it has been pointed out that the omission of this micromechanism detracts from the believability of the model. because believability is closely tied to falsification and validation, the addition of this micromechanism adds heuristic value. the new micromechanism constructs a new metabolite when an enzyme successfully metabolizes a compound. a pseudo-random number draw is tested against the bileratio parameter specific to the compound . the hepatocyte decides whether the metabolite goes into the bilecanal or into the intracellular space. the dynamics of the bilecanal are identical to the core. once inside the bilecanal, solute steps forward each simulation cycle the number of spaces designated by coreflowrate. when a metabolite in the bilecanal reaches the outlet of a ss it moves to the bilecanal inside the next ss. if the outlet goes to the cv, the metabolite is moved, logged, and destroyed. that process maps to metabolite in bile entering the bile duct. the bilecanal is specified with a "circumference" as if it, like space a, had a two dimensional grid wrapped around it. this geometry is used solely as a constraint to estimate how many metabolites may flow from a predecessor ss to a successor ss. it is not used when metabolite enters the bilecanal or when all metabolites are pushed forward within the data structure. so doing provides a minimal model for the constraints on bile flow and its output.

a manual search of the parameter space for a parameter vector that satisfies the sm produced the parameters in table  <dig>  of particular importance are bilecanalcirc, bileratio, solutebindingcycles, metabolismprob, binderspercell, dosageparams, and the various values for jumpprob. a value of  <dig> for bilecanalcirc means that only a single compound can flow from a predecessor ss to a successor ss in a single step. note that bileratio is compound specific and is  <dig>  for metabolizable diltiazem but  <dig>  for both sucrose and metabolite; because sucrose does not enter cells and enzymes cannot bind metabolites , their bileratio values are irrelevant. the dosageparam values for these experiments are set so that dosage is an impulse and all compounds are created and injected into the pv in a single simulation cycle. it is also important to note that the jumpprob values in table  <dig> are biased significantly outward so that the tendency is for compound to move from space a to space c.

manual searches were performed on the cluster using only  <dig> mc samples, which produced outflow profiles that were smooth enough for decision-making, but were not smooth enough to satisfy the sm. when a 28-sample result showed promise, experiments having  <dig> or more samples were undertaken to actually test the hypothesis that the parameterized mechanisms will produce outflow profiles that achieve the sm. in the latter experiments, the point was only secondarily to validate against the wet-lab data given the new mechanisms. the primary objective was to test the equivalence of the cluster and ec <dig> platforms. in order to do so, the parameter vector would have to produce a pair of results that validate in both the cluster and the ec <dig>  if either pair of profiles, from the cluster or cloud, failed to validate, then there was a significant experimental distinction between the local cluster and ec <dig> 

in principle the simulation could be configured to produce identical outputs when run with identical pseudo-random number seeds on both platforms. however, it is important to note that such identical experiments should be explicitly avoided. determinism is critical for software engineering and verification, where the computational system is not the subject of experimental exploration and hypothesis falsification. as laid out in  <cit> , in order to synthesize heuristically valuable computational analogues that are useful for mechanism discovery and falsification, the system must be suitable for experimentation, just as in vitro systems are the objects of experimentation for wet-lab studies. further, for simulations where the coupling is tight between the context  and the internal mechanisms, as it is when identical deterministic results are expected of a simulation, the model tends to be too abstract, inductive, and very fragile to context  <cit> . the method of repeating stochastic experiments where some elements are tightly controlled and others are completely uncontrolled is more biomimetic and it mimics wet-lab methods more closely while enhancing scientific impact and usefulness. for these reasons, the experiments reported herein use the full stochasticity of the isls over and above the underlying platform differences and place all emphasis on experimental distinguishability or lack thereof.

RESULTS
validation of an enhanced isl
to demonstrate methodological scientific equivalence, we must use the same model scaling and validation/falsification method on both platforms. to improve realism and heuristic value, we altered the isl phenotype by making three changes: a biliary elimination and flow space, an improved metabolism mechanism, and an alternative method of dose input. changes to in vitro models can have unintended consequences. the same is true with multiscale models like isls. because the character of an isl outflow profile is a consequence of networked micromechanisms, change can alter previously validated behaviors. thus, new validation evidence is required. the enhanced isls that were executed on the local cluster showed a potentially abiotic behavior that had two possible causes. 1) it was a consequence of micromechanisms that were unintentionally altered by the enhancements, or 2) it was an artifact of a small lobule sample size. to test the latter, we needed the option to scale the execution platform to handle a larger number of lobule samples. rather than buying more hardware to add to the local cluster, we chose to replicate the cluster architecture in the cloud.

validation experiments: local and cloud clusters
experiment results are shown for the local cluster in figure  <dig> and for the cloud cluster in figure  <dig>  the outflow profile for diltiazem is superimposed on the target zone, which designates the sm band derived from wet-lab data, as described above. as with the initial experiments in  <cit> , when sm ≥ 80% the isl and wet-lab profiles are designated experimentally indistinguishable. we specified that acceptable demonstration platform equivalence would be when sm ≥ 80% for two independent sets of experiments on both cloud and local cluster.

outflow profiles for diltiazem, d, in normal livers  <cit>  provide the center for the sm band in figures  <dig> and  <dig>  the width of the band is derived from the coefficient of variation of the sucrose outflow profiles from  <cit> . the upper and lower edges of the target zone are {d ± coefficient of variation}. to show the smoothness and asymptoticity of the profiles, the finite differences {dn - dn-1} are also plotted. the outflow profiles for the 49- and 70-sample experiments failed to satisfy the sm in two important ways: the presence of an apparent large period oscillation and high variance. the high variance can be smoothed to show a much more well behaved profile; but for the purposes of this cross validation exercise, the raw data provide more insight into events occurring during the simulation experiments.

the variance in wet-lab outflow profiles is typically rather constant. deviations about the trend line are typically random, and those observations are the basis for sm target bands in figures  <dig> and  <dig>  however, the patterns in figures 5a, b and 5a, b are different: all four, 49-sample studies showed significant oscillations in variance prior to about  <dig> seconds that appeared somewhat abiotic. the apparent variance oscillations caused fluctuations in the outflow profiles. one of the runs in cluster and cloud  exhibited strikingly large variances. the variance oscillations are obvious in the finite difference scatter plot in figure 5a. the outflow peak is reinforced by the first peak of the oscillation, making the leading edge of the profile seem markedly false in comparison to the peak shape of the validation band, which is sharper. the apparent oscillation in variance reaches minima at approximately  <dig>   <dig>  and  <dig> seconds in figure 4a; it reaches minima at approximately  <dig> and  <dig> seconds in figure 5a.

variance oscillations were still evident in the 70-sample local cluster and cloud studies, but overall variance decreased. because of the isl's structure, as the number of lobule samples increased, the variance decreased and the asymptoticity increased. that expected trend is evident in the finite difference plots.

both pairs of 84-sample studies  achieved validation. although hints of the variance oscillation remain in the 84-sample outflow plots, they satisfied the sm. further, the variance oscillation is faint or absent in the  <dig> sample experiments executed in the cloud .

discussion
arguably, the more important methodological impact of validating these isls in both the local cluster and ec <dig> cloud platforms is on model sharing and computational experiment repeatability amongst various laboratories and collaborators. it is a simple matter to acquire and read the source code for a simulation like the isl. for an organization with ample resources, the appropriate systems administration, and programming skills, it is straightforward to set up a serial computer platform on which to execute an experiment. however, because synthetic models like isls are very concrete and implement fine grained, multilevel and multiscale networked mechanisms, experiments on serial computers are infeasible. similar models will face the same problem. hence, in order for a laboratory to repeat a given experiment or, more importantly, engage in its own exploration of the analogue, it must have access to a cluster and enough privileged user access to configure the tool-chain on that cluster. this obstacle is higher than it seems, for it requires a well-organized laboratory and the motivation and resources to do the work. in effect, this obstacle contributes to the dominance of engineering oriented modeling and simulation , and limits scientific, exploratory modeling to a privileged elite with large budgets.

although amazon's ec <dig> infrastructure costs some money, the semi-automated methods used to instantiate virtual machine images, terminate those instances, and pay for the service with a credit card allow anyone to construct and use their own cluster at minimal cost. during the exploration phase of this work, an individual 49-sample experiment cost approximately $ <dig>  representing what a curious scientist might pay simply to instantiate and run a single experiment. when compared with the cost of building, purchasing, or maintaining a cluster, or acquiring time on another laboratory's cluster and then arranging for it to be configured properly for these experiments, the ec <dig> costs seem quite low. however, this cost estimate is informal. kondo et al.  <cit>  reported a detailed treatment of costs for the ec <dig>  costs have also been reported for voluntary computing grid platforms like boinc  <cit> .

even though we attempt to replicate as much of the environment as possible, our objective, detailed in methods, was scientific repeatability, not data duplication. previous experiments  <cit>  only required  <dig> samples to show experimental indistinguishability, in part because prolonged rather than bolus dosing was used. however, some anomalous results from previous experiments had shown periodicity  <cit> . nevertheless, the 49- and 70-sample experiments in both the cluster and cloud failed to validate because of large variance oscillations. we conjectured that perhaps because the diltiazem outflow profile was relatively flat, the isl parameterizations needed to validate against it would show underlying oscillations that are not evident in profiles that decline more steeply. something causes isl outflow profile variance to fluctuate. it could be: a) the parameters, code, or input data differ between each experiment or b) the discretized nature of each lobule is sufficiently unique so that it takes a large lobule sample to produce an acceptable approximation of a continuous liver outflow profile. if it is a), then the apparent artifact would still be evident after a large number of samples; it would not be steadily washed out with additional samples. if it is b), then having experiments comprised of enough samples will make the oscillations cancel each other. the latter is what we observed; we needed at least  <dig> lobule samples  to achieve validation.

one way to conceptualize a discretized isl during operation is as a large set of oscillators. each space within an isl acts, to some extent, like a timed storage device. a compound comes into a space , is effectively sequestered there for a number of cycles as it moves about, and it then exits that space. it may or may not be sequestered again later along its path to the cv. hence each compound goes through a series of store-release processes, except that the store and release events are stochastic. the delay solutebindingcycles, however, is not stochastic. each lobule execution starts with  <dig>  compounds pulsed into the pv. each one encounters a large array of "inductors" that can retain a compound for an interval and then release it. when a large number of compounds get caught, released, caught, and released all at roughly the same intervals, we see what looks like a stochastic oscillator with a random signal riding atop a larger oscillation. such a signal is made more obvious by the impulse bolus and a small number of compounds. the only elements of the model interfering with the predictability of the combined oscillation are: 1) the stochasticity surrounding when each compound is bound and rebound and 2) that some compounds are metabolized and their product moved into the bilecanal, and no longer contribute to the profile. hence, as the number of compounds increases, the variance oscillations should disappear. indeed, the high variance that showed up in the 49-sample cluster run in figure 4b, disappeared when the number of samples were increased to  <dig>  furthermore, the variance swings diminished in each of the  <dig> then 84-sample studies on both the local cluster and in the cloud. finally, for the 152-sample studies in the cloud, the variance swings have disappeared completely and the outflow profile decays more smoothly. the similarity between the cluster and cloud results for  <dig> and fewer mc lobules demonstrates the equivalence of the models executing on the two platforms. further, confirming the low sample size conjecture with the 152-sample experiments demonstrates the value of having available the scalable cloud platform to extend the mechanism validation and falsification methods beyond the capabilities of the local cluster.

the results demonstrate experimental indistinguishability of isl outflow profiles from the local cluster and amazon ec <dig> cloud platforms for experiments consisting of  <dig> or more lobule samples. the results also demonstrated that mechanistic changes confined isl behaviors to a smaller, more constrained region of parameter space. the modifications provide an even more concrete and specific, falsifiable hypothesis about the referent mechanisms. making these changes and validating them against the coarse aspect of outflow profiles have made more specific the regions of the parameter and behavior spaces that are biomimetic without losing the generality of isl parameterization.

CONCLUSIONS
the results contribute evidence of silico-to-wet-lab phenotype similarity, specifically that isl behaviors can cover some of the behavior space of referent liver experiments. so doing allows for concrete representations of multilevel hepatic mechanisms in many distinct experimental contexts, without degrading the ability to parameterize an isl so as to accurately represent a particular context. to date, the scientific benefits of experimenting with multiscale biomedical models has been limited to small numbers of researchers with access to computer clusters. cloud technology coupled with the evidence of scientific equivalency has lowered the barrier and will greatly facilitate model sharing while providing straightforward tools for scaling simulations to encompass greater detail with no extra investment in hardware. the flexibility and dynamic expandability of a platform cloud infrastructure is important for open-ended, exploratory, mechanism-focused research.

abbreviations
cv: central vein; isl: in silico liver; mc: monte carlo; mpi: message passing interface; m&s: modeling and simulation; pv: portal vein tracts; sm: similarity measure; ss: sinusoidal segment

authors' contributions
gepr replicated the cluster architecture in the cloud. gepr designed and conducted validation experiments. gepr and cah evaluated validation experiments. gepr and cah wrote, read, and approved the final manuscript.

supplementary material
additional file 1
isl cloud supplement. instructions for executing an in silico liver  on ec <dig>  provided are detailed, step-by-step instructions for setting up an isl using ec <dig> 

click here for file

 additional file 2
isl-for-bmc. isl source code and enabling documents.

click here for file

 acknowledgements
this work was supported in part by the cdh research foundation and the alternatives research & development foundation.
