BACKGROUND
dna microarray technologies enable comparison of the genetic composition in a variety of organisms starting from mosaic bacterial genomes to chromosomal aberration in cancer cells. arrays are produced by printing discrete regions of the genome such as open reading frames, short oligonucleotide probes or whole genomes tiled onto a glass slide or nitrocellulose substrate in an ordered array of spots. each spot acts as a device to determine if the same region is conserved or stable in the genome of test and reference strains. this is determined by labelling genomic dna from the test and reference sample with different fluorescent dyes followed by co-hybridisation to the microarray slide. if a gene is conserved in both samples the fluorescence emitted at the corresponding position on the array will be a mixture derived from both labels. if the gene is present in only one sample then only one type of fluorescence will be observed. the use of a reference sample in co-hybridisation experiments internally controls for defects in slide printing and hybridisation  <cit> .

comparative genomic hybridisation  microarray studies are being used increasingly to look at alterations in chromosomal dna in a wide variety of circumstances. this ranges from studies looking at genome aberrations in cancer or tumour cells to genome diversity in bacterial cells. several studies have shown that during the development and progression of various cancers amplification, deletion or translocation of chromosomal segments occurs resulting in the malfunctioning of cellular processes. comparison of the log-ratios intensities from cgh microarray data from diseased versus control samples has been used widely to measure such changes  <cit> . indeed, several algorithms and software has been developed to identify such aberrations within the chromosome, with the goal being to identify regions of concentrated high and low log-ratios  <cit> . these software methods can be broadly categorised into smoothing or segmentation algorithms. the smoothing algorithms use information from a number of genes locally to assign the log <dig>  whereas the segmentation algorithms define the set of genes. it has been shown when there are many smaller regions with little consistency of log <dig> neither of these algorithms may be effective  <cit> .

in bacteria gene duplication and rearrangements occurs often and randomly throughout the chromosome  <cit> . however, the major driving force for bacterial evolution is horizontal gene transfer  whereby integrating viruses , transposons and other mobile elements are inserted within the host bacterial genome. the elements are flanked by direct nucleotide repeats and often inserted in the vicinity of trnas. insertion and deletion of these elements are common with repeated events of gene acquisition and loss resulting in a highly variable gene content  <cit> . therefore a major aim of bacterial cgh microarray study has been to assess the plasticity of bacterial genome structures both within and between species  to deduce the evolutionary relatedness of bacterial pathogens such as salmonella  <cit> , or to understand the genetic diversity of field and clinical isolates within bacterial species such as escherichia coli, shigella, mycobacterium, and staphylococcus, with respect to a representative sequenced strain  <cit> .

in bacterial cgh studies, the fluorescent signal intensity is used to estimate which genes are conserved or variable in unsequenced strains. however, one of the critical problems faced in interpreting microarray data from bacterial cgh studies where dna hybridisation results from unsequenced strains of field and clinical origin are compared to a control sequenced strain, is determining those genes that are conserved between and within species, from divergent or highly polymorphic genes , and absent genes. a conserved gene is hypothesised to have approximately equal signal intensity in the test  and control  channels, whilst a divergent or absent gene has a true signal in the control channel only. thus, the genome content of bacterial strains of unknown origin can be estimated by cgh microarrays, where a cut-off algorithm determines the position of separation between genes present and those divergent or absent. hybridisations with less bound material on the test channel than the determined cut-off are categorised as divergent or absent, those with higher signals are referred to as present. here we compare the naïve cut-off with other more dynamically derived cut-off algorithms and establish a robust process for analysing bacterial cgh microarray data, using strains selected from the pathogenic e. coli o <dig> serotype and an e. coli k <dig> laboratory strain, that may be automated in the future. a unique point in our study was the inclusion of genomic dna from three bacterial genomes in the control channel, to provide a baseline for all genes present in the e. coli o <dig> panarray used in this study.

RESULTS
process for the analysis of bacterial cgh data
microarray hybridisations were performed on three e. coli sequenced strains and  <dig> e. coli o <dig> test strains against the three sequenced strains, as described in materials and methods. these provided scanned images that were converted into signal and background intensity values for both the cy <dig> and cy <dig> channels, for each spot on each slide. the log <dig> was cleaned and normalised as described in the methods section. the data was then analysed using different cut-off algorithms described below. the process used for analysis of sequenced reference and unsequenced test strains from cgh microarray studies are shown in figure  <dig> 

validation
within the framework outlined in figure  <dig> the validation was carried out on microarray hybridisation datasets from three e. coli sequenced strains  using each of the cut-off methods described below. this resulted in a number of genes to be identified as present and absent or divergent. the validation step reported the number of correctly or falsely identified present and absent genes after comparing the data with blastn data for the three sequenced strains.

comparison of the cut-off algorithms
each algorithm calculated a cut-off using data from each slide with the exception of the naïve cut-off. using the cut-off, including various naïve cut-off scores, and blastn data, the sensitivity, specificity and the m-score  were calculated, weighted by prevalence, and validated the accuracy of each algorithm. in this application, the prevalence of the conserved genes was different between strains, so sensitivity was weighted by the conserved gene prevalence. the m-score allowed the combination of both summary statistics, in a manner that accounts for strains to have different proportions of conserved genes. figures  <dig> present histograms of the log <dig> data for the three sequenced strains, and the summary statistics calculated from the algorithms are given in tables  <dig> to  <dig> 

the number of genes estimated as correctly conserved , genes identified as conserved but actually are variable , genes identified as correctly variable , and genes identified falsely as variable , are given. the sensitivity, specificity, and m-score are also calculated, where the sensitivity = tp/, specificity = tn/, and m-score = sensitivity*prevalence + specificity*

* institute of food research method 

** genotyping analysis by charlie kim method 

*** minimum kernel density method 

the number of genes estimated as correctly conserved , genes identified as conserved but actually are variable , genes identified as correctly variable , and genes identified falsely as variable , are given. the sensitivity, specificity, and m-score are also calculated, where the sensitivity = tp/, specificity = tn/, and m-score = sensitivity*prevalence + specificity*

* institute of food research method 

** genotyping analysis by charlie kim method 

*** minimum kernel density method 

the number of genes estimated as correctly conserved , genes identified as conserved but actually are variable , genes identified as correctly variable , and genes identified falsely as variable , are given. the sensitivity, specificity, and m-score are also calculated, where the sensitivity = tp/, specificity = tn/, and m-score = sensitivity*prevalence + specificity*

* institute of food research method 

** genotyping analysis by charlie kim method 

*** minimum kernel density method 

assessment of the k- <dig>  data
the normalised data were visualised prior to applying the different cut-off algorithms . three well defined distributions were obtained. the data was focused on the primary mode centred about log <dig> =  <dig>  these genes were expected to be present in both test  and control strains . there was a minor secondary mode located to the right of the primary mode that also represented present genes. this feature was due to those genes that are not present in all control strains or when multiple copies of genes were present in the test strain, therefore, generating an elevated log <dig> 

the cut-off algorithms were then applied to the normalised data . from table  <dig>  the naïve cut-off was optimised at  <dig> , with a sensitivity of  <dig>  , and specificity of  <dig>  . therefore, although the naïve cut-off performed well due to the clearly defined modes and little replication error, the algorithm may perform poorly under different conditions when a range of modes have not been considered  <cit> .

the naïve cut-off at  <dig>  was shown in table  <dig> to present the highest m-score, with the mixture model the second highest , and minimum kernel density  the third highest . the ranked order of the algorithms in table  <dig> suggested that the naive cut-off at  <dig>  was the best approach due to the highest m-score. therefore, the optimal cut-off was data dependent and was empirically derived. hence, the disadvantage of this method is that it cannot be automated unlike the other approaches and a range of cut-off values needs to be considered to derive at the optimal value. the approach with the second highest m-score was the mixture model. however, due to the complexity of fitting mixture model the simpler mkd approach was considered more appropriate. an example of an out put from this algorithm has been presented for normalised mg <dig> data within figure  <dig>  and shows the simplicity of this approach.

* institute of food research method 

** genotyping analysis by charlie kim method 

*** minimum kernel density method 

assessment of the edl <dig> data
the normalised data was visualised before applying the cut-off algorithms . the distribution was focused towards a positively skewed primary mode centred about log <dig> =  <dig>  to the left of the primary mode were far fewer absent genes than seen in the k- <dig> example . therefore, the edl <dig> data set was less well separated into three modes compared to k- <dig> presenting greater difficulty in discerning the present genes from those absent or divergent. so interpretation of data from subsequent cut-off algorithms proved more difficult than for the k- <dig> data.

application of cut-off algorithms resulted in the naïve cut-off to be optimised at  <dig> , with a sensitivity of  <dig>  , and specificity of  <dig>  . the range of these results showed that the naïve cut-off was varied and unreliable because of the lower specificity especially at cut off value  <dig> .

from table  <dig> the naïve cut-off at  <dig>  was shown to present the highest m-score, then the mixture model, with the mkd method the third highest . although the ranked order of the algorithms in table  <dig> suggest the naive cut-off at  <dig>  was the best approach, five naïve cut-off values were used and only optimal results at  <dig>  displayed. as mentioned already, a range of naïve cut-off values is required to be considered for each data set making the method laborious and not suitable for automation. therefore the optimal automated method was by mixture modelling. however, given the cost in time and complexity of the mixture method the far simpler mkd approach is recommended as the m-score, sensitivity and specificity for this method was very similar to that of mixture modelling.

* institute of food research method 

** genotyping analysis by charlie kim method 

*** minimum kernel density method 

assessment of the sakai data
the normalised data was examined before applying any cut-off algorithms . it can be seen that the sakai data was similar to the edl <dig> data , but the naïve cut-off was optimised at  <dig> , with a sensitivity of  <dig>  , and specificity of  <dig>  . these showed consistent results for a cut-off above  <dig> , but specificity was poor for a cut-off of  <dig> .

the naïve cut-off at  <dig>  was shown to present the highest m-score, with the mkd method second highest , again suggesting that the mkd cut-off algorithm was the optimal, as well as the simplest automatic algorithm available .

* institute of food research method 

** genotyping analysis by charlie kim method 

*** minimum kernel density method 

the unsequenced field isolates
we have validated the proposed process using sequenced reference strains. in practice, we wish to compare the presence and absence or divergence of genes in unsequenced strains with respect to sequenced reference strains. we performed forty- four hybridisations on nineteen e. coli o <dig> test isolates, whereby genomic dna was extracted, microarray hybridisations performed and analysed using the processes described in figure  <dig> .

the first output of the analysis step for unknown test strains includes performing a scatter plot matrix of all test strains in a pair-wise manner to control strains to analyse the extent of diversity between unknown test strains and the sequenced control strains. an example of this is shown in figure  <dig>  which shows the extent of diversity of e. coli o <dig> strains 0864/ <dig>  and 0330/ <dig>  to the three sequenced control strains. the matrix of scatter plots  and the pearson's correlation coefficient  between the three sequenced and two test strains is shown. both strains show a higher correlation to edl <dig>  than sakai or mg <dig> 

in a similar manner pearson's correlation co-efficient was used to correlate the remaining unknown e. coli o <dig> strains to the sequenced reference strains . the data shows that the genomic composition of the majority of test strains, i.e. seventeen of the nineteen strains used in the study, were more similar to edl <dig> than sakai or mg <dig>  the range of correlation found between these strains represent typical variability found between clonal isolates, indicating these strains to be closely related  <cit> . the genomic composition of one o <dig> isolate  was more similar to mg <dig> than either o <dig> sequenced strains, and one isolate  showed genomic correlation with sakai, although the value was relatively low in comparison to the correlation of other strains to edl <dig> or k <dig> 

the second output of the analyses step identified genes that were consistently present for the k- <dig> like o <dig> strain and mg <dig>  and absent for edl <dig> or sakai like o <dig> strains. as a result a list of  <dig> genes that were consistently present for the o <dig> gene set and consistently absent from the k- <dig> gene set, was made. inversely there were  <dig> genes that were consistently present in the k- <dig> gene set and consistently absent in the o <dig> gene set, a list of which was also generated . genes from both lists were collated to form the third output consisting of  <dig> variant genes that were unique markers for each group. the gene list and the biological significance of these findings are currently being investigated and are likely to give clue to new virulence factors harboured by o <dig> strains  <cit> .

determining the effect of gene copy number and spatial correlation within bacterial genomes
to investigate whether there was a copy number effect in our data we compared single copy genes to known multiple copy genes in our control strains to look for differences in the normalised log <dig> data. to do this we needed to first account for the effect of using pooled reference dna. in our data set  <dig>  genes were present in all three genomes, whereas  <dig> genes were present in edl <dig> and sakai,  <dig> genes were present in mg <dig> and edl <dig>  there were  <dig>   <dig> and  <dig> genes unique to mg <dig>  edl <dig> and sakai, respectively. furthermore,  <dig> gene probes had greater than 80% identity with more than one region of the genome, causing a natural shouldering effect. the majority of probes with multiple hits  were found in edl <dig> and sakai, and  <dig> of these were found in all three strains  <cit> . figure  <dig> presents two modes, the primary and secondary mode that resulted when sakai was used as the test strain. genes included in the primary mode are present in all three genomes in single and multiple copy . whereas the secondary mode include genes present only in edl <dig> and sakai genomes, both in single and multiple copy. therefore, within these two modes there was no difference in log <dig> distribution between genes with single and multiple copy numbers throughout the chromosome. therefore the source of variation in these log <dig> distributions were not caused by copy number differences within the bacterial genome but instead by the number of copies of each gene present in the control cy <dig> channel as a result of using pooled reference dna.

we went on to examine the extent of genomic spatial correlation in bacterial genomes, which has been shown to be important in eukaryotic studies  <cit> . the o <dig> genome comprises essentially of a k <dig> chromosomal backbone that is interspersed with regions of insertions and deletions. therefore we examined the microarray data resulting from the sakai strain, using genes within the k <dig> genome. for looking at spatial correlations typical improvements can be gained by modelling smoothed or segmented log <dig> data. so, we assessed the ratio after smoothing with both an unweighted and a weighted moving average to the log <dig> data  <cit> . then the mkd algorithm was applied to the moving average scores and sensitivity, specificity and m-scores were generated. the sensitivity and specificity calculated were approximately 75% and 85%, respectively, and the m-score was less than 90%, which is much lower than when the simple moving average adjustment had not been applied . we believe this is due to many single deletion events occurring in the k <dig> genome represented in the sakai strain, with respect to mg <dig>  figure  <dig> shows the sakai data as indexed by the mg <dig> chromosome. within this plot the extent of the individual and multiple contiguous gene deletions for the sakai genome can be seen. in case of the single gene deletions the effect of smoothing and segmentation would cause a reduction in performance of the cut-off algorithm rather than offering any improvement.

discussion
using both the sequenced and unsequenced strains, we have generated a process for bacterial cgh microarray data analysis, as shown in figure  <dig>  although this process appears intuitive, steps are often left out of analysis plans leading to a mis-interpretation of results  <cit> . key advantages of the process described here includes a clear and simple process allowing bench scientists access to transparent analysis ideas, as well as to database repository curators through our reliability metric for data inclusion in terms of a minimum m, or sensitivity and specificity scores  <cit> .

a major difference in this study compared to many cgh experiments was using a pooled reference dna from three sequenced strains where in silico information was available . it was found that the number of strains present in the pooled reference was the major source of variation in signal intensity; as we found no evidence in bacterial genomes that multiple copy genes had any increase of signal intensity ratio compared to single copy genes . this is in contrast to cgh microarray studies used to analyse aberrations in tumour or cancer cells. in such studies a shift would be expected in mean of the multiple copy genes compared to the single copy genes  <cit> . although our results may be an effect of using multiple genomic dna in the control channels for our experiments, we believed that even if the array experimental design consisted of only a single genomic dna in the cy <dig> channel this effect would not replicate the eukaryotic tumour examples. the difference may be because within bacteria genomes multiple copy aberrations occur less frequently and to a lesser degree than in eukaryotic tumour cells, but requires further examination. a further difference highlighted in this study between bacterial and eukaryotic cgh microarray studies was the effect of spatial correlation. in this study we found that application of a weighted and unweighted moving average to the log <dig> scores to account for correlation between adjacent genes  <cit> , in fact decreased the sensitivity diagnostics. however, an adaptation of this method may be more appropriate for bacterial genomes and worth investigating in future.

the use of pooled dna as control for cgh array experiments is novel and in contrast to current practice in bacterial cgh microarray studies where often only one genome is represented in the control channel, despite the array representing several bacterial genomes. our method proved that inclusion of genomic dna from three genomes not only enabled all spots on the array to be validated but also provided greater coverage to understand the bacterial genomic diversity present in edl <dig> and sakai-like strains in comparison to k12-like strains.

as a result this process has identified each sequenced strain most similar to the  <dig> test strains included in this study, using pearson correlation coefficient, and a set of genes have been identified that separates the o <dig> group from k <dig>  the invariant genes within the edl <dig> and sakai subgroup may be indicative of potential virulence markers, whilst the mg <dig>  subgroup may be genetic markers present in potentially commensal strains, and is currently being investigated in further detail  <cit> . the process described in this paper delivers the flow of information through bacterial cgh studies from raw data to the final analysis stage. to date, no software has been made available to fully encapsulate this process to understand the mosaic nature of bacterial genomes.

the process included data cleaning and normalisation within the pre-processing step. the data cleaning step not only ensured the exclusion of control spots but also the removal of data with poor signal intensity, and is crucial to the correct interpretation of the data. although normalisation is not mandatory, it is highly recommended common normalisation approaches have included: dividing by the control channel; control genes; global slide mean intensity corrections; print tip median correction; and a loess print tip normalisation method  <cit> , a combination of several of these approaches were used in this study .

various methods have been described promoting ways to differentiate divergent or absent genes from conserved genes within bacterial cgh microarray data sets. in essence, this appears a simple task, and some researchers have used the midway point between conserved and divergent genes  <cit> . here, some of the more widely used algorithms to determine the cut-off in bacterial cgh studies were compared, along with two novel approaches, the mkd and mixture modelling approaches.

to compare the algorithms reliability metrics were calculated and contrasted. the algorithms were compared by combining the number of genes to be identified as present and absent or divergent from each cut-off with blastn data. an identity of greater than 80% produced by blastn search matched with hybridisation signal intensity. the validation step within the process allowed the quantification of correct and false classification of genes for the sequenced strains.

since, both the reference strains, and unknown strains were carried out from the same suite of experiments, the reliability in the validation stage of figure  <dig> was high as shown in tables  <dig> to  <dig>  these results from the control sequenced strains provided confidence so it was expected that the test strains would provide similar results.

from the analysis carried out on three separate sequenced strains of e. coli, we see that the naïve cut-off is a good approach at partitioning the genes into conserved and divergent, if the empirical evidence is used. however, a range of cut-off values need to be considered for this approach and the cut-off value found most suitable in this study was  <dig>  for edl <dig> and mg <dig>  and  <dig>  for the sakai data set. hence, using a fixed midway cut-off is potentially misleading, since the inherent variability between experiments could result in differences between different slide and isolates, especially as each hybridisation and normalisation will lead to a different distribution of cy3/cy <dig> per slide. therefore, because the method is not automatic, and the cut-off will vary between array-slides and experiments, the naïve cut-off is not appropriate in an unsupervised manner.

the cut-off between present and absent or divergent genes was defined as the position of presence when using the gencom and prowollik methods. however, the gencom, gack, porwollik algorithms are bound by the assumptions of symmetry, normality, and linearity, which were reflected in their m-scores. the further away the data is from meeting these assumptions the less able the algorithms were at correctly identifying conserved genes. therefore these assumptions can be invalidated when using genes that have multiple copies, or hybridise at lower than 100% blastn identity match which creates natural shoulders on the primary mode  <cit> . additionally, the gencom, gack, porwollik methods are likely to produce poor results when using a combination of strains for the reference control channel, as has been used in this study, and may be used for other studies where more than one bacterial genome is present on the array.

the similarity between the gencom and gack algorithms has already been reported  <cit> . in a larger study the gack was found conservative, since it did not describe genes that are either clearly present, or divergent/absent  <cit> . intuitively, a conservative estimate of the number of genes present is appealing. however, this complicated the downstream analysis process by classifying genes as present, absent, or not having enough evidence to decide. it introduced complacency into the analysis since miss-classifications were still present. furthermore, it may not be helpful since by not classifying the genes, they are either ignored from any downstream analysis, or inflate the pool of those possibly present. hence, the trade off between sensitivity and specificity should be considered, rather than ignored.

comparison between the prowollik and mkd methods presented some differences between the algorithm results. whereas the mixture modelling algorithm performed well in both the sensitivity and specificity, as shown in tables  <dig> to  <dig>  this cut-off algorithm had the advantage of modelling the gene variability rather than just the mean log <dig> value for each gene. thus, providing a genuine estimated gene probability of presence. this algorithm did have the disadvantage of modelling an unknown number of modes, suffered convergence complications, and was computationally intensive.

CONCLUSIONS
in summary the results indicated that the mkd method showed good sensitivity and specificity, and could be automated easily in future due to its simplicity. it was found that the time taken, the level of complexity and implementation was a disadvantage for the mixture model, whilst offering little improvement from the simpler mkd algorithm. also, the interpretation and understanding is more straightforward for the mkd algorithm than the alternatives, and is non-parametric. therefore, by using the simplest, but most informative algorithm the analysis was more inclusive and directed towards the empirical evidence.

the advent of genome sequencing has brought about a new era in understanding biological processes and has also driven the development of methods such as cgh microarrays to exploit this information. in this study we have described a process that encapsulates all the stages required for analysis of bacterial cgh microarray data, and included a way of ensuring robust conclusions. although the bacterial cgh experiments described in this paper involve looking at diversity within bacterial genomes, escherichia coli in particular, we believe that the method can be extended to any bacterial cgh microarray study.

