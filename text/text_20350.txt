BACKGROUND
humans are diploid organisms with two sets of chromosomes –  <dig> pairs of autosomes and one pair of sex chromosomes. the two chromosomes in a pair of autosomes are homologous, i.e., they have similar dna sequences and essentially carry the same type of information but are not identical. the most common type of variation between chromosomes in a homologous pair are single nucleotide polymorphisms , where a single base differs between the two dna sequences . snp calling is concerned with determining locations and the type of polymorphisms. once such single variant sites are determined, genotype calling associates a genotype with the individual whose genome is being analyzed. genotypes, however, provide only the list of unordered pairs of alleles, i.e., genotyping does not associate alleles with specific chromosomes. the complete information about dna variations in an individual genome is provided by haplotypes, the list of alleles at contiguous sites in a region of a single chromosome. haplotype information is of fundamental importance for a wide range of applications. for instance, when the corresponding genes on homologous chromosomes contain multiple variants, they often exhibit different gene expression patterns. this may affect an individual’s susceptibility to diseases and response to therapeutic drugs, and hence suggests directions for medical and pharmaceutical research  <cit> . haplotypes also reveal patterns of variation that are present in certain regions of a genome. this enables focusing whole genome association studies on tag snps , representative snps in a region of the genome characterized by strong correlation between alleles . finally, since each chromosome in a homologous pair is inherited from one of the parents, knowledge of haplotype structure can be used to advance understanding of recombination patterns and identification of genes under positive selection  <cit> .

haplotypes of an individual whose genome is sequenced can be assembled using short reads obtained by high-throughput sequencing platforms. each read provides information about the order of nucleotides in a fragment of one chromosome of the individual. recent advances in high-throughput sequencing allow single individual haplotyping on the whole chromosome level. previously, the comparatively shorter reads as well as short insert lengths limited the size of connected components. now, the unprecedented amounts of reads and increasingly longer inserts make haplotyping of an entire chromosome as a single connected block a distinct possibility. in particular, paired-end reads that may be separated by several thousands of bases allow us to link haplotype information over long distances and thus enable their reliable reconstruction. in the absence of any read errors , haplotype assembly for diploid species is trivial. however, due to the errors in the data processing pipeline steps that precede haplotyping, the assembly is computationally challenging.

various formulations of the haplotype assembly problem have been proposed  <cit> . in this paper, we focus on the minimum error correction  formulation, which attempts to find the smallest number of nucleotides in reads whose flipping to a different value would resolve conflicts among the fragments from the same chromosome. finding the optimal solution to the mec formulation of the haplotype assembly problem is known to be np-hard for the diploid case  <cit> .

prior work
haplotype assembly, also referred to as single individual haplotyping, was first considered in  <cit>  where three related formulations of the problem were described. it has been shown that the problem is computationally hard under various objective functions  <cit> . levy,  <dig>  <cit>  proposed a greedy algorithm for the haplotype assembly of a diploid individual genome. bansal,  <dig>  <cit>   used a greedy max-cut formulation of the haplotype assembly problem to significantly improve on the performance of  <cit> . bansal,  <dig>  <cit>   and  <cit>  relied on mcmc and gibbs sampling schemes to tackle the same problem. wang,  <dig>  <cit>  and  <cit>  used computationally intensive branch-and-bound and dynamic programming schemes, respectively, in search for near-optimal solutions to the mec formulation of the problem. recently,  <cit>  reformulated the haplotype assembly problem as an integer linear program that was then solved using ibm’s cplex. refhap  <cit> , also relying on a greedy cut approach, was recently introduced and applied to reads sequenced using fosmid libraries while hapcompass  <cit>  relied on a graphical approach to develop a scheme which resolves conflicts arising from incorrect haplotype phasing.

in recent years, genome sequences of polyploid species – characterized by having more than two homologous sets of chromosomes – have been extensively researched. examples of such organisms include potato  and wheat . as in the case of diploid organisms, complete information about genetic variations in polyploid species is provided by their haplotypes. haplotype assembly for polyploids, however, is significantly more challenging than that for diploid species. unlike the diploid case, there exist considerably fewer methods for the assembly of polyploid haplotypes. authors of the first one,  <cit> , addressed the polyploid haplotype assembly by extending the ideas of their hapcompass diploid assembly framework. more recently,  <cit>   investigated the polyploid setup using a branch-and-bound scheme.

main contributions
in this work, haplotype assembly is cast as a correlation clustering problem and efficiently solved using a novel algorithm that exploits structural features of the underlying optimization. correlation clustering, originally proposed by  <cit>  and analyzed in  <cit> , is a method for clustering objects that are indirectly described by means of their mutual relationships. in the context of haplotype assembly, the relationships between reads may conveniently be represented by a graph and an associated weighted adjacency matrix, and the problem of assigning reads to haplotypes leads to correlation clustering on this graph. for diploids, that can in principle be done using an algorithm for maxcut such as  <cit> , while for polyploids one could use an algorithm for max-k-cut  <cit> . both of these algorithms solve semi-definite programming  relaxations of the original integer programming objectives that arise in maxcut and max-k-cut a. the complexity of solving the sdps, however, is impractical for large-scale haplotype assembly problems. to this end, we develop a novel algorithm for finding low-rank approximate solutions to the aforementioned sdp problems with complexity that is only linear in the number of reads. the results on both simulated and real data sets demonstrate that the proposed algorithm, named sdhap, has higher accuracy and is significantly faster than the existing haplotype assembly schemes. the proposed method is scalable and needs only minutes to accurately assemble haplotypes of complex genomes on a standard desktop computer. in addition to the developed software, we also provide an in-depth analysis of the coverage required to achieve near-optimal haplotype assembly – a result with many practical implications and useful guidelines for the choice of parameters of sequencing experiments.

methods
haplotype assembly is preceded by the mapping of the reads obtained from a sequencing platform to the reference genome and genotyping. since homozygous sites do not contribute useful information nor cause any ambiguities in the haplotype assembly, they are omitted from the haplotype and read representations. we represent haplotypes by k strings, , each of length n, where k denotes the ploidy and n is the haplotype length . for convenience, each read is represented as a string of length n with entries {a,c,g,t,−} , where ‘ −’ indicates snp positions on the chromosome that are not covered by the read. the reads are arranged into an m×n matrix r according to their positions along the chromosome, where m denotes the number of reads and the ith row of r, ri, corresponds to the ith read. since the reads are relatively short compared to the length of the haplotype sequence, matrix r is sparse, i.e., a large fraction of its entries are −. the start and the end of the ith read are the first and the last position in ri that are not −. the length of a read starting at position i and ending at position j is equal to j−i+ <dig> and may include gaps. the goal of haplotyping is to infer  from the observed reads.

following genotyping, we identify alleles at each snp location. using the genotype calls, one can reduce the underlying alphabet to a ternary one having elements { <dig> ,−} in the diploid case, and quaternary alphabet { <dig> , <dig> −} in the triploid case. for higher ploidy, there is no further reduction in the alphabet size. in the case where two or more haplotypes share the same nucleotide at a given snp location , further reduction is possible by treating all nucleotides that are not a part of the genotype as errors and neglecting them.

preprocessing
before the actual assembly, disconnected haplotype components need to be separated, i.e., we need to identify haplotype blocks that are not connected by any reads. from r, we can generate an adjacency matrix and a graph having vertices that correspond to the snp positions . an edge is present between two vertices if a read covers the corresponding snp positions, i.e., if the components of a read at those columns in the matrix are not −. reads that cover only one snp position do not provide information that can be used to reconstruct a haplotype and are thus discarded from r. similarly, any snp position not covered by at least one read is removed. after forming the adjacency matrix, disconnected subgraphs or partitions need to be identified. this is done by implementing a simple queue. starting with the first vertex, all vertices connected to it are inserted in the queue. these vertices are labeled by k= <dig> to indicate the first subgraph. then in a first-in first-out manner, all vertices connected to the vertices in the queue are inserted into the queue provided they have not been previously labeled. once the queue is empty, a new unlabeled vertex is chosen and labeled as k= <dig>  and the process is repeated until all vertices are labeled. this procedure leads to partitioning of the matrix into smaller disconnected matrices .

problem definition
let us define a measure of distance d between two symbols a and b from the alphabet used to represent the snp fragment matrix r as  d=1ifa≠−andb≠−anda≠b, <dig> otherwise. 

denote the hamming distance between read ri and haplotype h̲l as hd=∑j=1nd. then the minimum error criterion  formulation of the haplotype assembly problem is concerned with minimizing z over h̲l, where the objective function   z=∑i=1mminhdri,h̲ <dig> hdri,h̲ <dig> …hdri,h̲k, 

and m denotes the total number of reads.

the all-heterozygous and heterozygous/homozygous case
ideally, the snp matrix r should only contain true heterozygous sites determined in the genotyping step. however, in practice, false positives from the genotyping procedure lead to the presence of columns in r that correspond to both homozygous sites as well as heterozygous ones. our method can detect the potential presence of genotyping errors and enable correction of a large fraction of incorrectly called heterozygous sites, hence improving the mec score of the final solution to the haplotype assembly problem.

problem reformulation
sequencing reads that are used in haplotype assembly projects may be the short reads generated by illumina platforms, the long reads obtained from pacific biosciences instruments, or the long reads from jumping libraries in  <cit> , to name a few. consequently, the snp fragment matrix may be either a fat matrix  or a tall one , depending on the technology used. while short illumina paired-end reads generally lead to limited lengths of connected haplotype blocks, technologies that provide long reads and/or large insert sizes enable very long blocks. in the latter scenario, the apx hardnessb result essentially implies that exact inference, being of exponential complexity, is no longer feasible. therefore, computationally efficient approximate inference methods that enable fast yet accurate haplotype assembly are needed.

to quantify the relationships between the reads, we evaluate a measure of similarity for each pair of rows of the snp fragment matrix as described next. define a graph g= where  denotes the set of vertices corresponding to the rows of the snp fragment matrix,  is the set of the edges connecting the vertices in , and w denotes the set of weights associated with the edges. for any two reads i and j that overlap in at least one position, we define the weight of an edge between the corresponding vertices vi and vj as  wij=ksim−kdissimksim+kdissim. 

here ksim denotes the number of overlapping positions where the reads have an identical base and kdissim is the number of positions where they are different. then g= is a correlation graph where the edges connecting vertices associated with similar reads  should have positive weights, while the edges connecting vertices associated with dissimilar reads should have negative weights. in the absence of sequencing errors, that is indeed the case and thus separating the reads into k different clusters corresponding to k distinct haplotypes is trivial. in the presence of errors, however, a positive weight no longer unambiguously implies that two reads belong to the same chromosome nor a negative one means that they belong to different chromosomes, hence making the separation problem difficult. we formalize it as follows: given a weighted graphg=, find k− <dig> cuts such that the sum of intra-partition edge weights is maximized and inter-partition edge weights is minimized. this effectively translates to performing ‘correlation clustering’ in machine learning/algorithms parlance.

haplotype assembly via correlation clustering
problem formulation for diploid species
in the case of diploid organisms, correlation clustering interpretation of the haplotype assembly problem leads to maximization of the cut norm of the adjacency matrix w,  maximizex∑i<jwijxixjsubject toxi∈{+ <dig> −1}i=1…m. 

defining x̲=t where t denotes the transpose, the above optimization can be restated as   maximizex̲x̲twx̲subject toxi2=1i=1…m. 

introduce a rank- <dig> matrix variable x=x̲x̲t. it is straightforward to show that x is positive semidefinite. thus the maximization  can be written as   maximizextrsubject todiag=e̲rank=1x≽ <dig>  

where e̲ denotes an m× <dig> all-ones vector. note that  <dig> and  <dig> are equivalent . this problem is hard to solve because of the rank constraint. relaxing the rank constraint leads to the following semi-definite program ,   maximizextrsubject todiag=e̲x≽ <dig>  

this sdp can efficiently be solved in polynomial-time  where m is the number of reads), and provides an upper bound on the objective of the quadratic program . the goemans-williamson randomized algorithm may then be used to find an approximate integer solution to the problem  <cit> . we omit the arguments behind the randomized algorithm for brevity and summarize the procedure below.

goemans-williamson algorithm for solving the maxcut problem: solve the sdp relaxation and denote the optimal solution by x∗.

compute the factorization x∗=vvt. let vi denote the normalized ith column of v.

rounding procedure: set s={}. uniformly generate a random vector η̲ on the unit n-sphere.

for i=1…m, if xi=vitη> <dig> assign vertex i to  ; otherwise, assign vertex i to s¯ .

find the value of the obtained cut x̲twx̲.



repeat the rounding procedure and output the assignment with best cut value.



problem formulation for polyploid species
in the case of polyploid species, haplotype assembly can be cast as the correlation clustering problem where the goal is to partition the set of reads into as many subsets as there are haplotypes. let the ploidy of an organism be k> <dig>  e.g., k= <dig> for triploids, k= <dig> for tetraploids, and so on. given the clustering graph g= representing the reads, we would like to partition the vertex set  into k partitions such that the sum of intra-partition edge weights is maximized and inter-partition edge weights is minimized.

to this end, we first need a suitable way of defining variables that can take one of k possible values  <cit> . let y̲j be one of the k vectors {a <dig> a <dig> …,ak} that are defined as follows: take an equilateral simplex ∑k in rk with vertices {b̲ <dig> b̲ <dig> …,b̲k}. let c̲k=b̲1+b̲2+⋯+b̲kk be the centroid of ∑k and let a̲i=b̲i−c̲k, for 1≤i≤k. assume that ∑k is scaled so that ∥a̲i∥= <dig> for 1≤i≤k. note that this definition of the variables y̲i implies  y̲ity̲j≥−1k−1i<ji,j=1…my̲ity̲i=1i=1…m. 

to see why this is true, note that for any k, the entries of yi̲ are −1k except for one of the components which is equal to 1−1k. this object is then normalized by its 2-norm and thus  ∥yi̲∥2= <dig>  when we multiply  <dig> such normalized vectors, it is straightforward to see that the resulting inner product yi̲yj̲=−1k− <dig> . finally, this equality is relaxed to an inequality to turn the problem into a convex problem.

now we can state the correlation clustering formulation of the haplotype assembly problem for the k-ploid species as the optimization   maximizey̲∑i<jwijy̲ity̲jsubject to∥y̲i∥=1i=1…my̲ity̲j≥−1k−1i,j=1…m,j<i. 

define matrix Ŷ whose ith row is y̲it and introduce y=ŶŶt. the optimization  is a vector program which we relax to a semi-definite program of the form   maximizeytrsubject todiag=e̲yij≥−1k−1i,j=1…my≽ <dig> 

and solve using interior-point methods; here we relaxed the rank of y from k− <dig> to m. as in the diploid case, a randomized rounding algorithm may then be used to find an approximate integer solution . note, however, that there are m <dig> constraints which make the complexity of solving the sdp very high, approximately o.

for long haplotype blocks, directly solving the semi-definite programming formulation of the assembly problem in either diploid or polyploid setting is computationally infeasible. it is therefore of interest to explore underlying structural features of the assembly problem and derive fast sdp methods tailored for finding the solution to problems with such features. in the following two sections, we exploit sparsity of the underlying graphical representation of the haplotype assembly problem and the prior knowledge that the solution is low-rank to develop fast yet highly accurate algorithms for solving the sdps  and .

low-rank sdp solutions for haplotype assembly of diploid species
barvinok,  <dig>  <cit>  and  <cit>  independently studied the scenario where the optimal solution to an sdp has low rank. in particular, they considered the rank-r optimal solutions x∗ to an sdp such that r2≤m where m denotes the number of constraints of the sdp. the existence of x∗≽ <dig> implies the existence of some vo∈rm×r such that x∗=vovot. if the optimization over x=vvt is replaced by an equivalent optimization over v, the number of variables can be greatly reduced and hence the optimal solution can be found with potentially significant computational savings.

the graph g= representing the haplotype assembly problem is inherently sparse, and hence we can rewrite  as   maximizev∑i<jwijvitvjsubject to∥vi∥=1i=1…m, 

where vi denotes the ith row of v. note that by expressing the objective function in terms of v rather than x, we no longer need to explicitly impose the positive semidefinite constraint on x. if the graph is very sparse, most wij’s are  <dig> and the computation of the objective function is fast. moreover, it is convenient to convert  into the following unconstrained program,   maximizev∑wijvitvj∥vi∥∥vj∥. 

denote the objective in  by m. this optimization is no longer convex; however, for r>r∗ , the stationary point of the non-convex problem  does in fact coincide with the optimal solution of the convex program .

adaptive rank update
to solve , we rely on adaptive rank scheme where we initialize v as an m× <dig> matrix. in the subsequent steps of the algorithm, the number of columns of v is increased until v becomes rank deficient . each step of our proposed scheme requires computation of the objective function  and its gradients, which has complexity o. clearly, we also need to find the rank of v as the algorithm progresses. this is done by computing a singular value decomposition  of v  operations) and declaring that the rank of v is equal to the number of singular values that are larger than a predefined threshold εth .

gradient computation
we compute the gradient of the objective function in  with respect to vi,  ∂m∂vi=∑k∈eiwi,k∥vi∥2vk−vi∥vk∥∥vi∥ <dig>  

from the computed gradient, we arrive at the following simple update rules for fast iterative solution of the sdp relaxation ,  vi←∑k∈eivkvi←vi∥vi∥. 

convergence and stopping criterion
we keep track of how the ratio of the gradient to the objective function changes through the iterations. when this ratio becomes smaller than a predefined tolerance value εtol, we terminate the algorithm. since the gradient descent scheme ensures that the objective of the optimization problem is non-decreasing, convergence of the algorithm is guaranteed.

randomized projections and greedy refinement
the result of the previously described optimization procedure v^ is of rank r^. in order to obtain a rank  <dig> solution, we project v^ onto a random vector of size r^ and take the sign of the projection. we generate multiple projections and choose the one among them leading to the largest value of the objective function in  as the solution. the number of projections needed for the expected value of the objective function to meet certain performance guarantees is ≈o)  <cit> .

in the scenario where there are no genotyping errors, the previously described procedure provides the haplotype pair . this solution is further refined by greedily exploring whether sequential alterations of the bases along the haplotype sequences might lead to even lower mec scores. in the scenario where genotyping errors are present, we use the previously described procedure to partition the reads into  <dig> clusters. in order to assemble the haplotypes from the partitions, we employ the following strategy: for every snp location and for each partition, we rely on majority voting to decide on the corresponding haplotype position. this may result in both heterozygous and homozygous sites. finally, the assembled haplotypes are further greedily refined by testing if sequential alterations of the bases lead to any improvement of the mec scores, which has complexity o. we formalize the proposed scheme as algorithm  <dig> given below.



fast lagrangian relaxation for haplotype assembly of polyploid species
in the previous section, we described a fast and accurate method for haplotype assembly of diploid species that relies on solving low-rank sdp relaxation of the problem. for the polyploid setting, we need to solve a constrained sdp . to this end, we employ a fast, low-rank lagrangian scheme followed by randomized projections and a greedy refinement of the k-ploid haplotypes.

following factorization y=vvt, we can re-phrase the sdp formulation  of the haplotype assembly problem for k-ploids  as the optimization   maximize∑i<jwijvitvjsubject to∥vi∥=1i=1…mvitvj≥−1k−1i,j=1…m,i<j,wij≠ <dig>  

unlike the unconstrained optimization  that arises in the diploid setting, the above optimization problem is constrained . in order to solve it with practically feasible and scalable complexity, we consider its lagrangian relaxation and solve the dual problem using a minorization-maximization technique.

in particular, our scheme iteratively finds  infλij≤0supvℒ,  where ℒ is the lagrangian of  and λ={λij} is an m×m matrix collecting all lagrange multipliers associated with inequality constraints  since they are readily enforced by the projection step explained later in this section). therefore, the lagrangian is given by  ℒ=∑wijvitvj+∑λijvitvj+1k− <dig>  

the minorize-maximize iterative procedure consists of an inner and an outer loop. in the inner loop , we find supvl by keeping λ fixed. for this, we rely on the same idea of cyclic coordinate descent with adaptive rank update as described in the diploid section. specifically, we make the following updates of the ith row of v,  vi←∑j∈eiwij+λijvj,vi←vi∥vi∥. 

in the outer loop  we keep v fixed and update λ.

the subgradient for λij is vitvj+1k− <dig>  a simple updating rule for λij is of the form  λij←λij+αvitvj+1k− <dig>  

where α is a pre-defined step size. since λij are constrained to be less than or equal to zero, the above update rule is modified as  λij←minλij+αvitvj+1k− <dig> . 

to accelerate the convergance, let us introduce εg≥ <dig> that defines a guard interval. if λij<−εg, we make a further modification and update λij as  λij←λij2−μvitvj+1k− <dig>   where μ≥ <dig> is a damping parameter that can be tuned according to the accuracy requirement of the final solution  <cit> . this exponentiation in the lagrange multiplier update improves the speed of convergence of the proposed scheme.

convergence and stopping criterion
detecting convergence is slightly more complicated in the polyploid case as the objective does not increase monotonically. we use a short window of  <dig> latest iterations to smoothen the value of the objective function and use the obtained value to test the convergence .

randomized projections and greedy refinement
the solution v^ to the fast lagrangian scheme described in this section has rank r^. to obtain k partitions sought after in the k-ploid haplotype assembly problem, we project v^ onto an r×k matrix q with random entries and assign the ith read to the jth partition if the  entry of p=v^q is the largest component of the ith row of p. we generate multiple projections and choose the one among them leading to the largest value of the objective function in  as the solution.

for the no-genotyping-errors setting, the previous scheme provides h̲ <dig> …h̲k. a few rounds of greedy iterations that explore if local alterations of the bases along the k haplotype sequences may improve the mec score are conducted. for the case where genotyping errors are present, we use the previously described procedure to partion the reads into k clusters. to assemble the haplotypes from the partitions, we use the majority voting scheme as described in the diploid section. finally, the assembled haplotypes are further greedily refined by testing if sequential alterations of the bases lead to any improvement of the mec scores, which has complexity o. we formalize the proposed scheme as algorithm  <dig> given below.



RESULTS
we tested performance of sdhap using both simulated and experimental data, as described next. our codes are written in c and the benchmarking tests are conducted on a single core intel xeon machine with  <dig> ghz and 12gb ram. we compared sdhap with cplex  <cit>  , hapcut  <cit>  , refhap  <cit>   and haptree  <cit>  .

performance on real datasets
huref data
we first tested sdhap on the huref dataset  <cit>  which contains single and mated reads sequenced using a dideoxy sanger sequencing technology with an average coverage of ≈8x. table  <dig> compares the accuracy  and runtimes of sdhap with those of aforementioned existing techniques. as can be seen from the table, the mec scores obtained with sdhap are significantly better than those of the competing algorithms except for cplex. the complexity of cplex, however, scales exponentially with the haplotype length which makes it impractical for very long haplotype blocks. as evident from the table, our sdhap is faster than any of the other considered schemes. we should point out that unlike sdhap and refhap neither hapcut nor haptree make homozygous calls, which adversely affects their performance both in terms of mec and  switch error rate .table  <dig> 
comparison of mec and runtimes for different schemes applied to huref data



chr #
huref
cplex
sdhap
hapcut
refhap
haptree
mec
t
mec
t
mec
t
mec
t
mec
t
mec and running times for cplex, sdhap, hapcut, refhap and haptree algorithms applied to huref data. sdhap is more accurate than all schemes except for cplex . however, for longer blocks, the complexity of the cplex scheme may become very high as evident from chromosomes  <dig>   <dig> and  <dig> 



fosmid data
to investigate how sdhap performs when employed for the assembly of very long haplotype blocks, we tested it on the fosmid dataset analyzed in  <cit> . table  <dig> shows the accuracy and runtime comparison of sdhap with several competing schemes. as can be seen from the table, the mec scores of sdhap are better than those of hapcut, haptree and refhap; its runtimes are comparable to those of refhap, while hapcut and haptree are very slow when the coverage is low and read lengths long . overall, sdhap seems to be robust with respect to the nature of the dataset, e.g., it is fast, compared to other techniques, regardless whether being applied to huref or fosmid datasets.table  <dig> 
comparison of mec and runtimes for different schemes applied to fosmid data



chr #
fosmid
cplex
sdhap
refhap
hapcut
haptree
mec
t
mec
t
mec
t
mec
t
mec
t
mec and running times for cplex, sdhap, hapcut, refhap and haptree haplotype assembly strategies applied to fosmid data. sdhap is more accurate than the other schemes except cplex and significantly faster than hapcut or haptree. for chromosome  <dig>  haptree did not complete its run within  <dig> hours and hence the corresponding entry is missing.



performance of the algorithm on simulated data
diploid
to characterize how the accuracy of sdhap depends upon coverage and haplotype block lengths, we perform tests on simulated data sets. in particular, we generate data sets containing paired-end reads with long inserts that emulate the scenario where long connected haplotype blocks need to be assembled. the snp rate between two human haploid chromosomes is estimated at  <dig> in  <dig>  <cit> . we generate snps by randomly choosing the distance between each pair of adjacent snps based on a geometric random variable with parameter psnp . to simulate a sequencing process capable of facilitating reconstruction of long haplotype blocks, we randomly generate paired-end reads of length  <dig> with average insert length of  <dig>  bp and standard deviation of 10%; sequencing errors are inserted using realistic error profiles  <cit>  and genotyping is performed using a bayesian approach  <cit> . at such read and insert lengths, the generated haplotype blocks are nearly fully connected .

accuracy of haplotype assembly is naturally expressed in terms of switch errors – the number of switches  that are required to obtain the true haplotype phase. this can be expressed as a rate: the number of switches required divided by the number of opportunities for switch error. while our tests of the performance of sdhap on real datasets are expressed only in terms of the mec scores, for the simulated datasets we know the ground truth and therefore characterize the performance of sdhap in terms of both mec and switch error rate . table  <dig> compares the mec, swer and running times of sdhap with those of hapcut, haptree and refhap. we make these comparisons for haplotype block lengths of  <dig> and  <dig> at coverages of  <dig>   <dig> and  <dig>  sdhap’s mec score is lower and its swer is nearly half that of the competing schemes. the running times of sdhap are at least  <dig> times lower for haplotype block lengths of  <dig> . overall, sdhap clearly outperforms the other consideredmethods.table  <dig> 
comparison of swer, mec and runtimes for different schemes on simulated diploid data



data
simulated data
sdhap
refhap
hapcut
haptree
mec
swer
time
mec
swer
time
mec
swer
time
mec
swer
time
mec, swer and running times  for sdhap, refhap, hapcut and haptree algorithms for simulated data of different lengths  and with different coverages . the data contains a fixed 1% fraction of genotyping errors. sdhap is more accurate in terms of mec and swer and faster by almost an order of magnitude compared to other schemes for longer blocks.



since cplex is originally designed to find an optimal solution to the haplotype assembly problem, we compared sdhap with both the original cplex as well as its heuristic variant proposed by  <cit>  for different haplotype block lengths, coverages and error rates. we set a time limit of  <dig> hours for the optimal scheme to complete the assembly task and, if the optimal scheme did not succeed, ran the heuristic method allowing another  <dig> hours for the completion of the assembly task. table  <dig> and table  <dig> show the mec scores, swer and runtimes for the considered methods. as can be seen from the tables, cplex did not complete the task for block lengths of  <dig> and most of the block length of  <dig>  for block lengths of  <dig> and error rates 1%, cplex achieves the best mec scores and swer but its runtimes are significantly slower than those of sdhap. for very long blocks and high error rates, neither the optimal cplex method nor its heuristic variant provided a solution except in one instance where sdhap actually performed better .table  <dig> 
comparison of swer, mec and runtimes for sdhap and cplex on simulated diploid data with 1% error rate



data
simulated data
sdhap
cplex
cplex
mec
swer
time
mec
swer
time
mec
swer
time
mec, swer and running times  for sdhap, refhap, hapcut and haptree algorithms for simulated data of different lengths  and with different coverages . the data contains a fixed 1% fraction of genotyping errors. sdhap is more accurate in terms of mec and swer and faster by almost an order of magnitude compared to other schemes for longer blocks.
comparison of swer, mec and runtimes for sdhap and cplex on simulated diploid data with 5% error rate



data
simulated data
sdhap
cplex
cplex
mec
swer
time
mec
swer
time
mec
swer
time
mec, swer and running times  for sdhap, refhap, hapcut and haptree algorithms for simulated data of different lengths  and with different coverages . the data contains a fixed 5% fraction of genotyping errors. sdhap is more accurate in terms of mec and swer and faster by almost an order of magnitude compared to other schemes for longer blocks.







polyploid
to test the performance of sdhap for the assembly of polyploid haplotypes, we generate data in the same way as described in the previous section . we study the performance of sdhap when applied to the assembly of haplotypes with ploidy k= <dig>   <dig> and  <dig>  figure  <dig> shows the swer of sdhap as a function of the coverage for various block lengths. as can be seen there, the coverage required to obtain a chosen target swer increases with the ploidy. . the algorithm is tested for coverages 5kx, 10kx and 5k2x, where k denotes the ploidy. from the simulation results, it appears that the required coverage increases approximately with the square of the ploidy. for example, the coverage needed to achieve swer below 1% for triploids  is approximately 45x, for tetraploids  the required coverage is around 80x, and for hexaploids  the algorithm requires coverage of ≈180x. in figure  <dig> we show the runtimes of sdhap  as a function of the coverage for different ploidy.figure  <dig> swer for polyploids. switch error rates of sdhap applied to polyploid data as a function of coverage for different block lengths and error rates. to achieve the same swer, higher coverages are needed for longer blocks and higher ploidy.



tables  <dig>   <dig> and  <dig> compare the mec, swer and runtimes of sdhap when applied to the haplotype assembly of triploids, tetraploids and hexaploids as a function of coverage and block length with those of haptree  <cit> . note that haptree, previously shown to outperform the only other existing method for haplotype assembly of polyploids  <cit> , assumes exact knowledge of the underlying genotypes and that its performance deteriorates in the presence of errors. genotyping from next-generation sequencing data, however, is typically erroneous  <cit>  and hence we compare the performance of sdhap and haptree in the presence of genotyping errors . as can be seen from the tables, sdhap outperforms haptree in terms of both swer and mec. the complexity of sdhap is roughly linear in the size of the haplotype block while the complexity of haptree grows significantly with the size of the block. in fact, several of haptree simulations could not be completed within 48hrs .table  <dig> 
comparison of swer, mec and runtimes for different schemes on simulated biallelic triploid data



dataset parameters
genotyping error rate
sdhap
haptree
mec
swer
t
mec
swer
t
mec, swer and running times for sdhap and haptree algorithms on biallelic triploid simulated data. for l =  <dig> and c =  <dig>  haptree did not complete the task in  <dig> hrs.
comparison of swer, mec and runtimes for different schemes on simulated biallelic tetraploid data



dataset parameters
genotyping error rate
sdhap
haptree
mec
swer
t
mec
swer
t
mec, swer and running times for sdhap and haptree algorithms on biallelic tetraploid simulated data. for l =  <dig>  haptree did not complete the task in  <dig> hrs.
swer, mec and runtimes of sdhap for simulated hexaploid data



dataset parameters
genotyping error rate
sdhap
mec
swer
t
mec, swer and running times of sdhap for biallelic hexaploid simulated data. haptree completed the task within  <dig> hrs in only one case, , where it achieved mec= <dig>  swer= <dig> , and t =  <dig> s, all inferior compared to the results of sdhap in the table.



remark: sdhap is designed to minimize the mec score which, as pointed out in  <cit> , cannot distinguish between identical pairs of snps on the haplotypes of a polyploid. for example, when a triploid has pairs of snps {ac,gt,gt} at the same positions of its haplotypes, mec cannot be used to distinguish between the two chromosomes containing the snp subsequence gt . however, this does not impede the ability of the mec criterion to enable separation of polyploid haplotypes provided they are sampled by paired-end reads sufficiently long to resolve segments of identical snps – as demonstrated by the results presented in tables  <dig>   <dig> and  <dig> 

connectivity
the lengths of sequencing reads, insert sizes and their variations, and snp rates are of fundamental importance for the achievable performance of haplotype assembly and connectivity of snp positions. figure  <dig> and figure  <dig> show the distributions of the haplotype block lengths for huref and fosmid data for all  <dig> chromosomes, respectively. as can be seen there, majority of the blocks are shorter than  <dig> snps. while this has been a major issue with previous generations of sequencing technologies, with the ability of sequencing longer reads and fosmid technologies that allow insert lengths as large as 100kb, one can expect achieving complete connectivity in future. in our simulation studies, we focused on long inserts that enable near-complete connectivity of the haplotype blocks. for a more detailed discussion, please see  <cit>  and the references therein.figure  <dig> block lengths histogram for huref data. histogram of block lengths for huref data.



homozygous positions
chen,  <dig>  <cit>  demonstrated presence of homozygous sites in haplotype blocks assembled using high-throughput sequencing data. in figure  <dig>  we show the histogram of the fraction of homozygous positions in the haplotypes assembled from huref data using sdhap. as seen there, approximately 1− <dig> % of the positions are homozygous. to address this issue,  <cit>  suggested using alternative measures of performance such as minimum weighted edge removal . however, as our results demonstrate, optimizing the mec objective with the added capability of calling homozygous positions results in a very low swer.figure  <dig> histogram of homozygous positions. histogram of the fraction of homozygous positions as a function of chromosome number for huref data. on average, around 1% positions are falsely called heterozygous.



CONCLUSIONS
in this paper, we introduced a haplotype assembly scheme for diploid  and polyploid  species that relies on our novel technique for solving low-rank semidefinite programming optimization problems. highly accurate and computationally efficient, the proposed sdhap algorithm also addresses the important issue of having homozygous positions in the data – a problem that is neglected by most existing haplotyping schemes. the method is tested on real and simulated data for both the diploid and polyploid scenarios, showing that it outperforms several existing methods in terms of both accuracy and speed. we also provide important guidelines for the required coverage needed to achieve near-optimal haplotype assembly. in future, we expect to extend the developed method to jointly perform genotyping and haplotyping.

endnotes
a in semidefinite programming, one minimizes a linear function subject to the constraint that an affine combination of symmetric matrices is positive semidefinite. such a constraint is nonlinear and nonsmooth, but convex, so semidefinite programs are convex optimization problems. semidefinite programming unifies several standard problems  and finds many applications in engineering and combinatorial optimization  <cit> .

b in complexity theory, the class apx is the set of np optimization problems that allow polynomial-time approximation algorithms with approximation ratio bounded by a constant .

competing interests

the authors declare that they have no competing interests.

authors’ contributions

algorithms and experiments were designed by shreepriya das  and haris vikalo . algorithm code was implemented and tested by sd. the manuscript was written by sd and hv. both authors read and approved the final manuscript.

