BACKGROUND
reliable high-resolution prediction of protein structure remains a formidable challenge and it becomes more and more evident that we are entering the era in which high-resolution predictions and molecular designs will make increasingly important contributions to biology and medicine  <cit> . the high-resolution models could be built by means of various comparative modeling procedures, although it is also sometimes possible to obtain good models in a template-free modeling of small globular proteins  <cit> .

determining and properly quantifying the properties that are characteristic for protein native structures are of primary importance for the construction of an accurate tool for the model quality assessment. several different approaches to the optimal model selection have been proposed – such as the use of empirical or knowledge-based potentials  <cit>  derived from the databases of experimental structures. more straightforward, although more expensive computationally, is the evaluation of conformational energy by means of molecular mechanics force fields  <cit> . another approach to the model selection is the structural clustering, especially useful when large set of models must be assessed  <cit> . finally, learning-based scoring functions can be developed using machine learning methods e.g. support vector machines  <cit> , neural networks  <cit> , etc.

it is widely believed that the native conformation of a protein corresponds to the global minimum of the free energy surface defined by the protein's conformational space and the molecular interactions. a straightforward protein modeling by the all-atom energy minimization remains impractical due to the high complexity of the interactions and astronomical size of the conformational space to be searched. thus, most approaches used for exploring the protein's energy surface have resorted to essential simplifications in the description of the polypeptide chain geometry and definition of molecular interactions. properly designed reduced models make possible very effective search of the protein's conformational space. model simplifications, while beneficial in filtering out the majority of unrealistic structures, limit the degree of accuracy that can be achieved. in most contemporary approaches to protein structure prediction large sets of alternative models are built. proper selection of the best model is in many cases as difficult as obtaining very good models .

even in the simplest case of protein structure prediction – comparative modeling, the exact structure of a target protein differs from its nearest structural template used in modeling. such deviations can not be corrected on the low-resolution modeling level, a more detailed representation of the protein and more realistic force field are needed. unfortunately, more complex energy functions produce more rough energy landscapes, which consequently makes sampling much more difficult. thus, it seems reasonable to split the modeling process into two stages: fold assembly  followed by the model refinement/selection procedure, using a more detailed representation  and a more exact interaction scheme.

the first attempts at using the all-atom modeling as a final stage of hierarchical approach were applied to gcn <dig> leucine zipper – a very simple homodimer coiled-coil consisting of two 33-mer monomers  <cit> . the simulations were held in the times when even short macromolecular simulations were hardly possible due to limitations of computer power. ~ <dig> Å backbone rmsd  was achieved by means of reduced modeling of gcn <dig> leucine zipper, followed by a molecular dynamics annealing protocol  <cit> . such improvement was possible only with the help of α-helical constraints applied to each residue. a decade after the pioneering work by vieth et al.  <cit>  molecular dynamics was still too expensive for significant protein structure refinements. more recently, explicit solvent molecular dynamics and implicit solvent energy calculations on  <dig> small, single-domain proteins allowed a successful ranking of the near-native conformations and the best structure selection from predictions generated by rosetta method  <cit> . however, the simulations were unable to refine the best structures. de novo models produced by rosetta were also subjected to molecular dynamics simulations performed in explicit water  <cit> . rmsd values of the starting models increased during the short simulations, but longer simulations appeared to generate tighter packing of helices and regularization of β-strands in some cases. very encouraging result was also obtained by simmerling et al.  <cit> , who managed to significantly improve assembled on a lattice, low resolution structure of 29-mer cmti- <dig> protein . the final model had the correct packing of β-strands and was much closer to the native structure .

very interesting hierarchical approach to protein folding was developed by levitt group  <cit> . first, a large set of compact decoys was generated on a very coarse-grained lattice. then, fragments extracted from known structures were fitted to the lattice scaffolds. subsequently an elaborated procedure for the model selection and evaluation was performed. quite good structures were finally predicted. to some extent the present approach follows this idea, although the higher resolution lattice decoys enable a higher resolution modeling by the entire hierarchical scheme.

currently, probably the most successful refinement procedures use the all atom force-field that focuses on the short range interactions and monte carlo minimization. unfortunately, the methods consume a lot of computer power and can be used only for small protein domains  <cit> . the authors suggest that the primary bottleneck in a consistent high resolution prediction appears to be the conformational sampling. insufficient sampling misses the native basin and a false minimum could be selected.

here we show that by using a combination of a relatively high resolution sampling in a reduced conformational space, with the model selection by an all-atom detailed potentials and a high performance computing, the high resolution structure prediction can be achieved . to get such result the reduced models need to be diverse enough to cover the near-native subset of the conformational space. cabs  modeling tool was employed for this purpose  <cit> . cabs model was successfully used by kolinski-bujnicki group during the casp <dig>  experiment – the average score of the models submitted by this group was the second best among about  <dig> groups participating. interestingly, inspections of the simulation trajectories after publication of the target structures have shown, that there were always better models  than those submitted to the casp <dig> server. the lack of specificity of the cabs force field in a  <dig> Å vicinity of the native structure, was the main reason of the poor model selection. in this range the cabs energy is poorly correlated with rmsd for the majority of proteins. during the casp <dig> experiment the group mentioned above, did not have sufficient computer resources for the all-atom refinement. also, the role of even brief all-atom refinement in the proper model selection was underestimated at that time by the authors. nevertheless, several submitted models  were of very high accuracy, similar to the accuracy of crystallographic structures .

in the present work we have demonstrated that a short, all-atom minimization with fixed cα positions can properly rank-order large sets of near native decoys generated by cabs. in this context it becomes apparent that critical for the high-resolution protein structure prediction is ability to generate sets of models that contain some near-native structures. in comparative modeling with cabs, it could be achieved by using restraints extracted from various templates with alternative alignments in the uncertain regions. to our knowledge, that's the first approach enabling a meaningful refinement of large protein domains. the procedure proposed here may also work for small proteins in the template-free modeling. in such cases very large and diverse sets of decoys need to be generated and properly clustered before the all-atom based model selection. to further evaluate the proposed method for model assessment and ranking, we also performed tests on models generated by modeller  <cit>  – probably the most popular, versatile and quite accurate computational tool for comparative modeling. such models are collected in the moulder testing set  <cit>  – a comprehensive and well evaluated, present-day decoy set. numerous state-of-the-art methods for model selection were tested using this set. our method performed similarly well, or even better than majority of the other methods. very rigorous criteria of the model ranking assessments were used to make this comparison.

RESULTS
construction of the cabs decoy sets
we tested the proposed model ranking protocol on large sets of near-native decoys. we constructed a benchmark of  <dig> proteins  <cit> , which are representative in respect to their length and secondary structure content . none of these proteins was present, or had detectable homologs, in the library of the protein fragments used for the backbone reconstruction .

particular columns contain: the pdb code, the fraction of alpha helices, the fraction of beta strands, the protein length and the fraction of correctly built structures .

several studies have utilized different energy functions to discriminate the native structure among sets of decoys built in different ways  <cit> . typically, the decoys have been generated by means of various threading procedures. unfortunately, the decoys' sets built by threading contain many incorrect structures, mainly due to the alignment problems in the threading algorithms, resulting in incorrectly paired tertiary contacts or wrong secondary structure assignments. in contrast, park & levitt decoys' set  <cit>  was generated by means of a lattice modeling with constrained native secondary structure and covered wide range of rmsd values. decoys built by rosetta from lee et al. work  <cit>  exhibited varying topologies with locally optimized structure. the size of this set  is probably not sufficient for a clear estimation of the correlation between rmsd and energy. that makes such decoys' sets less challenging then those investigated in the current work. while threading methods are limited to the existence of structural analogs, high-resolution lattice models can be used efficiently in comparative modeling as well as in de novo structure predictions  <cit> .

in this work, long monte carlo simulation with the cabs model  <cit>  were performed in order to generate protein-like near-native decoys with rmsd in the range of  <dig>  –  <dig> Å from native . the cabs model features high computational efficiency and has the ability to cover the near-native conformational space  or can be used in de novo structure prediction. as can be seen from the example given in figure  <dig> and figure  <dig>  the near-native decoys generated by cabs consist of structures varying mainly in the most flexible regions as loops or near the ends of the secondary structure elements. we have decided to limit the range of the decoys diversity from about  <dig>  to  <dig> Å from the native. this is a typical range for the comparative modeling.

the main objectives of the use of molecular mechanics force field after the coarse-grained stage modeling in our work are: improving filtering of the crude models, providing better correlation with similarity to the native, and then identification of the best model . to reliably verify the correlation between the molecular mechanics energy and rmsd we decided to divide each protein subset  onto  <dig> bins, using rmsd from the native structure as a criterion for the classification . from each bin,  <dig> models or less if there weren't as many, were randomly selected. in this manner approximately  <dig> decoys were selected for each protein, with a broad spectrum of the quality of models .

from simplified to all-atom representation
employing simplified protein representation for exploration of the vast conformational space  brings the necessity of reconstruction of the reduced models to the all-atom representation, compatible with the classical all-atom modeling tools  <cit> . rebuilding procedure may also be beneficial when structures from different sources  are being compared  <cit> . recently, during the extensive tests of available methods for reconstruction of protein backbone from cα-trace  <cit> , we found that in the cases of the high accuracy models  the best performance is achieved by the procedure employing insertion of well adjusted fragments from known protein structures  <cit>  . such procedure improves the local geometry of the backbone. to assure the best possible reconstruction, we applied this method to our benchmark set. similarly to the main-chain backbone atoms, the side-chains were reconstructed and their conformations optimized using sybyl. it is worth noting, that the increase of the number of experimentally determined high-resolution structures in the protein data bank  may lead to further improvements in the all-atom reconstruction methods that use protein fragments from the pdb.

cabs decoys evaluation by all-atom minimizations
evaluation of protein models were done by all-atom minimization with frozen alpha carbons using amber <dig> ff <dig> force field and amber charges  <cit>  implemented in sybyl. the effect of solvent has been neglected and a uniform value of the dielectric constant was set equal to  <dig>  due to the frozen positions of the alpha carbons, this is probably an acceptable approximation. what important, it is often unknown whether the target sequence is a part of a larger oligomeric structure  <cit> . if it is, the solvation energy term would unnecessarily penalize for the exposed binding part of the protein surface. moreover, the ranking of large sets of decoys of relatively large molecules requires as fast as possible computations. that would be impossible, or very difficult, with the explicit treatment of the surrounding solvent  <cit> . also, the fixed positions of the alpha carbons prevent from evolution of the all-atom systems into directions of non-native local minima. on the other hand, a significant repacking of the model structures is rather unlike within the frozen cα approximation. the underlying assumption is that the set of decoys contains a fraction of a good-geometry near-native structures.

the results of minimization are illustrated in figure  <dig>  for each protein, the decoys' energies after  <dig> iterations of the sybyl minimization were plotted as a function of the cα-trace rmsd. for all proteins, resulting energies as a function of rmsd divide into two ensembles: wedge shaped low energy values  and abnormal high energy values . the abnormal high energy values, observed for a fraction of the decoys, resulted mainly from bond stretching and the van der waals repulsive energy contribution due to the rebuilding inaccuracies leading to the overlaps of some atoms. the decoys were produced by the low resolution search, with a very simplified representation of the side chains. this flattens the energy landscape but it also may result in a distorted geometry of the cα-trace. this is in the agreement with the observation that physic-based energy functions are sensitive to small displacements as opposed to the statistical energy functions  <cit> . the rebuilding procedure aimed at adjustment of protein fragments as closely as possible to the initial cα trace, and consequently was not always capable of constructing structures without some local defects. structures with small errors can be easy filtered out by a short minimization – range of  <dig> iterations, regardless of the protein length. this is sufficient to reject the decoys with the local defects  and it takes about  <dig> minutes per one structure of a large protein domain  on a single linux box. interestingly, in all cases such short minimization leads practically to the same correlation between energy and rmsd as a  <dig> times longer minimization . however, we found that while in the case of the high accuracy decoys a longer minimization didn't bring any substantial changes to the ranking, for filtering out the medium accuracy decoys  from the worse models, a longer minimization  led to better results in the identifying the best model .

the number of steric mistakes grows with the protein length . the exception is 2grrb, an all-alpha protein which was rebuilt the most accurately from the whole set. while clashes could be easily removed via a short relaxation of the entire structures, constructing instead a larger number of the reduced space decoys  seems to be a more effective option.

at this point it should be added, that there is nothing specific about the decoys generated by cabs with the subsequent all-atom rebuilding. the casp <dig> assessments have shown, that the local geometry of the cabs models was on average the same as the local geometry of the models built by means of other high-performance methods for protein structure prediction. this is mainly due to the fact, that various all-atom reconstruction procedures employ in a similar fashion protein fragments extracted from the high-resolution crystallographic structures. thus, the proposed method should work similarly well for decoys generated by means of different modeling algorithms.

evaluation of the moulder testing set
to test the ability of our protocol to discriminate a medium-accuracy models , from a low-accuracy models we used moulder decoys' set, evaluated by eramian et al.  <cit>  using  <dig> individual assessment scores, including physic-based energy functions, statistical potentials, and machine-learning scoring functions. each of the targets from the set was modeled using a template of <30% sequence identity, corresponding to challenging comparative modeling cases. no two alignments of a given target shared >95% of identically alignment positions. the target-template alignments were obtained using moulder  <cit>  with modeller  <cit>  to create  <dig> different target-template alignments.

of the  <dig> targets subsets, only  <dig> contain models better than  <dig> Å . we decided to reduce the testing set to these  <dig> proteins, since the sensitivity to small structural displacement make physical force-fields less suitable for the assessment of models with larger errors  <cit> . before the minimization procedure, coordinates of the alpha carbons of the models were extracted and subjected to rebuilding procedure, identical to the one applied to the cabs decoys' set.

particular columns contain: the pdb code, the secondary structure type, the protein length, the range of cα rmsd , the median of rmsd , the average Δrmsd of our method, in the brackets: the average Δrmsd of the best method  <cit>  and a ranking – the number of methods that outperformed our procedure , the Δrmsd on the whole subset.

the performance of the methods with the moulder decoys' set were measured by average rmsd difference  between the model identified as the best of the set and the model with the lowest rmsd. each of the sets of  <dig> models was split into  <dig> randomly populated smaller sets of  <dig> models. the purpose of this division was to reduce the impact of individual target sets on the final ranking and to increase the robustness of the benchmark. for each  <dig> model set, the model with the lowest cα rmsd  was used as a reference to calculate the Δrmsd measure. we followed the same rules, despite the fact that the average Δrmsd value of the  <dig> model set is not suited for evaluating of our procedure, which is aimed at assessing much larger sets due to its characteristics. our method narrows down the number of models in the testing set rejecting the fraction of them , due to their small inaccuracies. sensitivity for the small displacements is the price for the high discriminatory power  <cit> . additionally, due to the same reasons, it is desirable to provide a few copies of the model with the small differences, to maximize the chance of the accurate scoring. such sets of models , representative for a various type of conformations, can be easily extracted from reduced modeling trajectories by structural clustering  <cit>  and subjected to the evaluation procedure.

two worst cases, which are the two largest proteins from the set, illustrate the effect of narrowing down the number of models and insufficient number of good models in the set. the former situation is observed for the subset of 2fjbl models, where only one third had been the subject to the  <dig> iteration minimization, while the rest was rejected after  <dig> step minimization . the latter could be observed in the case of the 2cmd subset of models, which is the subset with the smallest number of models better than  <dig> Å .

omitting these two worst cases, our method performed similarly to the rosetta scoring function which is apparently the most successful in de novo high-resolution small protein structure prediction  <cit> . Δrmsd value averaged for the  <dig> subsets of proteins was  <dig>  for our procedure  and  <dig>  for the rosetta. corresponding values for two physic-based approaches used in the study by eramian et al. were  <dig>  and  <dig>  for gb  and eef <dig> , respectively  <cit> . the authors noted that in the selecting the best model from a set of very similar models eef <dig> and gb were more accurate than many of the statistical potentials tested. according to their suggestion it is possible that different relaxation schemes would have produced better results. they took also into consideration, that by including the solvent model, the oligomeric proteins were presumably harder to evaluate than monomers.

the ability of the all tested methods to identify native-like models greatly varied across different targets  <cit> . the most accurate methods that obtained the best results for the  <dig> targets considered here  were: dfire, modcheck and modpipe_combi implementing different kinds of statistical potentials and psipred/dssp   <cit> .

as we said earlier, the minimization with frozen cα has to be performed on a sufficient number of models. the number of models in the studied moulder subsets  seems to be enough . obtained Δrmsd on the whole subsets surprisingly well correlate with the minimum values of the rmsd , confirming the usefulness of our procedure in the high-accuracy modeling protocol. clearly, performance of our methods improves with increasing average quality of the decoys. thus, the analysis of the moulder decoys indicates that the best results of the proposed procedure are expected for the sets of relatively good models. this is actually a nice finding, since ranking of very bad models isn't useful anyway. it is also worth to mention that the final selection can be likely improved by a structural clustering of the best scored models.

CONCLUSIONS
in summary, the proposed simulation protocol makes possible fast and reliable assessment of high resolution structures for relatively large proteins. bradley et al.  <cit>  have shown recently that a high resolution de novo structure prediction can be achieved for small proteins by using an all-atom refinement procedure in the last stage of prediction. the cost of application of the high resolution refinement for large proteins was estimated by authors to require orders of magnitude more computing power, than the  <dig> cpu days required for small proteins  <cit> . a single, 500-step, minimization sufficient for the ranking of the smallest proteins  took in present work approximately  <dig>  minutes. the approach described here may be a good alternative for the refinement of small proteins and could be applied as a means of the best model selection in a large scale modeling, regardless of protein length.

protein model filtering in the endgame of protein structure prediction protocols faces the following two challenges: fold identification  and the selection of the best models from a set of good models . the results of this work apply mostly to the model selection in comparative modeling. recent casps test have shown that the best comparative models are built with a lot of human intervention using an assortment of well known modeling tools  <cit> . the challenge is to automate the protein prediction and produce even more accurate models with no need of the human assistance during the prediction protocol. thus, consistent and accurate last stage of modeling is needed, i.e., producing and filtering the high-resolution predictions. elaborate human intervention can be compensated by a high-throughput modeling, employing sampling of alternative alignments  and an efficient scoring of the large number of obtained decoys.

the need for a reliable detection of the best native-like models from a set of different predictions produced in the recent cafasp <dig> experiment  led to a new category of model assessments – model quality assessment programs . although performance of currently available mqaps indicate that some of these methods may be useful for new automated procedures, high false positive rates are observed, and the mqap methods were suggested to be used as additional elements of the prediction protocols rather than as a simple post-filter  <cit> .

our method gives surprisingly consistent correlation of the all-atom energies with rmsd distance from the native structure. decoys within  <dig> Å from native were examined and no false positive cases were noted. as mentioned before, the correlation of the cabs energy with rmsd in this range is often insignificant. the starting energy of the all-atom structures are uncorrelated with the cabs energy and are extremely high, mostly due to the overlaps of the side chains. the molecular mechanics energy decreases rapidly during the initial stage of minimization, mainly due to the improvement of side-chain rotamers position. for a fraction of decoys  the energy plateaus at a high level due to non-resolvable steric clashes. the subset of the low-energy structures, easily distinguishable from the high-energy ones, exhibits the above mentioned, nearly perfect correlation with the values of rmsd from the native structure. this provides a very strong support for the idea of multiscale high-resolution protein modeling. more extensive molecular dynamics simulation, than described here, might lead to even better model ranking and refinement. in the case of fixed cα-traces, a longer than performing  <dig> iterations minimization is not necessary – the results do not change anymore.

finally, we would like to return to the two important assumptions of the present method: fixed positions of the alpha carbons during the minimization and the in vacuum molecular mechanics. obviously, these assumptions significantly reduce the cost of computations for large sets of decoys. the frozen alpha carbon approximation works very well for the model ranking, although it eliminates possibility of a significant refinement of the entire structures. model ranking exercises performed on the moulder set by others have shown clearly that there is very little added value with use of more rigorous molecular mechanics procedures  <cit> . the same conclusion could be drawn from our experiments with defrosted alpha carbons and with a continuous model of solvent . the ability to generate sets of decoys containing a significant fraction of the near-native structures by coarse-grained modeling  remains a key factor for the high-accuracy structure prediction.

