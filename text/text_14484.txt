BACKGROUND
it is well-established that nucleosomes have significant preferences as to dna sequences they bind, and that these sequence preferences play an important role in a range of dynamic nucleosomal processes  <cit> . in order to better study correlations between sequence effects and biological function, it is necessary to get a grasp on the energetics of nucleosome-dna interaction. several approaches have been put forward. sequence-dependent models that directly address the mechanics of dna, such as the rigid base pair model  <cit>  can be combined with a suitable model for the nucleosome to access the energetics of nucleosome-bound dna .

another option is to use a bioinformatics model that defines a probability distribution on the space of all possible nucleotide sequences. the logarithm of such a probability distribution relates linearly to the free energy of a sequence when wrapped into a nucleosome. one such probability-based model has been put forward by segal et al.  <cit>  and this particular model now proves to be of interest beyond its original purpose, in that it can also be used in silico to provide a computationally efficient approximation to biophysical models that are themselves computationally too intensive. by speeding up the calculation of the affinity of a sequence for the nucleosome by a factor of around  <dig> , this approximative scheme makes it possible to use the biophysical nucleosome model of eslami-mossallam et al.  <cit>  to perform genome-wide analyses of nucleosome positioning signals. with this method, we have performed all-gene analyses of promoter regions for numerous organisms  <cit> , a feat that would have been computationally intractable without it.

here we perform an in-depth benchmarking analysis of this approximation to the eslami-mossallam et al. nucleosome model. we will examine to what accuracy the computationally efficient model approximates the predictions of the underlying model for the first chromosome of s. cerevisiae, and how this accuracy depends on several factors, such as the stringency of the assumptions that go into the approximation, the size of the sequence ensemble from which the model parameters are derived and the application of smoothing filters on those parameters. in doing so, we may also indirectly draw some conclusions as to the accuracy that may be expected of models such as that of segal et al.  <cit> , trained on experimental sequence ensembles.

methods
model
since a nucleosome wraps  <dig> base pairs worth of dna, the space of possible sequences contains  <dig> or about  <dig> possibilities. it is impossible to enumerate all of these, so a simple function is needed for the probability distribution.

segal et al. do this by treating a dna sequence as a markov chain of order  <dig>  where the probability of a nucleotide at a certain position depends only upon the preceding nucleotide. the probability of the sequence as a whole is the product of the probabilities of all the nucleotides it is composed of. more precisely, defining s as a sequence of length  <dig>  consisting of nucleotides s
i with i from  <dig> to  <dig>  
  <dig> p=p⋂i=1147si=ps147|⋂i=1146sip⋂i=1146si 



  <dig> =∏n=1147psn|⋂i=1n−1si, 


where we have applied the chain rule of probabilities. if we now introduce the assumption we mentioned earlier, that the probability of a nucleotide depends only on the preceding nucleotide, we find the expression given by segal et al., i.e. 
  <dig> p=p∏n=2147p. 


we should stress that the value of quantities like p depends not just on the value of s
n  but also on the position along the nucleosome, n. these probability distributions for, in the case of segal et al., dinucleotides, can be obtained by analyzing a suitable ensemble of sequences that have high affinities for the nucleosome. segal et al. generate such an ensemble from the genome they are interested in making predictions for, by mapping actual  nucleosome positions along the dna. although the original model did not perform very well  <cit> , this model has been applied with success – after a refinement of the model and employing a better training data set – to predicting nucleosome positions, by field et al.  <cit>  and kaplan et al.  <cit> .

these experimental probability distributions do not capture only the intrinsic mechanical preferences of the dna. they also capture inherent biases in the sample  and biases of the experimental method. this makes it difficult to evaluate the accuracy of the model, since both the training of the model and its testing generally rely on the same experimental methods, and there is the risk that agreement between the model and reality is overestimated because the model correctly fits experimental artifacts. therefore it becomes of interest to study the model in a theoretical framework, where we can isolate the purely mechanical effects.

ensembles to inform this type of bioinformatics model can also be generated from a theoretical nucleosome model using the mutation monte carlo  method  <cit> . this method adds mutation moves to a standard monte carlo simulation of a nucleosome, thereby sampling the boltzmann probability distribution of pairs of sequences and spatial configurations , 
  <dig> p=e−βe. 


by sampling the sequences during the mmc simulation, the spatial degrees of freedom of the nucleosome model are marginalized and one obtains the probability distribution of the sequences 
  <dig> p=∫dθe−βe 


and their free energy 
  <dig> f=−ktlog). 


note that in eqs. 4– <dig> we have neglected the overall normalization of the probability distributions by the partition function z, and hence a constant offset −k
t log to the free energy. because the probabilities we derive are simply relative frequencies with respect to our sequence ensemble, they are inherently normalized  and we have no information on the partition function. this is not usually an impediment as we are mostly interested in relative energy differences.

sampling the entire sequence space is not feasible, but making the same assumption about long-range correlations in the sequence preferences as segal et al., we can assume that we may write our p as in eq.  <dig>  it turns out it is feasible to produce a sequence ensemble large enough that the distributions p may be determined.

generalization of the dinucleotide model
we used an mmc simulation of the model put forward by eslami-mossallam et al. at 1/ <dig> of room temperature to generate an ensemble of  <dig> sequences, from which the oligonucleotide distributions were derived . at each position, we counted the number of instances of every mono-, di- and tri-nucleotide and divided these by the total number of sequences in order to obtain probability distributions.

this gives us the joint probability distribution p and not the conditional probability p that we need for eq.  <dig>  this is easily remedied. we can rewrite eq.  <dig> as 
  <dig> p=p∏n=2147pp=∏n=2147p∏n=2146p. 


we see that we can write this equation in terms of the probability distributions of mono- and dinucleotides that we can find from a sequence ensemble. analogously, if we want to expand the model to trinucleotides, we insert the assumption that the probability of a nucleotide depends only on the previous two  and we find 
  <dig> p=∏n=3147p∏n=3146p. 


this model can thus be applied using probability distributions for di- and trinucleotides, both to be obtained from a suitable sequence ensemble. the result easily generalizes to tetranucleotides and beyond. for mononucleotides, the model simplifies to 
  <dig> p=∏i=1147p. 


analysis
segal et al. test their model by predicting nucleosome positions along the genome they are studying and comparing with reality and they find that their model has some predictive power, even on genomes on which the method was not trained. however, their study is inevitably hampered by small statistics and their use of natural materials. the latter makes it difficult to judge the quality of their model.

the in silico methods allow us to test the model, as an approximation to the full underlying model, much more rigorously. because we can explicitly calculate the energy of a given sequence, we can directly measure the correlation between the energy given by the theoretical nucleosome model and the probability calculated by the bioinformatics model. using a standard monte carlo simulation of the nucleosome with a given sequence, we can measure the average energy 
  <dig> es=∫dθee−βe 


of the sequence. unfortunately, calculating the free energy using the eslami-mossallam nucleosome model is not straightforward, and we will be comparing <e>s as predicted by the biophysical model with f as predicted by the approximative model. at finite temperature, these quantities are not the same, differing by an entropic contribution. however, at low enough temperatures they converge. we will compare the predictions at 1/6th of room temperature, as some finite temperature is needed for the statistical simulations to function. in performing this comparison, we thus provide an upper limit for the discrepancy between the approximation and the real <e>s.

in order to generate an energy landscape with which to compare the results of the probability-based models, we take the first chromosome of s. cerevisiae  and perform a monte carlo simulation of the nucleosome wrapped with each 147-base-pair subsequence of the chromosome, using the nucleosome model put forth by eslami-mossallam et al. after letting the simulation equilibrate, we sample the energy of the system and take the average. in order to be able to compare this energy landscape with a probability landscape, we calculate the  probability distribution and normalize this over the set of sequences for which we calculated the energy, and then take the logarithm to regain our  energy landscape.

analogously, we use the probability-based model to generate a probability landscape of the same sequence. this we normalize over the set of sequences analyzed and convert to an energy using eq.  <dig>  we find that this procedure is about five orders of magnitude faster than using the full biophysical model.

we only know the free energy up to some constant offset, but by making sure both the real energy landscape given by the energetic model and the approximate energy landscape provided by our probability-based model have the same normalization, we can readily compare the two.

in doing so, we may draw some conclusions about this kind of markov-chain model not only as it relates to the nucleosome model we consider here, but about the assumptions that go into it in general, i.e. the explicit assumption of short-range correlations and the implicit assumption that the sequence ensemble on which the model is being trained is large enough. to test the first assumption, we extend the dinucleotide model used by segal et al. to mononucleotides  and trinucleotides  and compare their accuracy. for the second, we examine the accuracy of these three models as a function of the ensemble size on which they are trained.

RESULTS
we tested and compared three different probability-based models, namely the segal et al. dinucleotide model, its simplification to mononucleotides and its extension to trinucleotides. following the methodology outlined in the previous section, we arrive at correlation plots for the energy as given by the energetic model and as predicted by the probability-based models. the results are presented in fig. 1a–c.
fig.  <dig> accuracy analyses of the various models, benchmarked on the first chromosome of s. cerevisiae. a histogram of the energy prediction pairs of the full model and mononucleotide approximative model for the same sequences. the black diagonal indicates perfect agreement. b, c as a for the dinucleotide and trinucleotide approximations, respectively. d comparison of the root mean square deviations of the approximative predictions from those of the full model. the grey bars indicate the rmsds of ‘bad’ models, defined for the full and average signals as a uniform landscape, and for the periodic signal as the real landscape shifted out of phase. the other values, for the mono-, di- and trinucleotide approximations are compared with these bad models. indicated above each bar is a percentage indicating the value relative to the corresponding bad model




as we might expect, the longer the oligonucleotides we use, the better the agreement becomes. an important cause of the deviation from perfect agreement, apart from the spread, is a clearly visible deviation in the slope. the mononucleotide model significantly underestimates the spread in energies. this means that the mononucleotide model is not capturing effects that set sequences apart from each other. this effect is expected and should be remedied by going to longer oligonucleotides. indeed we see this deviation greatly decreased for the dinucleotide model, and even more so for the trinucleotide model.

for a more detailed grasp on the quality of the predictions, we separate out two components of the energy landscape that are important on their own. the first is the periodicity of the energy landscape. due to the helical nature of dna, energy landscapes for the nucleosome show a roughly 10-base-pair periodic signal. it is important that any model for nucleosome affinity gets the frequency and phase of this periodicity right. the second property, complementary to the periodicity, is the overall energy level of the sequence. this aspect will show us how well the model captures long-range effects.

for the purposes of benchmarking, we define the local average as the 11-base-pair running average of the energy landscape, i.e. over about one period. the pure periodicity of the signal we analyze by subtracting from the signal its local average as just defined, making the signal oscillate around zero. our benchmarking results then consist of the root-mean-square deviation  for the full signal , for the locally averaged signal and for the pure periodicity signal.

to get a sense of what the rmsd values we find actually mean, we compare them to the rmsd value we find when we use a bad model. for the overall signal and the locally averaged signal, we define this bad model to be one that contains no sequence information at all, i.e. a perfectly uniform landscape. for the periodicity, this is not such an interesting comparison because for a periodic signal, a uniform landscape is still right twice per period. instead we utilize as a bad model the same signal, but shifted by half a period, to push it out of phase.

rmsd values gathered from such bad models tell us about the typical size of the structures in the energy landscape that our models need to predict. we can then measure the rmsd from our benchmarked models relative to this scale. fig. 1d displays the results. we see a decrease in rmsd when going to longer oligonucleotides in each of the three cases. the dinucleotide model, as used by segal et al., already performs well, with an overall rmsd of 7%. noteworthy, it is much more accurate than the mononucleotide model. however, we see that we could improve our results still by going to trinucleotides. especially the local average is predicted much more accurately by the trinucleotide model, cutting the rmsd by about a third.

the importance of sample size
because we can produce large ensembles of sequences in silico with the mutation monte carlo method, we are now also in a position to get a measure of how large an ensemble we need for our models to make accurate predictions.

in their  <dig> study, segal et al. manage to build an ensemble of ∼ <dig> sequences. apart from the inherent biases that may be present in their ensemble due to their use of nonrandom yeast dna, this is not a very large ensemble, and we should check what the effects of such limitations are.

in a later study, kaplan et al. perform a similar study, where they obtain  <dig> , <dig> sequence reads.  <cit>  the ensemble is again trained on the yeast genome, which is some  <dig> , <dig> base pairs long. the number  <dig> , <dig> should therefore not be mistaken for the ensemble size. there must necessarily be many duplicate and strongly overlapping sequences in their ensemble, which arise artificially because only a small subset of sequence space is available for sampling. giving a meaningful number for the effective sample size of such an ensemble is difficult. however, a sequence of ∼ <dig> base pairs can yield 104− <dig> completely non-overlapping nucleosome sequences, which we may employ as a conservative estimate.

later similar work using the mouse  <cit>  and human  <cit>  genomes has yielded larger ensembles. these genomes are two orders of magnitude larger than that of yeast, and so also provide that many more non-overlapping sequences.

in our in silico simulations, we built an ensemble of  <dig> independent sequences from which we derived our probability distributions. we took subsets of these sequences to see what the effects of smaller sample sizes are. the problem when statistics are small is not just that the probability distributions are less accurate. we additionally run into the issue that some rare dinucleotides simply do not appear in the ensemble at all. the estimate of their probability then becomes zero. the problem is that if any of the factors in eq.  <dig> is zero, the entire product becomes zero, rendering the model useless.

for segal et al. and kaplan et al. this problem does not arise, because they do not need to work at low temperatures, but also because they apply a smoothing to their probability distributions. they estimate the probability p
n of a dinucleotide by averaging over not just position n, but also n− <dig> and n+ <dig>  this is justified by the observation that their experimental method does not provide them with a sharp resolution down to the base-pair to begin with. the effect of such smoothing is not a priori clear, however. in a landscape with 10-bp periodicity, taking a 3-bp running average could have averse effects. such smoothing may not be necessary or beneficial when applied to higher-resolution data.

we therefore propose an alternative method, where instead we consider a probability of zero, for any position, a failure of the ensemble. in such a case we conclude that we simply do not have any information, i.e. we artificially insert a flat conditional probability of  <dig> .

in fig.  <dig> are presented the rmsds of the full landscape, as predicted by our probability-based models, with probability distributions derived from various ensemble sizes. we find that smoothing the distributions gives results that are strictly worse than simply assuming no information when an issue arises.
fig.  <dig> variation of the rmsds of the various models with the size of the sequence ensemble from which their parameters are calculated. solid lines: zero-probability issues are dealt with by assuming zero information. dashed lines: probability distributions are smoothed with a 3-bp running average. the performance when smoothing is strictly worse




we can conclude from this plot that the model of kaplan et al., even with a conservative estimate for their effective ensemble size, should perform well. the dinucleotide model converges to its maximum accuracy at only  <dig> sequences. of course, caveats surrounding the non-randomness of the dna being sampled remain.

for larger experimental ensembles  it is advisable to move to a trinucleotide description. starting from 5× <dig> sequences, this model becomes more accurate than the dinucleotide model.

CONCLUSIONS
with the methods available for the first time to produce sequence ensembles for nucleosome affinity based on an energetic model of the nucleosome, we investigated the capacity of a class of probability-based models to approximate real energetics. as an approximative scheme to the nucleosome model of eslami-mossallam et al.  <cit> , we find errors on the order of  <dig> kt. this is not an insignificant disagreement, but depending on the application, this price may well be worth paying for the vast reduction in computational complexity by a factor of  <dig> unoptimized. vast increases in speed can also be expected for other complex biophysical models.

considering the assumption of short-range correlations, we find that dinucleotide models such as those used by e.g. field et al. and kaplan et al. already perform well, with a root mean square deviation of about  <dig> kt . however, we also find that improvement could be achieved by going to a trinucleotide model , and by avoiding the smoothing of the probability distributions.

we also looked into the effects of small ensemble sizes, and we find that an ensemble such as used by field et al., although caveats must be acknowledged as to likely inherent biases in their experiment, is sufficient for the dinucleotide model to reach its fundamental accuracy. for larger ensembles  such as provided by the mouse or human genome, however, we recommend that the trinucleotide approximation be used for higher accuracy.

we hope, however, that our work will motivate the experimental community to look into mapping nucleosomal sequence preferences experimentally using more random dna sequences than are provided by natural genomes. a starting point could be a very similar study done on dna rings  <cit> . this would allow us to better examine the intrinsic sequence preferences of nucleosomes without biasing them towards a genomic context.

additional files

additional file  <dig> mononucleotide distributions. table in tab-separated format denoting the mononucleotide probability distributions generated by our mutation monte carlo simulation. 





additional file  <dig> dinucleotide distributions. table in tab-separated format denoting the dinucleotide probability distributions generated by our mutation monte carlo simulation. 





additional file  <dig> trinucleotide distributions table in tab-separated format denoting the trinucleotide probability distributions generated by our mutation monte carlo simulation. 




abbreviations
mmcmutation monte carlo

rmsdroot-mean-square deviation

