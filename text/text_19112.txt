BACKGROUND
successful comprehension of connected speech can be seen as an auditory scene analysis problem  <cit>  involving the matching of the acoustic properties of the incoming voice signal with memory representations of speech. this is not a linear bottom-up process, but one that can be modified in a top-down fashion by long-term memory traces of, for example, one’s native language mediating syntactic and semantic content  <cit>  as well as information concerning affective aspects of the speaker  <cit> . prior expectations of the stimuli can dramatically alter the perception of acoustically distorted speech, rendering initially unintelligible stimuli entirely comprehensible . despite increasing efforts in the study of the neural basis of speech perception, it has proven difficult to distinguish between the cortical processes related to the bottom-up extraction of acoustic features of speech and facilitating top-down mechanisms, such as recognition memory for linguistic content. one reason for this is that in most studies the effects related to speech comprehension have been investigated by comparing brain responses to intact speech with those elicited by acoustically degraded versions of the stimuli. having two or more acoustically different types of stimuli poses a major problem: acoustic variability in itself leads to differences in the response and, consequently, the relative contributions of acoustic feature processing and speech comprehension become confounded and, thus, it is difficult to isolate the effects specific to speech comprehension. an experimental paradigm which controls for acoustic variability would appear to be needed in order to simultaneously capture both the bottom-up and top-down aspects of speech comprehension.

in electro- and magnetoencephalography  recordings of the human brain, the presentation of short-duration  auditory stimuli typically results in transient responses and, in the case of long-duration stimuli , the transient responses are followed by a sustained response. these responses are suited for revealing both the spatial and temporal evolution of cortical activity. notably, the transient auditory n1m response generated in the auditory cortex  <cit>  is sensitive to multiple aspects of speech such as fundamental frequency  <cit> , formant transitions  <cit> , intonation  <cit> , place of articulation  <cit> , the periodic structure of vowel sounds  <cit> , and the phonetic features of consonants  <cit> . these observations indicate that the auditory cortex carries out parallel bottom-up processing of the acoustic properties of speech sounds, independently of the subject’s attentional focus. further, the n1m has also been found to be a sensitive measure of the extraction process with which the human brain segregates speech signals from various types of noise contributions  <cit> . while these eeg/meg studies, utilizing short  isolated vowel sounds in no-task  recording conditions, have revealed the link between transient activation and the bottom-up extraction mode in acoustic feature processing, the role of top-down influences on the processing of meaningful speech has remained largely unaddressed. therefore, the above studies should be complemented by investigations focusing on the sustained activity elicited by connected speech, thus potentially revealing how activity spreads to multiple cortical brain areas performing speech processing.

a growing body of hemodynamic evidence suggests that the cortical processes underlying speech comprehension operate in a hierarchical fashion, with the auditory cortex activated by the acoustic properties of sounds , and the regions anterior and posterior to auditory cortex being sensitive to the intelligibility of speech  <cit> . the anatomical areas underlying the speech comprehension network have been investigated by contrasting cortical responses to speech with responses to closely matched non-speech stimuli, such as noise-vocoded sounds  <cit>  or tonal stimuli  <cit> . these studies indicate that areas in the superior temporal sulcus  respond more vigorously to speech than to non-speech stimuli, and that sts regions anterior and posterior to auditory cortex are sensitive to the intelligibility of the stimuli. in addition, when speech stimuli with different levels of intelligibility are presented, overlapping temporal cortical areas seem to be activated during passive listening  <cit> , active listening  <cit> , and active recognition tasks  <cit> .

currently, the evidence pertaining to the role of auditory cortex seems to be contradictory, with findings indicating that this brain region is either sensitive  <cit>  or insensitive to speech intelligibility . given that top-down information - such as prior expectations of the stimuli - can substantially alter the perception of degraded speech  <cit> , it seems plausible that the extraction of acoustic cues from the distorted signal might be enhanced through feedback connections from higher-order cortical areas to auditory cortex. as a result, these changes in cortical processing might be observed in the activity of the auditory cortex as well. given that hemodynamic measures lack in temporal acuity, the millisecond resolution of eeg/meg measures of transient and sustained brain activity  might provide complementary information to fmri findings on this issue.

the current meg study assesses how the intelligibility of connected speech is reflected in the temporal evolution of activity in auditory cortex and surrounding areas. the study capitalizes on the phenomenon that the intelligibility of speech signals can be manipulated without changing the acoustic structure of the stimulus. accordingly, the experiment consisted of three consecutive sessions during which the same set of sentences was presented in distorted, undistorted, and - once again - in distorted form. as a result, acoustically identical distorted stimuli were perceived as either unintelligible or intelligible, depending on whether the subject had previously been exposed to an undistorted, intact version of the sentence. any possible change in brain activity, then, cannot be attributed to changes in the acoustic structure of the stimuli but, rather, to the top-down processes related to speech comprehension. we hypothesized that the activity generated in auditory cortex and reflected in the n1m and p2m responses would be sensitive to the acoustic variation in the speech signal  <cit>  whereas the sustained activity following transient activity might be responsive to whether speech is intelligible, and less sensitive to the acoustic attributes of the stimuli. given the novelty of the proposed experimental paradigm, our aim was to proceed with caution and to provide a tentative description of brain events related to speech intelligibility uncontaminated by the effects caused by attentional engagement  as well as by planning and execution of motor responses. therefore, the current study focuses on brain activity obtained in the passive recording condition.

methods
participants
ten healthy, right-handed volunteers  participated in the study with written informed consent. in a pre-measurement questionnaire, all the participants declared themselves to be native, right-handed finnish speakers with normal hearing. the experiments were approved by the ethical committee of the helsinki university central hospital.

stimulus material
the stimuli were created from speech data spoken in finnish by a professional logopaedist. the recordings were made in an anechoic chamber with a high-quality condenser microphone . the speech waveforms were sampled at a rate of  <dig>  khz with an amplitude resolution of  <dig> bits. to remove any low-frequency fluctuation picked up by the microphone, the signals were high-pass filtered with a 6th order butterworth filter .

the speech data recorded consisted of  <dig> finnish sentences, comprising six to seven words  of the finnish language. the sentences were constructed from three parts, including seven starting words, three sentence stubs, and four ending words. the starting and ending word was always a noun whereas sentence stubs involved nouns, objectives and verbs. in order to prevent any transient, time-locked activity occurring in the averaged data during the sustained field time range , the sentence stubs were constructed so that the acoustic structure of all the sentence stubs deviated from each other. this procedure resulted in a set of syntactically correct sentences which included both semantically meaningful  as well as meaningless  exemplars. in order to arrive at a sufficient signal-to-noise ratio  in meg recordings, each subject was presented with a total of  <dig> sentences, with  <dig> random repetitions from the original 84-sentence set.

the quality of the generated sentences was degraded by decreasing the amplitude resolution of the temporal waveform using the uniform scalar quantization  technique  <cit> . in this procedure, the maximum absolute value of each sentence is first determined. by using this value, the sentence is scaled to cover the full dynamics of the 16-bit amplitude scale, that is, each signal sample is rounded off to its nearest integer number and there are altogether  <dig> such integers between the smallest negative value  and the largest positive value . a distorted version of each sentence is then computed by using 1-bit quantization, in which the number of quantization levels is radically reduced from  <dig> to just two. all in all, this quantization procedure yielded two versions of each sentence which, in the following, will be referred to as the undistorted  and distorted  sentence. after the quantization, the intensity  of the undistorted and distorted sentence was equalized and, finally, the onsets and offsets of the stimuli were smoothed with a 5-ms hann window. examples of waveforms can be found in  <cit> .

experimental design
the experiment was designed to study the effects of acoustic degradation and intelligibility of connected speech on cortical activity . during the experiment, the participants first listened to the distorted sentences , then to the undistorted versions of these sentences , and finally to the distorted sentences again . the offset-to-onset inter-sentence interval was  <dig> seconds. the subjects were instructed to focus their gaze on a fixation cross while listening. the sentence was followed by a 1-sec break. subsequently, a question screen inquiring whether the sentence was intelligible or not was displayed, and the subject responded with a button press  within a 3-sec time window. after the active recording condition, the sentences were presented in the passive condition during which the subjects were under instruction to watch a silent movie while ignoring the auditory stimuli.


intelligibility of noise-distorted speech has been studied widely in psychoacoustics and speech communication technology, and both objective and subjective assessment measures of speech intelligibility have been adopted. objective measures to predict speech intelligibility in the presence of noise involve, for example, the articulation index  <cit> , the speech transmission index  <cit> , and the speech intelligibility index  <cit> . subjective measures make use of listeners and various types of speech material, either synthetic or natural, involving stimuli such as phones, words, sentences and sometimes even free conversation. in subjective evaluation, speech stimuli are typically played only once to listeners who then write down what they believe to have understood when listening to the corrupted utterances. an intelligibility score is then computed as a percentage of speech elements correctly reported. subjective intelligibility scores used in speech perception studies include, for example, word error rates  <cit>  as well as sentence and consonant identification rates  <cit> . in the current study, however, neither objective measures nor the subjective scoring methods mentioned above were used in the evaluation of speech intelligibility. instead, subjective intelligibility was evaluated by asking the subject to grade the speech sentence in a binary manner as either completely intelligible or unintelligible. by choosing the former, the subject indicated that she/he had understood the sentence correctly. by choosing the latter, the subject indicated that she/he was unable to understand the sentence. instead of counting quantitatively the percentage of words correctly recognized, this binary subjective intelligibility score measures solely whether the subject considers that she/he was able to understand the meaning of the heard sentence. therefore, it is a genuinely subjective means to evaluate intelligibility of corrupted speech and well suited for use in the present study to investigate speech processing in human subjects.

data acquisition
auditory evoked fields were recorded using a 306-sensor whole-head meg device  in a magnetically shielded room. the recorded signals obtained by the  <dig> gradiometers were sampled at a rate of  <dig>  khz and low-pass filtered online with a cut-off frequency of  <dig> hz. horizontal and vertical eye movements were monitored by two electrode pairs. the position of the participant’s head with respect to the meg sensor array was determined with four head-position indicator  coils before the beginning of each measurement session. the hpi coils were localized with respect to the nasion and the preauricular points using a 3-d digitizer. the head-based coordinate system was defined by the x-axis passing through the preauricular points , the y-axis passing through the nasion , and the z-axis as the vector cross product of the x- and y-unit vectors. the participant was instructed to remain stationary during the experiment. the auditory stimuli were binaurally delivered to the subject’s ears through plastic tube earphones with an average intensity of  <dig> db spl.

offline preprocessing of the meg data
in order to exclude gradiometer sensors with recording artifacts or a low snr, the recorded data was first visually inspected. to suppress magnetic noise, spatio-temporal signal-space separation  was performed for the data using elekta neuromag maxfilter. two data sets with different filtering and averaging parameters were extracted from the data, one for examining the transient n1m and p2m responses, and the other for studying the sustained field . the data set for the transient response analysis was filtered with a 2- <dig> hz band-pass filter, and the data set for the sustained field analysis with a  <dig> hz low-pass filter.

for averaging the data, the epochs were time-locked to the beginning of the stimuli, and amplitude-corrected using a 100-ms pre-stimulus baseline. epochs with excessive magnetic field gradient amplitudes  or with large eye movements  were excluded from the average. a 500-ms window was used in averaging the transient response data, and a 3000-ms window for the sustained field data. the data of one subject was discarded due to a weak snr and eye-movement artifacts. furthermore, the data of another subject was excluded from the analyses of the sustained field due to an insufficient number of averaged epochs.

meg data analysis
latencies, amplitudes and source localization of the transient responses
the latencies and amplitudes of the n1m and p2m responses were determined separately in each hemisphere from the peak values of the responses. the peak amplitude was calculated as the maximum magnitude of the vector sum from the sensor pair exhibiting the maximum response within a 90- <dig> ms time window for the n1m, and a 150- <dig> ms window for the p2m response.

the source locations of the n1m and p2m responses were estimated by equivalent current dipole  analyses  <cit>  conducted separately in the left and right hemisphere. a single ecd was fitted to the data at the n1m and p2m peak latency using a subset of  <dig> gradiometer sensors covering the temporal areas of each hemisphere. a spherical model was used to model the conductivity of the head. the ecd analyses were carried out using the elekta neuromag xfit source modeling software.

amplitude of the sustained field
the sensor pairs yielding the maximal transient responses also exhibited prominent sustained fields. the amplitude of the sustained field  was quantified as the mean amplitude of the vector sum over a predefined time interval. two distinct phases were observed in the aef waveform: a large deflection within the 300- <dig> ms time window, and a relatively static part within the 1000- <dig> ms time interval. thus, in each hemisphere, the amplitude of the sustained field was calculated separately over the 300- <dig> ms , and 1000- <dig> ms  time intervals.

current distribution estimates
to study the spatial distribution of cortical activity, noise-normalized minimum-norm estimates  were calculated. to this end, noise-covariance matrices were computed from the 100-ms pre-stimulus baselines of the individual epochs in the maxwell-filtered raw data. the forward solutions and the inverse operators were calculated for each session by employing a boundary-element model computed using average head and skull surface reconstructions provided with the mne-suite software.

mne and sloreta  <cit>  estimates were computed at 5-ms intervals for the data of each individual subject. to study the current distribution during the transient responses, both estimates were averaged over 40-ms time windows centered at the peaks of the n1m and p2m responses. the peak latencies of the responses were obtained from the gradiometer analyses . fixed time windows of 300- <dig> ms  and 1000- <dig> ms  were used in analyzing the current distribution during the sustained field responses.

region of interest analysis
the cortical surface used in calculating the mne estimates was divided into  <dig> regions of interest ,  <dig> regions in each hemisphere. the rois used in the analyses are depicted in figure  <dig>  the rois were labeled according to their physical location: anterior/central/posterior, superior/inferior, and temporal/parietal region. the rois were selected from the parietal and temporal cortices with emphasis on examining the spread of activation from the primary auditory cortical areas within the superior temporal gyrus to regions anterior and posterior to the auditory cortex, and to the parietal regions  <cit> . the rois were centered on the auditory cortical areas within the superior temporal gyrus . we emphasize that due to a lack of individual structural mri data, the rois should be regarded as approximations of particular cortical areas only. for example, wernicke’s and broca’s area lie roughly within pst and ast, respectively, and motor areas within the pre- and postcentral gyrus can be found in aip and cip, respectively  <cit> . the mean current for each roi was calculated separately for the transient and sustained responses as the average of the mnes over all the voxels within each roi . the mne values were extracted from the original mnes without noise normalization.


statistical analyses
repeated-measures analysis of variance  was used to analyze 1) the amplitudes, latencies, and source location of the transient responses, 2) the mean amplitudes of the sustained responses, and 3) the mean currents within each roi. all the anovas were of the design 2×2× <dig>  with the within-subjects factors of hemisphere , recording condition , and degradation . the effects of intelligibility  on the cortical activity measures were analyzed by post-hoc comparisons  of the degradation factor levels.

RESULTS
intelligibility of the sentences
during the meg measurements, the subjects first listened to sentences acoustically degraded through amplitude quantization. these were then followed by the same set of sentences in undistorted sound quality. finally, the subjects heard the degraded sentences again . subjective intelligibility, that is, the proportion of sentences the subject reported having understood was  <dig> %  for the undistorted sentences, and it increased substantially between the first and the second presentation of the degraded sentences for all of the subjects . the mean proportion of sentences reported as intelligible was  <dig>  percentage points lower for the first presentation  than for the second presentation of the degraded sentences = <dig> , p< <dig> ). this increase in subjective intelligibility from 30% to 80% for acoustically identical stimuli demonstrates how a single presentation of intact speech material can drastically alter the subject’s ability to comprehend degraded connected speech.


activity in auditory cortex
in the first stage of the analyses, the local activation of the auditory cortex was studied separately in the left and right hemisphere by using the pair of gradiometer sensors in each hemisphere exhibiting the largest responses. prominent n1m and p2m responses were elicited at the beginning of all sentences, as depicted in figure  <dig>  which shows the first  <dig> ms of the aef. the aef over the longer, 0- <dig> ms period is shown in figure  <dig> 


amplitudes of the transient responses
the mean amplitudes of the n1m and p2m responses were larger for the distorted sentences than for the undistorted sentences. the mean amplitude of the n1m increased from  <dig>  ft/cm to  <dig>  ft/cm as a result of sound degradation = <dig> , p< <dig> ). the p2m response was also larger for the distorted  than for the undistorted  sentences = <dig> , p< <dig> ). the effect of distortion on the p2m amplitude was more pronounced in the right hemisphere than in the left = <dig> , p< <dig> ). post-hoc tests revealed that this hemispheric asymmetry was due to a smaller p2m response for the undistorted sentences in the right hemisphere  than in the left . furthermore, the n1m and p2m amplitudes were equally large for the first and second presentation of the degraded sentences, indicating that the increase in perceptual intelligibility  was not reflected in the amplitudes of the transient responses.

latencies of the transient responses
in comparison to responses elicited by the undistorted sentences, degradation of sound quality resulted in earlier n1m and p2m responses . the latency of the n1m decreased from  <dig> ms to  <dig> ms = <dig> , p< <dig> ), and that of the p2m from  <dig> ms to  <dig> ms = <dig> , p< <dig> ). in addition, the mean latencies of the n1m were longer in the left hemisphere  than in the right = <dig> , p< <dig> ). however, the effect of distortion on the p2m latency was larger in the right hemisphere than in the left = <dig> , p< <dig> ), with the degraded stimuli eliciting the p2m response  <dig> ms earlier in the right  than in the left  hemisphere . moreover, given that both the first and the second presentation of the distorted stimuli resulted in unvarying n1m and p2m latencies, it appears that the intelligibility of the degraded sentences does not affect the timing of transient brain activity.

source locations of the transient responses
changes in the source locations of the transient responses were investigated by fitting a single equivalent current dipole  at the response’s peak latency in each hemisphere. the source locations of the n1m and p2m responses were modified by sound degradation and attention. in the left hemisphere, the ecds for the n1m response were located  <dig>  mm medial for the distorted stimuli  compared to those for the undistorted stimuli = <dig> , p< <dig> ). in addition, the n1m sources were more superior during active listening than in the passive conditions = <dig> , p< <dig> ). however, the effect of attention on the n1m source location depended on sound degradation = <dig> , p< <dig> ). post-hoc tests showed that the n1m ecds were more superior in the active condition only during the first presentation of the distorted sentences .

sound degradation resulted in a  <dig> -mm shift of the p2m sources along the anterior-posterior axis, the sources of the p2m being more posterior for the distorted stimuli  than for the undistorted stimuli = <dig> , p< <dig> ). furthermore, the p2m ecds were more medial in the right hemisphere during active listening  than in the passive conditions = <dig> , p< <dig> ). the effect of attention on the lateral-medial position of the p2m sources was dependent on sound degradation = <dig> , p< <dig> ). more specifically, the ecds for the active and passive conditions diverged only when the stimuli were undistorted .

sustained responses
the early  and the late  part of the sustained field  were analyzed separately . the early sf had a higher mean amplitude than its late counterpart = <dig> , p< <dig> ). the early sf was stronger in the active conditions  than in the passive conditions = <dig> , p< <dig> ). in addition, the effect of sound degradation on the early sf amplitude approached statistical significance = <dig> , p< <dig> ), the distorted sentences yielding larger responses  than the undistorted ones . post-hoc comparisons revealed that the early sf amplitude elicited by the first presentation of the degraded sentences was significantly increased in comparison to the amplitude for the undistorted stimuli . overall, the early sf was more pronounced in the left hemisphere, with this asymmetry approaching statistical significance = <dig> , p< <dig> ). all other statistical comparisons yielded non-significant results.

activity in auditory cortex and surrounding areas
in the second stage of the analyses, we elucidated how activity in auditory cortex and the surrounding areas evolves over time during the processing of connected speech. to this end, brain activity was analyzed using minimum norm estimates  during the time ranges of transient  and sustained  brain activity. the current distribution was studied by dividing the temporal and parietal cortical surface into  <dig> regions of interest  and calculating the mean currents over the voxels inside these regions . the statistical results on the intensity of cortical activity within specific rois are given in table  <dig>  a graphical summary of the effects of acoustic degradation and intelligibility of speech on the strength of cortical activity within specific rois is shown in figure  <dig> 


anova results for the hemisphere, degradation and attention of the source current strength within specific rois during the n1m, p2m, early sf and late sf time windows. only statistically significant effects are shown.

n1m time range
the strongest source currents during the n1m time range were localized in the vicinity of the auditory cortical areas . cortical activity was more pronounced in the right hemisphere  than in the left  within the anterior inferior parietal , anterior superior temporal  and posterior superior parietal  regions. furthermore, the mean current strength was also higher in the right hemisphere than in the left within the central superior parietal  area, although this hemispheric asymmetry was observed only during passive listening of the sentences. the mean currents during the n1m were affected by stimulus degradation in the central inferior parietal  and central superior temporal  regions. in the case of cip, post-hoc tests indicated that the mean current was higher during the second presentation of the degraded sentences  than during the undistorted sentences . within the cst, both the first and the second presentation of degraded speech yielded stronger activation  than undistorted speech . altogether, the central parts of the superior temporal and inferior parietal regions were responsive to acoustic degradation of speech during the n1m time range.

p2m time range
the current distribution was more widespread during the p2m than during the n1m . the mean currents were stronger in the right-hemispheric ast, psp and central inferior temporal  regions  than in their left-hemispheric counterparts . moreover, the average current strength was higher during passive  than active  listening within the psp. a number of regions exhibited sensitivity to the intelligibility of speech in the passive listening condition: in the ast and cst, the cortical activity was weaker during the  first presentation of the degraded sentences  than during the undistorted stimuli  and the second presentation of the distorted stimuli . a comparable intelligibility effect was observed also in the anterior inferior temporal  and posterior inferior temporal  areas, with the unintelligible stimuli yielding weaker currents than the intelligible ones. in the pit, however, only the second presentation of degraded speech  yielded significantly stronger currents than the first presentation of the same stimuli . in turn, in the ait, only the undistorted sentences  resulted in significantly stronger activation than the first presentation of degraded stimuli . taken together, these findings suggest that a number of regions within the temporal cortex could be sensitive to the intelligibility of speech during the p2m time window, given that increased currents seem to be associated with the presentation of intelligible sentences.

early sf time range
cortical activation during the sustained field  was distributed over a large area within the temporal and parietal regions . as demonstrated in figure  <dig>  the cst and cit regions were sensitive to degradation of speech during the early sf, with the distorted sentences resulting in stronger activity  than the undistorted ones . furthermore, the effect of acoustic degradation on the early sf was dependent on attention within the cip and pst areas. the pst, in particular, was sensitive to speech intelligibility in the passive condition as the unintelligible first presentation of degraded speech  yielded a weaker mean current than in the two other conditions . a similar effect was observed also in the cip, although it failed to reach statistical significance in the post-hoc comparisons. thus, it seems that pst and, possibly, cip are sensitive to speech intelligibility during the early part of the sf.

late sf time range
as shown in figure  <dig>  activation within the csp and psp was stronger in the right hemisphere  than in the left  during the late sf. in addition, mean currents were higher in the csp during passive  than active  listening of speech. the current strength during the late sf varied also with stimulus degradation in the cip, although this effect depended on the listener’s attentional state. more specifically, in the passive condition, the first presentation of the degraded sentences resulted in weaker currents  than during the second presentation of the degraded stimuli . the mean current during the undistorted sentences  was also stronger than the current elicited by the first presentation of the degraded sentences, although the difference was not statistically significant.

discussion
in the current study, we set out to investigate how the intelligibility of connected speech is reflected in behavioral measures as well as in the concomitant activity in the auditory cortex and surrounding brain areas. by varying the intelligibility of the stimuli while keeping the acoustic features of the stimuli constant, the experimental design allowed us to tentatively identify cortical processing related to speech comprehension. initially unintelligible, acoustically distorted sentences resulting in a 30% subjective intelligibility rating, were perceptually changed by presenting intact, undistorted versions of the sentences. upon a second presentation of the acoustically distorted versions of the sentences, their intelligibility increased markedly, up to 80%. these perceptual changes were reflected in the transient and sustained activation of auditory cortex and surrounding brain areas.

in the gradiometer analyses, local activity of the auditory cortex at  <dig> ms as indexed by the n1m response was sensitive to the acoustic structure of speech in that the distorted stimuli elicited stronger activation with an earlier peak latency than the undistorted stimuli. an increase of response amplitude and decrease of latency was observed also at around  <dig> ms, in the p2m response. the amplitude and latency effects of the p2m were substantially more pronounced in the right hemisphere than in the left. these findings indicate that transient activity of the auditory cortex is sensitive to the acoustic properties of sound during the early  processing stages of connected speech, and that the right hemisphere is more sensitive to acoustic variability than the left. the initial transient responses were followed by a sustained response, arising at around  <dig> ms, and appearing to consist of an early  and a late  phase. the early phase was more prominent in the left hemisphere and increased in amplitude when subjects attended to the stimuli. compared to the preceding transient activity, the sustained activation was less sensitive to acoustic distortion of speech.

in the mne analyses, the auditory cortex and surrounding areas exhibited divergent, bilateral activity patterns associated with acoustic feature processing and speech intelligibility. during the n1m time range, an increase in cortical activity due to stimulus degradation was observed in regions extending from the superior temporal gyrus  to the inferior parts of the postcentral gyrus . interestingly, a number of areas within the temporal cortex were sensitive to speech intelligibility during the p2m time range, with the intelligible stimuli - both distorted and undistorted - resulting in stronger activity than the unintelligible stimuli. this activation encompassed the auditory cortex, the inferior frontal gyri , the anterior part of the superior temporal gyrus , and the posterior part of the inferior temporal gyrus . during the early phase of the sf , the auditory cortex was more active in response to the distorted than the undistorted sentences, regardless of their intelligibility. in contrast, cortical activity in the posterior parts of the superior temporal gyrus  was stronger only during intelligible speech, regardless of whether the stimulus material was acoustically intact or distorted.

in the present experiment, the stimuli were distorted by using amplitude quantization, which has been shown to decrease substantially the intelligibility of isolated speech sounds . this was also the case in the current study, as the distorted sentences were initially very difficult to understand. however, after the subject was exposed to the undistorted versions of the sentences, the comprehensibility of the distorted sentences increased considerably. it is unlikely that the intelligibility effect seen in both behavioral and brain measures is an effect due solely to the repetition of the distorted stimuli given that the gap between repetition  was around  <dig> minutes. this time span makes it improbable that the subject could have been drawing on any echoic or short-term memory resources. instead, this increase in comprehension was most likely caused by top-down mechanisms utilizing the long-term memory representations which were instantly activated  during listening to the intact versions of the stimuli. similar changes in the perception of acoustically identical speech-like stimuli have been observed also using noise-vocoded sentences  <cit>  and sine-wave speech stimuli  <cit> . however, in these cases the perceptual changes were brought about through extended training sessions, whereas in the current context, these effects were immediate, and observable after already a single presentation of the undistorted versions of the stimuli. thus, depending on the experimental setup, it now appears to be possible to study brain mechanisms of perceptual learning occurring over a long time scale as well as rapid activation of linguistic memory representations.

the changes in the acoustic structure of the speech stimuli brought about by distortion were reflected in both the transient and sustained activation patterns of the auditory areas. in contrast, the temporal regions anterior and posterior to auditory cortex  were insensitive to degradation. the observed increase in the amplitude of the transient responses is in line with earlier results employing the same distortion method  <cit> . these studies have demonstrated that the amplitude increase of the n1m and p2m responses is related to an increase in harmonic frequencies in the signal spectrum brought about by quantization. according to this explanation, the additional harmonics activate a larger number of neurons involved in the pitch extraction process. in the current study the latency of the transient responses was also affected by the distortion, with earlier n1m and p2m latencies for the distorted sentences. this finding deviates from our earlier results using isolated speech sounds , for which the response latencies remained unchanged when the stimuli were distorted. one reason for these differences may lie in the experimental design: in previous studies by miettinen et al.  <cit> , short-duration isolated vowels were repeated at a fast rate whereas in the current case long-duration sentences with a complex, continually evolving spectral structure were presented with intervening long silent periods. similar latency results were recently reported by obleser and kotz  <cit> , who found that the n1m response peaks earlier and is larger in amplitude for distorted sentences than for their undistorted counterparts.

in the present experiment, the auditory cortex was highly responsive to distortion of speech, which is consistent with prior hemodynamic studies showing that the core auditory areas are sensitive to acoustic differences in speech stimuli  <cit> . the regions surrounding the auditory cortex, in turn, were sensitive to the intelligibility of speech, with stronger activation elicited by intelligible speech regardless of whether the stimulus material was distorted. these findings are congruent with the above fmri results, in particular with those by okada et al.  <cit> , who observed a bilateral sensitivity of both the anterior and posterior superior temporal regions to speech intelligibility. importantly, we observed that, already during the p2m time range, areas in the vicinity of the auditory cortex were sensitive to speech intelligibility as well . this intelligibility effect, observable presumably because of the temporal resolution of the meg, might reflect the influence of top-down feedback from higher-order cortical areas on the activity of auditory cortex. similar findings have also been reported by wild et al.  <cit>  and sohoglu et al.  <cit> , who demonstrated that prior expectations of speech content modulate the activity of auditory cortex during listening to distorted speech.

the novel experimental paradigm introduced here points to several interesting possibilities for future research. firstly, one should keep in mind that the current intelligibility effects in cortical activity were observed in the passive condition which always followed the active condition, and it therefore remains to be clarified whether there were carry-over effects from one to the other. this interesting issue, related to the decay time of recognition memory, clearly deserves further study. secondly, an important question for future investigation is how the number of sentences used in the experiment affects intelligibility and behavioral performance. assuming the memory system probed with the current paradigm has a capacity limitation, increasing the number of sentences should at some point lead to decreased performance. indeed, the intelligibility of the sentences in the current study might have been facilitated by the limited number of words and sentence stubs used to construct the stimulus material. thirdly, in studying the priming of memory representations of speech, a further step, requiring a larger set of sentences than in the current case, would be to average brain responses selectively based on the behavioral performance , and to study how this is reflected in the activation of brain areas. we expect that this approach would lead to even more pronounced intelligibility effects in cortical activity than those reported here.

CONCLUSIONS
the current study utilized an experimental setting which allowed for physically identical, distorted speech stimuli to be perceived as either unintelligible or intelligible due to a single intervening exposure to the undistorted versions of the stimuli. in the n1m time range , the auditory areas within the superior temporal sulcus seem to be sensitive to acoustic degradation of speech. thereafter, in the time range of p2m , auditory cortex as well as cortical regions anterior and posterior to this area appear to be responsive to the intelligibility of speech. following this transient brain activity, during the early sf , the region most sensitive to speech intelligibility was located in the posterior part of the superior temporal gyrus of each hemisphere. in terms of auditory scene analysis  <cit> , the current experimental setup could be seen as providing a new methodological approach to studies of auditory scene analysis. this phenomenon has traditionally been studied using sequencing of auditory stimuli alternating in, for example, frequency, intensity, or spatial location , whereas the experimental distorted-undistorted-distorted setup proposed here allows one to study how meaningful entities such as speech are segregated from a noisy signal by matching incoming acoustic signals to memory representations. our results indicate that in building a coherent whole from various auditory confluences, the acoustic attributes of incoming auditory signals are identified rapidly in the auditory cortex within the first  <dig> ms. the activity of the auditory cortex appears to be modulated through feedback connections, as indicated by the fact that already at  <dig> ms sensitivity to speech intelligibility emerges in this and surrounding areas. in making the unintelligible suddenly intelligible, these modulations can result in substantial changes in the way we perceive connected speech.

abbreviations
aef: auditory evoked field; anova: analysis of variance; ecd: equivalent current dipole; eeg: electroencephalography; fmri: functional magnetic resonance imaging; hpi: head-position indicator; meg: magnetoencephalography; mne: minimum norm estimate; roi: region of interest; sf: sustained field; sloreta: standardized low resolution brain electromagnetic tomography; snr: signal-to-noise ratio; usq: uniform scalar quantization.

competing interests
the authors declare that they have no financial or any other competing interests.

authors’ contributions
ht, pm and pa designed the experimental setup of the study and prepared the auditory stimuli. im carried out data acquisition, performed the statistical analyses and prepared the first draft of the manuscript. all authors participated in the writing process and have approved the final version of the manuscript.

