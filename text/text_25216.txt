BACKGROUND
alleles at two loci are said to be in linkage disequilibrium  when they are correlated or statistically dependent. the term refers to the idea that in a large homogeneous population subject to random mating, recombination between two loci will cause any initial association between them to vanish over time. in observed data, however, non-zero allelic associations are pervasive, particularly at short distances, but also at long distances and even between chromosomes. these associations arise in a complex interplay between processes such as mutation, selection, genetic drift and population admixture, and are broken down by recombination. the patterns of association are of interest, partly because they underpin the relation of genotype to phenotype at the population level, and partly because they reflect population history.

patterns of ld may be represented in different ways. a common method is to display pairwise measures of ld as triangular heatmaps  <cit> : in these displays, ld blocks  stand out clearly. early work in the hapmap project led researchers to hypothesize that the human genome consists of a series of disjoint blocks, within which there is high ld, low haplotype diversity and little recombination, and that are punctuated by short regions with high recombination   <cit> . subsequently various authors  <cit>  reported that genetic variation follows more complex patterns, for which richer models are required.

discrete graphical models  <cit>   provide a rich family of statistical models to describe the distribution of multivariate discrete data. they may be represented as undirected graphs in which the nodes represent variables  and absent edges represent conditional independence relations, in the sense that two variables that are not connected by an edge are conditionally independent given some other variables. to motivate this focus on conditional rather than marginal associations, consider three loci s <dig> …s <dig>  and suppose that initially s <dig> is polymorphic and s <dig> and s <dig> monomorphic, so that two haplotypes  and  are initially present. suppose further that a mutation subsequently occurs at s <dig> in the haplotype , and another at s <dig> in the haplotype , so that the population now contains the four haplotypes , ,  and . observe that in general s <dig> and s <dig> are marginally associated , but in the subpopulations corresponding to s2= <dig> and s2= <dig> they are unassociated: in other words, they are conditionally independent given s <dig>  more complex mutation histories give rise to more complex patterns of conditional independences that can be represented as graphical models  <cit> .

other authors have used graphical models for the joint distributions of allele frequencies. usually, in high-dimensional applications, attention is restricted to a tractable subclass, the decomposable graphical models  <cit> . in the first use of decomposable models in this context  <cit> , models were selected using a greedy algorithm based on significance tests. in  <cit>  methods and programs for selecting decomposable graphical models using monte carlo markov chain  sampling were described. these methods are computationally feasible for modest numbers of markers , but not for modern snp arrays with hundreds of thousands of snps per chromosome. to improve efficiency, the search space may be restricted to graphical models whose dependence graphs are interval graphs <cit> . these are graphs for which each vertex may be associated with an interval of the real line such that two vertices are connected by an edge if and only if their intervals overlap. in this context the ordering of snps along the real line is their physical ordering along the chromosome. mcmc sampling from this model class may be performed more efficiently  <cit> . this work was extended in  <cit>  to a more general subclass of decomposable models, namely those in which distant marker pairs  are conditionally independent given the intervening markers.

in an alternative approach  <cit>  latent mixtures of forests have been applied, in order to accommodate short-, medium- and long-range ld patterns. also directed graphs  have been applied, selecting edges and their directions using causal discovery algorithms  <cit> . there are close links between decomposable models and bayesian networks .

a rather different approach to modelling the joint distribution of allele frequencies  <cit>  is implemented in the software package beagle  <cit> , which is widely used to process data from snp arrays. the approach is based on a class of models arising in the machine learning literature called acyclic probabilistic finite automata   <cit> . these are related to time-variant variable length markov chains. for phase estimation and imputation beagle uses an iterative scheme analogous to the em algorithm, alternating between sampling from a haplotype-level model given the observed genotype data  and selecting a haplotype-level model given the samples . a similar computational scheme for decomposable graphical models has been described and implemented in the fitgmld program  <cit> .

characterization of genetic variation at the population level is of fundamental importance to understanding how phenotypes relate to genotypes. some specific uses to which joint models for allele frequencies have been put include 

 <dig>  insight into the population history of different genomic intervals. under simplifying assumptions, the ancestral history of a short genomic interval can be reconstructed from a decomposable graphical model for the snps in the interval  <cit> .

 <dig>  quality control of genome assembly, in that some motifs may suggest errors in snp positioning.

 <dig>  phase estimation and imputation as described above  <cit> .

 <dig>  derivation of more informative covariates involving multiple loci to plug into genomic prediction models  <cit> .

 <dig>  use in pedigree simulation to model ld in founders, for example in connection with gene drop simulation  <cit>  and assessment of snp streak statistics  <cit> .

in this paper decomposable graphical models are used to model fine-scale, local ld patterns. by local is meant that the proposed methods are designed to capture short-range associations between loci, but not long range ones. the same is true of other approaches  <cit>  mentioned above. here, an efficient, linear-time algorithm is developed to select a model using a penalized likelihood criterion, and it is shown how such a model may conveniently be displayed, allowing fine-scale ld structure to be visualized.

methods
graphs and graphical models
the following notation and terminology is mainly based on  <cit> . a graph is defined as a pair g=, where v is a set of vertices or nodes and e is a set of edges. each edge is associated with a pair of nodes, its endpoints. here only undirected graphs are considered, that is, with graphs undirected edges only. two vertices α and β are said to be adjacent, written α∼β, if there is an edge between them. the neighbours of a vertex is the set of nodes that are adjacent to it. a subset a⊆v is complete if all vertex pairs in a are connected by an edge. a clique is a maximal complete subset, that is to say, a complete subset that is not contained in a larger complete subset.

a path  between vertices α and β in an undirected graph is a set of vertices α=α <dig> α <dig> …,αn=β where αi−1∼αi for i= <dig> …,n. if a path α=α <dig> α <dig> …,αn=β has α=β then the path is said to be a cycle of length n. if a cycle α=α <dig> α <dig> …,αn=α has adjacent elements αi∼αj with j∉{i− <dig> i+1} then it is said to have a chord. if it has no chords it is said to be chordless. a graph with no chordless cycles of length ≥ <dig> is called triangulated or chordal.

a subset d⊂v in an undirected graph is said to separatea⊂v from b⊂v if every path between a vertex in a and a vertex in b contains a vertex from d. the graph g0= is said to be a subgraph of g= if v0⊆v and e0⊆e. for a⊆v, let ea denote the set of edges in e between vertices in a. then ga= is the subgraph induced bya.

the boundary bd of a vertex set a⊆v is the set of vertices adjacent to a vertex in a but not in a, that is ,.

 bd={v∈v:v∼wfor somew∈a}∖a. 

let g= be an undirected graph with cliques c <dig> …ck. consider a joint density f() of the variables in v. if this admits a factorization of the form

 f=∏i=1kgi 

for some functions g1()…gk() where gj() depends on x only through xcj then f() is said to factorize according to g. if all the densities in a model factorize according to g, then the model is said to be g-markov. when this is true g encodes the conditional independence structure of the model, through the following result : whenever sets a and b are separated by a set c in g, a and b are conditionally independent given c under the model. this is written as a⊥ ⊥b | c. a decomposable graphical model is one whose dependence graph is triangulated.

selecting graphical models for chromosome-wide ld
suppose that n observations of p snps from the same chromosome are sampled from some population. the variable set is written v=, and it is assumed that these are ordered by physical position on the chromosome. the variables may be either observed genotypes or inferred haplotypes, if these have been imputed: the former are trichotomous and the latter binary. here an algorithm to use these data to select a graphical model for the distribution of v is described. it is based on a penalized likelihood criterion

  ic=−2ℓ^g+αdim 

where ℓ^g is the maximized log-likelihood under g, dim is the number of free parameters under g, and α is a penalization constant. for the aic, α= <dig> and for the bic, α= log. the latter penalizes complex models more heavily and so selects simpler models. under suitable regularity conditions, the bic is consistent in the sense that for large n it will select the simplest model consistent with the data .

a technical but nonetheless important point is that calculation of the correct model dimension is not straightforward for high-dimensional models, since not all parameters may be estimable. a simple expression exists however for the difference in model dimension between two decomposable models that differ by a single edge . this is useful when the search space is restricted to decomposable models and the search algorithm only involves comparison of models differing by single edges .

a forward-backward approach to estimate the graphical model for v is used. the first  step is based on a greedy forward search algorithm. to take account of the physical ordering of the snps, the algorithm starts from a model g0= where e <dig> is the set of edges between physically adjacent snps: this model is called the skeleton. to seek the minimum bic  model, the algorithm repeatedly adds the edge associated with the greatest reduction in bic : only edges whose inclusion results in a decomposable model are considered. the process continues until no more edges improve the criterion. the search space in this step is the class of decomposable models that include the skeleton. note that this algorithm – as with almost all greedy algorithms — is not guaranteed to find the global optimum.

there are several advantages to initially constraining the search space to include the skeleton. an unconstrained search starting off from the null model  would not take the physical ordering into account. since the graphs considered are consistent with this ordering, they are conveniently displayed as ld maps, as illustrated below. because decomposable models contain no chordless cycles of length ≥ <dig>  two distal loci cannot be linked by an edge unless there are sufficiently many intermediate  edges to prevent the occurrence of such cycles. thus the algorithm proceeds by filling out around the skeleton by incrementally adding edges. the effect of this is that only local ld is modelled. since both association and  proximity are required, the edges included are more likely to be for real , so in this sense the model selection process is more reliable. in addition, restricting the search space in this way improves computational efficiency. the linear-time algorithm described in the following section also builds on the physical vertex ordering.

in the second  step the graph found in the first step is pruned, again using a greedy algorithm that seeks to minimize the bic  criterion by removing edges, without requiring that the skeletal edges are retained. keeping these in the model would be appropriate if adjacent snps are in high ld, but this is not always the case. for example, there may be recombination hotspots, or genome assembly errors may have led to errors in positioning the snps. the graphs may be used to examine whether this has occurred.

to display the resulting graph, a suitable graph layout is needed. after some experimentation, one such was obtained by modifying the horizontal dot layout in rgraphviz  <cit> , by exponentially smoothing the y-coordinates and replacing the x-coordinates with the snp-number. in this way the y-coordinates are chosen so as to clarify the graphical structure: a consequence of this is that nodes with large absolute y-coordinates tend to signal genomic regions of high structural complexity.

for some purposes it is helpful to use more objective measures of complexity, and two such measures are used here. consider a chromosome with p snps. for each i= <dig> …p− <dig>  the heighthi of the interval between snps i and i+ <dig> is defined to be the number of edges of the form , where j≤i and i+1≤k, and the widthwi is defined to be the maximum value of |k−j| of such edges. note that by the global markov property, when hi= <dig> or equivalently wi= <dig>  the snps v≤i={vj:j≤i} are independent of v>i={vj:j>i}. similarly, when hi= <dig> or equivalently wi= <dig>  v<i⊥⊥v>i|vi.

as a measure of haplotype diversity, the entropy <cit>  is used here. suppose that in some genomic interval there are k distinct haplotypes with relative frequencies f <dig> …fk. then the entropy is defined as h=−∑i=1…kfilogfi. it is large when there are many distinct haplotypes that are equally frequent.

a useful way to display marker data over short genomic intervals is the sample tree <cit> . this summarizes a set of discrete longitudinal data of the form x=,…xq) for v=1…n. a rooted tree is constructed in which the non-root nodes represent partial outcomes of the form  for k≤q present in the sample data. edges may be coloured with the marker value and labelled with the corresponding sample counts, or drawn with width proportional to the sample counts.

a fast selection algorithm
for high-dimensional models, the first step of the algorithm described above can be computationally demanding. to address this a much faster algorithm for the same task is now described. this involves combining a series of overlapping marginal models of lower dimension. call the block length  l and the overlap k. thus the first block is v1={v <dig> …vl}, the second is v2={vl−k+1…v2l−k} and so on. in the applications described here l= <dig> and k= <dig> are used.

suppose that the true model for v= is g, and that g^ is the estimate of g obtained by performing the first step of the algorithm described in the previous section. the goal is to construct an approximation g~ to g^ by combining models g^i= obtained by applying that same algorithm to blocks vi for i= <dig> ….

a way to construct a model g~ <dig> for v1∪v <dig> by combining g^ <dig> and g^ <dig> is now described. let m∗=max{w:∃∈Ê1withv≤l−k}. then g~ <dig> is defined as g~12=, where Ê10={∈Ê1:w≤m∗} and Ê20={∈Ê2:w>m∗}.

the rationale for this is that marginal models may include spurious associations on the boundaries. for example, let g <dig> and g <dig> be the subgraphs of g induced by v <dig> and v <dig>  respectively. then the marginal distribution of v <dig> will not in general be g2-markov, but it will be g2∗-markov for a graph g2∗ derived from g <dig> by completing the boundary of each connected component of gv∖v <dig> in g <cit> . so g^ <dig> will tend to contain edges not in g <dig>  to estimate the boundary of v1∖v <dig> in g, g^ <dig> is used: the boundary is estimated to be contained in the set l−k+ <dig> …,m∗. hence by only including edges ∈Ê <dig> with w>m∗ edges in this boundary are omitted. similarly, the boundary of v2∖v <dig> in g <dig> may contain spurious associations, so Ê <dig> may contain unnecessary edges. to avoid this the overlap length k is chosen to be sufficiently large so that m∗< min{v:∃ ∈e <dig> with w≥l+1}. if the maximum width was known in advance this inequality could be ensured by setting k to be twice the maximum width, but so large a value appears to be unnecessary in practice.

the algorithm proceeds in the obvious fashion combining g~ <dig> with g^ <dig> and so on. since the construction may result in chordless cycles it is necessary to triangulate the resulting graph g~. this can be done using the maximum cardinality search algorithm  <cit>  which can be implemented in linear time.

assuming that the time for deriving the estimates g^i for fixed l is bounded, the algorithm described here is by construction linear in p. but the proportionality constant can be expected to be depend on various factors, such as the density of the minimum aic/bic model.

implementation
the methods are implemented in a set of r functions which are provided as additional files  <dig> and  <dig> to the paper. the selection algorithms build on the forward search algorithm implemented in the stepw function in the graphd package  <cit> . currently this function requires that there are no missing data. the backward search algorithm was developed by the author in the course of preparing this paper, and has subsequently been implemented in the graphd package. the functions to perform the selection algorithms, and others to work with and display the graphs, build on the following packages: igraph  <cit> , graph  <cit> , rgraphviz  <cit>  and graphd. the triangular heatmaps were produced using the ldheatmap package  <cit> . simulation from a fitted decomposable model was carried out using the grain package  <cit> . the package rjpsgcs provides an interface between r and the fitgmld program which was used to perform the algorithm of  <cit> .

RESULTS
to illustrate the methods described above, they were applied to snp data obtained from three commercial pig breeds. in all  <dig>   <dig> and  <dig> pigs of the duroc, landrace and yorkshire breeds were genotyped using the illumina porcine snp <dig> beadchip  <cit> . after preprocessing on the basis of call rate, minimal allele frequency and other quality criteria, missing values were imputed using beagle  <cit> . using the methods described above, decomposable graphical models were selected for each chromosome and breed, using data at the genotype level. the bic penalizing constant α= log was used throughout.

figure  <dig> compares the running times of the two algorithms, and confirms that the fast algorithm is approximately linear in the number of snps. table  <dig> gives more detailed information. it is seen that the estimate g~ is indeed a good approximation to g^, and that the algorithm is considerably faster than the standard greedy algorithm.

the table shows the performance of the algorithms applied to the pig data. n and p denote the numbers of observations in the data and number of snps available after filtering. the edge sets found in steps  <dig> and  <dig> of the standard algorithm are denoted es and esp, respectively: for the fast algorithm they are denoted ef and efp. the numbers of undershoots and overshoots, i.e. |es∖ef| and |ef∖es| for step  <dig>  and |esp∖efp| and |efp∖esp| for step  <dig>  are also shown. the corresponding running times in seconds are denoted ts, tf, tsp and tfp, respectively. the backward step is much faster than the forward step in both cases . overall, the fast algorithm is  <dig>  times faster than the standard algorithm, and its inaccuracy, assessed as ∑/∑, is  <dig> . the last column shows the maximum width of g=. the computations were run under redhat fedora  <dig> linux on a intel i <dig> four-core  <dig> ghz machine with  <dig> gb ram.

figure  <dig> shows the ld graph for one chromosome : the same graph is obtained with both algorithms. it is a long thin graph with  <dig> nodes and  <dig> edges. it has  <dig> connected components, of which  <dig> are isolated vertices — suggesting snp positioning errors — and four are long intervals with  <dig>   <dig>   <dig> and  <dig> snps. curiously, the subgraph induced by the last  <dig> snps contains  <dig> connected components, which suggests positioning errors in this region. the most striking feature of the graph is that for about 80% of the chromosome, a simple serial or near-serial association structure is found, but for the remaining 20% more complex patterns of ld are observed. similar results are found for all chromosomes and breeds.

genomic intervals with simple association structure tend to be associated with low haplotype diversity, and intervals with complex structure with high diversity . to further compare the low and high complexity regions, two representative intervals of the same length were selected: snps numbered 1800- <dig> , and snps numbered 2470- <dig> . these have sample entropies of  <dig>  and  <dig> , respectively. their subgraphs are shown in figure  <dig>  and their sample trees in figure  <dig>  these show the low diversity of the low complexity interval, and the relatively high diversity of the high complexity interval. corresponding triangular heatmaps are shown in figures  <dig> and  <dig>  the low complexity interval has high ld whereas the high complexity interval shows a more mosaic structure.

thus low complexity regions tend to consist of series of haplotype blocks with high ld and low haplotype complexity, and may be dominated by a few common haplotypes. in contrast, regions of high complexity tend to have high haplotype diversity and little or no haplotype block structure. it must be stressed that the snps in such regions are not generally in linkage equilibrium: on the contrary, complex patterns of association, not marginal independences, are observed.

figure  <dig> and table  <dig> show the results of applying the algorithms to unphased, genotype data, but as mentioned above they may also be applied to phased haplotype-level data. this implicitly regards the inferred haplotypes as a random sample of size 2n from an underlying population of haplotypes. analyses based on inferred haplotypes may be subject to a loss of efficiency  <cit> . for the current data phase imputation was carried out using the beagle software  <cit> : applying the fast algorithm to the phased data for duroc chromosome  <dig> resulted in the graph shown in figure  <dig>  this is slightly denser than the graph in figure  <dig>  with  <dig> more edges. this may be ascribed to the reduced model dimension due to the use of binary rather than trichotomous variables, which leads to a weaker penalization of complex models in . haplotype- and genotype-level graphical models are related but in general distinct: it has been shown that they are identical only when the haplotype-level graph is acyclic  <cit> .

to assess the accuracy of the algorithm, twenty simulated data sets each with the same number of observations as the data sets analyzed above  were constructed. the first ten were generated by taking n random samples from the skeleton, that is, the graphical model with edges between physically adjacent snps only, fitted to the duroc chromosome  <dig> genotype data. the second ten were generated by taking n random samples from the model shown in figure  <dig>  fitted to the same data. the results of applying the fast algorithm to each data set are summarized in table  <dig>  for the skeleton, the edgewise false negative rate was  <dig>  and the edgewise false inclusion rate was  <dig> . for the model in figure  <dig> the corresponding rates were  <dig>  and  <dig> . to visually represent the latter results, figure  <dig> shows a graph g=, in which the edge colours represent the frequency that the edge was found in the  <dig> models. the graph suggests that model uncertainty is primarily restricted to the genomic intervals with complex dependence structure.

the table summarizes the inaccuracy of the fast selection algorithm applied to data sets simulated under two models for duroc chromosome 1: the skeleton, and the more complex ld graph shown in figure  <dig>  the columns labelled undershoot and overshoot show |e∖ej| and |ej∖e|, where e and ej are the edge sets of the true model and the model found by the algorithm, respectively. for the data sets simulated under the skeleton, the edgewise false negative rate 110|e|∑j|e∖ej| is  <dig>  and the false positive rate 110|e|∑j|ej∖e| is  <dig> . under the more complex ld graph, the corresponding rates are  <dig>  and  <dig> .

finally, for comparison purposes the algorithm of  <cit>  was applied to the duroc chromosome  <dig> genotype data. this algorithm automatically imputes phase and missing marker data, cycling between imputation and model selection as sketched in the background section. the selected graph is shown in figure  <dig>  like the graphs shown in figures  <dig> and  <dig>  it is long and stringy, but rather more dense. this greater density may be explained by the use in  <cit>  of a metropolis acceptance rule that is based on the penalized likelihood , but with a smaller penalizing constant α= log. a detailed comparison of the two algorithms would be valuable but is not attempted here.

discussion
this paper has introduced an efficient, linear-time forward-backward algorithm to estimate chromosome-wide probabilistic graphical models of fine-scale linkage disequilibrium, and has described a convenient way to display these models. in illustration, the methods have been applied to data obtained from three commercial breeds of pigs using the illumina porcine snp <dig> beadchip.

the forward part of the algorithm proceeds by combining a series of overlapping marginal decomposable models of dimension l and overlap length k. this implicitly assumes that the maximum width of the true graph is at most k. the resulting model is then triangulated and backward selection performed. the search space closely resembles that of  <cit> , which samples from decomposable models with a given maximum width, using sliding windows. the difference in approaches lies primarily in the search method: here greedy search to optimize a penalized likelihood criterion is used, whereas in  <cit>  mcmc sampling methods are applied.

the r function used for greedy forward search currently requires that the input data contain no missing values, so it was necessary to impute missing values prior to performing the algorithm, and the beagle software  <cit>  was used for this. this raises the possibility of circularity, or more precisely, that the model selection is influenced by constraints or assumptions implicit in the models used by beagle. but given beagle’s high imputation accuracy with such data it seems unlikely that this plays an important role here.

the algorithm was found to have high accuracy when applied to simulated data sets of the dimensions considered here , with edgewise false positive and negative rates of around 3%. that it is good at reconstructing the model generating the data is reassuring. needless to say, the algorithm does not necessarily identify the “true” model, which may not be in the search space. as noted previously, the approach captures short range associations, but not long range ones. moreover, higher edgewise false negative rates may occur when observed data are used if there are infinitesimal departures from a model that are not detectable at the given sample size, as these are filtered out when data are simulated under a selected model.

a comparison of triangular heatmaps with ld graphs is instructive. the former are compact graphical representations of all pairwise marginal associations for a set of snps. they are particularly well-suited to identify ld blocks, which stand out as highlighted triangles. ld graphs supplement heatmaps by showing patterns of association but not their strength. since they are parametric models they can be put to a number of quantitative uses as described in the background section. at one level, heatmaps and ld graphs convey similar information, since intervals with simple dependence structures tend to appear as series of ld blocks in the heatmap, whereas those with complex structures tend to occur in the inter-block regions and have a more mosaic appearance. but the graphs provide a more incisive characterisation of genetic variation, building on the concept of conditional rather than marginal dependence. in this regard, it may be helpful to regard conditional independence statements as expressing the notion of irrelevance, in the sense that a⊥ ⊥b | c implies that if we know c, information about a is irrelevant for knowledge of b. thus the graphs say something about connections between specific snps, for example about which snps are required to predict a specific snp.

the graphs may also be useful in fine mapping. genome-wide association studies seek to find the genetic basis of complex traits, typically using single locus methods - that is, by identifying snps with strong marginal association with the complex trait. due to ld, many snps in a genomic region may exhibit strong association with the trait, making it hard to identify the causal loci. a way to address this is to assess the effect of a putative causal snp on the complex trait in a linear model that also includes terms for the two flanking snps, in order to adjust for confounding with nearby effects due to ld. this implicitly assumes a simple serial dependence structure between the snps, and when the dependence structure is more complex such adjustment might be insufficient, leading to false positives. this may be prevented by including terms for the neighbours of the putative causal snp, not just for the flanking snps. any snp is separated from the remaining snps by its neighbours, so by the global markov property it is independent of the remaining snps given its neighbours. a similar method has been proposed based on bayesian networks  <cit> .

like graphical models, the apfa models that underlie beagle may be represented as graphs that encode a set of conditional independence relations, but in a very different way. in the present context, for example, nodes in apfa graphs represent haplotype clusters rather than snps. where the model classes intersect, apfa graphs are much more complex than the corresponding dependence graphs, and so less amenable to visualization.

a striking feature of the ld graphs for the pig data was that for roughly 80% of the genome, simple serial or near-serial ld patterns were found, but for the remaining 20%, more complex patterns were observed. the regions with the simple serial structure tend to have low haplotype diversity, which is to be expected in livestock breeds with small effective population sizes  <cit> . perhaps more unexpected is that roughly 20% of the porcine genome exhibits complex ld patterns, forming islands of relatively high genetic diversity. this information may be useful in an animal breeding context, to identify regions with high genetic variation. it will also be interesting to compare graphs obtained using different snp densities in a given breed or species to examine whether and how their topologies change with varying marker densities.

CONCLUSIONS
the proposed algorithm is efficient and makes it feasible to estimate and visualize chromosome-wide ld models on a routine basis.

competing interests
the author declares he has no competing interests.

supplementary material
additional file 1
ldgraph. the r functions implementing the methods of the paper.

click here for file

 additional file 2
examplescript. r code illustrating how the r functions may be used.

click here for file

 acknowledgements
the data were kindly provided by the danish pig research centre and in particular tage ostersen. the work was performed in a project funded through the green development and demonstration programme  by the danish ministry of food, agriculture and fisheries, the pig research centre and aarhus university. ole f. christensen, peter sørensen, luc janss and lei wang are acknowledged for helpful discussions.
