BACKGROUND
as biology has transformed from a descriptive to a quantitative field, there has been growing interesting in creating mathematical models which describe the dynamic evolution of biological processes. thus rather than taking measurements pre vs. post perturbations, there has been growing interest in modeling the dynamic progression of biological responses  <cit> . however, given the exigencies of biological experiments, there are often limitations in the signal to noise ratio of a given measurement leading to large variations between replicate measurements as well as relatively few replicates despite the lack of precision in these measurements. because of the desire to mathematically model the dynamics of the system, it is not appropriate to only determine whether the signal shows a statistically significant change over the given time course, but also that the dynamic variations amongst all the different time points are important.

an example of a dynamic biological signal which is of interest researchers are the changes in mrna gene expression level over time in response to external perturbations such as gene silencing, induction of disease states, or the administration of a drug/toxin <cit> . the study of this specific signal has evolved from determining which systems show statistically significant changes, to modeling the progression of this change to obtain intuitions about the underlying mechanisms. an example of this would be the use of temporal gene expression profiles to probe the underlying mechanism which governs the pk/pd response of an organism to a drug, or the dynamic response of an organism in response to a severe injury <cit> .

the standard procedure for obtaining the necessary information consists of taking a set of gene expression measurements at predetermined time points and reconstructing the dynamic signal. due to the low signal to noise ratio associated with these experiments, replicates are taken in order to compensate for the lack of fidelity in the signals. however, because of issues such as cost in terms of time and money, oftentimes very few replicates are obtained at each time point. therefore, while it may be relatively simple to determine whether the system changes in a measurable fashion during the time horizon of the experiment via statistical tests such as anova <cit> , t-test <cit> , edge <cit>  or sam <cit> , determining whether the dynamic of the response of the system has been accurately captured is a problem which has been less well addressed. we propose the creation of an algorithm formulated specifically to address this issue. the significance of this difference can be illustrated in figure  <dig>  figure corresponds to the gene expression profile of prp <dig> pre-mrna processing factor  <dig> in rat which is present in the gene expression omnibus dataset under accession number gds <dig> <cit> . this gene contains four replicates per time point. in the expression profile for this specific gene, it is possible to determine via an edge whether the gene has been differentially expressed over the experimental time course. however, depending upon which subset of  <dig> replicates is selected, the dynamic response of the gene expression profile appears to change significantly. therefore, the question which we need to answer is whether sufficient replicates have been measured such that the dynamics which will later be fitted via various mathematical models represents the underlying response.

therefore, the question is not how to select for biologically relevant signals, but rather how to select for the signals whose inherent expression dynamics given the large biological variance and the limited number of replicates is accurately captured by the ensemble average. while there exist several methods for assessing the quality of a signal given a limited number of replicates, such as calculating the signal-to-noise ratio  <cit>  or utilizing the co-efficient of variance  <cit> , these methods have significant drawbacks. the two primary drawbacks with these methods is that they are insensitive to the number of replicates used within a given experiment, and secondly while one can easily set a cut-off for a given score, it is difficult to interpret the meaning of this cutoff statistically i.e. associating the cut-off with a p-value.

as a generalized method for assessing the quality of a single temporal signal given a specific number of replicates, we need to satisfy two intuitions:

 <dig>  the accuracy of the ensemble average increases as the number of replicates per time point increases

 <dig>  the accuracy of the ensemble average increases as the coefficient of variance decreases at each time point

RESULTS
for all of the datasets, the p-value cutoff was selected at p < . <dig> for both the edge as well as the loocv quality assessment. while it is arguable as to whether such a threshold is appropriate given the number of genes present within the dataset <cit> , what we seek to show is that for a given threshold that filtering genes based upon the accuracy as well as differential expression exhibits a stronger link between co-expression and co-regulation than merely selecting the genes based upon their differential expression via algorithms such as edge. for all of the datasets, the selection of genes based upon the quality of their dynamic expression profiles showed this consistent trend as exemplified by the greater percentage of statistically enriched ontologies figure  <dig>  this matches well with our original hypothesis that if we were to cluster genes whose measured dynamics were more accurately captured, then the association between co-functionality and co-expression becomes stronger. thus through the loocv pre-filtering step, we see a decrease in the number of genes which have been included, but which do not truly correlate with the genes in a given cluster.

looking at the results for the three datasets in table  <dig>  we can see that the increase in the percentage of significantly enriched ontologies is due primarily to the fact that the number of non-significantly enriched ontologies is decreasing, while the number of significantly enriched ontologies remains relatively constant. therefore, the function of the loocv algorithm seems to be the removal of genes which do not appear to show significant co-functionality with the other genes they are grouped with. looking at the intersection of

in all of the cases, we see that after running the loocv filter as well as the selection via edge, the total number of ontologies that have been selected is lower, whereas the number of statistically significantly enriched ontologies remains relatively constant. thus, the improvement in the total number of ontologies appears to be due primarily to the removal of genes that do not show significant co-functionality.

for the gds <dig> chronic corticosteroid dataset, we see the smallest amount of improvement between filtering the dataset utilizing an edge vs. utilizing edge along with the proposed loocv filtering algorithm. the pre-selection step via loocv yielded  <dig> genes, of which edge determined that  <dig> of them were differentially expressed. this is in contrast to running edge independently in which  <dig> genes were selected as being differentially expressed. in this instance, it would appear that filtering via loocv identifies a subset of genes in which over 75% of the genes show significant differential expression. in terms of the gene ontology enrichment, it is evident in figure  <dig>  that there is a consistent improvement in the percentage of significantly enriched ontologies when edge is used in conjunction with loocv.

the burn dataset , showed the greatest improvement when the loocv algorithm was used as pre-filtering step. as a result of the selection via edge  <dig> genes were selected as being differentially expressed. the result of running the loocv algorithm by itself yielded  <dig> genes, out of which  <dig> were selected to be differentially expressed under edge. pre-filtering this dataset for quality before conducting selection for differential expression showed the greatest level of improvement due to the fact that it contained the fewest number of replicates as the fact that it was run on an older array figure  <dig> 

for the acute corticosteroid dataset , after the initial filtering via loocv, there existed  <dig> genes, of which  <dig> were shown to be differentially expressed via edge. when conducting the selection via edge itself,  <dig> genes were denoted as being differentially expressed in figure  <dig>  we can again see a notable and significant improvement in the percentage of significantly enriched ontologies present within the data. the gds <dig> data shows an intermediate level of improvement with respect to the other datasets. this dataset was expected to show a lesser degree of improvement than the burn dataset  due to the fact that it was run with more replicates per time point, and was expected to show a greater degree of improvement than dataset corresponding to the chronic infusion of corticosteroids  because it is an older array , and because it has fewer replicates per time point than the gds <dig> dataset.

in general, for all of the datasets, the majority of genes which were selected as to having their dynamics being reliably measured also showed significant activation though the reverse is not true. this is not surprising because the loocv quality assessment requires that the presence of a change or lack of change to be consistent for all data points, whereas techniques such as the edge only attempt to detect a significant change over the time course of the experiment. however, with a sufficient number of replicates or an increase in the signal to noise ratio, both sets should be essentially the same as seen in the dataset associated with a chronic administration of corticosteroids. however, in the cases where the number of replicates is quite small or the system has a low inherent signal to noise ratio, the differences can be significant.

given the fact that the number of significantly enriched ontologies appears to be reasonably constant whether we use the pre-filtering step or not, one may assume that the intersection of the significantly enriched ontologies between the two cases is quite high. however, from our results, this did not appear to be the case. running the pre-filtering step along with edge would yield 55â€“60% commonality in terms of the significantly enriched ontologies identified for the two corticosteroid related datasets gds <dig> and gds <dig>  in the case of the burn dataset gds <dig>  it was found that the commonality between the two sets changed from 60% when  <dig> clusters were used to less than 10% when  <dig> clusters were used. furthermore, not all of the ontologies found after running loocv and edge were found to be enriched when edge was run by itself. given the large percentage of genes which passed the loocv pre-filtering step which also showed differential expression, this result appears to suggest that the enrichment of individual ontologies can be quite sensitive to the incorporation or removal of a few genes.

discussion
the primary issue which this algorithm was developed to address was the fact that just because a gene shows significant changes in its temporal expression does not mean that such a gene expression profile has been measured in such a way which is amenable to mathematical modeling. given the difficulty in quantifying the accuracy of a given mathematical model, clustering was used as a surrogate. in the dataset which was obtained with a newer affymetrix array and a higher number of replicates, we generally found that genes which showed significant activation were also very likely to have been accurately measured. however, for some of the older arrays, we have found that this was not to be the case. in one case, we found that many of the genes which had been reported as having statistically significant changes in activity did not have profiles which were amenable to modeling.

aside from the simple explanation that such variability between the replicates is due biological variability, we hypothesize that other factors may play a role and by identifying these factors, we can minimize the variations between samples. such factors may include issues with the microarray itself as evidenced by the minimal difference between the proposed loocv quality assessment metric and the edge when utilizing the newer rae230a array vs. the older rg-u34a arrays. other factors which may play a role is the imperfect synchronization between replicates, especially for genes with quick early responses <cit> . thus, while researchers have attempted to balance the number of time samples taken and the presence of early, rapidly varying signals, additional care may be needed to carefully synchronize the systems to minimize the effect of incorrectly synchronized replicates adding significant variability into each replicate.

due to uneven temporal sampling, one significant issue has arisen, specifically how to deal with the samples which encompass a shorter time duration vs. samples that represent the response over a longer duration of time. for instance in the case of the gds <dig> dataset, the sampling rate ranges from  <dig> minutes to  <dig> hours. thus while, the majority of the signal in terms of duration of time may have been well captured, the overall correlation coefficient may be low given the high variability in the early time points figure  <dig>  this is a problem which not only affects the proposed algorithm, but also many other algorithms designed to processes high throughput gene expression data.

the reason for this problem is the fact that the algorithms essentially treat the data as a vector of values without time dependence. essentially the data points themselves are all given equal weight whether they take place during a short period of time, or whether the data point encompasses a greater period of time. thus the correlation coefficient or clustering analysis may not also agree with one's judgment utilizing visual inspection of the data. however, while the results of the algorithm may not agree with one's intuition when visually assessing the data, the fact that researchers have selected such an uneven sampling strategy means that the dynamics early may play just as important role as the later dynamics despite their transient effect. therefore, while there exists algorithms that will normalize the data based upon the time duration via techniques such as interpolation or curve fitting <cit> , they may miss or minimize the fact that earlier time points may in fact be more important biologically. despite the ambiguities in how the uneven sampling should be treated, our framework opens up a possible extension which can address this issue. rather than taking the ensemble average of a subset of replicates, we can interpolate/fit curves from the data with even sampling points, and then calculate the correlation coefficients from these curves and utilize the correlation coefficient in the same manner as presented.

while the intent of our algorithm was to identify a set of gene expression profiles whose temporal profiles are amenable to modeling, we assessed the effect of these high quality expression profiles through an analysis of clustering results. in our evaluation of clustering quality, we have established the fact that our algorithm identifies a condensed set of genes which show a strong co-functionality/co-expression relationship, and rejects many genes which do not show co-functionality with the dominant biological processes due to incorrect cluster assignments due to ambiguities in the underlying signal. however in many cases, this specificity comes at the expense of generality, with some of the results exhibiting fewer total enriched gene ontologies. thus, if one wanted to use this reduced set of genes as a representative population, one important question is whether this smaller set of reduced ontologies exhibit a more focused set of biological processes or whether there is significant amount of information which has been rejected.

to make this assessment, we evaluate the set of genes that have passed edge, but rejected by the loocv algorithm, and the set of genes that pass both filters. when this evaluation is performed upon our three datasets, we see an interesting result. in the set of differentially expressed genes which do not pass the loocv filter, we see that the majority of enriched ontologies are the same as the overall set of differentially expressed genes . in the cases that additional ontologies are found in this set, very few of the additional biological processes have been previously associated with injury, inflammation, the immune response, or metabolism which are hallmarks of burn injury or corticosteroid administration. this is in contrast to the set of differentially expressed genes which do pass the loocv filter. in this case, many additional biological processes were found to be over-represented. furthermore, these additional processes have been linked to our experimental perturbations table  <dig>  because of the high number of additional processes which appear to be enriched under the loocv case, as well as their relation to the experimental conditions, we hypothesize that this smaller set is in fact more targeted.

it is evident that in all cases the subset that passed both loocv and edge introduced more additional ontologies. furthermore, one of the interesting observations is that through the rejection of a population of genes, we are better able to see evidence of biological processes that are associated with inflammation, the immune response, metabolism, and injury  which are hallmarks of our experiments associated with the anti-inflammatory effects of corticosteroids or the response to a significant burn injury.

from this result, it appears that the set of genes rejected by the loocv filter are qualitatively similar to the original set of differentially expressed genes. this is in contrast to the set of differentially expressed genes which pass the loocv filter, which show significant differences in the identified ontologies. by looking at the set of ontologies which have identified after loocv filtering, but not present under selection via edge, it appears that loocv filtering is able to identify ontologies which predominately relate to the biological processes associated with our experimental perturbations. however, because it is difficult to assess whether "unrelated" ontologies reflect an artifact of the selection/clustering/enrichment process, or part of important, but previously uncharacterized processes, one strategy may be to utilize a union of both the results obtained from edge filtering and set of enriched ontologies after the additional loocv filtering. similar to the fact that loocv was designed as an addendum to gene selection algorithms to identify high quality temporal profiles for modeling, the additional ontologies that have been identified serve as an addendum to the original processes identified via the original gene selection process. because we have shown that these additional processes are relevant, we see this as adding information to what has been previously identified. thus, the loocv filtering step should not supplant results from edge or any other selection algorithm, but can be used to complement the original results.

CONCLUSIONS
one of the primary motivations for creating a new way of performing gene selection is that given inherent biological variability as well as deficiencies in measurement precision, the temporal evolution of a given piece of data may not be an accurate reflection as to the underlying dynamics. therefore, if one were to mathematically model a given dynamic response, one must be certain that the data is sufficiently precise. given the difficulties in evaluating the change in utility between modeling accurately vs. inaccurately measure data, the effect upon ontology enrichment was used instead, and we found that in all cases, there was an improvement in the overall quality of the clustering results, though with better data acquisition platforms and experimental design this improvement was minimized.

though most of the analysis has been performed upon microarray data, this data was selected primarily for the ease by which it would be possible to evaluate the improvement, this technique can be expanded to other data types, and evaluate whether sufficient data has been obtained to for modeling purposes. thus, this technique can be expanded for use in techniques such as elisa over multiple time points or metabolite measurements over multiple time points, and not just microarray data, thus allowing the researcher to determine that a sufficient number of replicates has been obtained, or that more replicates are needed.

