BACKGROUND
since the introduction of microarray technology in the 1990s, a large number of data sets have been produced in the field of transcriptome profiling and made publicly accessible through specialised repositories like the gene expression omnibus  at nih <cit>  or arrayexpress at ebi  <cit> . based upon the massive analysis of these types of data, a number of different approaches have been taken to develop integrated knowledge about the coexpression of genes  <cit> , to search for regulatory elements in upstream regions of genes  <cit> , to define transcriptional modules  <cit> , to understand relationships between the interactome and transcriptome  <cit> , and other applications.

the calculation of correlation coefficients between pairs of genes takes place at the very beginning of most studies involving the massive analysis of transcriptome microarray data sets, in particular those aimed at constructing transcription networks. the ultimate quality of these networks can therefore be influenced by numerous factors that are capable of affecting the quality of the correlation coefficients. among these factors is the inter-laboratory reproducibility, which is of great importance because massive analysis requires collecting results originating in multiple laboratories.

apparent discrepancies between the results of several independent transcriptomic studies in the same system, e.g. several types of mouse stem cells  <cit>  or human embryonic stem cells  <cit> , have highlighted the need for more research and for deeper analysis of the factors that control the accuracy and inter-laboratory reproducibility of microarray technology. in particular, large comparative studies addressing various technical points, involving several laboratories and comparing different platforms, have been conducted  <cit> . the interpretation of these results is difficult because of the large number of confounding effects that can bias any particular study . regarding the calculation of correlation coefficients, however, it has been possible to order these confounding effects according to their relative importance, revealing that the lab-effect is the most dominant. indeed, significant differences can be observed between the results obtained when the same platform is used in different laboratories to study the same biological systems. when conducted in a realistic way, i.e. leaving each laboratory free to select the protocols used, all studies converge towards the conclusion that commercial oligonucleotide microarrays deliver results that are far more reproducible than those generated by cdna microarrays. for example, in one study, a systematic evaluation of the relevance and biases of genetic networks extracted from different expression datasets showed that the biological relevance of networks constructed from affymetrix data was remarkably higher than the relevance of gene networks inferred from cdna data  <cit> . since these two types of platform are comparable in terms of sensitivity, specificity, and accuracy when they are carefully handled in the same laboratory  <cit> , we conclude that the standardization of the entire process, which is a characteristic of commercial oligonucleotide platforms, plays the main role in the reproducibility of the results by minimizing the lab effect. in contrast, cdna platforms are strongly penalized by the absence of recommended protocols for the preparation of targets and the possibility of using different hybridization units, scanners, and software  <cit> . for these reasons, we have decided to focus our attention on data generated by commercial oligonucleotide platforms. even more precisely, the affymetrix platform appears to be optimal for analysis because concurrent platforms are poorly represented in the repositories.

correlation values between genes may be obtained in two ways. specifically, genes can be considered to be positively or negatively correlated either because their absolute expression levels follow a similar or inverse course across several biological conditions , or because relative changes in their expression levels vary in the same or opposite direction in a series of comparisons between two biological conditions . because individual signals obtained with double-channel platforms, such as spotted oligonucleotide or cdna microarrays, cannot be used reliably, one must use variation measures  to calculate the correlations between genes. in contrast, with single-channel techniques such as in-situ oligonucleotide chips, no internal controls are necessary and the normalized signals can therefore be used to calculate gene expression correlations across several conditions. it follows then that the vast majority of massive analyses of transcriptomic results obtained to date with single-channel technique have only used coexpression.

one conclusion of analyses of inter-platform and inter-laboratory studies is that variation data are far more reproducible than expression level results . using covariation instead of coexpression should therefore improve the quality of the correlation coefficients calculated from single channel results, which account for about 70% of the publicly available data  <cit> .

while clustering techniques are often used to assemble substantial subsets of genes  <cit> , when correlations are to be calculated on all possible pairs of genes most studies rely on the use of classical correlation measures such as pearson's product-moment coefficient or spearman's rank correlation coefficient. in contrast, other techniques like the mutual information score  <cit> , normalized differences  <cit> , or cosine correlation distances  <cit>  are rarely used. to our knowledge, the suitability of the widespread use of these correlation measures for massive microarray analysis has not been questioned. the most obvious criticism we could make in this regard is that this type of statistical tool is too simplistic in view of the complexity of the relationships that exist between the transcript levels of any two genes observed across numerous biological conditions. further, as the number of conditions under consideration rises, the probability increases that the two genes will have a positive correlation in one subset of conditions and a negative correlation in another subset. depending on the relative importance of the two subsets, the correlation coefficient, calculated using either covariation or coexpression, will give a positive, negative, or non-significant value because all the conditions are analyzed together  <cit> . for example, there should be a positive correlation between a transcription factor and its targets. however, it is possible that, under a specific set of biological conditions, the expression of another isoform  <cit>  of the factor or a post-transcriptional modification  <cit>  will drastically change the factor's activity and give rise to a negative correlation with its targets . this specific problem has long been recognized in the field of unsupervised machine learning, leading to the development of bi-clustering techniques  <cit> . however, nothing comparable has been developed to date for the calculation of gene correlations.

another point to consider comes from inter-platform and inter-laboratory studies. the reproducibility of variation is sometimes assessed by considering the correspondence at the top , i.e., the percentage of changing genes in a test list that are also present in a reference list of the same size. more generally, methods comparing the most varying genes found in two independent studies are better able to support conclusions regarding inter-laboratory  <cit>  or inter-platform consistency  <cit>  than are methods relying on correlation coefficients.

taken together, all of these results indicate that the popular methods that have been used to date for the massive analysis of results obtained in different laboratories with single channel technologies, which rely on correlation between expression levels, are not the best adapted and can be outperformed by methods comparing lists of genes showing statistically significant variations. we have developed such a method that allows the calculation of both positive and negative significant correlation scores using gene expression variations between pairs of biological conditions as data. when considering a given set of biological conditions, all studied using the same chipset model based on the single-channel technique, we first perform systematic comparisons between any two conditions / <dig> comparisons). for each comparison, genes are classified as increased , decreased , or not changed  by application of our rank difference analysis of microarray  method . these multiple comparisons result in the assignation of an ordered string of n symbols to each gene, e.g. iddnnidid....dnnnnnnid. in the second step, two covariation matrices  are constructed by calculating statistically significant positive and negative correlation scores for any pair of genes by comparing their strings of symbols  <cit> . this new method, applied to four different large data sets encompassing three different species , allowed us to construct covariation matrices  with similar properties. further, we developed a technique to visualize, in three dimensions, the covariation networks encoded in these cvms, and found that the local assignation of the probe sets was conserved across the four chipset models used. finally, the application of adapted clustering methods allowed us to delineate six conserved functional regions that we characterized using gene ontology information.

RESULTS
data filtering
in this first section, we present the data that we used, explain how the data were organized, and describe briefly the quality filters that we developed. the filters were used to eliminate data that could potentially affect the construction of cvms .

the method that we used to calculate cvms relies on our ability to perform systematic comparisons between any two biological conditions present in sets of experiments coming from different laboratories and on the availability of datasets obtained over a large panel of experimental conditions with the same standardized collection of probes. these two criteria, plus others described in the introduction, led us to restrict our analysis to datasets originating from affymetrix platforms, and to consider four chipset models in which the number of biological conditions studied was greater than  <dig> as of october  <dig> . we organized the downloaded data into a structure in which the samples are grouped according to the biological conditions and experiments. with this nomenclature, experiments and samples correspond, respectively, to geo data sets  and geo samples  in geo. a given biological condition groups all the replicated experimental points .

for each model listed, the number of experiments , the number of biological conditions , and the number of samples  are indicated. columns labelled "before filtration" indicate the raw figures at the time the data were downloaded. columns labelled "first filtration", "end filtration", and "nr filtration" indicate the figures obtained once the unusable  and redundant samples  were eliminated. for these three groups, the number of samples is twice the number of biological conditions.

a preliminary step in the rank difference analysis of microarray   <cit>  is the transformation of each signal into a relative rank, i.e. the placing of the signal value into the signal distribution, expressed on a continuous scale of 0– <dig>  this transformation is important because it acts as a normalisation procedure that allows different samples to be compared. we eliminated all samples having more than  <dig> missing probe sets  or more than  <dig> probe sets with identical signal values .

as rdam can take advantage of duplicated measurements to evaluate the noise distribution and improve statistical power, we did not consider biological conditions represented by only one sample, and we selected exactly two replicates for all of the other biological conditions. we calculated the distance between any two samples  and identified the two most similar replicates for each biological condition. we then inspected the scatter plots of the ranks of duplicates. some duplicates were clearly of poor quality, e.g. with correlation coefficients smaller than  <dig> , and the corresponding conditions were not used. conversely, some duplicates were abnormally similar, e.g. with correlation coefficients greater than  <dig> , and were discarded as well because they were technical replicates. table  <dig> summarizes the effects of this first filtration step on the four chipset models used in this study: 18% of the experiments  and 32% of the biological conditions  were eliminated.

following the first filtration step, we performed systematic comparisons between each pair of biological conditions. a visual inspection of the heat maps representing such comparison revealed that conditions with technical replicates or with poor quality duplicates stood out systematically from the rest by showing a higher or lower level of variation, respectively. the downloaded data were initially processed either with mas <dig> or mas <dig>  the two main currently available versions of the affymetrix analysis suite . comparison heat maps showed that the level of variations in mas4/mas <dig> comparisons was significantly higher than in mas4/mas <dig> or mas5/mas <dig>  these observations led us to eliminate mas <dig> samples from our systematic comparisons. table  <dig> summarizes the effect of this final filtration step on the four chipset models used for this study: 38% of the experiments  and 47% of the biological conditions  were eliminated.

another potential source of bias in the construction of cvms is the presence of multiple experiments that contain similar biological conditions. in order to give the same weight to each biological condition and to eliminate redundant conditions, we constructed a distance matrix and defined as redundant those conditions that shared more than 80% identity. after this process, the number of biological conditions in chipset models hg-u <dig>  hg-u <dig>  mg-u <dig>  and rg-u <dig> was  <dig>   <dig>   <dig>  and  <dig>  respectively .

construction of cvm
following the application of the quality filters described in the preceding section, we had multiple pxt matrices at our disposal for each chipset model, with p representing the number of probe sets and t the total number of comparisons. for example, the results of the analysis of the hg-u <dig> dataset can be synthesized under  <dig> ×  <dig> matrices, since  <dig> biological conditions passed the quality filters . each matrix may contain one of the results produced by the rdam analysis, such as the estimated total variation, the p-value, the false discovery rate , or the sensitivity. in these matrices, positive and negative values refer, respectively, to increased and decreased variations. from these raw values, we then constructed an oversimplified matrix in which variation was coded as either increased , decreased , or not changed . the decision rule necessary to make this transformation of numerical into symbolic values can be any expression that uses the statistical quantities assigned to each variation. in our case, we constructed two symbolic covariation matrices by using an fdr threshold value of either 10% or 1% . the number of symbolic matrices can be extended, if necessary, by considering, for example, conditions with a given property, e.g. a definite pathology or condition  <cit> .

calculation of cvm
the variation profile of a given probe set across all the comparisons is represented by an ordered series of symbols shown as a line in the symbolic matrix. if we extract two lines from the symbolic matrix, keeping them in register, and consider the different combinations of symbols in a particular comparison , we can encounter three different types of informative relationship between the two corresponding probe sets: the two probe sets can have the same type of variation  and are thus said to be positively correlated ; the probe sets can have opposite variations  and are said to be negatively correlated ; or only one type of variation is ascertained  and they are said to be questionable . the number of columns belonging to each of these classes  is counted and the corresponding percentages are referred to as corr, anti, and quest . these calculations, which allow the construction of three independent variables, are extended to all the possible pairs of probe sets, producing one matrix of positive correlation scores , one matrix of negative correlation scores , and one quest matrix, with each matrix having the same pxp dimension.

finally, we applied the same procedure to a randomized submatrix of size 1500xt in order to statistically eliminate non-significant values from the corr and anti matrices. because each line was randomized independently, the number of i, d, and n symbols in each line was conserved, but no significant positive or negative correlations could exist between any two lines . the resulting corr, anti and quest matrices could then be used to determine the noise and to extract the pairs of probe sets in which the corr and anti values were outside of the noise. panel a of figure  <dig> shows that for a given  coordinate the quest value was consistently higher in the randomized comparisons than in the raw data. we therefore traced the surface defined by the 5th minimum of quest for each  coordinate and set the corr and anti values of points located above this surface to zero. then we re-calculated the corr and anti values that were outside of the noise, using informative relationships exclusively, in order to properly process probe sets that had significant variations in a small fraction of the comparisons. for example, if  <dig> comparisons are considered, two probe sets that are positively and negatively correlated in  <dig> and  <dig> comparisons, respectively, and questionable in  <dig> comparisons will have their corr values changed from  to ). the resulting corr and anti matrices can be interpreted as networks: probe sets i and j, indexed by the ith line and the jth column, are interpreted as nodes, linked by two valued edges equal to corr and anti, respectively.

for each of the models listed, four networks were constructed based upon the combination of two independent factors: either all the biological conditions retained at the end of the two selection steps or only the non-redundant ones , and either 1% or 10% fdr . for each network, the table displays the number of biological conditions , the number of comparisons performed , the number of positive correlation scores in corr , the number and percentage of statistically significant positive correlation scores , the mean connectivity of a node , and the mean and median positive  or negative  correlation values linking a node to its neighbours. the values in the last five columns were computed on statistically significant correlation scores. the #raw, #sel and %sel values for the negative correlation scores in anti were similar to those of corr and are therefore not indicated.

performance of correlation scores
to examine the differences between our correlation scores and the widely used pearson's correlation coefficients, we performed an evaluation using a cross validation-like approach. first, we defined three classes of probe set pairs, characterised by the different combinations of our positive and negative correlation scores. for each of these classes we calculated how many probe set pairs had the pearson's correlation of their expression levels more than  <dig>  or less than - <dig>  . we noticed that the probe set pairs that had a marked imbalance between positive and negative scores were properly selected by the pearson correlation coefficient method, albeit with a reduced efficiency. for example, 99% of the probe set pairs defined by us as mainly positively correlated in the hg-u133-nr <dig> network – i.e. with positive and negative scores respectively above  <dig> and - <dig> – also had positive pearson's correlation coefficients. however, only 18% of these pairs had correlation coefficients that were greater than  <dig> . the same is true for the mainly negatively correlated probe set pairs, but in this case the fraction of selected pairs was even smaller, with only 12% of them having a correlation coefficient of lower than - <dig> . it is worth noting that the selection efficiency can be largely improved by defining more homogeneous classes. for example, using refined positively correlated probe set pair classes with negative scores of above - <dig> or equal to  <dig> instead of above - <dig> enables a selection efficiency of 25% and 60%, respectively. we can therefore infer that correlation coefficients applied directly to expression levels are very sensitive to the presence of small subsets of biological conditions in which the correlation is inverted relative to the main trend existing in all the other conditions. in contrast, our method is able to find about 25% of the probe sets pairs with positive and negative scores respectively greater than  <dig> and lower than - <dig>  which remain undetectable when correlation coefficients are used .

second, we selected three subsets of probe set pairs which had pearson's correlation coefficients of greater than  <dig> ,  <dig> , or  <dig>  , and estimated the fraction that belonged to the covariation network. in all cases, the retrieval rate using our method was high, reaching, for example, 80% when the probe sets were correlated at more than  <dig> . in this particular case, however, we noticed that the network constructed using the correlation coefficient had  <dig>  times fewer links than our network.

based on these observations, we concluded that our method clearly outperforms methods based on the use of correlation coefficients in the selection of pairs of probe sets that are only partly correlated. since the fraction of such probe sets is expected to increase in parallel with the number of biological conditions used to construct correlation matrices, this method is thus particularly well suited for the massive analysis of transcriptomic microarray results.

cvm validation
to ensure that our method delivered reliable results, we applied several validation techniques. first, the information stored in a pair of positive and negative cvms describes a network in which each pair of nodes is linked by at most two edges, corresponding respectively to a positive and a negative correlation, and taking their values in the ]  <dig> ] interval. in such a network or graph, the location of a particular node can be characterized by two neighbourhoods, i.e. the combination of all the nodes which are either positively or negatively correlated with this node. comparing the neighbourhoods of probe sets targeting the same genes is a good approach for assessing whether cvms constructed from different chipset models have high degrees of similarity. secondly, we devised a technique to add a geometrical structure to the networks to visualise their internal structure. we observed that the correlation values did not obey the triangular inequality, i.e. f, corr) < = fcorr)+f, corr), with corr and anti being the positive and negative correlations between nodes i and j, respectively, and f being a simple function calculating a value analogous to a distance by combining the positive and negative correlation values existing between i and j. without such a property, a network remains an abstraction and cannot be represented in a 2d or 3d space in a realistic way. once the nodes have been mapped to definite positions in space, the geometrical properties of the structure, such as the distance between a given node and its first neighbour, and structural properties such as its organization into clusters can be used to compare networks.

topological network similarity assessed by measuring probe set neighbourhood similarity
a striking feature of the cvms that we constructed is the variability of their mean connectivities in both intra- and inter-chip comparisons. for example, if we consider the nr <dig> networks, which we favour for most of our applications, the mean connectivity ranges from  <dig>  to  <dig> , and the two human chipsets differ by a factor of  <dig> , with  <dig> links for hg-u <dig> and  <dig> links for hg-u <dig>  nevertheless, we expect that some kind of similarity exists across all the networks despite differences between the species and the biological conditions used.

to assess this similarity, we calculated, for each pair of probe sets targeting the same gene in two different chipset models, the probability that the observed overlap between their neighbourhoods occurred by chance. letting c be the number of common probe sets between two chipset models, i be the rank of a probe set in a first network and j the rank of a probe set targeting the same gene in a second network, n1i and n2j be the number of neighbours for the ith and the jth probe sets in their respective networks , and i be the observed number of common probe sets between n1i and n2j, we calculated the probability of observing at least i common probe sets using the hypergeometric probability h that the observed overlap is due to chance in the case of total independence between the two networks; we called this probability the neighbourhood similarity p-value.

we were first interested in using this approach to compare probe set pairs targeting the same gene within a given chipset model. we assumed that examining such probe set pairs would allow us to eliminate any negative interference that might exist between two different chipset models due to the heterogeneity of the biological conditions used or to the observed inter-chip mean connectivity variability. the distribution obtained in these conditions should therefore give us a kind of standard that we could use to interpret the inter-chipset comparisons. considering that more than two probe sets could target the same genes, we decided, in this case, to retain the two extreme p-values calculated for each of the possible probe set pairs combinations. therefore, probe sets targeting the same genes were split into two categories: i) unique pairs  providing one series of p-values, and multiple pairs  providing two series of p-values. as a control, we also constructed two randomized versions of the unique category: in the "random pairs" category, the second probe set was taken randomly, and in the "random network" category the second probe set had its neighbours randomized before calculating the p-value. the five distributions corresponding to these four categories are displayed in figure  <dig> 

plotting the correlation scores against the log <dig>  as shown in figure  <dig> for the unique pairs category, showed that most of the probe set pairs with a log <dig> of less than - <dig> were also related by a high positive correlation score; these two characteristics led us to consider them as truly targeting the same gene transcript. we hypothesized that the few pairs that had similar neighbourhoods but with correlation scores equal to zero were composed of probe sets targeting two alternatively spliced transcripts of the same gene, with different regulation . we decided to use this log <dig> limit of - <dig> to determine, in tables  <dig> and  <dig>  the fraction of probe set pairs that could be considered consistent on the basis of their high neighbourhood similarities.

for each corr network, the table indicates the number of probe set pairs considered  and the percentage of probe set couples  with a log <dig> of less than - <dig> in three categories of probe set pairs: labelled random pairs, unique pairs, and multiple pairs . for the multiple pairs category, the probe set pair having the best p-value is retained. the final column  presents the mean percentage of links that are common between two sub-networks constructed by considering probe sets with more than  <dig> links and by picking the first or the second probe set from all the probe set pairs with a log <dig> < - <dig>  anti network values are very similar.

the fraction is calculated with the best value of the multiple pairs category. for each comparison between two networks, the table indicates the chipset model of the first and second network , the name of the networks , the number of probe set pairs considered , and the percentage of probe set pairs  with a log <dig> of less than - <dig> in networks constructed either on positive  or negative cvms . the column  indicates the percentage of links shared between two sub-networks constructed by examining probe sets with more than  <dig> links and picking the first or second probe set from all the probe set pairs having a log <dig> < - <dig> 

it is worth noting that the random pairs have a notable fraction of consistent pairs . at least two explanations could account for the high level of consistency observed between the randomly selected probe sets. first, the networks under consideration are scale-free  and contain several hubs connected to a large proportion of the other nodes. as a consequence, two probe sets selected at random have a high probability of counting most of these hubs as neighbours. second, the high mean connectivity  means that the network can be subdivided into a large number of overlapping subsets that have a high degree of inter-connectivity – an analysis technique known as biclustering- and the probability that two random probe sets will be found in one of these subsets is therefore not negligible. finally, it can be seen that the heterogeneity within multiple pairs is very high, since the worst p-value curves are superimposable on those of the random pairs , and the fraction of consistent pairs calculated from the best p-values is  <dig>  times the fraction of unique pairs . in view of this analysis, we conclude that the fraction of consistent pairs calculated from the best p-values of multiple pairs is the right statistical tool to use to estimate the neighbourhood similarity between two networks, because it increases the fraction of pairs suitable for this type of comparison. we expect that the fraction of consistent pairs that can be obtained when testing for the similarity of two networks is around 55% and 70% for the nr <dig> and nr <dig> networks, respectively.

the presence of numerous consistent pairs – i.e. with a log <dig> < - <dig> – allowed us to construct two sub-networks with pair-wise correspondence between all of their nodes by randomly assigning each member of a pair to one of the sub-networks. counting the fraction of links that is conserved between these two sub-networks is another way of measuring the overall reproducibility of the methods. we found that the mean reproducibility ranged from 42% to 88%, depending on the connectivity of the nodes considered . the final columns of table  <dig> and table  <dig> show one example of such a measure, for nodes having a connectivity of greater than  <dig> .

we used the same approach to compare matched networks constructed in two different chipset models. the results obtained were similar to those displayed in figures  <dig> and  <dig> for intra-chip comparisons ; table  <dig> presents the numerical values obtained measuring the neighbourhood similarity. the first conclusion we were able to reach from this analysis was that the similarity of the networks across different chipset models and/or different species remained very high: the fraction of consistent probe set pairs was  <dig>  ±  <dig>  and  <dig>  ±  <dig>  for the nr <dig> and nr <dig> corr networks, respectively. it is worth noting that the mean best value obtained by matching different chipset models  in the nr <dig> corr networks was directly comparable to the best value obtained within individual networks . this means that the 1% fdr selection truly succeeded in finding positive correlations, which were conserved across the three species studied independently of the set of biological conditions used. indeed, this level of conservation drops when a 10% fdr selection is applied ; we may suppose that this is a direct consequence of the disparity among the biological conditions used. considering that this effect was of the same magnitude in the comparison between the two human networks and in inter-species comparisons, we conclude that differences in regulation between species cannot be easily detected if the networks are not normalized with respect to their biological conditions.

taken together, all of these observations indicated that some kind of invariant topological organization underlies the structure of all of the networks that we constructed. however, as we only considered the first neighbours of each probe set, i.e. all the probe sets that were correlated with it, we cannot conclude that this shared organization exists at every scale. it seems sensible, however, to think that as long as small scales are considered our networks are congruent. finally, we observed that there was a clear difference, which had not been observed in our previous study restricted to single networks, between the corr and anti fraction values, with the corr values being greater by  <dig> ±  <dig> points  and  <dig> ±  <dig> points . we can therefore speculate that negative correlations are far more sensitive to the nature of the biological conditions used to construct the network than are positive correlations, and that the former have greater weight in the shaping of networks.

geometrical network similarity assessed by measuring probe set neighbourhood distance and degree similarity
the geometrical representation of a network in 2d or 3d space is another important point to be considered. to map each node to a particular set of coordinates, we devised an algorithm called keiko which uses a physical paradigm. for a given pair of probe sets, we calculate the difference between the positive and negative correlation scores and interpret the resulting positive or negative values as an attractive or repulsive physical force, respectively. we then place the nodes randomly on the periphery of a 3d universe and let the attractive and repulsive forces act incrementally by repeatedly activating forces restricted to a given range, starting with the highest absolute values and finishing with the lowest . as this aggregation process advances, the discrete clusters which are present at the beginning get progressively closer, ultimately forming what we call a probe set map . we show in figure  <dig> six successive steps of this aggregation process, showing that the discrete foci which appeared at the very beginning  got progressively closer  and finally formed a clump at the end of the process . in each network, numerous probe sets stayed outside of the final probe set map .

to check the continuity of the structures that appeared in the initial steps, we used the software gene diver  <cit>  to delimit dense regions buried in a sea of unorganized points and observed how these clusters of points evolved throughout the process. as shown in figure  <dig>  clusters that appeared in the first steps were maintained in the final configuration; we thus concluded that our method allows probe sets to follow stable trajectories.

another issue that we addressed was the influence of the random starting positions of points on the reproducibility of the process. we constructed several keiko maps from one network and compared these maps to one another. to compare any two maps and assess the reproducibility of the network geometry, we first searched, for each node, the closest node in a map used as a reference, which was given a rank equal to one; we then measured in another test map the distance and the rank which separated the two corresponding nodes. a distance score was calculated by determining the ratios of the median distances before randomization of the test map to the median distances after randomization . similarly, a ranking score was calculated by determining the ratios of the median neighbourhood degrees . the upper parts of table  <dig> and figure  <dig> show that the process is largely reproducible, although the random positions of the points at the start of the process introduce some local fluctuation.

the three top lines show the geometrical and topological scores calculated between replicates of the hg-u133-nr <dig> keiko map, giving an indication of the reproducibility of the mapping process. the lower part of the table shows the scores calculated between maps constructed on different chipset models. the scores were calculated as indicated in the legend to figure  <dig> 

the same technique, applied to the comparison of maps constructed from different chipset models, allowed us to confirm that the high level of similarity that we had observed on networks, at least at a small scale , was still present when these networks were mapped to a definite 3d configuration with keiko . indeed, figure 8-a shows that clusters of points located in any given region of the map in a particular network remained grouped when mapped in another network. visual inspection of figure 8-b shows that the lines linking a given probe set in two different networks are not strictly parallel. taking into account that the distance or rank between a given probe set and the closest different probe set does not change dramatically when measured in a matched network , we can conclude that the topological similarity detected in the previous section – most of the first neighbours of a probe set in a given network are also first neighbours of the same probe set in a matched network – is reflected by a geometrical similarity – the closest probe set to a given probe set in one network stays close to the same probe set in a matched network – when our mapping procedure is applied.

structural and functional organisation of the networks
the above analysis showed that our methods succeeded in constructing networks whose geometrical and topological properties were well conserved over a short range. in this section, we show how larger conserved functional regions can be defined and how a synthetic view of the large scale structure of a network can allow conclusions to be drawn about the general organization of the network.

large scale structural organisation of the networks
the clusters found by gene diver in the keiko probe set maps could have been used to study the large scale structural organisation of the networks, as it was possible to identify clusters sharing the same genes across the four networks . however, as local density is not an invariant characteristic of keiko maps , we preferred to concentrate instead on finding a more robust definition of density that was independent of any geometrical representations, using the clustering coefficient to this effect. in this case the clusters are formed by probe sets which are linked together by a high number of edges. the markov clustering algorithm  <cit>  is designed to find this type of cluster by simulating a random walk inside the network and interpreting the strength of a link – in our case the difference between the positive and negative correlations between two probe sets, set to zero if negative or below a given limit – as the probability of following that link. we observed that clustering networks using limits of  <dig>   <dig>  or  <dig> gave comparable results, and that clustering with too high of a limit, e.g.  <dig> or  <dig>  resulted in too few probe sets being located in clusters of reasonable size. we therefore used limits of  <dig>   <dig>  and  <dig> and compared, for each limit, the clusters obtained in the four networks constructed with a 1% fdr. we determined a set of six common regions, with each region possibly resulting from the merging of several clusters in a particular network , combining 71%, 60%, 67%, and 68% of the probe sets present in the probe set maps of the hg-u <dig>  hg-u <dig>  mg_u74v <dig>  and rg-u <dig> nr <dig> networks, respectively. all of the probe sets belonging to clusters not used in the construction of the regions and all of the non-clustered probe sets belonging to the probe set map were assigned to regions  <dig> and  <dig>  respectively, for the sake of convenience .

the dendrogram of figure  <dig> shows that all of the regions, except for region  <dig> of rg-u <dig>  are perfectly grouped with respect to their ranks, and that each group of regions is well separated from the others, meaning that we succeeded in delineating six different regions that are well conserved in the four networks. for the six regions, we measured the mean percentages of conserved probe sets to be 39% ±  <dig>  48% ±  <dig>  29% ±  <dig>  53% ±  <dig>  57% ±  <dig>  and 60% ±  <dig>  respectively.

the probe sets located in each region are highly connected, with a mean connectivity of  <dig>  ±  <dig>  and  <dig>  ±  <dig>  for the positive and negative correlations . another characteristic of the regions is the large fraction of edges emanating from the probe sets located in a given region that target probe sets located in the same region . the equality of the positive and negative connectivity does not mean that the two types of correlation have the same importance inside the regions. in fact, we found that positive correlations were predominant inside the regions, having a mean value of  <dig> ±  <dig>  which was much greater than the negative correlations, which were measured at  <dig> ±  <dig>  in comparison, calculating the same values between a given region and the other five regions showed that positive and negative correlations had the same weight: the correlation values were equal to  <dig> ±  <dig> and  <dig> ±  <dig>  respectively, and the connectivity was equal to  <dig> ±  <dig> in both cases . from these averaged properties, we can characterize the network as a structure comprised of six loosely connected regions that contains highly connected probe sets with high positive correlation and low negative correlation.

by mapping all the probe sets belonging to a given region onto the probe set maps constructed with keiko, we found that the corresponding points were also localized to a defined region made up of a dense subset of points localized in a small part of the whole space, with very few intersections with the other regions. this means that the application of two very different algorithms produced two congruent outcomes: either a clustering of probe sets by markov clustering, or a disposition of the probe sets in a 3d space by the keiko algorithm. the combination of the two types of information – regional membership and position – allowed us to obtain a synthetic view of and have a more precise perspective on the structural organisation of the networks coded inside the cvm. we calculated for each pair of regions an inter-region correlation trend . each region was then displayed both as a circle positioned at the barycentre of its probe sets and as a closed line delimiting its maximal extension. the positive and negative correlation trends are represented by red and blue lines, respectively, whose widths reflect the trend strength. figure  <dig> displays the four maps obtained with this procedure.

we observed, first, that the probe set map architecture is shaped by both attractive forces, which are responsible for the formation of dense regions of probe sets and for the proximity of some regions, and repulsive forces, which are responsible for the separation of some regions. for example, the hg-u <dig> and hg-u <dig> networks, which have strong parallel negative trends, are outstretched , while rg-u <dig> is constrained into a triangular shape by strong orthogonal negative trends . conversely, as most of the inter-region trends in mg-u74v <dig> are slightly negative, it is not surprising that the corresponding structure is symmetric and round . another interesting observation is that positive correlation trends are not transitive over long distances: if regions a and b are close and positively correlated, and so are regions b and c, then regions a and c have a positive correlation trend as well. but if we add a more distant region d that is positively correlated with, e.g. c, then regions a and d may have a negative correlation trend, as exemplified by regions  <dig>   <dig>   <dig>  and  <dig> of figure 10-a. in other words, a chain of positively correlated regions is generally terminated by negatively correlated regions.

finally, we note that it is impossible to obtain a consensus description of the network structure: six well-defined regions, each composed of highly correlated probe sets, are indeed present in all four networks, but the relationships between these regions are not constant over the four networks, and some regions that are mainly negatively correlated in a given network are positively correlated in another network, e.g. regions  <dig> and  <dig> in the hg-u <dig> and hg-u <dig> networks. given the high similarity between the networks demonstrated above, we conclude that their organisation into six regions is a strong and fundamental characteristic, whereas their observed relationships are largely dependent on the particular subset of biological conditions used to construct them.

small scale structural organisation of the networks
the structure of the core network, i.e. composed of six regions, becomes more apparent when the cvms are displayed after having been reordered by clustering positive correlation measures independently in each region using annealing clustering  <cit> , as shown in figure  <dig> for the hg-u133-nr <dig> cvm . in accordance with what is shown in figure  <dig> and with the conclusions of the preceding section, this representation shows that the six regions are loosely connected. in addition, the clustering makes it obvious that the probe sets are organized within each region into overlapping groups of tightly correlated probe sets – a characteristic of functional modules- and that the interactions between the modules within a given region are strong. however, we were unable to find a simple way of identifying modules that were common to the four networks; better analytic techniques and/or normalization of the biological conditions could potentially show that the different networks are similar at this level of organization. it can also be seen in this particular example that in regions  <dig>   <dig>  and  <dig> most of the probe sets are present within modules, whereas in the other three regions about one-third of the probe sets are sparsely correlated. it might not be coincidental that these sparsely correlated probe sets are never implicated in positive correlations between the regions, and that only probe sets within modules can have both positive and negative correlations between the regions.

functional organisation of the networks
we used the generator tool  to calculate p-values for all the go terms associated with the genes present in the different regions, and asked whether the regions were homogeneous with respect to their gene ontology annotations. by considering the five go terms with the most significant mean p-values, we were able to determine, without ambiguity, the main characteristics of each bona fide region within the three branches of the go nomenclature. to give a short description for each region, we selected the five most representative terms from the three go branches ) and constructed the following descriptors, using one representative term for each go branch, corresponding respectively to regions one to six: immune system/signal transduction/lysosome , nucleic acid metabolic process/nucleic acid binding/nucleus , cell adhesion/extracellular matrix structural constituent/extracellular matrix , energetic metabolism/nadh dehydrogenase activity/mitochondrion , nervous system/ion channel activity/neuron  and metabolism/oxidoreductase activity/endoplasmic reticulum . the six regions are therefore related to the immune system, nuclear processes, cellular adhesion, energetic metabolism, the nervous system, and metabolism, respectively.

several observations allowed us to conclude that these six regions form a well-established structure that is present in all four networks when studied at a functional level. first, most of the terms with a p-value of <=  <dig>   were directly or indirectly related to the main characteristic of their region. second, the characteristic terms had extremely low p-values and were present in a high proportion of the genes. third, we calculated two scores for go terms having p-values equal or inferior to  <dig>   for each region and each network, in order to assess the degree of homogeneity in each region; we found that the similarity score, which is the percentage of go terms found in at least three networks, and the purity score, which is the percentage of go terms specific to the region under consideration, were both high, with means of 97% and 73%, respectively . we also found, however, that three regions of the rat network were not really pure and were enriched in genes normally assigned to other regions in the three other networks: region  <dig> was enriched in go terms associated with region  <dig> , and regions  <dig> and  <dig> contained many terms linked to region  <dig> .

another function of generator allows the clustering of genes to groups that have go terms in common. we constructed another score that measures the number of clusters containing a given go term. this information allowed us to detect groups of genes with very specific functions . for example, the go term "dna unwinding" was very specific and was found only eight times in the list of go terms corresponding to the human chipsets. in hg-u <dig> and mg-u <dig>  this term was found in two clusters: one cluster of  <dig> and  <dig> genes, respectively, mainly related to transcription , and one cluster containing  <dig> and  <dig> genes, respectively, linked to the regulation of chromatin and dna packaging . in hg-u <dig> and rg-u <dig>  all the "dna unwinding" terms were found in one cluster per dataset, containing  <dig> genes  and  <dig> genes , respectively, related to chromosome organization. this score also confirmed the difference between regions  <dig> and  <dig>  by listing the terms with the highest mean scores calculated for the four networks, we found that in region  <dig> all the terms were related to energy production , while in region  <dig> most of the terms were connected to specialized catabolic or anabolic functions .

discussion
our study highlights the importance of doing quality controls and underlines the effects of various parameters on the structure and quality of the constructed networks. a typical example is the dramatic reduction in mean connectivity, which was reduced by a factor of three when the redundancy of biological conditions was taken in account . another example is the presence of artefactual variations, which were observed in comparisons between data analyzed by two different versions of the affymetrix software. the significant impact of the analysis method used raises particular concern since there is no required field for indicating the analysis method used when data are deposited on the geo repository site. we are confident that all of the data we ultimately used were analysed by mas <dig> because they were all uploaded by the end of  <dig>  a time when alternative analysis methods were not frequently used. in a recent paper, lim wk et al.  <cit>  estimated the impact of normalization procedures on the correlation structure. testing the hypothesis that highly co-expressed gene pairs are more likely to share common go terms and to predict protein-protein interactions than are uncorrelated genes, they showed that the mas <dig> algorithm outperformed the rma  <cit>  and li-wong methods  <cit> , and also that the grcma methods  <cit>  were of little use because they introduced many correlation artefacts. in view of this evidence, it seems advisable to work directly on raw data  and to normalise the probe set signal computation by applying the same adapted algorithm. the celsius project, which is designed to be a repository of raw affymetrix data and of various analytic methods, makes this approach perfectly realistic  <cit> . independent of the algorithm used to calculate the pair-wise correlations, normalisation is certainly one of the most important steps for attaining a high degree of reproducibility when networks constructed in different species are compared. accordingly, we think that our results could be greatly improved by normalising the network against the targeted transcripts. the statistical study of probe set neighbourhoods  shows that two probe sets assigned to the same genes can have divergent neighbourhoods. we have obtained preliminary results showing that this occurs mainly because they target alternatively spliced transcripts with opposite modes of regulation. by grouping the probe sets targeting the same sets of transcripts and having the same neighbourhoods, it should be possible to construct transcript-oriented networks that are simpler, more complete and more reproducible  <cit> .

to validate our method, we followed a widely used approach that assesses the quality of a particular list of genes by showing that the overrepresented go terms characterising the genes belong to the same functional class  <cit> ; this approach has been extensively applied to the clusters of genes found in coexpression networks  <cit> . to increase the validity of the test, we verified that the large regions we detected with the markov clustering algorithm were not only characterised by a derivative of the "guilt by association" method  <cit>  but were also conserved across four networks encompassing three evolutionarily close mammalian species. the extension of the study to four networks allowed us to add a test confirming the invariance of the local network topology by considering the reproducibility of the neighbourhood of a given gene across the four networks. the keiko method that we developed – which enables the creation of 2d and 3d geometrical representations of the network while keeping the essential information encoded in cvm, with negatively and positively correlated probe sets being farther away or closer, respectively, according to the strength of the correlation – facilitates the visualisation and comparison of the networks. at the smallest scale, we observed that the local architecture is well conserved and is mainly shaped by strong positive correlations. with respect to the relationships between the regions defined at a higher scale, we observed that all four networks displayed a low mean inter-region connectivity. however, the strength and direction of the mean inter-region correlations were not conserved. we presume that this lack of reproducibility was mainly due to the lack of normalisation in the biological conditions used.

several studies have used different species to construct coexpression networks  <cit> , and it is interesting to see how our findings compare to their results. among these studies, those aimed at delineating modules are not adapted for such comparisons. modules are subsets of co-regulated genes associated with particular biological contexts – the subset of conditions under which co-expression takes place- and they form collections of numerous small overlapping lists of genes  <cit> . in contrast, our method partitions the entire gene set into a relatively small number of large, non-overlapping groups. j.m. stuart et al.  <cit>  searched for pairs of genes whose expression was significantly correlated in four evolutionarily diverse organisms  in order to identify physiologically important, evolutionarily conserved examples of gene coregulation; they defined  <dig> components. as they did not list the metagenes belonging to each of the  <dig> components that they defined, we used the general biological description that they provided for each component and searched for regions with an overrepresentation of the go terms linked to that description. most of the correspondences were many-to-one relationships: components  <dig> ,  <dig> ,  <dig> ,  <dig> , and  <dig>  corresponded to region  <dig> ; and components  <dig>  and  <dig>  corresponded to region  <dig> . conversely, some components were split and gave rise to one-to-many relationships: component  <dig> had one function  linked to region  <dig>  and another function  associated with region  <dig> . similarly, component  <dig> had one function  linked to region  <dig>  and another  associated with regions  <dig> ,  <dig> , and  <dig> . component  <dig>  corresponded to region  <dig> , component  <dig>  had no clear counterpart, and component  <dig>  was too loosely defined to find any correspondence.

by applying principal component analysis  to the expression levels of orthologous human and mouse genes in different tissues, chen et al.  <cit>  identified  <dig> conserved gene expression response modes , each one characterized by a list of the most significant genes . one of these specialized modes  corresponds to region  <dig> . five modes which are characterized by go terms, kegg pathways, and cog classes corresponding to the same general function ) match in a one-to-one ratio to regions  <dig>   <dig>   <dig>   <dig>  and  <dig>  five heterogeneous modes ) correspond to multiple regions, and one mode  has no particular match with any region. no mode has a clear match with the nervous system , although several nervous system tissues were present in the samples used.

taking all of these studies together, we note that there were three types of potentially problematic results: first, there was a coexistence of groups with very general functions that corresponded to the six regions we defined  and groups with very narrow functions ; second, some separated groups had the same function ; and finally, some groups were heterogeneous . we conclude that our method, when applied to inter-species comparisons, was capable of defining reproducible groups that had biological functions with the same degree of generality. since our method generates networks with high mean connectivity, we assume that using high values of pearson's correlation coefficient, e.g. higher than  <dig>  or even  <dig> , results in sparse networks  which could explain the occurrence of very specialized groups and the splitting of otherwise homogenous large regions into several isolated groups when clustering methods are applied.

CONCLUSIONS
the distinction between coexpression and covariation has never been clearly addressed with respect to the massive analysis of transcriptomic microarray data, and the choice between these two approaches has been mainly dictated by technical considerations: single- and dual-channel techniques results are de facto considered to be coexpression and covariation, respectively. although several arguments, derived from large-scale studies on the reproducibility of inter-laboratory microarray results, converge toward the conclusion that variations  are far more reproducible than are signals, it has only been in time course analyses that the benefits of the covariation approach have been recognized and single-channel results specifically manipulated to give covariation values  <cit> . the method that we have devised to calculate probe set correlation scores is the first to implement these principles, allowing the covariation approach to be applied to the major compendium of microarray results represented by affymetrix data. another advantage of our method is its ability to deal with circumstances in which the correlation between two given probe sets is positive in one comparison subset and negative in a second one. in such cases, the widely used pearson's correlation coefficient is ineffective, while our method can properly calculate both positive and negative correlation scores.

we applied our method to four data sets covering three different mammalian species . in addition, we conducted in-depth comparisons between the corresponding covariation networks in order to validate our method. first, we determined that the four networks shared a similar network topology by measuring the probe set neighbourhoods. second, we developed a technique to display the networks in 3d, revealing that the local assignation of the probe sets was conserved across the four networks. finally, we applied the markov clustering algorithm and showed that the transcriptional networks are organized into six loosely connected regions, each representing one of the main physiological functions in mammalian species and containing around  <dig> genes.

