BACKGROUND
applications and databases available online for bioinformatics research are rapidly proliferating  <cit> ; however, the absence of effective discovery tools for these resources prevents them from being combined in workflows to create powerful bioinformatics machines.

typically, in service-oriented architectures, dynamic discovery of tools is made possible by registering tool metadata in a shared repository or registry--for example, uddi. in bioinformatics, some metadata repositories such as biomoby  <cit>  and feta  <cit>  also recognize the importance of sharing data formats between tools, and make use of this strategy to implement integration architectures. such repository approaches have collected large sets of registered services and data types, making a manual discovery process difficult and time consuming. support for this task has become crucial.

in general, a discovery process aims to segregate a set of services or data-types that satisfy a given number of requirements from the larger pool of available resources; for example, what services are able to process my molecular sequence?

these discovery processes can be based on syntax or semantics. in bioinformatics, a syntax-based discovery process is often unsatisfactory because it presumes knowledge of the names of the objects or services to be searched. semantics-based discovery processes enable a more accurate discovery mechanism since the descriptions are generally structured and well defined. magallanes uses a syntactic approach for text-based searches and a semantic approach to combine different services, guided by semantic knowledge about the input and output data types.

the essential task is to design and implement a search engine adapted to the demands of bioinformatics. in this document we present a programmatic library called magallanes that provides necessary support for flexible and expandable discovery of services, which in turn simplifies the creation of workflows. results from magallanes' discovery process can be used as input for workflow generation because web services are automatically combined based on semantic descriptions .

as a proof-of concept, we have developed several variations of the same client  that use magallanes as a discovery engine . these clients can also be embedded in third-party applications.

implementation
related work
in this section, we outline related work. since our work consists of a software library which can be re-used in other client software, we look at two aspects: search and workflow composition functionality in existing clients. although magallanes support standard wsdl web services, our efforts so far have focused on biomoby services since there is a large set of services to search and those services are easily composed due to a shared data type ontology.

therefore, we selected the most prominent clients for biomoby web services for this overview of related work.

service discovery by clients
biomoby-compatible clients, such as mowserv  <cit> , seahawk  <cit> , remora  <cit>  and taverna  <cit> , provide support for service searches in various degrees . typically, services are located by specifying an input data type  or by name .

biocatalogue  <cit>  is a public curated catalogue of life science web services that provides a keyword based search and allows filtering the results by some metadata like service type, provider, country, etc. biocatalogue could benefit from the functionality of magallanes.

automatic workflow composition
gbrowse  <cit>  and seahawk  <cit>  are two end-user oriented clients with a data-centric approach. users either specify input data by submitting a data file which the application analyses to determine the correct data type  or they specify identifiers of a sequence in an external database . available services to further process data are presented once the user data and data type is known to the applications. seahawk has a slightly more sophisticated approach that lets the user specify what he/she wants to do with the data  instead of choosing the next service specifically. both applications allow users to save their results as taverna workflows.

the strategy in  <cit>  is to simplify interactive service composition of biomoby services. in each step of the workflow construction process, only those services that are compatible and more likely to be useful are displayed. this is achieved by ranking the services according to several aspects, such as semantic similarity of data type inputs; or by non-functional measurements such as number of retrievals of service definitions from mobycentral. their composition algorithm aims to limit the number of services presented to the user. the algorithm considers not only direct compatibility and polymorphism , but also includes services whose input/output data type matches the requested data type either directly or recursively. results are further ranked by their popularity as measured by requests to the central registry, and by the disparity between data types in the ontology.

summarizing the related work
as is evident from section 'service discovery by clients', search functionality in clients is heterogeneous. users would benefit from having a standardized way of locating services. client software developers could, instead of re-inventing search algorithms, focus on providing a user-friendly graphical interface. this shows the need for a freely shared software library.

regarding workflow generation, authors in  <cit>  do not address the problem of locating data types as input to the workflow generation algorithm. this problem becomes serious considering the proliferation of data types in biomoby .

while there is clearly a need for service composition support, current approaches fail to recognize the difficulties in a) find initial data type and b) automatically creating an initial version of a workflow. therefore, we have addressed both a) and b).

magallanes' architecture
existing workflow composition approaches do not assist the user during initial selection of input and output data types, even though this step can be quite complicated. magallanes aims to do two things: simplify the discovery task, and integrate discovery with composition.

magallanes consists of a java library with algorithms and data handling routines built using the modular api  <cit> . the modular api uses specific wrappers called accesses to map different data types and web-services repositories into a unified model . magallanes can access and manage various remote repositories using a standardized interface, and benefits from a cache system to reduce processing time. currently, the modular api can access to biomoby, inb, acgt and standard wdsl repositories. in order to support another repository, a new access must be implemented .

magallanes' api is organized in two main modules: search engine and workflow composition.

search engine
the search engine module provides google™-like methods for finding web resources using a scoring system based on the number of occurrences and relative word positions of matching hits. currently it is endowed with and/or operators and regular expressions. the searching space defined by the resource metadata is easily expandable . the algorithm initially searches for words similar to the keywords on the metadata descriptions. the similarity threshold can be setup as a configuration parameter . if no hits occur, it becomes necessary to fall back on approximate expression matching. there are two widely used approaches for approximate expression matching: the hamming distance  <cit> , which compares strings of the same length and the levenshtein distance  <cit> , which compares two strings not necessarily having the same length, measuring by the minimum number of insertions, deletions, and substitutions of characters required to transform one string into another. levenshtein distance is also known as the matching with k differences or errors. if the search does not generate hits, a "did you mean?" module in magallanes pops up to aid the user. this module offers plausible alternatives to the user's query by computing the levenshtein distance automatically  to identify words similar to each keyword, and to estimate the distance using multiple keywords.

magallanes uses a feedback module to continually learn and refine its discovery capabilities. any client software using magallanes is able to access this feedback module, which records user selections of resources associated with specific keywords. the module stores this information and records the 'feedback' value associated to the keyword-resource tuple . this value is adjusted when the user selects another resource using the same keyword.

selected kr tuples increase their feedback value  using the function v = v*α + , where α is a decay value  to slope the learning curve, and correspondingly all the remaining kr tuples with the same keyword decrement their value to v*α.

results are ranked by combining the metadata matching information  with the feedback information when available. a score is computed as the average value of both. the first value is computed as the hits density in the matching space; in other words, the rate between the number of hits and text length. all density values are averaged and normalised to  <cit>  to produce the metadata score. the feedback value for a given resource is the average of the kr tuple value for all the keywords used in the query.

finally, magallanes also allows the use of third-party discovery functionality. for instance, several repositories implement discovery strategies based on web service compatibility with a given data type . intuitively, the consecutive application of this strategy can be exploited to create a sequence of compatible services that connect a given input with another target data type, in "pipeline" fashion. this motivates the next major area of functionality offered by magallanes: the automatic arrangement of services to connect differing data types, including the management of user interactions to refine results.

automatic workflow composition
the workflow management consortium  defines a workflow or workflow model as the complete or partial automation of a process in which information or tasks are passed from a participant to another according to a defined set of procedural rules.

bioinformatics research can often benefit from connecting several applications in sequence to form a workflow . manual construction of wfs is complex and prone to error, particularly in bioinformatics where data comes in a multitude of formats. combined with the difficulty of using distributed web services, composing a meaningful wf can present a challenge to life scientists.

automatic workflow generation  aims to automate the task of connecting independent services. two services can be connected if the output of one is compatible with the input of the other. therefore, the task of automatic workflow generation is to find the shortest non-redundant sequence of services, meaningful to the research, that match outputs with inputs to link the source to the target data type.

workflow generation support can be either semi-automatic, interactively giving advice on suitable services for each step in workflow construction, or fully automatic, where the scientist only provides input and output data sets and the algorithm generates the complete workflow.

in simplest terms, the automatic wf-builder in magallanes proceeds to identify all the services that produce a target data type as output. all the data types used as input for such services are used a target in the next step.

a well defined data type hierarchy will provide the required semantics to generate meaningful workflows.

a breath-first with pruning algorithm  <cit>  speeds up the process of finding the shortest path from source to target .

the following definitions are needed for formal statement of the algorithm: let d be the set of all the data types, and d* be the set of all the possible subsets of data types , thus d* = {d ⊆ d }.

let t be the set of all the registered tools in the current repository and let p be all the tools i/o combinations . therefore, p = {i, t, o: i, o ∈ d, t ∈ t | i is an input of t and o is an output of t }.

here h- : d → d* is a function defined to consume a given data type or any of the corresponding sub-data types as input: di → d = { dj ∈ d: dj is subtype of di }.

as can be observed, h-: d* → d* extends the function to receive a set of data types to return all the subtypes of any of them di → d = { dj ∈ d | ∃ dh ∈ dj : dj is subtype of dh }.

the inverse function h+: d → d* defines the supertype di → d = { dj∈ d: di is subtype of dj } and correspondingly h+: d* → d* extends the function to use collections of inputs: di → d = { dj ∈ d | ∃ dh ∈ dj : dh is subtype of dj }.

using these definitions we can outline the algorithm:

function compass

   input: source: d

      target: d

   sources: ∅

   unexplored: {target}

   explored: ∅

   depth : 0

   maxdepth: infinity

   while unexplored != ∅

      current: first of unexplored

      unexplored = unexplored - current

      explored = explored ∪ current

         if depth  > maxdepth

            end

         if current ∈ h+

            sources = sources ∪ current

            maxdepth: depth 

            continue

         ∀ p =  ∈ p: po = current

            unexplored = unexplored ∪ pi

            depth : depth  +1

            suc  = suc  ∪ pi

outputs:

sources: possible wf's input

suc : p ∈ p backtrack information from dt to a source.

when the sources set is empty means a partial solution was obtained; otherwise, a full solution has been reached. it is possible to modify the algorithm's behaviour to search for solutions other than the shortest by managing the depth threshold parameter and by using a-posteriori refinement of the solution space by user interaction.

to illustrate the algorithm behaviour, a trivial example with a reduced set of data types  and services registered in a repository will be used . the example consists of obtaining a set of amino acid sequences in fasta format  that are similar with a given an aaseq .

schematic representation of a reduced set of data types and associated services able to process these different types of data. although tools names are descriptive, a long description is available as supplementary material .

the content of the variables during the execution of the algorithm evolves as follows : 'sources' will contain all the possible wf inputs; 'unexplored' is a list with the remaining dts to analyse, in this case the target fastaaamult dt and 'explored' is the list with the already analysed dt. the variable 'maxdepth' contains the depth of the shortest solution, and depth  contains the specific values for each data type. finally, suc  is the set of services that produces the target from <dt> source in depth  steps.

tracing information showing the content of the main variables during the step-by-step algorithm execution.

in step  <dig>  'current' is assigned with 'fastaaamult', this data type is removed from the 'unexplored' list and added to the 'explored' list. for each tool that returns the 'fastaaamult' data type , add the inputs to the 'unexplored' list  and set the depth of each input to depth + <dig> . finally, add the tool to the input data types' successors.

in step  <dig>  proceed as in step  <dig> by taking 'blasttext' from the 'unexplored' list as the 'current' and including 'aaseq', 'nnseq' and 'fastaaa' to the 'unexplored' list, all of them with a depth value equal to  <dig> 

in step  <dig>  'current' takes 'aaseq' from the 'unexplored' list. since 'aaseq', is the source data type, the algorithm adds 'aaseq' to the 'sources' list and removes it from the 'unexplored' list. set maxdepth to the depth  value .

in step  <dig>  'current' takes and moves 'nnseq' from the 'unexplored' to the 'explored' list. since there is no tool that returns 'nnseq' data type, go to the next step.

in step  <dig>  proceed as in step  <dig> by selecting 'fastaaa' from the 'unexplored' list as the 'current' data type. the algorithm ends at this step because the 'unexplored' list is empty.

the only possible workflow input is sources = {aaseq}. to built the graph , wf sources are retrieved from sources and the successors can be recursively obtained from suc  is closer when using aaseq to call runblastp or runtblastn).

RESULTS
in this section, we describe the role and operation of magallanes client. magallanes' api functionality is available as a set of java methods  that can be used by external clients in such a way that results can be used to invoke web services, recover data type descriptions, build up workflows, etc. specific clients can be developed to fulfil services inter-operability, to enhance search engines, include the "did you mean?" method, etc. . it is noteworthy to observe that 'did you mean methods' has a long tradition of support of searching engines in the web environment, so what we claim is the novelty of the design, incorporation and utility of this type of strategies in the bioinformatics application domain.

magallanes is a client with simple but powerful architecture for resource discovery and workflow composition . as previously explained, the bottom layer of magallanes' api supplies a uniform view of different data models by managing the uniform representation of resources from different repositories.

magallanes, coded in java, provides:

• integration of different repositories through the use of the modular api in such a way that discovery can be defined for multiple or individual repositories;

• the capability to extend the metadata discovery space to web links available in descriptive metadata;

• 'did you mean' assistance methods and user-profile learning capabilities;

• wide and extendable functionality to include new calling methods using the types of data available; and finally,

• several alternatives for user interaction, using the gui library to directly built-up a desktop application, or developing a web-based tool by direct use of magallanes' api; or as a discovery engine embedded in third-party applications.

just like magallanes' api, magallanes client is organised into two main modules: searching and workflows. both are gui coded as java swing components  <cit>  for querying and managing results, providing extra functionality such as service-data type compatibility.

the wf composition module is independent from the discovery engine, but the client can send data between the modules . the interactive gui uses the jung framework  <cit>  for graphing.

magallanes is available as desktop application, java web start, web page, web service implementation, or portlet in a grid-environment . it can also be embedded in third-party applications like the jorca client .

for a deeper exploration of magallanes' capabilities, let us assume that we wish to obtain a multiple sequence alignment in newick format using a given generic sequence as the starting point. the 'did you mean?' module manages spelling mistakes  and suggests a set of possible solutions  related with the word 'sequence', among them the 'genericsequence' data type that can be used as initial input into a wf. note that genericsequence is an example of a biomoby specific data type that is likely unknown to the average user. the 'did you mean?' module was able to inform the user about this data type. a second search using 'tree' keyword will identify "newick_tree" that can be entered as target data type. then the algorithm selects the "shortest" path, which could be extended by relaxing the depth threshold or by expanding one more level in a particular step.

in some research endeavours, a problem arises when only a partial solution can be found and there are no services to connect a given data type with the source data type by reverse analysis. magallanes manages this situation by inserting a black-box service to complete the pathway, letting the user search manually for a solution.

wf editing
although wf editing is beyond the scope of magallanes, the generated wf model can be examined by an expert user using the developed gui, which has two switchable and synchronized graph layouts to provide alternative graph representations. the expert user understands how to analyse the wf model and, at the end of this stage, the model can be validated and accepted. in addition, before validating, the expert can review further feedback on the quality of the wf model derived by the quality of the individual services that make up the workflow. some systems such as mowserv  <cit>  store information about service performance: cpu time, availability rate, frequency of use against alternative services, etc.

the resulting wf model can be stored in a scufl format for editing using the taverna  <dig>  <cit>  application. final mapping with end-point services will occur in run-time.

discussion
scoring system: resource retrieval
the rationale of the scoring system used to rank resources is to combine the learning rate based on traditional kr voting systems with user's feedback. this enriches resource identification and adapts resources more rapidly to user needs. for instance, magallanes stores feedback information using a file system--local in the case of desktop installations or on the server for web-based installations. in these different conditions, context sensitivity can be controlled by managing the learning rate. for instance, the typical scenario for desktop implementations is an individual user, thus a rapid learning rate is appropriate to accelerate adaptability to user preferences. however, a web-based application is designed to be used by several users, so a slower learning rate would improve stability and better reflect group behaviour.

wf composition
depth- and breath-first with pruning implementations were both evaluated for wf composition. depth-first was able to identify the shortest solution by using an adaptive threshold. however, the repetitive exploration of the potential-solutions space is a challenge when efficient implementation is the goal.

initial breath-first implementation drives a forward exploration from source to target data type with poor response times. the main reason for the excessive response time is related to the large number of services that uses a generic object as input; the typical root in the data type taxonomy system . as result, object-input services always appear to be initially compatible.

however, a breath-first backward implementation from target to source data types produces good response times. basically, this is because there are many more ways to consume a data type in current repositories than ways to produce data of a specific data type.

in graphical terms, forward connection is equivalent to the query, 'which services can use a given dt?' this approach becomes expensive because of the large number of services that consume the root object . backward compatibility asks 'which services return a given dt?' situations can arise when the list is empty since several dt are used as input by the services ; however, in general, the list is much shorter than in the forward approach.

our breath-first algorithm is quicker than the deep-first one, although it consume more memory. to ensure that memory demand in breadth-first approach do not represents a problem we have tested the algorithm using the university of calgary's biomoby repository with around  <dig> services and  <dig> data types registered. no memory problems arose.

about the "did you mean?" methodology
levenshtein distance is used to identify similar words in the repository, producing a ranked list of possible solutions available to the client. this strategy ensures an up-to-date dictionary that is adapted to a specific repository , but it becomes influenced by the quality of annotations in the repository .

the initial search approach is perfect matching which can produce an aesthetic situation: if a spelling mistake is made during the metadata resource annotation and the same mistake is used as query keyword, magallanes will identify the mistake as the best solution and will suppress the "did you mean?" module. however, this functionality  can be managed by the magallanes' api.

another discovery alternative is the use of dictionaries or ontologies to link related concepts, such as "fasta → sequence → genome" providing semantic information that can be exploited by reasoning engines. however, a generic approach fits better for a broad range of applications, and also, the system is easily extendable to incorporate specific discovery mechanisms.

complex workflows 
without losing the advantages of generality, the described procedure enables users to discover alternative pipelines that connect source with target data types. however, some services need more than one input data type that, in turn, needs to be obtained via another pipelined branch of services. the current solution uses the iterative application of the algorithm for each of the needed branches and integrates them into a global solution .

service discovery
mowserv  <cit>  provides several alternatives for service discovery. the taxonomies for services and data types are presented in a tree which the user can browse and search. the search shows the number of hits and highlights the hits in yellow. the user must expand the trees to identify the hits. furthermore, the trees always contain all resources, not only the matching hits, which make it difficult to find the hits. searches are limited to full-string matching . additionally, services can be located based on the input data . recent additions to mowserv allow users to search for services based on input data types, service type  and output data types.

as mentioned before, seahawk  <cit>  analyses user data to determine the correct biomoby data type. based on this information, the application presents the user with available services grouped according to keywords . there is no direct searching of service or data type descriptions.

remora  <cit>  displays available services based on the currently produced data type during workflow construction. furthermore, the application provides search functionality that selects search terms based on direct or partial matches. search terms must be exact and cannot be misspelled.

taverna  <cit>  has updated its search functionality in version  <dig> . the application now allows direct and partial matching of nodes in the tree of available activities  showing also the number of matches. the tree is automatically filtered to show only matching results.

taverna has a plug-in for feta that allows the user to discover services based on name  or additional constraints where the user can select concepts from a list . concepts used in the search are taken from the mygrid ontology  <cit> .

workflow generation
magallanes performs automatic workflow composition, generating the entire sequence of services from start to finish datatypes. after the workflow has been generated, the user can select alternative paths. this strategy is less interactive than  <cit> , but magallanes was never designed to be a complete workflow editor. instead, its focus was on service, data type and workflow discovery. service and data type discovery allows clients to find the required data types and services using text searches in descriptions. workflow discovery is not only supported in the obvious way, by treating workflows as services, but also by generating interesting workflows on-the-fly, allowing users to "discover" potentially interesting workflows and then export them to a fully-fledged workflow editor such as taverna.

magallanes' workflow generation is based on the hits generated in the search engine  where the user can choose to select a data type as source or target for the workflow generation algorithm.

as proof of concept we choose to test the workflows generation module to reproduce already published workflows like  <cit> . the workflow reported by kerhornou and guigó supports the clustering of co-regulated genes, producing as main result a hierarchical clustering in newick format from a collection of dna sequences.

magallanes is able to find multiple alternative paths to solve that problem from a fasta_na_multi  to produce a clustering in newick format . the main path of that workflow  can be obtained making only four branch selections on magallanes. this task only took a few minutes to complete with magallanes, comparing to around two months needed for the manual elaboration of the same workflow  .

CONCLUSIONS
one of the most relevant research methods in bioinformatics is intensive use of distributed web-accessible resources. as a number of recent technical publications suggest, appropriate tools for resource discovery and for composition of complex workflows have become urgently needed. both discovery and composition are the new paradigms to support data processing in massive genomics analysis. in this document, we have acknowledged those new working paradigms and proposed effective solutions.

the magallanes software library supplies an integrated framework to develop powerful discovery engines that help researchers find web-services and associated data-types. the rationale for magallanes' design has been efficiency and usability. there is consensus in the genomics research community that one of the biggest barriers to the integrated use of remote resources is difficulty of locating the appropriate resource. several techniques have proposed to solve this problem, with varying degrees of success. magallanes represents advancement in practical web-resource discovering tasks, regardless of application domain. approximate keyword matching and user profiling have demonstrated the power of simple approaches similar to the most commonly used way to locate web pages--search engines.

a second important feature available in magallanes is its capacity to build up workflows by automatic and efficient analysis of alternative pathways. these pathways go from an initial type of data to a desired output by using a set of available and compatible services. rigorous evaluations of different algorithm implementations lead to an efficient breath-first pruning algorithm from target to source followed by a backtracking procedure.

the magallanes client integrates different sources of resource metadata outperforming current client search capabilities. moreover, the inclusion of indirect information from the available web page links usually embedded in description metadata extends the scope of discovery.

various implementations of magallanes client have been deployed to demonstrate the potential utility of the magallanes' api. different variations of the same client  demonstrate the versatility of the software library. several of these clients are being used in real installations such as the national institute of bioinformatics  and acgt-eu project, to exploit biomoby-based repositories. web services from the ebi are also among the available service catalogues.

although many interesting improvements are already planned for magallanes, the current approach is an important step in the integrated exploitation of web services, with user interaction and client usability in the application domain of bioinformatics.

availability and requirements
•project name: magallanes.

•project home page: 

•operating system: platform independent.

•programming language: java.

•other requirements: java  <dig> or higher.

•license: free software.

•any restrictions to use by non-academics: none.

authors' contributions
jr designed and programmed magallanes. jk tested the application and helped with the manuscript. ots conceived of the study, participated in its design and coordination and helped to draft the manuscript. all authors have read, participated in, and approved the final manuscript.

supplementary material
additional file 1
supplementary material. information about magallanes' api, complex workflows composition, how to extend the search space, detailed algorithm trace, did you mean algorithm example, repositories' statistics and published workflows discovery.

click here for file

 acknowledgements
this work has been partially financed by the national institute for bioinformatics  a platform of genoma-españa and the eu project "advancing clinico-genomic trials on cancer" .
