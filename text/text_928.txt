BACKGROUND
current computational methods for prediction of protein function rely to a large extent on predictions based on the amino acid sequence similarity with proteins having known functions. the accuracy of such predictions depends on how much information about function is embedded in the sequence similarity and on how well the computational methods are able to extract that information. other computational methods for prediction of protein function include structural similarity comparisons and molecular dynamics simulations . although these latter methods are powerful and may in general offer important 3d mechanistic explanations of interaction and function, they require access to protein 3d structure. computational determination of a 3d structure is well known to be resource demanding, error prone, and generally requires prior knowledge, such as the 3d structure of a homologous protein. this bottleneck makes it important to develop new methods for prediction of protein function when a 3d model is not available.

recently a new bioinformatic approach to prediction of protein function called proteochemometrics was introduced that has several useful features  <cit> . in proteochemometrics the physico-chemical properties of the interacting molecules are used to characterize protein interaction and classify the proteins into different categories using multivariate statistical techniques. one major strength of proteochemometrics is that the results are obtained directly from real interaction measurement data and do not require access to any 3d protein structure model to provide quite specific information about interaction.

proteochemometrics has its roots in chemometrics, the subfield of chemistry associated with statistical planning, modelling and analysis of chemical experiments  <cit> . in particular it is closely related to quantitative-structure activity relationship  modelling, a branch of chemometrics used in computer based drug discovery. modern computer based drug discovery is based on modelling interactions between small drug candidates  and proteins. the standard approach is to predict the affinity of a ligand by means of numerical calculations from first principles using molecular dynamics or quantum mechanics. qsar modelling is an alternative approach where experimental observations are used to design a multivariate regression model.

with xi denoting descriptor i among d different descriptors and y denoting the biological activity,  qsar modelling aims at a linear multivariate model

y = wt x = w <dig> + w1x <dig> + w2x <dig> + ... + wdxd     

where w = t are the regression coefficients and x = t. the activity y, may be the binding affinity to a receptor but may also be any biological activity e.g., the growth inhibition of cancer cells. in comparison with numerical calculations from first principles and similar approaches, the main advantages of qsar modelling are that it does not require access to the molecular details of the biological subsystem of interest and that information can be obtained directly from relatively cheap measurements.

the joint perturbation of both the ligand and protein in proteochemometrics yields additional information about the different combinations of ligand and protein properties for an interaction than can be obtained in conventional qsar modelling where only the ligand is perturbed. in recent years, various other bioinformatic modifications of conventional qsar modelling have been reported. these include simultaneous modifications of the ligand and the chemical environment  in which the interaction take place  <cit> , and three-dimensional qsar modelling of protein-protein interactions that directly yields valuable stereo-chemical information  <cit> .

although proteochemometrics has already proven to be an useful methodology for improved understanding of bio-chemical interactions directly from measurement data, the quantitative proteochemometric models designed so far have not yet been subject to a detailed and unbiased statistical evaluation.

a key issue in this evaluation is the problem of overfitting. since the number of ligand and protein properties available is usually very large, to avoid overfitting, one has to constrain the fitting of the regression coefficients. for example, in ridge regression  <cit> , a penalty parameter is tuned based on data to avoid overfitting, and in partial least squares  regression  <cit>  the overfitting is controlled by tuning the number of latent variables employed. in proteochemometrics as well as in many qsar studies reported, the performance estimates reported are obtained as follows: 1) perform a k-fold cv for different regression parameters, 2) select the parameter value that yields the largest estimated performance value, and 3) report the most promising model found and the associated performance estimate. although this procedure may seem intuitive and may yield predictive models  the performance estimates obtained in this way may be heavily biased. interestingly, this problem was recently addressed in the context of conventional qsar modelling  <cit> , and has also been discussed in earlier work, see  <cit> .

as an alternative or complement to constraining the regression coefficients, one may also reduce the variance by means of variable subset selection . in qsar modelling, many algorithms for vss have been proposed based on various methodologies, for example optimal experimental design  <cit> , sequential refinements  <cit> , and global optimization  <cit> . vss is used to exclude variables that are not important for the response variable, in the process of model building. variables that are not important receive low weights in both a pls and a ridge regression model, however if the fraction of unimportant variables is very large  <cit>  the overall predictive power of the model is reduced. in this case vss can improve the predictivity. however, if the fraction of unimportant variables is rather small, the quality of the model will not be improved by using vss, it might on the contrary be slightly reduced. however, the interpretability of the model will in both cases be improved.

although many of the advanced algorithms for vss are powerful, they are all computationally demanding. therefore, in order to keep the computing time down in our use of the double loop cross validation procedure employed here, conceptually and computationally simple algorithms for vss were used instead of the more advanced ones presented, e.g. in  <cit> . most likely, the more advanced algorithms would yield more reliable models with even higher predictive power than for the models designed here. however, the main issue of interest in this paper is to confirm the potential of proteochemometrics.

in previous reported proteochemometrics modelling, all available examples were used in the vss. these were split into k separate parts and a conventional k-fold cross validation  was performed. however, since all available examples were used, there were no longer any completely independent test examples available for model evaluation. interestingly, this problem of introducing an optimistic selection bias via vss was recently also pointed out in the supervised classification of gene expression microarray data  <cit> .

in this paper we employ a procedure that can be used to perform unbiased statistical evaluations of proteochemometric and other qsar modelling approaches. an overview of this so-called double loop cv procedure is presented in figure  <dig>  and may be regarded as a refinement of the current practice in proteochemometrics in the following respects:

 <dig>  k <dig> different variable subset selections are performed, one for each step in the outer cv loop. this avoids optimistic selection bias.

 <dig>  the best performance estimates  found in the inner loop by means of k2-fold cv are computed, but not reported as the model's performance estimate. this avoids the second optimistic selection bias mentioned above.

 <dig>  an unbiased performance estimate, p <dig>  is computed in the outer loop and is reported as the performance estimate of the modelling approach defined by the procedure in the inner loop . p <dig> is the result of different models that are designed and selected in the inner loop. it reflects the performance that one should expect on average.

 <dig>  repeated k1-fold cvs which yield information about the robustness in the results obtained .

in addition to these refinements, this work also demonstrates the potential of fast and straight forward alternative methods for vss and regression in the inner loop. moreover, it indicates that the performance estimates reported by certain software packages for qsar may be quite misleading.

we reanalyzed two of the largest proteochemometric data sets yet reported. the first data set is presented in  <cit>  and contains information about the interactions between  <dig> combinations of  <dig> different compounds with  <dig> different human and rat amine g-protein coupled receptors. in total, there are  <dig> ×  <dig> =  <dig> possible interactions and the basic task is to fill in the 483- <dig> =  <dig> missing values. the second data is presented in  <cit>  and contains information about the binding of  <dig> different compounds  to  <dig> human α1-adrenoreceptor variants . as for the first data set, there are not interaction data available for all the  <dig> ×  <dig> =  <dig> possible interactions, but for  <dig>  see  <cit>  for more details about this data set. below these two data sets are referred to as the amine data set and the alpha data set, respectively.

RESULTS
software
computer programs were written in matlab  to integrate the double loop procedure in figure  <dig> with robust multivariate linear regression using partial least squares  regression and ridge regression. these programs also contained two simple and fast methods for variable ranking called corrfilter and plsfilter. for details, see the methods section.

parameters
the joint variable selection and pls tuning performed in the inner k2-fold cv loop was performed with k <dig> =  <dig>  the different values of nd  evaluated were  <dig>   <dig>   <dig>   <dig>   <dig>  ...,  <dig>   <dig>   <dig>  ...,  <dig> for the amine dataset and  <dig>   <dig>   <dig>   <dig>   <dig>  ...,  <dig>   <dig>   <dig> for the alpha data set. the values of nl considered were either the number of latent variables  <dig>   <dig>  ...,  <dig>  for both the amine and alpha data set or the degree of rr penalty  <dig>   <dig> ,  <dig> , ...,  <dig>  for the amine data set and  <dig>   <dig>   <dig>   <dig>   <dig> for the alpha data set. in the outer k1-fold cv loop, the same number of splits  was used as in the inner loop. on the global level, the complete experiments were performed  <dig> times using different random partitions of the complete data sets.

unbiased predictive power
in table  <dig> a summary of the results from  <dig> randomly selected partitions of the complete amine data set are presented in the form of the mean values and standard deviations obtained. the number of molecular descriptors and latent variables selected in the inner loop are summarized in table  <dig>  the average values of the biased q <dig> obtained in the inner loops look quite promising for the plsfilter method  and is even higher than the value reported in earlier studies  <cit> . however, the corresponding unbiased performance estimate p <dig> is much smaller . the q <dig> values for the models obtained after variable selection using corrfilter are significantly lower than when using plsfilter, but the p <dig> values are almost on the same level for the two variable selection methods when no variable selection at all is used. corrfilter reduces the number of descriptors to about one third of the initial number, but corrfilter still selects more than twice as many descriptors than plsfilter . since the main reason for variable selection is improving the interpretation of the model by reducing the number of descriptors, this indicates that one should select plsfilter instead of corrfilter. in figure  <dig>  the external  predictions used to compute p <dig> for the pls model using plsfilter show that there is useful predictive power, but only for examples with mid-range pki values. the model has poor predictability for both low and high pki values, indicating that the standard design procedure used in the inner cv loop does not always yield reliable models. this confirms earlier findings  <cit> , that maximization of the unbiased performance estimate q <dig> is not always reliable, and also indicates that unreliable designs can be detected by means of the outer cv loop employed in this work.

the estimated performances of the models for the alpha data sets are presented in table  <dig>  here both the q <dig> and p <dig> values are high and the difference between the two measures is smaller than for the amine data set. this indicates a lower level of overfitting. the number of descriptors selected in the variable selection is much lower for the alpha data set  than for the amine data set. both the high p <dig> values and the display of the external prediction in figure  <dig> show that the models have high predictive power. also, the predictive power is significantly higher after variable selection than without. this is an example when variable selection does not only improve the model interpretability, but also the the model predictivity. the above results indicate, for example, that a combination of plsfilter, pls regression and model selection by maximization of q <dig> produces individual models with predictive power. the relative standard deviation of the predictive power is less than 5% for the two data sets considered. however, the number of variables selected has a relative standard deviation of 455/ <dig> = 25% and 69/ <dig> = 35%, respectively. moreover, the standard deviation in the number of latent variables  is approximately one  or 15%. in conclusion, the individual models are quite different but essentially all of them yield useful predictions.

comparisons to other programs
to verify that our computations using matlab are comparable to computations by other programs, such as simca, golpe and unscrambler, models without variable selection were performed with all four approaches. in the comparison we have compared q <dig> values for models based on all descriptors built with pls using between one and ten latent variables for the amine data set. all the q <dig> values were computed using the leave out cv method with five random groups and are presented in figure  <dig> 

remarkably, the q <dig> values obtained with simca  <dig>  are much higher than for the other methods. this is due to the fact that simca does not use the standard formula  to compute q <dig> , for some general information see  <cit> .

robustness and interpretability
to study the robustness and interpretability of the set of models obtained using the two data sets considered, two different levels of information were computed and presented. the first level of information consists of two histograms displaying, for each ligand block ), and for each transmembrane region , how often different kinds of descriptors are selected. the histograms are based on the  <dig> observations obtained in the 5-fold cross validation performed  <dig> times using different, randomly selected, partitions of the data set. the descriptors are divided into receptor descriptors and ligand descriptors that are further subdivided into original descriptors, cross term descriptors, and absolute valued cross terms. in figure  <dig>  hit rates for receptor and ligand blocks in the  <dig> different 5-fold cross validations performed are presented, both for the original, the cross term, and the absolute valued cross term descriptors. in figure  <dig> a and 5b, the results for the amine data set are presented. figure  <dig> c and 5d displays the corresponding results for the alpha data set.

the second level of information displays the average and the standard deviation of the contribution of the different tm regions in the receptors for creation of receptor-ligand affinity according to the  <dig> different models designed. the contributions were calculated exactly as described in  <cit>  for a single model, and then the average and standard deviation were calculated. therefore, the results presented in figure  <dig> corresponds to fig.  <dig>  in  <cit>  where the results for a single proteochemometric model were presented. as before, for each tm region, the contributions to the affinity by different ligands is displayed, this time the variance  information is added. the top of figure  <dig> shows the detailed contributions of tm regions to affinity, for each possible combination of ligand and receptor, according to the  <dig> different proteochemometric models designed using the amine data set, pls regression and the plsfilter vss algorithm. the bottom part displays the corresponding results for the alpha data set when employing ridge regression and the plsfilter vss algorithm.

discussion
in summary, the results reported here confirm earlier reports on the potential of proteochemometrics modelling for prediction of biological activity. it is interesting to note that the vss did increase the predictivity of the models for the alpha data set, but not for the amine data set. the vss for the alpha data set did also reduce the number of variables to approximately 4% of the original variables, while for the amine data set 15–38% of the variables remained after vss. this indicates that for models where many variables receive low weights  the vss can significantly improve the model, whereas for a data set like the amine data set, with less low weighted variables, the vss does not improve the model even though it can improve the interpret ability of the model.

the basic goal of proteochemometric modelling is to obtain a single quantitative model that can predict biological activities accurately and which can be easily interpreted biochemically. in this context, it is important to stress that the only role of the outer loop employed in this work is to obtain unbiased estimates of the average performance of the design procedure considered in the inner loop. the additional random splitting of the data sets is used on top of this to gain information about the stability in the performance estimates. thus, for procedures in the inner loop that yield small variances around a high average of p <dig>  there is statistical support that a single design will yield useful predictions. in order for a single model to be chemically interpretable as well, all the models selected in the inner loop should yield approximately the same number  of variables and the constraints on the regression coefficients  in all models should be approximately equal. with this in mind, the results presented in this work indicate that it is possible to design single proteochemometric models with predictive power based on the two data sets considered but that there is a relatively large variance  in the variables selected and the constraints put on the regression coefficients. this indicates that although a single proteochemometric model would be useful for predictions, a detailed chemical analysis of such a model would be uncertain. more reliable information should be gained from a careful joint analysis of all the models  selected in the inner loops of the different evaluations performed. for example, as briefly discussed in  <cit> , the variables selected with the highest frequency should be of great interest. thus, systematic and simultaneous biochemical analyses of all the models selected in the inner loops of this kind are required. for illustrative purposes of the complexity and potential of such analyses, here we have presented frequency distributions indicating which variable blocks are selected frequently in the two modelling problems considered.

moreover, we have also presented estimates of the variability  in estimating the contributions to affinity, between various combinations of ligands and receptors, from different transmembrane regions. in figure  <dig> , histograms display how often different kinds of descriptors were selected in the  <dig> models designed for the amine data set. one conclusions is that for corrfilter, the absolute valued cross terms are selected three times as often as ordinary cross terms. another conclusion is that for plsfilter, fewer variables are selected and there is no obvious preference for one of the two types of cross terms. for the alpha data set it is obvious from figure  <dig> c that only tm <dig> and tm <dig> are important to the model. from figure  <dig> c and 6d, it is also obvious that the cross terms  are selected less often than the ordinary descriptors.

although earlier findings have been confirmed, one should note that there are a number of differences between the present and earlier studies which makes detailed comparisons difficult: 1) in earlier work different variable subset selection methods were employed and in some attempts there were no subset selection at all. 2) the normalization and use of nonlinear cross terms differ between the present and earlier studies of the alpha data set. 3) the limited forms of external predictions attempted earlier e.g., in  <cit>  are not directly comparable with the present results. 4) different software packages have been employed for model selection and performance estimation.

CONCLUSIONS
this work employs a methodology for unbiased statistical evaluation of proteochemometric modelling and confirms that proteochemometric modelling is a new bioinformatic methodology of great potential. the statistical evaluation performed on two of the largest proteochemometric data sets yet reported indicates that detailed chemical analyses of single proteochemometric models may be unreliable and that a systematic analysis of the set of different proteochemometric models produced in the statistical evaluation should yield more reliable information. finally, although this work has focused on confirming the potential of proteochemometrics, the kind of systematic unbiased performance estimation employed here is of course also relevant for closely related areas of bioinformatics like microarray gene expression analysis and protein classification.

