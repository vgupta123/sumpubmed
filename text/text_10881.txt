BACKGROUND
the field of bioinformatics has gained momentum over the past two decades. the wealth and heterogeneity of biological data available in the public domain provides rich resources for bioinformaticians, biologists, chemists and clinicians. the latest nucleic acids research databases special issue  <cit>  recorded over  <dig> biological data resources. this reflects the technological advancements in the field and the complexity of the field that has developed many sub-disciplines.

users of biological data resources, especially in clinical environments and whenever biological processes are modelled, need an integration of those specialised resources. if we consider that many data resources have a collection of analysis tools associated with them, then the huge potential for combining these tools for analysis is traded off for the technical complexity. many of these tools run on a command line, where there are differences in formats and the semantics of files to be exchanged between them. the installation and maintenance of these tools can introduce large overheads to data analysis. alternative web-based interfaces are provided for many tools. this reduces the installation overhead for the user and the propensity for version heterogeneity, but it also often reduces the function of the tools. users can submit large batch jobs, but they are limited to numbers or time.

workflows provide the possibility of programmatic access to distributed tools and resources. the taverna workflow workbench  <cit>  allows users to chain together pre-made building blocks of web services and other services to build complex analysis pipelines. the myexperiment  <cit>  site allows for the sharing of these workflows, helping people share informatics methods in the community. this way, researchers are helped to avoid reinventing approaches and prefer reusing established templates if readily available. this works nicely for public data and web services.

a challenge remains to integrate public services with the freedom to execute command line applications. the incorporation of command-line applications in such workflows is a more recent development, not yet widely adopted. we will explain why we think that this is not surprising and how the availability of linux distributions contributes to a wider acceptance of that principle.

applications may be executed either locally or remotely, possibly as a service, in a queuing system or a computational grid. the community has become excellent at sharing data, and similarly excellent in sharing the code  of applications, but there was yet no environment allowing for the integration of it all – either locally or remotely. the researcher demands the highest responsiveness from his applications and needs to protect precious data, especially in pharmaceutical or clinical environments. both are in strong favour of local installations for applications and public data resources.

the traditional installation of open source software requires its compilation from source code. the skills and interest needed for packaging software are different from software development. more complex software will require the prior installation of other software, a recursive process. this is laborious and often technically challenging because of the differences between platforms and version incompatibilities. a workflow demanding the availability of a particular application on a local machine will thus be considered non-executable, unless the given application has already been installed.

once installation is successful, scientists are reluctant to update a working environment, so computer networks quickly become heterogeneous and not maintainable with multiple users. when different members of a community work with different versions of the same software, it becomes more difficult to collaborate.

on linux, several distributions are preparing packages with readily usable software that are up- and down-gradable, and allow libraries of several versions to be installed. debian  <cit> , founded in  <dig>  is one of the oldest distributions. it was for many years the only distribution that was managed as a democratic society, to which users could upload packages and vote for their destiny. the community of computational biologists should team up with the community behind the linux distributions and extend the it infrastructure respectively. this frees considerable resources for research labs and may be crucial for many smaller groups.

the remaining challenge was to connect those tools with the data and web services. one can certainly use various forms of direct access to download complete databases and experience a complete working environment. however, the prospect of integration with taverna and therefore being able to access web services and workflows directly seemed particularly attractive. a use-case plugin was developed  <cit>  to allow regular applications to be interpreted as services by taverna and can be accessed in a similar way to web services.

the dependence of the biological research community on computational and data services will increase over the upcoming years. the strong computational demands of the services and the increase in complexity of the in silico research fosters the collaboration of individuals from many sites.

implementation
debian-med
the debian project is an open society of enthusiasts from around the globe who collaborate on maintaining an operating system based on the linux and freebsd kernels. programs are distributed as binary packages ready for use, built on debian's network of autobuilding  <cit>  machines, from source code that is further annotated and uploaded as packages by individuals. debian supports today's most prominent platforms, thus rendering them available from mobiles to supercomputers and for all common processors. packages invite feedback from users with the bug tracking system  <cit> .

around  <dig>  users have allowed the counting of their installations via debian's popularity-contest initiative  <cit> , started in  <dig>  separately counted are installations of packages that were forwarded to derived distributions  <cit> . the most prominent of these is ubuntu  <cit> , for which more than  <dig>  million users are reporting. packages are described verbosely and are translated to many languages  <cit> . more formally they may be selected by manual assignment of terms from a controlled vocabulary  <cit> .
                

technical constraints for the packaging are laid out in the debian policy document  <cit> . changes to it are discussed on the project's mailing lists and may be subject to voting by contributors to the distribution. the ubuntu linux distribution adopts the debian packages for their own software "universe" and several packages are co-maintained by developers of both distributions.

everybody can volunteer to maintain a package in debian. there is no general exclusion of any software, as long as it is redistributable. for auto-building on many platforms, the source code and the libraries that it needs must be available. to allow for improvements, one must be allowed to edit the code. all this is more formally specified in the debian free software guidelines .

for complex suites, packagers have an option to share their effort with the community  <cit> . such group maintenance was made possible with the advent of debian blends  <cit> . a blend is a thematic flavouring of the distribution, with its own respective portal, source code management and mailing lists. this helps keep the debian community together and attracts new users. the blend's infrastructure displays the available software together with bibliographic and registration information. users can thus cite and register, i.e. help the upstream developers to perpetuate their projects by showing their impact.

integration of the command line in workflows
packages may come with multiple executables. even with only a single binary, the exact parameter setting would not necessarily be clear from any given context. the respective packages' manual pages list a series of the most common contexts and describe the inputs, outputs and command line arguments for these. one also sees the need to sequentially execute multiple binaries on a single command line to complete one piece of elementary work.

to facilitate automatic execution, we introduced a description of such shell-based workflow elements in a machine-readable equivalent of a man page. we refer to it as “usecases” since there may be multiple ways a program is used with very different parameter sets, but there should be only a single description for a tool to achieve a particular purpose. a single debian package can therefore provide the application for multiple such use cases. we have created a web-based repository to browse through a database of such use cases and offer a form to add new ones. internal use is also well supported by allowing the user to download and locally maintain the collection of custom workflow elements as an xml file.

to bring the shell-executed workflow elements together with web services, we have developed a plugin to taverna. the plugin reads the usecase descriptions from a url, and then controls the execution of the described programs on the user's behalf. every use case is therefore available to be embedded in taverna's regular workflows and mime types can be set to help with visualisation of final or intermediate results.

RESULTS
this section summarises the results of the debian med  <cit>  <cit>  blend for medical- and bioinformatics. it is unique in its form, since all its packages are intrinsic parts of debian. the blend does not extend the distribution, it is simply a filtered view of what is available for the life sciences. its packages help descendent efforts like biolinux  <cit> .

debian med provides a web portal interface, allowing users to browse packages of interest and select specific tasks from those packages. for bioinformatics, tasks of particular interest include “imaging”, “statistics”, and “bio” or “bio-dev”. packages with an emphasis on computation have also been collected under the task “cloud”. every such task is associated with a regular debian metapackage, allowing the easy installation of a whole set of packages at one time.

focus on software for medicine and bioinformatics
at the time of writing, debian med offered  <dig> packages for bioinformatics in accordance to the debian free software guidelines' demands on free software, another  <dig> do not fulfil this criterion. a further  <dig> packages are co-maintained and may be built locally, but have never been requested as an integral part of the distribution. these preliminary packages are made available to help the community to work with programs with a license that does not allow the redistribution of their source code, or for which the packaging is not yet completed. another  <dig> packages are for developing new applications, like those for the bio*  <cit>  programming libraries.

even though the focus is on the bioinformatics packages, to package particular software often means first packaging many more general libraries that are needed as a build- or runtime-dependency. those additional packages are not listed with debian med but will become one of  <dig>  other regular debian packages.

the popularity contest  infrastructures of debian and ubuntu presents lower bounds for the number of installations of any package in the distribution. common tools like t-coffee are reported as installed  <dig> times in debian and  <dig> times in ubuntu. the r package qtl  <cit>  was reported  <dig> times, outrunning even gromacs  <cit>   <dig> , emboss  <cit>   <dig>  and autodock  <cit>   <dig> . this information is updated on a weekly basis and is thus a valuable extension of the information from the download statistics at the original site for grant writers. for debian, popcon also offers a graphical development of those numbers over time – showing increasing absolute numbers. when normalised to the total number of users, most packages show a decrease between the years  <dig> and  <dig>  which may reflect increasing success of debian outside the academic world, followed by a stabilisation or a modest increase.

however, when comparing those numbers with the average size of scientific conferences, then the immediate outreach of the packaging seems equivalent. it becomes obvious that the user base of ubuntu for the scientific packages is around  <dig> times larger than for debian. the total number of reporting users is about  <dig> times larger and given the practical equivalence of the two distribution for bioinformatics research, ubuntu with its more frequent release cycles has the latest versions integrated sooner with its releases. this may have contributed to the observation of debian's lower numbers for the development packages ) or latest expensive technologies like for next generation sequencing data analysis with maq  <cit>   or velvet  <cit>  . this trend may change since debian now introduced official rebuilds of latest-version packages against the stable distribution, termed "backports"  <cit>  and will be interesting to observe.

the distributions' installation statistics help the communication with the software developers. it is the first almost immediate feedback that they get from the distribution. some developers, like those of autodock and ballview  <cit> , follow the distribution's bug reports directly or otherwise contribute to the presentation of their software in the distribution. they may be formulating a description of their package or upload new versions of their software directly to the distributions.

integration of tools and data
with an increasing number of packages available, the interaction between those tools becomes more important for analysis. at first sight, this addresses the establishment of regular workflows in bioinformatics that are expected to compose analysis pipelines from tools from many packages. a second issue is the challenge to extend the concept of packaging to the distribution of the exact same version of public databases on different sites. this is only an issue if the public instance cannot be used via the web directly, e.g. to avoid the risk of someone monitoring the query or because of a higher latency. the data are likely to update more frequently than the tools interacting with them, and the tolerance towards working solely with official releases varies between sites. the sharing of input between multiple applications is ongoing work, for which many bioinformatics groups around the globe have provided solutions independently. to tap into that wealth of experiences and use it to share the effort to maintain the infrastructure is our impetus.

debian med is currently investigating the acceptance of a perl script, named getdata  <cit> , that knows how to install some of the most common databases in bioinformatics and how to do the post-processing to have that data readily available for emboss, blast  <cit>  or other associated tools. the information is stored in a hash table, and getdata knows how to extend that hash from files in a configuration folder. this is a preparation for database packages that would only consist of such tiny configuration files that depend on getdata and recommend a series of tools that know how to read the data. by some automatic mechanism in that package's post-install script or by a manual trigger by the user, the database would then be downloaded and indexed with no further human intervention.

however, there are limits in disk space and, more importantly, in the bandwidth available. now that we are storing data of complete genomes for comparison, a shared environment should be identified, potentially in the cloud.

cloud computing for sharing a virtual computer
cloud computing provides scalable, flexible access to larger computer processing power and storage. for academics, there are free cloud resources available . commercially, resources such as amazon provide an on-demand service.

in clouds, virtual machines are used to instantly reconstitute one particular environment from a selection of images offered, or self-assembled, on one or more computers. this brings the advantage of letting scientists work on copies of the same system and further increases the reproducibility of workflows composed of local debian med services


                cloud computing offers an increased flexibility for workflow infrastructure in many ways. rather than providing the service directly, a community may decide to offer a cloud image of the service. the images can be adapted for local needs or be run redundantly to avoid single points of failure. multiple cloud instances can also be organised to be accessed together and share the work, e.g. in a manner known from high performance computing by installing a batch system  <cit> .

in debian med, all packages are versioned and have set dependencies. this allows users to exactly specify a production working environment, which assists in the sharing of debian med images as well as in the collection of accurate provenance information. it is equally straightforward to adapt a public specification for local needs or to publicly discuss or automate the setup. tools like live-builder.debian.net can create system images that are usable either on local computers or on large cloud services  <cit> . in addition, packages of arbitrary previous versions  <cit>  can be substituted from the default ones, to assemble precise combinations of tools and thus generate system images that reproduce environments used for past projects.

the amazon public data initiative  <cit>  already exemplifies how bioinformatics databases of public interest can be shared in a cloud environment. debian images can access and work with such data. and the community can, by a manual effort or by using getdata, perform a similar initiative, albeit on a smaller scale.

workflows
the usecase plugin provides a new service type in the taverna workbench. usecase services present a mechanism for incorporating arbitrary command line tools into workflows. the command-line tools can be configured to run locally, or on remote machines accessed via secure credentials such as ssh or grid certificates. multiple invocations of a service can be achieved by calling the same tool on a number of computing nodes at the same time, thus allowing faster running of workflows over a distributed network of machines.

workflow developers can therefore write and test workflows locally on small amounts of data. a simple change of configuration can then run the workflow on a grid or cloud on much larger data sets.

taverna workflows can include not only tools within a packaged distribution, but also calls to other services such as wsdl operations, queries of a biomart database or invocations of r scripts. the workflows can be uploaded to the myexperiment website to be shared, either publicly or with specific groups of people. figure  <dig> presents an example workflow for the structural alignment program mustang  <cit> . with the tool boxshade  <cit>  to pretty-print sequence alignments. the original manuscript on the taverna plugin  <cit>  explains the syntax to describe the binaries in debian or other linux distribution to appear as workflow elements.
this graphical representation of a workflow in taverna was taken directly from workflow  <dig> at the myexperiment site. it requests two pdb ids to retrieve the respective protein structure and present it to the mustang protein structure alignment tool. its result files are directly offered also as outputs of the workflow. it is only one alignment that is passed throught the tool boxshade for pretty-printing.

the development of workflows goes hand in hand with the sharing of such expertise via the myexperiment website. with the usecase plugin, workflows could be composed entirely of packages from debian med. this renders the workflows more accessible for commercial in-house adoption, where data or methods are sensitive.

CONCLUSIONS
the dynamics of the elements presented here, i.e. the debian med distribution, the cloud infrastructure and the workflow suite, form a symbiosis towards a readily usable infrastructure for performing and sharing biological research and services. any command line application can be integrated with any workflow in taverna via local or remote execution. sharing workflows, services and data sources is not trivial, but can be managed successfully with the infrastructure presented here.

cloud computing infrastructures offer a new way of working. expensive local computing infrastructures are not required if researchers can have access to cloud computing resources on demand. small research groups can therefore tap into resources that were previously not accessible to them  <cit> .

authors' contributions
sm and aw drafted the manuscript, kw, cp, hk and sm revised it. hk implemented the use case interface to taverna that was now adopted by aw. sm implemented the web interface to use cases and with cp prepared the getdata script. at, cp, dp and sm are all frequent contributors to the packaging for the debian med initiative.

availability and requirements
• project name: debian med

• project home page:http://debian-med.alioth.debian.org

• operating systems: debian linux and derivatives

• programming language: no restrictions

• license: any that allow the free redistribution of the software

• any restrictions to use by non-academics: no , none added by debian

list of abbreviations
aws: amazon web services; blast: basic local alignment search tool; dfsg: debian free software guidelines; emboss: european molecular biology open software suite; mime: multipurpose internet mail extensions; popcon: popularity contest; url: uniform resource locator; wsdl: web service definition language;  xml: extensible markup language;

competing interests
the authors declare that they have no competing interests.

