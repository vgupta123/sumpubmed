BACKGROUND
biomedical named entity recognition  is a fundamental technique for literature mining. it can be applied to various applications, such as disease-treatment relation extraction  <cit> , gene list creation  <cit> , semantic relation extraction between concepts in a molecular biology ontology  <cit> , and gene function identification  <cit> . bio-ner influences the performance of applications both in precision and recall. however, choosing an appropriate assessment method may depend on the context in which a bio-ner text mining system is used.

in the early days, large corpora were not available and most researchers had to build small, ad-hoc corpora to evaluate their systems. the main drawbacks of such evaluations are:  developers and annotators usually belong to the same group.  the corpora are usually not available to other researchers.  only few or limited kinds of proteins and genes are annotated.  the corpora do not have explicit tagging guidelines so that such evaluations lack objectivity since it may be easy to design a system to fit a certain corpus; also, it is difficult to perform cross-system comparisons due to the specificities between different datasets and domains.

in recent years, the genia  <cit> , genetag  <cit> , and iprolink  <cit>  corpora were released. the first two are most frequently used in bio-ner evaluation. therefore, we describe them in detail below.

genia consists of  <dig>  medline abstracts retrieved using the mesh search terms human, blood cell and transcription factor. these abstracts are then annotated manually. based on the genia ontology, the genia corpus classifies each biomedical ne according to its chemical structure, which is usually independent of the biological context in which it appears. details on ne taxonomy in genia can be found in . in genia, the number of ne classes is  <dig> 

genetag includes a total of  <dig>  sentences selected from medline abstracts. on average, one medline abstract comprises ten sentences; therefore, genetag amounts to about  <dig>  abstracts. to ensure the heterogeneity of the genetag corpus, the medline sentences were first scored for term similarity to documents with known gene names. then  <dig>  high-scoring sentences and  <dig>  low-scoring sentences were chosen at random. these sentences were run through abgene  <cit>  and manually annotated with gene and protein names by biochemistry, genetics and molecular biology experts.  <dig>  of these sentences were used in the biocreative- <dig> task 1a  <cit> . genetag has only one class per se, as it groups proteins, dnas, and rnas into the newgene class. based on gene names found in genbank, this newgene class includes domains, complexes, subunits and promoters   <cit> .

these three corpora provide relatively generous amounts of training data to perform objective evaluation metrics on machine-learning-based systems. besides, they have explicit tagging guidelines and are freely available. as a consequence, most ner systems are evaluated on these corpora using "exact match" as the primary matching criterion. judged according to the exact-match criterion, a candidate ne can only be counted as a match if both its boundaries and its class fully coincide with an annotated ne.

however, requiring exact matches may not be necessary in every bio-ner application. for example, in a relation extraction application, the goal may be to determine if a sentence mentions a gene and its function. if this is done using patterns with wildcards  <cit> , exact ne boundaries are usually unnecessary; only the existence of an ne matters.

using exact match, we encounter another major problem with nes whose boundaries have many variations. in real-world cases, certain nes may be tagged in several ways, having either flexible boundaries or fitting into multiple categories . this problem of annotation inconsistency is intrinsic to the annotation of any corpus whether by human or machine. inter-annotator agreement for bio-nes is between 87%  <cit>  and 89%  <cit> . inconsistencies also exist in the work of single annotators. there are several reasons for these discrepancies: in some cases, the tagging guidelines do not define how a certain phrase should be tagged; in others, multiple tagging is allowed; and in still others, human errors occur. ner systems that come across one of these irregularly tagged nes may correctly identify a valid ne without exactly matching the corresponding human-annotated ne. using exact match in these cases will not only generate false positives but also false negatives, effectively producing two errors where none exist – for example, simultaneously missing the target ne and tagging a partial match.

some previous works have also addressed these problems. seki and mostafa  <cit>  proposed four matching criteria in addition to exact match. they compared kex  <cit>  and yapex  <cit>  systems, which were developed in  <dig>  these were both rule-based systems and their evaluation dataset contained only  <dig> abstracts. using different matching criteria, the authors analyzed these two systems' characteristics. although, this evaluation was performed with rule-based systems on a small dataset, researchers can still use these matching criteria to refine a bio-ner system or post-process the results. for example, fukada et al.  <cit>  calculated the core-term recognition performance in the first stage and used this to refine their system in the second stage. the jnlpba  <dig> bio-ner task  <cit>  included left match and right match in its evaluation tool to provide alternative perspectives on bio-ner evaluation. the biocreative  <dig> task  <cit> , on the other hand, allowed several possible correct annotations, of which ner systems need only match one. their multiple-tagging scheme tags all possible meaningful boundaries of an ne and can provide more versatile assessment of bio-ner systems. however, most annotated corpora do not adopt this annotation scheme.

in this paper, we present a comprehensive survey of commonly used matching criteria, explain their potential uses and definitions, and compare their characteristics. then, we implement these matching criteria in the jnlpba's evaluation tool, and re-evaluate the top four systems that took part in the jnlpba  <dig> task  <cit> . our evaluation indicates that right match and left match may be appropriate alternatives to the combination of exact match and multiple tagging. finally, we demonstrate that users can flexibly define their own relaxed criteria according to their needs.

ner evaluation metrics
most ner evaluation systems use precision, recall, and f-score to measure performance. precision is the number of nes a system correctly detected divided by the total number of nes identified by the system. recall is the number of nes a system correctly detected divided by the total number of nes contained in the input text. f-score combines these two into a single score and is defined in the following equation:

f−score=2×precision×recallprecision+recall
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwggbgrcqghsislcqwgzbwccqwgjbwycqwgvbwbcqwgybgccqwglbqzcqgh9aqpdawcaaqaaiabikdayiabgena0kabdchawjabdkhayjabdwgaljabdogajjabdmgapjabdohazjabdmgapjabd+gavjabd6gaujabgena0kabdkhayjabdwgaljabdogajjabdggahjabdygasjabdygasbqaaiabdchawjabdkhayjabdwgaljabdogajjabdmgapjabdohazjabdmgapjabd+gavjabd6gaujabgucariabdkhayjabdwgaljabdogajjabdggahjabdygasjabdygasbaaaaa@65b5@

since the boundaries and categories of bio-nes are often ambiguous, various matching criteria and class-merging strategies have been used for bio-ner system evaluation. they are summarized below.

matching criteria for bioner
one might think that only exact matches can be considered correct. however, in many applications, finding pieces of information is better than finding nothing at all. for example, in response to the question, "alzheimer's disease is caused by mutation in which gene?" a system extracts "ps1" as opposed to "ps <dig> gene," then we could consider giving full marks or at least partial marks to that system. furthermore, exact match may not reflect the true performance of a system. for example, say we need to identify a protein-protein interaction from the phrase "il- <dig> activates p21ras proteins." a human expert annotates the phrase as "<protein> il- <dig> </protein> interacts with the <protein> p21ras proteins </protein >." a bio-ner system, meanwhile, comes up with the annotation: "<protein> il- <dig> </protein> interacts with the <protein> p21ras </protein> proteins." if the bio-ner system's result is compared to the human-annotated phrase, we see a boundary matching error in <protein> p21ras </protein>, where "protein" is not included in the tag. however, for a relation extraction system, this error could be acceptable, since the system has correctly identified "p21ras" as a protein, and this information is adequate to extract the relationship "il- <dig> interacts with p21ras." similarly, "the p21ras protein" or "the p21ras" could also be considered correct. in this situation, we need an alternative matching criterion other than exact match.

other examples of inconsistent boundary tagging can often be found in annotated corpora, where one can find the same descriptive adjectives annotated as parts of following nes in some cases but not in others. in fact, it may even be hard for biologists to decide whether descriptive adjectives such as "normal" or "activated" should be considered part of entity names. take "human" for example. in the jnlpba task, of the  <dig> times it occurred before or at the beginning of an ne in the training data, it was annotated as a part of the ne only  <dig> times. but in the gold standard test data, it was included  <dig> times out of  <dig>  <cit> . this irregularity confuses bio-ner systems and weakens the reliability of evaluation based on the exact-match criterion. to provide alternative evaluation perspectives, researchers have developed a variety of rules that relax matching criteria to different degrees. they are listed below alongside their definitions. in the following section we discuss their potential to assess the performance of bio-ner systems.

boundary relaxation
left match
if the left boundary matches exactly, the tagged ne is scored as a match. using this rule, certain errors may be judged as correct, such as p21ras above. in these cases, the rightmost head words which represent the ne's category are skipped. this error may be acceptable in relation extraction and go-id assignment applications  <cit>  since the category matches, and the core term is successfully identified.

right match
if the right boundary matches exactly, the tagged ne is judged as correct. applying this rule, errors due to missing or including preceding adjectives can be scored as correct. for example, in the sentence "we identified a putative stat binding site in the promoter region of p <dig> " regardless of whether "putative stat" or "stat" is tagged as a protein, it is counted as correct using right-match criterion.

left/right match
if a tagged ne exactly matches either boundary of the human-annotated ne, the hit is counted as a match.

partial match  <cit> 
a detected ne is counted as correct when any fragment composing the ne is correctly detected.

approximate match  <cit> 
according to the approximate-match criterion, a tagged ne must be a substring of the human-annotated ne or vice versa. left- and right-match criteria can be considered more restricted subsets of the approximate-match criterion.

name part/fragment match  <cit> 
using fragment match, each token in an ne is considered separately. this criterion is used to assess what percentage of an ne has been correctly recognized. using this criterion, longer terms have more weight than shorter terms. it provides an alternative method of assessment, since most other criteria only treat a hit as right or wrong and cannot measure degrees of matching.

core-term match  <cit> 
to be considered correct, machine-annotated nes must contain a core term. core terms identify an ne. they often have unique orthographical features, such as capital letters, numerical figures, and special symbols – for example, "sap" from the ne "p <dig> sap kinase." some nes are composed of a core term, head noun/phrase on the right, and adjective on the left. this criterion is useful for bio-ner systems that extract core terms as the first step. however, because it is only possible to identify core terms by hand, few systems use this matching criterion.

multiple-tagging match
the genetag annotation guidelines were designed to define true positive gene/protein names in terms of their specificity and semantics  <cit> . each sentence in genetag is annotated with acceptable alternatives to the gene/protein names it contains. such annotation scheme allows for partial matching with specificity and semantic constraints, and is a more meaningful measure of the performance of an ner system than unrestricted partial matching. for instance, the specificity constraint allows entities such as tat dna sequence, but not dna sequence. semantic constraints are rules stating that the tagged entity must represent its true meaning in the context of the sentence. these constraints are geared towards multiword entities, especially ones that include numbers, letters and acronyms. for example, the name in the phrase "rabies immunoglobulin " requires rabies because rig implies that the gene mentioned in this sentence refers to the rabies immunoglobulin, and not just any immunoglobulin. unlike traditional exact match, ner systems identifying any alternative tagging of an ne are scored as correct.

categorical relaxation
certain categories can be merged to reduce the ambiguity of ne classification.

merging protein, dna, and rna
in the biocreative task 1a, dna, rna and protein names are placed in the same category. no distinctions are made between genes, proteins, rna, domains, complexes, sequences, fusion proteins, etc. while finer-grained classification is possible, it is not really feasible in practice because even human annotators agree only 77% of the time on protein, gene and rna classification  <cit> . also, although machine learning systems can correctly categorize proteins, genes and rnas with 78–84% accuracy  <cit> , most bio-ner systems do not make these distinctions because the information is not required. hatzivassiloglou et al.  <cit>  also found that their machine learning algorithms did not perform well against a human baseline model, suggesting that either the human model was correct, and the decreased performance was due to classification difficulty, or the machine-learning programs were penalized for being more consistent than the human model. either way, the inclusion of these categories in the gold standard would be a significant additional source of ambiguity.

merging cell line and cell type
due to the large number of inconsistent or ambiguous annotations of these two categories in biomedical corpora, combining "cell type" and "cell line" into the single class "cell" can effectively reduce the number of annotation errors detected. in addition, in some biomedical applications, such as semantic role labelling, we only want to extract the location of molecular events in a text, no matter whether they are in vitro  or in vivo .

RESULTS
experiment design
in our experiments, we aim to examine the impact of different tagging constraints on the measurement of system performance. we conduct four experiments described below.

experiment  <dig> examines the impact of multiple tagging. we compare the performance of systems that participated in both the jnlpba task and the biocreative task by simulating the biocreative classification scheme on the jnlpba dataset. in biocreative, no distinctions are made between genes, proteins, rna, domains, complexes, sequences, fusion proteins, etc  <cit> . we re-annotate protein, dna and rna in the jnlpba dataset as newgene. then we run the best four jnlpba systems , and the best four biocreative systems , on the two datasets. of these systems, three  took part in both tasks, while two  entered only one task each but used similar features and the same crf package – set biocreative and per jnlpba. we use the exact match criterion for system performance evaluation.

since most annotated corpora do not adopt the multiple-tagging approach used in biocreative, in experiment  <dig>  we test a range of other matching criteria comparable to multiple tagging. we randomly sample five subsets from jnlpba's test set , each containing  <dig>  sentences. the experiment is conducted on the top four bio-ner systems in the jnlpba task. seven matching criteria are compared, i.e., exact, left, right, left/right, partial, approximate, and fragment match.

experiment  <dig> aims to demonstrate how users can customize assessment of bio-ner to suit their specific applications. consider the following scenario. a biologist may need to refer to all related macromolecules  of a cell, when reading a biomedical article. in this application, there is no need to distinguish among protein, dna, and rna. in addition, it is quite difficult to distinguish cell line from cell type. for example, in the jnlpba test set, "t cell" is classified as cell line  <dig> times while  <dig> times as cell type. therefore, we propose the relaxed match criterion that merges protein, dna, and rna classes into macromolecule  <cit> , and cell-line and cell-type into cell. furthermore, the annotation of the left boundary tend to be more inconsistent due to the ambiguity of including the left boundary words as part of an ne. for example, "normal" is tagged as the left boundary word of an ne  <dig> times while  <dig> times not in the jnlpba test set. as a result, we choose the right match in our relaxed match. we conduct an additional experiment on the same four bio-ner systems as in experiment  <dig> 

in experiment  <dig>  we test five basic matching criteria from the strictest to the loosest – exact, left/right, approximate, partial, and uncategorized partial – on the jnlpba corpus by using the top  <dig> jnlpba system, zhou et al.'s  <cit>  in order to find the percentage of false negatives.

experiment 1
the results are reported in table  <dig> and table  <dig>  which show the precision rates, recall rates, and f-scores for newgene on the jnlpba and biocreative datasets.

using the exact-match criterion, f-scores are lower on the jnlpba than on biocreative. we believe that the disparity in performance between the two is due to our evaluation system's lack of alternative-tagging rules, which the original biocreative task employs. the other reason is that, though our modification of the jnlpba task's evaluation system using biocreative's unified protein/dna/rna class improved scores, it is not a perfect re-creation of biocreative's newgene class, because the two tasks' original class definitions differ. since some users may need biocreative's evaluation scheme but do not have the same annotation scheme as biocreative in their corpora, we are looking for a matching criterion that can be an appropriate alternative to biocreative's method.

experiment 2
to find the closest matching criterion to biocreative's, we apply two statistical methods: hypothesis testing and correlation coefficient. for each matching criterion, we test if its hypothesis  – whether its average f-score is equal to biocreative's – can be accepted at confidence level α =  <dig> . for hypothesis testing, we perform the third experiment. since the experiment is conducted on four bio-ner systems and each system runs on five datasets, we end up with  <dig> samples from the jnlpba dataset. in table  <dig>  we show the evaluation results for all seven matching criteria . only the left-match and right-match criteria pass this hypothesis test. right match, which has the largest correlation coefficient between its f-score and the f-score evaluated on biocreative , is, therefore, the criterion closest to biocreative's multiple tagging method.

*the condition for accepting h <dig> is t ≤ t <dig> ≤ t, where t = - <dig>  and t =  <dig> .

it can be observed from table  <dig> that left match is second to right match by only a slight margin. this finding can be explained by the following observation: while most nes have head nouns either on their right or left boundaries, more have them on the right. right match and left match are both potential alternatives to biocreative's multiple-tagging method. if we want to avoid overestimating performance of systems that are only adept at tagging right boundaries, we can simultaneously double check using left or exact match. it is also worth mentioning that left/right match is inferior to both right match and left match in terms of hypothesis testing results and correlation coefficient. this may imply that boundary conditions can only be loosened to a certain extent.

experiment 3
we compare the best systems' performance evaluated using the traditional five-class exact-match criterion and the proposed relaxed-match criterion. the results are shown in table  <dig>  where we only report the best rates among the four systems. using the relaxed-match criterion, the best bio-ner system can achieve  <dig> %,  <dig> %,  <dig> % in precision, recall, and f-score respectively, which more realistically reflect the performance of this specific application.

experiment 4
the results are reported in table  <dig>  which shows the precision rates, recall rates and f-scores that the best jnlpba participant system achieved under five basic matching criteria – exact, left/right, approximate, partial, and uncategorized partial – as described at the beginning of the methods section. the maximum recall rates are  <dig> % for exact,  <dig> % for left/right,  <dig> % for approximate, and  <dig> % for partial match. the last column reports the performance gains achieved by ignoring all ne classification. the maximum recall rate of  <dig> % represents a gain of  <dig> % over partial match. the remaining  <dig> % represents nes that have been completely missed. of course, if no matching parts exist, it is impossible for post-processing to fix the boundary errors by extending core terms  <cit> . according to our analysis, many complete-miss errors were due to inconsistent annotation in the jnlpba  <dig> training data, especially untagged cell-line nes. we found that many instances of "t cell," "peripheral blood neutrophil," and "nk cell" were not tagged as cell line. this inconsistency confuses machine learning algorithms, leading to a large number of false negatives. how to uncover false negatives remains a challenging issue in bio-ner.

CONCLUSIONS
we present a survey of commonly used matching criteria, explain their potential uses and definitions, and compare their characteristics. we also compare two popular bio-ner evaluation methods – those used by biocreative and the jnlpba. from our statistical tests , we find that right match has no significant difference and has the highest correlation coefficient with biocreative's multiple-tagging criterion. in addition, left match is also comparable to biocreative's multiple-tagging criterion though slightly inferior to right match. in biomedical applications where strict exact-boundary match is not necessary, right or left match may be sufficient and useful. researchers can use both criteria to evaluate bio-ner systems and use the results for further analysis to improve the systems. our study shows that, evaluated with the relaxed match, the best system's performance is above 80%. users can flexibly define their own relaxed criterion according to their application context.

