BACKGROUND
systems biology is a recent approach to life sciences that poses unprecedented computational challenges  <cit> . these challenges are rooted in the way systems biology projects are approached and are specifically the following. investigations frequently span multiple years and are carried out by multiple cooperating laboratories with complementary, interdisciplinary skills. large and complex datasets measuring different properties of the system studied are acquired and need to be analyzed and included in theoretical models. the long duration of such projects necessitates the ability to cope with investigators leaving the lab during the project and handing over the data to their successors. furthermore, data analysts and mathematical modellers also need to get access to all or to a subset of the data for down-stream analysis. the fact that more and more diverse datasets are acquired and analyzed over a longer period of time by a large number of researchers within one project needs to be reflected in the way data are stored, managed, indexed, queried and integrated. until recently, biologists used file management systems on their personal computers to manage their results, a strategy that is ill-suited for the requirements of data sharing in systems biology research and which does not scale to the data output of modern instruments used for data acquisition.

for a long time, data management in life sciences has been considered a side-aspect of data analysis. domain-specific analysis procedures are often closely tied in with some sort of data management. examples of this approach for genomics are mimas, mimir, gnomex, and biological networks  <dig>   <cit> , examples for proteomics include cpas, prism, 2ddb, cpfp, maspectras  <dig> and prohits  <cit> . the advantage of this approach is that it can give researchers a "turn-key solution" if the analysis fits their needs. on the other hand, it may create high migration effort when the data analysis requirements are changing during a project in a way which is not supported by the analysis platform. we argue that this is a common case for long-running projects that use cutting edge technologies.

an alternative approach is to use generic workflow managers and implement the analysis procedures as nodes of the workflow manager. today, many good solutions for scientific workflow automation are available  <cit> . in order to scale up, workflows can be parallelized to run on grids using middleware systems like p-grade  <cit> . the promise of workflow managers is that analysis procedures of heterogeneous provenance can be combined into one workflow and can be run repeatedly and reproducibly, providing the flexibility that the tightly integrated systems are lacking. there are, however, some pitfalls also for this approach. first of all, for each preexisting analysis procedure there is the need to write â€žglue code" to integrate it as a node into the particular workflow system. furthermore, as of today no commonly accepted workflow definition language and representation exists, so that workflows created for different workflow managers are mutually incompatible. however, work for creating a common workflow format that is independent of a particular system and that can be supported by many bioinformatics workflow managers is ongoing and expected to become available with openms/topp  <dig>  . finally, employing a workflow manager without giving consideration to data sources and data sinks used by the workflow usually leads to solutions where data management is reduced to file-based data storage. this will create the usual problems of file-based solutions regarding scalability and the need for tedious manual processes for data provisioning and sharing.

while data analysis methods and algorithms change quickly to follow the latest advances in the field, a data management system needs to provide a stable basis for measurement data and analysis workflows for many years. data from multiple generations of measurement devices and many versions of analysis pipelines need to be kept available for reference, comparison and integration. during this time, the appropriate data representations  may change as community standards evolve and more sophisticated analysis methods become available. a software ecosystem that is well prepared for such challenges will consist of many software components that fulfill a specific function optimally for a given research project and that can be integrated in the overall pipeline with as little effort as possible, ideally by 'plug-and-play'. to that end, the components need to be easy to integrate with each other and must not try to offer all functionality by themselves but rather provide interfaces to other components which offer the required functionality. due to the fluid nature of the research process, there is no  a formal component model. thus, the components of such a software ecosystem have to be written with a focus on ease of integration. we argue that a critical component is a domain-specific data management system which acts as a hub for the data being measured and analyzed, providing the data sources and sinks to the various analysis components, as well as being a repository for data sharing. one of its functions is to bridge the gap between incompatible workflow systems used for different analysis phases. to this end, it has to be independent of any particular measurement or analysis pipeline, independent of a particular data representation and able to evolve its data representations and support multiple representations in parallel. to support the necessary integration with analysis pipelines, it needs to be open for extension and integration. furthermore, it needs to offer strong data provenance tracking capabilities, advanced search functionality and a scalable backbone for data ingestion, provisioning and life-cycle management. an early representative of some of these ideas is the genome database acedb , as it tries to address the flexibility in modelling biological objects  <cit> . recent systems that focus on flexible data and metadata ingestion, sharing and querying, as well as integration with analysis systems include sbeams  <cit> , b-fabric  <cit>  and seek  <cit> .

here we report how we addressed the challenges described above by developing openbis . openbis consists of a core and a framework layer offering services that other software can use via programming interfaces to extend openbis to specific data types and workflows. the core of openbis contains a powerful and flexible annotation service that includes support for data provenance tracking, it has a project structure for experiments and data, handles user authentication and authorization as well as data import and export, and supports data format migration and data archiving. emphasis has been given to provide access to all services of the openbis framework via application programming interfaces  that allow integrating openbis with other software tools for primary and secondary data analysis, visualization and automation. we have also implemented web-based graphical user interfaces based on those apis. to customize the system for specific technologies such as next generation sequencing, high content screening  <cit> , quantitative imaging or mass spectrometric methods used in proteomics and metabolomics research we have adapted openbis to the specific data structures and annotations, mechanisms for data uploading, visualization and support for domain specific queries. different laboratories involved in systemsx.ch, the swiss initiative in quantitative systems biology, and also other large research consortia are productively using openbis. we expect that its use will grow as systems biology approaches are becoming more prevalent in the life sciences.

implementation
guiding principles
openbis has been designed to be open for extension and integration. it uses extensively the design principle of "loose coupling", which is essential for open software systems. the concept has originally been introduced in behavioral science  <cit>  and since then successfully applied to the design of software systems  <cit> . the loose coupling approach requires a system to use an interface of another system that requires the least knowledge about it. loose coupling makes a system agnostic about the operating environment it runs in or components it interacts with and thus allows software developers with different backgrounds and skills to join forces in building larger systems.

one of the consequences of focusing on integration is that in openbis, all metadata are configurable so that they can match the metadata provided or required by other systems that openbis is integrated with. the same holds true for the logic of data ingestion procedures. a consequence of following the loose coupling approach is that we focus on programming interfaces for openbis that can be used from different programming languages and operating systems. for "pull" operations, for example querying for the datasets of an experiment, openbis provides access to data by either a file system, a web-service or a relational database. as not all programmers are familiar with using web-services, we also wrote some command-line tools which encapsulate one or more web-service calls and can be used as a proxy for the web-service. for "push" operations, for example notifying a workflow system that bulk data have become available on a cluster file system, openbis uses the concept of message passing. as message channels, we use either files on a file system , a relational database, or e-mails.

a principle we followed to enable good scalability is the separation of bulk data  and metadata. as data ingestion, provisioning and querying on high-throughput and high-content data can involve expensive operations in terms of cpu usage, i/o and network bandwidth, these operations should not be performed on the same system that maintains and provides access to the metadata which deals with comparably small amounts of data and thus can easily scale up to many experiments and samples. in openbis, we introduced the application server , short as, for dealing with metadata including data provenance data and one or more data store servers, short dss, that deal with bulk data. all communication of the as with a dss is asynchronous.

a principle openbis adheres to in order to provide good traceability of results is that of immutable datasets: a dataset, once created, has a unique identifier and cannot be changed anymore. when data are derived from one or more datasets, a new child dataset of the original dataset is created rather than changing the original dataset. this way, a dataset has a well-defined meaning, for example when referring to it as an input of a data workflow.

furthermore openbis is independent of a particular data representation. whatever representation or format a data measurement or analysis pipeline delivers can be stored in openbis without the need of changing or transforming it. if a second representation is required or an index of the data needs to be built for later querying, this can be done when running the data ingestion procedure and provided as a separate representation. we encourage users to create a new dataset for each representation and put different representations of the same data into one "container dataset", which acts as a proxy to all representations. to make a container dataset transparent to the user, openbis will show its file system view by fusing the files of all contained datasets. new representations can be added at a later time by transforming the old representation, creating a new dataset from it and adding that to the container dataset, a process which can be run as a background thread of a dss.

openbis overview
the main goal of openbis is to support biological research data workflows in the realm of systems biology from the source, for example the microscope, the mass spectrometer or the sequencer, to data publication. this is accomplished by creating an information system which extracts data and metadata from the measurement instruments, integrates with data processing pipelines and visualization and analysis tools, and allows users to interact with the system in multiple ways . for the sake of brevity, we will use the term data processing whenever we refer to primary data analysis and data analysis when we refer to secondary analysis. data from the different measurement platforms are uploaded into the system using dropboxes. a dropbox corresponds to a directory on the file system that is monitored for incoming files and directories. copying data in one of the configured dropboxes triggers the etl  process which extracts metadata, creates datasets annotated with metadata in the database and adds datasets to the data store. integration with other computational tools is achieved via apis. users can access openbis to organize, search, share or publish data and metadata. data can be exported to excel. bulk export is provided by a web interface or command line tool. overall, openbis is a flexible system allowing users to access raw and processed data from different downstream applications and track data provenance, independent of the instrument or software vendor, independent of geographical location of the instrument for a longer period of time. due to its architecture openbis is very scalable, i.e. it can store and make available large amounts of data and is configured for distributed storage.

logical structure of the data repository
to organize the stored data in a logical and transparent manner and to manage access privileges we created a hierarchical structure of the data. this was accomplished by using the entities i) data space, ii) project, iii) experiment, iv) sample and v) dataset . permission rules are applied at the highest level, the data space. these rules determine what a user is allowed to see and which operations he is able to perform. a data space contains projects that group one or more related experiments. an experiment is an empirical approach to acquiring data. the experiment in turn typically contains at least one sample, the object being measured or observed in an experiment. a sample can have one or more datasets associated with it, where a dataset is a set of files containing the values of the actually measured or derived data. for example, the same microtiter plate  being read twice by a microscope will result in two different datasets which both can be associated with a single sample. such a hierarchical structure is a prerequisite for organizing larger collections of experimental datasets efficiently. for example, both raw data and processed data can be stored as individual datasets which in turn are linked to each other and to a sample or an experiment. the hierarchical data structure further is capable of establishing parent-child relationships between samples and between datasets.

metadata
to describe the structure and meaning of data with all its contents in context, and to facilitate finding and using acquired data, metadata are stored and indexed along with raw and analyzed data. metadata in openbis can be either structured or unstructured and can be provided and searched for in a flexible way. we support unstructured metadata through file attachments at the level of the project, experiment or sample, very much like an e-mail attachment. alternatively, structured metadata allow researchers to provide custom properties  to experiments, samples and datasets that are defined as a set of property types which are stored in the database in a consistent and transparent manner . a property type can be regarded as a field definition with a name, label, description and value where the value is constrained to be a known data type such as an integer, a floating point number, a date, a boolean value, a free-text field, a hyperlink, a term of a previously defined controlled vocabulary or, for semi-structured metadata, an xml structure. property types are subsequently assigned at the level of the experiment, the sample or the dataset by linking them with a given experiment type, sample type or dataset type. properties can be dynamic, i.e. they can be evaluated using a script based on other metadata in the system. due to this flexible annotation service, openbis provides a generic mechanism for establishing metadata models that are often specific to an experimental venture. to effectively handle research specific annotations or attributes, openbis uses an entity-attribute-value  data model  <cit> .

data repository
in order to efficiently store and retrieve the large amounts of data associated with systems biology studies and to cope with many different measurement technologies, openbis employs a hybrid data repository, featuring a relational database management system   <cit>  for index information, metadata and selected results, and a flat-file data store for bulk data. by default, raw data and result data are stored in the data store which is a managed flat-file store consisting of one file system , or multiple file systems . experiment-related metadata are always stored in the rdbms, file metadata may be stored there optionally. the hybrid data repository combines flexibility and scalability. the system is flexible because it stores the metadata model reflectively in the database, i.e. the model is data itself. therefore, the metadata model can be chosen for each individual use case whether data are best represented in the original data files or, for time-critical queries, in an optimized file format or the rdbms. the system is scalable because i) the core database  is kept at a reasonably small size, ii) the queries which regularly deliver very large result lists are optimized, and iii) there is support for segmented and distributed storage of bulk data.

openbis deployment model
the basic openbis deployment consists of two servers, the application server  and the data store server  . generally speaking, the as manages the metadata and links to the data while the dss manages the data itself. to this end, the as sets up and uses an rdbms to persist users, authorization information, entities like data spaces and samples and their metadata, as well as index information about all datasets. the dss manages the datasets in the data store, which is not writable by other parts of the system. different types of clients like e.g. a web browser, a graphical matlab client, or a command-line client can access openbis through the as and dss.

one as can manage multiple dsses. this is a feature that is important e.g. if the data are acquired in multiple, distributed laboratories with a local dss at each site. a dss, by design, is not just an 'internet-ready file server'. it can run custom queries on the data it stores, it may  employ its own rdbms for result data that it will populate automatically on data upload. furthermore the system can be extended to use additional companion servers for data transport, such as the datamover or cifex for data import and export .

client
in order to interact with the system, users need to use a client. out of the box, openbis provides a graphical web application, a set of command-line tools, which communicate with openbis over the network, and a few knime  <cit>  nodes. in many cases, users will use a client that is specific to their domain and use case. to support use of the openbis backend from such clients, we provide apis that are accessible over a tcp/ip network. clients come in many forms and flavors and the writers of the client software are inventive in how they integrate. the methods we see being used beside the apis include setting a link to a view of the openbis web application , calling an openbis command-line tool, writing to a dropbox directory and let the dropbox script write response files, or accessing custom relational databases that are populated by a dropbox script.

application server
the as is the central point of access for clients who wish to interact with the system. it stores metadata about samples, experiments, and datasets in an rdbms. the actual data is stored and managed by the dss; the as mediates access to one or several dsss.

the as itself can be decomposed into several layers: the presentation layer, the domain layer, and the data access layer . the presentation layer serves the html and javascript for the web client. the domain layer provides services for clients wishing to interact with the business objects . these services are not only used by the web client, but by command-line programs and the dss as well. example services include queries such as samples matching particular criteria, and registering entities in the database. the data access layer is the interface between the as and the database; it is private to the as and not accessible from the outside.

data store server
the data that comprise a dataset, be it images, spectra, analysis results or features, are managed by and accessed through the dss. these data are stored in the data store. the dss is responsible for querying, reporting on and creating visualizations of datasets.

in flat storage mode, the data store will consist of one share which in general will be served by a file server . in segmented storage mode, one data store has multiple data shares, where different file servers could serve each share. the dss manages the fill state of the shares in a background process.

the dss consists of a presentation layer, domain layer, and a data access layer . the presentation layer is responsible for displaying data. in the most basic cases, this may just be a matter of providing access to an image via a url for use in an html "img" tag. slightly more involved, but still quite simple, would be a case of rendering a tab-separated-value file in the form of an html table. more complex cases require a deeper understanding of the data. examples of more complex presentation logic in the dss include generation of data visualizations, such as heat maps, and compositing images representing multichannel data. a part of the dss presentation layer is the data set uploader, a tool for web-based batch upload of data sets. to avoid the web browser's size limitation of  <dig> gb for uploaded files, this tool is implemented in java and executed via java webstart.

the domain layer of the dss is responsible for providing access to the data and understanding their format and semantics. also in the domain layer are the etl threads which are responsible for registering new datasets when they arrive. in the typical use of the dss, an etl thread watches a folder for activity and registers the data when a new file or folder appears. the data access layer of the dss consists in the generic case of a filesystem abstraction. this abstraction allows the dss to use file metadata from an rdbms to speed up search and list operations on slow storage, transparent access to container files  and fusing the directory listings from multiple datasets being in the same dataset container. this layer may furthermore provide access to additional data sources like bespoke relational databases.

cifex
beside openbis, we have developed other applications that can be used in concert with openbis: cifex and datamover. cifex, the cisd file exchanger, is a web-based application for exchanging and transferring large files. as the web browser limits uploads to  <dig> gb, cifex has a java webstart-based gui tool to overcome this limitation. it supports resuming up- and downloads and performs checksumming to ensure file integrity. we have integrated cifex with openbis to enable downloads of large or huge datasets that are available in openbis. if a user has direct access to the file system of the data store, cifex is not required, but many users do not have that access and furthermore cifex can be used to share a selected subset of data when a user to share with does not have an openbis account. for this use case, e-mailing datasets as attachments is often not an option anymore as they can be multiple gigabytes in size. for these use cases, cifex functions as a conduit for transferring data to and from openbis .

we run an instance of cifex for users of eth zurich. the cifex system is open source and available for download  <cit> .

datamover
in order to reliably transfer data from one location to another the datamover program was developed. it can use either a locally mounted file share, an ssh tunnel or an rsync server for the actual transfer. one example is transferring data produced by a measurement device attached to a dedicated measuring computer . the files produced are often very large, the storage space available on the measuring computer is limited, and network access to the storage repository may not be entirely reliable at all times. datamover detects new files on the measuring computer and transfers them to another location such as the data center or compute cluster. it thus robustly and automatically handles the potential pitfalls that may occur in the data transfer process such as network interruptions or storage devices reaching their capacity limit.

authentication and authorization
each user must login to gain access to openbis. openbis features a plugin-based system for authenticating users. authentication plugins are available for ldap v <dig> , crowd, an identity management system written by atlassian  <cit> , and a simple file-based approach . within the application access permissions are defined by assigning roles to users or groups of users. a role determines  what a user or group of users is allowed to see and what operations a user can perform. a user having administrative permissions can grant access rights to other users or user groups to facilitate sharing of data. as a result of the established authentication and authorization service access to secured information can be restricted to particular users based on the role each particular authenticated user has in the system. thus, unauthorized users are prevented from gaining access to secured resources.

usage and applications of openbis
users can interact with openbis in several ways. the software framework provides a set of commonly required core functionalities, such as the management of data and metadata, the search capability for and the sharing of data, the import and export of data, and the interfacing with the system per se. these core functionalities are stable and part of the openbis download package and thus can be used 'out-of-the-box' by biologists. integration of openbis as a new system in a specific research workflow very often is achieved by implementing additional custom functionalities, such as visualization, publication and bespoke query operations of the data. implementing such custom functionalities requires a trained software developer and usually a tight interaction between the scientist and the software developer. for an overview of openbis functionalities see additional file  <dig> 

data and metadata management
to organize data in a logical and safe way researchers apply a hierarchical data structure using the openbis entities . for each research group a data space is created to which permission and visibility rules apply that are defined by the access privileges. access permissions can be defined by assigning for example an observer  or user role  to individual users or user groups to keep all data restricted to all members of a project. following publication, researchers can easily change permission settings to make their data and metadata publicly available. within each data space each research group can organize its projects. a project typically contains the experiments which contain all the datasets and metadata files created from the individual samples run by a particular measurement instrument. to provide custom annotations to datasets, samples or experiments, sets of property types are defined which in turn are grouped and assigned at the level of the experiment, sample or dataset type, respectively. using these capabilities, custom metadata sample submission and update forms can be created.

data search, export and sharing
to share data in collaborative projects the dataset search functionality in combination with the application of table filters and the bulk export functionality using cifex can be utilized. datasets are searched based on dataset code, dataset type , file type , or any other metadata stored for the dataset, sample or experiment . applying a filter on it allows the user to find the relevant entries. standard filters are applied to filter the content of a table column. numerical filters allow numerical comparison operations. finally selected data can be conveniently exported through the spreadsheet export functionality or downloaded by the export data functionality. this will redirect the user to the file exchange system cifex. this way openbis can be used to share data, for example, for down-stream analysis by mathematical modellers.

data import
for different usage modalities data are uploaded to an openbis dss using one of three methods: a dropbox, the web-based upload or a custom application using a remote api .

dropboxes are directories located either on a local file system or on a file server that are monitored for write activity by the dss. if the dss finds new data in a dropbox, it will execute an 'extract transform load'  routine configured for this dropbox. the etl routine will perform all steps that are necessary to make the new dataset known to openbis, link it with the appropriate entities , extract metadata required for searching and transform it to a pre-defined format that may be needed to facilitate querying or integration with other tools . the etl routine has many degrees of freedom on what to do with the data arriving and is designed as a set of configurable plugins. however, we recommend that the etl process also stores the original data without modification in the data store if storage space permits.

using the datamover as a companion server, dropboxes can be extended to bridge the internet, possibly crossing institutional boundaries. we found dropboxes to be a very efficient 'loose coupling' interface between systems that need to exchange bulk data. while designed and optimized for unattended operation, some laboratories prefer dropboxes even for users uploading data manually.

the web-based upload tool can be triggered from the web client. for the actual data upload step, it will redirect the user either to the data set uploader program or to a cifex server.

the remote api is a set of java classes that provides a streaming-based programming model to upload any set of files which constitute a dataset. compared to dropboxes, it allows for a tighter integration of the application using it, can provide immediate feedback  and supports upload over any wan that allows for https traffic without additional network configuration. as the use of java as a programming environment can be limiting in some cases, openbis also contains a command line tool for data upload that uses the remote api internally. this tool has been used to integrate the openbis upload with e.g. labview- or script-based data providers.

interfacing: web interface, command-line tools, apis
to ease the interaction with openbis, different interfaces optimized for particular use cases have been implemented: a web application, command-line tools, and java language-based remote apis . the web application runs on all modern web browsers, making it accessible to users of the popular desktop operating systems  without prior software installation. it is designed for interactive use  and provides amenities such as automatic defaulting of values and immediate validation of user input.

the command-line tools are interfaces to openbis designed for interactive use by expert users and automated use in scripting environments. they lack the data input conveniences of the graphical interface, but in exchange support precise and concise data input. the rigid and terse interaction style of these tools make them well suited for employment in the integration of openbis into automated processing pipelines or into custom-built user interfaces, particularly in cases where the apis cannot be used.

the openbis apis come in two flavors: a java service facade and a json-rpc web service. the java service facade provide programmers with classes and methods for integrating openbis into their applications. by using these apis, a laboratory or research group can seamlessly embed openbis into their own, custom-developed, applications, such as workflow or visualization tools. the apis, though implemented in java, are not limited to use in java-based applications. many modern data processing environments, such as matlab and r, also provide functionality for utilizing java apis. in fact, we offer an api variant of the hcs-api for matlab, designed to support the idioms of the matlab language. the json-rpc web service, while not as high-level as the java service facade, is usable in all computing environments that support web standards. in particular it enables usage of the openbis backend from any web application. we found that small web apps that specifically cover one use case are often more appealing to casual users than the built-in openbis web application that combines all functionality but may be perceived as complex.

data visualization and publication
to visualize data it is common to integrate specific visualization tools with the openbis framework. a simple way is the implementation of a reporting plugin for the display of informative html files like, e.g. those generated by an illumina hiseq  <dig> sequencer's software. more sophisticated visualization tools are for example the plate viewer displaying the layout of a multititer plate or the visualization of feature vectors as graphs, or the protein viewer  displaying protein identifications and quantifications as implemented for customizations for hcs and proteomics, respectively. integration with an already existing viewer such as an image intensity-rescaling tool are implemented using an api. regarding the publication of data, simplified views of the whole system can be easily implemented by reconfiguring some parts of the openbis graphical web client. from the different views listed it is evident that based on the openbis framework almost any kind of viewer can be implemented for any data type of interest. it is also not uncommon for a web app to use both the json-rpc web service interface and custom openbis views where applicable.

data query
in openbis it is straightforward to amend a database with result data and metadata when a new dataset is added, and thus build up specialized result databases. such result databases can be used along with the core database to create sophisticated queries  that go far beyond the metadata search functionality described above, store them in the openbis core database and make them available to biologists who would not be able to formulate such queries themselves. queries have names and descriptions, can be parameterized and the query results are shown in a tabular web view and available to users for interactive searching and exporting to excel.

RESULTS
applications
openbis is a framework for information systems that can be adapted for individual use cases. this is done by developing extensions to the base system and by configuring the system including the extensions to the operating environment.

developing a new extension for openbis requires close communication between the researchers using the system and the software developers. we find that an iterative process between users and developers is required to get a refined solution. the users describe what they need to do with the system and give their input on how they would like it to work, the developers work out a design that they can implement. focusing on the most important aspects of an extension first and quickly providing an implementation allows the users testing their workflow with the real software in order to refine their requirements.

to illustrate how extensions are developed, two rather different examples of established applications are presented. our setup of openbis for a next generation sequencing core facility represents a straightforward measurement workflow while the setup for proteomics data is part of a complex data processing workflow employed in a large distributed research project. the next generation sequencing extension took about  <dig> person months to develop, the proteomics extension in comparison about  <dig> person months.

next generation sequencing
the openbis framework was extended for a core facility to organize the daily workflow and to manage and distribute next generation sequencing data and metadata files. the current instance hosts datasets from more than  <dig> sequencing samples from over  <dig> different research groups with about  <dig> users.

we customized the openbis framework to provide the core facility with a sample submission and tracking system . users can enter the sample metadata within the respective incoming data space of the corresponding research group. once the biological sample arrives at the core facility a facility member moves it from the incoming data space to the effective data space, where the metadata are only editable by the core facility personnel. the current metadata sample submission form and the sequencing services are configured for illumina sequencing. they can easily be adjusted to alternative sequencing platforms by defining new sets of property types or by using metadata annotation systems provided by the respective instruments. a tracking system provides an automated e-mail functionality establishing a communication link between the core facility and the researchers indicating the status of key steps in the experimental process, such as the completion of dna library processing and the generation of quality control and sequencing datasets.

this use case of openbis is similar to gnomex, a genomic experiment data repository and analysis project center  <cit> .

the next generation sequencing data and metadata are organized hierarchically in openbis. for each research group a data space is created where permission and visibility rules apply. within the individual data space each research group can organize its projects. a project typically contains the experiments, which contains data and metadata files derived from one sample measured in one flow lane.

three types of samples have been created for modelling the sequencing setup: first, the 'illumina flow cell' sample type encompassing all eight flow cell lanes containing all raw data as they are provided by the sequencer. the access to this sample type is restricted so that only core facility members can view and download data of a complete flow cell. through the implementation of a reporting plugin informative html files generated by the illumina pipeline software like run summary and status reports, demultiplexing statistics, error curves and intensity plots can be displayed. additionally, it is possible to compare several runs concerning different parameters such as absolute numbers of reads and sequence yield, which are based on the illumina summary report.

second, the 'illumina flow lane' sample type containing the sequence data files  and the metadata as tsv or soft files. by request from the customer additional files such as wig/bigwig, bed/bigbed, sam/bam or quality control files are provided. third, the 'illumina sequencing' sample type containing the metadata for the original and processed biological sample as they are entered into the system. the multiplexing of barcoded samples in one flow lane can be covered in openbis because the relation between the illumina flow lane and the illumina sequencing sample types is n:m. this means several illumina sequencing sample types are connected to one illumina flow lane . but also the opposite case  occurs when an illumina sequencing sample is re-sequenced. therefore, the metadata has not to be re-entered. a facility member simply adds the connection between the two samples.

to give researchers access to their sequence data and metadata they have two options. either they click on a link contained in the tracking e-mail or they log in to the openbis application. within the sample browser one can look through samples according to sample type or data space. simple text searches as well as advanced, criteria-based regular expressions searches can be performed on experiments, projects and samples.

to export data conveniently the spreadsheet export functionality was implemented. for example, a group leader wishes to get a list of all control samples, which have been sequenced in his laboratory. within the sample browser filters can be applied for any data column, subsequently data can be exported to excel. finally bulk export of data or metadata files can be performed from all dataset tables , which is supported by cifex.

as an example of the next generation sequencing extension we have uploaded the datasets of a functional genomics experiment to data space quantitative_genomics_facility of the openbis demo instance  <cit> . the experiment is comprised of chip-seq data , which led to the conclusion that polycomb group  proteins preferentially target promoter regions  <cit> . pcg proteins constitute a major chromatin based gene silencing system that is required for stable and heritable maintenance of repressed gene expression states  <cit> .

the sequencing data of one flow lane sample can consist of several datasets . these datasets are not fixed and can be defined by the openbis instance admin. in figure 5a five different datasets are shown: srf, fastq_gz, quality_check, alignment and wiggle. each of these datasets holds files and folder. by clicking on the datasets one can download the data or in case of pdfs or images like in figure 5b and 5c the files are shown within a frame, depending on the used browser. both images mentioned before are descriptive for the produced sequencing data and can give first hints on problems about the run. figure 5e is not viewable in openbis directly. the created wig file needs to be uploaded to the ucsc genome browser  <cit>  and viewed therein. although it would be extremely efficient to use a bigwig file and simply reference from the ucsc genome browser to this file stored in openbis, offering a secure access via https, username and password is currently not supported by the ucsc genome browser.

proteomics data
openbis for proteomics was developed to manage measured and derived  data generated in the context of a large interdisciplinary systems biology project of systemsx.ch. typically data are produced from different samples measured by several different types of mass spectrometers and need to be consistently processed and transparently stored. openbis serves as the central hub in this project . tandem mass spectrometry  data from multiple instruments are converted into an open standard data format   <cit>  and subsequently imported into openbis. metadata from the instruments and the conversion are automatically captured and stored in openbis. the system has currently datasets from more than  <dig> injection samples and is used by more than  <dig> users.

processing of the data is executed by p-grade  <cit> . p-grade is extensively using the openbis api, datamovers and dropboxes to facilitate data selection, raw data export and derived data import, respectively. the result of the protein identification and quantification workflow is automatically uploaded into openbis. metadata associated with the search are routinely captured and stored in association with the derived datasets.

we demonstrate the utility of the openbis proteomics extension by using a recently completed study  <cit> . streptococcus pyogenes  were grown under various conditions gaining insights in various proteins having a role in infection. the bacteria were incubated with albumin, fibrinogen, immunoglobulin g and plasma. ms/ms analysis of these differentially treated samples were compared using openbis. the data of the corresponding experiment have been uploaded to data space proteomics of the openbis demo instance  <cit> .

to organize the data in a logical and transparent way the hierarchical structure using the openbis entities was applied . within the proteomics data space the s. pyogenes project is created which in turn contains three experiment types . permission settings are configured at the data space level such that normal users have observer rights but no editing rights. the biological experiment  includes the biological samples in duplicates derived from s. pyogenes cultivated under different growth conditions  . the ms injection experiment  includes the ms injection samples and the respective datasets . the ms search experiment  includes the ms search samples and one dataset, a protxml file. in addition, metadata about the search are captured as attributes. in this example, as described above, the protxml file contains both protein identification and quantification data.

to visualize both identification and quantification data, a customized protein viewer has been implemented . researchers can individually customize the view of this table by defining personal table settings to show or to hide columns or to set the sequence of the columns. in figure 8a one can see that incubating s. pyogenes with 10% plasma induces the expression of udp-glucose 6-dehydrogenase . this protein is required for the synthesis of the hyaluronic acid capsule of s. pyogenes, which in turn is known to be a major virulence determinant involved in resistance to phagocytosis  <cit> . within this viewer, expression based custom filters as well as custom queries based on calculated columns have been integrated for the quantification data for a set of predefined sample treatment conditions. in this example the s. pyogenes protein identification and quantification data have been adjusted to the biological question of interest. the quantification data have been aggregated on the treatment condition and the mean was calculated thereof. additionally, the protein identifications have been filtered for proteins that have been induced upon plasma treatment . through the spreadsheet export functionality the quantification data of two udp-glucose associated enzymes are finally downloaded and represented as a column plot .

discussion
we have developed the open source software framework openbis. this framework is easily extensible to store and process information from any biological experiment, it can flexibly handle metadata and is scalable to very large data. we have presented two of these extensions in this study, openbis for next generation sequencing and for proteomics. we have created other versions of openbis, specialized on other experiment types. notably, we have built a version for hcs images, libraries and result data and a version for high-throughput metabolomics. we are currently working on integrating microscopy data, with a focus on light microscopy images.

there are several advantages of building extensible and customized systems on a core framework, both for the developers and the users. for the developers, building on a core framework provides a set of commonly required core functionalities "out of the box" and a template of how to add new functionality consistently and with low effort as they use well-defined extension points. from the user's perspective, in particular the data analysts and mathematical modellers who want to get data out of the system automatically or semi-automatically for down-stream analysis, access to data should be possible via custom apis like e.g. a matlab-api. core functionalities, such as the browsing and searching of data, the management of metadata or permissions, the import and export of data are handled all in a consistent and familiar way. the ease of use of the software allows the researcher to focus on the data instead of becoming acquainted with a new application again.

in our view the extensibility of the framework using loose-coupling techniques is key to be able to adapt it to the various use cases and experimental technologies quickly since many of the required technologies regarding data ingestion, integration and analysis have yet to be developed. while data acquisition, processing and particularly analysis pipelines often change rapidly to incorporate the latest improvements in method development, data and information systems are expected to provide long-term stability. thus, in our view a tight-coupling approach of data analysis and information systems is counter-productive.

the framework by itself with minor configuration work is useful for data and metadata management and provenance tracking. it answers questions like "what datasets has a given collaborator measured two years ago on the orbitrap using hek <dig> cells and what database searches did we do on it?" the effort for setting up a system for this purpose is less than 10% of the work to implement it from scratch and will work more reliable and scale better. for further data analysis, a bioinformatician traditionally will copy the datasets he is interested in to his laptop, write a script to query the relevant information from it, create a new representation and run all of it on the command line. in contrast, when investing the same work to write a reporting script for openbis, he will be able to use the result to feed analysis programs, get an ajax web page with the query result as a searchable table that can be used by biologists and will have it available as a web service call that can be used e.g. in a custom web page. the copy of the data on his laptop becomes optional.

support for sophisticated research data workflows requires integration with analysis software and requires additional work to extend the system. as openbis is well documented, it enables other laboratories to extend it themselves for their own needs  <cit> . extensions within a domain, like adding new metadata models or writing a parser for a new measurement device, can be done by a biologist. adapting the framework to a new biological domain, however, will require the know-how of a professional software engineer. the effort for creating such domain-specific extensions is significantly reduced, compared to writing the same functionality from scratch. based on our experience, the effort is less than half.

it is becoming common practice to store all data with associated metadata, especially in data intensive disciplines. this is particularly important for large collaborative and distributed projects. the openbis framework includes a flexible annotation service to store and search for a custom set of properties for experiments, samples and datasets by using property types. any kind of custom sample submission form can be created. alternatively, metadata can be imported from excel sheets or automatically extracted from log files by the etl process upon upload. once stored, metadata can be used by the system to create custom export formats like the soft format required by ncbi geo  <cit> .

as high-throughput experimental approaches and usage of multiple measurement technologies within one project and distributed projects become prevalent in modern biology, scalability of biological information systems becomes an important issue. the framework we built is scalable for four reasons. first, the hybrid data store separates metadata and index data from bulk data, keeping the core data small enough to be fast to search. second, queries in the core database are optimzed for performance. third, multiple  data stores are supported as part of one system, and each data store can use multiple data shares transparently to allow building very large repositories in a cost-effective way, and fourth, archiving datasets to cheap offline and near-online storage and bringing archived datasets back to online storage is an integrated feature of the framework.

openbis for next generation sequencing was developed for use in the quantitative genomics facility of the department of biosystems science and engineering of eth zurich to support the organization, annotation and distribution of genomics data. to make the facility more efficient and high-throughput capable, openbis is retrieving the data directly from the sequencer without raw images. the srf files and quality metrics are generated after the sequencing ended and before the data will be put into openbis. during the registration of a complete flow cell openbis links the data to the already available metadata. additional annotations to experiments, samples and datasets have been added as needed by defining new property types. export and exchange of data and metadata for collaborators is additionally supported by cifex  <cit> . we are currently working on integration of openbis with the galaxy workflow system  <cit>  that our data analysts like to use for further analysis of the sequencing data, as well as the ucsc genome browser  <cit> .

openbis for proteomics has been fully integrated into a proteomics workflow at the institute of molecular systems biology at the eth zurich. raw data and metadata are captured from the mass spectrometers and stored in the openbis database. p-grade, a web portal framework with an integrated workflow management system has been integrated via an api with openbis to support the building and execution of proteomics computational applications, such as tpp  <cit>  or openms  <cit> . data resulting from processing pipelines are automatically imported into openbis and linked to its raw data. although the current system has been designed to handle protein identification and quantification data, it can also work with protein-protein interaction data, though we are aware of some possible improvements required for this use case. the integrated setup has many advantages. researchers can perform mass spectrometric analyses much faster and with less manual steps. based on the metadata stored in openbis, the scientist can process the data externally and re-import them into openbis. search results become eventually available in openbis and can be made accessible to collaborators easily. due to the metadata stored in openbis data can be tracked and handed over to researchers following up a particular mass spectrometric study. furthermore, openbis stored data and metadata can provide the basis for subsequent data integration into public resources such as the peptideatlas  <cit> .

today, it is possible to keep data from different measurement technologies on one openbis server and to link these results based on biological samples, genes and proteins. however, framework support for this is limited. as more complex linking of result data from systems biology approaches is becoming standard, we believe that it will be a large area of further development to support this in the openbis framework.

CONCLUSIONS
openbis provides the life sciences community with a useful software framework which is under active, continuous development. multiple laboratories involved in several systemsx.ch projects, eu-funded projects as well as institutional core facilities actively use it. besides being a tool for data provenance tracking and a database for integrating with processing and analysis workflows and visualization tools, it is also used to publish data to collaborators and the scientific community, either directly or via reporting databases. while the focus of openbis is to satisfy the needs of large-scale data handling platforms, it has been used successfully by smaller laboratories as well.

availability and requirements
â€¢ project name: openbis

â€¢ demo instance: http://openbis-demo.ethz.ch

â€¢ project home page: http://www.cisd.ethz.ch/software/openbis

â€¢ documentation and download: https://wiki-bsse.ethz.ch/display/bis

â€¢ operating system: mostly platform independent, some advanced functionalities require a modern unix-like operating system like e.g. linux

â€¢ programming language: java, jython

â€¢ installation requirements: java runtime environment  <dig> , postgresql  <dig>  or  <dig> 

â€¢ license: apache software license  <dig>  

â€¢ restrictions: access to helpdesk support and services requires an agreement with the center for information sciences and databases of eth zurich. non-profit research organizations can get basic support for free.

authors' contributions
ab wrote a major part of the paper, tested and created user documentation of the software, ia, pb, fje, ke, pg and tp wrote software, cr wrote software and contributed to the manuscript, mk contributed to the manuscript and integrated the software in a next generation sequencing data pipeline and implemented extensions for this area, aq drove bioinformatics requirements for proteomics data, tested the extensions for proteomics data and provided feedback, cb contributed the section on chip-seq data to the paper and co-designed the extensions for ngs, lm designed the extensions for proteomics data the integration in proteomics data workflows, ra contributed to the structure of the paper, and provided guidance and logistical support for the project, and br designed the software, supervised the project and wrote part of the paper. all authors read and approved the final manuscript.

acknowledgements and funding
we thank johan malmstrÃ¶m and peter kunszt for critical reading of the manuscript and peter kunszt and the sybit project of systemsx.ch for creating a unique collaborative environment where openbis could be developed. the authors also thank adrian honegger, charles ramin-wright and christian ribeaud who have contributed to the project in the early development stage, and claus hultschig and sean walsh for contributing to requirements analysis in collaborative projects.

this project was funded by systemsx.ch, the swiss initiative for systems biology, the eth zurich, 3-v biosciences, inc. and by the 6th framework program of the european commission through the agron-omics and basysbio collaborative projects.

supplementary material
additional file 1
glossary. glossary of terms used specifically in openbis.

click here for file

 additional file 2
application and data store server architecture. both the as and the dss consist of three layers, the presentation layer, the domain layer and the data layer.

click here for file

 additional file 3
selected openbis functionalities. core functionalities of openbis as well as hcs, metabolomics, proteomics and next generation sequencing specific functionalities are listed in a table.

click here for file
