BACKGROUND
poxviruses are one of the largest and most complex animal virus family known  <cit> ; the subfamily chordopoxvirinae comprises at least eight genera . the orthopoxvirus variola virus  was wiped out of a human history in the 1970s, thanks to the success of vaccination. smallpox eradication counts as one of the greatest triumphs of modern medicine. before the eradication, smallpox caused from  <dig> to 35 % case-fatality rates . it is a highly contagious and strictly human disease, which caused an estimated 300– <dig> million deaths during the 20th century alone  <cit> .

human smallpox has a simian sister, monkeypox virus . this virus, also of the orthopox genus, causes an endemic disease, first recognized in africa in  <dig>  but with an outbreak in  <dig> in the united states, that was traced to imported monkeypox virus-infected west african rodents  <cit> . mpxv causes smallpox-like disease in humans; in africa the disease typically kills between  <dig> and 10 % of its human victims  <cit> . smallpox vaccination is known to lower the risk of contracting monkeypox, but there is no specific immunization against monkeypox per se. due to a declining immunity to orthopoxviruses in the general population, there is a risk that monkeypox might emerge as a significant human pathogen.

with better timeliness and accuracy, whole-genome sequencing  holds promise of revolutionizing health surveillance systems and possibly of resolving many current limitations associated with poor pathogen discrimination . genomic information offers a profound increase in the resolution of pathogen type, enabling possible identification of geographic origin and whether the agent is previously known or represents a novel mutant. mature data processing methodologies developed to address the decades-long preponderance of sanger sequencing data are not always adaptable to the characteristics of wgs sequencers, which have produced prodigious data, but only over a period of ten years  <cit> . bioinformatician is turning the computational challenges in the wgs technology to opportunities for developing new algorithms, or improving efficiencies of existing ones as to processing raw data into medically useful sequences.

wgs technology produces millions of short sequence fragments . the reads are ordered and combined into longer sequences called “contigs” . finally the contigs are ordered to produce a complete genomic sequence. this highest level ordering can often exploit known genomic reference in agents, like poxvirus, for which there are reliable genomic sequences available. of course, this entire assembly process, whether de novo or against a known reference, demands appropriate and efficient computational algorithms.

given an accurate genomic sequence, it becomes possible to deduce unique snp/indel profiles to facilitate quick-and-easy field diagnosis of a particular strain. such unambiguous diagnosis, in turn, triggers a range of epidemiological tracking and public health surveillance and control systems.

at a theoretical level, the new genomic information supports phylogenetic clustering analysis, which can place an outbreak strain into the broader context of the poxvirus “family tree”. such analysis often revels the ultimate geographic origin of a new strain, a rough chronology of its emergence, and the plausibility that it is a cross-over from a xenobiotic strain, like monkeypox. both tactical and theoretical motives have served to put pathogen genomic sequencing high on the priority lists of public health agencies, such as the cdc.

a definition of “high-quality” has been promulgated by genome assembly gold-standard evaluations   <cit>  and applied in two competitions for quality assembly: “assemblathon”  <cit>  and the “de novo genome assembly assessment project”   <cit> . these efforts evaluated the contigs generated by popular assembler software. assembly of the complex poxvirus genomes were poorly represented in the competitions: only the assembly of swinepox genome was evaluated, and that only from simulated data with fixed read-lengths of  <dig>   <cit> . it is well recognized that good performance of assembler software on simulated data may not reflect its performance on real data, which often include gaps, inverts, and rearrangements, usually generated from shorter reads. moreover, these evaluations were focused on the computational times and ignored the peculiarities of real, complex genomes.

for example, in the long poxvirus genome, gaps  are common, and may be of biological significance . moreover, poxvirus genomes are known to contain “inverted terminal repeats”  – longer or shorter, but comprising as much as 1 % of the genome, and prone to hair-pin loop-outs  <cit> . itr’s have traditionally confounded poxvirus sequencers, who have usually simply ignored them in published sequences. regions of ambiguity can be  addressed by laborious, “manual” sequencing of pcr amplicons  <cit> , but these approaches are costly and time-consuming, and thus defeat the main advantages of wgs.

to our knowledge, no one has systematically reported on procedures or insights relevant to the assembly from short reads of genomes with peculiarities like those in monkeypox, although numerous papers have compared quality of assembly for different types of sequencing data, such as long reads. the general computational challenge to the assembly of genomes with repetitive sequences has been addressed  <cit> . but the general case seems to oversimplify the particular problems of repetitive regions in the poxvirus genome which has inverted terminal repeat  at each end of the genome with a size range of  <dig> to 12kbp . the itr region contains additional local repeats within global repeats, which pose a special challenge to the assembly problem. this unique feature is presented in the entire family of poxviruses, comprising a large number of species-specific viruses, infecting a long list of mammalian species, and of significant agricultural and wild-life impact. the focus of the study was on the mpvx short reads , because short-read sequencers have become a major diagnostics tool for epidemiologists, providing fast results as needed during outbreak investigations. we are actively pursuing evaluations of long-read methods, such as pacbio, with a view to obtaining a single contig which covers the entire genome. so far, we have not attained this goal, but feel the present study is a worthwhile, interim report.

our work with poxvirus genomic assembly posed many questions. is it even realistic to expect that de novo assembly algorithm can arrive at a single contig covering the whole poxvirus genome, especially from the short reads available from “next generation sequencing” technology ? if an unassembled region is observed, how do we decide whether the problem lies with the bioinformatics tools, or with sequencing chemistry? can we simply ignore such problems, or do they reflect biologically significant characteristics of the genome? methodological corollaries to these questions are: can obtaining additional short-read data improve the quality of assembly? if yes, how much additional data is needed? are there technical compromises which adequately cope with the problems while still supporting robust public health surveillance requirements?

to answer these questions, we first analyze a reference  genome. we then generate simulated wgs reads from the reference genome. the de novo contigs derived from new sequences are evaluated by comparing to the reference genome, which process identifies mis-assembly and gap regions. this strategy not only allows us to deduce a high-quality genomic sequence for the strain of the virus under study, but also allows us to understand the limitations of the assembler algorithms, and hopefully to remedy them.

we then show how gap-filling of the genome can be converted into the all k shortest path  problem. finally we propose a neural network method  to show that it is possible to finish a monkeypox genome of a clinical sample by utilizing this method.

RESULTS
monkeypox genome, itrs and tandem repeats
the repetitive sequences in the poxvirus genome have been reported previously  <cit> , but they have not been systematically correlated to the wgs analysis from short reads. in this section, we focus first on an analysis of a known monkeypox genome.

repeats, which can cause breaks in contigs and thus mis-assembly, can be divided into two groups: global and local. a global repeat is defined as a long sequence which is duplicated throughout the genome  <cit> . the itr repeat in the monkeypox genome is one typical example of the global repeat  <cit> . for example, the sequence of the monkeypox genome deposited in genbank  has a length of  <dig>  bp with  <dig> annotated coding sequences . the two ends of the genome  are identical, but inverted, with a length of  <dig>  bp. the many coding sequences which connect the two ends are abbreviated as black dots in fig. 1a, while the red sequences are inverted with respect to each other.fig.  <dig> 
a a simple illustration of monkeypox genome  with itr regions highlighted in red. the red sequences  are inverted with respect to each other. b
x-axis: the tandem repeat locations in monkeypox. y-axis: their period size , copy size  and the length  of the tandem repeat region. two black boxes highlight tandem repeats within the itr region. tandem repeats were calculated by tandem repeat finder  <cit> 



in contrast to global repeats, the local repeat contains a simple sequence, which is duplicated in tandem many times. in the monkeypox genome, local tandem repeats are found both within the itr regions and outside these regions; that is, local repeats may be nested within global repeats. specifically, there are total  <dig> tandem repeat regions  <cit> . four of them are in the itr region. as shown in fig. 1b, the full lengths of each tandem repeat varies from  <dig> to 250 bp, the unique period size are from  <dig> to 70 bp, and the copy sizes are from  <dig> to  <dig>  these tandem repeat regions seem to be spread more-or-less randomly in the genome. this genetic characteristic of the mpxv genome leads, mathematically speaking, to the branching path problem, and usually breaks the assembly  <cit> .

de novo assembly of simulated monkeypox data
with the repeat information in hand, we simulated the illumina® intrument’s  paired-end reads using two different lengths:  <dig> and 250 bp, and at various coverage depths . reads were simulated using the fastqsimulate tool in celera assembler . three types of errors at a level of 1 % were taken into consideration while doing the simulation; namely mistmatch, insertion and deletion  <cit> . the reasons for choosing these read lengths are: 100 bp covers the longest period size , while 250 bp is approximately the longest tandem repeat in monkeypox genome. these two lengths are supported by a standard illumina ngs techniques .

with these simulations, we are trying to address several questions: 1) are repeats the sole achilles’ heel of the genome assembly from short reads? 2) what is the relationships among read length, longest contig size, genome coverage, genome depth coverage? 3) what is the minimum coverage sufficient for obtaining a reasonable assembly? 4) will more sequencing reads alleviate ambiguities in de novo assembly?

results of the simulation experiments are tabulated in table  <dig>  for simulated 100-bp read lengths, while decreasing the total number of reads from  <dig> to 10k , we observed that coverage decreased from 25x to 5x, leading to a large increase of the total number of contigs and a decrease of maximum contig length. this is simply due to insufficient sequence data. twelve million, 100 bp-reads yields the fewest and longest contigs, which corresponds to about 90 % coverage of the genome with 100 % accuracy.table  <dig> statistics for de novo assemblies using simulated monkeypox reads



a general performance pattern can be perceived: to obtain maximal contig length, there exists an optimal coverage at given read lengths for any fixed set of assembly parameters. . although, this is the first report of simulation results based on the monkeypox genome, the phenomenon of diminishing returns with increasing sequencing effort has been reported before  <cit> . thus, for any simulated read-length, collecting additional sequence data beyond this optimum does not improve assembly, but costs unproductive effort, as when we increase the number of reads from  <dig> to 20m, at a 100-bp read length for the example in table  <dig>  we do not suggest that this is a mathematically provable theorem, nor are we certain of this physical basis. but it is a consistent feature over a wide range of assembly parameter settings, generating large disparities of contig lengths . we venture to surmise that the superfluous coverage introduces more noise to the data, in a way that confounds the random algorithm rather than contributing to finding longer contigs.

the simulations indicate that increasing the read length to 250 bp can be expected to improve the de novo assembly, but, of course, at a price of increasing reagent consumption and sequencing time . in table  <dig>  the longest contig length was achieved at 250-bp reads:  <dig> thousand reads provides 60x coverage, with a single contig of 189kbp. this observation highlights a rule-of-thumb that long reads require less coverage for assembly. regardless of simulated read length, assembly breaks in the itr regions due to the global and local repeats.

recasting the assembly task into a graph-theory context
before describing our results with real-world, monkeypox sequence data, it is useful to review briefly the concepts relating the de novo sequence assembly problem to graph theory. details and examples of our specific approach are laid out in the methods section below.

utilizing graph methods to deal with the assembly problem has typically involved constructing an overlapping, or de bruijn graph, where each node in the graph represents a short sequence fragment   <cit> . if there is an overlap between two fragments , or if k-mers have a particular prefix and suffix , an edge is added between the corresponding nodes. a weight is assigned to this edge, which takes into consideration the length of the overlap and possibly other factors . in effect, a contig can be considered as a path in the graph from an initial node to a terminal node . the intrinsic problem of deducing and ranking several possible contigs  which join a set of specific, overlapping, short fragments is equivalent to searching out a path  in a graph.

velvet  <cit>  has utilized a dijkstra-like algorithm to search out paths in de bruijn graphs. the approach confronts two of the general challenges: 1) a sequence repetition  represents a branch in the paths through the graph, at which the algorithm gets confused, and lacking some additional rule, the assembly process stops  <cit> . 2) as the number of reads increases, the complexity of the graph becomes so large that it overwhelms computation of the true path. the branch-point dilemma has been ameliorated somewhat by reporting the best path so far, before giving up in failure  <cit> . the complexity issue is greatly exacerbated by new generation instruments, which churn out many more, but significantly shorter, reads.

these previous efforts highlight the fact that global and local repeats are fundamental obstacles to the application of graph-theory to de novo assembly, because such repeats constitute branching points which terminate as broken contigs  <cit> . such gaps are the major challenge to “finishing” a genome, that is, to generating a single, unbroken contig. our approach provides a partial solution to these dilemmae by delineating multiple, “shortest” paths, that is, a few, alternative, unbroken contigs, one of which most likely represents the “true” genomic sequence.

rather than holding out for the shortest path, and risking failure, we settle for the k-shortest paths. at branch-points caused by repetitive sequences, we pursue all possible branches, and rank the ultimate complete paths by length. to solve the k shortest path problem  we resort to neural networks.

the all k shortest path problem  problem and a neural network method
to find all k shortest paths in a graph needs not only computation of the shortest path from the initial node to the end node, but also computation of the second, third… kth shortest paths . typically, it is solved by heuristic algorithms, such as the well-known dijkstra's algorithm, which can quickly provide a good solution in most instances. however, as the scale of problem increases, these methods become inefficient and may consume considerable amounts of cpu time. neural networks, which are massively parallel models, have been reported as an approach to circumvent these problems with the classical algorithms . continuous-time coupled neural networks in have been advocated as effective approaches  to solving shortest path  problems. in these methods, decision variables vij  are denoted as the neuron activation states. these state are described by a system of differential equations. a lyapunov  function is designed to drive each neuron to its stable state  <cit> . usually, the terms “neuron” and “node” were used interchangeably, though by “neuron” we imply also the system of differential equations associated with that node’s activation. recent techniques use vertices in the graph to denote neurons  <cit> . the neural dynamics are modeled by coupled differential equations in such a way that a smaller coupling strength  corresponds to an individual neuron’s earlier firing time. following excitation of the initial neuron, the signal propagates according to the graph topology and individual neural dynamics – across  network, from the initial node to terminal node through all paths, including the shortest path.

one advantage of this method is that the time required to propagate of the signal  is not dependent of the number of nodes in the graph, but only on the path lengths from the initial node to the terminal node  <cit> . it is this characteristic that addresses the second dilemma, namely, graph node-complexity inherent in ngs chemistry. to find the shortest path, one must calculate the individual node dynamics; closed-form solutions are available in system which contain only first-order linear differential equations .

we have previously extended a neural network method to the ksp problem  for a graph with multiple edges and demonstrated its independence of the number of nodes and edges but on a graph topology  <cit> . the computational intensity of the neural-ksp algorithm scales with the network topology -- performing better when all k shortest path lengths are small and the network is large  <cit> .

with respect to gap-filling, we have also explored “gapfiller”, a program which has been validated on bacterial datasets and the human genome  <cit> . we were not able to fill the  <dig> gaps in table  <dig>  thus, we embed the neural-ksp method as the final stage in our in-house pipeline for calculating finished viral genomes. this laboratory has sequenced  numerous poxvirus sequences; all previous, conventional methods of assembly have proven far more laborious. in the methods section, we lay out a generalized formulation, taking into account both individual node dynamics and overall network topology. this form offers a mathematical foundation for the neural-ksp method. applying this pipe-lined process has enabled us to obtain a finished, monkeypox genome of length of 197kbp, as shown below.table  <dig> statistics for clinical sample




application of the de novo assembly protocol to clinical monkeypox data
to explore the strategy for generating a finished genomic sequence from clinical data, we used an illumina hi-seq® instrument  to sequence monkeypox virus isolated from a human patient. we expect that the method would generate better results when applied to mi-seq data, since it generates longer reads. however, due to limited timeline and resources, we have not been able to compare systematically results with hi-seq and mi-seq protocols. we hope to be able to do so in a follow-up report. using standard assembly software, we expected multiple, short, gappy contigs. for this isolate, with a 33m-read datatset, the trimming process did not significantly reduce read length, as shown in table  <dig>  about 31m reads mapped to the human genome, and were excluded, leaving about 3m reads, presumably of viral origin. 

using conventional assembler software , these reads were assembled into  <dig> contigs, with a maximum contig length around 130kbp. these seven contigs were ordered against a bona fide monkeypox genome, leaving a draft genome with  <dig> gaps  and covering 96 % of the genome, including the itr region at one end ; these statistics are summarized in table  <dig>  the observations are similar to a previous study  <cit> .table  <dig> gap statistics for clinical sample

estimated gap sizea
startc

agap size estimated by abacas; baccession no. dq011154; cposition within the sequence



the “finishing” job thus was to fill in six gaps comprising all together  <dig> bases , plus around 7kbp missing itr regions. our strategy for doing this was to present, as input data to the neural-ksp method, all  <dig> m reads  plus the gap-flanking sequences  – one run of the neural-ksp for each of the six gaps.

the result of this process was to fill all six gaps, producing a single contig of  <dig>  bp . the finished genome is depicted in fig.  <dig>  relative to the template monkeypox genome. in this particular isolate, we observed few significant genomic variations, such as indels, implying that the virus infecting the patient was genetically close to the reference strain.fig.  <dig> the upper bar depicts the reference monkeypox genome; the lower bar is the finished genome of the clinical isolate, assembled by the protocol described herein. the dark red lines within the bars track regions of multiple base differences between the two genomes, such as indels. the resolution of these charts are insufficient to reveal single nucleotide replacements 



CONCLUSIONS
in summary, as final stage in a protocol to elucidate clinical poxvirus genomic sequences, we have utilized a neural-ksp method to “finish” the process by closing gaps remaining after conventional assembly. such finished sequences may enable clinicians to track genetic distance between viral isolates, a powerful epidemiological tool. in principle, the protocol should prove useful in any clinical viral isolate regardless if a reference-strain sequence is available. the protocol may prove especially useful in genomes confounded by many global and local repetitive sequences embedded in them.

