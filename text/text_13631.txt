BACKGROUND
dna microarray is a technology that can simultaneously measure the expression levels of thousands of genes in a single experiment. it is commonly used for comparing the gene expression levels in tissues under different conditions, such as wild-type versus mutant, or healthy versus diseased  <cit> . some of the genes are expected to be differentially modulated in tissues under different conditions, with their expression levels increased or decreased to signify the experimental conditions. these discriminatory genes are very useful in clinical applications such as recognizing diseased profiles. however, due to high cost, the number of experiments that can be used for classification purpose is usually limited. this small number of experiments, compared to the large number of genes in an experiment, wakes up "the curse of dimensionality" and challenges the classification task and other data analysis in general. it is well-known that quite a number of genes are house-keeping genes and many others could be unrelated to the classification task  <cit> . therefore, an important step to effective classification is to identify the discriminatory genes thus to reduce the number of genes used for classification purpose. this step of discriminatory gene identification is generally referred to as gene selection. gene selection is a pre-requisite in many applications  <cit> . it should be noted that, often, the number of unrelated genes is much larger than the number of discriminatory genes.

there are a variety of gene selection methods proposed in the last a few years  <cit> . among them, some methods assume explicit statistical models on the gene expression data. for example, baldi and long  <cit>  developed a gaussian gene-independent model on the gene expression data, and implemented a t-test combined with a full bayesian treatment for gene selection. these methods assuming certain models are referred to as model-based gene selection methods. other methods do not assume any specific distribution model on the gene expression data and they are referred to as model-free gene selection methods. for example, xiong et al.  <cit>  suggested a method to select genes through the space of feature subsets using classification errors. guyon et al.  <cit>  proposed a gene selection approach utilizing support vector machines based on recursive feature elimination. it has been reported that the results of model-free gene selection methods may be influenced by the classification methods chosen for scoring the genes  <cit> . nonetheless, model-based gene selection methods lack of adaptability, because it is unlikely possible to construct a universal probabilistic analysis model that is suitable for all kinds of gene expression data, where noise and variance may vary dramatically across different gene expression data  <cit> . in this sense, model-free gene selection methods are more desirable than model-based ones.

gene selection is to provide a subset of a small number of discriminatory, or the most relevant, genes that can effectively recognize the class to which a testing sample belongs. that is, it is to provide a classifier such that the classification error is minimized. the known dataset that is used for learning the classifier, or equivalently for gene selection, is referred to as the training dataset. in a training dataset, every sample is labeled with its known class. notice that if one class is significantly larger than the others, then the trained classifier might be biased. therefore, the desired gene selection methods are those that are not affected by the sizes of classes in the training dataset. a gene selection method is called stable if the selected genes are kept the same when duplicating all the samples in any class in the training dataset.

in this paper, we propose two novel gene scoring functions s <dig> and s <dig> to design two stable gene selection methods gs <dig> and gs <dig> , respectively, to be detailed in the methods section. according to the classification scheme proposed in  <cit> , our proposed gene selection methods are single gene scoring approaches. these two gene scoring functions non-trivially incorporate the means and the variations of the expression values of genes in the samples belonging to a common class, based on a very general assumption that discriminatory genes are those having different means across different classes, small intra-class variations and relatively large inter-class variations. this spherical data assumption does not involve any specific statistical model, and in this sense, the resultant gene selection methods gs <dig> and gs <dig> could be regarded as model-free. they are also shown to be stable theoretically.

RESULTS
we have applied our gene selection methods gs <dig> and gs <dig> based on the gene scoring functions s <dig> and s <dig>  respectively, to a total of  <dig> publicly available microarray datasets  <cit> : the leukemia  dataset  <cit> , the small round blue cell tumors  dataset  <cit> , the malignant glioma  dataset  <cit> , the human lung carcinomas  dataset  <cit> , the human carcinomas  dataset  <cit> , the mixed-lineage leukemia  dataset  <cit> , the prostate  dataset  <cit> , and the diffuse large b-cell lymphoma  dataset  <cit> , the first two of which have been used for several similar testings of gene selection methods. on each dataset, one portion was used as the training dataset for our methods to score the genes and the other portion was used as the testing dataset. for each specified number x we reported the classification accuracy, on the testing dataset, of the classifier based on the top ranked x genes using the training dataset. the quality of these top ranked x genes is justified on two aspects: 1) the classification accuracy of the resultant classifier on the testing datasets, and 2) for the first two datasets leu and srbct, the stability when the training datasets were partially changed. all the experiments were conducted in matlab  <cit>  environment on a pentium iv pc with a  <dig>  ghz processor and a  <dig> mb ram.

the datasets
the leu dataset contains in total  <dig> samples in two classes, acute lymphoblastic leukemia  and acute myeloid leukemia , which contain  <dig> and  <dig> samples, respectively. every sample contains  <dig>  gene expression values. we adopted the pretreatment method proposed in  <cit>  to remove about half the genes and subsequently every sample contains only  <dig>   <dig> gene expression values.

the srbct dataset contains in total  <dig> samples in four classes, the ewing family of tumors , burkitt lymphoma , neuroblastoma  and rhabdomyosarcoma   <cit> . every sample in this dataset contains only  <dig>  gene expression values. among the  <dig> samples,  <dig>   <dig>   <dig>  and  <dig> samples belong to classes ews, bl, nb and rms, respectively.

the glioma dataset  <cit>  contains in total  <dig> samples in four classes, cancer glioblastomas , non-cancer glioblastomas , cancer oligodendrogliomas  and non-cancer oligodendrogliomas , which have  <dig> ,  <dig>  samples, respectively. each sample has  <dig> genes. we adopted a standard filtering method  <cit> , that is, genes with minimal variations across the samples were removed. for this dataset, intensity thresholds were set at  <dig> and  <dig>  units. genes whose expression levels varied <  <dig> units between samples, or varied <  <dig> fold between any two samples, were excluded. after preprocessing, we obtained a dataset with  <dig> samples and  <dig> genes.

the lung dataset  <cit>  contains in total  <dig> samples in five classes, adenocarcinomas, squamous cell lung carcinomas, pulmonary carcinoids, small-cell lung carcinomas and normal lung, which have  <dig>   <dig>   <dig>   <dig>  samples, respectively. each sample has  <dig> genes. the genes with standard deviations smaller than  <dig> expression units were removed and we obtained a dataset with  <dig> samples and  <dig> genes  <cit> .

the car dataset  <cit>  contains in total  <dig> samples in eleven classes, prostate, bladder/ureter, breast, colorectal, gastroesophagus, kidney, liver, ovary, pancreas, lung adenocarcinomas, and lung squamous cell carcinoma, which have  <dig>   <dig>   <dig>   <dig> , <dig>   <dig>   <dig>   <dig> , <dig> samples, respectively. each sample contains  <dig> genes. in our experiment, we preprocessed dataset as described in  <cit> . we included only those probe sets whose maximum hybridization intensity  in at least one sample was  <dig>  all ad values ≤  <dig>  including negative ad values, were raised to  <dig>  and the data were log transformed. after preprocessing, we obtained a dataset with  <dig> samples and  <dig> genes.

the mll dataset  <cit>  contains in total  <dig> samples in three classes, acute lymphoblastic leukemia , acute myeloid leukemia , and mixed-lineage leukemia gene , which have  <dig>   <dig>   <dig> samples, respectively. in our experiment, intensity thresholds were set at  <dig> –  <dig> units. then the relative variation of expressions for each gene was determined by dividing the maximum expression for the gene among all samples  over the minimum expression , i.e. max/min. we filtered out the genes with max/min ≤  <dig> or  ≤  <dig>  that is, for max/min fold variation, we excluded genes with less than 5-fold variation and, for  absolute variation, we excluded genes with less than  <dig> units absolute. after preprocessing, we obtained a dataset with  <dig> samples and  <dig> genes.

the prostate dataset  <cit>  contains in total  <dig> samples in two classes tumor and normal, which have  <dig> and  <dig> samples, respectively. the original dataset contains  <dig> genes. in our experiment, intensity thresholds were set at  <dig> –  <dig> units, the same as in the mll dataset. then we filtered out the genes with max/min ≤  <dig> or  ≤  <dig>  after preprocessing, we obtained a dataset with  <dig> samples and  <dig> genes.

the dlbcl dataset  <cit>  contains in total  <dig> samples in two classes, diffuse large b-cell lymphomas  and follicular lymphoma  which have  <dig> and  <dig> samples, respectively. the original dataset contains  <dig> genes. we set intensity thresholds at  <dig> –  <dig> units, then we filtered out genes with max/min ≤  <dig> or  ≤  <dig>  after preprocessing, we obtained a dataset with  <dig> samples and  <dig> genes.

among the above  <dig> datasets , the first two, leu and srbct, have been used frequently for testing gene selection methods and classifiers. for each of them, if we use the ratio of the largest class size divided by the smallest class size to represent the level of unbalance, then the fifth dataset car is the most unbalanced. in our experiments, we have run every gene selection method on each of the  <dig> datasets to rank the genes, and for every x ≤  <dig>  the classification accuracies of the classifier built using the top ranked x genes have been collected . we chose to present part of the classification accuracies on datasets srbct and car in details  and to present only three values for x,  <dig>   <dig>  and  <dig>  for all eight datasets .

classification accuracies
using the top ranked genes selected by a gene selection method, together with their expression values in the training dataset, one can build a classifier that will decide for each testing sample the class it belongs to. only the expression values for those selected genes in the testing sample are used for such a decision making. this is a standard way to test the quality of those selected genes, to examine how well the resulting classifier performs. note that testing samples are not included in the training dataset. to this purpose, we define the classification accuracy to be the percentage of the correct decisions made by the classifier on the testing samples. we have compared our methods gs <dig> and gs <dig> with two other model-free gene selection methods cho's  <cit>  and f-test  <cit> . in our experiments, we have adopted two ways to build a classifier using the selected genes, one is through support vector machines   <cit>  and the other is through k-nearest-neighbor  search  <cit> . essentially, svms compute a decision plane to separate the set of chips  having different class memberships, and use this plane to predict the class memberships for testing chips. there are a number of kernels used in svms models for decision plane computing and we chose a linear kernel as described in  <cit> . a knn classifier ascertains the class of a testing sample by analyzing its k nearest neighbors in the training dataset  <cit> . we chose the euclidean distance in our knn classifier with k =  <dig> and predicted the class by majority vote  <cit> . the svm we used in matlab is from  <cit>  and we coded the knn by ourselves. for ease of presentation, the achieved classifiers are referred to as the svm-classifier and the knn-classifier, respectively.

figures  <dig> and  <dig> plot the classification accuracies of the svm-classifier based on four gene selection methods gs <dig>  gs <dig>  cho's, and f-test, on the srbct and car datasets, respectively. these classification accuracies were obtained through leave-one-out  cross validation. in loo cross validation, one sample was left out as a testing sample and the remaining were used as the training dataset, and this was done for every sample in the dataset. we have also conducted 5-fold cross validation, in which each dataset was randomly partitioned into  <dig> parts of equal size and in every experiment four parts were used as the training dataset . this was done for every four parts in the dataset and the process  was repeated for  <dig> times. the average accuracy over all these  <dig> testing datasets was taken as the 5-fold cross validation classification accuracy. all plots of the 5-fold  cross validation classification accuracies of the svm-classifier and the knn-classifier based on four gene selection methods gs <dig>  gs <dig>  cho's, and f-test, on the eight datasets are included in additional file  <dig>  especially, columns 2- <dig>  in tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> record these cross validation classification accuracies, for only three numbers of top ranked genes, that is,  <dig>   <dig>  and  <dig>  column  <dig>  records the highest cross validation classification accuracies on these eight datasets ever achieved by the svm-classifier and the knn-classifier, respectively, as well as the associated numbers of selected genes .

note that in the 5-fold cross validation, the classification accuracy is calculated as the average of  <dig> classification accuracies on  <dig> testing datasets. we have also collected their standard deviations . for three numbers  <dig>   <dig>  and  <dig>  the standard deviations are included in tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  essentially, all these four gene selection methods, gs <dig>  gs <dig>  cho's, and f-test, have very close standard deviations, and these standard deviations seem to be independent of classifier and dataset. consequently, looking at all the classification accuracies as shown in figures  <dig>   <dig> and tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>  one general conclusion is that our gene selection methods, gs <dig> and gs <dig>  perform at least comparably well to f-test and cho's, on all  <dig> datasets using both the svm-classifier and the knn-classifier. typically, our methods outperform significantly the other two methods on datasets srbct, glioma, lung, and car, which have unbalanced class sizes.

stability of the gene selection methods
given a training dataset , to test the stability of a gene selection method we duplicated all the samples in one class to produce a duplicated dataset. our gene selection methods gs <dig> and gs <dig> are shown to be stable theoretically  and therefore are not subjects to such a test. for each of cho's and f-test, it was applied on the duplicated datasets to report the same numbers of genes as it was applied to the original training dataset, and then the percentages of the common genes were recorded. note that the leu dataset and the srbct dataset give  <dig> and  <dig> duplicated datasets, respectively. table  <dig> collects these percentages.

we have also performed a similar experiment to test the stability when a small portion of the samples were removed from the training dataset. for each class in a training dataset, it was divided into three parts of equal size and every time one part was removed from the dataset to obtain a reduced dataset. then again, the percentages of the common selected genes using the original dataset and the reduced datasets were recorded. we tried in total  <dig> random divisions and the average of  <dig> percentages was taken to be the stability for this class. table  <dig> collects these stability results for gs <dig>  gs <dig>  cho's, and f-test. from these results, we can see that gs <dig>  gs <dig>  and f-test had very close stabilities on the reduced datasets, while cho's had the least over all classes.

CONCLUSIONS
in this paper, we proposed two stable gene selection methods gs <dig> and gs <dig>  which could also be regarded as model-free. from the comparisons made to one most recent method cho's and one most classic method f-test on eight publicly available datasets, gs <dig> and gs <dig> selected genes whose resultant classifiers achieved at least equally good and most of the time better classification accuracies. both leave-one-out and 5-fold cross validations confirmed our conclusion. we haven't run any biological experiments to verify each of the top ranked genes by our methods yet inconsistent to other methods. nonetheless, we believe that our methods would be good potential substitutes to the ones currently in use as our methods are model-free and stable.

