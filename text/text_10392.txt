BACKGROUND
most applications of oligonucleotide arrays, such as finding differential expressed genes or network reverse engineering, involve the comparison of different arrays. since array measurements are highly sensitive to the experimental conditions, comparison of arrays can be problematic. this is especially the case when experiments have been performed in different batches or experiments. several normalization methods have been developed to handle this problem .

in this work we focus on differences between arrays caused by amplification, hybridization and location-based effects. often used normalization methods such as rma do not take into account these significant technical effects, while methods such as pdnn and gc-rma only take into account the hybridization effect . we introduce a new normalization method that takes into account all these effects and improves performance over the existing methods. although this study focuses on affymetrix genechips, the resulting method can also be applied to other oligonucleotide platforms.

technical effects
the affymetrix platform uses arrays with short 25-nucleotide probes placed on them. to measure mrna expression, transcripts are amplified, fragmented and labeled, after which they are placed on the array to hybridize with the probes. after washing, the amount of hybridized rna can be measured per probe. the first step that can easily be influenced by experimental conditions is the amplification. in this process, a primer is used to bind to the poly-a tail of a transcript, after which t <dig> rna polymerase uses this primer to start the creation of new  copies of the transcript. we found that the array signal shows a negative bias for probes closer to the 5' end of the transcript . this effect has been identified earlier, and is part of the quality control measures in mas  <dig>  and the affy package  <cit> . some authors have suggested that the 5' end negative bias is caused by 5' end rna degradation. we found that an incomplete amplification  better explains the data . currently, such an effect is not taken into account by any normalization methods that we know of.

the second step that can be influenced is the hybridization and washing of the fragmented transcripts . it is well known that these effects are generally dependent on the sequence  <cit> . in several current normalization methods , the probe sequence information is used to remove signal bias and/or correct for the rate of binding of transcripts to the probe. in contrast this work focuses on using the hybridization model to reduce signal variance.

the third technical effect is based on the location of the probe on the array. some parts of the array show blemishes which reduce or increase the signal. we also find that large parts of the arrays can be affected between batches . we estimate this effect for every probe and remove it from the signal. a more simple 16-block grid based method is part of the mas  <dig>  method, while  <cit>  divide the array in subarrays and apply normalization on each subarray separately.

background removal after normalization phase
microarray pre-processing methods often have a background signal subtraction phase , followed by a normalization phase  and summarization of the probe sets. the primary goal of the first phase is to improve accuracy , while the second phase is performed to improve precision . current pre-processing methods differ mostly in the method of bias reduction. for example, mas  <dig>  uses mismatch probes to estimate the background, rma uses a general background distribution and gcrma a sequence-based model. for normalization, often general distribution-correcting methods such as quantile normalization  <cit>  or loess normalization  <cit>  are used. one can perform these methods using a single reference array, or use multiple reference arrays as is done in the ptr method  <cit> .

our first attempt at removing technical effects during the bias reduction phase did show that, although one can improve accuracy, it is hard to not simultaneously decrease precision. the reason for this is that the estimated correction factors  can be large and are estimated for each array separately with models that are simplifications of reality. to solve this, we perform the background subtraction phase after the normalization phase. consequently, within the normalization phase, differences in technical effects between arrays are corrected. for example, in the case of sequence effects we perform now variance normalization  instead of bias removal . as we normalize not only the true signal but also the background signal, this allows us to use the same background estimate for all arrays during the background removal phase. this way we can still improve accuracy, while simultaneously preventing the reduction of precision.

background removal within summarization phase
in general it is true that in cases that the measured fold change between arrays is low one is less certain that there is actually a 'real' fold change as opposed to situations where there is a large fold change. an important factor influencing the measured fold change is  the background signal. probes with a relatively large background signal will generally have a smaller fold change, if the fold change is calculated over the whole signal .

currently, most methods remove the background before the summarization of the probe sets. although this reduces signal and fold change bias, it also obscures the 'real' fold change. that is, the fold changes of probes with a large background signal will be blown up more than those of other probes. in fact, probes which measure only background signal for some arrays could get infinite fold changes if this was not prevented by limiting the amount of background subtracted. this has a major impact on the summarization of probe sets, as such probes become more important than they should.

one could choose to perform no background subtraction, preventing the dominance of high-background probes, at the cost of increased bias. but, even then probes that measure mostly background signal influence the summarization outcome. therefore, in our approach, we have moved the background removal not only after the normalization phase, but into the summarization phase. this allows us to model the importance of the probes during summarization according to the amount of 'true' signal they measure.

RESULTS
algorithm overview
to perform normalization, one has to determine what differences between arrays are caused by technical effects. however, performing pairwise comparisons of the arrays would lead to a quadratic number of comparisons, which does not scale for a large number of arrays. for this reason we construct a reference array  based on the median of the signal:  = mediani , and compare all the arrays to this reference array.

an often used model  for background removal is to split the signal in components according to s = o + b + x, where s is the measured signal, o represents the optical background, b is the background signal caused by non-specific binding, and x is the 'true' signal . here, we do not only use this model for background removal, but also for normalization, as some technical effects only affect a subset of these components. for example, amplification only affects x, while location-based effects appear to influence mostly b + x . our method works by first estimating values for these components for the reference array , obtaining   

with  being the reference optical background, non-specific binding background and 'true' signal . then, in the normalization phase, we first normalize for the optical, hybridization and amplification effects . these effects modify the components of the reference array signal into the measured signal in the following way:   

here, di is the optical effect difference, τ is the hybridization model and κ the amplification model. the parameters of these models  are fitted by minimizing the difference between sij  and  :   

this minimization is done robustly using a huber m-estimator  <cit> , for every array i separately. note that the difference is in log scale, as the technical effects we are removing are mostly multiplicative effects. we obtain  by using the optimized parameters in equation  <dig>  this enables us to get an estimate of the technical effect difference for an array i w.r.t. the median pseudo-array using tij =  - . we substract this technical effect difference from the measured signal sij for array i, to obtain the corrected signal for these effects, i.e.:   

using this signal, we perform in the next step array location correction ) and distribution correction using quantile normalization ) . the reason to do these corrections separately is that these effects cannot be effciently estimated using the approach of equation  <dig>  as technical effect estimations can be dependent on each other, one could choose to perform the normalization steps in an iterative fashion . we found, however, that repeating the first part  of the normalization is unnecessary, as it did not add noticeable performance. for the second part , one could first perform array location correction and then quantile normalization, or the other way around. however, as quantile normalization is a fast and idempotent procedure, we perform quantile normalization both before and after array location correction, i.e. . also in this case we did not find noticeable performance advantages over a non-iterative solution.

in the next subsections we discuss how to estimate the reference signal components , as well as the hybridization and amplification model . then we discuss the array location correction performed during step 2b, and the final step in the algorithm, the summarization and background removal .

estimating signal components
to divide the reference array signal  in its components  we first estimate  and . to estimate the reference optical background and the reference background due to non-specific binding, we use respectively a scalar and a sequence-based hybridization model . using these, we try to explain as much of the reference signal as possible. although we could use  as target signal, we found that using mini performs slightly better, presumably because it is a closer estimate of the 'only-background' signal. the estimate then becomes:   

where τj represents the hybridization model for probe j, ϕbg represents the hybridization model parameters, and wj is a weight assigned to each probe indicating whether the probe violates the background model and is calculated according to:   

the reason for this non-symmetric least-square weighting is to prevent an estimated background that is larger than the measured signal. in this work we use η =  <dig>  the model is estimated robustly using a  huber m-estimator. then we determine b and x, in such a way that s ≥ b + o using   

and   

hybridization model
the hybridization model τj is used three times: to estimate the reference background signal   as well as to estimate the background and foreground hybridization differences between arrays . the separate background and foreground hybridization difference model represents the notion that the non-specific hybridization process differs from the hybridization process of the targeted sequence  <cit> .

the hybridization model is based on the probe sequence. earlier studies have indicated that the position of nucleotides on the probe are important  <cit> . furthermore,  <cit>  as well as  <cit>  suggested to fit dinucleotide binding strength, based on hybridization experiments in earlier studies. each dinucleotide is presumed to have a multiplicative effect on the signal. as additive models are easier to handle, we use the following transformation: . this allows us to estimate the effects as additive components of .  is modelled according to two parts. one part models the influene of nucleotide pairs in the probe sequence ,j). the other part models the influence of the number of certain nucleotides present in the probe sequence ,j).

let ζj represent the  sequence of probe j, and ζj,m ∈  the m'th nucleotide on it. although we could fit a model where we determine an influence factor for every nucleotide pair for every position  is defined as {ζj,m,ζj,m+1}), this would lead to a large amount of variables. as these influence factors vary smoothly with probe position, it is possible to estimate the position effect with fewer parameters. to this end, we used b-splines , i.e.   

where ϕp,q is a knot weight q for nucleotide pair p, and bp,q is the corresponding b-spline basis function factor for position m on the probe. furthermore, i = p) is an indicator function determining if probe j has nucleotide pair p on positions .

this model assumes that all contributed binding affinities contribute linearly to the sequence effect in log-space. in reality we did not find this to be the case. for example, probes with relatively many adjoining c, g nucleotides reported consistently a higher signal than would be expected from the linear model. for this reason, we added factors to the model that depend on the number of certain nucleotides within a probe. let c) = ∑n∈n|ζj,x| ζj,x = n, v ≤ x ≤ w| represent the number of certain nucleotides n in a  the probe sequence ζj,, then the second part of the sequence model is:   

where r is a sequence range parameter with elements from r = {, , }, v ∈ {'a', ...'t', {'a','c'}, ...} contains the nucleotide sets that were counted in each probe, bv,r,q is the b-spline basis function factor with as parameter the number of nucleotides that fall within range r and are in set v, and ϕv,r,q is the corresponding weight for knot q. we use the different sequence ranges r to better model the non-specific background binding which often only hybridizes with a part of the probe sequence. the final model is then the summation of the two introduced parts:   

experimentally, we found that q =  <dig> knots with degree  <dig>  are able to fit the signal well. our results indicate that there is enough data to estimate the  <dig> parameters within ϕ without significant overfitting. this is to be expected as there are often more than  <dig>  probes on a microarray.

amplification effect correction
our experiments  indicate that the lower expression of 5' end probes is best explained by incomplete amplification of the probes. copies are made starting from the 3' end of the transcript, using a primer attached to the poly-a tail. however, not every copy will be complete, causing a 3' end bias in the signal we measure. this suggests that differences in this effect between arrays could be modeled using the distance between a probe and the poly a-tail. as it is somewhat complicated to determine the location of this poly-a tail w.r.t. to the probe set , and most probe sets are located close to the poly-a tail, we make the simplification that we consider the amplification differences to start at the 3' end of the probe set. we found that a simple  probe location model, pjai, where pj is the distance of a probe to the 3' end of the probe set and ai an amplification difference parameter, did not remove most of the amplification effect. we suspected that the amplification effect is actually sequence-dependent, and changed our model to include these effects:   

where probe-specific vector pj contains for each dinucleotide the number of occurences between the middle of probe j and the 3' end of its probe set, and the array-specific vector ai contains the parameters for each dinucleotide determining its role in amplification differences. an f-test shows that this model significantly improves the amount of amplification effect our model is able to fit over the location-only linear model .

array location effects
estimating the  array location effects within the model itself would lead to an unrealistic large number of parameters. for that reason, we estimate the array location effects separately. that is, for a given input signal xij, we calculate an output signal yij = l corrected for location effects. in the 'algorithm overview' section it is described how this function is used in the second normalization step. although we cannot exclude that location effects could affect the optical background signal, we found that normally only the hybridization signal  is affected . furthermore, even if there are location effects in the optical background they will not have a significant impact on the signal of expressed genes. for this reason, we estimate the location effect only for the hybridization signal. assuming that the input signal has already been normalized for optical background , we can use a common optical background estimate .

to perform the location correction, we determine the difference between the median signal over all arrays and the input signal, for each array i. the difference between these two is calculated after optical background subtraction, using:   

we use log scaling as the location effects affect the signal multiplicatively. next, all calculated residues ϵij are mapped to the arrays on the location of their corresponding probe j . if there is a location effect in a certain region of an array this will show as residues with a negative or positive bias. to robustly estimate this effect, we calculate for each probe on each array the median of the residues in the neighborhood. however, not every place on the array contains a probe, furthermore we do not use the mismatch probes. to handle the empty probe positions as well as probes near the border of the array, we choose to use a fixed size median box filter of  <dig> ×  <dig>  which takes into account only the valid probes under it. this means that the number of probes used by the filter depends on its position. the result of this filter are location effect values λij = median_box_filter for each probe on each array. to obtain the corrected signal we calculate .

summarization
the normalized signal  is summarized for each probe set pk . instead of using median polish  or the tukey bi-weight  we apply here a huber m-estimator which was found to lead to better results  <cit> . also, it enables us to use a more flexible model, which uses the estimation of the background signal, such that probes with a high background signal w.r.t. to the true signal obtain a lower weight. similar to other summarization methods, we estimate a mean probe set expression ωk, a probe set array factor αik containing fold changes w.r.t. to ωk, and a probe affinity factor ρj. these factors are estimated using the following model:   

to make the model identifiable, we use the constraints ∑iαik =  <dig> ∀k and ∑jρj =  <dig>  when the mean true signal level for a probe is low , the array effect αik will have less influence on the magnitude of the residual signal. due to this, the probe will have less influence on the final summarized signal.

a second modification we make is the removal of outlier probes. this has been suggested before in  <cit> . if the huber m-estimator  assigns a low weight to the measured values of a probe  for more than one-third of the arrays we remove the probe entirely as its quality for the other arrays is also questionable. however, we make certain that we keep always more than  <dig> probes within a probe set.

the fold change values αik are used for further analyses. these fold changes are corrected for bias as the background signal has been removed. using bias-corrected fold changes can be advantageous when performing for example network reverse engineering. however, as discussed in the introduction, one looses information on the reliability of the fold change. this is one of the reasons that methods such as mas  <dig>  and plier  do not perform that well when used with differential expression detection algorithms based on the magnitude of the fold change. to prevent this, we have added an option to backscale the fold changes using . this is the final output of the algorithm and is used in the subsequent analyses.

differentially expressed gene detection performance
to determine the detection performance on differentially expressed genes we used two spike-in datasets, one using the hg_u95a platform and one using the hg_u133a platform . we compared  the roc <dig> score for our method as well as several other popular normalization methods. the main result is that for both datasets the proposed method ) obtains the highest performance . if we look at the fold-change ranking, the next-best method is rma for the hg_u133a dataset and gcrma for the hg_u95a dataset. however, both methods do not perform consistently well over both datasets.

see additional file  <dig>  table s <dig> for a description of the methods. both fold change and the sam test statistic were used to determine roc <dig> scores . the 'single repl.' score is determined by applying the normalization and scoring methods to each of the three replicate array-sets individually and averaging the results, while the 'all' score is obtained by using all replicates simultaneously. the lower  <dig> rows describe versions of rdn which use a subset of quantile-quantile normalization , array location , hybridization  and amplification  correction to determine their individual influence.

every spike-in experiment was performed three times. we compared the performance both by using the methods on single replicates  as well as by applying the methods on the complete dataset simultaneously. as expected, the performance gain when using each replicate separately is larger, as replication is performed to reduce the effect of unwanted differences between arrays. however, even with replication our method outperforms other methods. in this context, it is also interesting to look at the sam test statistic score. many studies do not have multiple replicates for each sample , but use multiple samples per class. when comparing such classes, often the sam test statistic score is used, which takes into account variability within a class to determine the ranking. reducing variability caused by technical effects can improve its results. here we see that rdn performs better than many of the other normalization methods. the slightly higher sam score of gcrma for the hg_u95a dataset can be attributed partially to its use of a low probe expression cutoff, thereby removing probe sets with very low expression from consideration. if we apply a similar cut-off to the same amount of probes as in gcrma, we obtain a better sam-score of  <dig> . to determine the performance effect of individual components of rdn, we ran our method with certain normalization steps disabled . when performing only a standard quantile normalization in combination with the new summarization procedure, rdn already outperforms most of the other methods. enabling correction for location-specific effects as well as hybridization effects further increases the performance. amplification correction on the other hand does not significantly add to the performance for these experiments. this is as expected, as there are no significant differences in amplification between the arrays , presumably because the amplification was done once for all arrays, which in practice is not realistic for most experiments.

differential gene finding - hmsc dataset
the hmsc dataset has been measured in three batches, which each show differences in technical effects . as is the case for most biological datasets, we do not know the exact true outcomes for the hmsc dataset. for this reason, we can only use the internal consistency of the dataset after normalization as a measure. besides using the difference between replicate array pairs , we also looked at differentially expressed genes. we used the binary labels for gender and sample bone location for this purpose. the normalization methods are applied on the full dataset. then we determine for each batch separately the list of top genes w.r.t. to the class labels using sam, and determine the overlap between the lists of top genes from the different batches. for this, we calculate an overlap score. let tb,m be the list of m most significant genes for batch b w.r.t. to a certain set of class labels, then a score for each gene, g, can be calculated that expresses how often it is present in one of the lists of m most significant genes for each of the different batches:   

where |b| represents the number of batches, and i is an indicator function with value  <dig> if g is part of the top m genes of a batch, otherwise  <dig>  after determining the gene scores, we determine the total overlap score of all genes, o, by summing the individual gene scores. similar to an auc score, the final overlap score is calculated by determining the area under the o function with  <dig> ≤ m ≤ m, while normalizing for m, i.e:   

we report this score for various m in figure  <dig>  despite the large number of samples , rdn performs on average better than other normalization methods. the next best method  is also a method that corrects for sequence effects, confirming that this has a positive effect. we determined the influence of amplification correction by disabling it. here results are less conclusive. although we see an improvement for the location label, this is not the case for the gender label.

signal bias and background estimation
to remove signal bias, one needs a good background estimate. to compare the estimated background with the real background signal we used spike-in probes. for these probes we know that for a spike-in concentration of  <dig>  we should only measure background signal  <cit> . figure 6a shows the estimated and measured background signal. although we only use a linear model, the estimator gives a relatively good approximation of the actual background signal. a perfect prediction of the background cannot be expected as our model only looks at dinucleotides and nucleotide counts, leaving out the effect of longer matching rna sequences within the rna sample. the underestimation of the background is caused by using η =  <dig> in equation  <dig>  we used this value to prevent overestimation of the background, which would remove true signal. in figure 6b, we show the estimated dinucleotide weights w.r.t. to the probe position for the background model. as can be seen, 'cc' dinucleotides add to the background signal for a large part. furthermore, we see that dinucleotides such as 'gg', 'gc' are preferentially bound to the nucleotides closest to the 5' end.

to correct for signal bias, the estimated background is removed from the signal during summarization . to determine the remaining signal bias after this procedure, we compared the corrected expression levels for the spike-in probe sets with their actual spike-in concentrations . as these are log-log plots, unbiased signals should result in straight lines. in these plots, we see that plier and pdnn are the best performing method in terms of bias reduction . by default, rdn does not focus on bias correction as unbiased fold changes can negatively impact differential expression detection, as discussed in the introduction. in fact, after summarization it re-adds the background signal . due to this, rdn has approximately the same bias as rma_nbg, a method which does not remove background signal. however, if one requires more accurate signals one can also use a different setting for rdn, i.e. no backscaling and no underestimation of background . with this setting, rdn has a summed r <dig> measure between pdnn and plier.

signal precision for low, medium and high expression spike-ins
next to signal accuracy , one can also look at signal precision . however, one cannot use measures such as the standard deviation of a probe signal over multiple arrays. the reason for this is that background removal blows up the fold changes, especially of low expression probes. as each method uses different strategies for background removal, one can not directly compare such measures. a similar problem affects comparisons of signal precision using spike-in probes, where one determines the performance of the spike-in probe sets for different spike-in concentration ranges. for example, when testing low-expression spike-in probe sets, the performance for a method without background substraction will be affected by a relatively large number of high-expression false positives. for methods with background subtraction, it is the other way around.

therefore, we report here on an experiment  in which we compared fold changes of spike-in probe sets only to the fold changes of probe sets that are closest in average expression. that way, the results are only negligibly affected by confounding effects due to signal/fold change bias. in this experiment, the highest precision is obtained by rdn. it is interesting, however, that plier performs quite good for low spike-in concentrations, indicating that mismatch probes can be useful for improving signal precision. however, for high expression spike-in probe sets , plier does not perform that well, which is to be expected as mismatch probes are designed to report mainly background signal. methods which also correct for foreground signal effects  perform better here.

another method to quantify precision is to rank replicate array pairs w.r.t. non-replicate array pairs based on the difference in expression between the arrays in the pair. then one can compare the ranking using a method such as the auc score. we did this on the hmsc dataset  and found that rdn outperforms all other methods.

a third way to determine precision is to look at genes that have been spiked-in into the hmsc dataset. we run here into the same problem as with using standard deviations, namely that different methods can have different fold change scales. fortunately, we found that one gene  has been spiked-in at a different concentration for batch  <dig> of the hmsc dataset. this allows us to asses both the precision and fold change scale simultaneously, showing the strong reduction in technical variation obtained by using rdn compared to other methods .

inspection of dataset after normalization
to validate the used models, we determined whether the technical effects are indeed removed after normalization. results for the hmsc dataset  are shown in figure  <dig>  only for the less occuring types of probes, for example those with a gc content of  <dig>  we see that there are still differences between the different arrays. this is caused by the limited amount of knots used in the b-splines, which however does protect the model from overfitting. we also see that a large part of the amplification difference has been removed, although there are still some outlier arrays. it is interesting to note that sequence correction also plays a role here. the drop in the signal on the 3' end of the probe set, which can be seen in figures 9a and  <dig>  is mostly caused by the relatively larger number of 'a' nucleotides close the 3' end of a gene. that is, the position on the gene and the sequence composition of the probes are not completely independent, suggesting that sequence and amplification effects should be estimated simultaneously as is done in our model. the array location normalization effectively removes trends in the array images , although locations where a large correction was necessary still have more variabilty in the residual signal, as is to be expected.

discussion
the results show that the performance improvements shown cannot be attributed to one component of the algorithm. rather, it is a collection of improvements obtained by normalizing the different technical effects, removing the background after the normalization procedure, and, using background information in the summarization procedure.

our method currently assumes that, on average, the expressions of genes remains similar accross multiple arrays, allowing us to identify the technical effects. this assumption is also used in other widely used normalization methods such as quantile normalization. the limitation of these type of methods is that they cannot be used when all genes change their expression simultaneously upwards or downwards. rdn requires more computation time than other normalization methods . however, this amount of time is still small when compared to the time it takes to perform the microarray experiment. furthermore, it is easy to parallelize the algorithm. compared to the number of probes  our method uses a relatively limited numer of parameters. this prevents overfitting, so that only large scale effects affecting many probes simultaneously are removed. for platforms with even more probes one can choose to estimate the parameters with a subset of the probes. currently, the amplification correction is based upon the assumption that probe sets are close to the poly-a tail of the probe set. this is not always the case. we found that especially probe sets further away from the poly-a tail can have large variations in signal for all probes, correlating with the amplification differences. this is not corrected by our current method. given reliable distances towards the poly-a tail this could be added relatively easily.

the approach shown in this paper could also be useful for other oligonucleotide platforms. in this work we have mainly focused on gene expression. however, it could also be applied to chip-chip or tiling experiments.

CONCLUSIONS
we have introduced a new normalization method that corrects for hybridization, amplification and array location effects that occur when measuring expression using oligonucleotide arrays. the proposed robust difference normalization  corrects these technical effects by removing the differences in measured expression over different arrays instead of correcting for signal bias for each array. in this way, the proposed normalization procedure focuses on controlling the precision instead of the accuracy of the measured expression, which is more important for some applications . we have shown that the proposed rdn method increases performance, even on experimentally tightly controlled and three times replicated spike-in data sets. the method will be most useful for studies consisting of few independent replicates or those showing large batch effects.

