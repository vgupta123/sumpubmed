BACKGROUND
the microarray technology permits the simultaneous measurement of the transcription of thousands of genes. the analysis of such data has however turned out to be quite a challenge. in drug discovery, one would like to know what genes are involved in certain pathological processes, or what genes are affected by the intervention of a particular compound. a more basic question is 'how many genes are affected or changed?' it turns out that the answer to this basic question has a bearing on the other questions.

the proportion unchanged
in the two-component model for the distribution of the test statistic the mixing parameter p <dig>  which represents the proportion unchanged genes, is not estimable without strong distributional assumptions, see  <cit> . assuming this model the probability density function  ft of a test statistic t may be written as the weighted sum of the null distribution pdf  and the alternative distribution pdf 



if, on the other hand, we know the value of p <dig> we can estimate  e.g. through a bootstrap procedure as described in  <cit> , and thus obtain also .

the mixing parameter p <dig> has attracted a lot of interest lately. indeed it is interesting for a number of applications. here follow four examples.

1) knowing the proportion changed genes in a microarray experiment is of interest in its own right. it gives an important global measure of the extent of the changes studied.

2) the next example concerns fdr. suppose we reject null hypothesis j, and call gene j significantly regulated, when the corresponding p-value pj falls below some cutpoint α. the question that motivates the fdr concept, which originates from  <cit> , is: "what proportion of false positives is expected among the selected genes?" a goal would then be to quantify this proportion, and one possible estimate is



where '^' above a quantity  means that it is a parameter estimate, p is the largest p-value not exceeding α and p is the proportion significantly regulated genes which equals the proportion of the p-values not exceeding α, see also  <cit> . in practice p will be very close to α, and may be replaced by the latter. we thus obtain an estimate of p <dig> × α/p, which verbally translates into "the number of true null cases  multiplied by their probability of ending up on the top list  divided by the number of selected cases )". putting p <dig> ≡  <dig> above and rejecting hypotheses j with the estimated fdr less than or equal to β, will give a test procedure controlling the fdr at the level β, i.e. we may expect that the fdr is no more than β  <cit> . by finding good estimates of p <dig>  we may increase the power to detect more true positives at a given fdr bound.

3) knowing p <dig>  we may calculate the posterior probability of a gene being a differentially expressed gene  as



see  <cit> . also,  equals one minus the local false discovery rate:  <dig> - p <dig> = lfdr = p <dig> ft <dig> / ft  <cit> .

4) knowing p <dig>  it is also possible to estimate the number of false positives and false negatives at a given cutpoint α as a proportion of the total number of genes. call these proportions the false-positive and false-negative rates, and denote them by fp and fn, respectively. in the samroc methodology  <cit>  one calculates estimates of these quantities as



and



one may choose a p-value threshold αmin, which minimises the amount of errors fp + fn. alternatively, one may want to fine-tune the test statistic such that it will minimise the errors at a given threshold. or, one may try to do both, as suggested in  <cit> , see also  <cit> .

earlier research providing estimates of p <dig> include  <cit> . articles that compare methods for estimating p <dig> and fdr include  <cit> . i will focus on the fdr as the main use of p <dig> 

in this article, the formulation of the theory is in terms of p-values rather than in terms of test statistics. two basic assumptions are made concerning their distribution. first, it is assumed that test statistics corresponding to true null hypotheses will generate p-values that follow a uniform distribution on the unit interval, e.g.  <cit> . thus, under the null distribution, the probability that a p-value falls below some cutpoint α equals α. second, p-values are, unless stated otherwise, assumed to be independent. empirical investigations will assess the effects of deviations from the second assumption.

the use of p-values means lumping up- and downregulated genes together. however, one may look separately at the two tails of the distribution of the test statistic to assess differential expression corresponding to up- and downregulation.

this article will not concern how p-values are calculated, but rather how they are used to calculate estimates of p <dig> and fdr, and draw conclusions based on this evidence. it is assumed that p-values capture the essence of the research problem. neither does the article treat the choice of an optimal test statistic. for illustration the t-test will be used repeatedly without regard to whether there are better methods or not. by the t-test we mean the unequal variance t-test:  for sample means mean <dig> and mean <dig>  sample variances , , and sample sizes n <dig> and n <dig>  we apply the t-test to simulated normally distributed data and a permutation t-test to real data, where normality may be uncertain. all calculations were performed in r. the methods presented are available within packages for the free statistical software r  <cit>  and take a vector of p-values as input and output an estimate of p <dig> and of fdr. emphasis lies on methods available within r packages downloadable from cran  <cit>  or bioconductor  <cit> . inevitably any review will exclude interesting work, but time and space limitations will not permit an all comprehensive review. the new and highly interesting concept of a local false discovery rate   <cit>  only receives a cursory treatment.

this article builds on and finds motivation from the experience of the analysis of microarrays, which typically assay the expression of  <dig>  or more genes. however, the methods presented apply equally well to other high dimensional technologies, such as fmri or mass spectrometry.

false discovery rate
in the analysis of microarray experiments, the traditional multiple test procedures are often considered too stringent, e.g.  <cit>  and  <cit> . in the last decade alternatives based on the concept of an fdr have emerged. for more details consult e.g.  <cit> . there are different definitions proposed, but loosely speaking one would want to measure the proportion of false positive genes among those selected or significant. loosely put the fdr may be interpreted as the proportion of false positives among those genes judged significantly regulated. equation  is the fdr estimate presented in  <cit> .

denote by e the expectation  of any random variable x. with v the number of false positives given a certain cut-off and r the number of rejected null hypotheses, one may define the fdr as the expectation of the ratio of these quantities, or

,

where care is taken to avoid division by zero.

in  <cit>  and  <cit>  the fdr is estimated as the ratio of the expected proportion of false positives given the cut-off to the expected proportion selected. viewed as a function of a cut-off α, such that genes gi with pi less than α are judged significant in terms of p-values, following the continuous cumulative distribution function  f, the fdr estimate is



which is nearly equal to  with the exception that the p has been replaced by its upper bound, and the step-wise empirical distribution by a smooth version, either a parametric model or a smoothed version of the empirical distribution. thus, the fdr is now a continuous function instead of piece-wise continuous with jumps at each observed p-value. this ratio of expected values tries to estimate the expectation of a ratio: in general such an approach will give an overestimation, but in practice this will have little effect, see the additional file.

the related concept of the positive fdr, pfdr = e, the expectation conditional on at least one rejection, appears in  <cit> . other forms of fdr have been proposed such as the conditional fdr  <cit> , cfdr, defined as the expected proportion of false positives conditional on the event that r = r rejected have been observed : cfdr = e/r. this would answer to the question "what proportion of false positives may i expect in my top list of r genes?". under independence and identical distribution in a bayesian setting it is proved in  <cit> , that pfdr, cfdr and the marginal fdr, mfdr = e/e  <cit> , all coincide with p0α/f at the cutpoint α, cf. .

instead of p-values it has been suggested in to calculate q-values that verbally have the following meaning for an individual gene  <cit> :

the q-value for a particular gene is the minimum false discovery rate that can be attained when calling all genes up through that one significant  <cit> .

these q-values can be used to determine a cut-off similar to the classic 5% cut-off for univariate tests developed in statistics long ago. but in many applications one should not be too rigid about any particular value, since the emphasis often is on discovery rather than hypothesis testing: we generate hypotheses worthy of further investigation. thus the balance between false positives and false negatives will be crucial: rather than keeping the risk of erroneously selecting one individual gene at a fixed level, it is the decision involving thousands of genes given the amount of genes we can follow up on that is the focus, and where both types of error must be considered. the q-value does not fully address this problem, but nevertheless represents an improvement over the classical multiple test procedures in these applications.

more mathematically the q-value can be expressed as



taking minimum in  enforces monotonicity in pi, so that the q-value will be increasing  in the observed p-value. if the fdr is non-increasing, as it should, then q  = fdr .

additionally, the fdr offers a framework for power and sample size calculations, see  <cit>  and the new developments in  <cit> .

RESULTS
eight estimates of p <dig> and six of fdr  were tested on both simulated data and real data. the differing numbers are motivated below.

next follows a list of six p <dig> estimation methods and the corresponding r functions. the six were

 <dig>  the beta-uniform model   <cit> , which fits a mixture of a uniform and a beta distribution to the observed p-values; function ext.pi.

 <dig>  spacing loess histogram   <cit> , which fits a non-parametric spline that estimates the logarithm of the pdf;function splosh.

 <dig>  the lowest slope estimator   <cit>  ;function fdr.estimate.eta <dig> 

 <dig>  the smoother  <cit> , which fits a spline to a function of a cut-off value, namely the proportion of p-values greater than that cut-off divided by the expected proportion under the uniform distribution;function qvalue.

 <dig>  the bootstrap least squares estimate   <cit> , which is related to the previous estimate;function qvalue or estimatep <dig> 

 <dig>  the successive elimination procedure   <cit> ;selects a subset which represents the null distribution by behaving like a uniform;function twilight.

 <dig>  a new method based on a moment generating function approach ;function p <dig> mom.

 <dig>  a poisson regression approach ; an adaptation of  <cit> ;function p <dig> mom.

the bootstrap estimate and mgf did not participate in the calculation of fdr. the smoother gives the basically same value as the bootstrap estimate, and mgf is unnecessarily conservative for lower values of p <dig>  compared to pre.

the six were

 <dig>  bum fdr 

 <dig>  bh fdr .

 <dig>  splosh fdr 

 <dig>  smoother fdr or r function qvalue  <cit>  

 <dig>  sep fdr 

 <dig>  the new method pava fdr 

for brevity mgf, pre and pava fdr will all be referred to as new methods. it would be more exhaustive to say that pre is a minor modification of an existing method  <cit>  applied to p-values rather than test statistics and provided as a new implementation in r; and that pava fdr is based on  <cit>  with local splines replaced by isotonic regression and provided as a new r function. on the other hand mgf seems quite new. more details follow in methods.

for reference some graphs include an estimate of the sep local fdr, defined as lfdr = p0/f, estimating the probability that a gene whose p-value equals p is a false positive. furthermore, the ouput from r function locfdr applied to the real life data  and the transformed t-test statistics : z = Φ-1), and f the t-test distribution  gives perspective on the other methods and opens up an alternative route to making inferences. this function produces an estimate of the local fdr as a function of the transformed test statistic z  <cit> . in that same reference the author argues in favour of the cutpoint lfdr ≤  <dig> , which implies quite high posterior odds in favour of the non-null case : f1/p0f <dig> ≥  <dig> 

simulated data
two simulation models were used: one generating values independent between genes and the other generating observations displaying clumpy dependence  <cit> .

simulated independent data corresponding to two groups of four samples each were generated  <dig>  genes  <dig> times for each of the true p <dig> values ranging from  <dig>  to  <dig>  using the r script from  <cit> . normal distributions were chosen randomly from the ones in table  <dig>  the cases of degs correspond to the power of 41%, 54% and 97% given a t-test with a significance level of 5%  <cit> . briefly, the script generated a mixture of normal distributions and for each run calculated t-tests to obtain p-values.

to generate dependent data the protocol from  <cit>  was followed. this generates data following clumpy dependence in the sense of  <cit>  such that blocks of genes have dependent expression. first a logarithmic normal distribution with mean  <dig>  and standard deviation  <dig>  generated a profile for each gene. denoting by n a normal distribution with mean μ and standard deviation σ, random errors following a standard normal distribution n were added. to create dependencies genes were partitioned into sets of  <dig> and for each sample the same term from a n distribution was added to the expression of each gene in the set. finally, genes were randomly assigned to become degs with probability 1-p <dig> and for each gene a regulation term following either n or n, with equal probabilities, was added to the expression of one of the groups of samples of size  <dig>  the power to detect either of these two alternatives with a t-test at the 5% significance level equals 31% and 50%, respectively. the procedure generated for each of  <dig> iterations a set of observations of  <dig>  genes. the protocol gives rise to high correlation within the blocks of  <dig> . results for smaller or weakly dependent datasets appear in the additional file. here weakly dependent means that the clumpy dependence term follows a n distribution .

with the simulated independent data all methods for estimating p <dig> perform rather well, see table  <dig> and  <dig>  and figure  <dig> 

the new methods mgf and pre were very competitive on these data, and had both low bias and variation, excluding mgf at the  <dig>  and  <dig>  level. since mgf tends to overestimate p <dig> rather much in the lower range, one may prefer pre. for practical purposes though overestimation is desirable and enables control of the error rate .

the smoother and the bootstrap had good and quite similar performance. they give more or less the same variation and bias over the whole range. this variation can be a bit high though, especially when comparing to pre.

in the higher range bum gives a crude estimate of the true p <dig>  in a certain lower range however it underestimates. as we can see in figure  <dig>  however, the method considerably overestimates p <dig> in the higher range, which brings down the power to detect degs.

splosh has the advantage of fitting the observed distribution quite well, judging from some tests , compare also  <cit> . this enables a bayesian analysis as in . however, it does the fitting of the p-values close to  <dig> sometimes at the expense of the accuracy concerning the values at the other end, thus misses the plateau and the minimisation in  will give a misleading result. in particular, this tends to happen when there are few degs. as we can see in figure  <dig> the method underestimates p <dig> at the higher range , which is worrisome and may lead to underestimation of the error rate, which is undesirable for a method of statistical inference.

from tables  <dig> and  <dig> we can see that pre has the best over-all performance, followed by the smoother and the bootstrap. this does not however imply that the other methods could not be considered. the results vary quite a lot depending on the value of p0: lsl is quite competitive for p <dig> =  <dig> , but too conservative for p <dig> =  <dig> .

with simulated data it is possible to calculate the actual false discovery rate. figure  <dig> shows boxplots based on the simulated independent data. here qvalue and pava fdr stand out as most reliable. especially bum fdr has severe problems, often over the whole range of cut-offs, while splosh fdr does rather well.

the dependent data gave a slightly different picture. most importantly, the variation increased considerably. the results concerning p <dig> appear in figure  <dig>  interestingly underestimation becomes less of a problem for splosh this time. relatively speaking bum did better this time, and bh comes out worst. as far as estimation of p <dig> goes bum does quite well, see tables  <dig> and  <dig>  the method manages to overestimate at high p <dig> by consistently outputting  <dig>  at p <dig> =  <dig>  the estimate was nearly always equal to  <dig>  both qvalue and bootstrap underestimated by a small amount at p <dig> =  <dig> . sep emerges as a sharp contender, but suffers from a small underestimation at p <dig> =  <dig> . in fact, all methods except bum give underestimation at p <dig> =  <dig> , and for splosh it amounts to almost 7%.

the results concerning fdr appear in figure  <dig>  this time the bum method captures fdr in a competitive way, with the lowest median error but at the same time the second worst mean error. again the results for pava fdr appear quite stable and accurate, giving the second lowest median error and the lowest mean error.

results for  <dig>   <dig>  and  <dig>  simulated weakly dependent genes appear in the additional file. briefly, they resemble those of the independent case. for  <dig> genes however the variation is such that the value of using these methods seems doubtful.

real data
data from golub et al
these data represent a case where there are many degs. the data set concerning two types of leukaemia, all and aml, appeared in  <cit> . samples of both types were hybridised to  <dig> affymetrix hg <dig> arrays, representing  <dig> all and  <dig> aml. in reference  <cit>   <dig> genes were identified as degs using statistical analysis. the data consisted of average difference values and absolute calls, giving for each gene , respectively, the abundance and a categorical assessment of whether the gene was deemed absent, marginal or present. the average difference values were pre-processed as in  <cit> , and the proportion of samples where each probe set was scored present was calculated, giving a present rate. a permutation t-test was used to compare all and aml. figure  <dig> shows a histogram of t-test statistics revealing a bell-shaped region around the origin and large tails, indicating that a substantial part of the genes are degs.

visual inspection of figure  <dig> showing the p-value distribution also suggests that many genes are altered. the methods tested vindicate this : bum, splosh, bootstrap, qvalue, mgf, pre, sep, lsl give the estimates  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig>  and  <dig> , respectively. thus, roughly 35% of the genes are regarded as changed. the function qvalue finds  <dig> probe sets with a q-value less than 5%. the estimated of fdr appear in figures  <dig> and  <dig>  and show a great deal of concordance between methods. the curves rise to the respective estimate of p <dig> and and in doing so slowly diverge. sep approaches roughly  <dig>  in a vicinity of zero, while the other methods continue the decline.

the function locfdr applied to z = Φ- <dig> ) with f <dig> the cdf of the t-distribution with  <dig> degrees of freedom gives the estimate of p <dig>  <dig>  and outputs figure  <dig> which bears witness of the skewness and of the different local false discovery rates in the two tails of the distribution.

removing probe sets with less than 20% present rate will leave us with  <dig> probe sets, and qvalue indicates that there are  <dig> of them that are significant with a q-value less than 5% . in general it is wise to remove probe sets with low presence rate prior to analysis, since this will make the inference more reliable, compare  <cit> . doing so will most likely produce more true positives.

data from spira et al
next let us turn to a case where there are rather few degs. in  <cit>  the results from a microarray experiment where bronchial epithelial brush biopsies have been hybridised to affymetrix u133a arrays are presented. the biopsies come from three different subject categories: current smokers, never smokers and former smokers. the cel intensity files were downloaded from the ncbi gene expression omnibus   <cit> . the bioconductor package affy  <cit>  was used to normalise intensities with the quantile method, and to calculate the rma measure of abundance. the function mas5calls in affy output absolute calls.

here we will take a brief look at the comparison between former smokers and never smokers. the comparison may help identify genes that remain changed after smoke cessation. a fuller analysis would include more analyses, such as the current smokers vs. former smokers comparison, and possibly also adjust for the fact that former smokers tend to be older than never smokers .

graphical representations of the results appear in figures  <dig> through  <dig> 

discussion
over-all results will lump together performance under different conditions and may thus be less relevant for a particular application. for instance, in practice the performance for high p <dig> will probably matter more than that for lower values. when many genes are changed the cutpoint will likely be chosen based on other criteria than fdr, and hence the difference between methods becomes less relevant. however, the detailed results presented here should give the practitioner some guidance as to what methods could be considered. looking at the p-value histogram one can find some decision support in the choice of method. comparing the output from several methods provides further clues.

all the methods performed worse on the dependent data; both the estimate of p <dig> and fdr suffered. to some extent that may be due to the lower mean power of the alternatives in that simulation model. however, the methods were derived under the assumption of independence and the small difference in mean power of  <dig>  does not explain the great deterioration in most methods. indeed, for simulated datasets with weak dependencies the results came close to the independent case, see the additional file.

through all tests pre and pava fdr proved quite successful. of the methods for estimating fdr, qvalue has the advantage of a well-documented and good track record, and behaves well here. bum displays a varying performance, but does handle dependent data well. in practice it will be difficult to know where on the scale from independence to strong clumpy dependence a particular dataset will rate, if indeed it follows clumpy dependence at all. lsl, and bh, have some problems, but on the other hand they arise mainly at low p <dig>  where they probably matter the least. as noted above regarding the spira et al. data, sep lfdr and fdr stabilised at a value above  <dig>  when the cut-off approaches zero. in other tests sep performed well, particularly with independent data. bum in this case produced the estimate fdr ≡  <dig> which can hardly reflect the truth.

the locfdr method offers the possibility to choose between three different null type distributions. the choice of the null type n produced p <dig> estimates similar to those of the other methods. the need to specify the transform m may seem like an obstacle. but in many situations a parametric test statistic with a known null distribution exists. alternatively, m could be identified by modelling a bootstrap distribution  <cit> .

all the described methods assume the p-values were obtained in a reliable fashion, e.g. by a warranted normal approximation, a bootstrap or a permutation method. reference  <cit>  describes a case when a two-way anova f-distribution was used when the distributional assumptions were not met. the estimate of p <dig> gave an unrealistic answer. when permutation p-values were used instead their method gave a more realistic result. one always has to bear this caveat in mind. to further complicate matters, the permutation of sample labels approach is no panacea if the independence between samples assumption does not hold true, as detailed in  <cit> .  permuting within columns provided some remedy there. misspecifying the null distribution will jeopardize any simultaneous inference, whether based on fdr or not. it may pay off to consider the correlation structure in data, both in view of this finding and in view of the different performance of methods depending on the strength of correlations.

the q-value q has been criticised for being too optimistic in that it weighs in also genes that are more extreme than i when calculating the measure. note that a similar criticism could be levied against the classical p-value: the p-value gives the probability under the null hypothesis of observing a test statistic at least as extreme as the one observed. also, there is no clear stable, reliable and tested alternative. this is not to say that the q-value is unproblematic, but it still has been studied and used much more than e.g. the local fdr, which may suffer from high random variation, see examples in  <cit> . other examples from results section give evidence of stability. contrary to what one may anticipate the fdr is not always more stable than lfdr  <cit> . the concept of a local fdr seems quite interesting and may lead the way towards improved inference, and it begs a thorough investigation of the various recently published options.

to avoid pit-falls in the inference one must use the total information obtained from p <dig> and the fdr or q-value curve, see also storey in the discussion of  <cit> . there is not one cut-off in terms of q-value that will suit all problems. take the case of figure  <dig>  where one will have to accept a high fdr in order to find any degs. at the other end of the spectre, in figure  <dig>  the cut-off can be much more restrictive. the choice of cut-off must be made with a view to one's belief regarding p <dig>  and calculating the sum of  and  to assess to total of false positives and false negatives gives further guidance in this choice. in general it makes sense to choose a cut-off in the region , where αmin is the value which minimises the total relative frequency of errors committed fp+fn, see  and . however, since false positives and false negatives have different consequences with possibly different losses, it is difficult to state an algorithm that would cover all scenarios.

CONCLUSIONS
this article deals in the main with a simple frequentist framework for the analysis of microarray experiments. the conclusion is that the concept of the proportion of unchanged genes and the related concept of a q-value or false discovery rate are practical for such analysis. furthermore, there exists open source code that implements methods that address the needs of the practitioner in this field. new methods gave evidence of improved performance, allowing better control of the error rate and thus enabling a more careful identification of degs. issues still remain and improvements will probably appear over the next couple of years, but as a provisional solution these methods have much to offer.

