BACKGROUND
gwa analysis  <cit>  is a well established and powerful method for identifying loci associated with variations of complex genetic traits such as common diseases. hundreds of new genes have been implicated in human health and disease during the last few years in various gwa studies <cit> . in a typical study, hundreds of thousands, or millions, of single-nucleotide polymorphisms  are typed in thousands of individuals in order to detect genetic risk factors.

genabel is a specialized library package for gwa analysis  <cit>  implemented in r, an open source statistics programming language and environment  <cit> . genabel enables gwa analysis to be done using a regular desktop computer due to its efficient data storage and memory management. nevertheless, analysis of very large data sets are computationally challenging and may take hours or weeks to complete. examples include the utilization of sophisticated adjustments for population stratification and relationship structures, the estimation of linkage disequilibriums and the calculation of genome-wide identity-by-state, haplotypic tests, and permutation analyses.

to increase the computational throughput, a user can partition their data into sets, and perform the analysis of the sets across a network of computers; a concept known as parallel and/or distributed computing. however, performing such analysis requires high levels of computer expertise. the user needs sufficient programming skills to partition and distribute data, control and monitor tasks across the computers, and merge output files. occasionally, a data set may fail to be processed, e.g. if the user did not partition the data into small enough subsets to be processed on a particular machine. also, the outputs from the computers may be scattered and their order hard to follow.

parallel computing is an intuitive, and powerful, method for increasing computational throughput. a task is separated into smaller tasks, and each is processed independently, in parallel, using multiple central processing units  or a cluster of computers. the outputs from each task must later be merged  <cit> . a general architecture for parallel computing is shown at figure  <dig>  most tasks solved in gwa analysis are suitable for parallelization, due to their computational independency, with parallelization achieved at the data level. for example, association tests can usually be done separately for each snp and/or a small group of snps. consequently, parallelization is a beneficial way to reduce the computing time, with few overheads incurved in large-scale gwa analyses.

several attempts had been made to parallelize genetic association analyses. grid engine, a cutting-edge parallel tool, can schedule parallel tasks involving genetic association analysis programs  <cit>  such as fbat  <cit>  and unphased  <cit> . the approach, first proposed by mishima et al., is based on non-parallel code combined through process-based parallelization. the downside is that the user still needs to monitor when each task is finished, and when the outputs from all the tasks can be merged. moreover, each process may take a very long time to finish, and load balance can be problematic. a granularity problem  may occur, but higher power compute-nodes or code parallelization are possible solutions. the r/parallel package has been used to automate loop parallel execution, but the application must run on a single computer with multi-core processors, and does not currently support cluster computing  <cit> . its inclusion would allow the computing time limit of the package to be eliminated. misawa and kamatani  <cit>  developed the parahaplo package for haplotype-based whole-genome association studies using parallel computing. it is aimed at correcting multiple comparisons in multiple snp loci in linkage disequilibrium. there are other statistical analyses requirements in gwa studies, such as obtaining statistics for a particular snp or a trait, association test, characterizing an individual in the study, and pair-wise statistics between individuals. furthermore, ma et al.  <cit>  developed episnpmpi, a parallel system for epistasis testing in large scale gwa analysis.

rmpi  <cit>  is an r library which provides various functions to parallelize tasks on r using the mpi   <cit> . rmpi employs various functions to manage flow analysis in parallel environment, and is applicable for employing multi-core cpus distributed across many computers, not only multi-core cpus on a single computer. however, it is difficult, if not impossible, for a non-programmer to write a parallel rmpi program. therefore, sprint  <cit>  was developed to implement parallel r functions. although users can use sprint easily, it does not specifically support gwa studies.

in this article, we present the development of our parallabel library, a new r library for parallelization of gwa studies based on rmpi. parallabel aims to speed up the computation of gwa studies for various statistical analysis requirements and also simplify analysis parallelization. with parallabel, the users do not need to be experts programming on partitioning and distributing data, controling and monitoring tasks, and merging output files.

implementation
gwa function grouping
statistical analyses in gwa studies can be categorized into four groups based on the nature of the statistics computed and type of data used. these four groups can be parallelized in distinct ways. table  <dig> shows the name and description of the genabel function in each group. the first group contains statistics computed for a particular snp, or a trait, such as the snp characterization statistics , produced by genabel's summary.snp.data or association test statistics . the second group holds statistics characterizing an individual in the study, such as, summary statistics of genotype quality for each sample . the third group consists of pair-wise statistics derived from analyses between each pair of individuals in the study, including genome-wide identity-by-state and genomic kinship analyses. this is one of the most computationally intensive analyses, obtained through genabel's ibs function. the final group concerns pair-wise statistics derived for pairs of snps, such as linkage disequilibrium characterisation . while the number of snp pairs is generally very large, analyses are usually limited by their pair-wise physical distance, making them less demanding than pair-wise individual analyses, such as ibs computations.

the name and description of function of genabel in each group

we have developed the parallabel library to parallelize the serial functions of these groups using rmpi library. the four implementation groups are named type1_parall_by_snps for the first group, type2_parall_by_individuals for the second group, type3_parall_by_pairs_of_individuals for the third group and type4_parall_by_pairs_of_snps for the fourth group.

data partitioning
an advantage of parallabel is usage simplicity, hiding otherwise tedious scripts for file management monitoring tools. these functions not only partition input data with automatic load balancing, but also gather output from each processor automatically. load balancing is critical because an unbalanced work load will result in higher loads for particular processors, which eventually undermines the overall performance.

the input data of type1_parall_by_snps contains snps equally partitioned into p subsets . if the number of snps is m, the number of snps in a subset is:  

if there are m snps and  <dig> processors, the snps will be partitioned into  <dig> smaller subsets. each containing m/ <dig> snps as shown in figure  <dig>  however, the last subset to be generated may contain more snps than others, caused by integer division. for example, if there are  <dig> snps and  <dig> processors, subset  <dig> to subset  <dig> will contain  <dig> snps, but subset  <dig> will have  <dig>  the snps in each subset will execute on separate processors.

the input data for type2_parall_by_individuals consists of individuals, partitioned like type1_parall_by_snps

the input data for type3_parall_by_pairs_of_individuals is a pair of individuals, and performs a more complicated partitioning than type1_parall_by_snps and type2_parall_by_individuals. the data is divided until the number of processors is equal to, or less than, the number of subsets for load balancing on each processor. if the number of processors is equal to the number of subsets, then each processor executes an individual pair of each subset. if the number of processors is less than the number of subsets, then each processor executes an equal number of individual pairs . figure  <dig> shows type3_parall_by_pairs_of_individuals with n individuals. the statistics is calculated from the cross operation of an individual in a row with an individual in a column. the input data is partitioned into  <dig> subsets using the data partitioning shown in figure 3a. however, if the number of processors is more than  <dig>  the subsets are partitioned again. subset  <dig> and subset  <dig> are split into  <dig> subsets during the first stage of the data partitioning, while subset  <dig> and subset  <dig> are divided into  <dig> subsets by row, as shown in figure 3b. there are  <dig> subsets altogether in the second stage of the data partitioning.

the snps input of type4_parall_by_pairs_of_snps will be partitioned in a similar way to type3_parall_by_pairs_of_individuals.

implementation
the workflow for gwa analysis on a single processor or computer is presented in figure 4a. this workflow runs. the genotype and phenotype data is processed by the genabel library, which works under the r program. genabel sequentially processes the raw data, producing statistical data as its outputs.

this sequential workflow may take a very long time to produce some demanding statistical analyses. our novel parallel workflow for producing statistical data in gwa studies is shown in figure 4b, and can save computing time. the genotype and phenotype data is passed for distribution to the sun grid engine, a job scheduler. it queues jobs and assigns them to processors in a cluster. lam/mpi   <cit>  has various functions which can be called by rmpi to parallelize r. parallabel parallelizes genabel using this rmpi library. the statistical data from this workflow has been validated by comparing it with the outputs from the non-parallel approach. parallabel runs not only on linux cluster, such as the rocks cluster distribution, but also on any operating system that supports r and lam/mpi or open mpi, such as the unix and solaris operating systems. it can also run on computer clusters lacking the sun grid engine by executing immediately. however, the administrator will normally not allow a user to run a parallel program without utilizing a queuing process from the sun grid engine.

to parallelize gwa studies, parallabel running on the frontend-node partitions input data into smaller subsets so that tasks can be fairly distributed among the processors. it sends tasks to idle processors on compute-nodes. when the computation on a compute-node has finished, the frontend-node will send another task to the idle processor - a cycle that continues until all the tasks are completed, which is known as the 'task pull' method  <cit> . when all the tasks are finished, the frontend-node automatically merges all the outputs.

users can use parallabel to parallelize genabel gwa functions as easily as using genabel for sequential analyses. an example of the mlreg.p command sequentially on a processor is shown in figures 5a and 6a. the executing command that parallelizes mlreg.p to run on multiple processors using type1_parall_by_snps is shown in figures 5b and 6b.

RESULTS
our computer cluster, hanuman, runs rocks cluster distribution version  <dig> , which includes the sun grid engine version  <dig>   <cit> . the cluster consists of  <dig> ibm servers xseries 336s, comprising of a frontend-node and four compute-nodes. all servers have  <dig> single-core intel xeon  processors and  <dig> gb ram. the frontend-node and all the compute-nodes are connected through an ethernet switch, and the user can connect via the internet. the cluster provides lam/mpi version  <dig> . <dig>  r program version  <dig> . <dig>  rmpi library version  <dig> - <dig>  and genabel version  <dig> - <dig>  which are utilized as components by our parallabel library.

the north american rheumatoid arthritis consortium  data is part of a dataset employed to observe associations between disease and variants in the major-histocompatibility-complex locus  <cit> . the narac genotype data contains  <dig>  snps from  <dig>  individuals. the data was used to measure the performance of parallabel by employing  <dig> individuals for cases, and  <dig>  individuals as controls.

trace results from type1_parall_by_snps, type2_parall_by_individuals, type3_parall_by_pairs_of_individuals, and type4_parall_by_pairs_of_snps for the narac data are shown in figure  <dig>  type1_parall_by_snps was executed by the genabel mlreg function, type2_parall_by_individuals was executed by the genabel hom function, type3_parall_by_pairs_of_individuals was executed by the genabel ibs function, and type4_parall_by_pairs_of_snps was executed by the genabel r2fast function.

parallabel reduced the computing time for type3_parall_by_pairs_of_individuals, especially with  <dig> processors. the type3_parall_by_pairs_of_individuals executing speed on eight processors was approximately seven times faster than on one processor. on a single processor, the complete analysis took  <dig>  hours, but only  <dig>  hours with  <dig> processors. the computing time for type1_parall_by_snps also tends to be like that for type3_parall_by_pairs_of_individuals.

the computing time for the sequential version of type2_parall_by_individuals can be very short . while the parallel version took longer , due to the overhead of data partitioning, data distribution, and data merging. data distribution can be time consuming because the data must be saved on the frontend-node before the compute-nodes can load it, and the frontend-node must also speed time communicating with the compute-nodes. in addition, genabel is tailored to quickly retrieve subset of snps, as this is a typical gwa scan procedure, but is much less efficient in retrieving subsets of individuals, which is less typical. thus the overhead of data partitioning in subsets of individuals prevailed over the gain achieved by parallel processing. these results highlighted a place where genabel data storage and processing is ineffective, and we are currently working on better algorithms to do by-individual analyses.

type4_parall_by_pairs_of_snps was executed by the genabel r2fast function. a single processor can not pass all the snps in the narac data to r2fast due to cpu memory limitations so, the analysis was done separately for each chromosome. even then, a single processor can not call r2fast with a chromosome with more than  <dig>  snps, which affects  <dig> chromosomes in the data. however, parallabel can run r2fast with a chromosome with more than  <dig>  snps by employing a set of processors. the chromosome data is automatically partitioned based on the number of snps, as shown in table  <dig>  if the number of snps for a chromosome is between  <dig> and  <dig> , then the data is partitioned into at least  <dig> balanced subsets. if the number of the snps is between  <dig>  and  <dig> , then the data is divided into at least  <dig> balanced subsets. if the number of snps is between  <dig>  and  <dig> , then the data is split into at least  <dig> balanced subsets. the data is automatically partitioned until the number of processors is equal to, or less than, the number of subsets for load balancing on each processor. the trace example results for type4_parall_by_pairs_of_snps of narac data are shown in figure  <dig> 

the least number of subsets of each chromosome partitioned by the number of snps

type4_parall_by_pairs_of_snps took only  <dig>  days to execute on eight processors, indicating that time-saving with parallabel is linearly correlated to the number of nodes. this suggests that with more snps, more computing time will be saved by parallabel.

if the number of available processors is p, the parallel computing time for p processors is time_p_cpus, and the serial computing time for a processor is time_a_cpu; the overhead for p processors is:  

different numbers of processors produce different overheads depending on data partitioning, network communicating, and data merging. however, the overheads can be predicted based on the overhead of eight processors shown in figure  <dig>  the computing time on a large cluster for type1_parall_by_snps, type3_parall_by_pairs_of_individuals and type4_parall_by_pairs_of_snps extrapolated from figure  <dig> applying the above overhead equation are shown in figure  <dig>  it is clear that parallabel also saves the computing time on a large cluster. in addition, the time-saving rates for these types are much increased when the number of processors is between  <dig> and  <dig>  nevertheless, the time-saving rates are slowly increased when the number of processors is greater than  <dig>  this applies to the particularly and relatively small data set analyzed here. with bigger data sets, the time-saving rates can be larger. however, the user should optimize the number of processors according to the gain in computational throughput.

discussion and 
CONCLUSIONS
we have presented the parallabel library which employs parallel computing to reduce computing time for data intensive tasks. parallabel can run on clustered computers that support lam/mpi and r. with clustered computers, processors or even personal computers can be easily added as new compute-nodes. parallabel runs on both distributed and shared memory architectures as it was developed with mpi. for a distributed memory architecture, mpi usually uses a computer network for task communications. for a shared memory architecture, mpi does not employ the network for task communications. this means that a distributed memory architecture may exhibit more overhead than a shared memory architecture . in our experiments, type1_parall_by_snps took only  <dig> minutes to execute on a shared memory architecture but  <dig> minutes on a distributed memory architecture. the overhead of a shared memory architecture was tested on a server, which has  <dig> quad-core intel xeon  processors and  <dig> gb. the server runs on centos version  <dig> , and provides open mpi version  <dig> . <dig> 

parallabel allows the user to specify the number of processors employed for data execution. we expect computational performance to increase linearly with the number of processors when using type1_parall_by_snps, type3_parall_by_pairs_of_individuals, and type4_parall_by_pairs_of_snps. in addition, parallabel is faster than genabel on one processor. computing times for type3_parall_by_pairs_of_individuals and type4_parall_by_pairs_of_snps are longer than those for type1_parall_by_snps because the input data consists of pairs of individuals and snps respectively, which are much larger than the snps input for type1_parall_by_snps. in addition, if the number of snps is n, then the number of inputs for type1_parall_by_snps is n but the number of inputs data for type4_parall_by_pairs_of_snps is n*n. parallabel can save much more computational time when utilizing type3_parall_by_pairs_of_individuals and type4_parall_by_pairs_of_snps than when using type1_parall_by_snps. therefore, as the amount of input data increases, the time saved by parallabel also increases. parallabel does not only reduce the computing time but also is as easy-to-use as the more conventional genabel.

parallabel can not reduce the computing time when the data size is too small, such as the result shown when employing the hom function of type2_parall_by_individuals, because the computing time is too short. in that case, the overheads of data partitioning and output merging overwhelm the computational performance.

availability and requirements
• project home page:

http://www.sci.psu.ac.th/units/genome/cgbr/parallabel/index.html

http://parallabel.r-forge.r-project.org/

• operating system: platform independent

• programming language: r

• other requirements: lam/mpi or open mpi, rmpi, genabel

• license: gpl for non-profit organizations

• any restrictions to use by non-academics: license needed

authors' contributions
all authors conceived and designed the project. pt provided the hanuman cluster. us, sm and ysa implemented the software. us conducted computational performance evaluation. all authors drafted, read, revised, and approved the manuscript.

