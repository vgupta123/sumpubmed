BACKGROUND
this study addresses a major problem raised from a previous feasibility study  <cit>  of a high-throughput in silico vaccine discovery pipeline for eukaryotic pathogens. a typical in silico pipeline output is a collection of different protein characteristics that are predicted by freely available bioinformatics programs  <cit> . these protein characteristics  represent potential evidence from which a researcher can make an informed decision as to a protein’s suitability as a vaccine candidate. the problem is that this evidence can be in different formats, contradicting, and inaccurate culminating in large numbers of false positive and negative decisions. the current solution is to accept that candidates will inevitably be missed due to the nature of an in silico approach and to rely on the laboratory validation to identify false candidates. the study herein focuses on how to reduce the false error rates using a computational approach.

eukaryotic pathogens are extremely complicated systems comprised of thousands of unique proteins that are expressed in multifaceted life cycles and in response to varying environmental stimuli. a desired aim of an in silico approach for subunit vaccine discovery is to identify which of these proteins will evoke a protective, yet safe, immune response in the host  <cit> . it is currently impossible, however, to know within an in silico environment how a host will truly respond to a single protein or combination of proteins. consequently, an in silico approach is not an attempt to replace experimental work but is a complementary approach to predict which proteins among thousands are worthy of further laboratory investigation. vaccine discovery tools have been developed for prokaryotes  <cit> , though, there is no in silico pipeline available to the public for eukaryotic pathogens and no clear consensus as to what type of protein constitutes an ideal subunit vaccine. currently, the characteristics of proteins guaranteed to induce the desired immune response are poorly defined. nevertheless, some protein characteristics which are considered relevant to vaccine discovery are sub-cellular location; presence of signal peptides, transmembrane domains, and epitopes  <cit> .

the poor reliability of the in silico output arises because an unknown percentage of the in silico input  are acknowledged incorrect or missing. bioinformatics programs used to predict protein characteristics are, in general, inaccurate  <cit> . the inaccuracy can be a consequence of erroneous input data or overly simplistic algorithms, or simply due the complexity of the problem being solved. since most prediction programs are imprecise, it can be expected that a percentage of the predicted protein characteristics will be incorrect. the difficulty encountered by a program user is to ascertain which of these predictions are correct and can contribute to the collection of evidence that supports a protein’s vaccine candidacy.

given an in silico output, we propose that supervised machine learning methods can accurately classify the suitability of a protein, among potential thousands, for further laboratory investigation. applying machine learning algorithms to solving biological problems is not novel. however, applying them to classify eukaryotic proteins for vaccine discovery is novel and this is reflected by the presence of only a few publications on the topic  <cit> . we illustrate the proposal on an in silico output comprising evidence from proteins experimentally shown to induce immune responses  and hence expected to be likely vaccine candidates.

RESULTS
five datasets  containing evidence profiles were used in various ways to test the classification of a protein as either a vaccine candidate  or non-vaccine candidate . these evidence profiles for proteins from toxoplasma gondii, neospora caninum, plasmodium sp., and caenorhabditis elegans, were compiled from the output predictions made by seven bioinformatics programs .

name
a
b
athis is the name used to refer to the dataset throughout the paper.

bproteins  were initially grouped in accordance with the subcellular location descriptor in uniprotkb, then fine-tuned in accordance to cross-validation testing, epitope presence, and reference to other uniprotkb annotations and gene ontology. benchmark proteins were taken from published studies .

ccombination of proteins from membrane-associated, secreted, and unknown subcellular locations.

note: membrane-associated and secreted proteins are expected ‘yes’ classification for vaccine candidacy. neither membrane-associated nor secreted proteins are expected ‘no’ classification. there was an attempt to create an equal representation of yes and no classifications in the training datasets.

a
apredictive accuracies taken from publications by the creators of the programs. the prediction accuracy varies for different target pathogens.

bsignalp version  <dig> .

cprediction tools from the immune epitope database and analysis resource  .

darea under curve value . program uses different methods. for mhc i best method = artificial neural network   <cit>  and mhc ii best method = consensus  <cit> .

a typical profile is a mixture of data types corresponding to an accuracy measure, a perceived reliability, or a type of score for the protein characteristic being predicted , and program algorithm itself impeding precise prediction. this is irrespective of the target pathogen. the key question to be answered is whether we can classify potential vaccine candidates based on evidence profiles with hidden inaccuracies.

contents of evidence profiles
the columns in the evidence profile are as follows:  <dig> = uniprot id.  <dig> = number of predicted transmembrane helices .  <dig> = a ‘y’ or ‘n’ to indicate a predicted signal peptide  – a ‘y’ is more likely to be a secreted protein.  <dig> = probability of a secretory signal peptide .  <dig> = probability of a secretory signal peptide .  <dig> = predicted localisation based on the scores: m = mitochondrion, s = secretory pathway, u = other location .  <dig> = reliability class  – from  <dig>  to  <dig>  and is a measure of prediction certainty .  <dig> = expected number of amino acid residues in transmembrane helices  .  <dig> = expected number of residues in the transmembrane helices located in first  <dig> amino acids of protein. the larger the number the more likely the predicted transmembrane helix in the n-terminal is a signal peptide .  <dig> = number of predicted transmembrane helices .  <dig> = number of nearest neighbours that have a similar location .  <dig> = predicted subcellular location  .  <dig> = probability score encapsulating the collective potential of t-cell epitopes on protein with respect to vaccine candidacy . raw affinity scores derived from iedb peptide-mhc i binding predictor.  <dig> = probability score encapsulating the collective potential of t-cell epitopes on protein with respect to vaccine candidacy . raw affinity scores derived from iedb peptide-mhc ii binding predictor.  <dig> = expected ‘yes’ or ‘no’ vaccine candidacy .

classifying with one individual piece of evidence
the first test was to determine whether proteins could be correctly classified using an individual piece of evidence . figure  <dig> shows an example of how the test was applied. the sensitivity and specificity of the classification is shown in table  <dig>  the most notable observation is that non-vaccine candidates are predominantly correctly classified but the main trade-off is a substantial number of false negatives, as evidenced by the low sensitivity scores. the conclusion here is that there is no one individual input variable that can precisely determine the classification. this is not an unexpected result because each input variable represents only one particular protein characteristic and there is currently no one characteristic that conclusively epitomises a vaccine candidate.

input variable
a
b
c
t. gondii
plasmodium
c. elegans
abbreviations: sn = sensitivity; sp = specificity; t. gondii = toxoplasma gondii; plasmodium = species in the genus plasmodium including falciparum, yoelii yoelii, and berghei; c. elegans = caenorhabditis elegans; benchmark = dataset comprising evidence for t. gondii and neospora caninum proteins from published studies.

ainput variable = predicted protein characteristic .

btype = prediction type: transmembrane domains , secretory signal peptide , sub-cellular location , peptide-mhc binding .

cdata = data type: discrete , continuous , text .

the values underlined denote the best performing input variable for classifying the published proteins.

test criteria on input variable for binary classification:

phobius_tm: yes if number of transmembrane domains >  <dig> else no.

phobius_sp: yes if = ‘y’ else no.

signalp: yes if >  <dig>  else no.

targetp_sp: yes if >  <dig>  else no.

targetp_loc: yes if = ‘s’ else no.

tmhmm_aa: yes if >  <dig> 18$$ else no.

tmhmm_ first60: yes if > 10$$ else no.

tmhmm _tm: yes if number of transmembrane domains >  <dig> else no.

wolf_psort: yes if > 16$$ else no.

wolf_psort_annotation: yes if = ‘membrane’ or ‘secreted’ else no.

mhci: yes if >  <dig>  else no.

mhcii: yes if >  <dig>  else no.

$$a value recommended by the creator of the program.

 specificity=truenegativestruenegatives+falsepositives 

 sensitivity=truepositivestruepositives+falsenegatives 

classifying with a rule-based approach
the next test was to determine if a combination of two or more input variables could efficiently perform the vaccine classification by applying an appropriate rule. figure  <dig> illustrates the rule-based approach. a total of  <dig> combinations were tested with a programmed trial and error approach to obtain the maximum sensitivity and specificity. table  <dig> shows the best rule from each combination. the best result achieved when tested on the benchmark dataset was  <dig>  and  <dig>  for sensitivity and specificity respectively. there were two main observations made from the rule-based testing: a rule that works well with one dataset does not necessarily generalise to another, and it is difficult to strike the ideal balance between sensitivity and specificity. for example, judicious adjustments to the rule threshold values can capture all proteins classified ‘yes’ in a test dataset  but at the expense of more false positives. furthermore, if this adjusted rule is then applied to another dataset there are still false classifications. the conclusion here is that it is not feasible to compose a universal set of rules applicable to all datasets for the purpose of classifying proteins.

abbreviations: sn = sensitivity; sp = specificity.

note: in benchmark dataset, number of yes classifications = 70; number of no classifications = 70; total number =  <dig> 

classifying with machine learning algorithms
seven, popular, supervised machine learning algorithms were used in an attempt to improve on the rule-based approach. table  <dig> shows the sensitivity and specificity performance measures of the binary classification. the five datasets were used interchangeably for both training and testing. the table is presented as a matrix with training datasets in columns and test datasets in rows. for example, t. gondii dataset is used to build the decision tree model and tested on the benchmark dataset. included in the matrix are classification results from cross-validation, which are expected to approach  <dig>  . cross-validation results that greatly differ from  <dig>  suggest there is at least one problematic evidence profile. the combined species dataset is the combination of the t. gondii, plasmodium, and c. elegans datasets. the results, therefore, are positively biased when the combined species dataset is used for training and testing on datasets other than the benchmark. similarly, testing on the combined species dataset with species-specific trained models is also positively biased. the main benchmark for the algorithm comparison is the classification of the benchmark proteins using the combined species to train the model.

t. gondii
plasmodium
c. elegans
a
b
b
b
b
b
b
b
b
b
b
a
b
b
b
b
b
b
b
b
b
b
a
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
a
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
abbreviations: sn = sensitivity; sp = specificity; t. gondii = toxoplasma gondii; plasmodium = species in the genus plasmodium including falciparum, yoelii yoelii, and berghei; c. elegans = caenorhabditis elegans; combined species = combination of t. gondii, plasmodium, and c. elegans datasets; benchmark = dataset comprising evidence for t. gondii and neospora caninum proteins from published studies.


a
results from the same input data fluctuate. the algorithm-specific r functions were executed  <dig> times and the prediction outcomes  were averaged to calculate sn and sp.


b
obtained from multiple cross-validations i.e. the algorithm-specific r functions randomly used 70% of the training dataset to build a model and the remaining 30% was used in the binary classification test. the cross-validation was executed  <dig> times and the prediction outcomes were averaged to calculate sn and sp.

the values underlined denote the best performing training dataset for classifying the benchmark proteins.

in summary, the best benchmark performing algorithm  is naïve bayes; then adaptive boosting; followed jointly by random forest and support vector machines ; then neural networks, k-nearest neighbour, and finally decision tree. with the exception of decision tree, the difference in performance is so minimal that the ranked performance here could easily change given different training and test datasets and/or fine-tuning of the algorithm parameters. ultimately, there was no apparent difference between the algorithms with respect to solving this specific problem of classifying evidence profiles.

factors affecting performance of machine learning algorithms
it is the content of the training dataset and in particular the number of problematic profiles in both the training and test datasets that have the greatest impact on the performance of the algorithm. certain profiles are more problematic than others for some algorithms to classify and tend to be consistently misclassified. the t. gondii trained model performed the poorest when tested on the benchmark proteins irrespective of the algorithm used. it is tempting to assume that the poor performance from the t. gondii trained model was due to a misclassification of the target input variable for some of the evidence profiles. however, there are two other proposed reasons for this inaccuracy: the training dataset contains the least number of evidence profiles , but more importantly it contains three labelled profiles with questionable evidence . cross-validation is a useful indication that a particular profile is problematic. problematic profiles, both in the training and test datasets, tend to contain ambiguous evidence which can cause the algorithm to make an unexpected classification. based on cross-validation, the t. gondii data contained the most problematic profiles for all algorithms, followed by plasmodium, benchmark and c. elegans datasets. removing problematic profiles improves performance in cross-validation. it is therefore tempting to remove these problematic profiles from the training datasets for deployment but their removal negatively impacts performance. the motivation behind using the machine learning algorithms is to overcome the effects of erroneous evidence that is currently inherent in the in silico vaccine discovery output. consequently, the training data should retain problematic profiles for building models for deployment. they need to be retained in the application of the model because it is unclear whether these problematic profiles are incorrect or whether they are correct but rare . new profiles for classification are expected to contain an unknown percentage of similar erroneous evidence. algorithms vary in their ability to handle problematic profiles according to what other profiles are represented in the training dataset. for example, the combined species trained model is a collection of exactly the same profiles as those in the individual species trained models. however, the algorithms when trained with the combined species are able to correctly classify the problematic profiles more effectively than individual species trained models.

the results in table  <dig> show that there is no fundamental difference between evidence profiles from different eukaryotic species. for example, the benchmark dataset is composed of t.gondii and n. caninum data and yet both the plasmodium and c. elegans trained models outperformed the t. gondii trained model. the ideal training dataset for the classification problem described herein is one that contains the most variety of evidence profiles irrespective of the source species.

none of the algorithms can consistently classify evidence profiles without false predictions irrespective of the training dataset. each algorithm nonetheless performed better than the rule-based approach with a collective average sensitivity and specificity of  <dig>  and  <dig> . the main reason why the machine learning algorithms performed better than the rule-based approach in this study is related to how they handle erroneous evidence. for example, a classification rule, applied to a combination of input variables, fails when only one input variable is erroneous. machine learning algorithms, despite erroneous evidence in both the training and test datasets, can still exploit a generalised pattern within the collection of evidence for the purpose of classification.

a proposed classification system
the proposed classification system  uses the ensemble of classifiers, excluding the decision tree, to make a final classification based on voting and a majority rule decision from predictions of the individual classifiers. in the case of a tied vote, the decision is deemed a yes classification. the logic behind this decision is that false positives are preferential to false negatives as they can be identified later during the laboratory validation. table  <dig> shows the uniprot identifier for proteins from the benchmark dataset that were consistently incorrectly classified by the machine learning algorithms. at least one of the six algorithms failed to correctly classify six proteins  that were expected to be yes and three proteins  expected to be no. table  <dig> provides a description of these misclassified proteins. after applying the majority rule approach, all proteins were classified as expected. the final predicted classification of protein q <dig> was yes based on a tied decision. there are three possible reasons why a protein in the final classification process might be misclassified: 1) the expected classification is incorrect, 2) the majority of algorithms fail, and 3) the evidence profile is too problematic. the misclassifications in table  <dig> suggest that they were mainly due to the failure of a particular algorithm when considering the successful classification by other algorithms. the evidence profiles for q <dig> and b9prx <dig> are possibly problematic for the algorithms that made the misclassification. this is most likely because the algorithms have not been trained for a profile of this type i.e. the training dataset is failing. in this case , false positives can only be identified in the laboratory. interpreting the relationship between evidence profiles and an immune response in host remains a challenge to the in silico vaccine discovery approach.

protein identifiers e.g. q <dig> are uniprot ids. refer to additional file  <dig> for a description of the protein and its relevance as a vaccine candidate.

a
b
c
afinal classification takes into account predictions from each algorithm and the most frequent classification type is used i.e. a majority rule approach. a yes classification is adopted for tied votes e.g. q <dig> 

balgorithms are executed multiple times on the same input data. an in-house perl script summarises the multiple runs and indicates the number of times  the predicted classification of protein differs from the expected. proteins are regarded as misclassified if the number of times = 100%.

ccolumn headers: 1 = id, 2 = phobius_tm, 3 = phobius_sp, 4 = signalp, 5 = targetp_sp, 6 = targetp_loc, 7 = targetp_rc, 8 = tmhmm_aa, 9 = tmhmm_first <dig>  10 = tmhmm_tm, 11 = wolf_psort, 12 = wolf_psort_annotation, 13 = mhci, 14 = mhcii, 15 = expected classification.

abbreviations: ab = adaptive boosting, rf = random forest, svm = support vector machines, nb = naive bayes, knn = k-nearest neighbour, nn = neural network.

future developments
the outcome of the classification system is a list of proteins that are worthy of laboratory investigation. each protein in the list is assumed to have an equal chance of being a vaccine candidate. an improvement to the proposed classification system is to score the proteins according to a likelihood or confidence level that the classifications are correct. the r functions for svm and random forest support class-probabilities i.e. an estimated probability for each protein belonging to ‘yes’ and ‘no’ classes. for such an extension, the format of the training datasets are the same except the target value would no longer be a ‘yes’ or ‘no’ but a single probability score that attempts to encapsulate each snippet of evidence representing the evidence profile. determining such a score is a challenge that still remains. the advantage of an appropriate scoring system is that the proteins in the vaccine candidacy list can then be ranked. a caveat here is that the ranking is based on a confidence level of prediction. a protein with a high probability score does not necessarily imply a high probability of an immune response when injected in a host.

the proposed classification system is intended to illustrate a framework on which researchers can build more efficient systems. for example, only seven high-throughput prediction programs were used here to create the evidence profiles. there are other bioinformatics programs  <cit>  that could be used to predict similar or additional protein characteristics from protein sequences, such as gpi anchoring, molecular function, and biological process involvement. at the time of writing, there is no high-throughput standalone gpi predictor. appropriate values that support vaccine candidacy could be extracted from these extra program outputs and added to the evidence profile as additional columns in the training datasets.

there are examples of proteins with annotated interior subcellular locations that have been observed to induce an immune response  <cit> . it is assumed here that these proteins are not naturally exposed to the immune system but were exposed as a consequence of experimental conditions. nevertheless, the important point here is that they do induce an immune response and are potential vaccine candidates. these interior proteins are missed by the current proposed classification system. all protein types that induce an immune response in theory need to be addressed to create a totally encompassing system for in silico vaccine discovery. this can only be accomplished if distinguishing characteristics that exemplify antigenicity can be predicted given proteins sequences. a prediction program that distinguishes antigenic and non-antigenic interior proteins is sought.

CONCLUSIONS
we conclude the following when given a high-throughput in silico vaccine discovery output consisting of predicted protein characteristics  from thousands of proteins: 1) machine learning algorithms can perform binary classification  for these proteins more accurately than human generated rules; 2) there is no apparent difference in performance  between the algorithms; adaptive boosting, random forest, k-nearest neighbour classifier, naive bayes classifier, neural networks, and svm, when performing this particular classification task; 3) none of the algorithms can consistently classify evidence profiles without false predictions using the training datasets in this study; 4) there is no fundamental difference in patterns in evidence profiles compiled from different species e.g. a model trained on one species can classify proteins from another and hence no target specific training datasets are required; 5) an ideal training dataset is one that contains the most variety of evidence profiles irrespective of the source species e.g. quality and variety are indisputably the most important factors that impact the accuracy of algorithms; and 6) a pool of algorithms with a voting and majority rule decision can perform classification with a high degree of accuracy e.g. 100% sensitivity and specificity was demonstrated in this study by correctly determining the expected classification of the benchmark dataset.

vaccine candidates from an in silico approach can only be truly validated in a laboratory. there are essentially two options. one is to rely on laboratory validation to identify false candidates. the other is to use our proposed classification system to identify those proteins more worthy of laboratory validation. this will ultimately save time and money by reducing the false candidates allocated for validation.

