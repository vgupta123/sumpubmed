BACKGROUND
principal component analysis  or its equivalent singular-value decomposition  is widely used for the analysis of high-dimensional data. for such gene expression data with an enormous number of variables, pca is a useful technique for visualization, analyses and interpretation  <cit> .

lower dimensional views of data made possible, via the pca, often give a global picture of gene regulation that would reveal more clearly, for example, a group of genes with similar or related molecular functions or cellular states, or samples of similar or connected phenotypes, etc. pca results might be used for clustering, but bear in mind that pca is not simply a clustering method, as it has distinct analytical properties and utilities from the clustering methods. simple interpretation and subsequent usage of pca results often depends on the ability to identify subsets with nonzero loadings, but this effort is hampered by the fact that the standard pca yields nonzero loadings on all variables. if the low-dimensional projections are relatively simple, many loadings are not statistically significant, so the nonzero values reflect the high variance of the standard method. in this paper our focus on the pca methodology is constrained to produce sparse loadings.

suppose x is an n × p data matrix centered across the columns, where n and p are the number of observations and the number of variables, respectively. also, let sx = xtx/n be the sample covariance matrix of x. in pca, the interest is to find the linear combination zk = xvk, for k =  <dig>  ..., p, which maximizes   

with the constraints  and vk ⊥ vh for all h < k. pca can be computed through the svd of x. let the svd of x be   

where d is n × p matrix with th element di; the columns of z = ud = xv are the principal component scores, and the columns of the p × p matrix v are the corresponding loadings. the vector vk in  is the k-th column of v.

each principal component in  is a linear combination of p variables, where the loadings are typically nonzero so that pca results are often difficult to interpret. to get sparse loadings,  <cit>  proposed to use l1-penalty, which corresponds to the least-absolute shrinkage and selection operator .  <cit>  proposed to use the so-called elastic-net  penalty. however, lasso and en may not be satisfactory either, because it can still gives too many nonzero coefficients.  <cit>  proposed the smoothly-clipped absolute deviation  penalty for oracle variable selection. recently, in regression setting,  <cit>  proposed a new random-effect model using a gamma scale mixture, which gives various types of penalty, including the normal-type , cusped-type , and a new  unbounded penalty at the origin.  <cit>  showed that the new unbounded penalty can yield very sparse estimates that are better than lasso both in prediction and sparsity.

in this paper we use the random-effect model approach of  <cit>  for sparse pca ; the model gives unbounded gains for zero loadings at the origin, so it forces many estimated coefficients to zero. we improve the estimation further by shrinking the singular values from the svd of the data; the resulting procedure is called super-sparse pca . we provide some simulation studies that indicate that these spca methods perform better than existing ones, and illustrate their use using a cancer gene-expression dataset with  <dig>  genes. we also show how to modify the ordinary nipals algorithm  <cit>  to implement these methods computationally.

RESULTS
numerical studies
we first perform small simulation studies in order to assess the performance of the proposed sparse pca methods and compare them against other methods. we generate data matrix x =  where xi ∈ rn, as follows:      

where , ei ~ mvn, in is the identity matrix of order n and u and ei are independent for all i. this gives the true covariance matrix,  

where , Σ <dig> = ϕip- <dig> and jk is the k × k matrix of ones. here we consider cases  =  for n >p and  =  for n <p. based on  <dig> simulated data, we compare our new sparse pca method using the h-likelihood  with the lasso and en penalties for both spca and sspca methods. we also tried the scad method but the results are very similar to lasso, so we do not report results for scad.

from the svd of Σ we have the true first loading vector v <dig> = t. let  be the estimate of v <dig>  to evaluate the performance in estimation of the first loading vector, following  <cit> , we use the sine values of the angle between true loading and estimated loading as the measure of the closeness of two vectors, i.e.  

when v <dig> = , dist  =  <dig> 

the summary of estimation performance is given in table  <dig>  generally spca methods have much better estimation than the ordinary pca method. among spca methods, the condition-number constrained sspca method with hl is generally the best, although the improvement over the unconstrained method is not substantial. the improvement performance of hl over lasso and en is small when n >p, but it is substantial when n <p and the underlying signal is not very strong .

the median of dist and the median absolute deviation in parentheses.

pca*: pca using x*

to evaluate the performance in variable selection, in table  <dig> we report the percentage of selecting the true model , the median number of correctly estimated zeroes divided by the number of true zeroes  and incorrect zero estimates divided by the number of true nonzeroes . because it does not produce zeroes, the ordinary pca method never gets the true model and always gets  <dig> true negatives and  <dig> false negatives. the hl penalty outperforms the lasso penalty and generally better than the en penalty, particularly in identification of the true model. lasso identifies fewer true negatives compared to hl. the sspca methods with the hl and lasso penalties outperform the corresponding spca methods, but here again hl is better than lasso and en. the en performs worst when n <p and the underlying signal is not very strong .

percentages of selecting the true model, the median number of correct  <dig> divided by the number of zeroes and incorrect  <dig> divided by the number of non-zeroes.

pca*: pca using x*

finally, as a measure of the prediction power, we compute the test sample variance,  

where  is the estimated loading using data matrix  and xtest is the independent test data sets generated from  with same sample size n. the results are in table  <dig>  spca methods give better prediction power than the ordinary pca. except for the sspca method when n <p and the underlying signal is not very strong , six spca methods have similar prediction power.

the median of test variance with the median absolute deviation in parentheses.

pca*: pca using x*

analysis of nci data
in the analysis of microarray data it is often of interest to co-regulated genes, since they will point to some common involvement in molecular functions or biological processes or cellular states. pca is a useful tool for such analyses  <cit> ; since interpretation depends on comparing the relative sizes of the loading vectors, the sparse loadings in spca are much easier to interpret than ordinary pca. furthermore, the previous section also shows that spca has better estimation characteristics than the ordinary pca. for illustrations we consider the so-called nci- <dig> microarray data downloaded from the cellminer program package, national cancer institute http://discover.nci.nih.gov/cellminer/. only n =  <dig> of the  <dig> human cancer cell lines were used in the analysis, as one of the cell lines had missing microarray information. the cell lines consist of  <dig> different cancers and were used by the developmental therapeutics program of the u.s. national cancer institute to screen >  <dig>  compounds and natural products. the number of genes is p =  <dig> .

the proportion  of zero elements of first loading in nci data analysis.

to select the number of principal components, we use a permutation approach as follows. first, we randomly permute the expression values within each sample  of x to create permuted data xperm. then pca is performed on xperm to get the singular values . we perform p =  <dig> permutations, from which we can compute the p-values of the observed dk's. the number of principal components, k <dig>  is such that the p-value of dk's is less than  <dig>  when k ≤ k <dig> 

for nci data, we get k <dig> =  <dig> . the numbers of nonzero elements in the eight loading vectors  are given in table  <dig>  we also report the adjusted variance and cumulative adjusted variance as suggested by  <cit>  to get the explained variance properly when the principal component scores are correlated. note that the adjusted variance is equal to the variance in the ordinary pca because the principal components of pca are uncorrelated. despite the sparsity, in comparison with the ordinary pca, both of spca and sspca method give higher cumulative adjusted variance. in fact the sspca method gives extremely sparse results, with only  <dig> ,  <dig> and  <dig> nonzero loadings for the first  <dig> principal components, compared to  <dig> ,  <dig>  and  <dig>  for the spca method. up to the third principal component the latter has only slightly larger cumulative variance.

number of nonzero loadings and cumulative variance for different methods.

the top  <dig> most enriched biological process go terms and the associated p-values for the first three principal components from sspca.

comparative go analyses from the ordinary pca are given in the additional file  <dig>  we use the same number of top-ranking nonzero loadings as for the sspca, which are  <dig> ,  <dig> and  <dig> for the first  <dig> principal components, respectively. out of these, the number of overlapping probes between the sspca and pca are  <dig>   <dig> and  <dig>  these overlaps are substantially more  than expected under random re-arrangement. however, there is sufficiently large number of distinct probes in the two methods, so the go analyses could be different. the p-values from the sspca-based go analyses are more significant than those from the ordinary pca; this may be due to better estimation of the loadings, so that the sspca has better power than the ordinary pca in revealing biologically-important grouping of genes.

discussions and 
CONCLUSIONS
pca is one of the most important tools in multivariate statistics, where it has been used, for example, in data reduction or visualization of high-dimensional data. the emergence of ultra-high dimensional data such as in genomics, involving  <dig> s of variables but with only a few samples has brought new opportunities for pca applications. however, there are new challenges also, particularly on the interpretation of results. if we treat pca quantities such as the loading vectors as parameter estimates, the large-p-small-n applications typically produce very noisy estimates. this is obvious since the loading vectors are a statistic derived from the sample covariance matrix, and the latter is not well estimated.

it is well known that improved estimation can come by imposing constraints, and in this case sparsity constraint is natural. as pca scores capture some underlying biological processes, we do not expect every gene in the genome to be involved. out of possibly  <dig>  genes we can expect only a small fraction, probably less than  <dig> , to be involved in a cellular process. hence sparsity constraint can help in reducing the number of loading parameters to estimate.

imposing statistical constraints can be achieved by applying a penalty approach as used by the ridge regression or the lasso methods  <cit> . in this paper we have investigated a random-effect model approach using a gamma scale mixture, which leads to a class of penalties that includes the ridge and lasso penalties as special cases. one significant property is that it can produce unbounded penalties on the origin, which leads to stronger constraints and more sparse estimates. from our results it seems clear that the penalty approach alone is not able to yield sufficiently sparse pca for high-dimensional genomic data. additionally we also need the shrinkage on the singular values of the data matrix. in simulation studies we show that the proposed methods outperform existing methods both in estimation and model selection. hence we believe that the new spca methods are promising tools for high-dimensional data analyses.

for future works, it will be of interest to apply super-sparse technique in this paper to locally-linear methods of dimensionality reduction , partial-least squares  regression and classification methods , or other high-throughput data analysis method where dimensionality reduction is used .

methodology
nipals algorithm for pca
standard algorithms for svd  give the pca loadings, but if p is large and we only want to obtain a few singular vectors, the computation to obtain the whole set of singular vectors may be impractical. furthermore, with these algorithms it is not obvious how to impose sparsity on the loadings.  <cit>  described a nipals algorithm that works like a power method  for obtaining the largest eigenvalue of a matrix and its associated eigenvector. the nipals algorithm computes only a singular vector at a time, so it is efficient if we only want to extract a few singular vectors. also the steps are recognizable in regression terms, so the algorithm is immediately amenable to random-effect modification as needed to obtain the various spca methods proposed in this paper.

first we review the ordinary nipals algorithm: set the initial value of z <dig> as the first column of x, then

 <dig>  find 

 <dig>  normalize 

 <dig>  find z1: z <dig> ← xtv1

 <dig>  repeat steps  <dig> to  <dig> until convergence.

to obtain the second-largest singular value, first compute residual , then apply the nipals algorithm above by replacing x by x <dig> 

sparse pca via random-effect models
to impose sparseness on the pca loadings we first introduce the regression framework into step  <dig> of the nipals algorithm. denoting xj as the jth column of x, following  <cit>  we have  

where v1j is the jth element of the p ×  <dig> vector v <dig> , and ∊j is an error term. if z <dig> is assumed to be known, the ordinary least square  estimate for v <dig> is given by  

consider the penalized least-squares  estimation that minimizes   

where pλ is a penalty function. for example, pλ = λ|v1j| gives lasso,  gives ridge, and  gives en, where λ, λ <dig> and λ <dig> are tuning parameters. for the prediction the ridge-type penalty is effective and for sparse estimation the lasso-type penalty is recommended, so that en  <cit>  has been recommended as a compromise between the ridge and lasso methods.  <cit>  proposed to use en for sparse , but it gives less sparse estimates than lasso.

 <cit>  recently proposed the use of random-effect models to generate new penalty functions for sparse regression estimation. suppose that v1j is a random variable such that   

where θ is the dispersion parameter and uj follows the gamma distribution with a parameter w and density  

such that e =  <dig> and var = w. this model leads to a rather complex marginal distribution for v1j, characterized by parameter w and with density  

this model involves a computationally difficult integral, and its direct optimization is problematic due to the nonconvexity of -log fw, θ. to overcome these problems, first note that the random-effect model  can be written as   

where τj = ujθ and ej ~ n. this is the double hierarchical generalized linear model  <cit> . with the log link, we have an additive model  

this leads to the h-likelihood  of  <cit>    

where  

and fθ and fw are the density functions of u1j|uj and log uj, respectively. given , for the estimation of v <dig>   <cit>  proposed to use the profile h-likelihood  

where  solves dh/du =  <dig> 

 <cit>  showed that   

with , and the estimate of v <dig> can be found using the iterative weighted least squares  by solving   

using  and λ = ϕ/θ. in random-effect model approach, the penalty function pλ stems from a probabilistic model . as noted previously the proposed penalty pλ is nonconvex. however, by expressing the model for pλ hierarchically as  v1j|uj is normal and  uj is gamma, both models can be fitted by convex glm optimizations. thus, the proposed iwls algorithm overcomes the difficulties of a nonconvex optimization by solving two-interlinked convex optimizations  <cit> .

in general, the minimizer of the penalized least-squares  can be found using the iwls  with . the derivative  for lasso, scad and hl penalties are summarized in table  <dig>  when v1j =  <dig>  then  and the jth element of wλ is not defined.  <cit>  employed a perturbed random-effect estimate  for a small positive  δ = 10- <dig>  then,  is always defined. as long as δ is small, the diagonal elements of wλ,δ are close to those of wλ and the resulting estimates are nearly identical to those of the original iwls . in this paper, we report  =  <dig> when  <  <dig> .

other methods for sparse principal component analysis
 <cit>  also exploit the regression property of pca in order to obtain sparse loadings. they proposed an alternating minimization algorithm to minimize the criterion   

for deriving the first sparse loading vector v <dig>  given θ, this optimization problem becomes a naive elastic net problem for v <dig>  given v <dig> , θ can updated from svd of xtx v <dig>  these two steps are repeated until v <dig> converges. following  <cit> ,  is different from our objective function  even when we use the same penalty function. in fact,  is very close to the objective function of  <cit> , but we put the normalization constraint of the loading inside iterated procedure so that it could make a different result. in this paper, we used the function spca() in the r-package elasticnet for the en method in the simulation studies.

condition-number constraint for spca
as shown in the previous examples, the spca approaches above may not produce sufficient sparsity. for the moment suppose n ≥ p; the case where n <p can be dealt with by transposing the data; see the note below. from  we have the eigenvalue decomposition of the sample covariance matrix as  

where Λ = diag. let the p ×  <dig> random vectors x <dig>  …, xn be rows of x that have zero mean vector and true covariance matrix Σ with the non-increasing eigenvalues, λ <dig> ≥ ... ≥ λp. when our goal is to estimate Σ, the sample covariance matrix sx can be used. many applications require a covariance estimate that is not only invertible but also well-conditioned. an immediate problem arises when n <p, where the estimate sx is singular. even when n >p, the eigen-structure tends to be systematically distorted unless p/n is small  <cit> , resulting in ill-conditioned estimator for Σ.

 <cit>  showed that the eigenvalues of sx are more dispersed than those of the true covariance matrix, i.e. l <dig> tends to be larger than λ <dig> and lp tends to be smaller than λp. to overcome this difficulty,  <cit>  proposed a constraint on the condition number to achieve a better covariance estimation. the optimization problem with the condition-number constraint can be formulated as   

where a ≼ b denotes that b - a is positive semidefinite and t > <dig>  given κmax, for t  <cit>  proposed to use  

where α ∈ { <dig>  ... p} is the largest index such that 1/lα <t* and β ∈ { <dig> ..., p} is the smallest index such that 1/lβ >κmaxt*. their covariance estimators are   

where the eigenvalues . to estimate the shrinkage parameter κmax, they proposed to use the k-fold cross validation.

from  and , we can reconstruct x* with same singular vectors but shrunken singular values, i.e.   

where d* is n × p matrix with th diagonal element . thus, for condition-number constrained pca we use x* instead of the original data matrix x. as the procedure yields extremely sparse loading vectors, we call it sspca, for super-sparse pca.

 <cit>  considered the estimation of covariance matrix when p is not very large. however, for large p such as over  <dig>  in gene expression data, it becomes computationally too intensive. because the aim is to obtain a few singular vectors, not whole p singular vectors, when p >n in this paper we propose to apply the above algorithm to xt and the results are transformed back appropriately.

modified nipals algorithm for spca and sspca
for spca we replace step  <dig> in the nipals algorithm by  

where  is defined in . for sspca we also apply this modified step, but replace x by x* defined in .

tuning parameter selection
to complete the proposed algorithm we need to estimate the tuning parameters θ and λ = ϕ/θ in  and , respectively. first we note that from , marginally, v <dig> has mean zero and variance θ, so we use , where  is the estimated first loading vector from ordinary pca and  is the sample mean of . we use k-fold cross-validation for λ. following  <cit> , we select λ which maximizes the test sample variance  

where  is the estimated loadings from the kth training sets  and sx is the sample variance based on the kth validation set. for the numerical studies in section we use k =  <dig> 

authors' contributions
the first two authors  contributed equally to this work. yjl and yp conceived the study and wrote the manuscript, dhl and wjl performed data analysis and wrote the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
the supplementary report documents details on plot of the sspca scores, and gene ontology analysis of ordinary pca.

click here for file

 acknowledgements
this research is partially funded by a grant for the swedish science foundation.
