BACKGROUND
in the age of big data, there is an urgent need to develop effective and computationally efficient methods to convert data into knowledge. when we extract knowledge from big data, we often need to solve big optimization problems, which may involve thousands of, even millions of, decision variables. for example, in the “optimization of big data competition” organized at the ieee  <dig> congress on evolutionary computation, a set of big optimization benchmark problems were formulated, which have up to  <dig> decision variables extracted from eeg data analyses  <cit> .

in biology, techniques of high-throughput experiment such as dna microarrays bring the “big data” in molecular biology and system biology, which is made up with a large number of molecular entities and their products. finding the interactions between these entities is a key step to understand the governing mechanism of biological systems. the so-called gene regulatory networks, or grns in short, has been proved to be the most widely used model to analyze the dynamic behavior of biological systems. grns model the molecular entities as networks which consist of a group of nodes  which influence each other. and the objective of grns is to capture the interconnections among these genes. by reconstructing the complex interconnections within these grns, we can highlight inhibitory or excitatory interactions, as well as how intracellular or extracellular factors  affect gene products or deregulate cellular process. the reconstruction of a grn based on expression data is also called reverse engineering or network inference.

in recent years, various algorithms, especially evolutionary algorithms  have been proposed to infer grns by analyzing of gene expression data, such as genetic algorithm   <cit> , genetic programming   <cit> , evolution strategy   <cit> , and ant colony optimization   <cit> . however, the grns modeled by the above algorithms only consist of a limited number of genes. how to reconstruct the large scale grns is still an underdetermined problem in biology.

various models have been used to model grns. the simplest models are based on boolean networks. in the reverse engineering, boolean networks are used to infer both the underlying topology and the boolean functions at the nodes from observed gene expression data  <cit> .

in addition, continuous networks, an extension of the boolean networks, are also widely used to model grns. each gene is considered to be a node in the network, while the edge represents the relationship between genes. in biological systems, the activity level of genes is considered in a continuous range, continuous representation captures this feature, while the boolean model is not work. many methods based on continuous networks have been proposed to the inference of grns. such as linear regression based, and the mutual information  based methods. aracne algorithm  <cit> , the classical mi based method, calculate the value of mis of all gene pairs. if the calculated value exceeds the given threshold, then one regulatory interaction is inferred  <cit> .

many probabilistic graphical models have been proposed by researchers to measure the high-order dependency among distinct gene expression patterns. the bayesian network is one of the most popular methods used in the inference of grns. in the bayesian network, directed acyclic graphs are used to indicate the conditional dependency among random variables  <cit> .

despite that plenty of proposed models using gene expression data to infer grns  <cit> , these models only adopt scaled real values or boolean values to indicate the levels of gene expression  <cit> . fcms use fuzzy values, which integrate the benefits of both real values and boolean values, to figure out the real relationships in grns  <cit> . many researches  <cit>  have validated that fcms are efficient and powerful tool when it comes to modeling complex regulatory network systems.

thus, within this context, we focus on developing a new evolutionary approach framework to train grns, which is based on fuzzy cognitive maps, or fcms in short. as a type of effective computation tool, fuzzy cognitive maps  were proposed by kosko in  <cit>  for the first time. many work have been demonstrated the effective ability of fcms in modeling complex systems. an fcm is a network model to describe the relationship between different concepts in the real world. nodes in the network stand for the concepts and weighted edges are used to quantify the relationship between concepts. compared to traditional modeling techniques, fcms are more reasonable and intuitive description of human reasoning and have been successfully used in numerous practical application areas, such as medical diagnosis  <cit> , metabolism network modeling  <cit> , process control  <cit> , military science  <cit> , and modeling of software development  <cit> .

in recent years, lots of work has been carried out in the research of train fcms from data. in fact, given an initial state of an fcm, which is represented by a set of state values of constituent nodes, a trained fcm can simulate the evolution of state values over time to predict the future state values. thus, the objective of these learning algorithms is to determine weights in fcms so that the response time sequences of each node in the trained fcms can fit the observed time sequences as far as possible.

therefore, viewed from methodology, the learning objective of reconstructing fcm models from time series data can be described as an optimum formula. and the learning algorithm is to minimize the optimum formula as far as possible, which is simulating the observed time sequence. this will become a typical big optimization problem when the scale of fcm expands to a high level, since the number of decision variables, namely, the number of weights needs to be determined, increases quadratically with the number of nodes in fcms. for example, if we need to train an fcm with  <dig> nodes,  <dig>  <dig> decision variables need to be optimized is a typical large-scale optimization problem.

evolutionary algorithms  are a subset of generic population-based optimization algorithms inspired by the biological evolution. in real applications, eas often performs well to a wide range of problems. eas and other population based metaheuristics have been shown to be powerful in training fcms, including genetic algorithms   <cit> , particle swarm optimizations   <cit> , simulated annealing   <cit> , ant colony optimization  algorithms  <cit> , and memetic algorithms  <cit> .

stach et al.  <cit>  proposed a method to optimize fcms using real-coded genetic algorithm . the aim of rcga is to model the system structure from the time sequence which was observed from the complex system. since time series data only at two adjacent time point can be used as a training sample, the whole time sequence can be broken down into several pairs according to time point t and t +  <dig>  the estimated values of time point t +  <dig> can be calculated by the values of time point t in each pair. the objective of learning algorithm is to find a fcm that can reproduce the time sequence which is the same as the observed time sequence. the rcga first established the error between the output data of estimated fcm and the observed time sequence as an optimum formula, then, the operators of rcga were used to solve the optimum formula which is actually minimize the error. the experiments results in  <cit>  show that rcga have a big advantage over es.

subsequently, in order to train large scale fcms, stach et al. in  <cit>  applied a scalable divide and conquer strategy to speed up rcga. in divide and conquer rcga, the whole training data are divided into several subsets. then, a fcm model can be learned from each subset by rcga. and these learned fcm models are averaged as the final solution.

particle swarm optimization  was also extended to solve the optimization problem of training fcm by parsopoulos et al. in  <cit> . the learning algorithm aims to reduce the searching space continually and find a small region of candidate fcm models which is close to the real fcm model.

in addition of the above learning algorithms, other population-based algorithms were also used to learning fcms. for example, a simulated annealing  based fcm learning algorithm was proposed by alizadeh et al. in  <cit> . in the learning algorithm, the optimal object is the same as the one in  <cit> , except that the optimization algorithm is changed to sa. chen et al.  <cit>  proposed an ant colony optimization  algorithm for training fcms with no more than  <dig> nodes, where the weights were discretized. additionally, fcms were also applied to solve many practical problems. in a recent work by chen et al., biological grns were modeled as fcms, and the author proposed a hybrid method which combines inherently continuous aco algorithm with a decomposition-based optimization strategy to reconstruct biological grns with  <dig> genes from gene expression data  <cit> .

acampora et al.  <cit>  introduced a memetic algorithm to generate fcm models from historical data without a priori knowledge. extensive comparative studies performed on both synthetic and real-world data verified the competitive performance of the memetic algorithm.

however, fcms learned by most existing algorithms are always small-scale with dozens of nodes, and only the acord proposed in  <cit>  used fcms to reconstruct gene regulatory networks  with  <dig> nodes. thus, there is a demand to develop methods capable of training large-scale fcms based on time series data.

in our previous work, multi-agent systems and gas are combined to form a new algorithm named as a multi-agent genetic algorithm  for large-scale global numerical optimization  <cit> . the results shown maga performed well even for the optimization problems with  <dig>  <dig> decision variables. in  <cit> , maga was also used as the learning algorithm to solve constraint satisfaction problems and combinatorial optimization problems, and the experiment results show a good performance. maga was extended to successfully solve the big optimization problem extracted from eeg data analysis mentioned above  <cit> . moreover, in  <cit> , a new version of maga, termed as dynamic maga , was proposed, which can effectively train fcms with  <dig> nodes . however, dmaga formulated the fcm training problem as one single optimization problem, where all weights are determined simultaneously. such formulation prevents dmaga from being able to efficiently handle even larger fcms, such as fcms with  <dig> nodes.

in fact, to learn an fcm can be considered as to learn how each node in the fcm is affected by other nodes. thus, in order to make a training algorithm for fcms scalable to a large number of nodes, we can decompose an fcm learning problem into multiple optimization problems, where each optimization problem corresponds to the training of a single node. that is to say, for training an fcm with n nodes, the original optimization problem having n × n weights , will be decomposed into n optimization problems with each having only n decision variables. a similar idea has also been reported in  <cit>  in reconstructing grns based on fcms, which successfully reduces the size of the optimization algorithm and favors a distributed implementation. nevertheless, efficiently solving n optimization problems with n decision variables remains to be challenging, in particular when n becomes large.

to take full advantages of both dmaga and the above-mentioned decomposition-based optimization approach, this work proposes a new algorithm for training larger fcms by combining the decomposition based approach with dmaga, which is termed as dmaga-fcmd. fcms with various sizes are used to verify the performance of the proposed dmaga-fcmd. the results show that dmaga-fcmd is able to effectively train fcms with  <dig> nodes and significantly enhance the performance of the original dmaga. moreover, dmaga-fcmd is employed to reconstruct a biological grn, which is a challenging real-world problem. the proposed dmaga-fcmd is shown to outperform a few state-of-the-art algorithms on the benchmark problem dream <dig>  <cit> , which the data error is as low as  <dig> , where the results of other algorithms are around  <dig> . dream <dig> project is a grns inference challenge which aims at reconstructing network structure from simulated steady-state and time series data.

methods
decomposition-based fcm for grns reconstruction
the relations between different concepts in a complex system can be described as a directed graph which consists of nodes and weighted edges. to adequately describe the concepts, it is necessary to use real number to quantity the expression level of each concept. for an fcm consists of n concept nodes, the state values of these nodes are described as a vector c:  <dig> c=c <dig> c <dig> …,cn 


where, c
i∈ <cit> , i = . once the state value of each node is described, we need to describe the relations between nodes. here, we use a an n × n weight matrix w to define the relations, which is also the candidate solution to the fcm learning problem,  <dig> w=w11w12…w1nw21w22…w2n⋮⋮⋱⋮wn1wn2…wnn 


a weight is assigned to an edge connecting any two nodes to quantify the strength and the type of the relations between the two nodes. the absolute value of a weight represents how strong the source node affects the target nodes, while the positive or negative sign of weights denotes an excitatory or inhibitory relation  <cit> . in , all weights w
ij are in the range of , representing the causal relations between nodes i and j, i, j =  <dig>   <dig>  …, n. fig. 1a shows a simple fcm with  <dig> nodes and  <dig> edges, and the corresponding weight matrix is presented in fig. 1b, where, e.g., w
12 = + <dig>  indicates that there is an excitatory edge pointing from node  <dig> to node  <dig> with a strength of  <dig> . w
54 =  <dig> means there is no causal relation between nodes  <dig> and  <dig>  similarly, w
44 = - <dig>  suggests that node  <dig> has a negative effect of feedback regulatory on itself. the higher the absolute values of the weights is, the stronger the relations.fig.  <dig> a simple example of fcms  fcm structure,  the weight matrix




in fact, an fcm with n nodes can be decomposed into n single sub-maps in the following way. for simplicity, we focus on the input information of each node. each node and its input nodes can be regarded as a sub-map. every single map corresponds to a column vector in the weight matrix. in other words, the weight matrix in  can be represented as w = [w
 <dig>  w
 <dig>  …, w
n], where w
i = , i =  <dig>   <dig>  …, n. for example, the fcm in fig.  <dig> can be decomposed into  <dig> sub-maps shown in fig. 2a and b shows the corresponding weight relations of each single sub-map.fig.  <dig> the five single sub-maps for the example fcm in fig.  <dig>   <dig> single sub-maps,  the corresponding weight relations




when the activation degree of nodes are produced, we use the following equation to predict the activation degree c
it +  <dig> based on the known values c
it,  <dig> cit+1=g∑j=1nwjicjt,∀i∈ <dig> ,…,n 


where c
it is the state value node i at the t-th iteration in one response, and g is a sigmoid activation function that maps the activation level to the unit interval  <cit> ,  <dig> gx=11+e−λx 


where parameter λ decides the slop of the function, and a small value lead to highly nonlinear model. logistic transformation function is the most commonly used function in fcms and offers significantly greater advantages than other functions  <cit> . according to the recommendation in  <cit> , we set  <dig> as the value of λ in this paper.

fcm learning algorithms aims to find the relations between different concepts from response sequences. in computation terms, the objective is to learn the best interconnection matrix which performs the best on simulating the response sequences. specifically, the error between the responses generated by candidate fcm and observed response sequences are formulated as an optimization expression, and learning algorithms are used to minimize it. and the error mentioned above is labeled as data_error,  <dig> data_errorw=1nnsnt−1∑n=1n∑s=1ns∑t=1nt−1cnts−c^nts <dig> 


where n
s is the number of response sequences, n
t is the number of time instants in the response sequences, c
nt is the t-th desired state value of node n in the s-th response sequence, and Ĉ
nt is the t-th state value of node n in the s-th generated response sequence.

from the decomposition point of view, data_error in  is actually averaged over the data error of each node and can be re-formulated as  <dig> data_errorw=1n∑n=1n1nsnt−1∑s=1ns∑t=1nt−1cnts−c^nts2=1n∑n=1ndata_errornwn 


thus, the data error of node i can be represented as follows,  <dig> data_erroriwi=1nsnt−1∑s=1ns∑t=1nt−1cits−c^its <dig> 


when we calculate Ĉ
it, the state values of input nodes to node i in the desired response sequences are used. in this way, the data error of each node can be calculated independently. in the following text, eq.  is used as the objective function for optimizing the weights of the i-th sub-map.

decomposition-based dmaga for training fcms
different from the dmaga proposed in  <cit> , which is used to optimize the whole weight matrix simultaneously, dmaga-fcmd proposed in this paper optimizes the column vectors of the weight matrix independently. in the following, we first define the agents used in dmaga, and then introduce the genetic operators to be performed on the agents. finally, the detailed implementation of dmaga-fcmd is provided.

definition of the agents
in dmaga-fcmd, each candidate weight column vector w
i is regarded as an agent, where i means the i-th single sub-map.

definition 1
for a fcm with n nodes, each agent is represented as an n-dimensional vector. once the representation of agent is determined, the most important thing is to define the expression of energy of each agent. in this work, the energy is the negative value of data_error defined in . the dmaga-fcmd aims to increase the energy of each agent as much as possible in order to survive in the environment, which is equivalent to decrease the data_error. to realize the local perceptivity of the agents, the environment is organized in a lattice-like structure, in which each agent competes or cooperates with their neighbors. an agent interacts with its neighbors, exchanging information with each other, thereby achieving global information share. the lattice in which all agents live is defined as follows.

definition 2
all agents live in a lattice-like environment, l, which is called an agent lattice. the size of l is l
size × l
size, where l
size is an integer. each agent is fixed on a lattice-point and can only interact with its neighbors. the agent located at  is represented as l
a,b, a, b =  <dig>   <dig>  …, l
size, then the neighbors of l
a,b, neighbors
a,b, are defined as follows.  <dig> neighborsa,b⊆la',b,la,b',la,b",la",b 


where a′=a− <dig> a≠1lsize,a= <dig> b′=b− <dig> b≠1lsize,b= <dig> a″=a+ <dig> a≠lsize <dig> a=lsize,andb″=b+ <dig> b≠lsize <dig> b=lsize.


as shown in fig.  <dig>  there are four agents at most in the neighborhood for each agent. in the standard maga, each agent has to compete with the other four agents around it, in other words, the four agents around it are all its neighbors, so the agents with low energy are eliminated early, which make maga lost the diversity of the population under the huge selection pressure. thus, in dmaga-fcmd, in order to tune the selection pressure, each agent can select its neighbors dynamically according to the energy. namely, there is no need to compete with all the agents around it. thus, neighbors
a,b in is a subset of .fig.  <dig> the topology of the defined agent neighborhood




to dynamically determine the neighbor of each agent, at the beginning of each generation, all agents will be sorted according to a decreasing order of their energy and evenly divided into four levels. and the agents in the first/second/third/fourth level can select four/three/two/one neighbor randomly from . in this way, an agent with a lower level of energy will have a smaller number of neighbors, thereby improving its chance to survive than those in maga.

genetic operators for agents
four genetic operators are used to guide the evolutionary search, which are the neighborhood competition, the crossover, the mutation, and the self-learning operator, of which the neighborhood competition and crossover operators account for competition and cooperator among agents, while the mutation and self-learning operators are dedicated to improving the agents’ ability of local search and making use of knowledge to survive. suppose the four operators are performed on agent l
a,b = , and max
a,b =  is the agent with the maximum energy in neighbors of l
a,b.

neighborhood competition operator
if l
a,b satisfies energy > energy, which means l
a,b defeats all the neighbors and keep the current solution in lattice; otherwise, l
a,b lost the change to survive and have to be replaced by max
a,b which is the energy with maximum energy in the neighborhood. there are also two strategies determine how the agent l
a,b would be replaced. that is, if u < p
o, strategy  <dig> is adopted; otherwise, strategy  <dig> is used. here, p
o is a predefined probability, and u is a random number uniformly distributed in the interval of  <cit> .

strategy 1
a new agent new
a,b =  is generated as follows:  <dig> ei=‐ <dig> ω<‐ <dig> ω>1ω,otherwise,whereω=mi+u− <dig> ×mi−li,i= <dig> ,…,n 


since there might be still some useful information in l
a,b even if it is a loser, strategy  <dig> uses both l
a,b and max
a,b to generate a new agent, which is used to replace l
a,b.

strategy 2
randomly select two integers k and s satisfying 1 < k < s < n, then a new agent is generated as follows to replace l
a,b,  <dig> newa,b=m <dig> m <dig> …,mk− <dig> ms,ms− <dig> …,mk+ <dig> mk,ms+ <dig> …,mn 


crossover and mutation operators
the orthogonal crossover and gaussian mutation operators are applied on l
a,b and max
a,b. the reader are referred to  <cit>  for the details of these operators.

self-learning operator
in maga, a small scale maga is introduced as the self-learning operator for agents to be able to use knowledge, which however, is still a sort of random-based local search strategy. therefore, to make use of the properties of fcms, in dmaga, a one dimensional search strategy is adopted to implement the self-learning operator. in this work, we also perform this self-learning operator on l
a,b, and more details can be found in  <cit> .

implementation of dmaga-fcmd
dmaga-fcmd optimizes each sub-map sequentially. for each sub-map, the neighbors of each agent is first determined, then each agent compete with its neighbors according to their energy, and the agent with a higher level of energy survives in the population. once the competition is performed, the crossover and mutation operators are performed on each agent with a probability p
c and p
m, respectively. then, the best agent in the current generation improves its energy through self-learning operator.

algorithm  <dig> shows more details of dmaga-fcmd, where l
it represents the agent lattice at the t-th generation for the i-th single sub-map, cbest
it represents the best agent in the t-th generation for the i-th single sub-map, and best
i represents the best agent found for the i-th single sub-map.

RESULTS
experiments on synthetic fcms
in this section, the dmaga-fcmd is tested on synthetic fcms. for fully test the performance of the dmaga-fcmd, the scale of fcms is varying from  <dig> to  <dig> nodes. in order to show the improvement of dmaga-fcmd over dmaga, extensive comparisons are made between these two algorithms. in addition, dmaga-fcmd is compared with three representative existing methods based on evolutionary algorithms, namely, rcga  <cit> , acord   <cit>  and differential evolution   <cit> , where the results of rcga and de are taken from the literature. acord and dmaga are run under the same experimental settings as those of dmaga-fcmd. all experiments on dmaga-fcmd are conducted for the same parameter settings, which are given in  <cit> .


data_error, which is presented in , is an important measure to evaluate the performance of different learning algorithms. unlike the data_error is used to evaluate the fitting ability of time sequences, model_error is a direct comparison between the weights of the learned model and the target model,  <dig> model_error=1n2∑i=1n∑j=1nwij−w^ij 


where ŵ
ij is the learned weight of between nodes i to j.

in order to evaluate the performance of the algorithms under comparison in predicting the existence of edges, the problem of training fcms is extended and transformed into a binary classification problem. that is, the target fcm model and the learned fcm model are transformed into binary networks according to a predefined threshold. the absolute weights that are larger than the threshold are transformed to 1; otherwise  <dig>  once the transformational rule is determined, we need to choose a value of the predefined threshold. in this paper, we choose  <dig>  as the value of the threshold which is the same value used in  <cit> . in fcms, we usually think that the causal relation with strength less than  <dig>  usually has no significance in practical problems.

the author of  <cit>  also gives the definition of positive and negative edges. according to the definition, the edges with absolute weights larger than  <dig>  are identified as negative edges; otherwise, they are identified as positive edges. the ss mean is employed to evaluate the performance,  <dig> ssmean=2×sensitivity×specificitysensitivity+specificity 


where  <dig> sensitivity=ntpntp+nfn 
  <dig> specificity=ntnntn+nfp 


where tp, fn, tn, and fp are defined in table  <dig>  t  means the correctly identified edges. f  means the edge is identified as the opposite character. p  means positive edge and n  means negative edge which has defined above . for example, n
fp is the number of false positive edges, which means the number of negative edges that are incorrectly identified as positive edges.table  <dig> definition of tp, fn, tn, and fp


tp
fp
fn
tn



in this paper, the artificial response time sequences used to train fcm models are generated by a two-step method proposed in  <cit> . first, random real numbers which should be within the interval  are assigned to the weights of the interconnection matrix, an additional file shows this in more detail . however, according to the viewpoint in  <cit> , which is if the weight between two nodes is smaller than  <dig> , the relation between these two nodes can be ignored in practical application, we check each weight whether its absolute value is smaller than  <dig> . if so, the weight will be set to  <dig>  second, the state value of response time sequences at the initial time point are assigned by random value ranging from  <dig> to  <dig>  then, the subsequent time sequences can be generated according to  based on the fcm model and initial state values.

here, the threshold value  used in this work is always a default parameter in many work  <cit>  about fcms. however, different threshold value will affect the algorithm performance. in order to explore the impact of the threshold value on algorithm performance, we conduct the following experiment. when the network  was learned, we set the value of threshold ranges from  <dig> to  <dig> in the step of  <dig>  and get a series of networks. then we calculate the data_error, model_error and ss mean for networks with different threshold values. from fig.  <dig>  we can see that the value of threshold affect the algorithm performance greatly, no matter what the density of the network, the performance of the algorithm decreases with the increase of threshold value. when the threshold value is bigger than  <dig> , the value of ss mean is even  <dig> which means there is no existence of edge in this network is predicted correctly. thus, it is important to select an appropriate threshold for the performance of the algorithm.fig.  <dig> comparison in terms of  data_error,  model_error and  ss mean on fcms  with various of threshold value which ranges from  <dig> to  <dig>  the red line and blue line show the comparison experiments for fcms with 20% density and 40% density, respectively




in real-world, the network structures of fcms are always sparse. there are only a fraction of links between nodes in fcms. for this reason, we need to control the density of the random fcm model. when we generate the fcm models by the method mentioned above, a large part of weights are assigned to  <dig> and the others are set to nonzero random real numbers. for example, if the edge density of 20% is expected for an fcm with  <dig> nodes,  <dig> random edges  will be selected and random values will be assigned to these edges.

the size of generated fcm varies from  <dig> to  <dig> , and for each size, the edge density of 20% and 40% are used. in order to compare with other algorithms, experiments on three scenarios are conducted. the first scenario has one response sequence with  <dig> time points each , the second scenario has five response sequences with four time points each , and the third scenario has  <dig> response sequences with  <dig> time points each . figure 5a shows an original fcm with  <dig> nodes with 20% edge density and fig. 5b shows the corresponding fcm learned by dmaga-fcmd. by comparing the original and learned fcms, we see that the network structure is fully correctly learned and the errors between the original and learned weights are smaller than  <dig> . comprehensive comparative results in terms of data_error on fcms with 5 ~  <dig> nodes are presented in fig.  <dig>  and a detailed comparison of the fcms with  <dig> and  <dig> nodes is reported in table  <dig>  the comparison in terms of model_error is given in fig.  <dig> and table  <dig>  the results are averaged over  <dig> independent runs for fcms with 5 ~  <dig> nodes and  <dig> independent runs for fcms with  <dig> and  <dig> nodes.fig.  <dig> 
a the original fcm with  <dig> nodes, b the learned fcm by dmaga-fcmd



fig.  <dig> comparison in terms of data_error on fcms with various number of nodes.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%


n
s-n
t

fig.  <dig> comparison in terms of model_error on fcms with various number of nodes.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%.   <dig> nodes, density = 20%.   <dig> nodes, density = 40%


n
s-n
t



as can be seen, in terms of data_error, dmaga-fcmd, dmaga and acord all reach  <dig>  for fcms with  <dig> nodes, no matter whether the edge density is 20% or 40%. for fcms with  <dig> and  <dig> nodes, although data_error of dmaga-fcmd is larger than that of acord, it is much smaller than those of rcga and de. for fcms with 40 ~  <dig> nodes, dmaga-fcmd outperforms acord on  <dig> out of  <dig> fcms and outperforms all other methods with different edge densities and n
s-n
t combinations. moreover, table  <dig> shows that dmaga-fcmd clearly outperforms dmaga on fcms with  <dig> and  <dig> nodes.

the comparison in terms of model_error shows that for fcms with 5 ~  <dig> nodes, dmaga-fcmd always outperforms dmaga, rcga, and de. for fcms with  <dig> nodes, acord performs better than dmaga-fcmd, but for fcms with  <dig> and  <dig> nodes, dmaga-fcmd outperforms acord on  <dig> out of all the  <dig> cases regardless the edge density and n
s-n
t combination. moreover, all the averaged model_error of dmaga-fcmd for fcms with  <dig> and  <dig> nodes is smaller than  <dig> ,which is consistently smaller than that of dmaga.

in addition, the above results show that the performance of dmaga-fcmd in terms of data_error and model_error is not very sensitive to the number of nodes, the number of response sequences, and the number of time points in a certain range, where at least four time-points are required and the number of nodes are limited less than  <dig>  it indicates that dmaga-fcmd is robust and is scalable to the size of fcms to a certain extent.

note also that, the experimental results of dmaga-fcmd are obtained with less than  <dig>  ×  <dig> fitness function evaluations , whereas the results of compared algorithms use 3 ×  <dig> fitness function evaluations, which are much larger than that of dmaga-fcmd. as can be seen from fig.  <dig>  dmaga-fcmd efficiently reduces the computational cost compared with dmaga, especially for large-scale fcms. therefore, dmaga-fcmd has exhibited better performance with lower computational cost compared to the state-of-the-art, demonstrating that the algorithm is very competitive for solving large-scale problems.fig.  <dig> the number of fitness evaluations consumed by dmaga-fcmd and dmaga, respectively




figure  <dig> report the performance of dmaga-fcmd in terms of ss mean. as we can see, for fcms with 5 ~  <dig> nodes, dmaga-fcmd consistently outperforms dmaga, rcga and de, and also outperforms acord on  <dig> out of all the  <dig> cases. for fcms with 40 ~  <dig> nodes, dmaga-fcmd performs better than all the other learning algorithms on all cases. table  <dig> shows the comparative results in terms of ss mean of dmaga-fcmd and dmaga for fcms with  <dig> and  <dig> nodes. as can be seen, compared with dmaga, dmaga-fcmd significantly enhances the ability of dmaga in training larger fcms.fig.  <dig> comparison in terms of ss mean on fcms with various number of nodes.   <dig> nodes density = 20%,   <dig> nodes density = 40%,   <dig> nodes, density = 20%,   <dig> nodes, density = 40%,   <dig> nodes, density = 20%,   <dig> nodes, density = 40%,   <dig> nodes, density = 20%,   <dig> nodes, density = 40%,   <dig> nodes, density = 20%,   <dig> nodes, density = 40%,   <dig> nodes, density = 20%,   <dig> nodes, density = 40%


n
s-n
t



experiments on dream <dig> in silico network challenge
gene regulatory networks  have been widely used to model, analyze and predict the behavior of biological organisms. a grn aims to build the relationships between a set of molecular entities and is often modeled as a network composed of nodes  and edges   <cit> . in this section, we employ the proposed dmaga-fcmd to reconstruct a biological grn based on gene expression time series data, known as dream <dig>  <cit> , a widely used benchmark for evaluating reverse engineering methods  <cit> . the gene expression time series data were generated based on the network structures of escherichia coli and saccha-romyces cerevisiae. time series database contains a variety of network sizes with  <dig> and  <dig> genes. perturbation and noise on expression profiles are generated by differential equations. there are five separate networks for each type of networks.

in the dream <dig> data, under the perturbations of internal noise and measurement noise, there are five time series for each gene. each time series contains  <dig> time points. the first  <dig> time points evaluate the response of gene networks in the presence of perturbations. the perturbations are revoked at the 11-th time instant. so the last  <dig> time points reflect the response of gene networks after the wild type network is restored. in this experiment, we use the last  <dig> time points and test the performance of dmaga-fcmd in terms of data_error and ss mean.

in the experiments, we perform several efficient evolutionary-based algorithms, dmaga, rcga, d&c rcga and de, one dream data. we also perform lasso  <cit>  for comparison. the experimental results are shown in fig.  <dig>  as can be seen, data_error of dmaga-fcmd perform the best for grns with  <dig> genes except one case . for grns with  <dig> genes, lasso is better than dmaga-fcmd in terms of data_error, and dmaga-fcmd performs better than other evolutionary-based algorithms. we can also see that the decomposition strategy is always useful, because the dmaga-fcmd is always better than dmaga, d&c rcga, which is rcga with decomposition strategy, is always better than rcga no matter the number of genes is  <dig> or  <dig> fig.  <dig> experimental results in terms of data_error for grns with   <dig> genes and   <dig> genes.  five experiments for grns with  <dig> genes.  five experiments for grns with  <dig> genes




discussion
the proposed algorithm dmaga-fcmd enhances prediction, which is valuable for machine learning. as the decomposition strategy, dmaga-fcmd has reduced the computational cost. reconstructed fcms can be modeled as a big data optimization problem. previous studies indicate that the number of variables needs to be optimized increases quadratically with the scale of fcms, resulting an exponential increase in the number of candidate solutions. currently, most learning algorithms can only handle small scale fcms which number of nodes is smaller than  <dig>  in this work, we show that the dmaga-fcmd can handle large fcms with  <dig> and  <dig> nodes . the experiment results also show that dmaga-fcmd efficiently reduces the computational cost compared with our previous work dmaga, especially for large-scale fcms, which demonstrating that the algorithm is very competitive for solving large-scale problems.

in the dream <dig> data, the dmaga-fcmd performs better than the other evolutionary-based learning algorithms. however, in the five experiments for grns with  <dig> genes, the lasso is better than dmaga-fcmd. the performed lasso takes into account the sparseness of the network. therefore, the properties of lasso algorithm provide a good direction for us to improve the dmaga-fcmd.

CONCLUSIONS
in this paper, we propose a new algorithm, termed as dmaga-fcmd, to train large-scale grns based on fcms using time series data by introducing the decomposition based optimization approach into the dynamical multi-agent genetic algorithm. the dmaga-fcmd has shown to be well suited for learning causal relations of complex systems. extensive experiments are conducted on both synthetic fcms and dream <dig>  a challenge to reverse grns from simulated time series data. experimental results show that dmaga-fcmd is able to effectively train fcms with up to  <dig> nodes and improves ability of reconstructing grns with high accuracy, outperforming the compared state-of-the-art learning algorithms. our results also indicate that dmaga-fcmd is promising for solving larger-scale grns, which will be considered in our future work.

additional file

additional file 1: synthetic fcms. in this work, the scale of synthetic fcms is varying from  <dig> to  <dig> nodes, and the density is 20% and 40% for each scale. the method used to generate fcms is as the same as the method proposed in ref.  <cit> , we also described the method in “results” section, page  <dig>  




abbreviations
dmaga-fcmddynamical multi-agent genetic algorithm with the decomposition-based model

fcmfuzzy cognitive maps

grngene regulatory networks

