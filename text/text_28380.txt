BACKGROUND
two exciting developments in recent years have the potential to change the face of biomedical natural language processing or text mining . one of these is the attention that the community has begun to turn to using full-text journal articles as inputs--previously , the majority of bionlp work had been done with abstracts as input. the other development is the birth and growth of pubmedcentral . pmc is a repository for full-text journal articles, both open access and traditionally copyrighted. as of  <dig> it contains  <dig>  million full-text  journal articles, and is growing rapidly, creating an unprecedented opportunity for text mining.

there has been an awareness since the early days of the modern era in bionlp that there are differences between the content of the abstracts and the content of the bodies of journal articles. various authors  have described particular aspects of these differences. however, no one has yet attempted a broad characterization of the differences between them. structural differences between them also have not been addressed. this article does just that, with an emphasis on those aspects of the content and structure of abstracts and ft bodies that have implications for text mining. with that focus, this article can be seen as not only descriptive of the differences and similarities between ft and abstracts, but as suggesting a roadmap for developing the language processing tools and infrastructure that will be necessary to fully exploit the new data available to us in pmc.

as our input, we primarily use a corpus called craft, the colorado richly annotated full text corpus  <cit> . craft consists of  <dig> full-text journal articles, i.e. including both the abstract and the body of the article.  to ensure biological relevance, the contents of craft were selected in cooperation with the mouse genome informatics database. all articles are relevant to mouse genomics and have been curated by mgi personnel, and they are all the basis for at least one gene ontology gene annotation in the mgi database. in an ongoing three-year project expected to be completed in  <dig>  we are carrying out extensive linguistic and semantic annotation of the craft data. this includes annotation of linguistic features such as full syntactic parse trees and semantic categories such as gene names and gene ontology terms. all articles were available in the pubmedcentral xml format, which segments an article into its abstract and body, and within the body segments the introduction, materials and methods section, results, discussion, acknowledgements, bibliography, etc. this segmentation allows us to compare the abstract versus the body of the  <dig> articles in craft.

our goal was to compare abstracts and bodies to features that are relevant to language processing. out of the many such features that may exist, we picked a number that were both amenable to automatic assessment and that could lead to actionable information on how to advance the state of the art. we also used some of the manually annotated data for these assessments. we detail below the specific features that we examined. briefly, they included  structural aspects of the abstracts and bodies;  morphosyntactic and discourse features of the contents;  distributions of semantic classes of named entities; and  differential performance of text mining tools.

related work
various authors have looked at general distributional differences of semantic content between abstracts and article bodies  <cit>  looked at the case of protein-protein interactions and concluded that high coverage could not be achieved without processing article bodies  <cit>  found that biologically relevant words are more dense in abstracts, but that there are many more of them in article bodies  <cit>  found that more than half of the information on protein-protein interactions in articles is found in the body. similarly,  <cit>  examined a single article and found that only seven of nineteen unique interactions in the paper were mentioned in the abstract, with the rest occurring only in the full text.

other authors have investigated other aspects of full text versus abstracts  <cit>  looked at the effectiveness of searching full text versus abstracts for information retrieval purposes and found that searching full text was more effective as measured by map, p <dig>  and ipr <dig>  especially when spans of the full-text articles were considered  <cit>  downsampled  <dig>  sentences from  <dig> full-text articles and annotated them for a variety of factors that affect coherence phenomena. they found that coreference is very important, as is dependency between separate sentences  <cit>  note that sentences found in introduction, methods, results, and discussion sections are not necessarily the type that would be expected in those sections. for example, they found "methods"-type sentences in a results section. they annotated sentences in full-text documents as to which section type they actually represented and trained a classifier to accurately label sentence types. crucially, they found that a classifier which was trained to perform the same task on abstracts did not perform well when applied to full text.

still other authors have pointed out that there are challenges posed by full-text journal articles that are not an issue with abstracts. for example,  <cit>  and many others since have noted problems in dealing with non-ascii characters, particularly greek letters. full text articles can also present other challenges, such as the recognition and clean-up of embedded tags, non-ascii characters, tables and figures, and even the need to convert from pdf to textual format. access to full text is an especially troublesome issue.

however, as these are not language processing problems per se, they are not addressed further in this work. other relevant work has dealt with the processing of full-text articles in general. early work by friedman and rzhetsky  <cit>  led to the development of the genies system, which uses both semantic and syntactic grammars to process full-text articles. more recently, rzhetsky's geneways system has also dealt with full text, analyzing nearly  <dig>  ft articles and using the results to populate a database of almost  <dig>  million statements about signalling pathways at a precision of 95%. in the very recent past, the trec genomics  <cit>  and biocreative competitions  <cit>  have led to a large increase in work on full-text articles.

RESULTS
we examined the following:

structural aspects:

• distribution of sentence lengths

• incidence and types of parenthesized text

morphosyntactic and discourse features:

• incidence of coordination, negation, passives, and pronominal anaphora

• distributions of semantic classes of named entities:

• distribution of gene/protein names

• distribution of mutations

• distribution of drug names

• distribution of diseases

differential performance of text mining tools:

• gene mention performance

• mutation mention performance

structural aspects
distribution of sentence lengths
sentence length has a consequence for natural language processing: the performance of full syntactic constituency parsers as evaluated on the sentence level  is poor for long sentences  <cit> : <dig>  for this reason, if article bodies have notably longer sentences than do abstracts, we will need either more and better training data for them, or even a different approach than traditional constituent structure parsers, such as dependency parsing. on the other hand, if article bodies have notably shorter sentence lengths than do abstracts, then we can expect them to be easier to parse than abstracts. to test whether there is a difference between sentence length distributions in article abstracts and bodies, we used a sentence segmenter  to split each section into individual sentences. we then tokenized each sentence on white space.  sentence length was then determined in words for each sentence in the corpus.

statistically significantly different pairs at p < . <dig> are marked by *. pairs that are not statistically significantly different are marked by --.

incidence and types of parenthesized text
we looked at the incidence and types of parenthesized text for two reasons. one is that parenthesized text is out of the scope of normal syntactic rules, and therefore can be expected to pose a challenge for syntactic parsers. the other is that different types of syntactic text require  different kinds of handling by information extraction and other applications. for example:

• parenthesized gene symbols and abbreviations allow for improved coreference resolution within a text, as well as gene normalization.

• parenthesized data might be a target for information extraction applications  <cit> ,  <cit> .

• parenthesized p values might be valuable for the automated extraction of meta-analyses.

• parenthetical text, by definition, may be completely ignorable.

• parenthesized table and figure captions are often indicators of assertions with experimental validation  <cit> .

• parenthesized citations are useful for establishing rhetorical relations between papers  <cit> , synonym identification  <cit> , and curation data  <cit> .

thus, we are interested in the overall frequencies of parenthesized text in abstracts and article bodies, and are also interested in the distribution of types of parenthesized material between the two. 

to test whether there is a notable difference between abstracts and article bodies with respect to parenthesized materials, we wrote an application that counted parenthesized text strings and attempted to classify them, using regular expressions, into one of nine categories :

• abbreviations or gene symbols )

• citations 

• data values 

• p-values 

• parenthetical statements )

• figure/table pointers 

• singular/plural markers )

• parts of gene names )

• list elements , striatal neuron-packing density , and striatal neuron number )

• "unknown"

for those categories for which we could do a principled assessment of the correctness of the classification, we manually examined the results of the classification. we found accuracies ranging from a low of 76%  to a high of 100% . we point out that our evaluation was very stringent, so that e.g. in the case of the data value category, if the parenthesized text contained data values but also contained a p-value, we marked it as incorrect. the number of instances of parenthesized materials that the script could not classify was not small-- <dig> % for the abstracts and  <dig> % for the article bodies--so the number of instances of each category that we report is probably an underestimate; the recall of the script is apparently about 75%, modulo any instances that might not fit into one of these nine categories, such as mixed data and p-values.

we found marked differences both in the incidence of parenthesization and in the distribution of types of parenthesized materials between the abstracts and the article bodies. these are summarized in table  <dig>  regarding incidence, parenthesis usage in the article bodies was almost triple that in the abstracts, with  <dig> per thousand words in the bodies and only  <dig> per thousand words in the abstracts. the distribution of types was different as well, with abbreviations and gene symbols accounting for over half of the uses of parentheses in the abstracts , but only 11% of the uses of parentheses in the article bodies. there was also a smaller diversity of uses of parentheses overall in the abstracts. only six different types of parenthesized material were seen in the abstracts:

list enumerators 

p-value 

data 

singular/plural 

abbreviation or gene symbol 

parenthetical statements ).

in contrast, eight of the nine uses of parentheses that we looked for were observed in the article bodies--all of the six that were seen in abstracts, plus tables and figures , and citations . handling parenthesized material may be a fruitful way to increase our yield of information from full-text articles.

morphosyntactic and discourse features
for significance testing of the data in this section, we used a two-sample two-tailed t-test on normally distributed data, and the mann-whitney-wilcox signed-rank test on data that was not normally distributed. the results are summarized in table  <dig> 

values marked with * are significantly different at the p < . <dig> level. values marked with ** are significantly different at the p < . <dig> level.

incidence of coordination
the incidence of conjunctions was assessed because conjunctions are known to cause difficulties in parsing. as mcclosky et al. put it, "conjunctions are about the hardest things in parsing, and we have no grip on exactly what it takes to help parse them" . we counted every instance of and, or, and but in the text, and calculated the ratio of coordinators to total tokens. the difference between abstracts and bodies was not statistically significant by two-sample two-tailed t-test, with only slightly more in article bodies-- <dig>  coordinations per thousand tokens, versus  <dig>  coordinations per thousand tokens in abstracts.

incidence of passives
we looked at the incidence of passives since they pose some processing challenges for text mining, including the inversion of normal word order between agent and theme and the optionality of agents. to find passives, we looked for the text string ed by. this underestimates the incidence of passives, since it misses instances where there is no agent, and instances where there are multiple passive verbs in sequence . however, the undercounting applies equally to the abstracts and to the article bodies, so the comparison between them remains valid. the incidence of passives was statistically significant between the two text types  by the mann-whitney u wilcox test, with more passives in the article bodies  than in the abstracts .

incidence of negation
we looked at the incidence of negation because like conjunction, handling negation is a perennial problem in natural language processing  <cit> . to detect negation, we looked for the text strings no, not, and neither in the text sets. the incidence of negation was statistically significant between the two text types  by mann-whitney-wilcox signed-rank test; again, the article bodies had a higher incidence at  <dig>  per thousand tokens of text, versus  <dig>  per thousand tokens of text in the abstracts.

incidence of pronominal anaphora
we looked at the incidence of pronominal anaphora since it gives an estimate of the importance of coreference resolution. anaphoric reference has been cited as a cause of low recall in biological information extraction systems more often than would be suspected on the basis of studies that have actually looked at anaphora and coreference resolution in molecular biology texts  <cit> . we searched for pronouns by using a simple regular expression. this somewhat overestimates the incidence of it, since it does not differentiate between true pronominal uses and nonreferential uses . however, again, the overestimate applies equally to the two text types, so the comparison remains valid. the incidence of pronouns is significantly different between the two text types  by two-sample two-tailed t-test; abstracts had a modestly higher incidence, at  <dig>  per thousand tokens of text versus  <dig>  per thousand tokens of text in the article bodies.

sentence complexity
having analyzed these various features of structure and content, we can ask a broader question: is there a difference in complexity between sentences in abstracts and in full text? answering that question requires selecting a measure of sentence complexity. we examine here both measures of sentence complexity broadly construed, and of readability specifically. we note that the distinction between these is an unclear one, and that even the two notions in isolation are problematic. for example, when we ask if one sentence is more complex than another, we must assume some notion of what it is complex for, and surely this must be more than just complexity for a syntactic parser. similarly, with respect to difficulty of reading, we must ask who is doing the reading and for what purpose.

regarding syntactic complexity,  <cit>  looked at a variety of linguistically motivated measures of sentence complexity and came to the conclusion that even sophisticated measures of complexity turned out to return the same results as a simple word count. longer sentences are more complex than shorter sentences, and not even sophisticated measures give a better ordinal assignment of sentence complexity than that. we have already shown that there is a statistically significant difference in length between abstract sentences and body sentences--body sentences are significantly longer, and therefore more complex. however, szmrecsányi's own index of syntactic complexity did not reveal a difference in distributions of complexity between the two genres as evaluated by kullback-leibler divergence.

we also calculated a readability metric for both genres. following the work of  <cit> , we calculated the percentage of function words in sentences in the two genres and averaged it across the two text collections  <cit>  found this percentage of function words to increase as readability increases. interestingly, the readability metric shows no difference in readability between the two genres--abstracts have an average percentage of function words of  <dig>  and bodies have an average percentage of function words of  <dig> . overall, these negative findings in the face of the statistically significant difference in sentence lengths between the two genres merit further investigation, but must remain the subject of future work, since it appears to reflect the power of the metrics themselves, rather than characteristics of article bodies versus abstracts. we discuss the implications for parsing below.

differential performance of text mining tools
gene mention tagger performance
our corpus includes mark-up of all gene and protein mentions in  <dig> files of the corpus, roughly according to the biocreative guidelines. we ran three gene mention systems  with up to three models apiece on the data. we found that for every tagger and every model, performance was higher on the abstracts than on the article bodies. table  <dig> shows the precision, recall, tp, fp, and fn for each combination, and figures  <dig>   <dig> and  <dig> show them graphically. f-measures were generally about  <dig> points higher on the abstracts than on the bodies, were never less than  <dig> points higher, and in one case was  <dig> points higher.

the first row gives the number of gene mentions in each set of texts.

mutation detection performance
to explore the performance of mutation detection in full text versus abstracts, we ran the mutationfinder  <cit>  mutation detection system across both sections of the corpus. mutations are not annotated in the corpus, so we ran the system and then examined its outputs manually. this allowed us to determine precision, but did not allow us to determine recall. performance did not differ markedly. mutationfinder has previously been reported to have a precision of . <dig> on abstracts. we found it to have a precision of  <dig>  on abstracts , and . <dig> on full text. thus, performance differed somewhat, but is still quite high on the article bodies.

syntactic parser performance
to compare the performance of a constituent parser on abstracts versus bodies, we applied the stanford lexicalized parser   to a subsection of the craft corpus. the experimental corpus used to compare parser performance consisted of  <dig> abstracts  and  <dig> article bodies .  we analyzed the resulting parses against the craft gold-standard using evalb , which implements the parseval metrics bracket recall and tag accuracy as described in  <cit> .

bracket recall is a reflection of the how well a parser has identified constituent boundaries in its candidate parses. candidate constituents that are neither substrings nor superstrings of candidates in the gold-standard parse are called incompatible. inversely, compatible candidate constituents are those that do not cross gold-standard constituent boundaries.  bracket recall is calculated by dividing the number of compatible constituents in a candidate parse by the total number of constituents in the gold-standard parse. the mean bracket recall of the stanford parser on abstracts was 57%. on the article bodies the mean bracket recall was 59%. the difference was not statistically significant.

part of speech tagging
tag accuracy is calculated by dividing the number of words tagged with the correct pos in the candidate parse by the total number of words in the sentence. on the abstracts stanford's mean tag accuracy was 83%. on the body dataset, the mean tag accuracy was 81%. the better performance on the abstracts was statistically significant  using the mann-whitney u wilcox test.

distribution of named entity types
except for genes, distributions differed markedly.

statistically significantly different pairs at p < . <dig> are marked by *.

gene mentions
there was a distinct difference in distribution of gene mentions between abstracts and bodies, with an average of  <dig> gene mentions per abstract , and  <dig> per article body . this corresponded to a frequency of  <dig> gene mentions per thousand words in the abstracts / <dig> ) versus  <dig> gene mentions per thousand words )in the article bodies. however, genes were found both in abstracts and in bodies, with all bodies containing at least one gene mention and all but three abstracts containing at least one gene mention. the distributions of densities were not significantly different by the mann-whitney u wilcox test. 

mutation mentions
to explore the distribution of mutation mentions, we ran the mutationfinder mutation detection system, as described above. the distribution of mentions of mutations differed markedly between the two parts of the articles. only one abstract contained any mutation mentions, while eighteen bodies contained a total of  <dig> mutation mentions, for an average of  <dig>  mutation mentions per body but only  <dig>  mutation mentions per abstract. the densities of mutation mentions were significantly different at p < . <dig> by the mann-whitney u wilcox test, with densities of . <dig> / <dig> ) mutation mentions per thousand words in the abstracts versus . <dig> mutation mentions per thousand words / <dig> ) in article bodies.

drug mentions
to explore the distribution of drug mentions, we used a simple dictionary-based approach, which has been claimed to be adequate for drug names ). as the dictionary, we used the drugcards data from drugbank . the distribution of disease mentions was quite different between the two parts of the articles--only 19/ <dig> abstracts had drug mentions, but 85/ <dig> bodies had drug mentions. the average number of drug mentions per paper was quite different in the abstracts and the bodies, with on average only  <dig>  drug mentions per abstract but  <dig>  drug mentions per body. the density of drug mentions was significantly different in abstracts and in bodies at p < . <dig> by the mann-whitney u wilcox test, with  <dig>  / <dig> ) drug mentions per thousand words in the abstracts and  <dig>  / <dig> ) drug mentions per thousand words in the bodies.

disease mentions
to explore the distribution of disease mentions, we used the banner system. the distribution of disease mentions was quite different between the two parts of the articles. 32/ <dig> abstracts had no disease mentions, but only a single article body lacked any disease mentions. this corresponded to an average of  <dig> disease mention per abstract and  <dig> mentions per body. the density of disease mentions was significantly different in abstracts and in bodies at p < . <dig> by the mann-whitney u wilcox test, with  <dig>  / <dig> ) disease mentions per thousand words in the abstracts and  <dig>  / <dig> ) disease mentions per thousand words in the bodies.

implications of distributions and densities
a limitation of this paper is that we have not studied the effects of these distributions on any particular task, such as information extraction or information retrieval. however, there is evidence from the work of lin  <cit>  that searching full text is more effective than searching abstracts, especially when the search is restricted to text spans rather than full bodies.

what is and is not normally distributed in biomedical journal articles?
this study allowed us to determine, for a number of categories of data, what is and is not normally distributed between abstracts and article bodies. we summarize this in table  <dig> 

overall, more phenomena are not normally distributed than are normally distributed between the two genres.

CONCLUSIONS
we found that abstracts and article bodies do differ from each other structurally and in terms of discourse features: article bodies have longer sentences and make much heavier use of parenthesized material of various sorts. both of these findings have implications for the role of syntactic parsers in biomedical information extraction. the latter finding uncovers an opportunity for the extraction of various kinds of information from full text that is not available in abstracts. we also found that an important class of language processing system, gene mention systems, performs differently on articles and abstracts, with performance being notably higher on abstracts. part of speech taggers also perform differently, with performance being better on abstracts. distributions of mentions of most semantic classes differed markedly between abstracts and article bodies. a limitation of this study is that it is primarily descriptive. however, it does suggest some directions forward for full-text nlp. overall, these findings suggest that to move forward with text mining from full text journal articles, we will need better parsers, improved ability to handle passives and negation, the ability to deal with parenthesized text, and further attention to the detection of a variety of semantic classes in addition to genes and proteins. this suggests the necessity of retraining gene mention systems and other taggers on full text, and by extension the importance of building full-text corpora. future studies will pursue concrete solutions to these issues.

