BACKGROUND
one of the consequences of the explosion in numbers of fully sequenced and annotated microbial genomes is that we are now facing the challenges of comparative pan-genomics  <cit> . the microbial pan-genome, as defined by  <cit> , is the number of essentially different genes found within a population at a specified taxonomic level, usually within a species, though this can be extended to higher levels, such as genus. as multiple genomes of the same species are sequenced, one can construct the pan-genome, and begin to compare pan-genomes from different species.

having a set of fully sequenced and annotated genomes from several strains within a species, one is interested in two sets of genes. the first is the set of core genes, i.e. the genes found in every strain within a species. the size and content of the core genome is interesting for characterizing the genomic essence of the species. the other set is the pan-genome, which is the total number of different genes found in all strains within the species. the size of this pan-genome, relative to the number of genes found in a typical strain, is an indicator of the plasticity of the species, and could be reflective of its potential for adaptation in a diverse environment.

the true core- and pan-genome sizes, here denoted γ and η respectively, will most likely remain unknown for any species, since it is impossible to sequence and annotate all existing strains. thus, we have to rely on estimates based on existing data. the problem of estimating the size of the core- and pan-genome was first approached by  <cit> . they used an exponential function to explain the number of new genes introduced by each new sequenced genome, and by extrapolating this they came up with some estimates of the pan-genome size. the core-genome size was also estimated in a similar way. modified versions of this approach have later been used by others. for example the number of new escherichia coli genes contributed by each additional genome sequenced was first estimated to be rather large –  <dig> genes by  <cit> . more recent estimates, based on  <dig> different isolates from a wide variety of strains, brought the number of expected novel genes per new genome to be around  <dig>  with approximately  <dig>  genes estimated to be in the total e. coli pan-genome  <cit> . based on comparison of  <dig> e. coli genome sequences, we have previously estimated the number to be around  <dig> novel genes per genome, with a pan-genome size of just under  <dig>  genes  <cit> .

one of the implications of early pan-genome estimates is that some bacterial species might have an "infinite" pan-genome  <cit> . this is a dramatic statement, especially since it can be largely due to a bias from their use of an exponential model, which inherently assumes the pan-genome can be divided into two groups: the core-genes always present in all genomes, and the dispensable genes, equally likely to occur in any genome. the latter part of this assumption is often far from reality, which we will show in this paper. this was also recognized by  <cit> , who was the first to introduce a mixture model to estimate the core- and pan-genome size. unfortunately, they also imposed some rather heavy restrictions in their model, making their pan-genome estimates biased towards larger values.

we will, however, extend the good idea of  <cit>  in this paper, and by avoiding their heavy restrictions hopefully come up with more realistic estimates of core- and pan-genome sizes.

RESULTS
algorithm
gene families
for a given species g different genomes have been sequenced and annotated. the first step in any pan-genome analysis is to come up with a list of gene families in the current sample. a deeper analysis of this problem is not the focus of this paper, and we have at this stage taken the approach used by  <cit>  and  <cit> . first an all-against-all blasting  is performed, and only alignments with at least 50% identity along at least 50% of both sequences are considered. two sequences belong to the same gene family if both their reciprocal alignments fulfill the 50-50-cutoff rule. the results of this procedure is typically stored in a pan-matrix m = {mij} where each row corresponds to a gene family and each column to a genome. if gene family i has at least one member in genome j then mij =  <dig>  else mij =  <dig> 

mixture model
the pan-genome size, η, is the number of gene families found in all strains, also including the gene families not yet observed in the g genomes sequenced so far. summing row i in m we get the number of genomes in which gene family i has been observed. tabulating all these row-sums gives us the number of gene-families observed in  <dig> ..., g genomes, which we denote y <dig> ...yg. the sample pan-genome size is , while yg is usually listed as the sample core-genome size. the true pan-genome size also includes y <dig>  the number of gene families observed in zero genomes so far. hence η = n + y <dig> and estimating η is equivalent to predicting y <dig> 

in order to predict y <dig> we need a model that relates y <dig> to y <dig> ..., yg. consider y = . since the total sum of gene families, η, is constant y is a multiniomial vector if we assume independence between gene families, i.e. y ~ mult. the multinomial probabilities θ =  are the probabilities of a gene family to be detected in  <dig> ..., g genomes, respectively. the expected value of y <dig> is e = ηθ <dig> due to the multinomial model. also, a similar argument leads to e = η. combined they lead to   

using n as an estimate of e we can predict y <dig> if we can estimate θ <dig>  this estimate can be found by assuming some degree of smoothness across the multinomial probabilities. one way of obtaining this is by using a binomial mixture model. this means we assume   

where πk is the mixing proportion and   

is a binomial probability mass function with detection probability ρk. thus, the multinomial probabilities are expressed as a combination of k binomial probability mass functions . the shape and location of these binomial pmfs will determine how θg are related to each other, and more specifically how θ <dig> relates to θg, g =  <dig> ..., g. figure  <dig> illustrates this idea for a three component model, i.e. we use the combination of three binomial pmfs to describe the  <dig> multinomial probabilities. component k in this mixture model may be interpreted as a class of gene families with probability ρk of being detected  in a genome. if ρk is low, these genes are typically rarely observed in the sequenced genomes, and vice versa. a binomial mixture like this was also used by  <cit> .

it is natural to reserve one of the mixture components for the class of core genes. core genes are special, since these genes should always be present in all genomes, and it is natural to assign them detection probability  <dig> , as was also done by  <cit> . we define the first component as the core component, hence ρ <dig> =  <dig> .

estimation
the parameters of the binomial mixture model cannot be estimated directly from y, again because y <dig> is missing. this led  <cit>  to impose some heavy restrictions on their model, which is not necessary. a commonly used approach for such models is to estimate parameters maximizing the zero-truncated log-likelihood  <cit> .

considering a fixed n the vector y+ =  is also a multinomial, with probability θg/ for element g =  <dig> ..., g. thus, the zero-truncated log-likelihood is   

where θ <dig> ..., θg depend on π and ρ as described in  and , and c is a constant independent of these parameters. thus, for some choice of k, we estimate π and ρ by maximizing the criterion in , which only involves . this can be done with some iterative optimization algorithm. these estimates, denoted  and  for k =  <dig> ..., k, are used in  and  to get the estimates of θ <dig> and this is in turn plugged into  to compute the corresponding prediction .

the final part of the estimation procedure is to find the proper number of components k in the binomial mixture, i.e how many binomial pmf do we need to approximate the distribution of the observed data . since our criterion in  is a log-likelihood function for the data, we have adopted the bayesian information criterion  to select the proper model complexity  <cit> , a choice also supported by  <cit> . hence, we look for a k where   

is minimized, where  is the number of free parameters in the model since the sum of mixing proportions is always  <dig>  and the core component has a fixed detection probability ρ <dig> 

once we have determined the proper number of components k the estimated core- and pan-genome sizes are  

where  is the estimated mixing proportion for component  <dig>  the core component.

we have observed that the pan-size estimate may be heavily influenced by the chosen number of components, a generic property discussed by  <cit> . in order to stabilize the estimates,  <cit>  propose a bagging-based estimator, which we have adopted. this is a bootstrap procedure that will smooth the estimate over various choices of components, and making the final estimate more stable.

as an alternative to the binomial mixture model estimate, we have also included the chao lower-bound estimate  <cit>  when fitting to real data. this is a very simple procedure, where the pan-genome size is estimated by  

notice that this corresponds to y <dig> being predicted from y <dig> and y <dig> only.

implementation
all computations, including the parsing of blast results, setting up the pan-matrix and performing all estimations have been implemented in r  <cit>  and is freely available from the corresponding author. an r-package for microbial pan-genomics is under construction and will be made available as soon as it is operational.

testing
estimating core- and pan-sizes
we employed our method to data for  <dig> different bacterial species, who have all at least  <dig> different genomes sequenced and annotated at ncbi  <cit>  on january  <dig>   <dig>  the gene families were computed, for each genome as described above. estimated core- and pan-genome sizes are given in figure  <dig>  it is important to note that in this work we are discussing gene families, and not individual genes; although the two are closely related in bacteria, they are not identical. the number of components in the mixture-models was found by minimizing the bic-criterion. the bars on the right-hand side of figure  <dig> represent the fraction observed so far, of the total estimated pan-genome. francisella tularensis currently has the largest fraction covered, at 73%; this seems reasonable, in that the total pan-genome for an intracellular organism would be expected to be relatively small, compared to environmental isolates. the bacterial species with the smallest fraction of the estimated total pan-genome covered is that of e. coli, with a mere 30% covered so far, based on  <dig> genomes completely sequenced.

distribution of gene families
effect of growing data set
for one of the species, e. coli, we have already  <dig> fully sequenced genomes. still, the coverage, defined as sample pan-genome size divided by estimated pan-genome size, is as low as 30%. an interesting question is of course how many more genomes do we need to sequence in order to have a coverage of, say, 90% of the e. coli pan-genome? upon examination of this question, we discovered that this number appears to grow as more genomes are sequenced. that is, with only a few genomes sequenced, it might appear say that  <dig> genomes might be enough to cover the estimated pan-genome. however, even with only  <dig> genomes sequenced, now it looks as though perhaps around  <dig> additional e. coli genomes would be needed. in coming up with this estimate, we find that, as more e. coli genomes are sequenced, the total estimated diversity increases, resulting in a steep increase in the estimate of the pan-genome total size, as shown in figure  <dig> 

effect of gene prediction
the use of a mixture model makes it apparent that the estimate of pan-genome size must depend on how many gene families we observe in few genomes. especially those gene families observed in only one genome, are most likely important. these genes are often referred to as orfans. upon inspection of the data, we found that the annotation "hypothetical protein" is severely over-represented among the orfans in all  <dig> species . thus, false positives from the gene prediction, i.e. predicted gene who are not actually genes, are most likely influencing the number of orfans most since false positives typically are "hypothetical proteins". this makes the number of orfans uncertain, and estimation of pan-genome size even more difficult.

in order to quantify this effect, we made a re-analysis of the e. coli data, which is the largest data set. first, we removed 10% and 50% of the shortest hypothetical proteins in the data set, because we believe these are the most uncertain predictions. a pan-genome size was estimated for these reduced data sets. next, we also made a completely new prediction of genes for all  <dig> genomes using the easygene tool , and made another estimate from these data as well. the results are displayed in table  <dig>  the number of orfans drops dramatically consistent with the idea that perhaps a large fraction of the orfans are due to artifacts of gene finding. the pan-size estimates also tend to decrease as an effect of this, but the mixture model estimates show some variability.

the number of observed gene families in data set, the number of orfans , chao estimates and binomial mixture estimates of pan-genome size for the original e. coli data as well as reduced data sets. "reduced 10%" means the 10% shortest hypothetical proteins were removed from the original data set, and correspondingly for "reduced 50%". "easygene" is a new data set with genes predicted by the easygene gene prediction tool.

discussion
the use of a binomial mixture model for estimating the pan-genome size was introduced by  <cit> , but the use of mixture models for population size estimation is by no way new, e.g.  <cit> . the estimation of a population size has a long history in ecology, under the names of capture-recapture problems , or in epidemiology, called multiple record systems . mixture models are suitable when we are faced with a larger number of recaptures/records/genomes and heterogeneous detection probabilities, which is exactly the case for pan-genomics.

from our results in figure  <dig> we notice that for none of the species the optimal mixture model has  <dig> components. this would be expected if the gene pool could be divided into core-genes and dispensable genes, as implicitly assumed by  <cit> . there is always at least a third group, and frequently even more. this observation corresponds to the results shown by  <cit> , where they find that for bacteria and archaea in general, genes could be divided into three classes; core , shell  and cloud .

a reason for this heterogeneity in detection probabilities may be skewed sampling. if some of the sequenced genomes are sampled in the same "corner" of the population, the genes characteristic for this "corner" will occur more frequently than they should. another reason may be that some genes are simply frequently occurring in the population, reflecting a divergence from a fairly recent ancestor. in this perspective, it must be expected that there is a large number of true detection probabilities, which is at least partly supported by the fact that the more genomes we consider the more components we estimate .

the fact that microbial genomic diversity is caused by both vertical mutations and horizontal transfer makes it also plausible to expect heterogenous detection probabilities.

from figure  <dig> we also see that even for  <dig> genomes  we only estimate  <dig> components. in  <cit>  a mixture of  <dig> components were used for a data set of  <dig> genomes, which seems to be a too complex model. using too complex mixture models will tend to over-estimate the pan-genome size, since it makes the estimate of the smallest detection probability artificially small.

in figure  <dig> we see that a larger sample pan-genome tends to result in a larger estimated pan-genome.

this is due to the fact that larger data sets allow more complex models, and more complex models allow more extreme estimates. uncertainties, as indicated by the rough confidence intervals, also tend to grow when estimates grow, which is reasonable.

in figure  <dig> we have constructed a way to plot the estimated mixture models for comparative pan-genomics. in this picture the actual size of the core- and pan-genome is not visible, but we focus instead on the relative distribution of detection probabilities. some species, typically have a large proportion of stable genes , while at the other end of the scale we find those with little overlap between genomes. a larger number of components indicates a more complex pan-genome with respect to heterogeneity in detection probabilities.

from the results in figure  <dig> we can compute the coverage for each species, which is simply the size of the sample pan-genome divided by the estimated pan-genome size. ideally, we should expect this to increase as the number of genomes increase, because the sample pan size should approach the true pan size. there is no such tendency in our results. we even observe that two of the largest data sets  have two of the smallest coverages. figure  <dig> also clearly demonstrates that, at least for e. coli, as more genomes become available the pan-genome estimates get even higher. this is typical for a population with a large fraction of orfans. since orfans have a small detection probability, only a few of them will show up in every genome. hence, it requires a substantial number of genomes before we can estimate their true abundance. in this perspective, the binomial mixture model will tend to under-estimate the true pan-size for smaller data sets.

in table  <dig> we show that there are effects of possible false positive predicted genes on the estimates of pan-genome size. by removing hypothetical proteins from the data set, the number of orfans drops. this again leads to a decreased pan-size estimates. predicting new genes with easygene gives the largest reduction in orfans, but the effect on the mixture model estimated pan-size is less. this is due to the fact that the mixture model depends on the entire data distribution, not only the orfans.

our approach assume a closed pan-genome, i.e. η is a parameter. in an open pan-genome, the total number of genes is not fixed, and in a very long term perspective this is most likely the case, assuming new genes form and old genes disappear. however, in a reasonably short time window, the number of genes available to any population must be limited, and can be assumed constant. wether genes are shared vertically or horizontally within the population should have no impact on the closedness of the gene pool.

a recent publication  <cit>  has suggested alternative ways of estimating pan-genome size, based on power-laws and regression. our, more probabilistic approach, is fundamentally different, and more in line with existing methods in capture-recapture modelling. however, as suggested by the results in table  <dig>  a major problem in pan-genome size estimation is the fact that the data themselves are estimates, and thus the uncertainty in the computation of gene families will influence the results, sometimes severely. in order to improve the estimation of bacterial genomic diversity, future efforts should probably be focused on this aspect.

CONCLUSIONS
we have shown how to use binomial mixture models to estimate microbial core- and pan-genome size, and the vast literature on capture-recapture methods should be further exploited in microbial pangenomics, as it has been in closely related fields like metagenomics  <cit> . our results indicate that pan-genomes for bacterial species are in general large compared to the size of individual genomes. especially for e. coli, who has the largest number of completely sequenced and annotated genomes so far, we find that the pan-genome is significantly larger than the human genome. we also show that our pan-size estimates are most likely too moderate since the addition of new genomes tend to push them upwards. in order to improve reliability of estimates, more focus should be devoted to the computation of gene families.

authors' contributions
ls launched the idea of using capture-recapture methods and has done all programming and data analysis. ta has contributed to the choices of statistical methods and how to present them to a broader audience. dwu formulated the problem and supervised the choice of analyses to conduct. ls and dwu drafted the manuscript. all authors have read and approved the final manuscript.

