BACKGROUND
one of the major applications of dna microarray technology is to perform sample classification analyses between different disease phenotypes, for diagnostic and prognostic purposes  <cit> . the classification analyses involve a wide range of algorithms such as differential gene expression analyses, clustering analyses and supervised machine learning  <cit> , etc. in classification analyses of microarray data, gene selection is one of the critical aspects  <cit> . efficient gene selection can drastically ease computational burden of the subsequent classification task, and can yield a much smaller and more compact gene set without the loss of classification accuracy  <cit> . in addition, a smaller number of selected genes can be more conveniently and economically used for diagnostic purposes in clinical settings  <cit> .

in the presence of thousands of genes in microarray experiments, it is common that a large number of genes are not informative for classification because they are either irrelevant or redundant  <cit> . based on a review of the definitions of relevance  <cit> , the genes can be classified into three disjoint categories, namely, strongly relevant, weakly relevant, and irrelevant genes  <cit> . strong relevance indicates that the gene is always necessary for an optimal subset and cannot be removed without affecting the classification accuracy. weak relevance indicates that the gene is not always necessary but may become necessary for an optimal subset at certain conditions. irrelevance indicates that the gene is not necessary at all for classification accuracy. hence, an optimal gene subset should include all strongly relevant genes, none of irrelevant genes, and a subset of weakly relevant genes. based on the relevance definitions of genes, in classification analyses, a redundant gene is the one  does not provide much additional information if another informative gene is already present in the chosen gene subset.

various approaches have been developed for gene selection to extract relevant genes from thousands of genes in microarray experiments, such as clustering methods  <cit>  and pair-wise correlation analyses  <cit> . while multiple methods are available, it is well accepted in the field that a good gene selection method should be able to: 1) simplify the classifier by retaining only the relevant genes  <cit> ; 2) improve or not significantly reduce the accuracy of the classifier; and 3) reduce the dimensionality of the dataset  <cit> .

traditionally, the methods for gene selection are broadly divided into three categories: filter, wrapper and embedded methods  <cit> . a filter method relies on general characteristics of the training data to select genes without involving any classifier for evaluation  <cit> . many filter methods are usually mentioned as individual gene-ranking methods  <cit> . they evaluate a gene based on its discriminative power for the target classes without considering its correlations with other genes. although such gene ranking criteria are simple to use, they ignore correlation among genes, which may result in inclusion of redundant genes in selected gene set used for classification  <cit> . redundant genes will increase the dimensionality of the selected gene set, and in turn affect the classification performance, especially on small samples  <cit> . in order to minimize redundant genes, correlation analyses have been incorporated in gene selection to remove redundant genes and improve classification accuracy  <cit> . the wrapper methods utilize the classifiers as evaluation functions and search for the optimal gene set for classification  <cit> . but the wrapper methods may suffer from excessive computational complexity. in contrast to the filter and wrapper approaches, the embedded methods perform the selection of genes during the training procedure and are specific to the particular learning algorithms  <cit> . for the wrapper and embedded methods, the search schemes are always involved to identify the optimal gene set for the sample classification. searching the whole gene subset space may discover the optimal gene subset with respect to an evaluation criterion. however, an exhaustive search is usually computationally prohibitive. thereby some partial search schemes are proposed, such as sequential forward selection, sequential floating forward selection, sequential backward elimination, sequential floating backward elimination and random search  <cit> . these partial search schemes are practically more feasible but provide no guarantee for identifying the optimal gene set  <cit> .

in the earlier literatures, some excellent studies have highlighted the advantages of controlling classification error in yielding an optimal gene set, such as the study in the reference  <cit> . in this article we develop a novel gene selection method based on the bayes error. although the bayes error has been used for feature selection in classification analyses, its use for gene selection in microarray data is very rare. it is well known that the bayes error can provide the lowest achievable error rate bound for a given classification problem  <cit> . theoretically, the bayes error is the best criterion to evaluate effectiveness of gene set for classification  <cit> , and the bayes error depends only on the gene space, not the classifier itself  <cit> . from this point of view, by controlling the bayes error it is feasible to find the optimal or sub-optimal gene set for a given classification problem without designing the classifiers. however, it is usually difficult to estimate directly the bayes error rate analytically. an alternative is to estimate an upper bound of the bayes error, which could be obtained by an error estimation equation based on the bhattacharyya distance  <cit> . with this method, we can indirectly use the bayes error for gene selection by controlling the upper bound of the bayes error. this strategy is more promising than those requiring gene selection and classifier design simultaneously, as in the wrapper methods. considering the promising aspects of the bayes error, we propose in this study an approach, bbf , for gene selection. our selection algorithm is implemented in two steps: 1) first the relevant candidate genes are selected by a criterion function; and 2) the criterion controlling the upper bound of the bayes error is applied to the relevant candidate genes in order to remove the redundant genes.

application
to evaluate the performance of our proposed method in practice, we analyzed five publicly available microarray datasets: 1) colon cancer dataset; 2) dlbcl dataset; 3) leukemia dataset; 4) prostate dataset; 5) lympho ma dataset .

colon cancer dataset
this dataset consists of expression levels of  <dig> samples of which  <dig> samples are colon cancer samples and the remaining are normal samples  <cit> . although originally expression levels for  <dig>  genes are measured,  <dig>  genes out of all the  <dig>  genes were removed considering the reliability of measured values in the measured expression levels. the measured expression values of  <dig>  genes are publicly available at  <cit> .

dlbcl dataset
this dataset contains  <dig> samples in two classes, diffuse large b-cell lymphomas  and follicular lymphoma , which have  <dig> and  <dig> samples, respectively  <cit> . the original dataset contains  <dig>  genes. after the quality control, the dataset contains  <dig> samples and  <dig>  genes. the measured expression values of genes are available at  <cit> .

leukemia dataset
this dataset, provided by golub et al.  <cit> , contains the expression levels of  <dig>  genes for  <dig> patients of acute lymphoblastic leukemia  and  <dig> patients of acute myeloid leukemia . after data preprocessing,  <dig>  genes remain. the source of the  <dig>  gene expression measurements is publicly available at  <cit> .

prostate dataset
this data set provides the expression levels of  <dig>  genes for  <dig> normal tissues and  <dig> prostate cancer tissues  <cit> . the experiments were run on affymetrix human 95av <dig> arrays. the data preprocessing step leaves us with  <dig>  genes. the data source is available at  <cit> .

lymphoma dataset
this dataset presented by alizadeh et al.  <cit>  comprises the expression levels of  <dig>  genes. it contains  <dig> samples and two classes: germinal center b cell-like dlcl  and active b cell-like dlcl. among the  <dig> samples,  <dig> samples are germinal center b-like dlcl and  <dig> samples are active b cell-like dlcl. the dataset is available at  <cit> .

RESULTS
in the gene preselection step, we select the genes with fwer ≤  <dig>  . in our experiments, knn and svm classifiers are employed to demonstrate the proposed method and its classification performance. we choose the euclidean distance in our knn classifier with k =  <dig> and predict the class label by a majority vote. for the svm classifier, we choose a linear kernel for decision plane computation.

we assess the performance of our method using the "leave-one-out cross validation" . loocv provides realistic assessment of classifiers which generalize well to new data  <cit> . the loocv method proceeds as follows: hold out one sample for testing while the remaining samples are used to make the gene selection and train the classifier. note that to avoid selection bias  <cit> , gene selection is performed using the training set. the genes are selected by our method using the training samples and then are used to classify the testing sample. the overall test error rate is calculated based on the incorrectness of the classification of each testing sample. table  <dig> summarizes classification errors of five datasets with knn and svm classifiers by our method.

for the colon dataset, as shown in table  <dig>  using the bbf method,  <dig> out of  <dig> samples are incorrectly classified by knn and  <dig> by svm, resulting in an overall error rate of  <dig> % and  <dig> %, respectively. according to the results of ben-dor et al.  <cit> , without gene selection the classification error was  <dig> % for knn, and  <dig> % for svm, respectively. this is a significant improvement compared to the accuracy obtained by all available genes. this implies that there are irrelevant or redundant genes which deteriorate the performance of the classifiers, and the appropriate gene selection could effectively improve classification accuracy. the colon dataset has been used by many studies. for example, liu et al.  <cit> , used "normalized mutual information" with greedy selection and simulated annealing algorithm for gene selection. they reported that using knn classifier the classification error is  <dig> % with  <dig> selected genes for greedy selection and  <dig> % with  <dig> selected genes for simulated annealing algorithm. ding and peng  <cit>  proposed a "minimum redundancy – maximum relevancy"  method. their best result by svm was  <dig> % with  <dig> genes, which means  <dig> out of  <dig> samples are incorrectly classified. compared with our results, liu's method  <cit>  used more genes for similar classification errors. using the svm classifier, ding and peng  <cit>  also selected  <dig> genes for best classification accuracy and the accuracy is slightly higher than ours. some studies demonstrate that accurate diagnoses could be achieved using the expression levels of 15– <dig> genes from colon dataset  <cit> . to sum up the above results, the necessary number of genes could be less than  <dig> for the colon dataset.

for the dlbcl dataset, in the original article, shipp et al.  <cit>  picked  <dig> genes by using their own weighted combination of informative genes. they correctly classified  <dig> out of  <dig> patients for a diagnostic error of  <dig> %. using our method  <dig> out of  <dig> samples are incorrectly classified by knn and  <dig> by svm, resulting in an overall error rate of  <dig> % and  <dig> %, respectively. however, only 5– <dig> genes are involved in classification and obtain similar classification error. yang, et al  <cit>  proposed gs <dig> and gs <dig> methods based on the ratio of inter-class and intra-class variation as a criterion function for gene selection. using knn they obtained classification error rate of  <dig> % with  <dig> genes by gs <dig> and  <dig> % with  <dig> genes by gs2; using svm they achieved classification error rate of  <dig> % for gs <dig> with  <dig> genes and gs <dig> with  <dig> genes. in contrast, we note that our results with knn method are almost as good as theirs, yet only  <dig> genes are involved in the classification procedure and our classification error with svm method is slightly higher than their results, but we use only  <dig> genes to reach similar level of performance.

for the leukemia dataset, using the bbf method, all samples are correctly classified by knn and svm, with  <dig> and  <dig> genes for the two classifiers, respectively. compared with other gene selection methods, our method appears to yield higher classification accuracy. for example, dettling and buhlmann  <cit>  adopted four classifiers  to classify leukemia dataset, and classification error was calculated based on loocv method. their best classification error result was  <dig> % with  <dig> genes by knn. some studies also come up with similar results to ours. weston et, al  <cit>  reported 0% classification error for a linear svm using  <dig> genes by loocv method, but they used more genes.

for the prostate dataset, in our results,  <dig> out of  <dig> samples are incorrectly classified by knn and  <dig> by svm, resulting in an overall error rate of  <dig> % and  <dig> %, respectively. dettling and buhlmann  <cit>  proposed an algorithm for selecting supervised clusters of genes to find the gene groups for classification. they used knn and aggregated trees methods, achieving the best results of classification error being  <dig> % with  <dig> gene clusters by knn. our results are comparable to theirs, but used fewer genes. gentile  <cit>  used on a incremental large margin algorithm for gene selection and yielded  <dig> % classification error estimated with  <dig> genes by loocv method. in contrast, we only used 11– <dig> genes and achieved better accuracy.

for the lymphoma dataset, using our method, the classification error rates of knn and svm are  <dig> %  and 0%, respectively. wang et, al.  <cit>  also analyzed this dataset using several different gene selection methods and classifiers by the loocv method. in their study, they combined the locally linear embedding method and svm classifier and yielded the classification error of  <dig> % with  <dig> selected genes. when combining signal-to-noise method with knn classifier, the classification error was  <dig> %. the best error rate result they reported was  <dig> % with  <dig> genes which is obtained by adopting the information gain method and a neuro-fuzzy ensemble model. diaz-uriarte and alvarez de andres  <cit>  reported the similar results to ours with the random forest method for gene selection, though they used a different estimation method for classification error rate. but they used more genes than our method. the results of our bbf method are comparable or outperform the above other results.

discussion
as an important statistical index for classification analyses  <cit> , the bayes error has rarely been used in classification analyses of microarray data. in this study, we propose a novel gene selection approach for microarray classification analyses. we introduce the bayes error into the gene selection procedure, which turned out to be beneficial for classification analyses. the experimental results show that our proposed method can 1) reduce the dimension of microarray data by selecting relevant genes and excluding the redundant genes; and 2) improve or be comparable to the classification accuracy compared with other earlier studies.

in classification analyses, the classification error is an important criterion for selection of an optimal gene set. some gene selection methods have been proposed to achieve the minimum classification error. among these methods, a typical one is the method proposed by peng et al.  <cit>  which incorporates the classification error estimation with the gene selection method to determine an optimal gene set. the method first selects the genes with the highest relevance to the target classes, and minimizes the redundancy among the selected genes, and then determines an optimal gene set which has the minimum classification error estimated by cross-validation methods. in addition, the researchers presented the theoretical analysis  <cit> and comprehensive experimental studies  <cit>  to prove that the criterion of "maximum relevance and minimum redundancy" can benefit selection of optimal features for classification. similar to the method of peng et al.  <cit> , our method uses the bhattacharyya distance to select the genes with the highest joint relevance to the target classes, while minimizing the redundancy among the selected genes. meanwhile, we can indirectly control the classification error due to the relationship between the bhattacharyya distance and the bayes error, and thus we can effectively avoid the computation of cross-validation error.

from the bayesian decision theory, it is known that 1) the probability of error of any classifier is lower bounded by the bayes error, 2) the bayes error only depends on the gene space, not the classifier itself, and 3) there is always at least one classifier that achieves this lower bound  <cit> . hence, from a theoretical point of view, it is possible to find out an optimal gene set for a given classification problem, rendering the minimum classification error. when selecting a set of relevant genes g with a minimum bayes error in all gene space, according to the theories of the bayes error, it can be guaranteed that at least one classifier may achieve this classification error. however, it should be noted that this optimal relevant gene set is classifier-specific. as pointed out  <cit> , there are no relevancy definitions independent of the classifiers. that means not all classifiers can achieve the minimum bayes error with the gene set g. this is because different classifiers have different biases and a gene which may be favorable for one classifier but may not for another. this phenomenon is also observed in our results. for example, in the colon dataset, for svm classifier the best classification error was  <dig> % with  <dig> genes, while for knn classifier we could achieve the best classification error with  <dig> genes. when more genes are involved, the error rate for knn classifier will increase. since no single subset is optimal for all classifiers, it would be sensible to adopt a strategy to incorporate a classifier into gene selection for classification like a wrapper method. as the two-stage algorithm proposed in the early study  <cit> , the first stage is to select relevant genes and eliminate redundant genes; the second stage is to search a more compact gene set for a specific classifier. this algorithm may not only yield an optimal or sub-optimal gene set for a specific classifier and increase the classification accuracy, but also decrease computation complexity when compared to a wrapper method. our method can be extended to adopt this algorithm.

in classification analyses, genes obtained from observations may not be all informative for target classes. it is necessary to pick out the relevant candidate genes even though some of them are redundant. for our gene selection system, a gene preselection step is used to select the relevant candidate genes based on their individual relevance to the target classes. but the gene preselection step alone cannot yield the optimal gene set for a classification problem because it cannot eliminate the redundant genes due to the correlations between genes  <cit> . efforts have been made to minimize the redundancy by measuring pair-wise gene correlations within the selected gene set  <cit> , performing clustering analyses  <cit>  and markov blanket filtering  <cit> . our proposed bbf method identifies the redundant genes by using bhattacharyya distance measure to minimize the bayes error. it is clear from equation  <dig> that if a gene is highly correlated with another gene, combination of these two genes may not contribute more to the bhattacharyya distance measure between two classes than any one of them. for an extreme example, i.e., the correlation of two genes is  <dig>  it is impossible to calculate inverse matrix of covariance matrix for bhattacharyya distance measure between two classes, and thus the redundant gene can be eliminated.

in addition, upper bound of the bayes error  is a critical parameter in this gene selection scheme. in the bbf method, we select genes according to their contribution to bhattacharyya distance, db. it has been proved that ε*b monotonically increases as db increases, but at a decelerating manner since the rate of increase of ε*b decreases with the increasing of db  <cit> . when db increases to a certain level, e.g.,  <dig> , increasing db may not efficiently improve classification accuracy. under this condition, with more genes selected, their contributions to classification accuracy turn out to be increasingly negligible. in the case of high ε*b, it may lead to a loss of some relevant genes; in the case of smaller ε*b, it may involve some genes of negligible effects for classification. with this in mind, we set a criterion, ε*b being  <dig> e- <dig>  for picking relevant genes.

CONCLUSIONS
in summary, the bbf method can effectively perform gene selection with reasonably low classification error rates and a small number of selected genes. our method may not only obtain a small subset of informative genes for classification analyses, but also provide a balance between selected gene set size and classification accuracy. this is confirmed by testing our method on the  <dig> real datasets.

