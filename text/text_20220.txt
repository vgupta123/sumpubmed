BACKGROUND
the problem of predicting protein structure from amino acid sequence has yet to be fully solved, and experimentally determining protein structures requires extensive human input. due to the relative ease of determining amino acid sequences, and the utility of structural information, the problem has attracted much attention. as the accuracy of protein structure prediction algorithms has greatly improved over the last ten years  <cit> , the ability to precisely determine the quality of protein structure predictions has gained importance. in an attempt to motivate improvements in this area, the most recent session of the critical assessment of structure prediction  included a model quality assessment category  <cit> . casp is a biennial competition  that has been organized to motivate advancements in the area of protein structure prediction. for this new category, given a putative structure, competitors were asked to submit a quality score between  <dig>  and  <dig> , or to assign an error estimate  to each residue of the structure. twenty-eight groups submitted full structure quality estimates, and eight of those submitted per-residue error estimates.

these eight groups assess structure quality using a variety of methods, which can be broadly grouped into four classes. the first class of techniques learns models to assess the per-residue error based on intrinsic properties of the predicted structures. the second class forms a prediction based on the quality of a target-template alignment. the third class determines the error by taking into account different per-residue error predictors. finally, the fourth class relies on the consensus of the predictions obtained from different protein structure prediction servers. techniques in the first class include proq  <cit> , victor/frst  <cit>  and quad  <cit> . proq uses neural networks to learn a model based on atom-atom contacts, residue-reside contacts, and agreement with predicted secondary structure. victor/frst uses a linear combination of four potential energy functions: a pairwise potential, a solvation potential, a torsion angle potential and a hydrogen bond potential. the relative weights for each function are optimized on the casp <dig> decoy set. quad combines secondary structure, hydrogen bonding, and solvent accessibility information to produce a fitness score for each residue in its structural environment. techniques in the second class include proqprof  <cit>  and subway  <cit> . proqprof trains a neural network on profile-profile comparisons from pairs of profiles for target/template sets. subway generates multiple suboptimal alignments to a template for a target sequence. the average deviation between the suboptimal alignments and the optimal alignment serves as an indicator of structure quality.  techniques in the third class include proqlocal  <cit>  and meta-mqap  <cit> . a prediction by proqlocal is a sum of the scores from proqprof and proq. meta-mqap queries other quality assessment servers and combines the results. lastly, pcons  <cit>  makes up the fourth class, which is based on the idea that within an ensemble of structures predicted from different servers for the target protein, recurring structures and structural motifs have an increased probability of being high-quality . driven by this principle, pcons determines the quality of a structure in two steps. first, it performs pairwise lgscore alignments  <cit>  between the query structure and all structures in the ensemble, and second it uses the average s-scores  <cit>  computed from these alignments to determine the per-residue error predictions. the results of the casp <dig> competition showed that pcons was the most successful approach in predicting both complete structure quality and per-residue errors and it significantly outperformed the next best scheme.

in this paper we focus on improving the per-residue error estimates using consensus-based methods. first, we examine several static methods for consensus prediction. second, we look at whether different predictors provide enough consistency for machine learning models to be effective. in doing so, we present the use of a linear perceptron, standard regression, support vector regression and a simple weight learning technique to learn an appropriate aggregation scheme. extensive experimental evaluation on data obtained from the casp <dig> and casp <dig> competitions demonstrates that constrained regression provides the biggest gains in performance over the previous best performing scheme . the results of these experiments indicate that an improved model quality assessment server could be built. this server would rely on a series of predictions from existing assessment servers in order to build a constrained regression model. given a new structure for assessment, the server would first query these servers. then, it would apply the constrained regression model to produce the final quality assessment.

RESULTS
static consensus error prediction methods
the work in this paper builds upon a consensus-based per-residue error prediction method rooted on the same principles introduced by pcons. the general procedure works as follows. let x be the amino acid sequence of the query protein and let sx be its predicted 3d structure. let  be the true 3d structure for x and let  be the structures of x predicted by k different structure prediction methods. for each residue xi of x, let dj be the distance between the ith residue of sx and the ith residue of  obtained after structurally superimposing sx and  using the lga program  <cit> . the predicted distance d between residue xi in sx and   is given by

   

where wj is a weight associated with the jth predictor and ∑jwj =  <dig> . the idea behind this approach, which is shared by pcons, is that each of the k structures over which it averages can be treated as an expert's prediction for x's real structure. thus, the per-residue error can be determined by a weighted average over the per-residue distances to the structure of each expert. the various wj parameters control how the distances between the query and the predictors are weighted. a straightforward approach is to treat each of the predictors equally by making all these weights equal , which corresponds to simple averaging.

the above method differs from pcons in two important ways. first, it uses lga alignments as opposed to lgscore alignments. an lga alignment is constructed by attempting to optimize both the longest continuous sequence that can fit under a certain cutoff, as well as a global measure of alignment quality. an lgscore alignment attempts to find the most significant  <cit>  non-continuous segment of a model. second, it averages the raw distances as opposed to s-scores  <cit> . s-scores were developed as part of an improvement over root mean square deviation calculations for global structural comparison  <cit>  .

to understand the impact of these choices, we examine the performance obtained by four different schemes. the first two schemes  use lga to compute alignments, and average either raw distances or s-scores, whereas the second two schemes  use lgscore to compute alignments, again averaging either raw distances or s-scores. using lgscore alignments and averaging s-scores is identical to the pcons approach, and so we equivalently refer to pcons as the lgscore-s-score scheme. the prediction performance achieved by these methods on two datasets obtained from the casp <dig> and casp <dig> competitions, , is shown in table  <dig>  the performance was assessed by calculating both the pearson correlation coefficient  and the root mean squared error  between the actual and predicted per-residue errors.

values in this table represent the pearson correlation coefficient  and root mean squared error  between the predicted and true per-residue distances. boldfaced entries correspond to the best performing scheme for each dataset and performance assessment metric.

these results show that the performance of the various schemes differs across the two datasets. for cd <dig>  lgscore-s-score achieves the best cc while lga-s-score obtains the lowest rmse. for the cd <dig> dataset, lga-distance shows the best performance in terms of cc and rmse. the improvements achieved by the best performing schemes on the two datasets and metrics when compared to the next best schemes are significant at p ≤  <dig> . comparing the performance of the schemes using distance-based averaging over those that average s-scores, we see that the former lead to better results on cd <dig>  whereas their relative performance on cd <dig> is mixed. a similar dataset specific behavior can be observed by comparing the performance achieved by the two alignment methods. lga-based alignments perform consistently better on cd <dig> whereas the relative performance of lga- and lgscore-based alignments on cd <dig> changes based on the averaging scheme  and performance metric.

learning-based consensus error prediction methods
motivated by the inconsistent performance achieved by the four consensus-based per-residue error prediction schemes discussed in the previous section, we investigate the extent to which we can obtain a consensus prediction method that achieves both a consistent and better performance across different datasets and performance assessment metrics. toward this goal we focus on techniques that instead of treating each of the predictors equally by statically setting the wj parameters to 1/k, they use machine learning methods to estimate the various wj parameters directly from the data. we formulate the problem of learning the wj weights as the following supervised learning problem. given a set of training examples xi, each described by the tuple , ⟨d <dig>  d <dig> ..., dk⟩), where dt is the actual distance between the ith residue of sx and the ith residue of x's true structure, learn the set of wj values such that

   

is minimized.

four different schemes were used to learn these weights: support vector regression , linear percep-tron, least-squares regression, and a variant of the least-squares regression that constraints the wj weights to be non-negative. note that with the exception of the linear perceptron, the other three learning methods do not enforce the constraint that ∑jwj =  <dig> . most of the learning schemes  outperform the static prediction methods  on both the cd <dig> and cd <dig> datasets. the overall best results for both datasets were obtained by the constrained regression method, which achieved an improvement in terms of rmse of  <dig> % and  <dig> % over the best performing static schemes for cd <dig>  and cd <dig> , respectively. its performance in terms of cc was similar to that of the best static scheme for cd <dig> and better by  <dig> % for cd <dig>  note that the relatively higher gains in terms of rmse when compared to the improvements achieved in terms of cc is due to the fact that the objective function of the learning methods  is designed to directly minimize the rmse of the predictions. overall, the prediction improvements and consistency achieved by the four learning schemes and constrained regression in particular show that learning methods can be used to learn how to weight the predictions of each server in order to obtained better per-residue error estimates.

values in this table represent the pearson correlation coefficient  and root mean squared error  between the predicted and true per-residue distances. boldfaced entries correspond to the best performing scheme for each dataset and performance assessment metric.

since the nature of the model learned by the above methods is that of weighting the distances of the predictions obtained from each server, an important question to answer is how these weights are related to the overall quality of the predictions produced by each server. figure  <dig> plots the weights learned by the constrained regression method against the quality of the structures generated by each server. the quality of a predicted structure was measured by its gdt_ts score. the values for the model weights and server gdt_ts scores correspond to the averages over the  <dig> and  <dig> structures used in the cd <dig> and cd <dig> datasets, respectively. from these plots we can see that even though there is a correlation between the weights and the gdt_ts scores , this correlation is not perfect. there are high-quality servers that are assigned low weights as part of the training, indicating that the information provided by them is redundant for estimating the per residue error. on the other hand, there are servers that do not perform extremely well, but they are still utilized by the learned model, suggesting that they provide key information for improving the error prediction.

eliminating missing predictions
a potential issue with the above problem formulation is that, due to the way the training set is constructed, it may contain missing values. the missing values arise when a predictor could not provide a prediction for an entire protein, or just some of its positions. these missing values are assigned a value of zero, which may confuse the learning algorithms, as a zero value can mean two different things depending on its source.

if the original dj value is just missing, then a zero is uninformative. however, if the zero represents a true dj value of zero then the distance between sx and  at position i is zero. this means that the two structures align perfectly at this position. in this case dj should be treated as a zero, but in the former case dj should simply be ignored. in order to address this problem we developed two methods for adjusting the training data in order to compensate for the issue of dj values of zero.

the first method eliminates the confusion due to missing values by simply filling the missing values using estimates obtained from the other examples in the training set.  the results achieved by this approach  show that there is no clear advantage in building models on the filled datasets. excluding the linear perceptron, which shows a markable decrease in rmse performance, the gains achieved by the other methods are limited.

values in this table represent the pearson correlation coefficient  and root mean squared error  between the predicted and true per-residue distances. boldfaced entries correspond to the best performing scheme for each dataset and performance assessment metric.

the second method addresses the missing values problem by creating a custom training set for each test position encountered. specifically, for a given test position, the training set contains only those examples that have at least the same set of predictors present as those in the test position. as the set of positions tested by the custom training set approach are a subset of the cd <dig> and cd <dig> datasets, we refer to them as the pd <dig> and pd <dig> datasets, respectively. exactly how these datasets differ from the cd <dig> and cd <dig> sets is described in the section called "evaluating weight learning algorithms". values for wj are learned from the training set, and these weights are used to classify the query position q according to equation  <dig>  table  <dig> shows the results from testing models built using such custom data sets.

values in this table represent the pearson correlation coefficient  and root mean squared error  between the predicted and true per-residue distances. boldfaced entries correspond to the best performing scheme for each dataset and performance assessment metric.

because these results were obtained on the pd <dig> and pd <dig> datasets they are not directly comparable to those reported in tables 2– <dig>  which were obtained on the cd <dig> and cd <dig> datasets. for this reason, table  <dig> also shows the performance of the weight-learning methods trained on the entire training set and the static consensus error prediction methods. these results are shown under the headings "global training" and "static consensus", respectively. analyzing the performance of the custom training methods we observe that constrained regression once again shows consistently good performance, with the best correlation coefficient for both pd <dig> and pd <dig>  constrained regression also shows the best rmse for pd <dig>  and is within 4% of the best rmse  for pd <dig>  however, comparing the results obtained by the custom training methods over those obtained by the methods trained using the entire training set , we see that the latter achieve consistently better performance. an explanation for this performance degradation is that the size of the training set of each custom training problem is relatively small, and as such the models are not as effective as those learned on the entire dataset.

CONCLUSIONS
the results presented in this study reveal several interesting trends. first, a machine learning framework can be utilized to learn models that intelligently weight the different servers in the context of consensus-based error prediction. second, constrained regression outperforms or performs on par with more complicated learning techniques while preventing over-fitting. this performance is consistent across two casp datasets, in which care was taken to include as much data as possible. third, filling missing values with an approximation does not produce consistent gains in performance. in the same vein, customizing training sets to assist machine learning techniques does not produce substantial gains in performance over static prediction methods on the same data. moreover, their overall performance is considerably worse than that obtained by the models trained using all the training data, even when they contain missing values.

the types of models studied in this paper learn how to weight the predictions of a fixed set of protein structure prediction methods. consequently, in order to use these models for assessing the quality of a protein's predicted 3d structure, the protein needs to also be predicted by the same set of prediction methods on which the model was trained. in our study, the training and testing of these models are done in the context of data obtained in the course of a casp competition, satisfying the above requirement. however, models of this type can also be trained and applied in order to assess the quality of a protein's predicted 3d structure outside the context of a casp competition. specifically, a set of existing publicly available structure prediction servers will be used to define the fixed set of protein structure prediction methods whose weights will be learned by the models. by querying this set of servers with sequences from recently released structures, the resulting predictions can be used for training. during error prediction, the same set of servers will be used to predict the structure of the query protein x and their pairwise lga alignment with sx will provide the dj distances in equation  <dig> in order to compute the per-residue error estimates along with the weights learned during training. moreover, as new prediction servers become available and old ones drop out of favor, the underlying models can be retrained to use the new set of prediction methods. note that in the above framework, by restricting the training set to sequences that have only been recently structurally characterized ensures that the prediction servers actually make predictions, rather than just querying a structure database for the true structure. finally, as the results in figure  <dig> indicate, for many servers, the weights that are learned are either zero or very small, suggesting that the above framework can achieve good performance with a relatively small number of servers.

