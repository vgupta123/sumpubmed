BACKGROUND
related work
metabolic networks allow the modelling of molecular systems to understand the underlying biological mechanisms in a cell  <cit> . metabolic networks are represented by the set of metabolic pathways. metabolic pathways are a series of biochemical reactions, in which the product  from one reaction serves as the substrate  to another reaction. the experimental determination of metabolic networks, based on known biological data such as dna or protein sequences, or gene expression data, is still very challenging  <cit> . thus, there have been several efforts to develop supervised learning methods to determine genes coding for missing enzymes and predict unknown parts of metabolic networks  <cit> .

most of the methods to predict metabolic networks assume that the genome annotation is correct, e.g., pathway tools  <cit> , a software application to predict metabolic networks using information from biocyc databases  <cit> . pathway tools uses a two part algorithm, in which part  <dig> infers the reactions catalyzed by the organism from the set of enzymes present in the annotated genome, and part  <dig> infers the metabolic pathways present in the organism from the reactions found in the part  <dig>  considering biocyc and metacyc have a huge amount of available data, this application can potentially make precise metabolic pathway predictions  <cit> . however, part  <dig> is based on the annotated genes, and if there are errors in the annotation, the inferred pathways will not be correct. therefore, these methods intrinsically carry error accumulations due to incorrect genome annotations.

to tackle this problem, we have previously proposed using information directly related to the sequence as the primary data   <cit> . as a result, we obtained the best accuracy values using support vector machine  methods combined with string kernels representing the sequence data. we experimentally demonstrated that svms supersede other methods, such as matrix kernel regression, for predicting metabolic networks. this is consistent with recent results showing the usefulness of svms in bioinformatics  <cit> . however, our solution  <cit>  was computationally expensive in terms of execution time because of sequence data manipulation.

other authors have also combined svm and other supervised learning techniques with kernel methods to predict metabolic networks . the main advantage of using kernel methods is that heterogeneous data can be represented and combined simultaneously. thus, if disparate types of data can be manipulated as kernels, data from many sources can be made to contribute uniformly to the information in a training set when building a model  <cit> .

yamanishi  <cit>  and kotera et al.  <cit>  described the theory and implementation of genies, a web application that allowed prediction of the unknown parts of metabolic networks using supervised graph inference and kernel methods. several algorithms were implemented in genies to find the decision or predictive functions for supervised network inference. some of these algorithms were kernel canonical correlation analysis   <cit> , expectation-maximization  algorithm  <cit>  and kernel matrix regression   <cit> . the authors developed several experiments, but they did not use sequence data. therefore, one of the motivations to extend our previous research  <cit>  was to use sequence data combined with these algorithms. as noted above, we obtained the best accuracy values with the svm method combined with sequence kernels, but with high execution times.

to address these high computational costs, we consider the results from allauzen et al.  <cit> , who proposed a method to predict protein essentiality using svms and manipulating sequence data using rational kernels. the authors designed two sequence kernels , which are instances of rational kernels. to handle the large amount of data , automata representation was used to create the rational kernels. their results showed that the final kernels favourably predicted protein essentiality. we note, however, that none of the previous works using rational kernels in bioinformatics  have considered problems related to biological network predictions.

based on the fact that the rational kernels described by allauzen et al.  <cit>  can be extended to other problems, we define new kernels to be applied to metabolic network predictions. in this research, we represent sequence data using rational kernels. rational kernels take advantage of the fast algorithms for, and efficient representation of, transducers for sequence manipulations to improve performance. as sequence data can be used, raw genomic or proteomic information may be considered, and this method avoids problems associated with incorrect annotation when predicting metabolic networks. additionally, the current work is the first to combine rational kernels   with known pairwise kernels  to obtain pairwise rational kernels. while the kernel techniques proposed in this paper can be applied equally to any machine learning tools that employ kernel methods, such as kcca, em or kmr, we have focused on svms as an illustration of their capability to reduce computational costs. we have also chosen svm methods in light of the experimental results we obtained in previous works  <cit> , as well as the efficiency and effectiveness of svm methods to predict protein essentiality  <cit> .

automata and transducers
automata define a mathematical formalism to analyze and model real problems through useful machines  <cit> . an automaton has a set of states , and transitions . the automaton moves from one state to another state  when activated by an event or function. one variant of an automaton is called finite state machine. a finite-state machine can be used to model a simple system, such as turnstiles or transit lights, or complex systems such as sophisticated spaceship controls  <cit> .

automata work on sequence of symbols, where Σ∗ denotes all the finite sequences using the symbols on the alphabet Σ, including ε that represents the empty symbol. in order to formally define automata and transducers, we will follow the notations used by cortes et al.  <cit> . an automaton a is a 5-tuple   <cit>  where Σ is the input alphabet set, q is the state set, i⊂q is the subset of initial states, f⊂q is the subset of final states, and δ⊆q××q is the transition set. a transition ι∈δ describes the actions of moving from one state to another when a condition  is encountered.

similarly, a finite-state transducer  is an automaton where an output label is included in each transition in addition to the input label. based on the above definition, a fst t is a 6-tuple   <cit> , where the new term Δ is the output alphabet and the transition set δ is now δ⊆q×××q. similar to the previous definition, a transition ι∈δ is the action of moving from one state to another when the input symbol from Σ is encountered and the output from Δ is produced.

in addition, automata and finite-state transducers can be weighted, where each transition is labelled with a weight. thus, a weighted automaton  is a 7-tuple  and a weighted finite-state transducer  is a 8-tuple   <cit> , where the new terms λ and ρ are: , the initial weight function, and , the final weight function. the new transitions for the was and wfsts are  and , respectively, where  represents the weights as real numbers.

as an example, a weighted transducer is shown in figure  <dig>  we use as delimiters the colon to separate the input and output labels of the transitions and the slash to separate the weight values . states are represented by circles, where the set of initial states are bold circles and the set of final states are double circles. only the initial and final states have associated weighs . example  <dig> shows how to compute the weight to the transducer t ) for two given sequences x and y. in this case, we define the alphabets Σ={g,c} and Δ={g,c}.figure  <dig> 
weighted transducer and weighted automaton representing sequences in the alphabet
Σ=Δ={g,c}
.
 weighted transducer t.  weighted automaton a .



example 1
the weight  associated to the transducer t in figure  <dig> for the pair =∈Σ∗×Δ∗ is computed as: t=1∗2∗3∗6∗1+1∗3∗1∗4∗1= <dig>  considering that there are two accepting paths labelled with input gcc and output ccg. these paths are: path 1:state 0↦state 0↦state 1↦state  <dig>  path 2:state 0↦state 1↦state 2↦state  <dig>  the initial and final values in the terms of t correspond to the weights of the initial and final states.

example  <dig> 
the weight  associated to the automaton a in figure  <dig> for y=ccg∈Δ∗ is computed as: a=1∗2∗3∗6∗1+1∗3∗1∗4∗1=48considering that there are two accepting paths labelled with ccg. these paths are: path 1:state 0↦state 0↦state 1↦state  <dig>  path 2:state 0↦state 1↦state 2↦state  <dig>  the initial and final values in the terms of a correspond to the weights of the initial and final states.

there are several operations defined on automata and transducers, such as inverse and composition. given any transducer t, the inverset- <dig> is the transducer obtained when the input and output labels are swapped for each transition. the composition operation of the transducers t <dig> and t <dig> with input and output alphabets both equal to Σ is a weighted transducer, denoted by t1∘t <dig>  provided that the sum given by  is well defined in  for all ∈Σ∗.

rational kernels
in order to manipulate sequence data, fsts provide a simple representation as well as efficient algorithms such as composition and shortest-distance  <cit> . rational kernels, based on finite-state transducers, are effective for analyzing sequences with variable lengths  <cit> .

as a formal definition, a function  is a rational kernel if there exists a wfst u such that k coincides with the function defined by u, i.e., k=u for all sequences x,y∈Σ∗×Δ∗
 <cit> . from now on, we consider the input and output alphabets with the same symbols , and only the terms Σ and Σ∗ will be used.

in order to compute the value of u for a particular pair of sequences x,y∈Σ∗×Σ∗, the composition algorithm of weighted transducers is used  <cit> :

first, mx, my are considered as trivial weighted transducers representing x, y respectively, where mx= <dig> and mx= <dig> for v≠x or w≠x. mx is obtained using the linear finite automata representing x by augmenting each transition with an output label identical to the input label and by setting all transition, initial and final weights to one. my is obtained in a similar way by using y.

then, by definition of weighted transducer composition:

=mxumy.

considering mx= <dig> and my= <dig>  we obtain =k, i.e., the sum of the weights of all paths of mx∘u∘my is exactly u=k.



based on this representation, a two-step algorithm is defined by cortes et al.  <cit>  to obtain k=u.
  

using algorithm  <dig>  the overall complexity to compute one value for the rational kernel is , where |u| remains constant. in practice, this complexity is reduced to  in many kernels which have been used in areas such as natural language processing and computational biology. for example, algorithm  <dig> for the n-gram kernel has a linear complexity .

kernels used in training methods for discriminant classification algorithms  need to satisfy mercer’s condition or equivalently be positive definite and symmetric - pds  <cit> . cortes et al.  <cit>  have proven a result that gives a general method to construct a pds rational kernel using any wfsts.

theorem 1
. if t is an arbitrary weighted transducer, then u=t∘t- <dig> defines a pds rational kernel.

n-gram kernel as a rational kernel
hofmann et al.  <cit>  have defined a class of similarity measures between two biological sequences as a function of the number of equal subsequences that they have. as an example of such measures is the spectrum kernel defined by leslie et al.  <cit> . similarity values are the results of summing all the products of the counts for the same subsequences. it is also referred to in computational biology as the k-mer or n-gram kernel. in the rest of this paper, we use the term n-gram to follow the notation of hofmann et al.  <cit>  and cortes et al.  <cit> .

the n-gram kernel is defined as  for a fixed integer n, which represents subsequences of length n. here, ca is the number of times that the subsequence b appears in a. kn can be represented as a rational kernel using the weighted transducer , where the transducer tn is defined as tn=cx, for all x,z∈Σ∗ with |z|=n
 <cit> . for example, for n= <dig>   is the rational kernel where z represents all the subsequences in Σ∗ with size  <dig> and t2=cx counts how many times z occurs in x.

allauzen et al.  <cit>  extended the construction of this kernel, kn, to measure the similarity between sequences represented by automata. firstly, they define the count of a sequence z in a weighted automaton a as , where u ranges over the set of sequences in Σ∗ which can be represented by the automaton a. this equation represents the sums obtained for each u, of how many times z occurs in u multiplied by the weight  associated to the sequence u in the automaton a .

then, the similarity measure between the weighted automata a <dig> and a <dig>  according to the n-gram kernel kn, is defined as:
  <dig>  

based on this definition and using algorithm  <dig>  the n-gram rational kernel can be constructed in time , as described by allauzen et al.  <cit>  and mohri et al.  <cit> .

yu et al.  <cit>  have verified that n-gram sequence kernels alone are not good enough to predict protein interactions. we address their concerns in our experiments by combining n-gram with other kernels that include evolutionary information.

pairwise kernels
we apply kernel methods to the problem of predicting relationships between two given entities, i.e., pairwise prediction. models to solve this problem have as an input two instances, and the output is the relationship between them. kernels used in these models need to define similarities between two arbitrary pairs of entities. typically, the construction of pairwise kernels k are based on simple kernels k, where . in this paper four different pairwise kernels are investigated: direct sum learning pairwise kernel  <cit> , tensor learning pairwise kernel   <cit> , metric learning pairwise kernel  <cit>  and cartesian pairwise kernel  <cit> .

all these pairwise functions guarantee the symmetry of the pairwise kernels k, i.e., k,)=k,), where x <dig> x <dig> y <dig> y2∈x. also, if the simple kernel k is pds , the resulting pairwise kernel k also is pds, for each of the pairwise kernels defined above  <cit> .

pairwise support vector machine
the rationale for the preceding discussion on representing disparate types of data as kernels is to enable us to use them in machine learning formalisms such as support vector machines . svms are used for classification and regression analysis, defined as supervised models with associated learning algorithms  <cit> . in this research, we use svms for classification. svms represents the data as vectors in a vector space . as a training set, several entities xi  classified in two categories are given. a svm is trained to find a hyperplane that separates the vector space in two parts. each part of the feature space groups the entities into the same category. then, a new entity x can be classified depending their location in the feature space related to the hyperplane  <cit> .

pairwise support vector machines, instead, classify pair of entities   <cit> . let us formally define the binary pairwise support vector machine formulation, following brunner et al.  <cit> : given a training data ,di), where di has binary values  is classified as + <dig> or -1), i= <dig> …,n, j= <dig> …,n and the mapping function Φ, then the pairwise svm methods find the optimal hyperplane, wtΦ+b= <dig>  which separate the points in two categories. one of the solutions is based on the dual formalism of the optimization problem described in cortes et al.  <cit> . in this case the decision function is:
  

where k is the pairwise kernel,  is the set of training examples, α is obtained from the lagrange multipliers as a function of w  and b is the offset of the hyperplane . in this case, α and b are the “learned” parameters during the training process. thus, f classifies the new pairs . for example, if f> =  <dig>   is classified as + <dig>  otherwise  is classified as - <dig> 

metabolic networks
in this work, the metabolic network is represented as a graph, in which the vertices are the enzymes, and the edges are the enzyme-enzyme relations . figure  <dig> represents a graphical transition from a metabolic pathway to a graph.figure  <dig> 
conversion from a metabolic network to a graph representation.
 part of the glycolysis pathways, from biocyc database  <cit> .  the resulting graph with the nodes  and edges .  table that represents known enzymes relations .



in a traditional representation of a metabolic pathway, enzymes are vertices , and metabolites are edges . following yamanishi  <cit> , we represent it differently, where the interactions between pairs of enzymes are considered discrete data points. for example, in figure  <dig>  the enzyme numbered ec  <dig> . <dig>  can create d-fructose-6-phosphate as a product, which is in turn used as a substrate by the enzyme numbered ec  <dig> . <dig> . this means there is an enzyme-enzyme relation between ec  <dig> . <dig>  and ec  <dig> . <dig> . then, we create a graph in which enzyme-enzyme relations become edges and enzymes are nodes as is shown in figure  <dig>  if there is a relation between two enzymes, such a relation is classified as + <dig> . enzyme-enzyme pairs for which no relation exists are classified as - <dig> . figure  <dig> describes these classifications, which are used as training set in the svm method.

using pairwise kernel and svm to predict metabolic networks
the input data, considered as the training example dataset ,di), is a set of known pairs of enzymes  classified in two categories . figure  <dig> shows an example of the input data, obtained from the metabolic network described in figure  <dig>  in figure  <dig>  enzymes are represented by ec number  and gene nomenclature .figure  <dig> 
diagram of pairwise svm applied to metabolic network prediction.
 an example of the pairs in the training set using the ec numbers  or gene names .  the pairwise kernel as a matrix, where the numerical values in each cell correspond to a measure of similarities, given two pairs of ec numbers  or two pairs of gene names .  a model is trained to estimate the parameters α
i,j and b of the decision function f.  given a new pair of ec numbers  or gene names  the decision function is evaluated and the pair is classified as interacting or non-interacting.


 <cit> , then k,) is computed using a simple kernel k  kernel described by ben-hur et al.  <cit> ). the pfam kernel ) describes similarity measures based on the pfam database  <cit>  between the gene x and the gene y. thus, the tensor product pairwise kernel k, using as a simple kernel the pfam kernel kpfam is defined as:
  

for example, in figure 3-bottom, if the genes are associated to the variables as follow: x1=yar071w,y1=yal002w,x2=ydr127w,y2=yal038w, the tensor product pairwise kernel is:
  

a pairwise svm based on the dual formalism of the optimization problem is represented in figure  <dig>  the parameters αij and b are learned, using the pairwise kernel, k, and the training dataset, . finally, new pairs of enzymes or genes  can be classified as interacting or not-interacting, depending the evaluation of the decision function f ). by predicting the gene interactions of the other unseen examples, all the metabolic pathways can be predicted.

the pairwise kernel computation is one of the most expensive tasks during the prediction of the metabolic networks in processing and storage. using sequence data causes even longer execution times and large storage needs. however, we have mentioned the advantages of using sequence data in order to avoid error accumulation because of genome annotation dependencies. as well, svms guarantee better accuracy values than other supervised learning methods along with sequence kernels for metabolic network inference  <cit> . therefore, we focus on improvement of the pairwise kernel computations and representation, by incorporating rational kernels to manipulate the sequence data. to accomplish this, we have proposed a new framework called pairwise rational kernels.

methods
pairwise rational kernels
in this section, we propose new pairwise kernels based on rational kernels, i.e., pairwise rational kernels . they are obtained using rational kernels as the simple kernels k. we have defined four prks, based on the notations and definitions in the background section above.

definition 1
given x⊆Σ∗ and a transducer u, then a function  is:

a direct sum pairwise rational kernel  if

k,)=u+u+u+u

a tensor product pairwise rational kernel  if

k,)=u∗u+u∗u

a metric learning pairwise rational kernel  if

k, ) = -u-u +u)2

a cartesian pairwise rational kernel  if

k,)=u∗δ +δ∗u +u∗δ +δ∗u

where δ= <dig> if x=y and  <dig> otherwise, ∀x,y∈x.



following theorem  <dig>  if we construct u using a weighted transducer t, such as u=t∘t- <dig>  then we guarantee u is a positive definite and symmetric  kernel. pds is a needed condition to use kernels in training classification algorithms. since all the kernels defined above are results of pds kernel operations, the prk kernels are also pds  <cit> .

algorithm
we have designed a general algorithm, algorithm  <dig>  to compute the kernels, using the composition of weighted transducers. this is a an extension of algorithm  <dig>  it uses as an input the transducers , , , , that represent the sequences x <dig> y <dig> x <dig> y2∈x and the weighted finite-state transducer u, and outputs the value of k,).
  

in our implementation described below, we use the n-gram rational kernel as the kernel u . then, the complexity of steps  and  are . step  adds a constant time complexity. we conclude that prks based on n-gram kernels can also be computed in time .

experiments
in this section we describe experiments to predict metabolic networks using pairwise svms combined with prks. we aim to prove the advantage of using prks to improve execution time during the computation of the pairwise kernels and the training process, while maintaining or improving accuracy values.

dataset
we used data from the yeast saccharomyces cerevisiae
 <cit> . this species was selected to compare our methods, implementations and results with other methods that also predict biological networks for saccharomyces cerevisiae
 <cit> .

the data for this species were taken from the kegg pathway  <cit>  and converted to a graph as described in the previous section . there were  <dig> nodes and  <dig> interacting pairs in the graph for this species. as we used svm methods for the metabolic network inference, we prefer a balanced dataset. in this dataset, we have an unbalanced proportions of interacting  and non-interacting  classified pairs . in order to balance our dataset, we followed the procedure recommended by yu et al.  <cit> , using the program brs-noint to select non-interacting pairs. yu et al.  <cit>  describes the bias towards non-interacting pair selection during the training process and the accuracy estimation. to eliminate this bias, the brs-noint program is used to create a “balanced” negative set to maintain the right distribution of non-interacting and interacting pairs. as a result, we obtained  <dig> non-interacting pairs for a total of  <dig> pairs in the training process.

training process and kernel computation
the known part of the metabolic network was converted in a graph and then obtained the pairs of training set, corresponding to figure  <dig>  the prk representation coincides with figure  <dig>  here, we describe the computation of prks , given the data from the yeast saccharomyces cerevisiae:

each of the  <dig> known genes were represented as a trivial weighted automaton  using the nucleotide sequences,

the n-gram kernel, with n= <dig>  was used as a rational kernel, then  ,

algorithm  <dig> was implemented to obtain the k values,

as an example, the tensor product pairwise rational kernel in definition  <dig> is obtained by:

kprkt,)=





.

finally, all the prk kernels k with positive eigenvalues were normalized to avoid the fact that longer sequences may contain more n-grams, resulting in more similarities  <cit> .



we implemented this method to compute the prks using open finite-state transducer  library  <cit>  and openkernel library  <cit> . the input data were nucleotide sequences of known genes, and the outputs were the pairwise rational kernel values as a similarity measure between pairs. example  <dig> shows the input and output values for the method described above, equivalent to figure  <dig>  but using sequence data.

example 3
given nucleotide sequences x <dig> y <dig> x <dig> y <dig>  which represent abbreviated examples of known genes in the dataset, x <dig> = gctaaattggacaaatctcaatgaaattgtcttgg y <dig> = atgtcctcgtcttcgtctaccgggtacagaaaa x <dig> = catgactaaagaaacgattcgggtagttatttggcgg y <dig> = atctacaagcgaaccagagtcttctgcaggcttagatthe tensor product pairwise rational kernel kprkt,) can be obtained using the 3-gram rational kernel, e.g., for z=tct, the values are:

 because, tct appears twice in x1

gctaaattggacaaatctcaatgaaattg

tcttgg,

 because, tct appears twice in y1

atgtcctcgtcttcgtctaccgggtacaga

aaa,

 because, tct appears once in x2

catgactaaagaaacgattctggtagttatt

tggcgg, and

 because, tct appears three times in y2

atctacaagcgaaccagagtctttctgcagg

cttagat.



with these results and other values corresponding to 3-gram rational kernel, the kprkt is computed as: kprkt,)= <dig> , where  <dig>  is a measure of similarity.

svm and predicting process
to implement the pairwise svm method, we use the sequential minimal optimization  technique from the package libsvm  <cit>  in combination with openkernel library  <cit> . during the training process, the decision function was obtained by estimating the parameters, as is shown in figure  <dig>  now, the prediction process allows classification of new pairs of nucleotide sequences as interacting or not interacting by evaluating the decision function. example  <dig> shows a description of the prediction process, similar to the process described in figure  <dig>  but using nucleotide sequences.

example 4
this example describe the predictor process. suppose we want to know if x = ctcaaagtcttaatgcttggacaaattgaaattgg, andy=tctacagagtcgtccttcgtctaccgggaaaat,which represent abbreviated nucleotide sequences, interact or do not interact. the decision function, f, was previously obtained during the training process . if the resulting value of evaluating the decision function f is greater than  <dig>  the pair  interact, otherwise the pair  do not interact. suppose that the evaluation is f=f=+ <dig> then, we predict that these nucleotide sequences  interact in the context of the metabolic network of the yeast saccharomyces cerevisiae. in this case, we used  <dig> genes during the training process, but the species has more than  <dig> genes  <cit> . then, the rest of the metabolic pathways can be predicted by classifying all other pairs of genes , as interacting or non-interacting, using the decision function f. note that the decision function is obtained once during the training process, but can be used as often as needed during the prediction process.

the advantage of using sequence data is that nucleotide sequences can be used, even if it is not annotated. also, any other type of sequence data, e.g., from high-throughput analysis, can be considered and combined, using a similar implementation.

experiment description and performance measures
we used pairwise svm with prks for metabolic network prediction, using the data and algorithms described above. we ran experiments for twelve different kernels. firstly, we used four prks described in definition  <dig> using the 3-gram rational kernel . in addition, a combination of prks with other kernels were considered. we included the phylogenetic kernel  described by yamanishi  <dig>  <cit>  and pfam kernel  describe by ben-hur et al.  <cit> . then, a second set of experiments were developed combining prks with the phylogenetic kernel . finally, we combined prks with the pfam kernel, obtaining kprkds-3gram+kpfam,kprkt-3gram+kpfam,kprkm-3gram+kpfam and kprkc-3gram+kpfam kernels. considering that the phylogenetic and pfam kernels were pds, the resulting combinations were also pds  <cit> .

to compare the advantages of the prks framework, we developed a new set of experiments with the same dataset, but without using finite-state transducers. we considered the pairwise  kernel, i.e., kt-3gram. kt-3gram denoted the pairwise tensor product described in the pairwise kernels section. to be consistent with the previous experiments, we combined the kt-3gram kernel with the phylogenetic kernel  and pfam kernel , i.e., kt-3gram+kphy and kt-3gram+kpfam kernels, respectively. the pairwise svm algorithm was used to predict the metabolic network using the same data set described above. table  <dig> describes the groups created to compare these kernels with the equivalent prks.table  <dig> 
groups for prk and pairwise kernel comparison


1
2
k
prkt-3gram
k
t-3gram
k
prkt-3gram + k
phy
k
t-3gram+k
phy
k
prkt-3gram + k
pfam
k
t-3gram+k
pfam

1kernels were taken from table  <dig> 


2computed with the tensor product pairwise kernel.
average auc roc scores and processing times for various prks




all the experiments were executed on a pc intel i7core, 8mb ram. to validate the model, we used the 10-fold cross validation method and measured the average area under the curve of receiver operating characteristic  score.

cross-validation method is a suitable approach to validate performance of predictive models. in k-fold cross-validation, the original dataset is randomly partitioned into k equal-sized subsets. then, the model is trained k times. each time, one of the k subsets is reserved for testing and all the remaining k- <dig> subsets are used for training. the final value is obtained as the average of the k results .

a receiver operating characteristic  curve is a plot of the true positive rate  versus the false positive rate  for different possible cut-offs of a binary classifier system. a cut-off defines a level for discriminating positive and negative categories. roc curve analysis is used to assess the overall discriminatory ability of the svm binary classifiers. the area under the curve  has been used as a metric to evaluate the strength of the classification.

in addition, the 95% confidence intervals  have been computed, following the method described by cortes and mohri  <cit> . the authors provide a distribution-independent technique to compute confidence intervals for average auc values. the variance depends on the number of positive a negative examples  and the number of classification errors, ranging between  <dig> and  <dig> in our cases.

RESULTS
ben-hur et al.  <cit>  report an average auc value of  <dig>  for pfam kernels, while yamanishi  <cit>  reports an average auc of  <dig>  for the phy kernel for predicting saccharomyces cerevisiae metabolic pathways. we have previously developed similar experiments but using svm methods  <cit> . as a result, we obtain auc values of  <dig>  for pfam kernel and  <dig>  for phy kernel, with execution times of  <dig> and  <dig> seconds, respectively. however, in all cases a random selection of negative and positive training data was used. as noted by yu et al.  <cit> , the average auc values obtained by random selection of data for training machine learning tools results in a bias towards genes  with large numbers of interactions. as such, the high auc results in these previous works cannot be directly compared to the results in this paper. we have employed the balanced sampling techniques suggested by yu et al.  <cit>  to combat bias in the training set. our results, with average auc values in the range  <dig> - <dig> , are comparable to and exceed in cases the results obtained by yu et al.  <cit>  with balanced sampling, which range from  <dig> - <dig>  across several different kernels for protein interaction problems. we have also obtained these results in execution times of 15- <dig> seconds. with the exception of the direct sum kernel, all of the confidence intervals are above the behaviour of a random classifier.

we developed one more experiment with the pfam kernel as a simple kernel of the pairwise tensor product  using a balanced sampling as suggested by yu et al.  <cit> . note that it is not a prk; it is a regular pairwise kernel using pfam as a simple kernel, similar to the example in the using pairwise kernel and svm to predict metabolic networks section. as a result, the average auc was  <dig>  and the execution time was  <dig> seconds. when we compare these values with the results in table  <dig> exp. i, we can see that the kernels kprkm-3gram and kprkc-3gram have better average accuracy  with lesser average execution times . in addition, when the pairwise rational kernel 3-gram was combined with the pfam kernel in the exp. iii, , the average accuracy value  was better than the pairwise tensor product , while the execution time just was increased  <dig>  seconds .

in order to statistically compares theses results, we applied the mcnemar’s non-parametric statistical test  <cit> . mcnemar’s tests have been recently used by bostanci et al.  <cit>  to prove significant statistical differences between classification methods. mcnemar’s test defines a z score, calculated as:
  <dig>  

where nfs is the number of times algorithm a failed and algorithm b succeeded, and nsf is the number of times algorithm a succeeded and algorithm b failed. when z is equal to  <dig>  the two algorithms have similar performance. additionally, if nfs is larger than nsf then algorithm b performs better than algorithm a, and vice versa. we computed the z scores considering algorithm a as the svm algorithm using the pairwise tensor product  and three different algorithm bs, using svm with three different prks from table  <dig> . in all cases, we obtained z scores greater than  <dig> , which mean the prks performed better. these z-score also proved that the difference was statistically significant with a confidence level of 99% .

the cartesian kernel has not been widely used since it was defined by kashima et al.  <cit> . kashima et al.  <cit>  used expression, localization, chemical and phylogenetic kernels to predict metabolic networks. each of these are non-sequence kernels. in the current experiments we computed, for first time, the pairwise cartesian kernel with a rational kernel  to represent sequence data for metabolic network prediction. cartesian kernels  <cit>  have been defined as an alternative to improve the tensor product pairwise kernel  <cit>  computation performance. in the three experiments shown in table  <dig>  we confirmed this definition, as we have obtained better accuracy and execution times when we used the cartesian pairwise rational kernel  rather than the tensor product rational kernel . comparing our results with kashima et al.  <cit> , we obtained better average auc values , and approximately the same average of the execution times . kashima et al.  <cit>  used non-sequence data and random selection of positive and negative data for training.
comparison of some pairwise rational kernels and pairwise kernels grouped by kernel types .




the power of using kernels is that almost any sort of data can be represented using kernels. therefore, completely disparate types of data can be combined to add power to kernel-based machine learning methods  <cit> . for example, coefficients describing relative amounts of metabolites involved in a biochemical reaction  can also be represented as kernels and added to strength the predicting model. for example, the reaction catalyzed by fructose-bisphosphate aldolase  splits  <dig> molecule of fructose  <dig> -bisphosphate into  <dig> molecules of glyceraldehyde 3-phosphate, where the relative amounts of substrate and product are represented by the coefficients  <dig> and  <dig>  respectively. a stoichiometric kernel therefore would encode coefficients for all substrates and products, where enzymes that do not interact would have stoichiometric coefficients of  <dig>  other authors  have defined and used similar types of stochiometric data, which can be converted into kernels to be consider with prks.

CONCLUSIONS
in this paper, we introduced a new framework called pairwise rational kernels, where pairwise kernels are obtained based on transducer representations, i.e., rational kernels. we defined the framework, developed general algorithms and tested on the pairwise support vector machine method to predict metabolic networks.

we used a dataset from the yeast saccharomyces cerevisiae to validate and compare our proposal with similar models using data from the same species. we obtained better execution times than the other models, while maintaining adequate accuracy values. therefore, prks improved the performance of the pairwise-svm algorithm used in the training process of the supervised network inference methods.

in these methods, the learning process are executed once to obtain the decision function. the decision function can be used as many times as necessary to predict interaction between the other sequences in the species and predict the metabolic pathways.

the methods in this research used sequence data  to predict these interactions. genes do not need to be correctly annotated as the raw sequences can be used. therefore, our methods were able to avoid the error accumulation due to wrong gene annotations.

as future work, our proposal will be used to produce a set of candidate interactions of pathways from the same and other species, that could be experimentally validated. as well, other pairwise rational kernels may be developed using other finite-state transducers operations.

competing interests

the authors declare that they have no competing interests.

authors’ contributions

arl implemented the algorithms and developed the experiments. arl, md and bf contributed equally to the drafting of this manuscript. all authors have reviewed and approved the final version of this manuscript.

