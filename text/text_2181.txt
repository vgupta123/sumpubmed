BACKGROUND
understanding a protein's functional role often requires knowledge of the protein's tertiary  structure. however, experimentally obtaining an accurate 3d structure can be labor-intensive and expensive, and methods for computationally predicting 3d structure are far from perfect. therefore, protein secondary structure provides a useful intermediate representation between the primary amino acid sequence and the full three-dimensional structure. the secondary structure of a protein is most commonly summarized via a labeling of the amino acids according to a three-letter alphabet: h = helix, e = strand, l = loop. knowledge of a protein's secondary structure can provide insight into its structural class, suggest boundaries between functional or structural domains, and give clues as to the protein's function. furthermore, because protein secondary structure prediction is often used as a subroutine in tertiary structure prediction algorithms, any significant improvement in secondary structure prediction is likely to yield improved tertiary structure predictions as well.

the earliest method for secondary structure prediction  <cit>  used a neural network to achieve a base-level predictive accuracy of  <dig> % from a dataset of  <dig> labeled proteins. in the ensuing  <dig> years, dozens of methods have been proposed for improving upon this baseline, with significant advances achieved by exploiting homologs of the query sequence  <cit>  and by employing methods, such as hidden markov models, which exploit patterns in the protein sequence  <cit> . state-of-the-art methods now achieve accuracies in the range of 77-80% on a variety of published benchmark datasets  <cit> .

our secondary structure prediction method combines a dynamic bayesian network  and a support vector machine . dbns and svms have already been used successfully to predict protein secondary structure  <cit> . a dbn is a type of graphical model, which is an intuitive, visual representation of a factorization of the joint probability distribution of a set of random variables. dbns are bayesian networks that can be extended in one dimension to arbitrary lengths. this type of model is therefore ideally suited to handling variable length data such as protein sequences. indeed, the hidden markov model, which has been used extensively to model protein sequences  <cit> , is a very simple example of a dbn. generative models such as dbns and hmms are also used in modeling torsion angles and in predicting the three-dimensional structure of proteins  <cit> . an svm is a non-parametric statistical method for discriminating between two classes of data  <cit> . svms have been applied widely in bioinformatics  <cit> . the svm operates by projecting the data into a vector space and finding a hyperplane that separates the classes in that space. svms are motivated by statistical learning theory, which suggests an optimal method for identifying this separating hyperplane. svms are thus functionally similar to neural networks, but can be mathematically represented as a convex optimization problem, meaning that the cost function has a single minimum, making it possible to identify a globally optimal solution in an efficient fashion. in this work, we first extend a previously described dbn  <cit> , combine it with an svm, and introduce several improvements that yield performance comparable to the state-of-the-art on an established benchmark.

in addition to improving the predictive performance of dbns, we introduce an algorithm for learning a sparse dbn for protein secondary structure prediction. in this context, sparse refers to a model in which a large percentage of the model parameters are zero. methods for encouraging sparsity have been the subject of much recent work in the statistical machine learning community  <cit>  because these methods have the potential to learn the trade-off between over- and under-fitting a given data set. in general, a model with many parameters will tend to overfit the training set and therefore fail to generalize to the test set. conversely, a model with too few parameters will underfit the training data and hence achieve poor predictive power on both the training data and the test set. ideally, a sparse learning algorithm will be allowed to fit a large number of parameters but, depending on properties of the training set, will choose to set some percentage of those parameters to zero. the sparse learner thus, analogous to a non-parametric model, balances model complexity against training set size, with the goal of balancing between under- and over-fitting. other technical benefits of the resulting sparse model include improved robustness to new test data and greater efficiency. in addition to technical advantages, sparse models enable us to discover correlations inherent in protein structure, including local correlations among neighboring amino acids as well as non-local correlations among Î² strands or coiled-coil regions. the algorithm we propose in this paper interleaves iterations of the expectation maximization  algorithm  <cit>  with a simple sparsification operation, which is straightforward and very effective.

RESULTS
comparison with the state-of-the-art
in our first experiment, we performed a seven-fold cross-validation on cb <dig>  which is a well-known and difficult benchmark dataset with  <dig> chains and  <dig>  amino acids  <cit> . the details of the cross-validation procedure is explained in "model training, parameter optimization and testing for cross-validation" section. to assign the true secondary structure labels, we mapped the eight-state representation of secondary structure labels  to three states with the following conversion rule: h, g, i to h; e, b to e and s, t, ' ' to l. for the experiments in this section, we did not apply the model sparsification algorithm introduced in this paper.

the results of the seven-fold cross-validation are summarized in table  <dig>  including the amino acid level accuracy , the segment overlap score   <cit> , and matthew's correlation coefficients   <cit> . in this table, a variety of secondary structure prediction methods--svmpsi  <cit> , jnet  <cit> , yasssp  <cit> , dbnfinal  <cit> , dbnpred , dbnn  <cit> , psipred  <cit> , svm_d <dig>  <cit> , destruct  <cit> , disspred  <cit> , and dspred --are evaluated with respect to the cb <dig> benchmark. we evaluated the statistical significance of the differences between the accuracies  reported in table  <dig> using a one-tailed z-test, which attempts to determine if one proportion is greater  than another. when we compare our method to disspred  <cit> , a  <dig> % difference in accuracy yields a p-value of  <dig>  from a one-tailed z-test. this difference is not significant when we set the confidence level to 95% or 99%. when we compare our method to destruct  <cit> , which is  <dig> % less accurate than our method, we get a very small p-value  in a one-tailed z-test.

therefore, the  <dig> % accuracy difference between our method and destruct is statistically significant. furthermore, in our experiments with the sd <dig> benchmark dataset  <cit>  , we have observed that a one-tailed z-test when applied to variants of our dbn yields p-values <  <dig>  for differences in q <dig> on the order of  <dig> %, which indicates statistical significance. therefore among the methods that we tested, our method achieves performance comparable to the state-of-the-art in secondary structure prediction.

in the same benchmark, we also analyzed the contribution of the svm classifier to the predictive accuracy. to analyze this, we implemented dbnpred, which computes predictions by taking the average of the marginal a posteriori distributions from the four dbns as described in "combining multiple dbns." the dbns in dbnpred are trained on the subset of proteins allocated for dbns . the results in table  <dig> show that combining the position specific scoring matrix  profiles and the four dbns using an svm classifier  performs 3% better according to the q <dig> measure and 4% better according to the sov measure than simple averaging of the a posteriori distributions from the dbns . this difference is statistically significant from a one-tailed z-test  and is mainly due to the following factors. first, the svm classifier uses the pssm profiles  as well as the a posteriori distributions generated by dbns, whereas the dbnpred only combines the a posteriori distributions to reach a final decision. therefore, the svm is learning the relationships among the pssms and the a posteriori distributions jointly. second, dbnpred takes a simple averaging of the distributions, but the svm classifier is able to assign more flexible weights to these features.

in our second experiment, we performed seven-fold cross-validation on sd <dig>  which contains  <dig> chains and  <dig>  amino acids  <cit> , and we compared the performance of our method to dbnfinal and the dbnn methods of yao et al.  <cit> . table  <dig> shows that our method outperforms dbnfinal by  <dig> % and dbnn by  <dig> % according to the q <dig> measure. in sov , we outperform dbnfinal by  <dig> % and dbnn by  <dig> %. this result and the  <dig> % increase in q <dig> evaluated on the cb <dig> set  are statistically significant as measured by a one-tailed z-test ; hence, our method outperforms the dbn methods of yao et al.  <cit> .

sparsifying the model while maintaining accuracy
a sparse model enables us to control the model complexity and balance between under- and over-fitting against training data. it also brings improved robustness to new test data and greater efficiency. not all sparsity levels are practically useful mainly because an over-sparsified model will typically have reduced generalization ability and will perform poorly on new test data. therefore, the primary goal of our study is to develop sparse models that maintain predictive accuracy while reducing the effective number of parameters in the learned model. accordingly, we measured the extent to which algorithm  <dig>  could successfully sparsify a given model. for this experiment, we considered the following three methods:  the two dbn classifiers that use psi-blast pssms as the input observations   the two dbn classifiers that use hhmake pssms , and  the dspred method with the four dbns and the svm. for methods  and , we performed a seven-fold cross-validation experiment on the sd <dig> dataset  <cit> , fixing the hyperparameters of the dbns as laa =  <dig>  lss =  <dig>  Ï =  <dig>  and Î± =  <dig> , where laa is the number of positions in the sequence window excluding the current position, lss is the length of the secondary structure label window excluding the current label, Ï is the sequence profile weight, and Î± is the weight of the covariance regularizer . for the dspred method, we performed a seven-fold cross-validation experiment as described in "model training, parameter optimization and testing for cross-validation" section, and we used the optimized values for the hyperparameters of the dbn and svm. therefore, in this method, the training set allocated for the dbns is half the training set allocated for the first two methods. for each training set, we first eliminated a specified percentage of the parameters from the dbns by applying algorithm  <dig> and then used the sparse dbn models to compute the a posteriori distributions of secondary structure labels. for methods  and  we computed the final secondary structure prediction by taking the average of the marginal a posteriori distributions from the dbns and for  we computed the final prediction by the svm . for this experiment, we set k = 1%, which corresponds to removing 1% of the edges at the end of each em iteration, and we considered a range of sparsity values . the results of this experiment are summarized in figure  <dig>  which suggests that we can eliminate 70% of the edge parameters of the dbns  when we use psi-blast pssms, 80% of the edge parameters when we use hhmake pssms and 95% of the edge parameters when we use the dspred method without significantly decreasing the accuracy of our predictions. to validate this result, we performed a one-tailed z-test. for method , we compared the performance of the fully dense model with the models obtained after removing 70%, 75% and 80% of the edge parameters. using a significance threshold of  <dig> , the performance after removing 70% of the edge parameters--corresponding to a decrease in predictive accuracy of only  <dig> %--is not statistically significant . for 75% removal, the accuracy drops by  <dig> %  and for 80% removal, it drops by  <dig> %, . however, even when removing 80% of the edge parameters, the loss in predictive accuracy is not large. when we remove all the edge parameters  the accuracy plummets to around 66%. this shows that the sparsification algorithm is removing redundant parameters first, which is a desired behavior for a sparsifier. once we start removing essential parameters, the accuracy falls quickly. note that removing all the edge parameters does not correspond to eliminating all the parameters in the dbn model, which explains why the accuracy is not zero . the statistical analysis performed for method  can also be performed for methods  and  but is omitted here for simplicity. when we use hhmake pssms the performance loss was  <dig> % at 80% sparsity and when we use the dspred method it was  <dig> % at 95% sparsity. note that the dbn model that uses hhmake profiles ) can generate more accurate predictions  of  <dig> %) and sparser models than the dbn model that uses psi-blast pssms only. combining both pssms by an svm classifier  yields even more accurate predictions  of  <dig> %) and is more robust to even sparser dbn models as shown in figure  <dig>  note that even if we eliminate all the edge parameters for the dbns, the dspred method still performs considerably well  of  <dig> %) because this only causes the a posteriori distributions to be less accurate, which is highly compensated by the availability of psi-blast and hhmake pssms in the svm's feature set.

the auto-regressive section of the model contributes to accuracy
the sparsification experiment presented in the previous section also allows us to test the hypothesis that the auto-regressive portion of the model is an important contributor to its accuracy. this hypothesis is most directly supported by the fact that a model with laa =  <dig>  lss =  <dig> achieves only 67% accuracy  <cit> . to investigate more directly the value of the auto-regressive portion of the model, we subdivided the edge parameters into two groups: current edges, which connect pairs of amino acids at the current position, and auto-regressive edge parameters that connect an element of the pssm vector at the current position to another pssm element in a neighboring position. figure  <dig> plots the percentage of current edge parameters and the percentage of auto-regressive edges that are retained as a function of the sparsity of the dbn model that uses psi-blast pssms only  in the previous section). not surprisingly, for every sparsity level, the current edges are preferentially retained by the model; on the other hand, even when we eliminate 90% of the edges, the model still contains  <dig> % of the auto-regressive edges.

furthermore, when carrying out this analysis, we observed that, even in extremely sparse models with 80-90% of the edges eliminated, the model still includes edges from all positions within the dependency window. for instance, if the laa parameter is chosen as  <dig>  then at 80% sparsity level, edges that remain in the resulting graph stem from all five amino acids that are neighbors of the current amino acid. this observation suggests that even if there is a strong correlation between the current amino acid and those that are three or four residues apart , other positions also contain useful correlations that contribute to the predictive accuracy.

recovery of true sparse model structures
we have demonstrated that the sparse learning procedure proposed in the "learning a sparse model for a dbn" section yields a model that provides highly accurate predictions. next, we would like to verify that the parameters learned by the model are accurate. to address this question, we use simulated data, because the true parameters associated with real data are not known.

our experiment consists of four steps. first, we learn the parameters of a dbn--state transitions, length distributions and multivariate conditional gaussians--from real data at different sparsity levels  using the algorithm described in the "learning a sparse model for a dbn" section. for this step, we use the sd <dig> benchmark  <cit> , and we set the model hyperparameters to laa =  <dig>  lss =  <dig>  Ï =  <dig>  and Î± =  <dig> . second, we use each trained model to generate a series of synthetic data sets of various sizes  by sampling from the parameters of dbn. third, we use the synthetic proteins to learn sparse models, again employing the algorithm in the "learning a sparse model for a dbn" section. as in step one, we consider a range of sparsity levels , and we also train from different numbers of synthetic proteins. finally, in step four, we compare, for each model, the true underlying parameters and the inferred parameters. in this experiment, we only considered the past dependency dbn in the "combining multiple dbns" section and we utilized pssms derived from psi-blast  <cit> .

to provide a more quantitative estimate of the difference between the learned and the true edge parameters, we repeated the synthetic data generation experiment ten times. we then averaged the absolute values of the edge parameter differences across all of the parameters in the model, which is called the mean absolute difference  metric. figure  <dig> plots the mean and standard deviation of the mad metric across the ten replicate experiments. the figure shows that, as the sample size increases, the mad metric decreases. for the largest dataset, the average difference between the true and inferred parameters is very small . figure  <dig> provides an alternative way of comparing the true and the inferred edge parameter values, based on comparing the inferred graph structures. both sets of results demonstrate that, given sufficient training data, the sparsification algorithm can successfully infer the correct graph structure.

sparse dbns identify significant correlations among amino acids
one motivation for employing sparse models is the improved interpretability of a model with fewer parameters. therefore, to complement the simulation experiment described in the previous section, we analyze the graphs of dbns  learned from real protein sequences, searching for evidence of various correlations that occur in different types of secondary structure. therefore, in this section, we are not generating any secondary structure predictions but training a dbn only and sparsifying the graphical model of the pssm profiles. in this type of analysis, the edges that remain in the sparsified model will represent the particular pairs of pssm elements that are strongly correlated.

local correlations
it is well known that, in helices, there is a hydrogen bond between every three or four amino acids, depending on the type of helix . this bonding pattern causes pairs of helix amino acids that are three and four residues apart to be statistically correlated. similarly, in Î² strands, amino acid pairs that are adjacent and those that are separated by one amino acid are strongly correlated due to hydrogen bonds and chemical interactions. in contrast, the correlations in loops are more irregular, with the highest correlation occurring between the adjacent amino acids.

to assess the relation between the learned graph and these known statistical correlations, we first set lss =  <dig>  so that we have one gaussian for each type of secondary structure element. we chose the input observation window laa =  <dig> so that we cover a wide range of local correlations. other parameters of the dbn are selected as Ï =  <dig>  and Î± =  <dig>  for simplicity. then we learned the parameters of the model on the sd <dig> benchmark and sorted the edges with respect to the edge coefficients. the results, summarized in figure  <dig>  show good agreement with the expected correlation structure. it can be observed that in helices, edges with separation distances of  <dig>   <dig>   <dig>   <dig> and  <dig> have high edge coefficients as compared to the other offset values. a similar pattern is obtained for the mean values of the edge coefficients. furthermore, most of the remaining edge coefficients in the resulting sparse model fall into one of these five offset bins. for Î² strands and loops, edges whose vertices come from adjacent positions as well as positions that are separated by one amino acid had high coefficient values. these results show that the sparse models can be used to capture the biological and statistical correlations that are characteristic of local secondary structure.

non-local correlations in Î² strands
the chemical interactions in Î² strands differ from those in helices and loops. specifically, in helices and loops, interactions are primarily local with respect to the amino acid backbone, whereas Î² strand interactions are both local and non-local. the non-local interactions in Î² strands arise mainly due to hydrogen bonds between amino acid pairs positioned in interacting Î² strand segments. we hypothesize that some of the remaining error in our secondary structure predictions--the difference between 80% accuracy and 100% accuracy--results from the failure of our model to capture these non-local interactions. to assess the extent to which such interactions occur and could in principal be captured by our model, we carried out an experiment in which we provided the dbn with additional information about the location of Î² strand interactions. we then measured the extent to which these non-local interactions yield correlation structure in the model. for training, we collected a set of  <dig>  protein chains. this dataset, called pdb-pc <dig>  was obtained using the pisces server  <cit>  . to analyze the non-local correlations in Î² strands we modified the probability density that is normally used in dbn to model the generation of pssms from each secondary structure segment. details of this updated version of the model can be found in the "model for analyzing correlations in Î² strands" section. having designed the model and the dataset, we set lss =  <dig> and applied the sparsity algorithm, eliminating 80% of the edges from the graphical model. because our dataset contains no helices or loops, the algorithm sparsifies the graph for Î² strands only. in this experiment, we used psi-blast's pssm profiles only as the observation data , and set the other hyperparameters to laa =  <dig>  Ï =  <dig>  and Î± =  <dig> . we obtained the non-local base pairing information from the dssp database  <cit> . for simplicity, we only considered the non-local residue pairs in the bp <dig> column of the database files.

after eliminating 80% of the edges from the graphical model, we observed that ~ 61% of the remaining edges are from local positions and ~ 39% are from non-local positions on the interacting Î² strand. thus, a significant percentage of edges are retained from positions that are related to non-local interactions. the percentage of correlations that remain in the resulting model is shown in figure  <dig> in a position specific manner. figure  <dig> illustrates the degree of correlation between a Î² strand residue at position i and residues at flanking positions  as well as residues flanking the paired amino acid at position j. as a control, we repeated the experiment using randomly selected, non-local residues  rather than the true pairing locations. the resulting flat correlation structure is shown as bars labeled "k" in figure  <dig>  this control experiment shows that the correlation structure on the interacting Î² strand is much stronger than would be expected by chance. note that the distribution we get for the local positions in figure  <dig> is slightly different from the distribution in figure  <dig> because in each of these experiments, we combined the set of model parameters from local positions and those that come from distal positions into a single model and sparsified this set instead of sparsifying the two sets separately. the explicit inclusion of non-local strand interactions into our model suggests that future work on improving secondary structure prediction should perform these predictions in the context of a strand interaction prediction procedure. in addition, our modified model allows us to discover significant correlations among the individual elements of the pssms. figure  <dig> shows the learned edge parameters that represent local correlations, and figure  <dig> depicts the corresponding edge parameters for the non-local correlations . this type of analysis may allow us to discover subtle relations among interacting amino acids and provide a deeper insight into protein structure. furthermore, the edge parameters values shown in figure  <dig> represent the propensity of possible amino acid pairs to make contacts  and can be used as a priori information in a contact map prediction or Î² strand pairing prediction algorithm, which relies on the prediction or residue contacts of a given protein.

CONCLUSIONS
our primary goal in this work was to develop and validate methods for predicting secondary structure and for training sparse dbn models. our method outperforms the dbnn method introduced by yao et al  <cit> , which is a dbn cascaded by a neural network. this performance improvement results from several factors: we use pssms derived from hmm-profiles in addition to psi-blast pssms, and we optimize the four hyperparameters: the amino acid profile window parameter laa, the secondary structure label window parameter lss, the diagonal covariance regularizer Î±, and the parameter Ï that balances the contributions from discrete and continuous functions. furthermore, we have demonstrated the utility of our proposed sparse model learning algorithm in three ways:  we can successfully eliminate 70-95% of the edge parameters in a dbn without significantly affecting the predictive accuracy of the model;  the learned graph structure successfully recapitulates the true underlying structure, and  the sparsity algorithm is able to capture local as well as non-local correlations among amino acids that are characteristic of structure formation.

the ability to reveal correlations among the elements of the observation vectors can be useful in a wide range of other problems in bioinformatics. for instance, a correlation analysis based on sparse models could be used for feature selection in other types of structure prediction algorithms such as contact map or solvent accessibility prediction. by sparsifying the feature set used by a classifier, it may be possible to jointly use additional feature representations such as psi-blast and hmm-derived pssms to obtain even higher accuracy. another application could be drug design simulations, where a short segment of amino acids that bind to a particular region in a target protein is designed by searching the space of possible amino acid combinations. instead of considering all possible combinations, the procedure might be significantly simplified by concentrating on the structurally and biologically meaningful alternatives. a similar correlation analysis can also be performed to discover other types of non-local correlations, such as disulfide bonds or interactions in coiled-coil regions. many coiled-coil type proteins are involved in important biological functions such as the regulation of gene expression and transcription factors. moreover, the gp <dig> hexamer unit contains coiled-coil regions initiating the entry of hiv virus into its target cell and therefore is closely related to hiv infection  <cit> .

many methods exist for achieving sparse models. in comparison to methods such as â <dig> regularization, our algorithm, which involves a simple truncation operation interleaved inside the standard em algorithm  <cit> , is quite simple. the em algorithm is computationally efficient on the proposed model. an em iteration for a single dbn with laa =  <dig> and lss =  <dig> on pdb-pc <dig> set of  <dig>  proteins takes approximately  <dig> min  <dig> sec on a single intel xeon  <dig>  ghz cpu. furthermore, for computing predictions on test proteins, a sparse model will be much faster than the corresponding dense model, simply because the model contains fewer parameters. for example, on the sd <dig> benchmark, a dbn with laa =  <dig> and lss =  <dig> at 90% sparsity level is  <dig>  times faster than the fully dense model when evaluated on a single cpu.

in future, we plan to further exploit the sparse modeling paradigm by extending our model to include additional types of observations and to identify even longer-range correlations among amino acids and secondary structure labels. as a second direction, we also plan to utilize sparse models to improve the feature set representation for other types of prediction tasks such as contact map prediction. in addition, the sparse non-local interaction patterns obtained in figure  <dig> characterize the propensity of residue pairs to interact and can be used as features directly in a contact map prediction or Î² strand pairing prediction algorithm. finally, the subsequent use of the secondary structure prediction method in a 3d structure prediction algorithm is also another future extension. for this purpose, it is possible to provide the posterior distribution generated from the dbns directly as input features to a structure prediction algorithm or convert the output of the svm to a probability  <cit> .

