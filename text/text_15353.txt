BACKGROUND
whole exome capture sequencing  and whole genome sequencing  using next generation sequencing  technologies  <cit>  have emerged as compelling paradigms for routine clinical diagnosis, genetic risk prediction, and patient management  <cit> . large numbers of laboratories and hospitals routinely generate terabytes of ngs data, shifting the bottleneck in clinical genetics from dna sequence production to dna sequence analysis. such analysis is most prevalent in three common settings: first, in a clinical diagnostics laboratory  testing single patients or families with presumed heritable disease; second, in a cancer-analysis setting where the unit of interest is either a normal-tumor tissue pair or normal-primary tumor-recurrence trio  <cit> ; and third, in biomedical research studies sequencing a sample of well-phenotyped individuals. in each case, the input is a dna sample of appropriate quality having a unique identification number, appropriate informed consent, and relevant clinical and phenotypic information.

as these new samples are sequenced, the resulting data is most effectively examined in the context of petabases of existing dna sequence and the associated meta-data. such large-scale comparative genomics requires new sequence data to be robustly characterized, consistently reproducible, and easily shared among large collaborations in a secure manner. and while data-management and information technologies have adapted to the processing and storage requirements of emerging sequencing technologies , it is less certain that appropriate informative software interfaces will be made available to the genomics and clinical genetics communities. one element bridging the technology gap between the sequencing instrument and the scientist or clinician is a validated data processing pipeline that takes raw sequencing reads and produces an annotated personal genome ready for further analysis and clinical interpretation.

to address this need, we have designed and implemented mercury, an automated approach that integrates multiple sequence analysis components across many computational steps, from obtaining patient samples to providing a fully annotated list of variant sites for clinical applications. mercury fully integrates new software with existing routines  and provides the flexibility necessary to respond to changing sequencing technologies and the rapidly increasing volume of relevant data. mercury has been implemented on both local infrastructure and in a cloud computing platform provided by dnanexus using amazon web services . while there are other ngs analysis pipelines, some of which have even been implemented in the cloud  <cit> , the combination of mercury and dnanexus together provide for the first time a fully integrated genomic analysis resource that can serve the full spectrum of users.

RESULTS
figure  <dig> provides an overview of the mercury data processing pipeline. source information includes sample and project management data and the characteristics of library preparation and sequencing. this information enters the pipeline either directly from the user or from a laboratory information management system . the first step, generating sequencing reads, is based on the vendor’s primary analysis software, which generates sequence reads and base-call confidence values . the second step maps the reads and qualities to the reference genome using a standard mapping tool, such as bwa  <cit> , producing a bam  <cit>   file. the third step produces a “finished” bam that includes sorting, duplicate marking, indel realignment, base quality recalibration, and indexing , and gatk  <cit> ). the fourth step in mercury uses the atlas <dig> suite  <cit>   to call variants and produce a variant file . the fifth step adds biological and functional annotation and formats the variant lists for delivery. each step is described in detail in the methods section, as is the flow of information between steps.

mercury has been optimized for the illumina hiseq  platform, but the generalized workflow framework is adaptable to other settings. the entire pipeline has been implemented both locally and in a cloud computing environment. all relevant code and documentation are freely available online  and the scalable cloud solution is available within the dnanexus library . sensible default parameters have already been determined so that researchers and clinicians can reliably analyze their data with mercury without needing to configure any of the constituent programs or obtaining access to large computational resources, and they can do so in a manner compliant with multiple regulatory frameworks.

local workflow management
implementing a robust analysis framework that incorporates a heterogeneous collection of software tools presents many challenges. running disparate software modules with varying inputs and outputs that depend on each other’s results requires appropriate error checking and centralized output logging. we therefore developed a simple yet extensible workflow management framework, valence , that manages the various steps and dependencies within mercury and handles internal and external pipeline communication. this formal approach to workflow management helps maximize computational resource utilization and seamlessly guides the data from the sequencing instrument to an annotated variant file ready for clinical interpretation and downstream analysis.

valence parses and executes an analysis protocol described in xml format with each step treated as either an action or a procedure. an action is defined as a direct call to the system to submit a program or script to the job scheduler for execution; a procedure is defined a collection of actions, which is itself a workflow. this design allows the user to easily add, remove, and modify the steps of any analysis protocol. a protocol description for a specific step must include the required cluster resources, any dependencies on other steps, and a command element that describes how to execute the program or script. to ensure that the xml wrappers are applicable to any run, the command is treated as a string template that allows xml variables to be substituted into the command prior to execution. thus, a single xml wrapper describing how to run a program can be applied to different inputs. valence can be deployed on any cluster with a job scheduler , implementing a database to track both the job  and the status  of any action associated with the job.

mercury users can easily incorporate new analysis tools into an existing pipeline. for example, we recently expanded the scope of our pipeline to include tiresias , a structural variant caller focused on mobile elements, and excid , an exome coverage analysis tool designed to provide clinical reports on under-covered regions. to incorporate tiresias and excid into the mercury pipeline, we needed only to specify the compute requirements and add the appropriate command to the existing xml workflow definition; valence then automatically handles all data passing, logging, and error reporting.

cloud workflow management
mercury has been instantiated in the cloud via the dnanexus platform . dnanexus provides a layer of abstraction that simplifies development, execution, monitoring, and collaboration on a cloud infrastructure. the constituent steps of the mercury pipeline take the form of discrete “applets,” which are then linked to form a workflow within the dnanexus platform infrastructure. using the workflow construction gui, one can add applets  to the workflow and create a dependency graph by linking the inputs and outputs of subsequent applets. inputs are provided to an instance of the workflow, and the entire workflow is run within the cloud infrastructure. the various steps within the workflow are then executed based on the dependency graph. as with valence, individual applets can be configured to run with a specific set of computational resource requirements such as memory, local disk space, and number of cores and processors. we are currently working to merge the local and cloud infrastructure elements by integrating the upload agent into valence, allowing valence to trigger a dnanexus workflow once all the data is successfully uploaded. such coordination would serve to transparently support analysis bursts.

the mercury pipeline within dnanexus comprises code that uses the dnanexus command-line interface to instantiate the pipeline in the cloud. the mercury code for dnanexus is executed on a local terminal. for example, one may provide a list of sample fastq files and sample meta-data locations to mercury, at which point mercury uploads the data and instantiates the predefined workflow within dnanexus. on average, on a  <dig> mbps connection, we were able to upload at a rate of ~14 mb/sec. we were able to parallelize this uploading process, yielding an effective upload rate of ~90 mb/sec. the size of a typical fastq file from wes with 150x coverage has a compressed  file size of approximately 3 gb. uploading such a file from a local server took less than five minutes. after sample data are uploaded to the dnanexus environment, the workflow is instantiated in the cloud.

progress can be monitored online using the dnanexus gui  or locally through the mercury monitoring code. to achieve full automation, the monitoring code can be made a part of a local process to poll for analysis status at regular intervals and start analysis of new sequences automatically upon completion of sequencing. when the mercury monitoring code detects successful completion of analysis an email notification is sent out. the results can either be downloaded to the local server or the user can view various tracks and data with a native genome browser.

performance
turn-around time for raw data generation on most ngs platforms is already considered long for many clinical applications, so minimizing analysis time is a primary goal of the mercury pipeline. by maintaining a high-performance computing cluster consisting of hundreds of 8-core, 48 gb ram nodes and introducing mercury into the sequencing pipeline, we can minimize wait times by ensuring that compute resources are always available for all sequence events as the instrument produces the data. to match compute resources to production requirements, we carefully monitor the run times  of each step in the mercury pipeline. table  <dig> describes the pipeline for each compute-intensive step, from the generation of reads and qualities from raw data  to generation of post-processed annotated variant files . resource requirements for each step are given in terms of fraction of an 8-core node  and ram allocated. note that some steps under-utilize available cpus because they require most  of the ram available on a given node. for data generated via wes from human samples, the mercury pipeline requires less than  <dig> hours of wall-clock time and  <dig> node-hours . run times and resource requirements will vary with data type, reference genome, and computing hardware configuration.

all estimates are approximate for whole exome and light-skim whole genome  sequenced on illumina hiseq and processed with the most recent versions of rta and casava. nodes are 8-core, 48 gb ram, with ~3 ghz intel cpus and ~1 tb of local scratch disk. steps include all aspects of the pipeline from building reads and qualities  from raw data , through alignment and bam generation using the bwa aligner, and bam finishing with gatk post-processing and duplicate marking, capture and coverage metric calculation, and bam file validation, finally producing variants from the atlas <dig> variant calling suite with annotations from our annotator, cassandra.

after porting each element of the mercury pipeline into the dnanexus environment, the tools  can be run on the cloud in environments with different cpu, ram, disk, and bandwidth resources to optimize wall-clock time and cost-efficiency. parallelization within a pipeline reduces the time for a single run, which is useful for quick development cycles or time-sensitive applications. in addition, many parallel pipelines can be run simultaneously. the current peak usage for the mercury pipeline on dnanexus is approximately  <dig>  cores. this throughput is a small fraction of the theoretical maximum that could be achieved in aws. for the standard implementation of mercury in the cloud analyzing a validation exome , the total wall time to produce an annotated variant call vcf  is approximately one day.

data management, sharing, reliability, and compliance
large projects present a large data management challenge. for example, the baylor-hopkins center for mendelian genomics has generated wes data from approximately  <dig>  samples and the human genome sequencing center at baylor college of medicine has generated more than  <dig>  wes and  <dig>  wgs data from research samples from the charge consortium  <cit> . as a pilot study, we processed  <dig>  wes data sets–approximately  <dig> terabytes of genomic data–from the center for mendelian genomics using mercury in dnanexus. using multi-threaded uploads we were able to deliver data into the cloud at an average rate of ~ <dig> exomes per day. once uploaded, the data is analyzed with mercury, and the resulting variants can be accessed for further analysis via a web gui. data can also be tagged, and these tags can be filtered or retrieved. runs of individual pipelines and tools can be queried in a similar way.

as datasets become larger, multi-site collaborative consortiums play an increasingly important role in contemporary biomedical research. a major advantage of cloud computing over local computing is that cloud storage can be shared across multiple organizations. instead of each collaborator maintaining a local copy of the data and working in isolation, cloud users can be given appropriate access permissions so some researchers can view and download the results, others can run analyses on the data and build tools, and those with administrative privileges can determine access to the project. this data paradigm is the only feasible approach to giving patients meaningful access to their own genomic data.

comparison to existing methods
a number of other tools and services provide similar functionality to mercury on dnanexus, with differing approaches to extensibility, ease of use for non-programmers, support for local or cloud infrastructures, and software available by default . for example, the academic service galaxy primarily focuses on extensibility and building a developer community  <cit> . seven bridges is a commercial service that combines a few fixed pipelines with a visually distinctive workflow editor. chipster is an academic service that packages a variety of ngs tools in addition to a variety of microarray tools and combines these with visualization of data summaries and qc metrics  <cit> . anduril is designed to manage a local cluster and contains packages for a variety of tasks, including alignment and variant calling as well as image analysis and flow cytometry, which are not addressed by the other cloud services surveyed  <cit> . with respect to the software used in sequence production pipelines, mercury is most distinguished by its atlas variant caller and the extensive annotations provided by its cassandra annotation tool .

*mercury can be run on local hardware via valence, but does not include all of the security and data sharing features of mercury in dnanexus.

future directions
as genomic studies transition from extensive us of whole exome capture sequencing methods to an emphasis on whole genome sequencing  <cit> , we can take advantage of mercury’s flexibility to adapt to shifting research priorities. currently, the sequencing community uses whole exome sequencing because of its comparatively low cost and because it enriches for biological signals that are readily interpretable. however, with changing price structures, the advantages of eliminating the capture step in the laboratory, improvements in lossless compression of sequence data, and improved annotation of the non-protein coding region of the genome , whole genome sequencing may become a more appealing option. early adopters of whole genome sequencing will certainly include medical sequencing for research and diagnostic purposes. therefore, a high priority for mercury is the continual updates to the annotation information using web-based databases. such web-based annotation updates can incorporate the latest developments related to genome function in near real-time.

clinically relevant annotation is of particular interest in the growing area of cancer somatic mutation. currently, disease-related annotation considers inherited disease alleles gleaned from omim , hgmd , and genome-wide association studies . similarly informative databases, such as the catalogue of somatic mutations in cancer , are emerging and will need to be fully vetted and incorporated into cassandra. the goal of mercury is to provide a simple solution for end-to-end sequence analysis so that non-expert users can obtain a list of annotated and prioritized variants as rapidly as possible, and so that expert users can augment and modify the pipeline to meet specialized needs.

CONCLUSIONS
the long-anticipated ngs data deluge  <cit>  has now arrived. the first personal ngs genome was published in  <dig>  <cit> , and today we estimate that the number of available exomes and genomes approaches one hundred thousand. it is equally impressive that the application of ngs to biomedical research and clinical medicine is rapidly becoming standard. such applications are driven by the utility of sequence data, as demonstrated by a number of instances where dna sequencing has been used not only for diagnostic purposes but also to reveal more efficacious therapies  <cit> .

by taking advantage of cloud computing and with mercury implemented on the dnanexus platform, we have demonstrated a powerful combination of a robust and fully validated software pipeline and a scalable computational resource. to date, we have analyzed thousands of samples , including a population study comprising  <dig>  samples and  <dig>  samples for which wgs and wes were generated, respectively, more than  <dig>  mendelian disease cases shared with data consumers all over the world, and smaller projects such as  <dig> exome trios and  <dig> cancer wgs tumor/normal pairs. to our knowledge, these projects represent the largest genomic analysis effort to be realized in the cloud to date. they presage a wave of genomic computing that will transform how genomic data is analyzed and delivered to the scientific community, into clinical practice, and eventually directly into the hands of patients and advocates.

