BACKGROUND
affymetrix genechip arrays are one of the most popular gene expression array systems used by researchers worldwide  <cit> . the purpose of an expression microarray experiment is to measure the abundance of each known transcript in the sample under investigation. abundance is inferred from the signal generated by a set of 11– <dig> probe pairs. each pair is composed of a perfect match probe , which exactly complements a region on the transcript, and a mismatch probe , which is identical to the pm probe except at the 13th base, where the reverse complement nucleotide is introduced  <cit> . the fluorescent signal from each probe, however, includes background noise that not only measures the transcript abundance, but also non-specific binding  and autofluorescence of the chip surface. mm probes were originally introduced by affymetrix to measure background noise. it has been shown by many groups that mm probes contain significant amount of the pm signal and are therefore unreliable as estimators of background noise  <cit> .

a gene expression experiment using the affymetrix genechip system usually involves a design step, a preprocessing step, an inference step and finally, a validation step  <cit> . the preprocessing step is of special importance; preprocessing transforms the raw fluorescence signals from each probe in a probeset into a composite gene expression value. the main goal of the preprocessing step is to remove non-biological variation from the raw data  <cit> . usually, the preprocessing step in affymetrix genechip array analysis includes three main treatments of the raw data. a background adjustment step separates the specific signal from the non-specific signal. a probe-level normalization step then removes non-biological variation between arrays. finally, a summarization step generates a single expression value for each gene from its corresponding probeset. the method described in this manuscript is an implicit physical model that modifies the background adjustment step.

background noise and non-biological variation of the signal generated from each probe are common phenomena in genechip microarray experiments  <cit> . the differences in the signal produced can be attributed to many sources: optical noise, cross-hybridization, dye-related contributions and probe sequence composition. many preprocessing algorithms have been developed in an attempt to correct for these artifacts  <cit> . according to allison et al.  <cit>  there is no clear winner among the available preprocessing algorithms. however, gcrma  <cit> , a modification of rma  <cit> , often performs as well as or better than other algorithms  <cit> . gcrma incorporates probe sequence composition into background adjustment, following the physical model of naef and magnasco  <cit> . the model describes a probe affinity that is dependent on its base composition and the position of each base along the probe and suggests that probe sequence can significantly affect the intensity of the signal generated from that probe, independent of the concentration of its target.

performance assessment of gcrma has been done using both spike-in  <cit>  and real  <cit>  datasets followed by quantitative real time pcr confirmation  <cit> . so far, a number of reports have been published recommending the use of gcrma for detecting differentially expressed genes and estimating relative expression, emphasizing its outstanding performance in detecting low-intensity, differentially expressed genes  <cit> . when comparing microarray analysis algorithms, irizarry et al.  <cit>  have argued for an approach that balances accuracy and precision. irizarry et al., define accuracy as the ability of the algorithm to detect the relative expression of a transcript without bias to its abundance . they define precision as low variance; this is characterized by a steady performance on replicates of the same sample. gcrma is among the few preprocessing algorithms that scores well in both accuracy and precision  <cit> .

in this study, we modified the portion of gcrma derived from the model of naef and magnasco  <cit>  to calculate probe affinity using position-specific dinucleotide information. the dinucleotide is a fundamental chemical unit that contributes a well-understood component to nucleic acid duplex stability and to the free energy of duplex formation during hybridization  <cit> . we applied the new model to different datasets, and achieved an improved fit to microarray data with r <dig> increasing by 5–10%. then, we tested the downstream effect of our modified background model on the performance of gcrma in detecting differentially expressed genes, when used to analyze two publicly available control datasets: the human genome u <dig> latin square dataset  <cit>  and the golden spikein dataset  <cit> . in both data sets, application of the dinucleotide model in background correction improved the detection of differentially expressed genes. therefore, we propose that probe affinity be modeled based on dinucleotide composition of the probe instead of the original single nucleotide approach.

RESULTS
dinucleotide affinity model
naef and magnasco  <cit>  model probe affinity  based on sequence composition as follows:

  ln⁡〈b/m〉=∑k=125∑l∈slkalk 

where b is the raw probe intensity, m is the median intensity of the array, l is the nucleotide index , k is the position of l along the probe , s is a boolean variable equal to  <dig> if the probe sequence has l at k and zero otherwise, and a is the per-site-per- nucleotide affinity. as an example, consider the following sequence: cgac, for which equation  <dig> reads:

 ln⁡〈b/m〉=+++++++++++++++ln⁡〈b/m〉=a1c+a2g+a3a+a4c 

equation  <dig> is a simple model that has four free parameters for each probe base . the values of these  <dig> free parameters are generated by linear least squares fit. given the large number of probes on each chip  over-fitting is not a concern.

the model defined in equation  <dig> can also be expressed as a polynomial of degree  <dig>  thus reducing the free parameters from  <dig> to  <dig> as shown below:

  ln⁡〈b/m〉=∑k=125∑l∈∑t=03slkaltkt 

by assuming the affinities can be modeled as a third order polynomial function of position, the number of free parameters in the model can be reduced from  <dig> to  <dig> with little loss of predictive accuracy as the polynomial generated with  <dig> parameters  closely matches the  <dig> independently estimated parameters  and the r <dig> of both models are similar .

in the dinucleotide model, we follow a similar strategy to the above, but we model composition-biased probe affinity using dinucleotides , which are a fundamental chemical unit in physical models of nucleic acid folding and hybridization rather than single nucleotides. the dinucleotide model is as follows:

  ln⁡〈b/m〉=∑k=125∑l∈nnslkalk 

where b is the raw probe intensity, m is the median intensity of the array, l is the nn nucleotide pair , k is the position of l along the probe , s is a boolean variable equal to  <dig> if the probe sequence has l at k and zero otherwise, and a is the per-site-per-dinucleotide affinity. we then again assume that the per-site-per-dinucleotide affinity follows a polynomial of degree  <dig> as a function of the position k as outlined in equation 4:

  ln⁡〈b/m〉=∑k=125∑l∈nn∑t=03slkaltkt 

this reduces the number of free parameters from  <dig>  to  <dig> , which makes this approach computationally feasible. as an example, consider the following sequence: cgac , for which equation  <dig> reads:

 ln⁡〈b/m〉=++++++++++++ln⁡〈b/m〉=4acg+15aga+40aac 

note that we do not explicitly fit the stacking energies of the nn pairs; rather we explicitly fit the nn pairs' affinities along the probe sequence position.

the fitted per-site-per-dinucleotide affinities are shown in fig.  <dig> for the latin square dataset. parameters obtained from other datasets are similar to the latin square dataset parameters . the figure shows that a probe with many an  pairs  tends to have much lower intensity than a probe with many cn pairs  especially when those pairs are located at or near the probe center. this is broadly what we expect from the single nucleotide model. however, examining the effect caused by second nucleotide in each nn pair shows a pronounced effect for certain dinucleotides, which cannot be captured in the single nucleotide model. this can be seen in fig. 2c and 2d. ga and gt rich probes are significantly brighter than gc rich probes, and ta rich probes are brighter than tc and tg rich probes.

the model defined in equation  <dig> was fitted to a number of datasets . fitting was performed on the pm and mm probes separately. table  <dig> shows a comparison between the native naef and magnasco  <cit>  affinity model  and our dinucleotide affinity model . we see that the dinucleotide model gives a better fit to microarray data by 5–10% on average , depending on the chip and probe type. note that both models perform better on the mm probes due to the higher background noise present in the mm signal.

r <dig> of naef and magnasco  <cit>  model  and the dinucleotide model for the five data sets used in this study. results presented as average r <dig> ± sd.

anc: number of chips.

bnp: number of probes.

c the differences in r <dig> between single nucleotide model and dinucleotide model are all statistically significant  using paired one-sided wilcoxon and t tests.

given that our fits contain between  <dig>  and  <dig>  data points , it seems unlikely that the improvements in performance of our model could be explained by the additional free parameters . nonetheless, to rule out this possibility, we fitted both the single nucleotide model   and the dinucleotide model  with  <dig> free parameters  to the latin square dataset using completely random probe sequences . we also performed the same test on shuffled probe sequences in which the probe's base composition is not affected, but the position of each base has been changed due to the shuffling process. the results of this analysis are shown in additional file  <dig>  we see that the r <dig> of the shuffled and random probe sequences are nearly identical, no matter which method is used. the presence of additional free parameters in our model, therefore, cannot by itself explain the improved performance over the naef and magnasco model. this strongly supports our argument that the gain in the r-squared values of the nn model comes from including dinucleotide information and does not arise trivially from the addition of free parameters.

background adjustment using dinucleotide affinity model
using a more accurate estimate of background noise should improve the quality of affymetrix genechip data. given the better fits observed using the dinucleotide affinity model, we expected it to improve the analysis results to some degree when applied to control datasets. we tested the downstream effects of using this model on the quality of microarray data. we chose to implement the model within gcrma  <cit> , since it already has the single nucleotide model implemented in its background correction procedure, and therefore the two models could be directly compared.

in gcrma, wu et al.  <cit>  model the signal intensity generated from each probe as:

  pm=opm+npm+s,mm=omm+nmm+ϕs 

where o is the optical noise, n is the background noise of non-specific binding, and s is the signal generated from specific binding between the probe and its intended target. the parameter φ reflects the fact that for some probe pairs, the mm signal may contain specific signal. the background components log and log are assumed to follow a bivariate distribution with means of μpm = h and μmm = h, where h is a smoothing function and α  is defined by equation  <dig>  in this paper, we make these same assumptions, but we derive α using equation  <dig> 

we reasoned that gcrma with background correction using the dinucleotide model, which we will subsequently refer to as gcrma-nn in this paper, would perform better than the native gcrma model. it is important to clarify that gcrma offers two options for background correction, the first of which uses a precomputed α  from the authors' own non-specific binding  experiments, while the second computes α directly from the data . in the following figures, we compare gcrma-nn  to gcrma-l  and gcrma-r .

latin square dataset
we obtained expression measures for the human genome u <dig> latin square dataset after processing it with gcrma-r, gcrma-l and gcrma-nn. the three expression measures were evaluated using two approaches. the first approach is based on affycomp  <cit> , a performance evaluation tool for preprocessing algorithms . the second approach is based on the number of true positives captured for all the  <dig> 2× comparisons of the latin square dataset at a cutoff of four false positives after using the cyber t test  <cit> . cyber t is a popular variant of the t test, in which a weighted standard deviation replaces the conventional standard deviation and an adjusted number of degrees of freedom is used instead of the conventional degrees of freedom.

performance of gcrma-r, gcrma-l and gcrma-nn as reported by affycomp based on  <dig> metrics is shown in table  <dig>  one notable performance enhancement of gcrma-nn over gcrma-l and gcrma-r is a 3–4% increase in the weighted average area under the curve  . this is a receiver operator characteristics  based metric, in which the absolute log-ratios for the expression summaries, for every comparison of any two pairs of the  <dig> arrays , are sorted. after that, the number of true and false positives is found, and then the number of true positives at  <dig> false positives is determined for each pair of arrays. finally, the resulting values are averaged over the three concentration groups , weighted by the number of probesets in each group and a score is recorded. note that a perfect algorithm will have a score of  <dig>  where all the true positives are captured before any false positive is recorded.

fourteen affycomp metrics for the u <dig> latin square dataset rounded to two decimal points.

a brief description of each metric is provided under the methods section.

examining table  <dig> shows that the increase comes mainly from the auc for low intensity targets . the low intensity genes make up most of the genes in a typical affymetrix experiment  <cit>  and are also the hardest to detect. algorithms that perform inference generally can detect large changes involving highly expressed genes. it is much more difficult to detect changes in the more frequently observed genes that produce low intensities on the array. gcrma-nn enhanced the detection of low intensity targets, while maintaining similar values for the medium and high intensity ones. the enhancement in detecting low intensity targets is also evident in the form of an increase in the low detection slope .

in the crucial category of low intensity genes, we argue that our algorithm outperforms most of the algorithms submitted to affycomp, including gcrma-r and gcrma-l. the affycomp webpage currently contains data for  <dig> algorithms for analyzing affymetrix microarrays. for each of these algorithms, affycomp defines accuracy as the slope obtained from regressing expression values on nominal concentration. an algorithm with a perfect accuracy would have a slope of  <dig>  reflecting a perfect correspondence between nucleotide concentration and signal. affycomp defines precision as the  <dig> % percentile of the log fold changes of null  probesets across arrays. a perfect algorithm would have a precision of  <dig> reflecting a fold change of  <dig> . figure  <dig> is a plot of precision vs. accuracy for the latin square dataset for the  <dig> algorithms submitted to the affycomp webpage. in figure 3a, we see that when looking at overall accuracy vs. precision, the gcrma-nn algorithm  performs about as well as gcrma-r  and gcrma-l . however, for the crucial low intensity genes, for which inference is the most difficult, gcrma-nn provides a better accuracy with no loss of precision .

since the results of affycomp suggest an improvement for the low intensity, hard to detect spikeins, we reasoned that inference performed with gcrma-nn would be more successful than inference with gcrma-r or gcrma-l. we therefore applied gcrma-nn, gcrma-r and gcrma-l to the u <dig> latin square dataset. we considered only the  <dig> 2× comparisons, in which the ratio of each spikein, between any two consecutive pair of arrays, is  <dig>  then we used the cyber t statistic  <cit>  to generate a list of p values for the null hypothesis that the mean signal intensity in each comparison is the same. the lists were ordered, and for each of the  <dig> comparisons we generated an roc curve. figure 4a shows the average of these  <dig> roc curves. for each roc curve, we determined the number of true positives captured at an arbitrary cutoff of four false positives . the result of this analysis is summarized in figure 4b. we see that gcrma-nn outperforms gcrma-r and gcrma-l with a small but significant improvement. one-sided wilcoxon and t tests reject the null hypothesis that gcrma-nn is the same as gcrma-r and gcrma-l with all tests p <  <dig> . these are consistent with the results we would have expected based on the affycomp comparison .

golden spikein dataset
in order to ensure that our data were valid for more than one control data set, we next applied gcrma-r, gcrma-l and gcrma-nn to the "golden spikein dataset"  <cit> , which is not included in affycomp. figure  <dig> shows a roc graph for the differentially expressed genes between the s and the c "golden spike" samples  detected by gcrma-r, gcrma-l and gcrma-nn. as in the latin square data, the graph shows that gcrma-nn is capable of capturing more true positives at lower false positive rate than both gcrma-r and gcrma-l. this supports our assertion that an improved background correction algorithm can have a noticeable effect on downstream analyses.

discussion
background estimation and correction are important steps in analyzing the data generated by genechip arrays. improving algorithms for these steps increases the amount of true "signal" that we can detect from microarrays. understanding background noise on genechip arrays, especially the part contributed by nsb signal, requires a deeper understanding of the behavior of on-chip hybridization. given that we lack a detailed physical model of on-chip hybridization derived from first principles, an empirical model that estimates the specific and non-specific signal based on the data on the array and probe sequence is a useful tool for understanding the on-chip hybridization process.

nucleic acid hybridization in solution is well approximated by the nearest neighbor model  <cit> , which describes duplex formation as a function of the two adjacent nucleotides and their stacking orientation. this approach was used by zhang et al.  <cit>  to model the on-chip specific and nonspecific hybridization using the free energy formation for the adjacent nucleotides. zhang et al. concluded that the on-chip hybridization parameters are different than the solution ones. using a different approach to background correction, naef and magnasco  <cit>  used single nucleotides to assign an overall affinity score for a probe based on its sequence away from the energy contributions of the dinucleotide pairs. this approach was used to perform background correction for the gcmra algorithm  <cit>  while the zhang et al approach was used to create the algorithm perfectmatch  <cit> . perfectmatch estimates the signal and the background at the same step while gcrma estimates background noise first then proceed to signal estimation. perfectmatch is, therefore, much more computationally demanding than gcrma as the parameter space searched by perfectmatch is vast and is sampled with monte carlo methods. direct comparison of gcrma and perfectmatch has proven controversial. such a comparison is beyond the scope of this report, and can be found elsewhere  <cit> .

in this report we combine some elements of gcrma and perfectmatch. we replace the single nucleotide model of naef and magnasco with a model in which the affinity of each probe is a function of its dinucleotide composition. because we use gcrma's approach of separating estimates of background and signal, we can use a linear model and avoid the monte carlo simulation approach of perfectmatch  <cit> . our approach is therefore both computationally more efficient and guarantees the best fit to the data. this approach enables us to examine the contribution of different dinucleotides at different positions to the raw probe signal , rather than assigning one weight function to all the dinucleotides, as is done with perfectmatch  <cit> . this allows our model to capture several important features of the background data such as the effect of the first versus the second nucleotide on probe affinity , and the effect of the stacking orientation . in general, we find that the dinucleotide approach has more power than the single nucleotide approach over a wide range of datasets .

the mechanism that determines why particular dinucleotides affect probe affinities the way they do is, in some cases, unclear. however, we observe that the nn model bears some similarities to the models of both naef and magnasco and zhang et al. all three models emphasize the importance of the probe middle region; this is probably due to the surface attachment, as well as to the relative instability of the free end in rna-dna hybridization. the effect of the stacking orientation is in agreement with the findings of zhang et al.  <cit> . the an versus cn  asymmetry  is in agreement with naef and magnasco  <cit> . when comparing these affinity curves to the original naef and magnasco result, it is important to recognize that the nn model considers the affinity of dinucleotides rather than single nucleotides. therefore, we do not necessarily expect to see the same asymmetry within cn or an, i.e. there will be no asymmetry between ca and cc , or between aa and ac . the nn model, however, does show unexpected behavior for the gn and tn dinucleotides. while both g and t show slight asymmetry in the naef and magnasco model, the effect of these two nucleotides is magnified in the nn model. gn contributes positively to the signal but not when the second nucleotide is c . tn contributes negatively but not when the second nucleotide is a . this trend is partially explained by the fact that t forms fewer hydrogen bonds than g, therefore contributing negatively, while the g has stronger binding, thus contributing positively. this trend is not consistent, and appears to be dependent on the adjacent nucleotide. it could also be due to the biotin label present on the rna target sequence.

when applied to two control datasets, gcrma-nn showed improved performance  especially on low intensity targets . we argue that this is due to better background correction for these targets; a higher percentage of low intensity signal will be made up of background, so it is therefore not surprising that better background correction will make more of a difference on low intensity targets. the detection of low intensity targets represent the most significant challenge to microarray analysis algorithms, which makes any enhancement in the detection of these targets significant.

CONCLUSIONS
incorporating dinucleotide information into a previously described probe affinity model increases the fit of the model by 5–10%. the dinucleotide affinities highlight the importance of the stacking orientation on probe behavior. this is in agreement with the physical models that describe hybridization binding affinities.

the results presented here show that the affinity of any single nucleotide is affected by its neighbor, in addition to its location along the probe. considering the second nucleotide offers more insights into the on-chip behavior of the four bases in relation to each other. such insights are important to develop a better understanding of the on-chip hybridization process and therefore better analysis procedures. the model described here enhances the performance of an existing widely-used preprocessing algorithm for genechip data. we expect the same model to enhance the performance of preprocessing algorithm for other types of arrays, in particular those used for snp analysis.

