BACKGROUND
the term-matching problem has been widely addressed in multiple contexts, which resulted in a number of string similarity metrics designed, applied and evaluated in various research studies  <cit> . in the biomedical domain, various asm methods are used by scientists to solve current research tasks such as retrieving sequences from existing databases that are homologous to newly discovered ones, and establishing multiple sequence alignment to discover similarity patterns to predict the function, structure, and evolutionary history of biological sequences  <cit> .

the recent expansion of healthcare information systems that draw from multiple medical databases has resulted in redundant information, among other problems. this phenomenon, also known as the duplicate detection problem, has caused problems with record linkage across medical databases. previous research has addressed problems such as patient record aggregation from multiple databases based on a minimum profile   <cit>  and term matching for source integration, spelling correction and biomedical data mining applications. in this paper, these tasks are considered in the context of terminologies such as systemized nomenclature of medicine clinical terms  and the unified medical language system   <cit> . approximate string matching  methods are used for augmenting, updating, and auditing umls vocabularies. asm methods are also important for facilitating biomedical information extraction, relationship search, and concept discovery  <cit> .

the umls is an extensive terminological knowledge base comprised of three major components: the metathesaurus, the semantic network, and the specialist lexicon and lexical tools. the current 2013ab release of the metathesaurus contains more than  <dig>  million concepts and  <dig>  million unique terms retrieved from over  <dig> source vocabularies  <cit> . umls source integration is a complicated multistep process and, despite the availability of numerous algorithmic tools, managing these vocabularies requires considerable human involvement. as additional sources are integrated into the umls, they will require reintegration with existing vocabularies  <cit> .

these disadvantages motivate the search for a new method for approximate string matching and umls-based evaluation. in this paper, we introduce the longest approximately common prefix  method for asm and present the results of its use to improve the operation of a number of applications in biomedical informatics and related domains.

it bears noting that, in contrast to the well-known specialist lexicon tools norm, word index or lvg  <cit> , lacp does not perform text manipulations. instead, it assesses the similarity or dissimilarity of two strings.

other three highly praised instruments, metamap  <cit> , ncbo annotator  <cit>  and conceptmapper  <cit>  are publicly available concept recognition systems designed for text annotation from various ontologies  <cit> . the general rationale of these tools is to split the input text into smaller constructions, such as phrases or tokens, which are subsequently looked up in a dictionary. for instance, metamap splits the input text into phrases and produces their variants. then it generates a candidate set, which is mapped to an ontology. the lacp method, introduced in this paper, may be used as an inner component of such a system for calculating the similarity of a candidate phrase or token when matching to various ontology terms. the authors consider implementation of a text annotation system incorporating the lacp method as a direction for future research.

the rest of the section is dedicated to the analysis of the relevant research approaches and the related work studying the application of well-known similarity measures in the biomedical domain.

tan et al.  <cit>  applied the classic levenshtein score incorporated with a particular threshold to medical ontology alignment. tolentino et al.  <cit>  utilized the levenshtein technique in combination with other string similarity algorithms to construct a umls-based spell checker. sahay et al.  <cit>  employed more advanced combinations of the jaro and jaro-winkler similarity metrics combined with term frequency/inverse document frequency  to compute similarity values between ontological concepts and phrases. cohen et al.  <cit>  described, implemented and evaluated the above-mentioned hybrid distances in the secondstring java toolkit.

plaza et al.  <cit>  applied heuristic rules with a clustering algorithm to the problem of biomedical text summarization. their work mapped terms found in a given document to umls concepts. using the relationships between the identified umls concepts, the authors then represented the document in a graph. they graphed the concepts and assigned sentences to clusters based on semantic similarity. finally, the most important sentences were selected to be included in a document summary.

zhen et al.  <cit>  introduced a tfidf string distance method within their clustering algorithm and applied it to biomedical ontologies. the evaluation of their method demonstrated superior values of the f-measure on two datasets derived from the mesh and go ontologies.

in a previous paper, we developed a novel markov random field-based edit distance  and applied it to the asm problem in go ontologies  <cit> . similarly, wellner et al.  <cit>  used conditional random fields in a distance metric method on a umls metathesaurus dataset. bodenreider et al.  <cit>  applied the cosine, jaccard and dice string similarity coefficients to aligning the umls semantic network with the metathesaurus.

yamaguchi et al.  <cit>  tested four similarity metrics for clustering terms, which appeared in the umls metathesaurus. the authors compared the performances of monge-elkan, softtfidf, jaro-winkler and the bigram dice coefficient methods evaluating these techniques on chemical and non-chemical terms grouped into two datasets. they demonstrated that normalized string distances performed better than the standard measures for the evaluation of precision, recall, and f-measure, and that similarity metrics required different parameters such as threshold values for chemical and non-chemical terms, among other findings.

sauleau et al.  <cit>  propose a novel method for linking medical records by examining the connections between stand-alone and clustered databases. the authors developed a three-step approach: 1) preprocessing the data and applying blockers, 2) matching pairs of records using the porter-jaro-winkler score calculation, and 3) clustering the data. the authors suggest that their method is useful for inserting new entities into large databases.

zunner et al.  <cit>  studied the semi-automated mapping of non-english terms to logical observation identifiers names and codes   <cit>  using the regenstrief loinc mapping assistant   <cit> . their approach resulted in a mapping rate of  <dig> terms per day, which they considered satisfactory.

in research by parcero et al.  <cit> , mapping a local terminology to the loinc dataset led to the development of an automated tool that uses an approximate string matching function. mcdonald et al. benchmarked jaccard, levenshtein, monge-elkan, and soft tfidf metrics for loinc integration, and the jaccard method was selected as the best choice for such a task  <cit> .

the present research employs the shortest path edit distance  algorithm we developed previously  <cit>  to compute a string distance based on substring matching and graph-based transformations. to adjust the dissimilarity values in the final results, we applied a re-scorer set according to the length of equal string prefixes. this final step produced a major improvement in results and inspired this paper on the longest approximately common prefix  method, a novel string similarity metric based on the approximate prefix match of two strings. this paper demonstrates how this fast string distance method provides performance that is superior to other methods on datasets from snomed ct and from multiple umls sources  in terms of average precision and maximum f <dig> 

methods
the longest approximately common prefix  method is based on an approximate histogram match of string prefixes. it identifies matches by determining the similarity value of a pair of strings. the method compares the histogram differences between the prefixes of two strings to parameter α. it begins its search in the first characters of the strings. the prefix length is returned when the histogram difference is equal to α or the last character of the shorter string is reached. the prefix length is then divided by the average length of the pair of strings. the division takes into consideration string lengths, since strings that have significantly varying lengths are more dissimilar than strings that do not. the division also assures that the value of the lacp function stays in the  <cit>  interval. the formula for the lacp function  is as follows:

  lacps,t=1-preflengths,ts+t/ <dig> 

where preflength is the length of the longest approximately common prefix. according to formula , for two identical strings, lacp is  <dig>  whereas lacp is  <dig> for two strings not sharing any common prefix under a certain selection of the parameter α. the formula for preflength is given in  below:

  preflength=iprefhistdiffs <dig> .i,t <dig> .i=α∩prefhistdiffs <dig> .i- <dig> t <dig> .i-1<α 

where prefhistdiff is a histogram difference function of string prefixes, α is a parameter, and s <dig> .i
 and t <dig> .i
 are prefixes of strings s and t of length i. for example, for the strings s = anorexia and t = angina, with an α =  <dig>  the preflength would be  <dig>  because two initial characters match and α allows only one mismatch. alternatively, with α =  <dig> the preflength would be  <dig> because two mismatches are allowed.

the histogram difference function for string prefixes is defined in formula :

  prefhistdiffs <dig> .i,t <dig> .i=i-hists <dig> .i∩histt <dig> .i 

where hist is a histogram, and i satisfies the inequality :

  1≤i≤mins,t 

a histogram is an array, that counts the number of occurrences of each distinct symbol in a string. in formulae  and , i denotes a prefix length. by subtracting from i the number of characters that are common to the histograms of both prefixes, the number of non-common characters remains in the difference. this number of non-common prefixes is matched against the parameter α, as is shown in formula . during the evaluation phase, we used α =  <dig>  which allowed two mismatches in histogram difference.

the expression hist ∩ hist denotes the histogram intersection of two string prefixes. figure  <dig> depicts the histogram intersection of two umls terms, ammonium and ammonium ion. the histogram of ammonium is in figure 1a, the histogram of ammonium ion is in figure 1b. the intersection  is computed as the minimum for each pair of argument values of the same character, with missing values in one argument omitted from the result.

for example, ammonium contains one “o” while there are two letters “o” in ammonium ion. as min =  <dig>  the resulting histogram in figure 1c contains the entry “1” for the letter “o”. as there is no blank in ammonium, there is also no entry for the blank character in the resulting histogram. in order to compute the size  of the histogram intersection in figure 1c, the sum of all the numbers in the result matrix is calculated. for figure 1c, the size of the histogram intersection is  =  <dig> 

an example of three strings sharing the same prefix is shown in table  <dig>  strings  and  comprise the first pair, and strings  and  form the second pair. clearly, the first pair of strings is more similar than the second pair. to account for this and similar cases, the length of the approximately common prefix is divided by the average string length in formula . in table  <dig>  strings  and  belong to the umls concept with concept unique identifier  c <dig>  while string  is associated with  c <dig> 

the lacp algorithm is in table  <dig>  the algorithm begins by setting the histogram intersection at  <dig>  the search for the longest approximately common prefix begins with the first character of each string. in steps  <dig> and  <dig>  the characters at the current position i of strings s and t are added to the corresponding histograms. in steps  <dig> through  <dig>  all characters in the histogram of string s are compared against the histogram of string t at the current iteration i. at this point, the search has advanced to the i-th character of each string. steps  <dig> and  <dig> describe the following: when a character c is found in both histograms, operation get retrieves the count of this character from both hists and histt. then the smaller of the two values is added to the intersection. the search continues until the parameter α is reached, as shown in line  <dig>  or the last character of the shorter string is processed, as specified in line  <dig>  in the latter case, the length of the shorter string is computed in line  <dig> 

despite its linear time computational complexity, the simplicity of the lacp algorithm ensures a short execution time. the big-o computational complexity is commonly used for estimating the speed of an algorithm in computer science. the calculation of the lacp method time complexity is shown in table  <dig>  the inner loop in step  <dig> is bound by the number of printable characters and therefore constant  <cit> . thus, the complexity of the lacp algorithm is linear, i.e., o, which is fast comparing to other algorithms evaluated in this paper.

lacp-based interactive spell checker
we have employed the lacp method to develop an interactive online spell checker  <cit>  for snomed ct terms. the spell checker is a program written in php, which connects to a mysql database containing snomed ct terms from the 2009ab edition of the umls. the goal of the application is to evaluate lacp performance by revealing the set of snomed ct terms that are similar to the user-provided input term.

the spell checker accepts an input query and interactively outputs the snomed ct terms satisfying the condition lacp < t. here, s is the input term, t is a snomed ct term, and t is a threshold. to reduce the run time, the algorithm limits the set of search terms by applying length criteria as described below.

there are several parameters that define the performance of the spell checker depending on the mode of operation. the length of a snomed ct term |t| that is considered a potential match is bound by formulas , , and  in conformity with each of the three modes of operation. parameters a and b are used in  to determine the values of the lower and upper limits for |t|, respectively. parameter α sets the upper bound for the number of allowed character mismatches in the prefixes of strings s and t. threshold t defines the “cutoff point” for the lacp score; a pair of strings s and t is considered to be a match when the lacp score is less than the threshold t.

three modes of operation are implemented:  a search with dynamically estimated parameters;  a search with static parameters; and  a search with user-defined parameters. in case , the search is limited to the database terms meeting the criterion , while α is defined in  and threshold t is  <dig> .

  max <dig> s-s10-3<t<s+s10+ <dig> 

for example, for string s = ischemia, |s| =  <dig>  thus, according to , the dynamic search would be limited to terms longer than  <dig> characters and shorter than  <dig> characters. in case , parameter α is set individually for each pair of strings s and t as shown in :

  α=mins,t <dig> 

in case , α is set to  <dig>  threshold t is  <dig> , and the length of a term should be in the following range :

  max <dig> s-3<t<s+ <dig> 

in case , a user selects parameter values from predefined sets. the search is restricted to terms with lengths within the interval .

  max <dig> s-a<t<s+b 

parameters a, b, and α are constrained to integers in the interval  <dig> . <dig>  and threshold t must be selected from the set { <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> ,  <dig> }.

the dynamic search option adjusts the number of allowed misspellings α along with minimum and maximum term length parameters according to the input query. the dynamic search offers flexibility without user intervention. the threshold t is set to  <dig>  for this search mode.

the static search option operates with constant parameter values. it allows only one misspelling. the lengths of the returned strings must be in the neighbourhood of ± <dig> characters of the input query length. this option decreases the search time for longer input terms compared to the dynamic search option.

the search mode based on user-defined parameters expands parameter options within pre-defined ranges. this mode is intended for users who are not satisfied with the results of the dynamic and static modes or who seek more refined results.

in summary, the dynamic option is suggested when results significantly vary in length from the search query. the static search option should be used when the resulting strings is expected to lie in the neighbourhood of the input term. the search with user-defined parameters is intended for fine-tuning results or for a more advanced search.

RESULTS
the lacp was compared to nine other well-known approximate string distance metrics: jaccard  <cit> , jaro  <cit> , jaro-winkler  <cit> , levenshtein  <cit> , monge-elkan  <cit> , needleman-wunsch  <cit> , smith-waterman  <cit> , tfidf  <cit> , and soft tfidf  <cit> . lacp was compared with these string matching methods on four datasets derived from version 2009ab of the umls . dataset d <dig> was obtained by counting occurrences of each concept unique identifier  within the umls  <cit> , retrieving all terms corresponding to the  <dig> most frequent cuis and eliminating records with duplicate terms. d <dig> was created in the same way, but limited to concepts from snomed ct  <cit> . d <dig> was built by retrieving the  <dig>  longest terms from the multiple umls sources. d <dig> was constructed by taking the  <dig>  longest terms from snomed ct.

secondstring  <cit> , an open-source java toolkit, was used as an experimental test bed. during the experiments, each term was matched against those within a set of candidate pairs. this type of set reduces the problem size and speeds up experiment execution. the candidate set includes pairs of terms from the dataset that share one or more common words. the goal was to determine whether every pair of terms has the same cui. using common performance evaluation methods from information retrieval  <cit> , we calculated average precision , recall  and maximum f <dig> values , , and ), and graphed precision-recall  curves for our method and for the competing techniques. precision and recall are tradeoffs against one another: on the one hand, it is possible to obtain the maximum value of recall with a low value of precision by retrieving all documents for all queries. on the other hand, the precision usually decreases as the number of retrieved documents grows. a single measure that trades off precision versus recall is the f measure, which is the weighted harmonic mean of precision and recall  <cit> .

  p=drdt 

  r=drnr 

  f1=2p∗rp+r 

in  and , d
r
 denotes the number of relevant items retrieved, d
t
 is the total number of retrieved items, and n
r
 is the number of relevant items in the collection.

lacp achieves the highest average precision for datasets d <dig> and d <dig>  and the best values of maximum f <dig> for d <dig>  d <dig>  and d <dig> . tfidf and soft tfidf achieve the best scores of average precision for d <dig> and d <dig> and the largest maximum f <dig> for d <dig>  it is worth noting that tfidf and soft tfidf demonstrate exactly the same values of average precision and maximum f <dig> for each dataset, although soft tdidf executes the operation at a significantly slower pace.

d

1
d

2
d

3
d

4
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
note: the best values for each column are formatted in bold italics.
f

1


d

1
d

2
d

3
d

4
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
note: the best values for each column are formatted in bold italics.

table  <dig> shows that lacp is the fastest method on every dataset. figure  <dig> depicts four precision-recall charts plotting interpolated precision values at  <dig> recall levels  <cit> . the horizontal axis shows  <dig> recall points; the vertical axis displays interpolated precision values. a method with a larger area under its curve demonstrates a better result. the differences in performance between lacp, tfidf and soft tfidf are easily apparent. for d <dig> and d <dig>  lacp consistently outperforms the other two methods. it is important to note, however, that on d <dig>  lacp experiences a rapid precision drop after recall =  <dig> , and that on d <dig>  lacp is inferior to most methods.

d

1
d

2
d

3
d

4
40
11
202
233
note: the best values for each column are formatted in bold italics.

discussion
the primary advantage of the lacp method is its short execution times, a feature that is highly desirable when dealing with the large data sets involved in medical informatics. the performance of the lacp method can be interpreted by studying the structure of the datasets d <dig>  -d <dig>  datasets d <dig>  d <dig>  and d <dig> have higher numbers of terms per concept compared to dataset d <dig> . thus, d <dig>  d <dig>  and d <dig> have a higher number of records that have the same cuis and have approximately common prefixes. this allows the lacp algorithm to outperform other more complicated well-known methods on d <dig>  d <dig>  and d <dig> 

however, the lacp method performed poorly on d <dig>  this is due to the large number of concepts with similar terms. as shown in table  <dig>  five terms share a 146-character-long common prefix, for example. by design, such terms are evaluated by lacp as very similar, which in fact is incorrect. large numbers of such similarly spelled umls terms with different identifiers leave no chance for the lacp algorithm to succeed in these contexts.
d

3


we note that the current online spell checker is a prototype. it has not been optimized for speed nor is it intended to compete with the well-known google instant search  <cit> , which displays search predictions as the user types a query. instead, our goal is to create a spell checker specifically for use with biomedical terminologies. the remarkable difference between the excellent performance of lacp on datasets d <dig>  d <dig>  and d <dig> and its disappointing performance on d <dig> indicates that approximate string matching methods exhibit a certain degree of domain dependence. in fact, as detailed in an extensive research report by rudniy  <cit> , domain dependence has been shown to be a common phenomenon.

CONCLUSIONS
lacp is a novel method we have developed for computing approximate string similarities based on assessing the length of approximately common string prefixes. the algorithm implements a normalization technique by dividing the length of the approximately common prefix by the average length of the pair of strings. lacp performed better than a number of well-known string similarity algorithms on three out of four datasets and demonstrated the shortest execution times on all four. for the average precision measure, lacp achieved the highest values of  <dig>  on dataset d <dig> and  <dig>  on dataset d <dig>  on d <dig>  lacp was second best, with an average precision of  <dig> . our method had the best values of maximum f <dig> on three datasets:  <dig>  on d <dig>   <dig>  on d <dig>  and  <dig>  on d <dig>  however, lacp experienced a drop in performance on dataset d <dig>  in terms of execution time, lacp was on average two times faster than the jaccard method, which achieved the second best times.

the lacp method demonstrated superior performance on certain types of biomedical datasets though its productivity has to be determined for other corpora. another common limitation of the approximate string matching methods lies in the inability to determine that differently spelled synonyms correspond to the same concept. for such cases, either semantic methods or expert insight are required.

in future work, we will attempt to identify the cause and solve the problem of performance variability due to differences in dataset characteristics. another branch of future research consists of investigating the best value for parameter α. the ultimate—though difficult—goal is to develop an approximate string matching method that recognizes and adapts to the distinctive characteristics of each dataset.

abbreviations
snomed ct: snomed clinical terms; umls: unified medical language system; asm: approximate string matching; lacp: longest approximately common prefix; tfidf: term frequency/inverse document frequency; cui: concept unique identifier.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
ar, ms, and jg participated in the algorithm design and evaluation, and drafted the manuscript. all authors read and approved the final manuscript.

