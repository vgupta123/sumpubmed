BACKGROUND
protein-protein interaction  plays essential roles in cellular functions. with the emerging new paradigm of systems biology, much of the research focus has been shifted from studying individual proteins and their functions to studying how they interact with each other, forming biological networks in fulfilling cellular processes. great advancements have been witnessed in experimental technologies, such as yeast two-hybrid  and coimmunoprecipitation , for detecting ppis  <cit> . still, the cost, time and other limitations associated with the current experimental methods have motivated development of computational methods for predicting ppis.

over the past few years, many computational methods have been developed for ppi prediction, utilizing information from various sources at different levels, from primary sequences  <cit> , to molecular structures  <cit> , to evolutionary profiles  <cit> , to microarray expression data  <cit> , to networks information  <cit> . in general, more sensitive prediction tends to require extensive information, e.g., phylogenetic information, and more specific prediction tends to require more detailed information, e.g., the structural information. the 3d structure of proteins plays an important role in ppi; as proteins interact with one another, their structures need to match, i.e. the interacting domains  must be folded into certain conformations so that they attract each other . for example, the interface at one protein that has a concave shape tends to require a convex shape for its interacting partner. various constraints at the interface may in turn impose constraints on the amino acid composition at the interfaces. therefore, identifying protein interaction at the domain level can serve as an important intermediate step toward an effective approach for prediction of ppi  <cit> , even though the inference from pairwise ddi to ppi can be complicated due to factors like the presence of multi-domains, e.g., combination of domains may block interactions that are otherwise suggested if solely based on individual domain interactions  <cit> .

general domain identification itself is a highly non-trivial task. sequence patterns based on such compositions typically lack enough uniqueness to be solely relied upon for domain identification. in fact, multiple sequence alignments of proteins that are known to contain the same domain show variations in sequence composition. hidden markov models  are among the most successful efforts to capture the commonalities of a given domain while allowing variations. a collection of hmms covering many common protein domains and families is available in the pfam database  <cit> .

for interface domains, the interaction sites impose strong constraints and therefore play a key role in identifying the domains. however, interaction site information is not readily available for many proteins and the dataset of ppis that have been resolved using crystallography remains relatively small. to tackle this issue, friedrich and coworkers developed a new method, called interaction profile hidden markov model , which modifies the ordinary profile hidden markov model  by adding to the model architecture new states explicitly representing residues on the interface  <cit> . this leads to improved accuracy in interaction domain identification.

despite the improvement, the membership of domain families is still established at the best via probabilistic modeling and therefore false positives and negatives are not uncommon. as shown in detail in section  <dig>  the error rates for identification of individual domains will multiply when used for predicting domain-domain interactions . more seriously, although supported by other evidences such as domain modularity of proteins and shared ddis among ppis, in most cases experimental verification in support of the ddi-ppi correspondence is still missing  <cit> .

in this work, we propose a new computational method to address these issues, in particular by extracting and selecting features encoded in the interaction domain profiles for ddi prediction. the method is based on a framework first proposed by jaakkola et.al., which combines generative models and discriminative classifiers in tandem  <cit> . in our case, the generative model is the iphmm representing domains that, based on the structural information, are known to be involved in ppi . once an iphmm is trained, it can be used to predict domains and interaction sites for proteins with only the sequential information as input. to mitigate the above-mentioned multiplying effect of false domain prediction on ddi prediction, the learning of interactions is deferred to a discriminative classifier, which is a support vector machine  in our case.

this two-stage framework allows us to do sophisticated feature extraction and selection from the domain profiles. for feature extraction, we represent proteins by vectors that are composed of the fisher scores, which are defined as the gradient, with respect to the model parameters, of the posterior probability when the proteins are aligned with the iphmm. because of the large number of parameters for a typical iphmm and the fact that not all parameters play equal roles in determining a protein's membership to the domain family described by the model, feature selection is necessary and essential. feature selection in this work is based on the singular value decomposition  technique. protein pairs are represented by concatenated feature vectors, which are used to train the svm classifier. the method is tested by leave-one-out  cross validation experiments with a set of interacting protein pairs adopted from the 3did database  <cit> , and the prediction accuracy is at about  <dig>  measured by roc score with significantly higher confidence as compared with a similar method.

 <dig> methods
in this section, we first introduce the interaction profile hidden markov models that are used to capture domain properties including the structural information. we then describe how to calculate the fisher score vector for a protein whose sequence is aligned to an iphmm. we end the section with the feature selection scheme based on svd.

the overall design is illustrated in the flow diagram of figure  <dig>  panel a shows two domain families, pfama and pfamb, with member proteins retrieved from the pfam database  <cit> . some member proteins in pfama and pfamb are known to be interacting based on structural information collected from the 3did database  <cit> . these proteins are paired as indicated by the double arrows, and are used as training  examples to construct iphmms for pfama and pfamb respectively. as shown in panel b, the architecture of the iphmm contains new states marked as mi to describe residues that are on the interacting interface. all member proteins in each family are then aligned to their iphmm and the fisher scores are computed as described in subsection fisher scores. as a result, proteins are now represented as fisher score vectors, and protein pairs are represented by concatenating their individual vectors, as shown in panel c. interacting pairs  and non-interacting pairs  are prepared for training and testing the svm, as described in subsection preparation of training and testing sets. the negative examples are mostly random sequences generated from the model consensus as described in subsection negative examples, except for one case where homologous family members are known to be negative, as studied in subsection case study of real negative examples. in the next step, as shown in panel d, svd is carried out for positive training examples, and the subspace svdpo, spanned by the positive singular vectors that make up for 80% of the variance in the training set, are used for feature selection. in panel e, after projecting all examples onto svdpo we obtain a training dataset made of vectors of reduced dimension with selected features. this dataset is used to train a support vector machine. through the pipeline, a sequence pair, shown in panel a as query, along with some negative examples, are reserved for testing, using the trained svm . in panel f, the distance of each test example to the hyperplane of the svm is plotted in a histogram, and the z-score of the query pair is used to predict a potential interaction. all these steps are detailed in the following subsections.

interaction profile hidden markov models
friedrich et.al.  <cit>  proposed a model in which the interacting sites within protein domains are modeled by a modified phmm, the iphmm. the model architecture takes into account both structural information and sequence data. every iphmm is, like phmms, a probabilistic representation of a protein domain family. the architecture of the iphmm follows the same restrictions and connectivity of the hmmer architecture  <cit> , with one important exception: the match states of the classical phmm are split into a non-interacting  and an interacting match state , as shown in figure  <dig>  the new match state is provided with the same properties of a match state in the ordinary profile hidden markov model architecture, i.e. these interacting match states are able to emit all amino acid symbols with probabilities, which are parameters to be fixed according to the training examples.

the parameters of an iphmm are estimated using maximum likelihood based on a multiple sequence alignment of the member proteins in the domain family, incorporating the annotation on their binding sites - all residue positions are labeled with the corresponding interaction status . these trained iphmms thus encode relevant statistical information about the domain, especially pertaining to the interaction with other domains. although the iphmm was proposed by friedrich et.al. for better identifying interacting domains from protein sequences, we show next that by extracting and selecting features from these models properly, we can build a powerful tool for ppi prediction.

negative examples
as explained in the introduction, our method as a discriminative approach requires negative interaction examples, i.e. pairs of protein sequences whose interaction has been deemed very unlikely. but it is a well known problem that negative examples for protein-protein interaction prediction are hard to attain  <cit> . given the current state of the art, when experimental methods like y2h do not report an interaction between two proteins, it does not mean that the interaction is ruled out. in other words, the absence of a positive signal does not imply a negative signal. for this reason the confirmed negative examples in interacting domain families are very scarce. for our method, this situation is even worse, because we are looking for pairs of proteins that share an interacting interface, but even so, it is known that they do not interact - we do have one such case where a real negative example is known, and a detailed case study is given in subsection case study of real negative examples.

to mitigate this problem, random sequences have been used as surrogate for negative examples, as reported in aloy and russell  <cit> , which also uses protein sequences and 3d structure information for ppi prediction and will serve as a benchmark to compare with. in generating random sequences, we try to simulate the family membership of each sequence to its corresponding domain family. specifically, our random generator  spawns sequences based on the consensus from the multiple sequence alignment of the training examples. then, on top of the consensus sequence, we identify those positions where interacting residues are annotated in any sequence from the family, and overwrite that residue with a random amino acid . these sequences are aligned to their corresponding iphmm, fisher vectors are extracted from the alignment, and their concatenated vectors produce our negative examples .

fisher scores
in this subsection we describe how to extract features from domain profiles, i.e. iphmms. deriving and using fisher scores from iphmms was first proposed in  <cit>  and is reviewed here for the sake of being self-contained. hidden markov models, including iphmms, are probabilistic models that capture the collective features and characteristics of members in a family, in our case, proteins that contain the same domain. the typical tasks with a trained phmm in general  are to calculate a) the probability p of an unknown protein x to belong to the family, and b) how it aligns best with other members in the family  <cit> . while p is useful in telling if x is a member, the prediction based on this single value is susceptible to being false.

the false rate can be compounded  if we use this method for predicting ppis, as previously pointed out  <cit> . for example, domains a and b are known to be interacting with each other, and are profiled as iphmms respectively. if protein x is shown to possess domain a and protein y is shown to possess domain b, then we would predict that proteins x and y interact with each other. if the accuracy for predicting the membership of x and y is 60%, then the accuracy for predicting the x − y interaction will fall to 60% × 60% = 36%.

to prevent such a compounding effect of misclassification, we propose to use another classifier - a svm - in tandem with the iphmm. to do so, instead of using the probability p  directly, we extract from this probability features that characterize both the domain profiles and the protein x. specifically, we calculate the so called fisher scores, defined as the derivatives of the probability for the query sequence x given the model θ with respect to particular parameters of the model. the use of fisher scores in svms was first proposed by jaakkola  <cit>  in the context of detection of remote protein homologs, and was later adopted for other applications in bioinformatics, including discriminating signal peptides from proteins with a single transmembrane domain  <cit> .

the benefits of using fisher scores are many. not only can they extract subtle and sensitive features as derivatives with respect to the model parameters, but also they allow for proteins with various lengths to be represented as vectors of the same length, where the vector components are derivatives and the vector dimension is determined by the number of model parameters that are deemed relevant and informative for the classification tasks. having proteins represented as vectors of the same length is necessary for the svm. feature selection is natively supported because one can choose which parameters in the model to use for calculating the fisher score. in this work we will focus on the emission probabilities of the match states.

if the probability of emitting amino acid x˜ from state s˜ is named θx¯,s¯, the fisher score of the model with respect to θx¯,s¯ is therefore defined as

  ∂∂θx˜,s˜logp=∈θx˜,s˜−∈ 

where ∈=∑x'∈ and the summation runs over the  <dig> different amino acids. the derivation of  is detailed in  <cit> . in this formula, ∈ can be seen as the expected posterior probability of visiting state s˜ and generating residue x˜ from that state. this expected value can be calculated, for any state s˜ and for any emitted amino acid x˜, from the posterior decoding matrix, which can be found efficiently using the forward and backward algorithms  <cit> . the literature denotes ∈ and ∈ as the sufficient statistics for the parameter θx¯,s¯ in the model. for this reason we say that the sufficient statistics of the entire model are embedded in the fisher scores.

another type of fisher score, the so called constrained fisher score, was shown in  <cit>  to behave very well in ppi prediction. in , p  is the sum of contributions from all paths aligning x to the model. one could rather focus on the probability of the best path s that aligns x to the model, p , and calculate the derivative of such p with respect to θ. this would give the constrained fisher score, formally defined as:

  ∂∂θx˜,s˜logp. 

feature selection with svd
as discussed above, the fisher score representation allows for choosing specific model parameters deemed relevant and informative in extracting features from the iphmm. intuitively, one feature selection scheme is that two sets of parameters are selected for comparison: emissions from the  match states  and emissions from the interacting match states . while this feature selection is simple and useful, such a variety of attributes may not be feasible. note that the average domain length in the datasets used in this study is  <dig> residues, leading to the same number of non-interacting match states  and interacting match states . multiplying this by  <dig>  the number of amino acids emitted from each state, the fisher score vector for a protein aligned against an iphmm has a dimension of  <dig>   <dig> on average. for ddi prediction, the fisher score vectors for a protein pair are concatenated, leading to a vector of dimension  <dig>   <dig>  feature selection using just the interacting match states will reduce the dimensions by half to  <dig>   <dig>  this number still presents a challenge even for classifiers like svms which are known to be superior than others in handling a large number of attributes  <cit> .

to further reduce the dimensions and select the most relevant and informative features, we use the singular value decomposition . svd, and pca , have been widely used in feature selection and dimensionality reduction. the underlying assumption is that the most relevant features may be composed as a linear combination of the given attributes, and the relevance of a feature is indicated by the corresponding eigenvalue for pca or singular value for svd: the higher the value the more relevant. therefore, it is convenient to rank the singular values and choose the features with top singular values.

note that svd is chosen over pca as the method for feature selection in this work primarily due to its lower computational cost: given n vectors of length  <dig>   <dig>  pca requires solving an eigenvalue problem for a  <dig>   <dig> ×  <dig>   <dig> matrix, but for svd the eigenvalue problem is solved for a n ×  <dig>   <dig> matrix, with n being the number of training examples, which is typically less than  <dig>  another difference between pca and svd is that in pca the covariance matrix has both its columns and rows corresponding to the original attributes whereas in svd the matrix has its columns corresponding to the original attributes and its rows corresponding to training examples . this leads to a slightly different interpretation for the combinational coefficients in the two methods: in svd the rows  play somehow more explicit roles in determining the combinational coefficients and thus how the features are composed from the given attributes; in pca the training proteins would be "summed up" in calculating the covariance between attributes. in our testing, however, this does not give rise to any noticeable difference in the performance of the overall approach.

let d = {po; ne} be the training dataset containing positive and negative examples, a matrix of size d × l, where d = p + n is the number of train vectors, p is the number of positive examples, and n the number of negative examples; l is the length of each vector. for our dataset , p is usually around  <dig>  and we make n = p. experiments showed that making n >p does not change the prediction results.

we could decide to extract the singular value decomposition from the entire dataset d. but interestingly enough, it was noticed that applying svd only on the positive dataset is sufficient. and this is reasonable, because the informative signals in our data come from the sequence pairs whose 3d structure  is known. the negative examples contain some of this information, plus random noise. including the negative examples in the svd would be wasteful, because the spectrum of the noise is flat, hence uninformative.

let us formally define the singular value decomposition. the svd of po attempts to find two sets of orthonormal vectors, {v^ <dig> v^ <dig>  …, v^r} and {u^ <dig> u^ <dig>  …, u^r}, where the former is an orthonormal basis for the row space of po, and the latter is an orthonormal basis for the column space of po. also, r is the rank of po, v^i is of length l and u^i is of length p. the svd defines a very special relation between the v^i and the u^i vectors. namely, for  <dig> ≤ i ≤ r, po·v^i = si·u^i, where si is defined as the "i − th singular value". therefore, if vpo =  and upo = , and s is a diagonal matrix that contains the singular values in descending order, then po·vpo = upo·s. since vpo is orthonormal, the positive dataset can be factorized as po = upo·s·vpot, where upo is of size p × r and vpo is of size l × r. the v^i vectors, which constitute a basis for the row space of po, provide a means to reduce the dimensionality of each example vector in the following way: if we create a new matrix vpored  of size l × k, where k <r , we could project any example vector e→ of length l onto vpored by simply doing e→red=t⋅e→. clearly, e→red is a column vector of length k. in other words, e→red is the dimensionality reduced version of e→. these k new dimensions can be thought of as the features of each protein-protein interaction vector .

 <dig> data
in this section, we describe the data that are used in our study and how the training and testing sets are prepared.

the database of 3d interacting domains 
the training and testing examples in this study are obtained from a relational database of 3d interacting domains   <cit>  that contains a collection of   <dig>   <dig> domain-domain interactions  in protein complexes with available information of three-dimensional structures via the protein data bank. the 3did criterion for physical interactions requires at least five contacts  between the two domains that have been detected, with a resolution of  <dig> Å rmsd. based on the structural information for the protein complexes at the atomic level provided by 3did, we are able to determine which amino acids actually take part on the interaction and to construct an iphmm. each domain in the 3did is characterized by training the model on all sequences that contain the domain using the procedure described by  <cit> . consequently, each ddi is now modeled by the two corresponding iphmms for the two domains.

preparation of training and testing sets
for training and testing our method, we used a subset of ddis extracted from 3did. each ddi is a family with interacting domains, dom. a and dom. b, whose members are i pairs of interacting proteins that have been found to physically interact, or in other words, there are i protein complexes with interacting proteins through the domain-domain interface and with distinct pdbid in the protein data bank. we used ddis with i between  <dig> and  <dig> and where the domain length  is smaller than  <dig>  these criteria look for families rich enough in information content for ensuring statistical robustness, but are not too big to avoid prohibitive processing times. with these filtering parameters,  <dig> ddis were selected. for every protein sequence that is part of a single ddi, the 3did database provides information to build binary vectors with the same length of the proteins and where the 1's indicate interacting amino acids. these vectors and the profile hidden markov models of each domain, extracted from pfam  <cit> , are used to create iphmms for both domains.

to illustrate how we prepare the training and testing datasets, let us take a single ddi as an example. first, all the protein sequences in the ddi are aligned to their corresponding iphmm . alignments are obtained by posterior decoding . as explained in subsection fisher scores, from such alignments we can calculate the fisher vectors. this algorithm can be efficiently executed through dynamic programming. as a result, each protein sequence can be numerically represented by the fisher score vector. positive examples are constructed by concatenating the fisher vectors of interacting protein pairs . negative examples are constructed as explained in subsection negative examples. the svd of the positive dataset is used to do feature selection on both, the positive and the negative train vectors . now, with the complete training dataset reduced to rk, we can train a support vector machine . that concludes training. the last three stages of this pipeline give the name to our model: fisher+svd+svm. we will use this name in the remainder of the paper.

for testing, a leave-one-out  strategy is followed. this guarantees that each positive example gets to be tested  once. in figure  <dig>  one interacting pair, reserved for testing , is used as a hypothetical query protein pair. the two sequences are aligned to their corresponding iphmms. for each ddi,  <dig> negative  examples are generated for testing. these sequences are also aligned to the iphmms. fisher vectors are calculated, the entire test dataset is projected on svdpo , and finally each length k test example is classified using the svm that was previously trained . the svm provides the distance to the hyperplane for each test vector. these distances are used to calculate a histogram including the random sequences and the positive test example . an accurate classification would place this example far away from the negatives, with a high z-score. therefore, z-scores can be used as a means of performance evaluation. we also report area under the roc  curve results, calculated from the sorted list of distances to the hyperplane.

 <dig> 
RESULTS
in this study, training and testing is implemented based on matlab built-in functions for svms, namely svmtrain and svmclassify. polynomial kernel with default parameters  was used. the number of iterative training and testing stages that are run per ddi is equal to the number of positive examples in the ddi. note that each ddi is independently trained and tested. for feature selection, we used as many components of the singular value decomposition as needed to account for 80% of the variance in the positive dataset. this strategy showed outstanding experimental results, not only in speeding up the learning, but also in de-noising the information content of the fisher vectors to improve prediction results.

performance evaluated by roc score and z-score
we tested our method on a dataset of  <dig> domain-domain interactions extracted from 3did, prepared for training and testing as described in subsection preparation of training and testing sets. four different types of fisher vectors can be created, as explained in subsection fisher scores: using the formula for the unconstrained fisher score  or the formula for the constrained score , and in each of those cases, taking the derivative with respect to non-interacting match states or taking it with respect to interacting match states. note that in each iteration our test set consists of one positive example and  <dig> negative examples. a perfect prediction from the svm would assign to the positive example a distance to the hyperplane higher than the distances for all the negatives. to measure the actual performance, the receiver operating characteristic  sore is used. all testing examples are sorted in a descending order by their predicted distance to the hyperplane, and a moving threshold is used to scan through the sorted examples to make prediction: higher than the threshold is predicted as positive and others as negative. the roc score is the normalized area under the curve  where the number of true positives is plotted as a function of the number of false positives. roc score thus defined has a value ranged from  <dig> to  <dig>  with the value  <dig> corresponding to the perfect prediction mentioned above, the value  <dig> to the worst performance, and a value  <dig>  to a random classifier. in this study, we further evaluate the performance by z-score. the predicted distances are used to produce a histogram, as schematically shown in figure  <dig>  panel f, where the horizontal axis is the predicted distance and the vertical axis is the number of examples receiving a specific predicted distance. the z-score of the positive test example is calculated as

 z=dx−d¯σ 

where dx is the predicted distance for example x, d¯ is the mean for all predicted distances and σ is the standard deviation. therefore, a higher z-score indicates greater separation from the average, and hence higher confidence about the prediction.

unlike what is reported in  <cit> , here we notice that the unconstrained fisher score outperforms its constrained counterpart. we reason that this is caused by the way how the negative examples are generated . these negative examples are highly similar to the positive examples in positions where none of the three dimensional structures show residue-residue contacts. these positions are more numerous than interacting positions, therefore the similarity between positive and negative pairs can be high. specifically, if the fisher scores are calculated only along the most probable paths  for positives and negatives, the family membership could push both types towards a similar ∂∂θx¯,s¯logp, making their differentiation difficult. on the other hand, when using the unconstrained fisher scores, the formula ∂∂θx¯,s¯logp takes into account all the possible paths that align the sequences to their model, therefore the regions where the negative examples take on random residues can make a difference. this phenomenon is illustrated with a real example in the study of feature selection, presented in figure  <dig> and figure  <dig> 

comparison with interprets on signaling, cytokines-receptors and peptidases-inhibitors families
we compared our method to an existing method using similar information for ppi prediction. in  <cit> , a method called interprets   <cit>  is developed. it takes a pair of query sequences and searches for homologues in a database of interacting domains of known three-dimensional complex structures. if one or more complexes are found, the query sequences are aligned to the closest homologues of known structure. given the residue to residue matching produced by the alignment, and after identifying the residues that make atomic contacts in the complex of known structure, interprets checks whether the query protein sequences preserve these interactions by means of empirical potentials. in other words, the pair of query sequences is scored for how well they preserve the atomic contacts at the interaction interface. statistical significance of the potential interaction is estimated based on a background of random sequences. another more recent technique in the same spirit of interprets is 3d-partner, although it also uses as input some other information such as contact residue interaction scores based on steric hydrogen bonds and electrostatic interactions  <cit> .

aloy and russell  <cit>  present prediction results running interprets on domain-domain interaction families that can be grouped into three broad categories: signaling, cytokines-receptors and peptidases-inhibitors families. we compared side by side the results of interprets with the predictions that our method makes on the same dataset. the comparisons are summarized in table  <dig>  interprets aligns the two query sequences to the closest homologues of known structure, and for each alignment it produces a z-score based on how the empirical potentials of the alignment compare with the potentials from a background of  <dig>   <dig> random sequences. since the homologues of known structure can be many, interprets calculates z-scores for each one of them and shows them in descending order . the interprets column in table  <dig> shows the averaged best z-score over a family, that is, for each predicted complex structure, we only use the best score reported by interprets. the table also shows the number of complexes found in each domain-domain family. according to  <cit> , a z-score ≥  <dig>  indicates a significance of the prediction of 99%, z-score ≥  <dig>  indicates a significance of 90% and when z-score < <dig>  the two proteins are predicted not to interact in the same way as the known complex structure.

on the other hand, when the fisher+svd+svm approach is used to predict the interaction of a pair of query sequences, all the complex structures homologous to the sequences are used to build the model, and one single prediction is produced. the results shown in table  <dig> for fisher+svd+svm correspond to the unconstrained formula derived with respect to non-interacting match states, as this is the type of vector that consistently performs best. as before, a loo strategy is followed for training and testing each family. for fairness, in these families we are also using, as interprets,  <dig>   <dig> random sequences in testing.

case study of real negative examples
we also tested our method, along with interprets, on protein pairs that contain the interacting domains but without actual interaction, thus called real negative examples. we mentioned before that this type of examples are difficult to find. in aloy and russell  <cit> , however, one instance of such pairs is reported and studied: the fibroblast growth factors  and their homologues, the fhfs. fgfs mediate cell growth, differentiation, migration, and morphogenesis by binding to the extracellular domain of cell surface receptors, triggering receptor tyrosine phosphorylation and signal transduction  <cit> . fgf homologous factors , however, do not bind to fgf receptors, instead they are associated with intracellular mitogen-activated protein  kinases.

we tested interprets and our method with one fhf protein, the fhf1b , against all the fgf receptors found in the fgf-ig domain-domain interaction family. figure  <dig> shows the molecular structure of the six complexes found in this family , along with the fhf protein . red and green balls in the structures indicate the interacting positions in the fgf  and in its receptor . the msas in figure  <dig> correspond to the sequences in the fgf family  and in the receptors family . the top msa is shown up to position  <dig> for space constraints. interacting positions have been marked with green and red boxes. the fhf1b sequence also appears as it is aligned to the fgf family.

for each structure in the fgf-ig family, we first run interprets and fisher+svd+svm on the positive example , and then we replace the fgf sequence with the fhf1b sequence, and leave the same receptor. this gives us six real negative pairs, on which we run both methods again. the results are listed in table  <dig>  it is expected for the z-score of the positive example to be greater than the negative, and this is being the case in all structures for interprets, and in all the structures but pdbid 2fdb for fisher+svd+svm. however, the positive signal is somewhat weak and marginal in the interprets results. on the other hand, for the fisher+svd+svm scheme, not only the positive example receives very strong z-score, but also the real negative is pushed to the very opposite side of the distribution, showing a strong ability for differentiation.

fibroblast homolog factor fhf1b was tested for interaction against all the receptors in domain-domain family fgf-ig

applicability of the fisher+svd+svm method
we finish this section discussing the relation between the proposed computational method and the biological systems that concern the problem of ddi. the intuition driving this work is that structural information plays a key role in ddi prediction, and the success and limitations of any computational method is affected by how effectively we can extract, transfer and represent such information so that it can be better learned by a classifier to make more accurate predictions. so the success of our method comes from the following: a) leveraging the iphmm to profile protein domains with structural input; b) using the fisher scores to extract features that are most relevant to the profile; c) doing de-noising and feature selection through svd. the limitations primarily come from two sources: 1) the reliability of the training data ; 2) assumption made by svd that useful features can be obtained from linear combination of attributes, although in real world scenarios this can only be an approximation. like any other computational method, this algorithm can complement the experimental approaches, which are typically more expensive and time-consuming. particularly, due to its use of structural information regarding interface residues, the method can be used as a simulation tool in protein engineering, for example, to predict if and how the interaction of two proteins may change with mutations at certain residues and narrow the experiments to the most promising mutants.

 <dig> 
CONCLUSIONS
in this work, we developed a computational method for predicting domain-domain interaction based on domain profiles. the method adopts a framework that is capable of combining in tandem the interaction profile hidden markov models for domains and the support vector machines for domain-domain interaction prediction. the framework enables feature extraction and feature selection between the two tandem stages. by leveraging the interaction profile hidden markov models trained on interacting protein domains whose structure is known, we are able to transfer the domain structural information to proteins that lack such information. we showed that the fisher scores computed from alignments of protein sequences to the interaction profile hidden markov models for domains offer more domain specific features characterizing protein sequences involved in interaction interfaces. the effect is more pronounced for non-interacting match states in iphmms, offering a powerful alternative when interacting residue information is not readily available. we also demonstrated that feature selection can play a key role in enhancing the signal-noise ratio for the next stage learning by the support vector machine. as a result of applying these techniques, our predictor is able to outperform another well known method based on the same sources of information. it is believed that by integrating the feature selection mechanisms with the learning process within a semi-supervised learning framework the method has the potential to be efficiently applied to genome wide prediction even with limited training data.

authors' contributions
ll conceived the initial ideas and supervised all aspects of the work. ajg designed and implemented the algorithms and the experiments. both authors wrote and approved the manuscript.

