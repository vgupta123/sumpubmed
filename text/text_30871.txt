BACKGROUND
protein-protein interactions are central to all the biological processes and structural scaffolds in living organisms. a protein is characterized by its 3-dimensional structure; and a biological process in which it takes part, for instance, sensing of light and transmitting that signal to the brain, is characterized by a pathway of interacting proteins. protein-protein interactions  play a key role in the functioning of the cells enabling signalling and metabolic pathways and facilitating structural scaffolds in organisms  <cit> . it has been suggested that an interaction network of human proteins can be used to understand disease mechanisms  <cit>  and thereby would be useful in drug discovery. several high throughput methods such as yeast 2-hybrid  and mass spectrometry methods help determine protein interactions. however these methods suffer from high false positive rates, and many protein interaction predictions supported by one method are not supported by another. for instance around 70% of the reported interactions identified through y2h in yeast estimated to be false positives  <cit>  and that only around 3% of the protein interactions reported in yeast are supported by more than one high throughput method  <cit> . in complex organisms like human, applying high throughput methods to test every possible protein pair  would be very expensive in terms of cost and effort. it is estimated that there are anywhere between  <dig> - <dig>  distinct protein-protein interactions in human; however only ~ <dig> interactions are known or suspected today  as per the human protein reference database  <cit> . computational methods are therefore necessary to complete the interactome expeditiously.

building on several decades of study of individual proteins relentlessly by biologists and on the advances in high throughput technologies, today it is possible to attempt prediction of protein-protein interactions based on indirect features, and algorithms have recently begun emerging, in particular methods to develop machine-learning-based computational models for protein interaction prediction. bayesian classifier  <cit> , random forest  <cit> , logistic regression  <cit> , support vector machines  <cit>  and decision tree  <cit>  have been applied for protein-protein interaction  prediction. they apply the available evidence of known interacting proteins  with the indirect information such as gene ontology annotation, gene expression correlation, sequence homology etc  to predict ppi. qi et. al  <cit>  and lin et. al  <cit>  have both shown that random forest performs the best among the various classifiers they had evaluated. qi et. al  <cit>  has suggested that the randomization and ensemble strategy applied in random forest enable them to handle noise better.

active machine learning
experimentally verified protein interactions are costly and difficult to obtain; therefore, strategies which minimize the amount of labelled data required in the supervised learning task would be useful. active learning is a type of supervised learning wherein the system selects the data points whose labels would be most informative in the learning task - i.e. selects which protein-protein interactions to validate or refute in the laboratory. instead of learning from a large pool of labelled data, the algorithm starts by processing all the unlabeled data, and asking for labels of select few data points. an oracle  returns the labels  for these data points and they are employed by the algorithm to update the classification function.

common strategies employed for performing data selection in active learning  <cit>  are density based, where a set of data points from dense regions are selected for labelling  <cit> ; or uncertainty based, where data points with maximum confusion or uncertainty with current classifier are selected  <cit> ; or representative based, in which data points most representative of the data set are selected  <cit> ; or estimated-error reduction based, where data points which offer maximal estimated error reduction to the classifier are selected  <cit> ; or ensemble based in which multiple criteria are employed and typically outperform single-strategy methods  <cit> . for instance, some active learning approaches combine density-based and uncertainty based strategies to achieve better performance. in general, supervised machine learning attempts to minimize a regularized loss  using an l0-norm , l <dig> , or l <dig>  and a decision-surface complexity measure to avoid over-training, i.e. the learned predictor  f* may be characterized as:  

in the above,  are the values of the features for instance i, yi is the real label , f() is the predicted label , d is the training data, and f is the set of possible predictor functions . for non-numeric labels 0- <dig> loss is typical. active learning builds on this criterion, by attempting to select the next  from the universe of possible instances such that if we knew its true label yi+ <dig> we could maximally improve our estimate of the best f*.

clustering is a common pre-processing step to select the representative data points. clustering techniques applied for active learning include k-means  <cit>  and k-medoids  <cit>  algorithms. uncertainty strategies include selecting data points closest to decision boundary of the classifier as in  <cit> , where the data points closest to the decision hyperplane of the svm classifier are selected for labelling. roy and mccallum  <cit>  apply active learning with a naive bayes classifier. here the samples  which on labelling would offer maximum reduction in expected error are selected. lewis and gale  <cit>  train a naive bayes classifier in combination with a logistic regression with an initial set of labelled samples. in every iteration unlabelled samples which have maximum uncertainty in class assignment based on the current classifier are selected for labelling. debarr and wechsler  <cit>  perform uncertainty sampling using a random forest classifier for spam detection. samples which are assigned a close-to- <dig>  probability of being spam by the current random forest classifier are selected for labelling in the next iteration. davy and luz  <cit>  perform history-based uncertainty sampling with a committee of recently-trained classifiers. in each current iteration, samples which have maximum disagreement among the classifiers in the committee are selected. this method was shown to perform better for the problem of text categorization in comparison to active learning which just uses the classifier built with current set of labelled data to do uncertainty sampling  <cit> . several approaches combine density based sampling and uncertainty based sampling to improve performance  <cit> . these methods select samples which are closer to the decision boundary and are good cluster representatives and therefore also sample high-density regions.

methods
datasets and feature descriptors
we use the dataset created and made available by qi et. al for evaluation of active learning algorithms developed  <cit> . at the time of compilation of the data,  <dig> pairs of proteins were known to interact; these pairs are referred to as positive pairs. a set of  <dig>  pairs not overlapping with the positive pairs were generated randomly. these pairs, referred to as random pairs are considered to be non-interacting pairs, as the probability of a randomly generated pair to be interacting is less than  <dig> in  <dig>  <cit> . of the newly discovered interactions, only  <dig> are found among the  <dig>  randomly generated pairs.

prediction of ppis is setup as a binary classification task: each feature vector corresponds to a pair of proteins and it is classified as interacting or non-interacting. the feature vectors were computed by qi et. al for both the interacting pairs and random pairs  <cit> . the vectors have  <dig> dimensions and contain features corresponding to gene ontology  cell component , go molecular function , go biological process , co-occurrence in tissue , gene expression , sequence similarity , homology based  and domain interaction , where the numbers in brackets correspond to the number of elements contributed by the feature type to the feature vector. the go features measure similarity of two genes based on the similarity between the terms they share in the gene ontology database. three go features were generated one each for the biological process, molecular function and cell component respectively. the  <dig> gene expression features were computed as the correlation coefficients of the protein pair using sixteen gene expression datasets in ncbi gene expression omnibus database. the 'tissue feature' is a binary feature indicating whether the two proteins are expressed in the same tissue. sequence similarity feature was obtained by measuring the blastp sequence alignment e-value for the protein pair. in 'domain interaction feature' the interaction probability of a protein pair is measured based on the interaction probability of the domains present in the two proteins. the 'homology ppi feature' is estimated based on whether proteins homologous to the given pair of human proteins, interact in other species  or not. the details of qi et al's compilation of these features may be found in their supplementary website  <cit> .

not all types of features are available for each protein-pair. in other words, for several protein pairs, the feature vectors contain several missing values . some pairs have feature vectors with 80% missing values , while some pairs have values for all the feature types . in order to maintain balance of feature coverage between positive and random pairs , a homogenous subset of data has been created such that every pair has more than 80% feature-coverage; . this subset is used in this algorithm development and evaluation. this homogenous subset has  <dig>  protein pairs in total.  <dig>  protein pairs were selected randomly from this for training and another  <dig>  for testing.

further, the positive and negative pairs are combined in a ratio of 20%-80% .

evaluation metrics
precision is measured as the fraction of correctly predicted protein interactions among all the pairs predicted by the classifier to be interacting. recall is the fraction of the interacting protein pairs which the classifier is able to correctly identify as interacting pairs. f-score is the harmonic mean of precision and recall. f-score measures the accuracy of the method by combining both precision and recall values. hence it can be used as the measure to compare the accuracy of the methods.

random forest classifier
a random forest  trains a set of decision trees on subsets of features. a majority vote of the decision trees is taken as the label of each test point. during the construction of a decision tree, for splitting each node, a subset of n out of the total n features is selected randomly, and the feature with maximum information gain out of the n is used to split the node. in this work, a random forest with  <dig> decision trees is constructed; to split the nodes, a subset of  <dig> features is selected from the total of  <dig>  of the  <dig> selected features, the feature offering maximum information gain is used to split that node. random tree implementation of the weka package was used to create the decision trees in the random forest  <cit> . minimum number of samples in each leaf node was set to be  <dig> 

active learning data selection strategies
to test the active learning component, all data is taken to be unlabeled data, and the active learning method asks for labels iteratively, based on the distribution of instances  and the learned decision function that is refined at each iteration. this process is repeated until the maximum number of labels is reached . in all the different types of data selection described below, labels are asked for  <dig> points in each iteration, and a total of  <dig> iterations are computed resulting with a total of  <dig> acquired labels. in other active learning experiments the number of iterations equals the number of label requests; we reduce the number of iterations to reduce classifier retraining.

a. baseline - random data selection
a random forest was constructed for  <dig> training data that differ from each other in the ratio of positive pairs they contain: 1%, 20% and 45% positive pairs respectively. size of training data is incremented from  <dig> to  <dig> pairs in steps of  <dig> pairs at a time. the  <dig> pairs in each iteration are selected randomly from the overall  <dig>  data points assembled for training. a random forest is retrained in each iteration, and performance on the test data is evaluated

b. density based
in this active learning technique, the data is clustered by a k-means algorithm. labels are requested for a fixed number  of data points in each iteration. the selected points are distributed across the clusters in proportion to the size of the cluster. let ni be the number of data points in cluster ci and n be the total data size. then, si, the number of points to be selected from cluster ci is given by  

and,  

in each cluster ci, si unlabelled data points closest to the centroid are selected and their labels are asked. the weka package  <cit>  was used to implement the k-means clustering.

c. uncertainty based 
in this active learning strategy, in the first iteration, the data whose labels are asked is selected randomly. a random forest is built with that data. in the following iterations, the data points selected for labelling are those which have maximum disagreement among the decision trees in the random forest. the entropy  in labelling the data point  is measured as  

where, p <dig> is the fraction of the decision trees in the random forest that label the protein pair as non-interacting, and p <dig> is the fraction that label the protein pair as interacting.

in each iteration,  <dig> data points with the maximum confusion are selected and their labels are obtained. these are added to the existing set of labelled data and a new random forest is trained from this data. this new random forest is used in the next iteration for selecting the maximal-confusion points.

d. uncertainty based 
this method is same as the previous method, except that in the first iteration, the data is selected by density  as opposed to selecting randomly.

e. uncertainty based with history
this method is based on the technique proposed by davy and luz  <cit>  in which entropy  is measured as the disagreement among the past 'm' predictions for a sample. we consider the past  <dig> predictions to measure confusion. the computation is carried out as follows:

pi <dig> = probability that the protein-pair 'x' is non-interacting according to the ith classifier.

pi <dig> = probability that the protein-pair 'x' is interacting according to the ith classifier.

where, i âˆˆ 

pa <dig> = average probability that the protein-pair 'x' is non-interacting according to past 'm' classifiers.

pa <dig> = average probability that the protein-pair 'x' is interacting according to past 'm' classifiers.

confusion is measured as the sum of relative entropy between the average prediction values and the individual classifier predictions.  

this method requires that the first 'm' classifiers be built by some other mechanism; subsequent iterations select data points using the confusion metric described above. since 'uncertainty based - density-based seed' performed best among other methods , the first 'm' classifiers were built using this technique.

RESULTS
coverage of feature space of proteins-pairs
as a first task in understanding the characteristics of the feature-space, we studied the coverage of each of the features . the data analyzed here was the entire set as created and made publicly available by qi et al  <cit> ; there are  <dig>  interacting pairs  and  <dig>  random pairs . the feature vectors were of  <dig> dimensions. figure  <dig> shows the number of protein-pairs for which each of features is available . as can be seen, each of the features is available for a large percentage of positive pairs, but for only about 20-40% of random pairs. an exception is the gene-expression type of features which are available for all the protein-pairs. there are fewer missing values for positive pairs compared to random pairs .

a small experiment has been carried out to estimate whether the feature-coverage is significantly different between the two classes. the elements in the feature vectors were replaced with 1's and 0's corresponding to "feature-present" and "feature-absent" respectively. in other words, if the gene ontology localization value is known, then that feature is set to  <dig>  irrespective of what the localization is. a random forest is trained on these new feature vectors. we call this new feature vector as the 'coverage vector'. this too has  <dig> dimensions, corresponding to each of the  <dig> elements in the original feature vector. the results of random forest classifier on these binary coverage vectors were: precision of 60%, recall of 56% and f-score of 58%, whereas the accuracy on the original feature vector was precision of 90%, recall of 13% and f-score 23%; the coverage vectors yielded better accuracy than actual feature vectors. it is shown that the coverage-vectors perform better than feature-vectors in classifying protein-pairs as positive or random.

the reason for this may be that a protein pair that is experimentally verified to be interacting is sufficiently important that it would also most likely have been characterized by several experiments, thereby contributing to several feature values being 'present' in the protein-pair vector.

in order to estimate the true capability of the learning algorithm to predict interactions without an indirect bias introduced due to feature-coverage, a subset of the dataset with every point having at least 80% feature coverage is created and used for the experiments in this work. in other words, all the feature vectors in this new dataset contain at least  <dig> out of the  <dig> features.

experimental setup
the training dataset containing 20% positive pairs has been selected for evaluating all the active learning algorithms. this is because in nature, the ratio of positive pairs is lower in comparison to non interacting pairs. however, as described in results section, when the percentage of positive pairs in the training data  is very low , the recall is extremely poor. this is another open challenge in this domain, which is not addressed in this work. to evaluate capability of active learning in comparison to non-active learning method, we chose the training dataset with 20% positive pairs.

each method was initialized with  <dig> labelled protein pairs. in k-means clustering, k, the number of clusters is chosen to be  <dig>  this value was chosen by trial and error. labels are asked for  <dig> data points per iteration by each algorithm. with the updated labelled data, a random forest is trained and its performance is evaluated on the test data in each iteration.

each of the algorithms is executed  <dig> times and the results are averaged. this is done because in two of the methods, the initial data is chosen randomly and hence performance could vary based on the initial data selected. further, building the random forest involves selecting a random subset of features at every node in each decision tree, and there could be performance variation between each build of the random forests. computing an average over multiple runs provides more reliable measures for comparing the performance.

performance comparison
the five algorithms described above were evaluated on the training and test data described above. the precision, recall and f-score for the various methods were computed.

the 'density based' method achieves its maximum f-score value at around  <dig> data points. this method achieves a recall of around 47% at  <dig> labelled samples but there is no significant improvement further. this is likely because 250- <dig> data points selected from the centres of the clusters is sufficient to represent the data distribution. further samples do not seem to provide additional information. this method however gives a lower precision in comparison to 'random' and other active learning methods. on analysis we find the clustering of the data to be not perfect. in the training dataset, all the clusters on average have  <dig> % purity, while the clusters which are dominated by interacting pairs have  <dig> % purity on average. further since most of the clusters are dominated by non-interacting pairs ,  <dig> % of the interacting pairs actually lie in clusters dominated by non-interacting pairs. these issues limit the maximum performance which can be obtained using a purely clustering based approach.

in the 'uncertainty based method with random seed', recall almost doubles in the first active learning iteration  . this causes the f-score to move above  <dig>  from around  <dig> . in the first iteration however there is a drop in precision. as described earlier this is due to the fact that uncertainty based method tends to select large number of interacting pairs. 65% of the data points selected by this active learner in the first iteration  are interacting pairs, much higher than the proportion in the data set. however, in the following iterations there is a gradual increase in precision which reaches  <dig> % at  <dig> data points .

the 'uncertainty based method with density based seed' gives a higher f-score in comparison to the uncertainty based random seed. it may be seen that selecting the seed not randomly but based on density, increases recall as expected   thereby leading to a better f-score.

the 'uncertainty based method with history' performs the best in terms of f-score and recall. a history of past  <dig> predictions  of the data points are taken into account. unlike the other active learning methods in which the f-score does not show improvement after the first few iterations, 'uncertainty based method with history'' has a consistent increase in f-score. it achieves 60% f-score at  <dig> labelled data points, with a recall of 51% and precision of 73%.

CONCLUSIONS
four different active learning algorithms were evaluated for the protein-protein interaction prediction task. the results show that active learning enables better learning with less labelled training data. density based method improved recall by selecting data that is representative of the unlabeled set. applying a density based seed data improves performance over using a random seed data in the confusion-based techniques. it is interesting to see that measuring disagreement among the past predictions  performs better than just confusion in predicting label of a sample with respect to the current classifier . the maximal entropy based methods seek labels for a large number of interacting-proteins in each iteration , despite the fact that the interacting proteins are in low proportion in the overall unlabeled set. this enables faster learning of the rules/characteristics defining positive interactions showing the suitability of these methods for the protein interaction prediction problem where the ratio of interacting pairs is very low in comparison to non-interacting pairs.

many of the human protein-protein interactions still remain undiscovered. understanding the human protein interactome can play a major role in the study of diseases and drug discovery  <cit> . the active learning methods described here achieve a higher accuracy by choosing the most informative protein pairs for labelling. the algorithms can be applied to select candidate protein-pairs whose interaction status if determined experimentally can aid in accurately predicting several other interactions computationally. this method can help in reducing the cost and effort building the human protein interactome, by substantially reducing the number of new in-vitro experiments required to determine specific p-p interaction pairs.

competing interests
the authors declare that they have no competing interests.

authors' contributions
algorithm development was carried out by tpm and mkg, in consultation with jgc. implementation of the algorithms has been carried out by tpm. manuscript has been prepared by tpm and mkg and has been reviewed by jgc.

