BACKGROUND
genomic studies, especially in the field of human health, generally do not focus on the majority of bases which are common to all individuals, but instead on the minute differences which are shown to be associated to a variable phenotype . these variants are caused by diverse processes, which modify the genome in different ways. one common task is to find the evolutionary history which most parsimoniously explains the differences between an ancestral genome and a derived genome.

the best known variants are short single nucleotide variants  or multiple nucleotide variants , which affect at most a handful of consecutive bases. these few bases are substituted, inserted or deleted, without affecting the neighbouring bases or the overall structure of the genome. especially when only substitutions are taken into consideration, this process can be fully understood using mathematical tools  <cit> : not only is it trivial to describe a parsimonious history that explains the appearance of these variants with a minimum number of mutational events, but posterior likelihoods can be computed across the space of all possible histories.

however, rearrangement events sometimes change the overall structure of the genome without changing a base. rearrangements can be decomposed into sequences of basic operations, known as double cut and joins   <cit> . in a dcj operation the dna polymer is cleaved at two loci then ligated again, so as to produce a new sequence from the same bases. a dcj operation can further be decomposed into single cuts or joins   <cit> . the dcj operation creates balanced rearrangements, i.e. without loss or gain of material. however, coupled with the loss or insertion of detached fragments, these dcj operations can explain all structural variants, including copy-number variants   <cit> . these svs are known to have significant impact on phenotype and health  <cit> , but the combinatorial complexity of rearrangement models has restricted their study.

in the absence of cnvs, it is possible to compute a parsimonious history in polynomial time  <cit> , but computing its posterior likelihood against all possible histories is computationally prohibitive, as it supposes a random markov walk across the symmetric group of order 2n, where n is the number of bases in the genome  <cit> .

however, in the presence of cnvs, even computing a parsimonious history is difficult, and several teams have approached this problem with slightly different assumptions. promising results were constructed around a model with a single whole genome duplication and local rearrangements, known as the genome halving problem . some studies did not allow for duplicated regions  <cit> . others allowed for duplications, considered as independent events on atomic regions, and focusing on the problem of matching segmental copies . bader generalized this model further  <cit> , allowing for larger duplications of contiguous regions along the original genome.

other studies extended the scj model with an approximate algorithm based on a restricted model of cuts, joins, duplications and deletions  <cit> . zeira and shamir demonstrated the np-hardness of computing an optimal history with fewest duplications  <cit> . they nonetheless presented a linear time solution to an scj version of the genome halving problem.

we described in  <cit>  the history graph. this data structure represents genomes and their evolutionary relationships, allowing for substitutions as well as rearrangements and copy-number changes to occur, much like the trajectory graph defined by shao  <cit> . this data structure explicitly represents genomic structure, and assumes that the extant and imputed ancestral genomes are phased, and therefore directly representable as sets of nucleotide sequences. in this model, we were able to compute, between bounds, the number of dcj events separating two genomes with uneven content, allowing for whole-chromosome duplications and deletions, which are free of cost. this model is extremely general, as it allows segmental duplication and deletion event to occur at any point in the evolutionary history and over any contiguous region at that time point. it also allows duplicate copies to be created then lost during the course of evolution.

however, in practice, it is much easier to estimate copy-number in a sample, using for example shotgun sequencing fragment density with regard to a reference, than it is to construct the underlying genomes. we therefore ask whether it is possible to evaluate the number of rearrangements between a sample and a supposed ancestral genome using copy-numbers and breakpoints alone. by evaluating the number of rearrangements, we are indirectly proposing to simultaneously infer the rearrangement events, and thus assemble the sample’s genome, using evolutionary parsimony.

we describe here a general model of structural variation which encompasses both balanced rearrangements and arbitrary copy-numbers variants . in this model, we show that the difference between any two genomes can be decomposed into a sequence of smaller optimization problems. mathematically speaking, we represent genomes as points in a ℤ-module of elements that we call flows, and histories as differences between flows. the overall problem of finding a parsimonious history is then replaced by that of finding an optimal decomposition of the problem, which we address with an ergodic sampling strategy.

RESULTS
directed history graphs
we refer the reader to  <cit>  for complete definitions of bidirected history graphs, layered histories and history epochs. in summary, a bidirected graph is a graph where each end of an edge has an independent orientation, it is said to connect to the head or tail of the incident node. atomic dna fragments, called segments, are represented as vertices. ligations connecting the ends of these fragments are represented by bidirected edges, called adjacencies. genomic sequences are represented by threads, simple disjoint chains of segments and adjacencies. directed edges, called branches, connect each segment to at most one ancestral segment. together, threads and branches form a bidirected history graph. a history graph is called a layered history if each thread can be assigned a positive integer depth, such that any segment in a thread of depth d has an ancestor in a thread of depth d− <dig> or none at all. we assume here that all segments with no ancestors occur in a single root layer. for d≥ <dig>  the subgraph composed of the union of threads of depth d and d+ <dig>  as well as all the interconnecting branches form a history epoch. see fig.  <dig> for an example bidirected layered history graph.
fig.  <dig> 
left, a layered bidirected history graph, where the segments are represented as arrows, thus indicating their head and tail sides. right, the corresponding sequence graph, where the segments are represented as thick, curved lines, and the adjacencies as thin straight lines. the copy number weighting, taken from the bottom thread of the layered history graph on the left, is indicated between parentheses for the segment and adjacency edges



given a bidirected history graph h we construct its directed history graphh′ as follows: each segment vertex is replaced by a segment edge from a tail to a head vertex, which are distinct. the orientation can be chosen arbitrarily, so long as the dna label reflects the sequence that will be read when traversing from tail to head. the bi-directed adjacencies incident on the head side of the segment are connected to the head vertex of the segment, likewise for the tail of the segment. if a branch connects two segments in h, a branch similarly connects their head vertices in h′, and another branch their tail vertices.

a directed history graph is trivially equivalent to its bidirected counterpart, so we will use the two concepts interchangeably. because a vertex in a directed history graph can be incident with at most one adjacency edge and at most one segment edge, each connected component of segment and adjacency edges in h′ is a simple path of alternating segment and adjacency edges, which we call a thread.

sequence graphs and copy-number
henceforth we assume all threads in a layered history are circular, as in fig.  <dig>  thus removing corner cases. the sequence graph g obtained from a layered history is constructed by contracting all the branches of h . duplicate edges between two vertices of g are merged together. this defines a projection from each edge of h onto the edge of g it is merged into. for a given thread t in h, its copy-number weighting on g is the function which assigns to each edge e of g the number of times t traverses a thread edge that projects onto e.

from here onwards, g is assumed to be a sequence graph, and e the set of its edges. using shotgun sequencing read density and breakpoint analysis, an approximate sequence graph for a thread-structured genome is obtained far more easily than the genome itself, motivating the analysis of such structures.

flow functions
we examine the properties of copy-number weightings on sequence graphs. the edge spaceℝe denotes the set of real-valued weightings on the set of edges e. ℝe is a vector space isomorphic to ℝ|e|.

a flow on g is a weighting in ℝe such that at each vertex the total weight of segment edge incidences equals the total weight of adjacency edge incidences. this equality on all of the vertices is called the balance condition, which characterizes flows. it is essential to distinguish between an edge and an edge incidence. in particular, if an adjacency edge connects a vertex v to itself, its weight is contributed twice to v. let f denote the set of all flows on g. because the balance condition is conserved by addition and multiplication by a real scalar, f is a subspace of ℝe.

let fℤ denote the set of integer-valued flows on g and fℤ+ the cone of non-negative integer flows. fℤ is a ℤ-modular lattice of the vector space f.

copy-number and flow functions
lemma 1
the copy-number of a circular thread is necessarily a flow in fℤ+.

proof
a circular thread describes a cycle that alternates between adjacency and segment edges. □

in a layered history graph h, the flow of a layer is the sum of copy-number weightings of the threads in that layer. the flow sequences of a layered history h is the sequence of its layer flows, and conversely h is called a realization of s. see fig.  <dig> for an example flow sequence.
fig.  <dig> the flow sequence associated to the layered history graph in fig. 1




a valid flow sequence is a sequence of non-negative integer flows  in fℤ+k such that for any segment edge e, if there exists i such that fi= <dig> then fj= <dig> for all i≤j≤k. in addition, for every segment edge e, f1= <dig>  this ensures that if the number of copies of a segment falls to zero, then it can not be recreated in any subsequent stage of the history, and that at the start of the flow sequence there is exactly one copy of each segment.

a cycle traversal is a closed walk through the graph, possibly with edge re-use, with an explicit starting vertex. a cycle is an equivalence class of cycle traversals defined by a common circular permutation of edges and its reverse.

lemma 2
any layered history has a valid flow sequence, and any valid flow sequence has a realization as a layered history.

proof
the first part follows easily from lemma  <dig>  to prove the second part, we first decompose each flow in the sequence into a set of threads. to decompose a flow of fℤ+ into a sum of circular thread flows, we color the edges of g that have non-zero weight such that segments edge are green, and adjacencies orange. edges with weight  <dig> are removed. each edge with weight of absolute value n> <dig> is replaced by n identical edges of weight  <dig>  creating a multigraph g′. this defines a trivial mapping from the edges of g′ to those of g. by construction, g′ is a balanced bi-edge-colored graph , therefore it is possible to find an edge-covering composed of color-alternating cycles. this can be done in polynomial time by a procedure akin to the typical greedy construction on balanced non-colored graphs  <cit> . by the mapping defined above, we defined a set c of cycles in g that alternate between segment and adjacency edges. by construction, each edge of g has as many cycle edges mapping to it as its flow weight.

we now demonstrate by induction that we can construct a history graph h from this set of cycle decompositions. if the flow sequence has length  <dig>  we simply construct threads that correspond to the cycles in c. because each segment has a flow of exactly  <dig>  there is a bijection between the newly created segments and the segments of g. now let us assume the property demonstrated for all flow sequences of length k or less. given a flow sequence , we first construct a realisation h′ of . we then choose for every segment edge e in g for which fk is strictly positive a segment edge in the leaf layer of h′ that maps to it. we create under this chosen segment copy fk+ <dig> novel descendant segment edges, and assign them an arbitrary order. we decompose fk+ <dig> into a set of cycles in g as described above. for each cycle , we pick a random traversal and create a thread, greedily using up available segments. once connected by adjacencies, these segments form a thread. by construction, each edge e is visited as many times in the set of cycles as newly created segments map to it, so this process is guaranteed to use up all the newly created segments. by construction, the total flow of these threads is equal to fk+ <dig>  the history graph thus extended is a realisation of . □

in  <cit> , we defined the minimum rearrangement cost r of a layered history graph g as the minimum number of dcj operations it would require to account for all the changes that occur in that layered history graph, assuming that whole chromosome duplications and deletions are free of cost. we demonstrated that this cost is np-hard to compute, but provided upper and lower bounds for it that can be computed in polynomial time.

in particular, for any directed history graph g, we assign to each vertex v a lifting ancestor a that is its most recent ancestor with an adjacency edge incidence, else it is a new artificial root node if no ancestor of v has an adjacency edge incidence. by adding for each adjacency edge  a lifted edge ,a) we obtain a lifted graph l. a lifted edge is trivial if it corresponds to an existing adjacency edge, else it is non-trivial. by removing the segment edges and branches from l, we obtain the module graph m, whose connected components are called modules. as demonstrated in  <cit> , a lower bound on the rearrangement cost of a directed history graph g is 
 rl=∑m∈mvm2− <dig>  where the sum is over the modules in m, and for each module m, vm is the number of its vertices, and an upper bound on the rearrangement cost of g is the number of non-trivial lifted adjacency edges in l minus the number of simple modules, i.e. modules in m in which every vertex has exactly one incident non-trivial lifted adjacency edge.

the lower bound is closely related to earlier results  <cit> .

by extension, we define the minimum rearrangement cost of a valid flow sequence as the minimum rearrangement cost across all of its realizations, and seek to demonstrate the existence of tractable bounds on this value.

within a flow sequence s=∈fℤ+k,k≥ <dig>  for any index 1≤i<k, the pair  is called a flow transition. its lower complexitycfi,fi+1l is defined as: 
 cfi,fi+1l=|v|−p−|c| 

where: 
v is the set of vertices of g.

p is the number of adjacency edges e where fi> <dig> 

c is the set of active components, i.e. connected components of vertices and adjacency edges e where fi+fi+1>0



theorem 1
given a layered history h, decomposed into bilayered history graphs , with flow sequence , and any index i<k, 
 cfi,fi+1l≤r 

proof
we will prove this by induction on the sequence length k. for k= <dig>  by definition of valid flow sequences, f <dig> is equal to  <dig> on all segment edges, and each vertex is incident on exactly one segment edge, thus every vertex is incident on exactly one adjacency e where f1> <dig>  this means that in each active component c∈c, the number of vertices vc is exactly twice the number pc of adjacencies e of c where f1> <dig>  in the unique bilayered graph we therefore have: 
 |v|−p−|c|=∑c∈c=∑c∈cvc2− <dig>  the lower complexity is therefore a reformulation of the bound rl given above.

if k> <dig>  to prove the theorem for the flow sequence  on , we remove g <dig> from h, and obtain a reduced history graph h′ composed of bi-layered sequence graphs , and its sequence graph g′. the sequence graph g′ is slightly different from g, in that each segment edge of g′ has a copy-number of  <dig> in f <dig>  whereas each segment edge of g has a copy-number of  <dig> in f <dig>  however, there exists a mapping from the vertices of the second layer of h to the first, implicitly defining a mapping Φ from the vertices of g′ to the vertices of g such that for any edge e in g and any i≥ <dig>  fi=∑e′:Φ=efi′. on g′ we compute the flow sequence of h′, s′=. these flows are all in fℤ+

by the inductive hypothesis, the rearrangement cost of gi+ <dig> is greater than cfi′,fi+1′l, as computed on g′. we now compare cfi′,fi+1′l to cfi,fi+1l, which is computed on g. for simplicity, we create a sequence of graphs  of an arbitrary length q, describing an evolution from g1=g′ to gq=g, and decomposing Φ into a sequence of projections. at each step of the sequence, exactly two nodes are merged. note that elements of this sequence are not necessarily valid graphs. edge labels  are preserved, and two edges between the same vertices and having the same label are merged together. projecting fi and fi+ <dig> across this sequence of graphs, we compute cl at each step. at each step, the number of vertices v decreases by  <dig>  if the vertices belong to two different active components, the number of active components decreases by  <dig> and p remains unchanged. otherwise, the number of active components remains unchanged and p decreases by at most  <dig>  as two edges could be merged into one. the sequence of values of cl therefore decreases, hence cfi,fi+1l≤cfi′,fi+1′l≤r. □

we define an upper complexity formula as follows: the duplication countdi is a weighting equal to min on each edge of g. in other words, it is the number of additional copies of each edge in fi+ <dig> beyond that in fi, up to a maximum of doubling the number of copies. the supra-duplication countsi is the sum across all adjacency edges of max−di,0)=max. the de novo edge countni is equal to fi+ <dig> if fi= <dig>  else  <dig>  for any vertex v we denote by dv the sum of di on all adjacencies incident on v and by nv the sum of ni on all adjacencies incident on v. the imbalance of a segment s between two vertices a and b is equal to max,db−,0) the global imbalanceii is the sum of imbalances across all segment edges. finally, li is the number of perfect components, where a perfect component is a component c∈c such that every vertex of c is incident upon exactly one adjacency e with fi= <dig> and fi+1= <dig>  and exactly one adjacency e′ with fi> <dig>  the upper complexitycfi,fi+1u is: 
 cfi,fi+1u=si+ii−li 

see fig.  <dig> for an example calculation of this upper complexity.
fig.  <dig> computing the upper bound on a sequence graph. note the existence of a perfect component connecting segment c to the rest of the sequence graph, containing nodes n
 <dig>  n
 <dig>  n
 <dig> and n
 <dig>  in total, we obtain s= <dig>  i= <dig>  l= <dig>  hence an upper bound of 2



theorem 2
given a flow sequence s=∈fℤ+k, and an index i<k, it is possible to find a realisation h with bilayered subgraphs  such that: 
 r≤cfi,fi+1u 

proof
we will construct gi directly. for every segment edge s of g, we create fi top layer segments, images of s. in the following, each vertex in gi is assigned an adjacency to a partner vertex in g, on the assumption that our construction algorithm determines independently which specific image in gi it is connected to.

we start by creating the segment edges of gi. on both ends of a segment s, we compute d+n and choose the smaller value ms. we select ms images of s to be marked for duplication. the duplicated images of s are assigned  <dig> descendants each. if fi=ms, an additional fi+1−2ms descendant segments are assigned at random to the duplicated segments, otherwise the fi−ms non-duplicated segments copies are assigned fi+1−2ms descendant segments as evenly as possible, such that the difference in number of descendants between any two of those segments is never greater than  <dig>  at the end of this process, segment copies with  <dig> descendants are marked for deletion. segments which are marked neither for deletion nor duplication are undetermined.

let d be the weighting on g equal to min, i.e. the number of images of an edge of g which are lost between the two layers of gi. by extension, for any vertex v, dv is the sum of d across all adjacencies incident on v.

we then connect the top segments. for every adjacency e of g with d> <dig> we create d images in gi marked for deletion, attached in priority to segments also marked for deletion. by conservation of flow balance we obtain, s being a segment incident on vertex v on g: 
 fi+1−fi=dv+sv−dv 

the number of segment images marked for deletion is therefore: 
 fi−fi+1−ms≤fi−fi+1+dv+nv≤fi−fi+1+dv+sv<dv 

hence all top layer segments marked for deletion can be connected to adjacencies marked for deletion. for every adjacency edge e with a duplication count d> <dig>  we create dduplicated images of this edge, connected in priority to duplicated copies of s, then to undetermined segments. we already connected all segments marked for deletion, hence duplicated adjacencies are connected exclusively to duplicated or undetermined segments. for every adjacency edge e with no flow change in g, we create fi images of this edge connected to the remaining unattached segments.

we finally connect the bottom segments. if two duplicated segments are connected by a duplicated edge, then two of their descendant segments are connected by adjacencies, thus creating two trivial lifting adjacencies in gi. otherwise, for each segment which is incident on an adjacency not marked for deletion, it necessarily has at least one descendant segment, which is connected by an adjacency to a descendant of its partner, thus creating one trivial lifting adjacency in gi. all remaining bottom segments are connected at random, conditional on respecting the adjacency counts specified by fi+ <dig> 

we now evaluate the upper bound rearrangement complexity of the corresponding dna history graph as quoted above from  <cit> . by construction, deleted adjacencies and adjacencies with no flow change do not create non-trivial lifted edges in gi. therefore only adjacencies e in g such that fi+1>fi give rise to non-trivial lifted edges in gi, unless they are connected to two duplicated segments. the number of duplicated adjacencies incident on a segment s which are not connected to a duplicated segment is bounded by its imbalance, hence there are at most ii adjacencies of this type across the graph. in addition, there are si bottom layer adjacencies which were added at random. hence the total number of non-trivial lifted edges is bounded by ii+si. the construction algorithm guarantees that each perfect component in g gives rise to a simple module in g, hence, by the upper bound from  <cit> , the cost is bounded by si+ii−li. □

primary extensions
we say that a valid flow transition  is a lookup if cfa,fbl=cfa,fbu. in this case it is easy to compute the rearrangement cost of the transition. if a transition is not a lookup, then one way to assess its cost is to sample flow sequences with the same initial and final flows that are separated by smaller, more incremental changes.

a valid sub-sequence s <dig> of a given valid flow sequence s <dig> with the same initial and final elements is called a reduction of s <dig>  and conversely s <dig> is an extension of s <dig> 

it is convenient to look at this in terms of flow differences, which are themselves flows. a valid flow transition  defines a flow that is the difference between flows fa and fb, i.e. the flow Δf=fb−fa. likewise, a valid extension  of  defines a sequence of nonzero flow differences δf <dig> …,δfk, where δfi=fi+1−fi, such that: 
 ∑i=1kδfi=Δf 

we call the multiset of nonzero flows δf <dig> …,δfk a decomposition of Δf.

if, for each i, either the flow transition  is a lookup or at least simple enough so that we can compute the transition cost in reasonable time, then by sampling decompositions of the overall flow difference Δf, we can arrange these into extensions of this flow difference, and evaluate these extensions efficiently, keeping track of the cheapest one we find. this provides a sampling strategy for producing plausible explanations for complicated observed flow transitions. to make this work in practice, we introduce the notion of a primary  flow, and demonstrate how to decompose a flow difference f into a sum of primary or near-primary flows. the primary or near-primary flow differences we get are usually a lookup, and if not, the time required to compute their cost is reasonable.

we first introduce primary flows, which are a generating set of fℤ, then a superset of these, the near-primary flows, which can be sampled ergodically with a simple algorithm.

we measure the elements of fℤ with the l <dig> norm, defined by ∥f∥1=∑e∈e|f|. a non-zero flow f is primary if there do not exist two non-zero integer flows f <dig> and f <dig> such that f=f1+f <dig> and ∥f∥1=∥f1∥1+∥f2∥ <dig> 

theorem 3
primary flows are a generating set of fℤ.

proof
we will decompose a flow f by induction. for k≥ <dig>  we will define a set sk of k non-zero flows  , such that f=∑f′∈skf′ and ∥f∥1=∑f′∈sk∥f′∥ <dig>  at the start, s1=. if there exists i≤k such that fik is not primary, then there exist two non-zero integer flows fb and fc such that fik=fb+fc and ∥fik∥1=∥fb∥1+∥fc∥ <dig>  we define sk+1=. it is straightforward to verify that f=∑f′∈sk+1f′ and ∥f∥1=∑f′∈sk+1∥f′∥ <dig>  we proceed until no non-primary flows remain. in that case, then f was successfully decomposed as a sum of primary flows. since the l <dig> norm of a non-zero flow is necessarily a non-zero integer, the total number k of flows we create in this way is bounded by ∥f∥ <dig>  the non-zero flow f was therefore decomposed as a finite sum of primary flows. □

a valid primary flow sequence is a valid flow sequence s= such that for each flow transition, , its associated flow change  is a primary flow.

corollary 1
any valid sequence of flows in fℤ can be extended into a valid primary flow sequence.

proof
we decompose the flow changes of a valid sequence into primary flow changes using the approach described in theorem  <dig>  at each step, a flow change f is replaced by two flows f <dig> and f <dig> such that a) f=f1+f <dig> and b) ∥f∥1=∥f1∥1+∥f2∥ <dig>  from equality a), it follows that at every edge e we have 0<|f|≤|f1|+|f2|. given equality b) we have at every edge e |f|=|f1|+|f2|, hence f <dig> and f <dig> have the same sign as f. therefore if f is the flow change between two non-negative genome flows fa and fb, fa+f1=fb−f <dig> is also non-negative. the extended flow sequence , with flow changes , is also a valid flow sequence. □

by the above result, any flow change f can always be decomposed, not necessarily uniquely, into primary flow changes i= <dig> .k such that f=∑i=1kfi where ∥f∥1=∑i=1k∥fi∥ <dig>  this type of decomposition is fast to compute, and useful to create instances of primary flow sequences.

however, these decompositions are intrinsically limited in scope. in particular, the second condition implies that when the flow change f is, say, positive on a given edge e, then ∀i<k,fi≥ <dig>  this forbids edge re-use, i.e. cases where adjacencies are temporarily created, then deleted, or segments temporarily duplicated, then deleted. edge reuse is common in actual rearrangement histories, including minimum cost histories, so we do not always want to impose this second condition.

given a flow f, a primary extension of f is a set {c1f <dig> …,cnfn}, such that {c <dig> …,cn}∈ℕ∗n and f <dig> …,fn are primary flows, such that f=∑i=1ncifi and the component flows f <dig> …,fn are linearly independent, i.e. no component can be written as a linear combination of the others. note that since the dimension of the subspace of all flows is at most ∥e∥, where e is the set of edges in the graph, no primary extension can have more than ∥e∥ components. further, since the components fi are required to be linearly independent, once these are specified for a given f, the coefficients ci are uniquely determined.

this definition eliminates trivially non-parsimonious cases where a simpler decomposition could be obtained with a strict subset of the component flows. this amounts to forbidding a some cases of homeoplasy, i.e. we allow a simple rearrangement event to happen multiple times, in which case the component is assigned a weight greater than  <dig>  for example in the case of tandem duplications, but we don’t allow distinct sets of components to have identical or inverse flow changes, creating a new configuration and then undoing it again for no net effect.

characterising the space of primary flows
we now demonstrate the ties between primary flows and the space of even length cycles.

for any weighting f in ℝe, its conjugate weightingf^ is the real-valued function on the edges of e defined by: 
 f^:e↦ℝe→fif e is a segment edge−fotherwise 

see fig.  <dig> for an example  even-length cycle,  its alternating weighting and  the conjugate of that weighting. the conjugate transformation is a linear involution, i.e.: f^^=f and ∀∈fℤ <dig> ∀∈ℝ <dig> pf1+qf2^=pf^1+qf^ <dig> 
fig.  <dig> 
a an even length cycle traversal, composed of darker edges indexed by their order in the cycle,  its alternating weighting, and  the conjugate of the alternating weighting . note how the alternating weighting in  respects the conjugate balance condition and the alternating flow in  respects the balance condition



if a weighting is such that the sum of incident weights at every vertex is  <dig>  it is said to satisfy the conjugate balance condition. a weighting f is a flow if and only if  f^ satisfies the conjugate balance condition.

for each edge e, let δe be the weighting that assigns  <dig> to e, and  <dig> to all other edges. the set {δe}e∈e forms a trivial spanning set of ℤe.

given the cycle traversal t=∈v×e2n of an even length cycle c , its associated alternating weighting is w=∑i=12niδei. because the weights of the edges in the cycle alternate between  <dig> and - <dig> along an even length cycle, an alternating weighting satisfies the conjugate balance condition, see fig. 4b).

the alternating flowf of an even-length cycle traversal t is the conjugate of its alternating weighting, i.e. f=ŵ=∑i=12niδ^ei. conversely, t is a traversal of f. because an alternating flow is conjugate to a weighting that satisfies the conjugate balance condition, it satisfies the balance condition defined above, and hence is a flow. see fig. 4c.

for the next few definitions, we assume that t=∈v×e2n) is an even length cycle traversal on g.

two distinct indices i and j are said to coincide if ei and ej end on the same vertex of g. the traversal t is synchronized if whenever two distinct indices i and j coincide, then  is odd. see fig.  <dig> for an example of a non-synchronized cycle traversal being split into synchronized cycles. note that a synchronized traversal cannot have more than two edges that end on the same vertex because for any three integers at least two are even or two are odd, and hence at least one of the pairwise differences among any three integers is even. hence, a synchronized traversal visits each vertex at most twice.
fig.  <dig> on the left, a non-synchronized cycle traversal: edges e
 <dig> and e
 <dig> coincide, although  is even. a simple operation transforms it into two synchronized cycles, such that the sum of alternating flows is unchanged. note that one of the two remaining cycles has edges e2′ and e5′ coincide. it is nonetheless synchronized and cannot be split into two even length cycle traversals since  is odd



a set of pairs of integers p is nested if there does not exist ,∈p <dig> such that i1<i2<j1<j <dig>  the traversal t is nested if the set of pairs of coinciding indices of t is nested. see fig.  <dig> for a counter-example and an example.
fig.  <dig> on the left, a non-nested cycle traversal: coinciding index pairs  and  are such that 2<5<7< <dig>  a simple operation transforms it into two nested cycles such that the sum of alternating flows is unchanged



the traversal t is tight if ei=ej implies that  is even. see fig.  <dig> for a counter-example and an example.
fig.  <dig> on the left, a non-tight cycle traversal: e
 <dig> and e
 <dig> overlap, although  is odd. a simple operation transforms it into a tight cycle traversal with identical alternating flow



lemma 3
a cycle traversal t is tight iff ∥ŵ∥1=|t|, the length of t.

proof
for an edge e of g, let ne be the  set of indices i such that ei=e. let us suppose that t is tight. this means that ∀e∈e,∀∈ne <dig> i=j. we then have: 
 ∥ŵ∥1=∑e∈e∑i∈nei=∑e∈ene=|t| 

conversely, let us suppose that there exists p and q such that ep=eq yet p−q is odd. this means that p= −q, we therefore have: 
 ŵ1=∑e∈e∑i∈nei≤∑e∈e∖{ep}ne+≤l−2<|t| 

□

lemma 4
a cycle traversal t that is both nested and synchronized is necessarily tight.

proof
assuming by contradiction that t is not tight, there exists two indices i<j such that ei=ej and j−i is odd. they cannot be traversed in opposite directions, else ei and ej− <dig> would coincide at their endpoints, even though −i is even, which contradicts synchronization. they cannot either be traversed in the same direction, else ei and ej on one hand and ei− <dig> and ej− <dig> on the other would coincide, which contradicts the nesting constraint. □

a simple cycle is a cycle without vertex reuse. a cactus graph is a graph in which any two simple cycles intersect in at most one vertex, which may be called an intersection point  <cit> . a connected graph is 2-edge connected if for any two distinct vertices a and b, two edge removals are sufficient to disconnect the vertices. it is well-known that a connected graph is a cactus graph iff it is 2-edge connected. a cutpoint in a graph is a vertex that if removed splits the connected component in which it resides into two or more connected components called subcomponents.

lemma 5
given a simple cycle c, one of its traversals t= and a set of properly nested pairs of indices in  <dig>  merging the vertices at the end of the edges indicated by the index pairs creates a cactus graph.

proof
we will demonstrate that the graph is 2-edge connected. let a and b be vertices in the graph. both a and b can be traced to the merging of two sets of vertices in c. let i be the sets of indices of original vertices merged into a, likewise for b. both sets cannot overlap, else they would be identical and a and b would not be distinct. because of the nesting constraint, no vertex in  can be merged to a vertex outside this interval, therefore cutting the edges at indices min) and max)+ <dig> breaks the graph into two components. because all the indices of i belong to the interval, whereas all the indices of i are outside of this interval, a is disconnected from b. thus, the graph is 2-edge connected, in other words it is a cactus graph. □

see fig.  <dig> for an example.
fig.  <dig> transforming a traversal by merging the end-points of nested coinciding pairs of edges ,,), shown with dotted lines) into a cactus graph



theorem 4
the set of primary flows is identical to the set of alternating flows of nested and synchronized cycle traversals.

proof
let f be a primary flow, and t= be a cycle traversal in g that alternates between edges where f is strictly positive and edges where f is strictly negative. we will demonstrate by contradiction that t is nested and synchronized.

we will first assume that t is not synchronized, i.e. there exist distinct indices i and j such that i and j coincide, yet  is even. without loss of generality, we assume that i<j. let t1={ei+ <dig> …,ej} and t2={e <dig> …,ei,ej+ <dig> …,el}. as i−j is even, both are even length, primary cycles. it follows that ŵ+ŵ=ŵ=f, and obviously |t1|+|t2|=l. since t alternates between edges where f is positive and negative, ei=ej implies i−j is even, hence t is tight. it is obvious that this property is preserved in t <dig> and t <dig>  hence, by lemma  <dig>  ∥ŵ∥1+∥ŵ∥1=|t1|+|t2|=l. therefore ∥f∥1=∥ŵ∥1+∥ŵ∥ <dig>  hence f could not be minimal. figure  <dig> gives an example.

we then assume that t is synchronized, but not nested, i.e. that there exist indices i<p<j<q such that  and  are pairs of coinciding indices. then the traversals 
 t1=v,e <dig> …,ei,ej,ej−1…,ep+ <dig> eq+ <dig> eq+2…,el  and 
 t2=v′,ei+ <dig> ei+2…,ep,eq,eq−1…,ej+ <dig>   where v′ is the th vertex traversed by t, can similarly be used as a decomposition of f, verifying that f could not be minimal . thus, t is nested and synchronized.

let t= be a nested and synchronized cycle traversal, we will demonstrate that its alternating flow f is primary. by lemma  <dig>  because t is nested, the subgraph of g it traverses is a cactus graph. because it is impossible to find three indices i,j and k such that ,  and  are all odd, t can visit any vertex at most twice. because all the edges in this cactus graph can be traversed in a single cycle traversal, it can be edge-partitioned into a set of simples cycles or chains such that at each vertex at most two components intersect. because t is synchronized, if f=f1+f <dig> is a decomposition of f and t <dig> is a traversal of f <dig>  t <dig> must necessarily switch between edges in different simple cycles whenever it reaches a cutpoint vertex, else the resulting cycle would not be even-length. thus t <dig> traverses all of the edges at least once, and is equal to t. therefore f is primary . □

near-primary extensions
although very useful to describe the extensions of a flow f, the ergodic exploration of primary extensions is difficult. figure  <dig> provides an example where there is no straightforward manner to transform one primary extension into another.
fig.  <dig> in this graph, represented twice, a conjugate flow f^ is represented with the edge colors. on red edges, f^= <dig>  on blue edges f^=− <dig>  there are exactly two ways of decomposing this conjugate flow as a sum of synchronized and nested alternating weightings, as indicated by the circular arrows. transforming the righthand decomposition into the lefthand one requires simultaneous modifications of all flows



instead, we focus on a superset of primary flows. a flow derived from a tight and nested traversal is called a near-primary flow. following lemma  <dig>  primary flows are a subset of near-primary flows. to illustrate the difference between the two sets, on fig.  <dig>  the left traversal is near-primary, the right one is primary.

a near-primary extension of a flow f is a set  for some integer constants  and near-primary flows  such that f=∑i=1ncifi and the component flows f <dig> …,fn are linearly independent.

lemma 6
the number of possible near-primary extensions of a flow sequence is finite.

proof
because a synchronized traversal visits each edge at most twice, there are only a finite number of synchronized traversals, and hence a finite number of near-primary flows. since the flows in a decomposition  of any flow f are linearly independent, n is at most |e|, and since we must have f=c1f1+…+cnfn, the coefficients c <dig> …,cn are uniquely determined by the choice of f <dig> …,fn. hence the number of decompositions is finite. □

this implies that any scoring scheme can be used to construct a valid probability mass function on the set of near-primary extensions of a flow sequence, which constitute a natural set of flow histories.

collapsing equivalent solutions
let k be the set of edges with zero overall flow change, and Ω the set of vertices which are only incident to k. to prevent a combinatorial explosion because of equivalent cycles going through Ω, Ω is collapsed into a universal connector vertex, ω, connected to all other vertices in the graph. in other words, if an alternating flow traversal has edges incident with vertices of Ω, it is represented as going to ω, self-looping on that vertex, then continuing out. this ensures that otherwise equivalent histories are not distinguished because of irrelevant labeling differences between interchangeable vertices. any alternating flow traversal which self-loops from ω to itself more than once can be automatically simplified into a simple flow traversal with lesser complexity, avoiding extrapolating useless operations on the edges of k.

given a valid flow decomposition, if an edge of k is duplicated then deleted, leaving no final cnv change, then the flows of the two events are summed up into a single flow. this constrains partially the space of possible histories, as it precludes a third event from being timed between the two combined events, but it reduces the search space. to mitigate this, this constraint is only applied to the end results of the sampling, as during the sampling stage we allow for events to have non-zero flow over edges of k.

after these two transformations, a valid flow decomposition is said to be collapsed.

ergodic sampling of near-primary extensions
if there exist tight and synchronized cycle traversals t1= and t2= such that they start at the same vertex v and overlap over their last p≥ <dig> edges, but e1≠ε <dig>  we produce a third cycle traversal: 
 t3=v,e <dig> e <dig> …,e2n−p,ε2ν−p,ε2ν−p− <dig> …,ε <dig>  

subsequently, the traversal t <dig> is broken up into a set of near-primary cycle traversals t={t1′,…,tk′}, using the transformations illustrated in figs.  <dig> and  <dig>  a merge consists in replacing {t <dig> t2} with {t <dig> t1′,…,tk′}. because 
 ŵ+…+ŵ=ŵ=ŵ−ŵ  we have ∀∈ℝ2: 
 αŵ+βŵ=ŵ−β+…+ŵ)  thus the space spanned by {t <dig> t2} is spanned by {t <dig> t1′,…,tk′}. see fig.  <dig> for an example.
fig.  <dig> two overlapping flows t
 <dig> and t
 <dig> transformed by a merge. here t={t
3}={}. in parentheses are indicated the weightings required to maintain equality of conjugate flow, namely, αŵ+βŵ=ŵ−βŵ




given two tight cycle traversals  and  that overlap over their first l edges, such that l is odd, we can construct three new cycle traversals: 
 t1′=v′,e <dig> e <dig> e <dig> el+1…e2nt2′=v′,e <dig> e <dig> e <dig> εl+1…ε2νt3′=v,e <dig> …el,e <dig> e <dig> e <dig> 

where e <dig> e <dig> and e <dig> are adjacency edges such that e <dig> connects v′ to ω, e <dig> connects ω to ω and e <dig> connects ω to v.

if l is even and greater than zero, then we can construct the following cycle traversals: 
 t1′=v′,e <dig> e <dig> el+1…e2nt2′=v′,e <dig> e <dig> εl+1…ε2νt3′=v,e <dig> …el,e <dig> e <dig> 

where e <dig> is a  adjacency edge that connects the destination vertex of el, v′, to ω, and e <dig> is a  adjacency edge that connects ω to the starting vertex of e <dig>  v.

this transformation is referred to as a split, since it separates out the edges e <dig> e <dig> …el, used in two cycles, into a separate cycle, that uses them only once. see fig.  <dig> for an example. as in the case of a merge, the resulting traversals are then further broken down into independent near-primary flows. because 
 ŵ+ŵ+ŵ=ŵ+ŵ  we have ∀∈ℝ2: 
 αŵ+βŵ=αŵ+βŵ+ŵ)  thus the space spanned by {t <dig> t2} is spanned by {t <dig> t1′,…,tk′}. see fig.  <dig> for an example.
fig.  <dig> two overlapping flows t
 <dig> and t
 <dig> transformed by a split into t1′=, t2′= and t3′=. in parentheses are indicated the weightings required to maintain equality of conjugate flow, namely, αŵ+βŵ=αŵ+βŵ+ŵ




theorem 5
the process of applying random merges and splits provides an ergodic exploration of all possible near-primary collapsed extensions of a given flow transition.

proof
we are given a near-primary collapsed extension h of a flow transition ∈f <dig>  and wish to reach another near-primary collapsed extension h° that also spans Δf=f2−f <dig>  because they are both collapsed, h° and h cover exactly the same set of vertices, namely e∖Ω.

we choose at random a near-primary component flow δf° of h°, and one of its tight traversals t°. since h and h° cover the same set of edges, it is possible to greedily define a sequence of flows of h that cover each of the edges of t°. the flows can be sequentially merged, since each shares an overlap of at least one vertex with a previously defined flow. following the greedy merge, they may be greedily split to recover δf°. it is thus possible to reconstruct any near-primary component flow of h° with splits and merges between the vectors in h. □

fixing invalid flow sequences
a flow sequence is a sequence of positive flows, from which a sequence of flow changes can be derived. however, a near-primary extension of a positive flow can not necessarily be ordered to define a valid flow sequence because cumulatively adding up the flow changes can produce negative values.

imposing that each flow of a flow sequence is a positive flow would require much computation and possibly break the ergodicity of the method described below. for this reason we preferred to impose a softer constraint, namely increasing the cost of a history showing inconsistencies. for every segment edge which goes from non-negative to negative or from  <dig> to positive flow, a penalty charge of  <dig> events is added to the history. this cost corresponds to the cost of temporarily duplicating a region then deleting it.

simulations
measuring accuracies of solutions
to test our model, we created  <dig> artificial rearrangement histories. each simulated history begins with a single chromosome composed of  <dig>   <dig>  or  <dig> atomic blocks of sequence in order to have a range not only in the number of rearrangement events per simulation, but in the density or breakpoint reuse of genomic rearrangements as well. the history is built by repeatedly randomly deleting, duplicating or inverting segments of contiguous blocks. the number of structural rearrangement operations applied was randomized between  <dig> and  <dig>  again, in order to generate histories with a range of complexity and density. for example, a simulation with  <dig> sequence blocks and  <dig> rearrangement events will have a higher density of events than a simulation with  <dig> sequence blocks and  <dig> rearrangement events; although the sequence of deletions, duplications, or inversions in both histories may be identical, in the smaller genome, they are more likely to overlap or occur in the same genomic regions.

given the final copy number profile and novel adjacencies of the rearranged genome, we sampled possible flow histories using importance sampling based on the above ergodic exploration of the space with ten independent runs of  <dig>  iterations each. each near-primary flow, representing a set of rearrangement events, received a likelihood score equal to the fraction of sampled histories that contain it. so as not to penalise valid flow transitions that are extensions of multiple flow transitions from the simulated history, we counted as true positive any sampled flow transition which could be decomposed as a finite sum of original simulated flow transitions.

we find that near-primary flow transitions predicted in the sampled histories but not present in the simulated history have low likelihood scores, and that correctly predicted events have high likelihood scores, as shown in fig.  <dig>  furthermore, out of the  <dig> events with a likelihood score greater than  <dig> ,  <dig>  of them match the true simulated events, so near-primary flows with high likelihood scores are more likely to represent the underlying simulated rearrangement history.
fig.  <dig> near-primary flows across  <dig>  sampled histories for  <dig> simulated rearrangement histories were merged and assigned a likelihood score. near-primary flows with high likelihood scores are enriched for simulated events, while near-primary flows with low likelihood scores do not represent the simulated flow history



after filtering for near-primary flows with likelihood scores greater than  <dig> , we determined the recall, precision, and f-score for each of the  <dig> simulated rearrangement histories. this is shown in fig.  <dig> as a function of the connectivity or breakpoint reuse of the simulated flow history . the accuracy of a sampled set of histories decreases as the near-primary flows become more connected, either through breakpoint reuse or through nested overlapping events. breakpoint reuse introduces coinciding edges in the sequence graph, as described in figs.  <dig> and  <dig>  these formations have alternative traversals, leading to ambiguity in the flow history. for trivial histories with a connectivity equal to one, representing no breakpoint reuse, we achieve up to  <dig> % accuracy, as measured by the f-score. for histories with connectivity greater than  <dig>  we achieve an average  <dig> % accuracy, largely due to a drop in sensitivity with increased breakpoint reuse. this indicates that certain complicated and entangled rearrangement events cannot be reliably segregated from the simulated near-primary flows. in 25/ <dig>  of the simulations, there exists a sampled history with a lower cost than the simulated history, so we would not expect all near-primary flows in the simulated history to have the highest likelihood scores in these cases.
fig.  <dig> we calculated statistics for  <dig> simulated histories with varying densities of rearrangement events. the ability to accurately predict a flow history decreases as the breakpoint reuse, the number of edges over the number of nodes, increases. most of this loss in accuracy comes from a decrease in sensitivity, or the simulated near-primary flows not being constructed in the sampled histories



discussion
as discussed in the introduction, there have been many attempts to define parsimomious histories given a set of rearranged genomes with deletions and duplications. because of the inherent complexity of the problem, different models made different simplifying assumptions. the model presented here is particularly flexible, in that it allows arbitrarily complex rearrangement events containing duplications and deletions of arbitrary contiguous regions, not just predefined atomic components or predefined rearrangement transformations. there exists a “free lunch” problem whereby if arbitrary de novo insertions are allowed, then any differences between the ancestral and derived genome can be explained by a parsimonious but absurd evolutionary history consisting of an entire genome deletion followed by the insertion of a completely new genome  <cit> . to preclude such trivially unrealistic solutions, our model does not allow insertions of entirely new sequence, only duplication of existing dna. in addition, this is the first such model which offers an explicit ergodic sampling procedure. in counterpart, we currently bracket the cost of each flow transition between an upper and lower bound, with the expectation that the two bounds meet for most encountered near-primary flow transitions. given their constrained size, it should be possible to exhaustively test the cost of near-primary flow transitions when these two bounds diverge. in addition, the collapsing of histories does preclude some event orderings.

theorem  <dig> only describes the cost of each epoch independently, and does not allow us to extrapolate the cost of entire histories. because the bilayered components of a layered history graph share intermediary layers, defining the structure of one layer constrains the possible layers immediately before and after it. it is therefore sometimes impossible to construct a realisation of a flow sequence such that every bilayered component has a cost below its corresponding upper bound. determining the cost of entire histories would presumably require reconstructing these histories in their entirety, as we describe in  <cit> . however, this explicit approach requires the ancestral and derived genomes to be fully determined, which is often not the case when working with sequencing data. the loss of precision therefore appears to be necessary to handle the incompleteness of genomic data.

the application of the theory described here would therefore be most relevant to un-assembled short read data, for example metagenomic samples or tumors. inferring the evolutionary history of the observed copy-number changes and alignment breakpoints would help both understand the evolutionary processes at play and determine the most likely sequences of the sampled genomes. however, such datasets pose further issues because they generally contain a diversity of different genomes, and the data is itself quite noisy. given that most of the properties demonstrated here hold for the edge space ℝe, we believe that it will be possible to extend the entire model to handle arbitrary mixtures of genomes, where copy-number is weighted by prevalence. noisy artifacts would thus be represented as low frequency elements that can be filtered out at the end of the analysis. as genomic sequencing progresses and long range data becomes more affordable, it might be possible to obtain near-finished assemblies straight from the data, in which case the model described in  <cit>  would be more relevant.

from a mathematical standpoint, the set of flows of g is very rich. it bears similarity to the cycle space described in  <cit> . the balance condition guarantees that a positive flow can be decomposed as a set of weighted threads, and that in a flow transition, every created adjacency is compensated by a either a corresponding adjacency deletion or a segment duplication, much like double entry book-keeping. the near-primary flows further provide us with a finite subset of flows, such that all possible realisations of the data can be sampled ergodically. however, unlike models for single base substitutions and balanced rearrangements, this model of evolutionary cost is not a distance function, because it is asymmetric, i.e. the distance between two genomes depends on which genome is ancestral. for example, duplicating a segment then translocating one of its copies generally requires two dcj operations, yet only one operation  is needed to return to the original genome.

CONCLUSIONS
we presented here a model to efficiently sample the space of possible rearrangement histories in the presence of duplications and deletions. confronted with the np-hard problem of inferring parsimonious histories described in  <cit>  from everyday genomic data, we opted for a simplification of the problem which allows us to adopt an efficient ergodic sampling strategy. through simulation, we verified that this approach produces exploitable results. in practice, it is capable of accurately discriminating true rearrangement events and sampling their possible orderings. there remain a few open questions, in particular whether it is possible to efficiently compute the rearrangement cost of any primary flow sequence.

not applicable.

funding
we would like to thank the howard hughes medical institute, dr. and mrs. gordon ringold, nih grant 2u <dig> hg002371- <dig> and nhgri/nih grant 5u01hg <dig>  and the european molecular biology laboratory  for providing funding.

availability of data and materials
all the code used in the simulations and sampling is available at https://github.com/dzerbino/cn-avg.

authors’ contributions
the theory was developed jointly by dz, bp, gh and dh. the simulations were coded and run by dz and tb. all authors read and approved the final manuscript.

competing interests
the authors declare that they have no competing interests.

consent for publication
not applicable.

ethics approval and consent to participate
not applicable.
