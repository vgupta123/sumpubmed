BACKGROUND
one of the most fundamental questions of modern comparative evolutionary phylogenomics is to identify common  genes that originated through complex biological mechanisms such as speciation, multiple gene losses/gains, horizontal gene transfers, deep coalescence, etc.  <cit> . when homologous sequences are identified, they are usually grouped and aligned together to form clusters. homologous dna  sequences can be further subdivided into two major classes: orthologs and paralogs. orthologs are defined as homologous genes in different species that arose due to speciation events, whereas paralogs have evolved from gene duplications. moreover, orthologous genes are more likely to exhibit a similar tempo and mode of evolution, thus preserving overall sequence composition and physiological function. paralogs, instead, tend to follow different evolutionary trajectories leading to subfunctionalization, neofunctionalization or both  <cit> . nevertheless this phenomenon, called the ortholog conjecture, is still debatable  <cit>  and requires additional validation since it has been shown that even between closely related species some orthologs can diverge such that they eventually loose common functionality.

the accurate detection of sequence homology and subsequent binning into aforementioned classes is essential for robust reconstruction of evolutionary histories in the form of phylogenetic trees  <cit> . to date, numerous computational algorithms and statistical methods have been developed to perform orthology/paralogy assignments for genic sequences . methodologically these approaches employ heuristic-based or evidence -based identification strategies, which produces varying frequencies of false positive or negative results. the majority of heuristic algorithms rely on the principle of reciprocal best hit  where blast  <cit>  hit scores  approximate evolutionary similarity between two biological sequences. further algorithmic augmentations of those heuristics, for instance markov graph clustering   <cit> , enables the definition of orthologous/paralogous clusters from multiple pairwise comparisons. despite their relatively low computational complexity, these algorithms have been shown to overestimate the number of putative homologies .

in this current era of next-generation sequence data researchers have gained access to tremendous amounts of “omic” data, including for non-model organisms. phylogenetic information, including species trees, is very limited, unreliable and/or completely unavailable for some poorly studied taxa, thus evidence-based methods are not directly applicable to infer homology. ebersberger et al.  <cit>  developed the first attempt to circumvent this problem, using a novel hybrid approach  for extraction of homologous sequences from est/rna-seq data using a profile hidden markov model   <cit>  based on a similarity search coupled with subsequent rbh derived from re-blasting against a reference proteome. the innovative feature of their approach is in the utilization of phmm as an additional evidence for homology. this architecture incorporates characteristics of multiple sequence alignments  for user pre-defined core orthologs. then, a hmm search is performed with each individual phmm using matching criterion applied to find putative orthologs in the proteome of interest. this method, however, has limitations and weaknesses, such asi) proteome training sets composed of phylogeneticaly “meaningful” taxa for construction of core ortholog clusters may not be available,

ii) identification of informative core ortholog clusters may be somewhat cumbersome due to incomplete and/or low coverage sequencing,

iii) the phmms may not contain any relevant compositional or phylogenetic properties about biological sequences that constitute msa, and

iv) inability to explicitly identify paralogy limits the use of hamstr for some evolutionary applications. hence, homologous clusters inferred from various multiple sequences require further validation to improve confidence in orthology/paralogy classification.



here, we propose a unique approach to identify false positive homologies detected by heuristic methods, for example hamstr or inparanoid  <cit> . our machine learning method uses phylogenetically-guided inferred homologies to identify non-homologous  clusters of sequences. this improves the accuracy of heuristic searches, like those that rely on blast.

methods
library preparation and rna-seq
for the experimental data set  we used  <dig> odonata  and  <dig> ephemeroptera  species. total rna was extracted from the eye tissues of each taxon using nucleospin rna ii columns  and reverse-transcribed into cdna libraries using the illumina truseq rna v <dig> sample preparation kit that both generates and amplifies full-length cdnas. prepped ephemeroptera mrna libraries were sequenced on an illumina hiseq  <dig> producing 101 bp paired-end reads by the microarray and genomic analysis core facility at the huntsman cancer institute at the university of utah, salt lake city, ut, usa, while all odonata preps were sequenced on a gaiix producing 72 bp paired-end reads by the dna sequencing center at brigham young university, provo, ut, usa. the expected insert sizes were 150 bp and 280 bp respectively. raw rna-seq reads were deposited in the national center for biotechnology information , sequence read archive, see additional file  <dig> 

read trimming and de novo transcriptome assembly
the read libraries were trimmed using the mott algorithm implemented in popoolation  <cit>  with default parameters . for the assembly of the transcriptome contigs we used trinity  <cit> , currently the most accurate de novo assembler for rna-seq data  <cit> , under the default parameters.

downstream transcriptome processing
in order to identify putative protein sequences within the trinity assemblies we used transdecoder , the utility integrated into the comprehensive trinotate pipeline  that is specifically developed for automatic functional annotation of transcriptomes  <cit> . transdecoder identifies the longest open reading frames  within each assembled dna contig, the subset of the longest orfs is then used to empirically estimate parameters for a markov model based on hexamer distribution. the reference null distribution that represents non-coding sequences is constructed by randomizing the composition of these longest contigs. during the next decision step, each longest determined orf and its  <dig> other alternative reading frames are tested using the trained markov model. if the log-likelihood coding/noncoding ratio is positive and is the highest, this putative orf with the correct reading frame is retained in the protein collection . for more details about the rna-seq libraries, assemblies and predicted proteomes see additional file  <dig> 

construction of drosophila data set
ten high quality drosophila raw rna-seq data sets  were obtained from ncbi . first we trimmed the reads using popoolation  <cit>  and subsampled the read libraries to the size of the smallest . then, two additional data sets corresponding to 50 % and 10 % of the scaled libraries were constructed by randomly drawing reads from the original full-sized libraries. finally, de novo transcriptome assembly and protein prediction were conducted as outlined above for these three data sets. these data sets were used to test whether homology clusters derived from low-coverage rna-seq libraries contain more false positives.

gene homology inference
to predict probable homology relationships between proteomes we used the heuristic predictor inparanoid/multiparanoid based on the rbh concept  <cit> . among various heuristic-based methods for sequence homology detection, orthomcl  <cit>  and inparanoid  <cit>  have been shown to exhibit comparable high specificity and sensitivity scores estimated by latent class analysis  <cit> , so in the present study we exploited inparanoid/multiparanoid v.  <dig>  for the purpose of simplicity in computational implementation. inparanoid initially performs bidirectional blast hits  between two proteomes to detect bbhs in the pairwise manner. for this step, we set default parameters with the blosum <dig> protein substitution matrix and bit score cutoff of  <dig> for all-against-all blast search. next, multiparanoid forms multi-species groups using the notion of a single-linkage. due to inefficient multiparanoid clustering algorithm, we had to perform a transitive closure to compile homology clusters for all species together. transitive closure is an operation performed on a set of related values. formally, a set s is transitive if the following condition is true: for all values a, b, and c in s, if a is related to b and b is related to c, then a is related to c. transitive closure takes a set  and creates all transitive relationships, if they do not already exist. when a set is already transitive, its transitive closure is identical to itself. in the case of the pairwise relationships produced by inparanoid, we constructed orthologous clusters using the notion of transitive closure, where gene identifiers were the values, and homology was the relationship.

for example, our od_s data set consisted of n =  <dig> proteomes, so we had to perform n×/2 =  <dig> pairwise inparanoid queries. a simple transitive closure yielded total  <dig>  homology clusters for od_s. the droso data set yielded  <dig> ,  <dig>  and  <dig>  homology clusters for 100 %, 50 % and 10 % respectively. then putative homologous genes were aligned to form individual msa homology clusters for the subsequent analyses using mafft v.  <dig> b  <cit>  with the “-auto” flag that enabled detection of the best alignment strategy between accuracy- and speed-oriented methods.

additionally, we utilized hamstr v.  <dig> . <dig>  <cit>  under default parameters to delineate putative orthologous sequences in the od_s proteome sets.  <dig>  core 1-to-1ortholog clusters of  <dig> arthropod species  for training phmm were retrieved from the latest version of orthodb  <cit> . we used rhodnius prolixus  as the reference core proteome because this is the closest phylogenetically related species and publically available proteome to the ephemeroptera/odonata lineage  <cit> . as previously described, each core ortholog cluster was aligned to create msa using mafft and converted into hmm profile using hmmer v.  <dig>   <cit> . bbhs against the reference proteome were derived using reciprocal blast.

construction of ground-truth training sets
the orthodb database is one of the most comprehensive collections of putative orthologous relationships predicted from proteomes across a vast taxonomic range  <cit> . this data is particularly useful for construction of training sets since orthodb clusters were detected using a phylogeny-informed approach collated with available functional annotations. hence, training sets constructed from orthodb clusters have the inherent benefit of both an evolutionary and physiological assessment resulting in more precise filtering for false positive homology.

the key to our method was the development of labeled training sets that were used to train supervised machine learning classifiers. previously, homology clusters were known and annotated in orthodb. there were, however, no annotated clusters that represented non-homology clusters from random alignments. thus, we created and annotated our own set of non-homology clusters through a generative process. we created these clusters in two different manners: randomly aligned sequences and evolving sequences from the homology clusters.

we extracted  <dig>  homology  clusters from the predefined orthodb profile called “single copy in > 70 % of species” across the entire arthropod phylogeny in the database, and then aligned them. non-homology  clusters were generated using: i) the alignment of randomly drawn sequences from the totality of the protein sequences with cluster size sampled from poisson , where λ =  <dig>  was estimated as the average cluster size of hs and ii) by evolving the sequences taken from h clusters. this process of evolving sequences was accomplished by using paml  <cit>  to generate random binary trees for each sequence within a cluster. the discretized number of terminal branches for each random tree was sampled from a normal distribution with mean  <dig> and a standard deviation of  <dig>  within each of the clusters, individual sequences were evolved using their respective randomly generated tree using seq-gen  <cit> . we used wag + i  <cit>  as the substitution model for the amino acid sequences during the evolving process specifying the number of invariable sites  at 0 %, 25 % and 50 %. then, to form nh clusters, a single evolved sequence from the terminal branches was selected randomly from each tree. by doing so, we simulated more realistic clusters in which the evolved sequences were diverged enough to be considered as non-homologous to each other.

from the h and nh clusters, two different sets of training, validation and testing partitions were formed. the first set  had an equal number of homology, randomly aligned, 0 % invariable-site evolved, 25 % invariable-site evolved and 50 % invariable-site evolved clusters within the combination of training, validation and testing data sets. the second set  consisted of 50 % of the training set as homology clusters while the remaining half of the training set was composed of equal parts randomly aligned, 0 % invariable-site evolved, 25 % invariable-site evolved and 50 % invariable-site evolved clusters. the combined data sets were then partitioned into training, validation and testing. this was done by randomly sampling from the pool of clusters and assigning 80 % of the clusters  to training, 10 %  to validation and the last 10 %  to testing.

attribute selection
ten different attribute features were selected  and calculated for individual msa of putative homology clusters and for training hs and nhs as well. to identify randomly aligned positions in msas, we utilized aliscore  <cit> , software based on the principle of parametric monte carlo resampling within a sliding window. this approach is more objective and exhibits less conservative behavior contrasted to commonly used non-parametric approaches implemented in gblocks  <cit> . we expected the number of randomly aligned positions for false positive homologies to be higher than for true homologs. additionally, several other simple metrics  were also derived. overall, incorporation of these attributes into a training set was used to increase the robustness of the performance of the machine learning algorithm. we also obtained amino acid composition for each sequence from each cluster and binned it into four classes according to physicochemical properties of amino acids , then compositional dispersion was calculated using an unbiased variance estimator corrected for sequence length. here we assumed that amino acid composition between closely related sequences would be preserved by analogous weak genome-wide evolutionary constraints  <cit>  and thus have diminished variance.table  <dig> all features that were used in order to train the machine learning algorithm. each of these features was calculated for each of the clusters



machine learning
for detection of false positive homology we utilized different supervised machine learning algorithms in order to learn from the labeled data instances. supervised machine learning algorithms take in labeled instances of a particular event as input. from these labeled instances, the algorithm can then learn from the features associated with the instance to perform classification on other, unlabeled instances. a number of different algorithms were used in order to find a model that performed well. waikato environment for knowledge analysis  software  <cit>  was utilized for training different supervised machine learning classifiers and for evaluating the test data sets. a set of models was trained and compared using the arthropod data set .

a number of different machine learning algorithms were evaluated. these algorithms included: neural networks, support vector machines , random forest, naive bayes, logistic regression, and two meta-classifiers. a total of seven models were trained for the arthropod data set. a meta-classifier uses a combination of machine learning algorithms in tandem to perform classification. the two different meta-classifiers utilized stacking with a neural network as the meta-classifying algorithm. stacking takes the output classifications for all other machine learning algorithms as input and then feeds them into another machine learning algorithm. the learning algorithm that is stacked on the others is then trained and learns which machine learning algorithms it should give more credence when performing classification. one of the meta-classifiers incorporated all the previously mentioned learning algorithms . the other meta-classifier used all the previously mentioned learning algorithms except for logistic regression. all parameters for each machine learning algorithm are summarized in table  <dig> table  <dig> the machine learning parameters used for each of the different algorithms in weka



training
the training data set was used as input to the machine learning model for parameter selection. for the arthropod data set, 80 % of the data were used for training, while 10 % of the data was reserved for validation and the last 10 % for testing. machine learning algorithms were utilized to learn from the combination of the h and nh clusters in the data set to differentiate the two. a trained model could then be used to classify unlabeled instances as homologous and non-homologous. there were a total of  <dig>  instances in the orthodb arthropod data set that were used as a training set for both the prop and the equal data sets. in the prop data set, there were  <dig>  h and  <dig>  nh clusters. in the equal data set, there were  <dig>  h and  <dig>  nh clusters.

validation
the validation data sets were used after the model had been trained on the training data set. by using the trained model on the validation set, the efficacy of the model could be seen. 10 % of the arthropod data set formed the arthropod validation set. the models trained using the arthropod training set were validated only with the arthropod instances. if the model did not perform adequately on the validation set, different parameters for the machine learning algorithms were modified in an attempt to improve the performance of the models. the re-trained models would then revalidate on their same, respective validation sets. the process was repeated until adequate performance of the learning algorithm was reached. the orthodb arthropod validation set consisted of  <dig>  instances for both the prop and equal data sets. the prop data set had 566 h and  <dig> nh clusters. the equal data set had 238 h and  <dig> nh clusters.

testing
all general steps of our pipeline are summarized in fig.  <dig> using the example of od_s processing. testing data sets were used only after all the models were finished being trained and validated. this is to ensure an honest measure of the predictive capacity of the models because the testing data were never used in order to evaluate how our model was built and to modify the models. the last 10 % of the arthropod data set was used as the arthropod test set. the arthropod test set from the orthodb contained  <dig>  instances for both the prop and equal data sets. the prop data set had 555 h and  <dig> nh clusters. the equal data set had 207 h and  <dig> nh clusters.fig.  <dig> a diagram of the workflow. this figure shows the different steps that were used in developing our machine learning model. arthropod phylogeny was generated in previous studies and deposited in orthodb. these sequences were then gathered from orthodb and used as our orthology and paralogy clusters. they were combined with generated non-homology clusters. the combination represents our training data set used to train the machine learning algorithms. the experimental data were assembled with proteins inferred from the assemblies. inparanoid was then used to identify putative homologs. once putative homologs were identified they were input into the trained machine learning algorithms for classification and subsequent cluster trimming



performance evaluation
we tested our filtering process by applying the arthropod classifiers trained on the ground-truth data set to the droso and od_s data sets. unlike the testing sets mentioned in the previous section, the ground-truth for these data sets was unknown. we examined the number of clusters filtered and conducted a manual inspection of a subset of the filtered clusters to verify the removal of only false positive homology clusters. because there are, to the authors’ knowledge, no other post-processing methods for cluster filtering that exist our approach is novel. the filtering processes that do exist are heuristic-based approaches, such as an e-value cutoff, that are built-in modules of the clustering software. therefore, for comparison, we only examined the number of clusters filtered from the output of inparanoid and hamstr.

RESULTS
as can be seen in table  <dig> for both the prop and equal data sets, the arthropod models all  had classification accuracies higher than 96 % on the validation set. on the test set, all models  had classification accuracies higher than 95 %. the algorithms that performed the strongest were the meta-classifiers. the meta-classifier using logistic regression performed best in both the prop and equal data sets. comparing the two different data sets, the models perform similarly whether given the prop or equal data sets. the only exception to this is the naive bayes classifier that performs much better  when given the prop data set. however, the models trained with the equal data sets were slightly better in terms of accuracy . in the arthropod models, we varied the size of the training set . the validation set accuracy of the meta-classifier with logistic regression plateaued and slowed growth after training on 5 % or more of the training instances. before this, their classification accuracies of all models were erratic with both increases and decreases as the training set size increased. the models behave differently when given varied amounts of data to train on . all models except for naive bayes increased in accuracy as the training data grew. logistic regression and the meta-classifier with logistic regression required the least amount of training data before they started to plateau. additionally we tested which features were the most meaningful for classification using meta-classifier with logistic regression . the “number of gaps” feature provided the best accuracy when 100 % of instances were used. since increased indel events are accumulated over longer evolutionary time periods, the inferred msas from such highly diverged sequences with lost signatures of common ancestry are expected to have multiple gaps. moreover, clusters prone to large amounts of missing data will be classified as nh using this feature. similar accuracy levels were achieved for the four amino acid composition and number of amino acids features. as we mentioned earlier, selection forces may preserve amino acid composition especially through the action of purifying selection  <cit>  making these features useful for h vs. nh cluster discrimination. other features, except for aliscore that exhibited an intermediate accuracy, had accuracy < 80 %, which might be explained by the fact that these features are less biologically meaningful.table  <dig> summary of arthropod machine learning model performance

this table shows the performance of each of the different learning algorithms that were trained, validated, and tested with the orthodb arthropod gene clusters

fig.  <dig> bootstrapping results for the machine learning models. bootstrapping was conducted using  <dig> replicates for each classifier. error envelopes can also be seen for each classifier. except for naive bayes, as the percentage of total training instances used during learning increases accuracy increases and the error envelope decreases

fig.  <dig> accuracy curves for individual features  using meta-classifier w/ logistic regression. the number of gaps, amino acid composition and number of amino acids features exhibit better predictive accuracy



lower coverage data sets are often used when performing transcriptomic and evolutionary analyses especially on non-model organisms. for instance, in a recent paper  <cit>  the authors inferred a phylogeny of many insect species using relatively small rna-seq library sizes averaging at ~ 3gb  compared to drosophila data sets . we expected the number of false positive clusters to increase with the decreasing sequencing depth. in order to examine this, three droso data sets were tested for the presence of false positives using the meta-classifier with logistic regression trained on the equal arthropod data set. indeed, we found that the number of false positive homology clusters increased in the subsampled droso data sets . these subsampled data sets allowed us to see the results that are common when homology clustering is performed on small libraries. applying the filtering process to the inparanoid and hamstr od_s clusters resulted in many removed clusters , implying that heuristic-based methods have increased rates of false positives. for filtering, we only used the meta-classifier with logistic regression. the removal of many clusters showed the overall poor quality of many of the putative homology clusters . this was expected due to the low quality transcriptome assembly that was caused by sequencing depth in addition to biological factors such as interspecific differential expression. the filtering process preserved higher quality clusters and finished almost instantly resulting in huge time savings when compared to manually curating the clusters. overall our method can be applied to filter homology clusters derived from closely related  as well as highly diverged taxa . we also note that the trimming procedure behaves more conservatively with increasingly diverged sequences.table  <dig> summary of inparanoid and hamstr cluster filtering

the number of clusters that were kept and removed for the od_s clusters from inparanoid and hamstr. filtering was accomplished using the meta-classifier w/ logistic regression model trained on the equal data set

fig.  <dig> examples of a high quality homology  and false-positive homology  clusters  classified by meta-classifier w/ logistic regression. all sequences within the homology cluster  belong to one protein family . the sequence in the false-positive homology cluster indicated by the arrow represents aprataxin and pnk-like factor whereas other sequences represent tyrosyl-dna phosphodiesterase



CONCLUSIONS
we have demonstrated a machine learning method that can be used to differentiate homology and non-homology clusters based on characteristics of known good and bad clusters. these results can be seen in our trained models’ ability to achieve high classification accuracy on the test data sets as well as by examining the number of clusters that were removed from the experimental od_s data set. we developed a training set of known good and bad clusters that was previously unavailable and made supervised machine learning impossible. using a feature set that we developed, we tested various machine learning algorithms and found that when trained on our training data sets that the meta-classifier with logistic regression consistently outperformed all other models and performed just as well as the meta-classifier without logistic regression.

applications of our method were also seen as we applied them to other data sets. our method was especially useful when applied to the od_s data set, by filtering out many clusters with false positive homology. we showed that our method is effective in settings where non-model organisms are being studied and the transcriptome assembly quality is low primarily due to low coverage sequencing or partial rna degradation.

this paper has demonstrated the usefulness of machine learning in finding homology clusters by quickly removing low quality clusters without using any additional heuristics. the clusters that are retained can then be used later in higher quality phylogeny reconstruction and/or other analyses of gene evolution. in the future, we aim to explore machine learning approaches to clustering sequences more deeply to produce more refined and reliable homology clusters.

additional files
additional file 1: 
summary of od_s rna-seq libraries. 

additional file 2: 
density estimation of rna-seq base coverage used in [
32
]. 

additional file 3: 
summary of droso rna-seq libraries. 



competing interests

the authors declare that they have no competing interests.

authors’ contributions

as proposed the idea and designed the experiments. msf trained the machine learning algorithms. msf and noj analyzed the data. mjc and smb provided reagents/data/analysis strategies. as, msf and noj wrote the paper. all authors read and approved the final manuscript.

we thank gavin j. martin and nathan p. lord for the generation of sequence data, t. heath ogden for providing specimens and eric ringger for his help with machine learning model selection and valuable discussion. we also thank the national science foundation for funding this research in the form of a grant awarded to both smb and mjc .
