BACKGROUND
the term biomedical event extraction is used to refer to the task of extracting descriptions of actions and relations among one or more entities from the biomedical literature. since the scientific literature contains a wealth of information about relationships among gene products that is not contained in structured databases, there has been sustained interest in developing methods that are able to automatically extract these relationships of interest. in recent years, there have been two community-wide shared tasks focused on the semantically rich problem of event extraction. the  <dig> and  <dig> bionlp shared tasks  <cit>  involved extracting events composed from a handful of different relation types of varying complexity. in this article, we describe our submission to bionlp  <dig> shared task genia task <dig>   <cit>  and report on additional experiments we have conducted.

in our approach, we decompose event extraction into a set of classification tasks that can be learned either independently or jointly using the search-based structured prediction framework   <cit>  in a formulation we proposed in earlier work  <cit> . searn is an algorithm that converts the problem of learning a model for structured prediction into learning a set of models for cost-sensitive classification . csc is a task in which each training instance has a vector of misclassification costs associated with it, thus rendering some mistakes to be more expensive than others  <cit> . compared to independently learned classifiers, searn is able to achieve better performance because its models are learned jointly. thus, each of these models is able to incorporate features representing predictions made by the other ones, while taking into account possible mistakes made. our intention is to explore how these two learning paradigms compare with each other, as well as with other approaches in the context of bionlp11st-ge <dig>  in addition to reporting results using the official evaluation using f-score, we also explore the range of precision and recall points that are achievable by the two approaches. moreover, we demonstrate how we can adjust the trade-off between recall and precision under searn by using a weighted loss function. finally, we report on experiments with the simple domain adaptation method proposed by daumé iii  <cit> , which creates a version of each feature for each domain.

an early shared task in biomedical information extraction was the learning language in logic  <dig>  genic interaction shared task  <cit> , which focused on protein-protein interactions . however, the datasets involved were rather small in size, not allowing confident conclusions on system performances. lll  <dig> was followed by the protein-protein interaction pair subtask of biocreative ii  <cit> . in this subtask, the annotated datasets provided were produced by gathering curated interactions from relevant databases. this meant that there was no text-bound annotation, thus making the application and evaluation of existing nlp techniques difficult, resulting in rather low performances. indicatively, the best performance achieved was  <dig> in f-score, while many of the teams scored below  <dig>  more recently, the bionlp  <dig> shared task  on event extraction  <cit>  focused on a number of relations of varying complexity using a text-bound annotation scheme. the performances achieved ranged from  <dig> to  <dig> in f-score, suggesting improvements in task definitions, data annotation and participating systems. following bionlp09st, the bionlp  <dig> shared task ge-nia task <dig>   <cit>  used the same event extraction task definition as it predecessor, but evaluated the submitted systems on event extraction from both abstracts and full papers. it had  <dig> participants with performances ranging from  <dig>  to  <dig>  in f-score.

methods
event extraction decomposition
each event consists of a trigger and one or more arguments. nine event types are defined which can be grouped in three categories, namely simple, binding and regulation. simple events include gene_expression, transcription, protein_catabolism, phosphorylation, and localization events. these have only one theme argument which is a protein. binding events have one or more protein themes. regulation events include positive_regulation, negative_regulation and regulation and are the most complex ones as they have one obligatory theme and one optional cause, each of which can be either a protein or another event, thus resulting in nested events. the protein names are annotated in advance and any token in a sentence can be a trigger for one of the nine event types considered. thus, the task can be viewed as a structured prediction problem in which the output for a given instance is a  directed acyclic graph  in which vertices correspond to triggers or protein arguments, and edges represent relations between them. in an example demonstrating the complexity of the task, given the passage ". . . sq  <dig> suppressed gp41-induced il- <dig> production in monocytes", systems should extract the three nested events shown in figure  <dig> 

trigger recognition
in trigger recognition, the system decides whether a token acts as a trigger for one of the nine event types or not. thus it is a 10-way classification task. we only consider tokens that are tagged as nouns, verbs or adjectives by the parser, as they cover the majority of the triggers in the data. this task is similar to word sense disambiguation, but it is simpler due to the restricted domain. the main features used in the classifier represent the lemma of the token which is sufficient to predict the event type correctly in most cases. in addition, we include features that conjoin each lemma with its part-of-speech tag. this allows us to handle words with the same nominal and verbal form that have different meanings, such as "lead". while the domain restricts most lemmas to one event type, there are some whose event type is determined by the context, e.g. "regulation" on its own denotes a regulation event but in "positive regulation" it denotes a positive_regulation event instead. in order to capture this phenomenon, we add as features the conjunction of each lemma with the lemma of the tokens immediately surrounding it, as well as with the lemmas of the tokens it has syntactic dependencies with.

theme and cause assignment
in theme assignment, we form an agenda of candidate trigger-argument pairs for all trigger-protein combinations in the sentence and classify them as themes or not. for each trigger-argument pair, a binary classifier is used to determine whether it has a theme relation or not. whenever a trigger is predicted to have a theme argument, we form candidate pairs between all the regulation triggers in the sentence and that trigger as the argument, thus allowing the prediction of nested events. also, we remove candidate pairs that could result in directed cycles, as they are not allowed by the task.

the features used to predict whether a trigger-argument pair should be classified as a theme are extracted from the syntactic dependency path and the textual string between them. in particular, we extract the shortest unlexicalized dependency path connecting each trigger-argument pair using dijkstra's algorithm, allowing the paths to follow either dependency direction. one set of features represent these paths and in addition we have sets of features representing each path conjoined with the lemma, the pos tag and the event type of the trigger, the type of the argument and the first and last lemmas in the dependency path. the latter help by providing some mild lexicalization. we also add features representing the textual string between the trigger and the argument, combined with the event type of the trigger. while not as informative as dependency paths, such features help in sentences where the parse is incorrect, as triggers and their arguments tend to appear near each other.

in cause assignment, we form an agenda of candidate trigger-argument between the regulation class triggers that were assigned at least one theme and the protein names and the other triggers that were assigned a theme. for each trigger-argument pair, a binary classifier is used to determine whether it has a cause relation or not. we extract features as in theme assignment, adding additional features representing the conjunction of the dependency path of the candidate pair with the path from the trigger to its theme.

event construction
in the event construction stage, we convert the predictions of the previous stages into events. if a binding trigger is assigned multiple themes, we choose to form either one event per theme or one event with multiple themes. for this purpose, we group the arguments of each nominal binding trigger according to the first label in their dependency path and generate events using the cross-product of these groups. for example, assuming the parse was correct and all the themes recognized, "interactions of a and b with c" would result in two binding events with two themes each, a with c, and b with c respectively. we add the exceptions that if two themes are part of the same token , or the trigger and one of the themes are part of the same token, or the lemma of the trigger is "bind" then they form one binding event with two themes.

finally, there are certain tokens such as "overexpress" that are consistently annotated with a simple event type and a regulation event type with the latter forming an event with the former as its theme. in the event extraction decomposition used, we predict one event type per token so it is not possible to produce this event structure. therefore, for the lemmas that have these additional regulation events, we generate them heuristically using a dictionary.

structured prediction with searn
searn  <cit>  forms the prediction of an instance s as a sequence of t multiclass predictions ŷ1:t made by a hypothesis h. the hypothesis consists of a set of classifiers that are learned jointly. each prediction ŷt can use features from s as well as from all the previous predictions ŷ1:t- <dig>  these multiclass predictions are referred to as actions and we adopt this term in order to distinguish them from the structured output prediction of an instance. the number of actions taken for an instance is not defined in advance but it is determined as the prediction is formed.

the searn algorithm is presented in alg.  <dig>  it initializes hypothesis h to the optimal policy π  which predicts the optimal action in each timestep t according to the gold standard. the optimal action at timestep t is the one that minimizes the overall loss over s assuming that all future actions ŷt+1:t are also made optimally. the loss function is defined by the structured prediction task considered. each iteration begins by making predictions for all instances s in the training data  . for each s and each action ŷt, a cost-sensitive classification  example is generated . the features are extracted from s and the previous actions ŷ1:t- <dig> . the cost for each possible action yti is estimated by predicting the remaining actions yt+1:t′ in s using h  and evaluating the cost incurred given that action . using a csc learning algorithm, a new hypothesis is learned  which is combined with the current one according to the interpolation parameter β . in order to interpolate between the learned hypotheses and the optimal policy we draw a random number between  <dig> and  <dig>  if it is less than iteration, then we use the optimal policy. otherwise, we use the learned hypotheses which is a weighted ensemble of the hypotheses learned in each iteration . the weights are set according to the equation in step  <dig>  which results in hypotheses learned in earlier rounds becoming less important their more recent counterparts.

in each iteration, searn moves away from the optimal policy and instead uses the learned hypotheses when predicting . thus, each hnew is adapted to the actions chosen by h instead of those of the optimal policy. when the dependence on the latter becomes insignificant , the algorithm terminates and returns the weighted ensemble of learned hypotheses without the optimal policy.

the interpolation parameter β determines how fast searn moves away from the optimal policy, and as a result, how many iterations will be needed to minimize the dependence on it. dependence in this context refers to the probability of using the optimal policy instead of the learned hypothesis in choosing an action during prediction. conversely, in each iteration, the features extracted  become progressively dependent on the actions chosen by the learned hypotheses instead of those of the optimal policy.

the decomposition of structured prediction into actions implies a search order. for some tasks such as part-of-speech  tagging, there is a natural left-to-right order in which the tokens are treated. however for others, including the task tackled in this paper, this ordering might not be appropriate. we discuss this issue in the next section.

structural information under searn is incorporated in two ways. first, via the costs that are estimated using the loss over the instance rather than isolated actions, e.g. counting how many incorrect pos tags will occur in the sentence if a given token is tagged as noun. second, via the features extracted from the previous actions, e.g. the pos tag predicted for the previous token can be a feature. note that such features would be possible in a standard pipeline as well, but during training they would have to be extracted using the gold standard instead of the actual predictions made by the classifiers, as they would be extracted during testing. while it is possible to use cross-validation to train a pipeline on its own predictions, however this is rarely done in practice.

finally, searn can be adapted to learn a pipeline of independently trained classifiers. to achieve this, β must be set to  <dig> so that there is only one iteration, the features that are dependent on previous actions must be removed, and the cost for each action must be set to  <dig> if it follows from the gold standard, or to  <dig> otherwise. this adaptation allows for a fair comparison between searn and a pipeline of independently learned classifiers.

biomedical event extraction with searn
in this section we describe how we use searn to learn the event extraction decomposition described earlier. each instance is a sentence and the hypothesis learned in each iteration consists of a classifier for each stage of the pipeline, excluding event construction which is rule-based. for this purpose we need to concretely define the way the prediction of a structured instance is performed , the optimal policy, and the method used to estimate the cost for each action .

algorithm  <dig> searn

1: input: labeled instances , optimal policy π, csc learning algorithm cscl, loss function ℓ

2: current policy h = π

3: while h depends significantly on π do

4:    examples e = ∅

5:    for s in s do

6:       predict h=ŷ <dig> ..ŷt

7:       for ŷt in h do

8:          extract features Φt=f

9:          for each possible action ytido

10:             predict yt+1:t′=h

11:             estimate cti=ℓ

12:       add  to e

13:    learn a classifier hnew = cscl

14:    h = βhnew + h

15: output: policy h without π

searn allows us to extract structural features for each action from the previous ones. during trigger recognition, we add as features the combination of the lemma of the token being classified and the event types  assigned to the previous and the next token, as well as the event type assigned to the tokens that have syntactic dependencies with the token being classified. during theme assignment, when considering a trigger-argument pair, we add features based on whether the pair forms an undirected cycle with previously predicted themes , whether the trigger has been assigned a protein as a theme and the candidate theme is an event trigger , and whether the argument is the theme of a trigger with the same event type. we also add a feature indicating whether the trigger has three themes predicted already, as triggers with more themes are rare. during cause assignment, we add features representing whether the trigger has been assigned a protein as a cause and whether the candidate cause is an event trigger.

since the features extracted for an action depend on previous ones, we need to define a prediction order for the actions. ideally, the actions predicted earlier should be less dependent on structural features and/or easier so that they can inform the more structure dependent/harder ones. in trigger recognition, we process the tokens from left to right since modifiers appearing before nouns tend to affect the meaning of the latter, e.g. "binding activity". in theme and cause assignment, we predict trigger-argument pairs in order of increasing dependency path length, assuming that, since they are the main source of features in these stages and shorter paths are less sparse, pairs containing shorter ones should be predicted more reliably. trigger-argument pairs with the same dependency path length are predicted according to the order they were added to the agenda, i.e. pairs with proteins as arguments are predicted before those that have other triggers as arguments. while we found this ordering of the actions to work well in practice, it could be improved by taking into account other properties of the trigger-argument, e.g. how frequently we encountered its dependency path in the training data.

the loss function sums the number of false positive and false negative events, which is the evaluation measure of the shared task. the optimal policy is derived from the gold standard and returns the action that minimizes the loss over the sentence given the previous actions and assuming that all future actions are optimal. in trigger recognition it returns either the event type for tokens that are triggers or a "no trigger" label otherwise. in theme assignment, for a given trigger-argument pair the optimal policy returns theme only if the trigger is recognized correctly and the argument is indeed a theme for that trigger according to the gold standard. in case the argument is another event, we require that its themes have been recognized correctly as well. in cause assignment, the requirements are the same as those for the themes, but we also require that at least one theme of the trigger in the trigger-argument pair to be considered correct. consequently, if a trigger is predicted with the wrong event type, the optimal policy would not assign any themes to it in order to avoid the false positive event it would incur. these additional checks are imposed by the task definition, under which events must have all their elements identified correctly. while the could reduce recall as the optimal policy avoids predicting some theme edges, it allows the algorithm to learn how to minimize the losses incurred due to its own wrong decisions.

cost estimation
cost estimation  is crucial to the successful application of searn. in order to highlight its importance, consider the example of figure  <dig> focusing on trigger recognition.

in the first iteration ), the actions for the sentence will be made using the optimal policy only, thus replicating the gold standard. during costing, if a token is not a trigger according to the gold standard , then the cost for all actions is  <dig>  as the optimal policy will not assign themes to a trigger with incorrect event type. such instances are ignored by the cost-sensitive learner.

in the second iteration ), the optimal policy is interpolated with the learned hypothesis, thus some of the actions are likely to be incorrect. assume that "sq" is incorrectly predicted to be a neg_reg trigger and assigned a theme. during costing, the action of labeling "sq" as neg_reg has a cost of  <dig>  as it would result in a false positive event. thus the learned hypothesis will be informed that it should not label "sq" as a trigger as it would assign themes to it incorrectly and it is adapted to handle its own mistakes. note that the costs for the other actions for that token remain  <dig>  assuming that the learned hypothesis would not assign themes to "sq" if it is predicted to be gene_exp, pos_reg or no_trigger. similarly, the action of labeling "production" as neg_reg in this iteration would incur a cost of  <dig>  as the learned hypothesis would assign a theme incorrectly, thus resulting in  <dig> false negative and  <dig> false positive events. therefore, the learned hypothesis will be informed that assigning the wrong event type to "production" is worse than not predicting a trigger.

the interpolation between the optimal policy and the learned hypothesis is stochastic, i.e. in each iteration beyond the first one the actions are taken either by the optimal policy or by the learned hypotheses, as described in the section "structured prediction with searn". therefore, the cost estimates obtained in steps  <dig> and  <dig> of alg.  <dig> vary according to the mistakes made by the learned hypothesis, thus affecting the cost estimates obtained. in order to obtain more reliable estimates, one can average over multiple samples for each action by repeating steps  <dig> and  <dig> of alg.  <dig>  however, the computational cost is effectively multiplied by the number of samples.

a different approach proposed by daumé iii  <cit>  is to assume that all actions following the one we are costing are going to be optimal and use the optimal policy to approximate the prediction of the learned hypothesis in step  <dig> of alg.  <dig>  in tasks where the learned hypothesis is accurate enough, this has no performance loss and it is computationally efficient as the optimal policy is deterministic. however, in event extraction, the learned hypothesis is likely to make mistakes, thus the optimal policy would not provide a good approximation to it. in the example of figure  <dig>  this approach would not alter the costs between the two iterations, as the optimal policy would avoid assigning themes to incorrectly recognized triggers, thus the learned hypothesis would not be informed of its mistakes.

in step  <dig> of alg.  <dig>  the cost of each action is estimated over the whole sentence. while this allows us to take structure into account, it can result in costs being affected by a part of the output that is not related to that action. this is likely to occur in event extraction, as sentences can often be long  and contain disconnected event components in their output graphs. for this reason we use focused costing  <cit> , in which the cost estimation for an action takes into account only the part of the output graph connected with that action. for example, in figure  <dig> the cost estimation for "sq" will ignore the events in the first iteration, while it will take them into account in the second one. seen differently, focused costing results in more reliable cost estimates than the standard costing  by reducing the number of actions taken into account.

csc learning with passive-aggressive algorithms
the searn framework requires a multiclass csc algorithm to learn how to predict actions. this algorithm must be computationally fast during parameter learning and prediction, as in every iteration we need to learn a new hypothesis and to consider each possible action for each instance in order to construct the cost-sensitive examples.

daumé iii et al.  <cit>  showed that it is possible to use any binary classification algorithm in order to perform multiclass csc. this is achieved by reducing multiclass csc to binary csc using the weighted all-pairs algorithm  <cit>  and in turn reducing csc to binary classification using the costing algorithm  <cit> . the main drawback of this approach is that it relies on multiple subsamplings of the training data, which can be inefficient for large datasets and many classes. zadrozny et al.  <cit>  observed that it is more efficient to incorporate the costs in the loss of the classifier when possible. this can be relatively straightforward in binary problems, but not in the multiclass ones.

with these considerations in mind, we implement a multiclass csc learning algorithm using the generalization of the online passive-aggressive  algorithm for binary classification  <cit> . for each training example xt, the k-class linear classifier with k weight vectors wt makes a prediction ŷt and suffers a loss ℓt. in the case of multiclass csc learning, each example has its own cost vector ct. if the loss is  <dig> then the weight vectors of the classifier are not updated . otherwise, the weight vectors are updated minimally so that the prediction on example xt is corrected . the update takes into account the cost of the mistake and the aggressiveness parameter , which allows the algorithm to handle noisy data. crammer et al.  <cit>  describe three variants to perform the updates which differ in how the learning rate τt is set for each example. in our experiments we used the variant named pa-ii with prediction-based updates. initial experiments showed little difference in accuracy between the variants, which is in agreement with the observations reported by crammer et al.

the full algorithm is presented in alg.  <dig>  since we are operating in a batch learning setting , we perform multiple rounds and average the weight vectors obtained, as in the averaged perceptron  <cit> . furthermore, since online learning depends on the order of the training examples but our data does not have a temporal aspect, we shuffle the examples in the beginning of each round.

RESULTS
in this section we compare the event extraction accuracy achieved by the system based on independently learned classifiers  versus the accuracy achieved by the system learning classifiers under searn. the purpose of these experiments is to assess the benefits of joint learning under searn. in the results reported below, we follow the dataset split of bionlp11st-ge <dig>  namely  <dig> abstracts and five full articles for training,  <dig> abstracts and five full articles for development, and  <dig> abstracts and five full articles for testing. to put these results in a wider context, we also compare against the other systems that participated in bionlp11st-ge <dig> 

for both independent and searn the aggressiveness parameter of pa and the number of rounds in parameter learning are set by tuning on 10% of the training set. for searn, we also set the interpolation parameter β to  <dig>  and use  <dig> iterations. thus, in the final iteration the probability of using the optimal policy is  <dig> ≈  <dig> . these parameters were tuned in preliminary experiments using the development data. for syntactic parsing, we use the output of the re-ranking parser  <cit>  adapted to the biomedical domain  <cit> , as provided by the shared task organizers in the stanford collapsed dependencies with conjunct dependency propagation  <cit> . the use of this publicly available resource allows for easy replication of our experiments. lemmatization is performed using morpha  <cit> . no other knowledge sources or tools are used. a pre-processing step we perform on the training data is to reduce the multi-token triggers in the gold standard to their syntactic heads. this procedure simplifies the task of assigning arguments to triggers and, as the evaluation variant used allows approximate trigger matching, it does not result in performance loss.

each row reports the results for each stage of the event extraction decomposition, with the last row containing the overall event extraction performance.

algorithm  <dig> passive-aggressive csc learning

1: input : training examples x=x <dig> ..xt, cost vectors c <dig> . . . ct ≥  <dig>  rounds r, aggressiveness 

2: initialize weights w0=

3: for r =  <dig> ..., r do

4:    shuffle 

5:    for xt∈xdo

6:          predict ŷt=argmaxk⋅xt)

7:          receive cost vector ct ≥ 0

8:          if ct>0then

9:          suffer loss ℓt=wt⋅xt-wt⋅xtct

10:          set learning rate τt=ℓt||xt||2+12c

11:          update wt+1=wt+τtxt

12:          update wt+1=wt-τtxt

13: average wavg=1t×r ∑i=0t×rwi

the results on the test dataset using searn are  <dig> / <dig> / <dig>   which would have ranked fourth in the shared task,  <dig>  f-score points below the best performing ensemble system faust  <cit> .  on the same dataset, the independent system achieves  <dig> / <dig> / <dig> , which while it would have ranked eighth in the shared task , it is  <dig>  f-score points below the result achieved with searn. in the full papers part of the corpus, our approach using searn would have ranked second with  <dig>  f-score points, slightly below the best reported performance at  <dig>  by umass  <cit> . while a direct comparison between learning frameworks is difficult due to the differences in task decomposition and feature extraction, we hypothesize that the superior performance of these systems is partly due to learning how to construct binding events, while our approach uses heuristics for this task. however, it is possible to model binding event construction with a classifier and learn it jointly with searn, which we leave for future work.

evaluation at varying recall-precision curves
in the previous section we evaluated the accuracy of the event extraction systems discussed using f-score, which by default favors balanced precision and recall scores. while searn achieves a better f-score than independent, it is important to note that they operate at different precision levels, with independent being substantially more precise at  <dig> % versus  <dig> %. therefore it is reasonable to ask whether searn achieves higher f-score simply because it operates at lower precision, thus if it was forced to operate at the same precision this would result in lower recall  than the one achieved by independent. in other words, the question we ask is whether searn learns more than a good set of thresholds for the classifiers used at each stage of the event extraction decomposition.

in this section, we explore the behavior of the two systems by adjusting the scores returned during prediction by the classifier used at each stage. in particular, we alter the score returned by the classifier for the negative class of each stage  by a parameter that can be either positive, thus resulting in over-generation, or negative, thus resulting in under-generation. each stage has its own parameter, thus each experimental run is defined by a set of three parameter values. altogether, we investigated  <dig>  sets of parameter values for both systems, the results of which we evaluate on the development data.

the results of figure  <dig> demonstrate that searn achieves better recall than independent at all precision levels. in particular, at 69% precision , searn achieves 44% recall versus 36%. this is also the case at even higher precision levels. for example, searn achieves 29% recall at 80% precision, compared to 13% by independent. finally, these observations are confirmed at the other end of the precision-recall trade-off. for example, at 20% precision searn achieves 54% recall compared to 43% by independent. thus we confirm that the improved predictive accuracy of searn is not only due to adjusting classification thresholds, but also due to generating appropriate training examples and learning structural feature weights.

controlling the trade-off between recall and precision
in the previous section we adjusted the trade-off between precision and recall in order to obtain a more complete comparison between classifiers learned with searn and classifiers learned independently. the ability to adjust this trade-off is of interest to users of event extraction systems, as they frequently need to adapt the behavior of a system to particular needs. for example, if a system is going to be used to populate a knowledge base whose users are not expected to verify its contents, then precision is more important than recall. conversely, if it is going to be used by users to navigate through the biomedical literature and recover rarely mentioned facts about proteins, then recall is more important than precision. similar observations were made in the context of biomedical named entity recognition by carpenter  <cit> .

as described earlier, the results reported in the previous section were obtained by evaluating the effect of  <dig>  sets of parameter values to adjust the classification scores at each stage. it is important to note that the effect of these values is not straightforward to anticipate, as it is difficult to predict how the classifiers for each stage will interact with each other. for example, it is impossible to know in advance how to adjust the theme assignment classifier if we adjust the trigger recognizer to over-generate. furthermore, in the case of complex classification pipelines, the effect of each parameter can be hard to predict, e.g. while over-generating causes is expected to increase recall since it results in more events predicted, it could also have the opposite effect, as it can change correctly extracted regulation events that do not have such arguments into incorrect ones. these issues are visible in the results of figure  <dig>  where for each recall level there is a range of precision values obtained by each system, some of them well below the best one or even  <dig> 

in order to choose the best set of parameter values at various precision levels, it is customary to use a development set, as done in order to report the results in the previous section. however this is not always desirable, as it requires part of the available annotated data to be withheld for this purpose and thus not used for training. furthermore, the procedure to find the parameter values that result in the desired trade-off between recall and precision must be repeated each time there is a change in the event extraction system.

instead of adjusting classification scores, it is possible under searn to adjust the trade-off between precision and recall via the loss function ℓ used to estimate the cost of each action . in our approach, as described in the section "biomedical event extraction with searn", the loss function is the sum of the numbers of false positives and false negatives. therefore, in order to learn a system with higher precision, we multiply the number of false positives with a positive weight, and conversely, in order to learn a system with higher recall we multiply the number of false negatives with a positive weight.

the results obtained using searn with different weights on false positives and false negatives are shown in figure  <dig>  in each experiment, one of these weights was kept to  <dig>  while the other one was set to  <dig> .. <dig>  thus resulting in  <dig> experiments in total. it can be observed that in all cases the trade-off achieved is a reasonable one, i.e. favoring precision over recall never results in the latter becoming prohibitively low, as well as the reverse. this demonstrates that the classifiers learned jointly under searn are adapted to each other in order to adjust the balance between precision and recall. the benefits of this method are more pronounced at the higher recall levels, for example it obtains 54% recall at 41% precision, while the same recall was possible only at 18% precision in the previous section. furthermore, while the trade-off achieved at high precision levels is not always as good as the one obtained by adjusting the scores of the classifiers directly, it is never substantially worse. most importantly, using weights on false positives and false negatives in the loss function is very stable and thus it can be used without a development set.

experiments with domain adaptation
bionlp11st-ge <dig> evaluated event extraction on abstracts and full papers. while the annotation guidelines used were the same, full papers are likely to contain richer vocabulary and linguistic phenomena than the abstracts. since we have annotated data for both we decided to address this task as a supervised domain adaptation problem. we experimented with the domain adaptation method proposed by daumé iii  <cit> , which creates multiple versions for each feature by conjoining it with the domain label of the instance it is extracted from . for example, during trigger recognition, the feature representing the lemma of a token becomes three features: the original lemma feature, a lemma feature for the abstracts domain and a lemma feature for the full papers domain. for each token, only two of these features will be active, according to the domain of the sentence the token is found in. for example, if the token is found in a sentence of an abstract, only the original lemma feature and the abstracts domain lemma feature will be active.

in our experiments, this simple domain adaptation method improved the accuracy of the classifiers trained under searn by  <dig>  f-score points on the development and  <dig>  f-score points on the test set, mainly by improving accuracy on the abstracts while preserving the already high accuracy on the full papers. this improvement is due to the domain-specific versions of the features that allow the flexibility to model the particularities of each domain independently. this version of the system would have ranked third overall with  <dig>  f-score points, and second if we do not take the best-performing ensemble system faust  <cit>  into consideration. table  <dig> contains detailed results per event type, event class and domain. in the regulation events that are more difficult to extract it would have ranked third overall and in the regulation events of the full papers it would have ranked first with  <dig>  f-score points,  <dig>  points better than the best-performing ensemble system faust. we hypothesize that the relatively limited impact of domain adaptation is due to the sparse features used in the stages of the event extraction decomposition, which become even sparser using this domain adaptation method, thus rendering the learning of appropriate weights for them harder.

CONCLUSIONS
we presented a joint inference approach to the bionlp11st-ge <dig> event extraction task using searn which converts a structured prediction task into a set of csc tasks whose models are learned jointly. our results demonstrate that searn achieves substantial performance gains over independently learned classifiers using the same features at all precision levels. furthermore, we suggested an efficient method to adjust the trade-off between recall and precision under searn in order to accommodate different usage scenarios. finally, we were able to improve our performance further using a simple domain adaptation method in order to handle the differences between abstracts and full papers. in the course of our experiments, we reported the second-best event extraction results by a single system.

competing interests
the authors declare that they have no competing interests.

authors' contributions
av wrote the code and ran the experiments. both authors were involved in designing the approach and writing the manuscript.

