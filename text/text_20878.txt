BACKGROUND
the background section presents the importance of computing elementary modes for metabolic system analysis, its computational difficulties and the existence of various known algorithms. a theoretical section brings these algorithms into a unified framework. in a following section we introduce a new approach, called the binary approach. although relying on concepts introduced in the theoretical section, this section gives enough practical details to be stand-alone for the implementer. results obtained from example networks and a conclusion section close the article.

definition and benefits of elementary modes
we consider a metabolic network with m metabolites and q reactions. reactions may involve further metabolites that are not considered as proper members of the system of study. the latter metabolites, considered to be buffered, are called external metabolites in opposition to the m metabolites within the boundary of the system, called internal metabolites. the stoichiometry matrix n is an m × q matrix whose element nij is the signed stoichiometric coefficient of metabolite i in reaction j with the following sign convention: negative for educts, positive for products. some reactions, called irreversible reactions, are thermodynamically feasible in only one direction under the normal conditions of the system. therefore, reaction indices are split into two sets: irrev  and rev . a flux vector , denoted v, is a q-vector of the reaction space q, in which each element vi describes the net rate of the ith reaction. sometimes we are interested only in the relative proportions of fluxes in a flux vector. in this sense, two flux vectors v and v' can be seen to be equivalent, denoted by v ≃ v', if and only if there is some α >  <dig> such that v = α · v'.

metabolism involves fast reactions and high turnover of substances compared to events of gene regulation. therefore, it is often assumed that metabolite concentrations and reaction rates are equilibrated, thus constant, in the timescale of study. the metabolic system is then considered to be in quasi steady state. this assumption implies nv =  <dig>  thermodynamics impose the rate of each irreversible reaction to be nonnegative. consequently the set of feasible flux vectors is restricted to

p = {v ∈ q : nv =  <dig> and vi ≥  <dig>  i ∈ irrev}     

p is a set of q-vectors that obey a finite set of homogeneous linear equalities and inequalities, namely the |irrev| inequalities defined by vi ≥  <dig>  i ∈ irrev and the m equalities defined by nv =  <dig>  p is therefore – by definition – a convex polyhedral cone  <cit> .

metabolic pathway analysis  <cit>  serves to describe the  set p of feasible states by providing a  set of vectors that allow the generation of any vectors of p and are of fundamental importance for the overall capabilities of the metabolic system. one of this set is the so-called set of elementary  modes . for a given flux vector v, we note r = {i : vi ≠ 0} the set of indices of the reactions participating in v. hence, r can be seen as the underlying pathway of v. by definition, a flux vector e is an elementary mode  if and only if it fulfills the following three conditions  <cit> :



in other words, e is an em if and only if it works at quasi steady state, is thermodynamically feasible and there is no other non-null flux vector  that both satisfies these constraints and involves a proper subset of its participating reactions. note that with this convention, reversible modes are here considered as two vectors of opposite directions.

the concept of elementary modes  has proven useful in many ways and has become an important theoretical tool for systems biology as well as for biotechnology and metabolic engineering . because the metabolic network structure becomes now available at a genome-scale for an increasing number of microorganisms, this approach is well-suited to today's metabolic studies. here, we give a short overview on the major applications and variants:

 identification of pathways: the set of ems comprises all admissible routes through the network and thus of "pathways" in the classical sense, i.e. of routes that convert some educts into some products  <cit> .

 network flexibility: the number of ems is at least a rough measure of the network's flexibility  to perform a certain function  <cit> .

 identification of all pathways with optimal yield: consider the linear optimization problem, where all flux vectors with optimal product yield are to be identified, i.e. where the moles of products generated per mole of educts is maximal. then, one or several of the ems reach this optimum and any optimal flux vector is a convex combination of these optimal ems  <cit> .

 importance of reactions: the importance or relevance of a reaction can be assessed by its participation frequency or/and flux values in the ems.

 inference of viability of mutants: if a reaction is involved in all growth-related ems its deletion can be predicted to be lethal, since all ems would disappear  <cit> .

 a more quantitative measure of the importance of a reaction has been given by "control-effective fluxes" . the cefs take also the efficiency of each mode as well as the absolute flux values of the respective reaction in the ems into account. cefs have been used to predict transcript ratios  <cit> .

 reaction correlations: ems can be used to analyze structural couplings between reactions, which might give hints for underlying regulatory circuits  <cit> . an extreme case is an enzyme  subset  or a pair of mutually excluding reactions .

 detection of thermodynamically infeasible cycles: ems representing internal cycles  are infeasible by laws of thermodynamics and thus reflect structural inconsistencies  <cit> .

 the framework of pathway analysis also allows us to combine and to study stoichiometric constraints together with regulatory rules  <cit> .

 minimal cut sets: ems allow for a computation of minimal cut sets that represent minimal cuts  in the network repressing certain metabolic functions  <cit> .

 the α-spectrum has been introduced to quantify the involvement of extreme pathways in a particular flux distribution   <cit> . since the decomposition of a flux vector into extreme pathways is usually not unique, the α-spectrum specifies a range of possible weights for each extreme pathway. the same could be defined for ems.

computational limitations and algorithm variants
due to the combinatorial explosion in the number of ems in large networks  <cit> , computing ems is known to be a hard computational task, so far restricting elementary-mode analysis to medium-scale networks. several algorithms  have been developed for computing ems. the two most prominent ones are the algorithm elaborated by schuster et al.  <cit>  and the recently developed null-space approach by wagner  <cit> . the latter considerably accelerated the computation speed and thus shifted the current limitation – at least for a typical pc – from computation time to the memory requirement.

here we show that both the schuster algorithm as well as that by wagner can be embedded in a more general algorithmic framework stemming originally from computational geometry. these studies do not only give a summarizing point of view, they also lead to a crucial modification of the existing algorithms, decreasing the required memory for computing and storing ems drastically.

RESULTS
a unified framework
elementary modes as extreme rays in networks of irreversible reactions
in the particular case of a metabolic system with only irreversible reactions, the set of admissible reactions reads:

p = {v ∈ q : nv =  <dig> and v ≥ 0}     

compared with  p is in this case a particular, namely a pointed polyhedral cone . this geometry can be intuitively understood, noting that there are certainly 'enough' intersecting half-spaces  to have this 'pointed' effect in 0: p contains no real line . the figure even suggests that a pointed polyhedral cone can be either defined in an implicit way, by the set of constraints as we did until now, or in an explicit or generative way, by its 'edges', the so-called extreme rays  that unambiguously define its boundaries. in the following, we show that elementary modes always correspond to extreme rays of a particular pointed cone as defined in  and that their computation therefore matches to the so-called extreme ray enumeration problem, i.e. the problem of enumerating all extreme rays of a pointed polyhedral cone defined by its constraints. an overview on general and current issues on extreme ray enumeration can be found in  <cit> . for the sake of consistency, we use this reference as a main source and adopt the same mathematical notations.

the pointed polyhedral cone is the central mathematical object throughout this work; therefore we shall introduce more precise definitions and results surrounding it.

p is a pointed polyhedral cone of d if and only if p is defined by a full rank h × d matrix a  = d) such that,

p = p = {x ∈ d : ax ≥ 0}     

the h rows of the matrix a represent h linear inequalities, whereas the full rank mention imposes the "pointed" effect in  <dig>  note that a pointed polyhedral cone is, in general, not restricted to be located completely in the positive orthant as in . for example, the cone considered in extreme-pathway analysis may have negative parts , however, by using a particular configuration it is ensured that the spanned cone is pointed  <cit> .

now we must characterize the extreme rays. a vector r is said to be a ray of p if r ≠  <dig> and for all α >  <dig>  α · r ∈ p. we identify two rays r and r' if there is some α >  <dig> such that r = α · r' and we denote r ≃ r', analogous as introduced above for flux vectors. for any vector x in p, the zero set or active set z is the set of inequality indices satisfied by x with equality. noting ai• the ith row of a, z = {i : ai•x = 0}. zero sets can be used to characterize extreme rays. for simplicity, we adopt in this document the following characteristic  as a working definition of extreme rays.

definition 1: extreme ray

let r be a ray of the pointed polyhedral cone p. the following statements are equivalent:

 r is an extreme ray of p

 if r' is a ray of p with z ⊆ z then r' ≃ r

since a is full rank,  <dig> is the unique vector that solves all constraints with equality. the extreme rays are those rays of p that solve a maximum but not all constraints with equalities. this is expressed in  by requiring that no other ray in p solves the same constraints plus additional ones with equalities. note that in  z = z consequently holds.

an important property of the extreme rays is that they form a finite set of generating vectors of the pointed cone : any vector of p can be expressed as a non-negative linear combination of extreme rays, and the converse is true: all non-negative combinations of extreme rays lie in p. moreover, the set of extreme rays is the unique minimal set of generating vectors of a pointed cone p .

lemma 1: ems in networks of irreversible reactions

in a metabolic system where all reactions are irreversible, the ems are exactly the extreme rays of p = {v ∈ q : nv =  <dig> and v ≥ 0}.

proof: p is the solution set of the linear inequalities defined by  where i is the q × q identity matrix. since it contains i, a is full rank and therefore p is a pointed polyhedral cone. all v ∈ p obey nv =  <dig>  thus the 2m first inequalities defined by a hold with equality for all vectors in p and the inclusion condition of definition  <dig> can be restricted to the last q inequalities, i.e. the inequalities corresponding to the reactions. inclusion over the zero set can be equivalently seen as containment over the set of non-zeros in v, i.e. r. consequently, e ∈ p is an extreme ray of p if and only if: for all e' ∈ p : r ⊆ r ⇒ e' =  <dig> or e' ≃ e, i.e. if and only if e is elementary. thus, all three conditions in  are fulfilled.

the general case
in the general case, some reactions of the metabolic system can be reversible. consequently, a does not contain the identity matrix and p ) is not ensured to be a pointed polyhedral cone anymore  <cit> . because they contain a linear subspace, non-pointed polyhedral cones cannot be represented properly by a unique set of generating vectors composed of extreme rays, albeit a set of generating vectors exists, sometimes also called convex basis  <cit> . one way to obtain a pointed polyhedral cone from  is to split up each reversible reaction into two opposite irreversible ones. note that this operation is completely analogous to a transformation step used in linear programming to obtain a linear optimization problem in canonical form: free variables v are also split into two variables v+ and v- with v = v+ - v- and v+, v- ≥  <dig>  <cit> . it has been noticed earlier that this virtual split does not change essentially the outcome: the ems in the reconfigured network are practically equivalent to the ems from the original network  <cit> . here we prove and precisely characterize this result.

we first introduce some notations. we denote the original reaction network by s and the reconfigured network  by s'. the reactions of s are indexed from  <dig> to q. remember that irrev denotes the set of irreversible reaction indices and rev the reversible ones. an irreversible reaction indexed i gives rise to a reaction of s' indexed i. a reversible reaction indexed i gives rise to two opposite reactions of s' indexed by the pairs  and  for the forward and the backward respectively. the reconfiguration of a flux vector v ∈ q of s is a flux vector v' ∈ irrev ∪ rev × {-1;+1} of s' such that



let n' be the stoichiometry matrix of s'. n' can be written as n' =  where nrev consists of all columns of n corresponding to reversible reactions. note that if v is a flux vector of s and v' is its reconfiguration then nv = n'v'.

if possible, i.e. if v' ∈ irrev ∪ rev × {-1;+1} is such that for any reversible reaction index i ∈ rev at least one of the two coefficients v' or v' equals zero, then we define the reverse operation, called back-configuration that maps v' back to a flux vector v such that:



theorem 1: ems in original and in reconfigured networks

let s be a metabolic system and s' its reconfiguration by splitting up reversible reactions. then the set of ems of s' is the union of

a) the set of reconfigured ems of s

b) the set of two-cycles made of a forward and a backward reaction of s' derived from the same reversible reaction of s

proof: see methods.

thus, the set of ems of the original network is equivalent  to the set of ems in the reconfigured network and therefore can be seen as a reduced set of extreme rays of the pointed convex polyhedron as defined by:

p = {v' ∈ q + |rev| : n'v' =  <dig> and v' ≥ 0}     

hence, ems computation can be derived from any extreme ray enumeration algorithm applied to the reconfigured network and followed by vector back-configuration and the elimination of meaningless vectors, namely the two-cycles.

note that exactly the same procedure – splitting reversible reactions into two irreversible ones – was carried out also in the original work of clarke  <cit>  on stability analyses in stoichiometric networks. clarke called the extreme rays of the corresponding cone  extreme currents. thus, extreme currents are identical to the ems in the reconfigured network and, hence, also  equivalent to the ems from the original network

all known algorithms for computing ems are variants of the double description method
in the following we present a simple yet efficient algorithm for extreme ray enumeration, the so-called double description method  <cit> . we show that it serves as a common framework to the most prominent em computation methods. to reach this generality, we concentrate on mathematical operations regardless to the actual data-structures used in the implementation. therefore we manipulate objects such as matrices, vectors or inequalities and leave their implementation into tableaus, arrays and so on to the next section.

a generating matrix r of a pointed polyhedral cone p is a matrix such that p = {x ∈ d : x = rλ for some λ ≥ 0}. the pair  is called a double description pair, or dd pair. as mentioned above, the extreme rays form the unique set of minimal generating vectors of p and thus, considered as set of d-vectors, the extreme rays of p form the columns of a generating matrix r that is minimal in terms of number of columns. the pair  is then called a minimal dd pair.

the strategy of the double description method is to iteratively build a minimal dd pair  from a minimal dd pair , where ak is a submatrix of a made of k rows of a. at each step the columns of rk are the extreme rays of p, the convex polyhedron defined by the linear inequalities ak. the incremental step introduces a constraint of a that is not yet satisfied by all computed extreme rays. some extreme rays are kept, some are discarded and new ones are generated. the generation of new extreme rays relies on the notion of adjacent extreme rays. here again, for the sake of simplicity, we adopt a characteristic  as a working definition of adjacent extreme rays.

definition 2: adjacent extreme rays

let r and r' be distinct rays of the pointed polyhedral cone p. then the following statements are equivalent:

 r and r' are adjacent extreme rays

 if r" is a ray of p with z ∩ z ⊆ z then either r" ≃ r or r" ≃ r'

initialization
the initialization of the double description method must be done with a minimal dd pair. one possibility is the following. since p is pointed, a has full rank and contains a nonsingular submatrix of order d denoted by ad. hence,  is a minimal dd pair which works as initialization and leads directly to step k = d. note that there is some freedom in choosing a submatrix ad or some alternative starting minimal dd pair.

incremental step
assume  is a minimal dd pair and consider a kth constraint defined by a not yet extracted row of a, denoted ai•. let j be the set of column indices of rk -  <dig> and rj, j ∈ j, its column vectors, i.e. the extreme rays of p, the polyhedral cone of the previous iteration. ai• splits j in three parts  whether rj satisfies the constraint with strict inequality , with equality  or does not satisfy it :

j+ = {j ∈ j : ai•rj > 0}

j <dig> = {j ∈ j : ai•rj = 0}     

j- = {j ∈ j : ai•rj < 0}

minimality of rk is ensured in considering all positive rays, all zero rays and new rays obtained as combination of a positive and a negative ray that are adjacent to each other  <cit> . for convenience, we denote by adj the index set of the newly generated rays in which every new ray is expressed by a pair of indices corresponding to the two adjacent rays combined. hence, rk is defined as the set of column vectors rj, j ∈ j' with



the incremental step is repeated until k = h i.e. having treated all rows of the matrix a. the columns of the final matrix rm are the extreme rays of p.

computing ems
the double description method together with theorem  <dig> offers a framework for computing ems. the only steps to include are a reconfiguration step that splits reversible reactions and builds the matrix a, and a post-processing step that gets rid of futile two-cycles and computes the back-configuration. the dimension of the space is given by the number of reactions in the reconfigured network: q' = q + |rev|. this results in the general algorithmic scheme as given in table  <dig> :

n ← reconfigured stoichiometry matrix 
aq ← q independent rows of a
for each unprocessed row ai• of a do
  j+ ← {j ∈ j : ai•rj > 0}
  j <dig> ← {j ∈ j : ai•rj = 0}   
 j- ← {j ∈ j : ai•rj < 0} 
  r' ← {rj : j ∈ j <dig> ∪ j+}    

        end if
 end for
 r ← r'
r ← r \ { futile two-cycles }
as mentioned in the introduction section, the two most efficient algorithms for computing ems available are the recently introduced null-space approach  <cit>  and the schuster algorithm  <cit> , that we call "canonical basis approach" . both algorithms handle reversible reactions directly. a direct handling of reversible reactions, meaning without network reconfiguration, is feasible in each setting and has been described in the respective original articles. this requires adapted adjacency tests. however, it does not affect the overall strategy. for simplicity, we describe these algorithms with networks of irreversible reactions only . we are now able to see that the algorithms of schuster and wagner differ basically only in the chosen initialization for r.

the canonical basis approach 
the matrix i represents q independent rows extracted from a = t and can thus be used for aq. the matrix aq- <dig> = i- <dig> = i gives the q extreme rays that obey to these q independent constraints and works as initialization of r.

the remaining constraints are 2m linear inequalities defined by nr ≥  <dig> and -nr ≥  <dig>  i.e. m equalities: nr =  <dig>  the processing of an equality constraint is done in a single pass by only keeping rays of j <dig> instead of j+ ∪ j <dig>  this is achieved by replacing the line r' ← {rj : j ∈ j+ ∪ j0} with r' ← {rj : j ∈ j0} in the part "processing of constraints in a given order" in table  <dig>  note that in the original schuster algorithm the values of arj  are explicitly stored throughout the algorithm  and adapted after each iteration.

the null space approach 
the idea there is to initialize r by a well-defined kernel  matrix k of n with a particular structure  row-echelon form):



which can be computed, for example, by the matlab command null. one can assume n to be of rank m, the opposite case being discussed below . this implies  to be of size m ×  and the identity of size q - m. this structure is obtained by allowing a reordering of the rows of k, i.e. of the reaction indices. without losing generality, one can assume that the reactions corresponding to the block i are indexed from  <dig> to q - m. consider the  × q matrix . for all x in p, there is some vector λ ≥  <dig>  such that x = kλ. reciprocally, for all λ ≥  <dig>  the vector x = kλ lies in p. thus  is a dd pair. since k is a kernel matrix, its columns are independent vectors therefore  is a minimal dd pair. k as defined in  works as initial value for r. hence, the initialization in this setup delivers directly k = q + m solved constraints.

the remaining constraints are m linear inequalities defined by ri ≥  <dig>  i = q - m +  <dig> ..q. the gaussian elimination step simplifies too



the right hand-side is practically a positive combination of the two vectors  and , because  is positive and  negative due to the definitions of j+ and j-.

adjacency tests
here we give explicitly the adjacency test in the case of reconfigured networks for each setup. variants handling reversible reactions directly were introduced for cba and nsa. they lead in general to more complex algorithmic steps for a little  memory gain.

the test is used when processing the constraint k +  <dig> to check whether two extreme rays r and r' of the cone p are adjacent. the adjacency test is based on definition  <dig>  note that for a given extreme ray r of the cone p, the considered zero set z is defined over the k constraints ak.

cba: as mentioned above, in a cba setup, equality constraints are solved within a single iteration. after the l-th iteration step, k = q + 2l constraints are processed, therefore . the last 2l constraints are satisfied with equality for all computed rays. we denote by zu the zero set of a vector v over the u first constraints. here, with u = q it matches to the set of non-participating reactions in v. the adjacency test is then equivalent to the search of a third extreme ray r" such that zq ∩ zq ⊆ zq. if such an r" exists, then r and r' are not adjacent.

nsa: after the l-th iteration step in an nsa setup, k = q + m + l constraints including p = q - m + l sign constraints are processed. thus . the last 2m constraints are satisfied with equality for all computed rays. therefore, the adjacency test is then equivalent to the search of a third extreme ray r" such that

zp ∩ zp ⊆ zp     .

thus, for nsa we only have to check the first p elements of the rays, in contrast to all q elements for cba. this is one reason behind the relative velocity of nsa compared to cba.

on redundancies and network compression
it is common practice to reduce the problem of extreme ray enumeration by restricting the input set to the set of irredundant constraints  <cit> . although the general problem of extreme ray enumeration is non-polynomial, the reduction into irredundant constraints is equivalent to linear programming and therefore of polynomial complexity. to our best knowledge, this important pre-processing has never been spelled out explicitly in the context of em computation. however some network simplification steps have been proposed earlier  <cit>  that deeply relate to the notion of redundancy removal. these simplifications include three heuristics that reduce the size of the original stoichiometric matrix n and thus the input size of the problem: the detection of conservation relations, of strictly detailed balanced reactions and of enzyme subsets.

conservation relations of metabolites are captured as linear dependencies between rows of the stoichiometry matrix n . this implies that some of the equality constraints in nr =  <dig> are linearly dependent. satisfying a maximal linearly independent subset of these equations suffices to satisfy all equations. therefore the problem can be reduced to , where  is the reduced stoichiometry matrix. for example, in figure 3a, metabolites b and c build up one conservation relation and thus one of these metabolites can be removed. note that conservation relations need not to be considered explicitly in the null-space approach since their removal does not affect the computed null-space matrix.

conservation relations only consider redundancies among the equalities. the general approach handles also inequality constraints. strictly detailed balanced reactions  <cit>  and enzyme subsets  <cit>  are particular cases of such redundancies. strictly detailed balanced reactions are reactions with null flux at any steady-state. many of them can be identified as null row vectors of k, the kernel matrix of n, and can be eliminated from the system. a non-trivial example is shown in figure 3b, where r <dig> is strictly detailed balanced and would be detected by using the kernel matrix. however, there may be further reactions with a fixed zero-flux in steady state that cannot be identified by k. some of those can be found by a simple analysis of n. for example, all the uni-directional reactions pointing into an internal sink  are certainly not participating in any steady-state flux .

an enzyme subsets is defined as a group of reactions with relative constant flux ratio at steady state. many of them can be identified as row vectors of k differing only in a scalar factor α. reactions r <dig>  r <dig> and r <dig> in figure 3d would represent one enzyme subset. assume one works on the reconfigured network and reactions r <dig> and r <dig> are members of the same enzyme subset. thus, at steady state, we have for the respective rates r <dig> = α · r <dig>  if α >  <dig>  the constraints r <dig> ≥  <dig> and r <dig> ≥  <dig> are redundant, r <dig> ≥  <dig> being sufficient. in that case the practice is to lump both reactions into one lowering the number of reactions . if α <  <dig>  the constraints r <dig> ≥  <dig> and r <dig> ≥  <dig> imply r <dig> = r <dig> =  <dig>  hence, a special case of strictly detailed balanced reactions. in this case we say that the reactions contradict each other. both reactions are not used and can be eliminated from the system as reactions r <dig> and r <dig> in figure 3e.

we identified another kind of redundancies. we call a metabolite m uniquely produced  if only one single reaction, say i, can produce  m for several consuming  it . in that case, balancing metabolite m at steady-state implies that ri is always non-zero whenever the other reactions connected to m are active. we can therefore lump each reaction consuming  m with reaction i and remove metabolite m, decreasing the dimension of the problem further . note that some enzyme subsets and strictly detailed balanced reactions can be seen as special cases of this type of redundancy.

elimination of redundancies and network compression should be done in a pre-processing step leading to a compressed network structure. thereby, it is important to detect and remove such redundancies iteratively until no further redundancy can be found. a matlab function compresssmat which removes all redundancies discussed above in an iterative fashion can be obtained from the corresponding author. after the computation of ems, lumped reactions can be expanded to their single components.

there is a general approach for identifying redundancies in a set of linear constraints that uses linear programming, for example with the software redund distributed together with the software lrs  <cit> . this approach does not require any iterative process, but only identify redundant inequalities. rows of a can be eliminated but no consequent column-wise reduction is done. therefore, a simple redundancy removal is not as powerful as the accompanying network compressions presented above. the method however has the advantage to be systematic and might lead in the future to further network simplifications not yet identified.

the binary approach
general idea
using the reconfigured network with only irreversible reactions we have shown that the most important algorithms for em computation belong to the same general framework. however, the original algorithms from schuster and wagner operate directly on the original network without splitting reversible reactions. at a first look, this seems to be more efficient since the dimension  is lower, decreasing seemingly also the memory requirement and the costs for adjacency tests. however, using the reconfigured network s' offers great simplifications. first, as already mentioned in an earlier section, the adjacency tests are easier to handle. the most important advantage, however, is the following. for the cba in s' it follows that all non-zero elements of a ray rk will be retained if a new ray is obtained by combining rk with another  ray because only positive combinations of rays are performed. the same holds for the nsa with respect to the p already processed inequality  constraints. this is of great importance since the adjacency test requires the information on zero/non-zero places in the rays only.

we illustrate this idea for nsa because this approach turned out to be more efficient than cba. we assume that n has full rank m, i.e. there is no conservation relation. in this section, all variables correspond again to the network with split reversible reactions.

as described above, for an initialization of r we use a kernel matrix k of n having form :



note that we use here the transposed representation of the tableau compared to wagner's original article  <cit> . since by eq.  only positive column combinations are performed during the algorithm, no negative number can show up in the upper part ) during the next iterations. the first row to be processed now is p = q - m +  <dig>  using the general algorithmic scheme provided above all rays with non-negative entries at row p are retained and all negative entries can be combined with positive ones that are adjacent to them to obtain a zero at position p.

assuming that the procession of the p-th row leads to a collection of t rays, we have:



the upper part, r <dig>  contains the p processed rows which only contain non-negative values. again, positive combinations of rays performed during the next iterations lead in the upper part to sums of non-negative numbers. hence, it is easy to keep track of the zeroes in the upper part r <dig> by the use of bit masks. after the procession of the p-th inequality constraint the p-th row  can be transformed to its binary representation and moved from r <dig> to r <dig>  using a binary representation for r <dig> has many advantages:

 for the next row p +  <dig> to be processed we have to perform the adjacency test for pairs of vectors , . this test only requires the first p elements of these rays ), hence, exactly the columns of r <dig>  test  can then be written as a simple  bit operation. two distinct vectors ,  are adjacent if and only if for all vectors rk distinct from  and , it holds:



. of course, the identical terms in the parentheses are computed only once.

 combination step of two adjacent rays ) reduces for the part in r <dig> to a simple or operation, which is already computed for . the other  components of the two rays  are combined as usual by eq. .

 bit operations as applied in r <dig> are not only fast, they are numerically exact in contrast to operations on real numbers.

 the binary representation requires much less memory. taking a typical 64-bit floating-point variable, storing r <dig> binary takes only  <dig>  % of the memory needed for real numbers. taking into account that in the worst case  the number of reactions in the reconfigured network is twice of that from the original one we still have a reduction in memory requirements of more than 96%. note that r <dig> is empty at the end of the algorithm, hence, all ems are then stored binary.

bitmap representations of ems have already been used in earlier implementations for accelerating the adjacency  tests. however, binary tableaus had then been stored and updated in parallel to the full  tableau of ems which is not necessary here.

after the whole processing, ems  are obtained for the reconfigured network s' as binary vectors. binary patterns of ems are completely sufficient for many applications of ems . however, a well-known lemma  ensures that this information is also sufficient to retrieve the real values up to a positive scalar:

lemma 2

in a d-dimensional euclidean space, let r be a ray of the pointed polyhedral cone p. the following statements are equivalent:

 r is an extreme ray of p

 rank) = d - 1

each obtained binary vector provides the zero set zq and its complement the reaction set r of an em e in the reconfigured network s'. lemma  <dig> says that the equation  and therefore

nrer =  <dig>     

admit a one-dimensional solution space, i.e. the dimension of the null space of nr is  <dig>  nr denotes the m × |r| sub-matrix of n containing all those reactions  of n which are involved in e. solving the homogeneous linear system  gives a vector that can be normalized and properly oriented for example by dividing it by the value on its first participating reaction . the reconstruction process reflects the fact that an em is – up to a scalar – determined by its participating reactions.

in a second post-processing step, we transform the  ems of s' back into their representation in the original network s by using the rules given before theorem  <dig>  note that it is also possible to transform first the binary ems from s' into the binary ems of s and then to reconstruct the real numbers  for the stoichiometric matrix of the non-reconfigured network s; see pseudo-code). in both cases, if the original network had been compressed during pre-processing, the ems can finally be expanded to their corresponding modes in the uncompressed network.

pseudo-code of the binary  approach
using the results of the previous sections we are now able to give a pseudo-code of the binary  approach . the code follows matlab style, which provides a convenient and comprehensible notation for operations on vectors and matrices. we use several native matlab routines . for concision, we also make the use of some other routines . the code of the latter routines is not given here explicitly but their names and accompanying comments should allow the reader to implement them. for readers not familiar with matlab notation we give in the methods section some basic explanations which should suffice for understanding the pseudo-code.

note that the pseudo-code in figure  <dig> is not given in its computationally most efficient form. it should just present the basic structure of the algorithm. there are two important issues in the algorithm we still have to discuss.

minimal number of zeros in extreme rays 
in the null-space approach, the m equality constraints are always solved for each ray during the procession of sign constraints. since any ray satisfies by lemma  <dig> at least a total of q -  <dig> constraints, this implies that at least q-1-m sign restrictions are solved by equality. hence each ray contains at least q-1-m zero-places. this fact can be used as a shortcut when checking the adjacency of two rays . at the end of the algorithm, it follows that the maximal pathway length |r|max, that is the maximal number of involved reactions in an em, reads :

|r|max = q -  = m +  <dig>     

initialization of r
as for the non-reconfigured network, the initialization of r for the reconfigured network can be done with a null space matrix k' of n' having the special structure . several of such kernel matrices may exist. we are interested in such a one that contains as many zeros as possible because the number of zeros in the starting tableau r has great impact on the number of ray combinations to be performed. for this purpose, it can be exploited that very sparse vectors of the null space of n'  are known, namely the two-cycles emerging by splitting up reversible reactions. we detail in method section a technique that incorporates as many two-cycles as possible into k to construct k'.

simple example
this section is devoted to illustrate our binary approach for computing elementary modes. figure  <dig> shows a simple example network consisting of four metabolites  and  <dig> reactions , whereof r <dig> is reversible. the stoichiometric matrix n of this network reads accordingly:



using our rules for removing redundancies, this network can be compressed as depicted in figure  <dig>  metabolite a is uniquely produced, hence, r <dig> and r <dig> can be combined to r1c and reactions r <dig> and r <dig> are lumped into r2c. r3c and r4c correspond to the original reactions r <dig> and r <dig>  respectively. finally, r <dig> and r <dig> are enzyme subsets and are combined to r5c. metabolites a and d can be removed, since they do not occur in any reaction anymore. thus, the network dimension could be reduced by two metabolites and two reactions. the stoichiometric matrix nc of the compressed system reads:



from this compressed network, we can compute a null space matrix having structure , here even without permuting rows :



kc would be the starting tableau in the original null-space approach. applying our binary approach we have now to split the  reversible reaction r4c in the compressed network ). this results in the stoichiometric matrix nc', where r4cb denotes the additionally introduced column of the backward direction of r4c:



now we need to determine a null space matrix kc' of nc', if possible in the sparse form as in eq.  . kc – as given in  – contains only irreversible reactions in the identity sub-matrix. therefore, without further rearrangements, we can already use it to construct kc' as described in the methods section. we introduce an additional row in the identity sub-matrix of kc  and an additional column representing the two-cycle from the split reversible reaction r4c:



kc' is now a proper initialization for the r tableau according to . the first four rows  can be seen as already completed, we therefore denote the starting tableau as r <dig>  according to  we can divide r <dig> into a binary  and unprocessed real number part:



we proceed now with the 5-th row . all columns with non-negative entries in r4c are retained . columns  <dig> and  <dig> have a negative entry at position r4c and are therefore combined with  <dig> and  <dig> to obtain a zero at position r4c. in the binary sub-tableau, the combination step is a simple or operation. thereby, using the obtained binary patterns, the adjacency test  must be performed for each pair of combined columns. here, all  <dig> possible pairs are adjacent. accordingly, after completing row  <dig>  tableau r <dig> has  <dig> columns and reads:



now we have already reached the last iteration step where r5c – the last row in real number format – is processed. columns 1– <dig> are retained and column  <dig> is combined with columns  <dig>  and  <dig>  however, the column pairs  and  are not pairs of adjacent rays. this can be detected in two alternative ways. the usual way is that both column pairs violate condition  because of column  <dig>  the second and quicker way is to observe that the minimal number of zeros in this network is  <dig>  and that their respective combinations would give columns with only  <dig> zeros. these combinations are therefore not included in the tableau. we obtain:



tableau r <dig> is the binary representation of the ems  from the split compressed network. now, the post-processing begins. first, we remove the spurious 2-cycle  raised by splitting r4c. then, rows r4c and r4cb are combined by an or operation and row r4cb is dropped. note, if a completely reversible elementary mode exists in the non-split network, it would lead to two ems – one for each direction – in the split network. in such a case, either both are kept or only one, then marked as reversible em. we have now obtained the  <dig> ems of the compressed network as binary vectors:



here, it is easy to reconstruct the real numbers of the ems from their binary patterns. for illustrating the general case, we reconstruct the first mode e <dig> using eq. :



the dimension of the null space of , hence of the solution space of eq.  is  <dig> . a scalable solution vector is t, normalizing to the first component yields the unique solution t. thus, the first em in the compressed network is e <dig> = t. reminding that we lumped the original reactions r <dig> and r <dig> into r1c and r <dig> and r <dig> into r5c, we can finally reconstruct the original elementary mode from the uncompressed network, that is r <dig> + r <dig> +  <dig>  × r <dig> +  <dig>  × r <dig> +  <dig>  × r <dig> 

results from real networks
we implemented the binary null-space approach  in matlab  and incorporated it into the fluxanalyzer  <cit> . the function includes a pre-processing step where the network is compressed as described. some sub-routines of the algorithm are performed by compiled c-code , since this proved to accelerate the implementation drastically. in order to check the capabilities of our algorithm we computed the elementary modes in realistic and large metabolic networks. the three networks  considered here are variants from a model of the central metabolism of escherichia coli investigated originally in  <cit> . for considering networks with different complexities we inserted an increasing number of substrate uptake or/and product excretion  reactions, which increase the number of ems much faster than the insertion of internal reactions. for a  comparison with the original nsa we used the program covern .

#reactions 
compressed network:
# reactions 
 <dig> 
 <dig> 
 <dig> 
comparing the required computation times, the binary nsa seems to be slightly faster than the original nsa. this observation should not be considered as a general result, since we cannot exclude that there are different potentials in optimizing the source code of covern and in fluxanalyzer, respectively. besides, different row orders in the starting tableau can generally result in different computation times. however, it seems that the original and the binary nsa are comparable with respect to computation time. the adjacency tests in the binary null-space approach need to consider more elements  but are simpler to perform because preliminary modes from a previous iteration cannot lose their elementary property. note also that implementing the full algorithm in c  might further accelerate the computation considerably.

using a special null space matrix k' as initialization of r  contributes considerably to a reduced computational effort. we can estimate this by the total sum  over the number of candidates pi occurring in the tableau before iteration i. in s <dig>  for example, . computing instead an arbitrary null-space matrix k' for n'  results in a more dense initialization for r and the naive initialization would lead to . the larger numbers of candidates increase the costs for adjacency tests and accordingly the running time drastically. this underlines that the success of the null-space approach  depends strongly on the initially chosen null space matrix.

generally, computing the stoichiometric coefficients of the ems from their binary patterns is in larger networks in low proportion to the overall computation time .

whereas the computational demands seem to be comparable for both null-space approaches, the memory requirements for the binary nsa are much lower, in particular during the last iterations. for this reason, the  <dig>  millions of ems from network s <dig> could be computed on a typical pc, whereas the original nsa ends in the 26-th iteration step  due to memory overflow.

discussion
elementary modes are smallest functional sub-networks, which can be interpreted geometrically as extreme rays from a pointed convex cone . the computation of extreme rays has been intensively studied by the polyhedral computation community and we think that the metabolic community can benefit from it. we shall also mention another abstraction of elementary modes within the framework of matroid theory  <cit> . in an oriented vector matroid, the elementary modes correspond to the positive circuits , which are minimal dependent sets. in fact, an elementary mode is a minimal linearly dependent set of the column vectors of the stoichiometric matrix . this has been mentioned only rarely so far  <cit> . matroid theory could be a source for new theoretical investigations on elementary modes and could lead to further improvements in the computation procedure as well as to new applications in the sense of metabolic pathway analysis.

adjacent extreme rays can also be detected by an algebraic characterization that completes definition  <dig>  <cit> :

 r and r' are extreme rays and the rank of the matrix az ∩ z is d-2

in practical cases the characterization of adjacency is mostly computed in its combinatorial form than its algebraic one  <cit> . however, improvements could be done by using both characterizations. in fact, the test on em length done before the actual adjacency test in our matlab pseudo-code is a consequence of the algebraic test. a striking feature of the algebraic test is that it only requires access to the two rays tested for adjacency  and to the fixed size matrix a, in practice to the stoichiometry matrix. in comparison, the combinatorial test implies a loop over all other rays . therefore, the algebraic test could be suited for distributed computing.

some theoretical issues of the combinatorial complexity of ems were discussed in  <cit> . an upper bound b for the number of ems is :



assuming that no conservation relations occur in the stoichiometric matrix, we obtain:



note that q and m should be taken from the non-split, compressed network to obtain the lowest upper bound. in larger, realistic networks, even if compressed, the values for b explode quickly. fortunately, the actual number of modes in real networks proved to be much smaller than the boundary , although it grows also exponentially. one reason is that many routes are not admissible due to violation of the sign restrictions. another reason is the low connectivity of many metabolites leading to sparse stoichiometric matrices.

a third reason is related to short pathway length. the upper bound reflects the case where all ems have maximal pathway length |r|max which is, by eq. , m +  <dig>  however, many ems, if not all, have a lower length immediately reducing the possible number of modes  <cit> . the pathway length distribution of the e. coli modes on glucose  is shown in figure  <dig>  the maximal length of an em in the uncompressed network is m +  <dig> =  <dig> +  <dig> =  <dig>  modes that are not involved in biomass synthesis, in particular, are much smaller. in terms of linear algebra this means that there exist vector sets w containing fewer than m +  <dig> column vectors of n that are linearly dependent. in polyhedral computation this phenomenon is known as degeneracy. generally, degenerate systems may cause annoying difficulties and must be handled often differently to non-degenerate systems, albeit they reduce here the number of modes. the algorithms related to em computation may be, in general, especially suited for computing extreme rays in such strongly degenerate systems, whereas other programs may be better suited for only weakly degenerate problems. for example, the software lrs  <cit>  implements the so-called reverse search enumeration algorithm  <cit>  that is polynomial for non-degenerate cases. note that the new binary approach as introduced herein can easily be adapted for computing extreme rays of any pointed cone as given in eq.  and may therefore improve the performance of extreme ray computation in many other applications.

albeit the general framework was formulated long time ago, the explicit introduction of the null-space approach was an important mile-stone in accelerating the computation of ems. the binary null-space approach as introduced herein increases the efficiency of this approach also with respect to the memory requirements and enables now to compute ems in networks significant larger as those investigated before. a simple computation gives the number of about  <dig> millions of ems in a network of  <dig>  reactions that can be stored in  <dig> gb ram . of course, only a fraction of this amount can be stored during the algorithm due to other  temporary variables. besides, reactions that are not yet processed are still stored as real numbers. the amount m of memory required for storing e modes after the procession of p reactions  is 

m = e · ).     

it depends on the evolution of the number of ems during the algorithm where the maximal memory demand occurs. generally, much larger networks can now be treated.

CONCLUSIONS
the four main results of this work are:  showing the equivalence between extreme rays and elementary modes,  showing that algorithms for computing elementary modes can be seen as variants of the double description method for computing extreme rays in pointed polyhedral cones,  introduction of a general framework and of new methods for redundancy removal and network compression,  introduction of the new binary approach for computing extreme rays and elementary modes.

the binary approach computes elementary modes as binary patterns of participating reactions that are sufficient to compute the respective stoichiometric coefficients in a post-processing step. for many applications – following the computation – it is even sufficient to operate on the binary patterns of ems. among all applications of ems presented in the introduction section, only the identification of all pathways with optimal yield, the "control-effective fluxes", and the α-spectrum need the explicit  coefficients, i.e. the reaction rates, in the ems. whenever needed, the explicit representation of an em can be determined  from its binary pattern.

the binary approach decreases the memory demand up to 96% without loss of speed and without loss of information giving the most efficient method available for computing elementary modes to date. the limiting step in computing elementary modes has thus been shifted back to the computation time. parallelization – as investigated within the traditional, not-binary, schema in  <cit>  – might lead to a further acceleration bringing us again a step closer to the complete set of ems in genome-scale metabolic networks.

