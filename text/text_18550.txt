BACKGROUND
an event-related brain potential  is extracted from an electroencephalogram   <cit> . since the eeg is a stochastic process, the erp is a multivariate statistical object, as well. it is a set of random curves , and a random curve cannot be simply represented by a single univariate feature  without loosing a lot of potentially useful information. nevertheless, erps are usually represented by univariate “components”, which most often are prominent peaks of the curves. an erp component is usually assessed by its peak value or by the area under the curve in a narrow time window around the peak. this method has the huge advantage that it is both simple and visually and intuitively clear. for instance, peak latency can be interpreted as brain processing time related to an erp-triggering event. more sophisticated methods for erp component extraction from single trials exist as well  <cit> . the statistical assessment of erp components is usually based on analysis of variance   <cit> . such assessment is sufficient for many applications. some inherent limitations of this approach have been addressed by “mass univariate analysis” methods based on permutation/randomization tests  <cit> .

there are, however, cases in which univariate assessment is not sufficient. for instance, erp-based brain–computer interfaces  require full-fledged multivariate assessment of the erp curves  <cit> . the t-cwt  <cit> , a feature extraction method based on the continuous wavelet transform   <cit>  and student’s t-test, was first introduced as one possible multivariate solution to the problem of single-trial erp classification at the international bci competition  <dig>  where it was compared to a variety of other feature extraction and classification methods ranging from simple and weak to advanced and powerful ones like, e.g., support vector machines   <cit> . t-cwt was a winner on two of the six erp datasets provided for the competition  <cit> ; these two were obtained in two different bci paradigms, “p <dig> speller” and “self regulation of slow cortical potentials ”  <cit> .

apart from bci applications, it has been shown how t-cwt can be used for detection and quantification of erps in other paradigms as well, e.g., for individual  diagnostics  <cit> . in the latter application, t-cwt was combined with a multivariate randomization test based on hotelling’s t2-statistic. similarly to the univariate randomization tests mentioned above  <cit> , which provide an effective correction for multiple univariate comparisons, the multivariate randomization test  <cit>  corrects the accumulation of chance bias arising from performing both t-cwt feature extraction and hypothesis testing on the same erp dataset.

a comparison of t-cwt to classical univariate erp assessment methods based on peak picking and area computation showed a clear advantage of multivariate t-cwt features over univariate measures  <cit> . more recently, t-cwt has been systematically evaluated in comparison to several other erp component detection methods ranging from simple univariate peak picking to tmax randomization tests  <cit>  performed on band pass filtered eeg and on t-cwt features  <cit> . this evaluation was performed with both simulated and real erp data at different levels of signal-to-noise ratio. sensitivity, specificity, positive and negative predictive values were assessed, and, as a result, t-cwt showed superior to all other methods, especially at low signal-to-noise ratios. it should be noted, however, that, in that study, t-cwt was used only as a feature extraction method and only mass univariate analysis , but no multivariate statistical assessment of the obtained features was done. in another recent study, t-cwt feature extraction was compared to two other wavelet-based erp component detection methods using the spikelet technique and wavelet asymmetry, respectively, with the result that both t-cwt and the novel wavelet asymmetry method showed a marked advantage over the spikelet technique in terms of detection accuracy: 83 % and 91 % vs. 43 %, correspondingly  <cit> . in the latter study, however, only a single erp component of interest, the n <dig>  was detected without any multivariate or mass univariate analyses of the whole erp curves.

although t-cwt features may well be interpreted as erp components  <cit> , the current article is mainly focused on t-cwt applications in which, as in the case of bci, the whole  difference between two erp curves is of primary importance, while differences in single components are only of secondary interest.

the goals of the this article are:  to provide a didactic geometric primer on some basic concepts of multivariate statistics as applied to erp assessment in general and to the t-cwt method in particular;  to present for the first time a detailed, step-by-step, formal description of the t-cwt algorithm and to make its matlab and gnu octave  <cit>  implementation publicly available as free and open source code  <cit>   released under the gnu general public license, version  <dig> ;  to demonstrate the t-cwt method in the assessment of example erp data  <cit>  obtained in a passive oddball paradigm  <cit> ;  to suggest and discuss conceptually novel applications of the multivariate approach in general and of the t-cwt method in particular for the purpose of hypothesis testing rather than for bci and single-trial classification.

an important new pre-processing step in the revised version of the t-cwt algorithm presented here is the novel multivariate outlier rejection procedure based on principal component analysis . other changes in the algorithm were aimed at simplification for the sake of easier understanding. for instance, the time-dependent filtering module  <cit>  was removed from the current version and the discrete wavelet transform   <cit>  was replaced by a discrete fourier transform   <cit> . visualization was simplified as well by plotting the one-dimensional linear discriminant function  instead of the two-dimensional t-value scalogram  <cit> .

multivariate statistics: a geometric primer
multivariate statistical analysis  <cit>  cannot be substituted by a multitude of univariate analyses, because the latter cannot address adequately the covariance of the data. even if the effect of the correlations between the variables on multiple univariate tests is taken into account by a proper correction of the overall α-level, e.g., by a permutation test  <cit> , the univariate approach still misses a lot of information contained in the covariance matrix .

in this section, some basic concepts of multivariate analysis are represented as geometric notions. this approach is meant as an aid to better understanding of these concepts through there association with familiar images from euclidean space.

erps as points in a vector space
consider a sample of n erp trials obtained from k eeg channels, in the time interval  0≤t<t, where t is the time relative to a triggering event. for each channel k= <dig> ,…k, and each single trial n= <dig> ,…n, the erp voltage curve vnk can be represented by a row vector vnk=vnk <dig> vnk <dig> …vnkl with components vnkl=vnk, where tl=t/l, l= <dig> ,…l are the equidistant sampling time points whose number l is obtained from the original sampling frequency r0:  <dig> l=r0t. the entire k-channel erp trial can be represented by the vector  <dig> vn=vn <dig> vn <dig> …vnk. thus, the erp is represented by a random vector v in a k×l-dimensional space, and the erp sample is represented by the n-by-k×l matrix  <dig> v=v1v2⋮vn. note that this notation is different from the standard notation in which vectors are represented by columns . here, vectors are represented by rows, as in the matlab and gnu octave  <cit>  implementation of t-cwt  <cit> .

mahalanobis distance and hotelling’s test
now, consider a random vector v and the corresponding single-trial erp sample v which, however, in this case, comprises two subsamples va and vb obtained under two different experimental conditions a and b . the two corresponding erps are then represented by va and vb and by the respective random vectors va and vb. assuming multivariate normal distributions  with equal covariance matrices, we want to compare the two erps by testing the hypothesis h0:v¯a=v¯b, where v¯a and v¯b are the mean vectors . geometrically, v¯a and v¯b represent two points in the k×l-dimensional space. hence, we can test h <dig> by testing the hypothesis that the distance between these two points is zero. thus, the problem of multivariate hypothesis testing can be reduced to the geometric problem of defining and measuring distance in this vector space and then performing a univariate test on this  distance.

like in the univariate case, the scale for measuring distance is provided by the variance or, more precisely, by its square root, the standard deviation . there are, however, two problems in the multivariate case:  we have different variances corresponding to the different vector components, and  these variables are correlated. the latter problem can be solved by principal component analysis . the principal component transform  represented by its corresponding matrix tp is an orthogonal transformation, i.e., a kind of rotation of the coordinate axes, such that, in the new coordinate system, the covariance matrix has a diagonal form . the transformation of the vector components is given by  <dig> vp=vtp,vp=vtp. the covariance matrix s is diagonalized by this transformation, i.e., the correlations between the new variables vpi,i= <dig> ,…k×l are all zero. because tp is an orthogonal transformation, the inverse transformation  is given by the transpose  <dig> v=vptp-1=vptpt,v=vptpt, and, correspondingly, the diagonalized covariance matrix sp is given by  <dig> sp=tptstp,s=tpsptpt. the diagonal elements  of sp are the squared standard deviations σpi <dig> of the uncorrelated variables vpi. normalizing these variables by the transformation  <dig> xi=vpi/σpi, we land in a familiar k×l-dimensional euclidean space where the scale is the same in all directions, i.e., the constant density ellipsoids  of the multivariate normal distribution of x are spheres  is known as pca “sphering” or “whitening”). in euclidean space, the squared distance d <dig> between two points xa and xb is simply given by the pythagorean theorem  <dig> d2=∑i=1k×lxai-xbi <dig>  substituting  into , we can compute the distance between v¯pa and v¯pb  <dig> d2=v¯pa-v¯pbsp-1v¯pa-v¯pbt. in , we use the fact that sp- <dig> is a diagonal matrix with eigenvalues σpi- <dig>  substituting the right parts of  and  into , we obtain  <dig> d2=v¯a-v¯bs-1v¯a-v¯bt. in , we use the property that s- <dig> is diagonalized by the same transformation as s. the expression  is called a “mahalanobis distance” . when s is substituted with an unbiased estimator and the resulting d <dig> is multiplied by a proper coefficient , it turns into hotelling’s t2-statistic whose p-value is obtained from the corresponding  t2-distribution and can be used to test h0′:d2= <dig>  which is equivalent to h0:v¯a=v¯b . thus, the multivariate problem of comparing two mean vectors  is reduced to the univariate problem of testing whether the mahalanobis distance between the mean vectors is different from zero.

linear discriminant analysis 
so far, we have shown that hotelling’s t <dig> is a kind of mahalanobis distance between two mean vectors defined by the natural metric provided by the covariance. but  bears another geometric interpretation as well. defining the linear discriminant function  d  as a column vector by  <dig> d=s-1v¯a-v¯bt, the mahalanobis distance  is expressed as a ldf value:  <dig> d2=v¯a-v¯bd. the column vector d defines a direction in space. the projection of the mean difference v¯a-v¯b onto this particular direction is the maximum of all projections of v¯a-v¯b onto all possible directions . linear discriminant analysis  is based on the construction of a separation plane which is perpendicular to d and passes through the middle between v¯a and v¯b, i.e. through the point / <dig>  this separation plane is defined by the equation  <dig> vd=12d. according to the optimal lda classification rule , an erp trial vx drawn from a new sample vx  is assigned to the population a if it lies above this plane, i.e.  <dig> vxd>12d, and it is assigned to the population b otherwise. the rule  is optimal because it minimizes the classification error rate.

moreover, the ldf value difference vx-12d is proportional to the distance from the point vx to the separation plane  and can be seen as a measure of the affiliation of vx with a or b.

the optimal lda classification rule  is based on the assumption that the a priori probability pa that an erp trial of unknown affiliation belongs to a is equal to the a priori probability pb that it belongs to b. if pa≠pb, the optimal lda classification rule  is generalized as follows . an erp trial vx is assigned to a if  <dig> vxd>12d+lnpbpa, and to b otherwise, i.e., the separation plane  is shifted in the direction of v¯a if pa<pb, and it is shifted in the direction of v¯b if pa>pb. note that  is a generalization of  because the logarithmic term is zero when pa=pb.

the ldf value vxd has an interesting property that should be noted, because it could be important for some interesting applications. since the ldf d is obtained from the sample v while vx is drawn from a different sample vx, the ldf value vxd has univariate normal distribution . this property can be used to reduce the multivariate erp vx to the univariate random variable vxd that exclusively and fully reflects the  erp difference between two particular experimental conditions, a and b. the important point here is that vxd can be used for other purposes beside classification. for instance, if the a-b structure of vx is known, the ldf value vxd can be subjected to student’s t-test in order to test the hypothesis about the mean a-b difference within vx. note the difference to hotelling’s t2-test: while in the mahalanobis distance , the ldf d is computed from the sample that is tested, in vxd it is computed from a different sample. this means that the multivariate structure of the a-b difference derived from v is imposed on vx in order to assess the  magnitude of the a-b difference within vx. this procedure provides a methodologically important alternative to hotelling’s t2-test, because it implies a conceptually novel approach to multivariate erp assessment which could be useful, e.g., in clinical applications .fig.  <dig> a mexican hat wavelet and a logarithmic sampling grid. the left plot
a displays a mexican hat wavelet as a function of time . the scale  is s= <dig>  s and the time shift  is t= <dig>  s. the right plot
b displays a log-grid in the t-s-plane . the scale-invariant sampling rate is r=10 pps. note that the number of sampling points in the time interval 0≤t< <dig>  at s= <dig>  is the same, as in the interval 0≤t< <dig>  at s= <dig> , and as in the interval 0≤t< <dig>  at s= <dig> , etc., and as the number of lines of points in the scale interval  <dig> ≤s< <dig> , which in turn is the same as in the interval  <dig> ≤s< <dig> , etc.



the problem of dimensionality
the multivariate erp model presented above is idealized. it can only work with few eeg channels, low time-sampling rates, short epochs and large number of erp trials. in most applications, however, the dimensionality of the vector space is very large and exceeds the number of trials. for instance, if the number of channels is k =  <dig>  the sampling rate  is r0 = 500 hz, and the epoch length is t = 1 s, then the number of time points is l = r0t = <dig>  and we have k×l =  <dig>  dimensions. if the number of trials is, e.g., n =  <dig> , the rank of the covariance matrix s is n-1= <dig> , which means that s is singular , and  cannot be applied. even if n-1>k×l and s is not singular, it is still necessary to reduce the dimensionality of the model because of  the loss of statistical power caused by the inclusion of noise variables in the d2-sum , and  the computational problems associated with too many variables and too small eigenvalues.

a standard solution to the dimensionality problem is given by pca : the variables vpi with small variances σpi <dig> are simply deleted from the model according to a certain criterion, e.g., all eigenvalues greater than the average are retained, or the largest eigenvalues explaining a certain proportion of the total variance are retained .

the t-cwt method provides a further solution to the dimensionality problem. it uses explicitly the special fact that the random vectors represent erp curves, i.e., continuous functions of time. the continuous wavelet transform   <cit>  and student’s t-test are used to extract certain “features” of the erp curves, which build the “feature space” whose dimensionality is substantially smaller than that of the original space. all the standard multivariate procedures described above  can then be performed in this feature space.

here, it is interesting to mention an alternative, recently proposed, dimensionality reduction approach based on effect-matched spatial  filtering  <cit> . while t-cwt reduces the dimensionality of the erp time curves, independently for each single channel, the ems filtering method reduces the number of channels , independently for each single time point, thus representing the multichannel erp by a single “surrogate time-course”. thus, in a certain sense, ems filtering can be seen as complementary to t-cwt and, for particular purposes, the two methods can be used in combination with each other.

the t-cwt method
the t-cwt method has already been described in the continuous notation  <cit> . here, it is presented in the discrete vector and matrix notation as well, because this discrete representation is the one that is used in the computational algorithm  <cit> .

cwt
the cwt  <cit>  wnk of the eeg signal vnk of the kth channel of the nth erp trial is given by  <dig> wnk=1s∫0tvnkψτ-tsdτ, where ψ is a wavelet function which is  well localized in both time and frequency, and  has a zero mean:  <dig> ∫-∞∞ψdt= <dig>  the approximate position of ψ/s) in time is determined by the time shift t, while the scale s, which is the inverse frequency, defines the approximate position in the frequency domain .

the cwt  is a linear transformation, which means that it can be represented by a matrix tw such that  <dig> w=vtw,w=vtw, where the random vectors v and w represent the erp in the time domain and in the time-frequency domain, respectively, and the matrices v and w represent the corresponding single-trial samples. the coefficients of tw in  are obtained by substituting vnk and ψ/s) in  with their respective discrete representations as in  and converting the integrals into corresponding sums. note, however, that the cwt  is highly redundant and the wavelet space defined by w is much “larger” than the original vector space defined by v. note also that tw is actually a block diagonal matrix built from k identical blocks, one per channel. the t-cwt computer application  <cit>  uses only one cwt block which is applied to each channel.

t-cwt
now, consider again two experimental conditions a and b , and the corresponding two samples of sizes m and n, respectively, of k-channel erps represented by the random curves vamk and vbnk, where: k= <dig> ,…k; m= <dig> ,…m; n= <dig> ,…n; and 0≤t<t. the corresponding cwts are computed by . then, student’s two-sample t-value tk is computed for each k and each scale-time point  from the corresponding cwt values wamk and wbnk:  <dig> tk=mnm+nw¯ak-w¯bkσabk, where w¯ak and w¯bk are the sample means and σabk is the pooled standard deviation computed from the corresponding sums of squares .

in the next step, each of the points , at which the functions tk reach a local extremum, are detected. in the last step, we define the t-cwt vector samples wa⋆ and wb⋆ by their respective components, the t-cwt features wam⋆kj and wbn⋆kj defined by  <dig> wam⋆kj=wamk,wbn⋆kj=wbnk. finding the local extrema of a function of two variables is an analytical operation, but its result can be represented by a simple projection in the wavelet space, i.e. selecting the vector components that correspond to the points  and discarding all other dimensions. this projection can be represented by the matrix tw⋆ which is obtained from tw in  by deleting the columns corresponding to the discarded space dimensions. the t-cwt vectors are obtained by substituting tw in  with tw⋆  <dig> w⋆=vtw⋆,w⋆=vtw⋆, where v is the “total” erp sample comprising the subsamples va and vb, and v is the corresponding random vector.

methods
this section provides a detailed, step-by-step, formal delineation of the t-cwt algorithm. in the brief intuitive descriptions published before  <cit> , most of the details were omitted. here, a rigorous mathematical delineation of all steps is presented for the first time.

pre-processing
theoretically, the t-cwt could be performed directly in the time domain defined by . the cwt  is, however, highly redundant and computationally demanding. that is why the dimensionality of the vector space must be reduced substantially before computing w and w⋆ .

frequency domain representation
the first pre-processing step is based on a frequency domain representation of the erps by a discrete fourier transform . the dimensionality of the vector space is reduced by deleting all frequencies larger than 2fc, where fc is a cutoff frequency defined by a cutoff scale  <dig> sc=1/fc. this is done as follows. first, we compute the orthogonal  dft matrix tf. the erp vectors  are then transformed by  <dig> vf=vtf,vf=vtf. geometrically, the dft  can be seen as a rotation of the axes, analogical to the pct . note, however, that tf is actually a block diagonal matrix built from zeros and k identical dft blocks, one per channel. 

as next, we retain only those columns of tf that correspond to frequencies fj fulfilling the cutoff condition  <dig> fj≤2fc=2sc,j= <dig> ,… a “reduced” matrix t^f is obtained from tf by deleting all columns corresponding to frequencies fj>2fc. the reduced dft is then given by  <dig> v^f=vt^f,v^f=vt^f. as in , the “inverse” transform  is given by  <dig> v^=v^ft^ft,v^=v^ft^ft. note, however, that, since t^f is not a square matrix, v^ and v^ are filtered versions of v and v.

in order to smooth the cutoff, the vector components corresponding to the frequencies fj of the last octave fc<fj≤2fc are attenuated gradually. this is done by multiplication with a diagonal matrix rf whose diagonal elements are given by the values of an envelope function r such that r= <dig>  for fj≤fc, and r=2-fj/fc, for fc<fj≤2fc. similarly, the vector components in the time domain, v and v, can also be multiplied with an appropriate window function before dft. this is done by left multiplication of t^f with a diagonal matrix rt whose diagonal elements are given by the values of the corresponding envelope function. the current t-cwt implementation uses a modified tukey window  <cit>  defined by the envelope function  <dig> f=12for0≤t<tin1fortin≤t<tout12fortout≤t<t where tin is the fade-in time and tout is the fade-out time.

chaining all transformations together, we obtain  <dig> v~f=vt~f,v~f=vt~f, where t~f is defined by  <dig> t~f=rtt^frf. for some purposes , it might be useful to represent the filtered erp back in the time domain by the “inverse” transform :  <dig> v~=v~ft^ft=vt~ft^ft. the number of frequency components per channel is  <dig> nf=1+4fct, where t is the length of the time interval. the dimension of the frequency domain space  is then knf, where k is the number of eeg channels. substituting  in  we obtain the following approximation:  <dig> nf≈4tsc in the t-cwt software  <cit> , the time-to-frequency domain transformation  is implemented by the function tcwt_t2f; the preceding computation of the transformation matrix t~f  is implemented by the function tcwt_prm2mat .


t-cwt
input
processing
output

function
files
steps
files: variables 

prm2mat
input parameter file

t2f
*
.t.mat
*
_a_const.mat
*
.ri <dig> mat : initial index vector

f2pc
*
.f.mat
*
*
.ri <dig> mat
*
.ri <dig> mat : outlier index vector

f2cwss
*
.f.mat
*
*
.pc.mat
*
.ri <dig> mat
_a_const.mat

pc2cnd2ri
*
.f.mat
*
.ri <dig> mat : outlier index vector
*
.pc.mat
*
.ri <dig> mat

f2x
*
.f.mat
*
*
.pc.mat
*
*
.cwss.mat
*
.ri <dig> mat
_a_const.mat

x2ld
*
.f.mat
*
*
.x.mat
*
.ri <dig> mat
the full names of the functions include the prefix ‘tcwt_’ . the functions listed above  operate according to the following general scheme:







pca and outlier detection
the multivariate outlier detection procedure proposed here is heavily based on pca not only for computational reasons , but also because a multivariate outlier can strongly influence the dimensionality of the model, producing “fake” dimensions that survive pca unless the outlier is excluded from the computation of the covariance matrix .

an important distinction should be made at this point. pca, as commonly used in erp applications  <cit>  is performed in the time domain and it usually includes an additional rotation of the axes  <cit>  following the initial one . this second rotation is aimed at obtaining more meaningful components, which, however, are not uncorrelated. in t-cwt, where pca is only used in the pre/post-processing , it does not include any additional rotations and the resulting components remain uncorrelated.

consider an erp represented via  in the frequency domain by the random vector v~f and the single-trial sample v~f, the latter comprising two subsamples v~fa and v~fb corresponding to two different experimental conditions; v~fa and v~fb may in turn comprise subsamples of trials obtained from different individuals. pct is performed according to , using the total covariance matrix obtained directly from v~f   <dig> vp=v~ftp,vp=v~ftp. then, components  corresponding to eigenvalues smaller than a certain cutoff value  are temporarily removed. the remaining qp variables vpi, i= <dig> ,…qp, are normalized by . from the normalized variables xi, for each n, the mahalanobis distance dn from the n-th single-trial erp xn to the total mean x¯ is computed according to   <dig> dn2=d2=∑i=1qpxni-x¯i <dig>  with growing number of trials, each of the terms of the sum in  rapidly converges to the square of a standard normally distributed random variable. hence, d <dig> is approximately χ2-distributed. with growing qp, the square root of a χ2-distributed random variable converges rapidly to a normal distribution as well, which means that d=d <dig> is approximately normally distributed.

the nth erp trial is temporarily marked as an outlier if  <dig> dn>d¯+cσd, where d¯ is the mean, σd is the standard deviation of d, and c is a heuristically chosen coefficient, .

the steps described above are repeated iteratively. trials marked as outliers  are excluded from the pca and the computation of d¯ and σd in the next iteration, but then they are tested again by  together with all other trials. principal components are also excluded for one iteration only until their number remains unchanged through two consecutive iterations. after that, the pca criterion is not applied any more and pct is performed further with a fixed number of components. this is done in order to facilitate convergence. also, in order to prevent oscillatory behavior, if the number of marked outliers does not increase after the current iteration, the outliers detected in the previous iteration are marked again together with those detected in the current iteration. the procedure ends when the set of detected outliers does not change any more .

if v~f comprises different individual datasets, a whole dataset is marked as an outlier  if a certain percentage of its trials are already marked. this criterion is applied, however, only if the number of single-trial outliers does not increase at the end of the current iteration. note that, like single trials, whole data sets excluded at a certain step, can nevertheless be included again later.

as a result of the procedure described above, both rows of v~f representing single-trial outliers and columns of tp representing “noise components” or “outlier components” are deleted. the reduction of dimensionality is thus represented by the reduced pct t^p and the corresponding “reduced” erp matrices v^p and v^p where  <dig> v^p=v~ft^p,v^p=v~ft^p. we use the “inverse” pct t^pt to represent the dimensionality reduction in the frequency domain  <dig> v~fp=v^pt^pt,v~fp=v^pt^pt. from  and , we finally obtain  <dig> v~fp=v~ft^pt^pt,v~fp=v~ft^pt^pt. note that in  we assume that all rows of v~f corresponding to outliers have already been deleted.

it is important to emphasize that, the principal components obtained by this procedure are identical with those which would be obtained if pca were performed in the time domain using the filtered erp, v~ . this is so, because the principal component axes that diagonalize the covariance matrix are unique . the frequency domain representation is solely a matter of computational convenience due to dimensionality reduction by frequency filtering. note also that although the dimensionality of the model is further reduced by the statistical “pca filtering” , the dimensionality of the frequency domain representation remains unchanged.

the above procedure can be additionally applied to each of the two subsamples v~fa and v~fb separately, using the principal components obtained from the whole sample v~f. the only difference is that no pca is done any more .

it is important to note that the pre-processing procedures described above can be used independently from the t-cwt feature extraction . for instance, the erp sample v~fp  can be represented back in the time domain by :  <dig> v~p=v~fpt^ft, and then used as input for other assessment procedures. in this article, the representation  is used solely for visualization purposes .

in the t-cwt software  <cit> , the pca-based multivariate outlier detection procedure  is implemented by the function tcwt_f2pc; outlier detection for each experimental condition separately with fixed pct obtained by tcwt_f2pc is implemented by the function tcwt_pc2cnd2ri .

t-cwt
log-grid sampling
in , the number of rows of the cwt matrix tw is equal to the number of components of w, which is equal to the number of sampling points in the s-t-plane of the wavelet ψ/s) in . this number can be significantly reduced by using the log-grid introduced in  <cit>  instead of a regular sampling grid. the vertices  of the log-grid  are defined by  <dig> sg=s0expgr),tg,h=sghr, where s <dig> is some unit scale, r is the scale-invariant sampling rate measured in points per scale , and g and h take integer values . the scale invariance can be expressed by the two properties: sg+r=2sg , and tg,h+r=tg,h+sg . the special case r= <dig> yields the dwt with its dyadic structure  <cit> . the t-cwt application uses only the part of the infinite log-grid  confined by the rectangle  <dig> sc2≤sg≤4t,0≤tg,h≤t. in , the minimal scale sc/ <dig> corresponds to the maximal frequency 2fc in . for a large number of log-grid vertices ng, a good approximation is given by  <dig> ng≈3r2tsc. thus, the number of cwt sampling points is significantly reduced, compared to the number of vertices of a rectilinear grid with a regular spacing defined in the same rectangle , e.g., by the original sampling frequency r <dig> applied to both axes as in . note that the sampling frequency r of the log-grid can be chosen to correspond to the original sampling rate r <dig> by setting r=scr <dig>  but this is not necessary and r can as well take an independent value r≠scr <dig> 

in the t-cwt software  <cit> , the log-grid   is implemented by a matrix of vertices computed by the function tcwt_prm2mat .

cwt and t-cwt from the frequency domain
as in , we define the cwt t^w of the filtered erp defined by  as  <dig> w^=v^t^w,w^=v^t^w, substituting  in , we obtain  <dig> w^=v^ft~w,w^=v^ft~w, where  <dig> t~w=t^ftt^w. the rows of the inverse dft tft are the discrete vector representations of the basic eeg oscillations sin and cos with frequencies fj=j/t, where j= <dig> , <dig> .. such that fj≤2fc according to . hence, t~w is computed by  using the log-grid sampling . the convolution integrals are represented by the corresponding sums with the original sampling rate r <dig> fig.  <dig> group and individual erp averages, student’s t-value curves, and ldfs. upper plots
a grand average and b individual average erps  elicited by  <dig> standard  and  <dig> deviant  stimuli in a passive oddball paradigm. the individual dataset was obtained from the eeg of participant ‘gim’ . the erps were filtered by a low-pass filter with cutoff frequency 25 hz  and by a statistical pca filter with proportion of explained variance pv = 99 %. the middle plots
c and d show the corresponding erp difference t-value curves , while the lower plots
e and f show the normalized linear discriminant functions  obtained by the t-cwt method for the whole group and for the individual dataset. the ldfs were computed for sc = 40 ms, pv = 99 %, and log-grid sampling rate r = 15 pps



like tw , t~w is a block diagonal matrix built from k identical blocks . the size of each block is nf×ng where nf and ng are the number of frequency components  and the number of log-grid vertices , respectively. from  and  we obtain the following approximation:  <dig> nfng≈12r2t2sc <dig>  the current implementation of t-cwt uses a mexican hat wavelet defined by  <dig> ψ=e-8t <dig>  note that  differs from the standard definition of the mexican hat, ψ=exp. the unity scale defined by  is four times larger than the standard. this is done for convenience: defined in this way, the scale corresponds better to the durations of the erp waves matched by the wavelet and to the periods of the oscillations sin and cos in .

in order to exclude the outliers detected above, we apply the obtained cwt t~w to the “reduced” matrices v~fp and v~fp defined by   <dig> w~=v~fpt~w,w~=v~fpt~w. the t-cwt features are computed by  and the t-cwt matrix t~w⋆ is obtained from t~w by retaining only the columns that represent the t-cwt features . substituting t~w in  with t~w⋆, we obtain  <dig> w⋆=v~fpt~w⋆,w⋆=v~fpt~w⋆. in the t-cwt software  <cit> , the computation of the cwt matrix t~w  is implemented by the function tcwt_prm2mat; the computations of the t-cwt scalogram tk , the t-cwt extrema and the t-cwt matrix t~w⋆  are implemented by the functions tcwt_f2cwss and tcwt_f2x .

post-processing in the feature space
the t-cwt features w⋆ are still strongly correlated, because one and the same erp component is found in more than one eeg channel represented by at least one extremum in each channel’s sub-scalogram. furthermore, not all such sets of extrema represent significant erp components. for these reasons, the dimensionality of the feature space is reduced further by pca and step-down selection of principal components. finally, the ldf is computed in the reduced feature space.

pca and step-down test
pct is performed in the feature space according to   <dig> wp⋆=w⋆tp⋆,wp⋆=w⋆tp⋆, and the set of components is reduced according to one of the pca criteria mentioned above. then, a subset of components, “selected principal components” , is selected by a step-down test  based on the natural ordering of the components . the “reduced” matrices t^p⋆, w^p⋆ and w^p⋆ are obtained by deleting the columns corresponding to the eliminated components in tp⋆, wp⋆ and wp⋆, respectively:  <dig> w^p⋆=w⋆t^p⋆,w^p⋆=w⋆t^p⋆, 

lda
the ldf dw⋆ is computed in the reduced feature space as in :  <dig> dw⋆=s^pab⋆-1w¯^pa⋆-w¯^⋆pbt, where w¯^pa⋆ and w¯^pb⋆ are the respective means of the two subsamples w^pa⋆ and w^pb⋆, and s^pab⋆ is the pooled covariance matrix. the lda separation plane is defined as in  by  <dig> w^p⋆dw⋆=12w¯^pa⋆+w¯^pb⋆dw⋆+lnpbpa. now, the transformations , , , and  can be chained together in order to represent the ldf dw⋆ and the ldf value w^p⋆dw⋆ back in the frequency domain and in the time domain:  <dig> w^p⋆dw⋆=vfdf⋆=vd⋆, where  <dig> df⋆=t^pt^ptt~w⋆t^p⋆dw⋆, and  <dig> d⋆=t~fdf⋆. finally, it might be useful to mention also the continuous time domain representation of the ldf value:  <dig> vd⋆=∑k=1k∫0tvkd⋆kdt, where k is the number of channels, t is the length of the time window, and vk and d⋆k are the continuous representations of v and d⋆ respectively .

in the t-cwt software  <cit> , the pca-based step-down reduction of the t-cwt features  and the computation of the ldf dw⋆  and df⋆  are implemented by the function tcwt_x2ld .

algorithm summary with links to the t-cwt software
in table  <dig>  the processing steps delineated above are summarized and linked to the corresponding functions and input/output data files defined by the t-cwt software  <cit>  . output file names displayed in the last column of table  <dig> are provided with references to corresponding mathematical variables and equations defined in this article. the wildcard symbol “ * ” denotes an erp dataset name and indicates that the corresponding t-cwt functions accept a whole list of such names as an argument and then iterate over the list, thus processing multiple erp datasets in a single call .

computational demands
the most computationally demanding procedures are the pca and the t-cwt including cwt and extremum detection from a scalogram.

pca
the number of matrix elements of the pct is k2nf <dig>  the covariance matrix has the same number of elements. using  we obtain the approximate total number of elements of both matrices:  <dig> np≈32k2t2sc <dig>  both the memory and the processing time required for pca are approximately proportional to np. each double-precision matrix element needs eight bytes of memory. the processing time depends on the central processing unit , but, as a rule of thumb, one microsecond per matrix element can be used for rough estimation of the time needed for one iteration of the pca-based multivariate outlier detection procedure .

t-cwt
the approximate number of  cwt matrix elements per channel is given by . the amount of memory consumed by t-cwt is proportional only to this number and does not depend on the number of channels k, because the t-cwt application  <cit>  performs cwt on one channel at a time. furthermore, if the processed dataset is too big, t-cwt does not transform all trials at once, but processes smaller blocks of trials , thus limiting the memory demand.

the time t-cwt needs to process one scalogram  is approximately proportional to the total number of non-zero cwt matrix elements nw=knfng. from  we obtain the following approximation:  <dig> nw≈12kr2t2sc <dig>  again, as a rule of thumb, one microsecond per matrix element can be used for rough estimation of the time t-cwt needs to process one scalogram .

the t-cwt software package  <cit>  includes the function tcwt_prm2info that evaluates both the exact values of np and nw and their approximations computed by  for a given set of input parameters. further, tcwt_prm2info makes a rough estimate of the computational demands based on the simple assumption of eight bytes of working memory and one microsecond of processing time per matrix element. all this is done in real time, without actually creating the corresponding matrices and can be very useful in the planning stage of a t-cwt project when the available computational resources must be taken into account.

example
consider, e.g., the following settings: k =  <dig>  t = 1 s, sc = 50 ms , and r = 15 pps. then we would have the following  numbers obtained with tcwt_prm2info: nf =  <dig>  ng =  <dig> , np =  <dig> , <dig>  and nw =  <dig> , <dig>  the memory used for pca would be about 430 mb and for cwt , about 116 mb. the estimated processing time for one pca iteration would be about 54 s, and for one cwt scalogram, about 69 s. on a powerful hardware , however, these computational times can be significantly shorter .

example: oddball paradigm
in this section, the t-cwt is demonstrated on example erp data  <cit>  obtained in a passive oddball paradigm  <cit> . since these datasets have already been described in detail elsewhere  <cit> , only the most important information about the experiment is provided here.

datasets
erp datasets were obtained from  <dig> healthy participants  in a passive oddball task  <cit> , in which  <dig> standard and  <dig> deviant stimuli were presented at a constant rate in a randomized sequence. the standard and the deviant stimuli were 50-ms-long, 75-db-loud sine tones, with a frequency of  <dig>  and  <dig>  khz, respectively; the interstimulus interval was  <dig>  s. the participants were instructed just to listen attentively to all tones . digitized eeg  was continuously recorded from nine scalp positions according to the 10– <dig> system: fz, cz, pz, f <dig>  f <dig>  c <dig>  c <dig>  p <dig>  and p <dig>  all electrodes were referenced to the linked mastoids. electrical eye activity was recorded by bipolar acquisition from the following sites: the two lateral orbital rims for horizontal eye movements, and fp <dig> and a site below the right eye for vertical eye movements and eye blinks. the first nine datasets  were excluded for technical reasons . the remaining  <dig> datasets were processed with t-cwt.table  <dig> individual hold-out error rates  and computational demands


fc

r

pca
average holdout error rates 
freq.
log
matrix
me ratio:
processing time
pv

a priori: 50 %
a priori:  <dig>  %
dom.
grid
elements
nfngsc2̲
std
dev
tot
std
dev
tot
nf
ng
knf
ng
r2
t2


data processing
before feeding the data into the t-cwt processor, the raw eeg curves were segmented and corrected for eye blink and eye movement artifacts, by a standard procedure  <cit> . the epoch length was 1 s, starting 100 ms before stimulus onset. then, the datasets were checked for series of more than one deviant trials. only the first trials of such series were retained, the following deviant trials as well as the first subsequent standard trial were deleted. the first ten trials of each dataset were also deleted. as a result of this reduction, in each dataset, remained  <dig> standard trials and  <dig> deviant trials. thus, the a priori probabilities  were ps= <dig>  % for standard trials and pd= <dig>  % for deviant trials. the datasets were converted from ascii format to the internal t-cwt format by the function tcwt_ascii2tmat  <cit>  and the epochs were reduced to windows of interest starting at stimulus onset and ending 600 ms later. the signals in these windows were then referenced to the 100 ms pre-stimulus baseline. these signals were processed by t-cwt.

in its current implementation  <cit> , t-cwt starts with a call to the function tcwt_prm2info which gives a rough estimate of the memory demands and the computational time . the next call is to the function tcwt_prm2mat that creates and saves to a file those transformation matrices which do not depend on the data, but are functions of global input parameters only . these matrices are the dft t~f  and the cwt t~w , and they depend on the original sampling rate r <dig> , the length of the time window t , the fade-in and the fade-out times tin and tout of the window function , the log-grid sampling rate r , and the cutoff scale sc . while r <dig> is defined by the time resolution setting of the eeg amplifier , the other parameters can be varied to achieve best lda classification results. in the current example, t, tin and tout were kept fixed: t = 600 ms, tin = 20 ms, tout = 200 ms, while r and sc took the following values: r=  <dig>   <dig>   <dig>  and 25 pps; sc=  <dig>   <dig>   <dig>   <dig>  and 30 ms. note that these values of sc correspond to cutoff frequencies fc=  <dig>   <dig>   <dig>   <dig>  and  <dig>  hz, respectively .

after computing t~f and t~w, the data were represented in the frequency domain by  using the function tcwt_t2f . the outlier rejection procedure described above  was performed on each dataset, first on the whole dataset , then on each of the two subsets, standard and deviant . the greatest eigenvalues explaining a certain percentage pv of the variance were retained after pct. different values of pv were tried in order to minimize the lda classification error rates : pv=  <dig>   <dig>  and 99 %. the outlier criterion was defined by setting c= <dig>  in . 

as a next step, the t-cwt features w⋆ were computed  as described in  using the function tcwt_f2x . the obtained set of t-cwt features was reduced further by pcas  and step-down tests , implemented by the function tcwt_x2ld . the same pca criterion with the same value of pv as in the outlier rejection procedure was applied. the overall α-level for the step-down test was set to αsd= <dig> . finally, the ldfs d⋆ of the individual datasets were obtained by , also implemented by the function tcwt_x2ld .

lda classification was performed according to  and classification error rates were computed using the hold-out method and the split-half method  for both unknown  and known  a priori probabilities.

in the “individual hold-out” method, all of the above steps but the first one  were performed on a dataset obtained from the original one by excluding one single trial. the ldf thus obtained was used to classify the excluded trial as standard or deviant by  and . the error rates were obtained by repeating the procedure for each single trial and each dataset and comparing the result with the true category of each trial . the hold-out method is very efficient, because it is almost unbiased and it uses the whole available statistical power, but it is also a computationally demanding procedure. in the t-cwt software  <cit> , the individual hold-out method is implemented by the function tcwt_f2holdout.

in the “individual split-half” method, all steps were performed on the first half of the trials of each dataset . the ldfs obtained from these “training datasets” were then applied to the second halves of the datasets, the “test datasets”. this method is quick and simple, but it has considerable loss of statistical power as a major disadvantage.

“individual biased” error rates were also computed in order to demonstrate the bias resulting from using the same dataset  for both training and testing.

in order to demonstrate multivariate hypothesis testing, only the last  <dig> deviant trials and  <dig> standard trials of each dataset were used as training dataset, while the first half of the trials were used as test dataset. the spcs obtained from each training dataset by the step-down method were applied to each test dataset and subjected to hotelling’s t2-test. both the individual split-half error rates and the biased error rates, as well as hotelling’s t2-tests were performed with the function tcwt_f2stats  <cit> .

finally, all individual datasets were pooled together into one large group dataset . outlier rejection  was performed with the average eigenvalue as a pca criterion in order to emphasize group features and to suppress individual and/or oscillatory features. the single-trial outlier criterion was defined by setting c= <dig>  in .  the dataset outlier criterion was defined by setting the minimum number of trials retained to 50 % of the trials in the dataset. t-cwt, pca , step-down test, and lda were performed as above . a variation of the hold-out method, the “group hold-out” was used to obtain classification error rates for the individual datasets: instead of excluding one single trial at a time, one whole individual dataset was excluded at each iteration. each error rate obtained in this way is based on a group ldf applied to the respective excluded dataset.

the t-cwt software  <cit> , provides the function tcwt_ri2ri1out that creates systematically hold-out indices of the pooled group dataset which are specially designed for the implementation of the group hold-out method. in a group hold-out index corresponding to a given individual dataset, all single trials belonging to this dataset are marked as “outliers”. the functions tcwt_pc2cnd2ri, tcwt_f2x, tcwt_x2ld, and tcwt_f2stats can use these index files to compute the corresponding group hold-out t-cwt features, group hold-out ldfs, and group hold-out error rates. the hold-out mode of operation of these functions is very similar to their normal mode outlined in the pseudocode at the bottom of table  <dig> .

for all obtained error rates , binomial distribution p-values were computed  to test the hypotheses whether these error rates were better  than the chance classification error rates defined by the a priori probabilities pd=50 % or pd= <dig>  % .fig.  <dig> hold-out error rates and computational demands as functions of sc, pv and r. these plots visualize the most important results displayed in table  <dig>  plots a–c show the average classification errors obtained with the individual hold-out method with equal a priori probabilities ps=pd =50 %  for different values of the cutoff scale sc and the percentage of variance pv explained by pca. the corresponding error rates obtained by using the knowledge of the real oddball probabilities ps =  <dig>  % and pd =  <dig>  %  are displayed in the plots d–f. the approximate processing time as a function of sc, or the respective cutoff frequency fc , is displayed in plot g. plot h shows how the number of the non-zero cwt matrix elements, measured in millions , and the respective memory usage, measured in megabytes , depend on sc or fc. plot i shows how both processing time and memory usage increase as a function of the log-grid sampling rate r 


id
group hold-out
individual biased
individual split-half
ind. hold-out
hotelling
nx
np
std
dev
tot
nx
np
std
dev
tot
nx
np
std
dev
tot
std
dev
tot
p-value
gim
 <dig> ∗
 <dig> ∗
 <dig> ∗
goi
 <dig> ∗
 <dig> ∗
 <dig> ∗
gus
 <dig> ∗
hah
hea
hii
 <dig> ∗
jue
kaa
 <dig> ∗
 <dig> ∗
 <dig> ∗
 <dig> ∗
kac
kud
mah
man
 <dig> ∗
 <dig> ∗
 <dig> ∗
muv
 <dig> ∗
 <dig> ∗
 <dig> ∗
ned
ots
rer
 <dig> ∗
 <dig> ∗
roc
 <dig> ∗
 <dig> ∗
rom
 <dig> ∗
 <dig> ∗
sca
sch
 <dig> ∗
 <dig> ∗
sck
 <dig> ∗
 <dig> ∗
 <dig> ∗
 <dig> ∗
sct
 <dig> ∗
 <dig> ∗
scw
 <dig> ∗
umd
 <dig> ∗
 <dig> ∗
 <dig> ∗
 <dig> ∗
usd
 <dig> ∗
wib
 <dig> ∗
 <dig> ∗
 <dig> ∗
zia
 <dig> ∗
 <dig> ∗
hotelling’s t2-tests were performed with only  <dig> deviant and  <dig> standard trials 



all t-cwt computations were performed with the t-cwt software  <cit>  in 64-bit matlab  <dig> . <dig>  , under gnu linux , on the high performance computing cluster bwgrid  <cit>  using a single quad-core cpu  per job . the t-cwt program was also tested with 64-bit matlab  <dig> . <dig>   and 64-bit gnu octave  <dig> . <dig>  <cit> , under gnu linux  and windows  <dig>  on less powerful desktop and laptop computers .

RESULTS
in this section, some example results are presented and discussed. these results were obtained with the t-cwt method from the example datasets  <cit>  described above.

results from the example oddball data
group averages of individual hold-out classification error rates for different values of the cutoff scale sc, the log-grid sampling rate r, and the percentage of variance pv explained by pca are displayed in table  <dig>  most of these results are visualized by bar plots in fig.  <dig>  both total errors  and errors for each category of trials, standard  and deviant , are displayed. the total error rates computed by taking into account the a priori oddball probabilities ps =  <dig>  % and pd =  <dig>  %  were, as expected, much smaller than those computed for ps=pd =50 % . note, however, that the error rates for deviant trials increased when the a priori probabilities were taken into account. it is also interesting to point out the following observation: while for ps =  <dig>  % and pd =  <dig>  %, the total error  was decreased by the error reduction in the classification of deviant trials , the corresponding total decrease in the case of equal a priori probabilities ps=pd =50 % , was caused by the decline of error of classification of standard trials .

both the numbers in table  <dig> and the plots in fig.  <dig> demonstrate how the quality of the ldf d⋆  can be optimized by minimizing the errors of classification through systematic variation of the values of the different input parameters sc, pv, r, etc.. this optimization can be very important for different applications of the t-cwt method. while this is obvious for cases in which classification is the ultimate goal of the application , other applications aimed at multivariate hypothesis testing may also use optimized ldfs for their purposes .

in table  <dig>  nf denotes the number of frequency components per channel , and ng is the number of log-grid vertices . the number of non-zero cwt matrix elements  is given by knfng, where k is the number of channels. table  <dig> shows that the me ratio /  is approximately constant. table  <dig> also shows how the computational time depends on the number of non-zero cwt matrix elements knfng which is a function of sc, r, pv, and the number of channels k. this function is defined by  which was confirmed by the results displayed in table  <dig>  the computational demands as functions of sc, fc and r are presented in graphical form in fig. 3g-i.

the total processing time for the hold-out method is the product of the processing time per non-zero cwt element, the number of such elements, and the number of scalograms. for the hold-out method, as applied to the example data, the latter was equal to the total number of trials =  <dig> datasets ×  <dig> trials per dataset =  <dig> . as the last column of table  <dig> shows, the processing time per matrix element is approximately constant. note, however, that this value of less than a half microsecond was obtained with matlab and certain hardware , and that the corresponding value for gnu octave and/or other hardware may be larger.

the results for sc = 40 ms, r = 15 pps, and pv = 99 % are presented in more detail. the latter values appeared to be optimal, because, as table  <dig> and fig. 3i show, larger values of r increased substantially the computational demands with little improvement of the results.

the average erp curves are displayed in fig. 2a for the whole group  and in fig. 2b for one individual participant . student’s t-test results for the difference between the erp responses to deviant vs. standard tones are displayed in fig. 2c for the whole group and in fig. 2d for the participant ‘gim’. note that, since these t-values were not corrected for multiple comparisons, they cannot be used for inference about the statistical significance of the difference between the two erp curves at each point in time  <cit> . it is interesting, however, to compare the forms of these t-value curves with the forms of the corresponding ldfs. the ldf obtained with the t-cwt method is displayed in fig. 2e for the whole group and in fig. 2f for the participant ‘gim’.

the grand average plots show that the erp response to deviant stimuli relative to the response to standards was dominated by the mismatch negativity   <cit> , the p <dig>   <cit> , and the negative slow wave   <cit> . the individual ldf displayed in fig. 2f shows, however, some additional oscillatory features which are not quite discernible in the time domain  and which, nevertheless, improve substantially the lda classification results obtained with this ldf for the participant ‘gim’ compared to those computed for the same participant with the group ldf displayed in fig. 2e .

the different lda error rates as well as the p-values obtained from hotelling’s t2-test  for each participant, for sc = 40 ms, r = 15 pps, and pv = 99 % are displayed in table  <dig>  total error rates  were tested by the binomial cumulative distribution function whether they were significantly smaller than the a priori probability for deviant  trials pd=  <dig>  %. the comparison between the different methods shows an advantage of the classifications based on individual t-cwt features over those based on group features. furthermore, the individual hold-out method provided lower error rates than the split-half method. this can be explained by the split-half method’s loss of statistical power .

discussion
a much more straightforward approach, however, would be to use directly the ldf value vd⋆  to measure the erp difference between the two experimental conditions and to interpret this difference as a measure of participants’ attention to the stimuli. this could be done in different ways as described in the following three cases.

in the first case, both v and d⋆ are obtained from the same erp sample v, comprising two subsamples va and vb obtained under two different experimental conditions a and b . then, the mean difference ldf value v¯a-v¯bd⋆ is exactly the mean mahalanobis distance  computed in the feature space. this same mahalanobis distance is used in hotelling’s t2-test, which, however, should not be used directly in this case, because the t-cwt features are extracted from the very sample that is tested. as already mentioned in the background section, a multivariate randomization test based on hotelling’s t2-statistic can be used instead  <cit> .

in the second case, the ldf d⋆ is computed from the same sample as v, but, this time, using t-cwt features obtained from a training dataset, vz . in this case, hotelling’s t2-test may be used, as this was done above in the assessment of the example data.

the third case, in which both the t-cwt features and the ldf d⋆z are computed from a training dataset vz, while v is drawn from a test dataset v, is the most important and will be discussed in more detail. as already mentioned above, this is the usual scenario in a typical bci application, which was also demonstrated above by the split-half method in the assessment of classification error rates. but the representation of the mahalanobis distance  suggests another usage of the ldf apart from single trial classification. namely, we can construct a new estimator dz <dig> of the mahalanobis distance by using d⋆z instead of d:  <dig> dz2=va-vbd⋆z. since d⋆z is computed from the training dataset vz, it can be treated as constant in all tests and analyses concerning the test dataset v. consequently, vad⋆z, vbd⋆z, and dz <dig> are all  normally distributed and can be subjected to standard univariate tests. it should also be noted that in  a whole complex pattern recognition scheme derived from vz  is imposed on v by simple multiplication.

the significance of  reaches, however, beyond mere computational convenience, because it conveys a whole concept of multivariate erp assessment. while, in most bci applications, this concept is clearly the most effective for single trial classification, and, therefore, also the standard one, it has not been used in other erp applications for testing hypotheses about mean erp differences.  in the following, some general ideas about possible applications of this multivariate concept are presented.

the two kinds of t-cwt features, group and individual, that can be extracted from the data suggest two different kinds of diagnostic applications of the t-cwt method: within-subject and between-subject. consider an erp paradigm testing a certain cognitive function in a group of individuals under three different conditions: x, experimental condition ; y, control condition ; and z, standard condition . assume that the cognitive function of interest is reflected by the erp difference between two sub-conditions, a and b, . now, the individual t-cwt features, spcs, and ldfs obtained from z can be used to assess the difference between x and y by means of student’s two-sample  t-test of the mahalanobis distance  and/or by comparison of classification error rates. this is an example of a within-subject application of the t-cwt method. note that the t-cwt method described above for the case of an a-b design can be easily extended to the case of only one sub-condition, a, when the erp v is compared to  <dig> .

as an example of a between-subject application, consider an erp paradigm testing a certain cognitive function in three different samples of individuals: x, a sample of patients suffering a certain disorder which has the impairment of this cognitive function as a symptom; y, a sample of healthy individuals; and z, a mixed sample of patients and healthy individuals. in this case, the group t-cwt features, spcs, and ldfs obtained from z can be used to assess the difference between x and y. further, the t-cwt method can be applied directly to the erp difference between x and y and the resulting t-cwt features, spcs, and ldfs can be used for classification and diagnostics of individuals from the general population . the greatest problem in the case of a between-subject comparison would be the substantial increase in variance due to individual differences.

finally, in both within-subject and between-subject applications, the ldf value  can be used as a measure of the magnitude of the  erp response in each of the conditions x, y, and z in order to investigate its relationship with other behavioral measures or with the amplitudes of single  erp components.

limitations
all of the above ideas about possible applications of the t-cwt method assume the existence of a cleverly designed erp paradigm that tests exclusively  a particular  cognitive function of interest without the resulting erp differences being affected by  other cognitive functions. in many cases, however, this assumption might not be true. this is a conceptual limitation, not only of the t-cwt method, but of any multivariate erp assessment method. in certain cases, the amplitudes of single erp components might be better measures of particular cognitive functions of interest.

a purely technical limitation of the t-cwt method is imposed by the computational demands of its current software implementation  <cit> . for instance, the usage of dense electrode arrays combined with long time windows and small cutoff scales, might result in practical unusability of the application, if no access to adequate computational resources  is provided. in spite of the good scalability of the application , the increase of computational time can be handled only by parallel execution of t-cwt jobs on several powerful cpus.

a notable limitation of the current study is the lack of evidence for  the usefulness of the novel pca-based multivariate outlier detection procedure introduced above. the results of the  assessment of the example oddball data were not sensitive to variations in the number of outliers detected and excluded by the procedure, depending on the value of the input parameter c . whether zero or as much as 20 % of the trials were rejected, the statistical assessments were practically not affected by these changes and the corresponding results remained virtually the same. it should be mentioned, however, that some unpublished evidence already exists, suggesting that the outlier detection procedure could be crucial in a between-subject design. this evidence comes from the  t-cwt assessment of erp data obtained in a previous study  <cit> .

CONCLUSIONS
in the present article, some basic concepts of multivariate statistics were introduced as geometric notions. erps were defined as random vectors  in a metric space, in which the distance between two points was derived in a natural way from the covariance of the data. pct and dft were introduced as rotations in this space. lda classification was described as computing a ldf vector, building a separation plane perpendicular to this vector, and assigning single-trial erp points to two different categories according to their position relative to this dividing plane. cwt and t-cwt were also defined as linear transformations represented by their respective matrices. all these mathematical constructs were used to provide for the first time a detailed, step-by-step, formal description of the t-cwt algorithm. its matlab and gnu octave implementation was also made publicly available as free and open source code released for the first time under gplv <dig>  <cit> .

a new multivariate outlier rejection procedure based on pca in the frequency domain was introduced. the time-dependent filtering and the dwt used in the previous version of t-cwt  <cit>  were replaced in the current version by simple uniform filtering via dft. this was done solely for simplification for the sake of easier understanding and not for improvement of the method. in fact, a new version of t-cwt is planned including a more flexible procedure for time-dependent filtering based on pca and dwt.

it should be noted that t-cwt is essentially a feature extraction method and t-cwt based classification does not necessarily imply lda as a post-processing procedure. classification can be performed using other methods as well, e.g., svm  <cit> . hypotheses can be tested by both mass-univariate  <cit>  and multivariate  <cit>  permutation/randomization tests. moreover, as already mentioned above, t-cwt can also be used in combination with other dimensionality reduction methods, e.g. ems filtering  <cit> . on the other hand, the pca-based multivariate outlier detection introduced here, can be used independently from t-cwt as a pre-processing procedure in other assessment algorithms. it is also important to emphasize that although t-cwt feature extraction can be computationally demanding, taking several seconds or even minutes for large scalograms , the t-cwt features, once computed, can be applied practically instantly in real time applications  via lda or other proper classification method.

the t-cwt method was demonstrated on example erp data  <cit>  obtained in a passive oddball paradigm. both group and individual t-cwt features were extracted from the data and were used for lda classification of single trials and for testing mean erp differences for each individual dataset via hotelling’s t2-test. different methods for estimation of classification errors were introduced and compared with each other.

finally, new ideas for further applications of the multivariate approach in general and of t-cwt method in particular were introduced on a conceptual level in the discussion. some of these ideas will be tested soon in a randomized clinical trial where erps are used for assessment of the sustained mindful attention developed by training in a course of mindfulness-based cognitive therapy for recurrent depression  <cit> .

availability of supporting data
the example datasets  <cit>  supporting the results of this article are available at http://tcwt.de/ or http://bioinformatics.org/tcwt/ as well as in the labarchives repository at http://doi.org/ <dig> /h4mp518t.

additional file

 <dig> /s12868-015-0185-z
t-cwt  <dig> : a software implementation of the t-cwt method for multivariate assessment of event-related potentials. a zip archive  of the t-cwt free and open source code for matlab and gnu octave  <cit>  released under gplv <dig> 



abbreviations
bcibrain–computer interface

cpucentral processing unit

cwtcontinuous wavelet transform

dftdiscrete fourier transform

dwtdiscrete wavelet transform

eegelectroencephalogram

emseffect-matched spatial 

erpevent-related potential

gplv3gnu general public license, version 3

ldalinear discriminant analysis

ldflinear discriminant function

pcaprincipal component analysis

pctprincipal component transform

scpselected principal component 

sssums of squares 

svmsupport vector machines

