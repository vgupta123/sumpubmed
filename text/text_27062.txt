BACKGROUND
microbial communities and how they are surveyed
microbial communities abound in nature and are crucial for the success and diversity of ecosystems. there is no end in sight to the number of biological questions that can be asked about microbial diversity on earth. from animal and human guts to open ocean surfaces and deep sea hydrothermal vents, to anaerobic mud swamps or boiling thermal pools, to the tops of the rainforest canopy and the frozen antarctic tundra, the composition of microbial communities is a source of natural history, intellectual curiosity, and reservoir of environmental health  <cit> . microbial communities are also mediators of insight into global warming processes  <cit> , agricultural success  <cit> , pathogenicity  <cit> , and even human obesity  <cit> .

in the mid- <dig> s, researchers began to sequence ribosomal rnas from environmental samples in order to characterize the types of microbes present in those samples, . this general approach was revolutionized by the invention of the polymerase chain reaction , which made it relatively easy to clone and then sequence rdna  in particular those for small-subunit ribosomal rna . these studies revealed a large amount of previously undetected microbial diversity  <cit> . researchers focused on the small subunit rrna gene not only because of the ease with which it can be pcr amplified, but also because it has variable and highly conserved regions, it is thought to be universally distributed among all living organisms, and it is useful for inferring phylogenetic relationships  <cit> . since then, "cultivation-independent technologies" have brought a revolution to the field of microbiology by allowing scientists to study a wide and complex amount of diversity in many different habitats and environments  <cit> . the general premise of these methods remains relatively unchanged from the initial experiments two decades ago and relies on straightforward molecular biology techniques and bioinformatics tools from ecology, evolutionary biology and dna sequencing projects.

briefly, the lab work involved in  <dig> s rdna surveys begins with environmental samples  from which total genomic dna is extracted. next, the  <dig> s rdna is pcr-amplified with pan-bacterial or pan-archaeal primers , cloned into a sequencing vector, and then sequenced  resulting in large collections of diverse microbial  <dig> s rdna sequences from these different samples. as sequencing costs have continually declined, environmental microbiology surveys have expanded correspondingly and  <dig> s rdna datasets have grown increasingly complex.

the size and complexity of data sets introduce a new challenge - analyses that one could carry out manually on small data sets now must be aided or run entirely on computers. and those analyses that previously were carried out computationally now must be made more efficient to have any hopes of being completed in a timely manner  <cit> .

how then is the microbial community sequencing data converted from reads off a sequencing machine to bar graphs, network diagrams, and biological conclusions? fortunately, even as data sets have expanded, most researchers analyzing rdna sequence data sets, even when they are very large, have a similar set of goals in their analysis. for example, most studies are interested in assigning a microbial identity to the  <dig> s rdna sequences and determining the proportion of these organisms in each sequence collection. and to achieve these , a similar set of steps are used  including aligning the rdna sequences in a dataset to each other so that they are comparable, removing chimeric sequences generated during pcr identifying closely related sets of sequences , removing redundant sequences above a certain percent identity cutoff, assigning putative taxonomic identifiers to each sequence or representative of a group, inferring a phylogenetic tree of the sequences, and comparing the phylogenetic structure of different samples to each other and to the larger bacterial or archaeal tree of life.

over the last few years, a large number of software tools and web applications have become available to carry out each of the above steps . in practice, even as new software became available, researchers still have to act as the drivers of the workflow. at each step in this process, different types of software must be chosen and employed, each with distinct data formatting requirements, invocation methods, and each associated with a variety of post-analysis steps that may be selected and applied. even after all of these steps have been completed, a wide variety of statistical and visualization tools are applied to these results to interpret and represent these data. in this context, there is a clear need for tools that will run a comprehensive set of analyses all linked together into one system. very recently, two such systems have been released - mothur and qiime. waters is our effort in this regard with some key differences compared to mothur and qiime.

motivations
as outlined above, successfully processing microbial sequence collections is far from trivial. each step is complex and usually requires significant bioinformatics expertise and time investment prior to the biological interpretation. in order to both increase efficiency and ensure that all best-practice tools are easily usable, we sought to create an "all-inclusive" method for performing all of these bioinformatics steps together in one package. to this end, we have built an automated, user-friendly, workflow-based system called waters: a workflow for the alignment, taxonomy, and ecology of ribosomal sequences . in addition to being automated and simple to use, because waters is executed in the kepler scientific workflow system  it also has the advantage that it keeps track of the data lineage and provenance of data products  <cit> .

automation
the primary motivation in building waters was to minimize the technical, bioinformatics challenges that arise when performing dna sequence clustering, phylogenetic tree, and statistical analyses by automating the  <dig> s rdna analysis workflow. we also hoped to exploit additional features that workflow-based approaches entail, such as optimized execution and data lineage tracking and browsing  <cit> . in the earlier days of  <dig> s rdna analysis, simply knowing which microbes were present and whether they were biologically novel was a noteworthy achievement. it was reasonable and expected, therefore, to invest a large amount of time and effort to get to that list of microbes. but now that current efforts are significantly more advanced and often require comparison of dozens of factors and variables with datasets of thousands of sequences, it is not practically feasible to process these large collections "by hand", and hugely inefficient if instead automated methods can be successfully employed.

broadening the user base
a second motivation and perspective is that by minimizing the technical difficulty of  <dig> s rdna analysis through the use of waters, we aim to make the analysis of these datasets more widely available and allow individuals with little or no programming and bioinformatics skills to still use the best software currently available. prior to waters, few microbiologists had the skills and time to invest for installing close to a dozen different pieces of software, troubleshooting them, preparing files, etc. if they wanted to do comprehensive  <dig> s rdna analyses. with waters we believe more biologists can get to the heart of microbial ecology questions and obtain results faster.

comparability and reproducibility
the third, complementary motivation to build waters was to "standardize" the  <dig> s rdna analysis methods thus facilitating comparability and reproducibility of results. although isolated reports have called for community-wide standardization of part of the  <dig> s rdna analysis process  <cit> , in the past most microbial ecologists have cobbled together software tools from different websites and individual software downloads in an ad-hoc manner that is hard to compare to other microbial analyses in other publications. in short, we sought to develop a reproducible, convenient "one stop shop" method for  <dig> s rdna analysis that was accessible for a user with only minimal computational expertise. data lineage and provenance information that is automatically generated during workflow runs provides rich additional opportunities for result validation and reproducibility  <cit> . very recently, two new programs, mothur  <cit>  and qiime  <cit> , have been published that also attempt to standardize  <dig> s rdna analyses. the similarities and unique attributes are discussed below and in table  <dig> 

along the left column, "use" indicates where or how the software is used; "align" indicates the alignment programs available; "chimeras" indicates the chimera removal software available; "otus" indicates the software used to detect and determine operational taxonomic units; "taxonomy" indicates the software used to assign taxonomy to otus; "trees" indicates the software used to build phylogenetic trees; "ecology" indicates whether or not ecological indices such as chao <dig> and the shannon index are calculated; "unifrac" indicates whether unifrac analyses are done within the software or whether data is formatted for downstream use in unifrac; "export db" indicates whether a quality-controlled, curated  <dig> s dataset is available for export and/or for comparison to the user's own dataset; "trim" indicates the availability of quality control trimming to remove sequence vectors or low-quality bases from the initial upload of sequences; "dataset size" indicates the estimated amount of sequences that can be readily processed through each software type. along the top are all known multi-tool  <dig> s rdna analysis software suites. note that these software are each under very active development. this table represents a snapshot in time of current tool availabilities. ml, maximum-likelihood; nj, neighbor-joining.

scientific workflows and the kepler system
in recent years, the concept of automating repetitive, complex informatics tasks has gained popularity and practice in many scientific communities  <cit> , and been widely used and implemented in the public sector for corporate use. this process, when applied to scientific research, is termed scientific workflow automation  <cit> , and a variety of different scientific workflow systems are available or under active development . based on our prior experience extending the kepler scientific workflow system, we chose to implement waters in kepler  <cit> . the use of a scientific workflow system in general and kepler in particular offers several advantages for use by the scientific community  <cit> , which are described below.

first, it is open-source and freely available, and thus ideal for academic development. second, unlike other systems, kepler is independently extensible, which means that developers can make changes to the underlying workflow system if the need arises. to our knowledge, no other workflow system allows developers to make major changes to the system for their particular application needs. for example, as part of waters, a custom data cache capability was developed that allows for incremental recomputation of results . such enhancements can also be contributed back to the shared source code repository and used by other projects.

kepler is also unique in that it supports different models of computation  via software components called directors. for example, waters employs a new comad director  to simplify handling of nested data collections  <cit> . similarly, developers who need to make deep changes to the system, e.g., in order to change workflow scheduling or data handling for specific applications or projects, can do so by customizing existing directors or devising new ones. other contemporary workflow systems do not allow such customization and instead only support a single way of running workflows .

fourth, a very active kepler community is constantly developing actors, the individual units within a workflow that perform specific operations on the data, for many fields in the natural sciences  <cit> . these actors now make up a large library, available via a public repository of usable, interoperable, and interchangeable actors.

kepler has been used in a wide variety of scientific domains and communities, ranging from astrophysics, to ecology, to particle physics  <cit> , and - importantly for  <dig> s rdna analyses - the phylogenetic and ecology communities  <cit> , which have similar needs and functions. in fact, the raxml actor that builds phylogenetic trees within waters already existed prior to the development of our workflow, demonstrating the intrinsic reusability and exchangeability of workflow actors.

incremental recomputation
finally, kepler has a built-in database that allows calculations to be cached and stored internally rather than recalculated anew every time. for instance, in analysis of  <dig> s rdna datasets, new data often become available sporadically as sequencing centers complete batch jobs. the addition of new data generally requires re-analysis of the entire dataset, but, by using the cache, previous intermediate data products, including alignments, chimeras, and taxonomy assignments, can be retrieved automatically from the database rather than being recalculated. therefore, the cache increases the efficiency of adding new data to a partially analyzed dataset. moreover, if new metadata parameters become available or are altered, the entire workflow can be re-run on the existing cached data and all new results files can be generated without the need for any heavy recalculations.

implementation
scientific workflow systems typically represent workflows as networks of components . in kepler, these components are called actors which can be viewed as independently executing processes, and which communicate by sending data through unidirectional pipelines . new workflow components can be added simply by choosing new instances of existing actors, or by building new "native" actors, i.e., implemented in java, the underlying implementation language of kepler. a workflow consisting of actors and their dataflow connections is then executed according to a schedule as prescribed by the director .

kepler supports a number of ways to add new actors to the system and to implement new actors. if the desired actor is not available from the library or a remote actor repository, one can either create new "native" java actors, or one can instantiate certain generic actors in new ways. the former requires programming expertise in java, while the latter doesn't. for example, to add a new data analysis step implemented in the r language as an actor, one only has to instantiate the generic r actor accordingly. this specialized r actor instance can then be stored as a new actor. other ways to add new actors to kepler include, e.g., instantiation of the command-line actor , or instantiation of the web service actor so that certain web services become new components .

for waters a number of custom actors were developed to perform the required microbial ecology functions. some, like the mallard and otuhunter actors , directly invoke pre-existing java-based algorithms. others, like the stap and infernal actors, invoke external non-java programs automatically, meaning that these actors run the programs on behalf of the waters user, wait for the results to be produced, and reincorporate these results into the internal data stream seamlessly. two additional features of waters are the options to use a computer cluster to accelerate compute-intensive processes  and to intelligently reuse existing results within the cache where possible to minimize computation.

the main waters workflow comprises  <dig> actors, each of which is a java class that implements a common actor interface. this interface allows the workflow system to communicate with the actor, directing it to take certain actions. the comad  computational model within kepler was used  <cit> . in the comad style of developing and executing workflows, a stream of structured data flows through a sequence of actors that together form a computational pipeline. much like workers in an assembly line, each actor in the pipeline can add or remove information from the data stream passing through it; newly computed data products are added to the stream, and data that will not be needed by downstream actors can be deleted. actors that conform to the comad paradigm need not process all the data that streams through them; instead, the workflow designer can declare with an xpath-like syntax,  which data in the stream each actors should process, giving the workflow designer many options for composing the structure of the data stream, grouping intermediate and final results with the raw data used to compute them, etc. it has been shown that this workflow modeling paradigm results in more robust, flexible, and change-resilient workflows when compared with conventional workflow designs  <cit> .

the waters workflow
in this section we describe step-by-step the analysis automated by the waters workflow and portrayed in fig.  <dig> 

sequence libraries import
because most sequencing centers in our experience return to users assembled quality-controlled contigs, the workflow begins its operation by importing a collection of sequences in fasta format. the design only assumes that the user has a collection of libraries generated from individual samples . the only other information input to waters is a file that allows the user to assign a single metadata table to the input libraries. each line of this file corresponds to one library, and any number of columns corresponding to distinct variables may be included in the table and used to group data during downstream analysis. this metadata is used, for example, to indicate from which environment or experimental condition each library was generated.

sequence alignment
for the first step, sequence alignment, two options are available. the default is the recently released infernal package  <cit> . infernal has the advantage over previous alignment methods that it takes into account the secondary structure of the  <dig> s rrna molecule, and that it is extremely fast. because it is able to discern homologous positions of secondary structure it can more easily and accurately determine group-specific insertions, which it subsequently removes from the alignment. infernal also very efficiently performs its alignment one sequence at a time, and thus can take advantage of parallel processing capabilities, e.g., of linux clusters.

alternatively, the stap aligner, which uses the clustalw algorithm for alignment and is part of the stap package for taxonomy assignment  <cit> , may be employed. while the stap aligner does not take into account rrna secondary structure, it also is fast and is the same alignment method used during the downstream taxonomy assignment step . additionally, many short, bar-coded  sequences are from the variable regions of the  <dig> s molecule, which infernal is unable to align. providing this alternate aligner gives the user flexibility depending on the type of sequences they are using and allows for comparisons between the two approaches.

chimera removal
during the pcr amplification process two non-identical single-stranded pieces of dna occasionally will anneal together at regions of high sequence identity. dna polymerase can amplify such hybrid pieces of dna because one piece of single-stranded dna can serve as a primer for dna replication and thus will lead to chimeras when the "primer" portion is of separate origin from the part added downstream of the primer. this will essentially contaminate the pcr product with chimeric sequences that were not present in the original sample. because these artificial sequences can skew the interpretation of the real data, they should be  removed from the input sequence libraries before further analysis. however, bioinformatics tools for detecting chimeras are still relatively new, and only a few programs are available to choose from. the mallard program  <cit> , based on the pintail algorithm  <cit> , is widely-used for chimera removal and was selected for use in waters. however, to automate mallard from within waters its graphical user interface  had to be eliminated for automation of the algorithm, although the program's parameters are configurable within the actor  and all of the original confidence interval options are available. an additional chimera checking program, bellerophon, was considered because it employs a different chimera detection methodology  <cit>  and could theoretically complement mallard. however, bellerophon is not yet available as a stand-alone package. chimera detection algorithms continue to be optimized and improved, and, in time, perhaps the chimera-removing actor in waters can be upgraded and replaced as newer software becomes available.

determination of otus
a key step in determining the total number of times a particular microbe is represented in a library is the clustering of highly similar sequences together to infer operational taxonomic units . otus are akin to molecular microbial species and are commonly based on 97% or 99% sequence identity levels. a single representative sequence is chosen from each cluster, and the count of sequences in the cluster can be used to compute the relative proportion of the corresponding otu in the total collection of sequences. those sequences not chosen to represent the otu in which they were clustered may be excluded from further analysis because their presence is included in the total sequence count for the otu.

a new piece of software, otuhunter , is used for clustering sequences and identifying otus in waters. otuhunter uses an implementation of the markov clustering  algorithm to group sequences together . briefly, otuhunter first calculates an all-by-all similarity matrix  from the alignment of the sequences in all libraries. it then simulates random walks via the mcl algorithm on the graph represented by the similarity matrix and measures the flow until the algorithm ends in a nearly idempotent matrix . the resulting matrix is interpreted as the clusters, and the sequence in a cluster over which the most flow went is chosen as a representative of that cluster . if there is more than one sequence with the same amount of flow within a cluster, one of them will be chosen arbitrarily as the representative.

taxonomy assignment
taxonomy assignments are made for each of the representative sequences chosen by otuhunter. the stap software  <cit>  is used for taxonomy assignments and can either be run locally or be configured to take advantage of the parallel processing capabilities of a linux cluster. as a side effect of our development of waters, stap can now be used in waters alone without pre-compilation by simply deleting all other actors in the workflow and creating a very short workflow involving the fasta input and stap actors. prior to waters, stap only could be run from the command line and after going through several installation steps.

phylogenetic tree inference
waters infers phylogenetic trees relating the otu representatives using either the neighbor-joining-based programs, fasttree  <cit>  or quicktree  <cit> , or the maximum-likelihood program, raxml  <cit> . a raxml actor had been developed previously and released as part of the kepler/ppod package  <cit> ; it submits compute jobs to the cipres  cluster at the san diego supercomputer center  <cit> . we also developed new kepler actors to run fasttree and quicktree as part of waters.

workflow development and testing
waters was created in an iterative, bottom-up manner, starting from small pieces of the overall target workflow. this iterative development allowed easy adaptation while the user requirements were still evolving. initial testing was performed by using small collections of sequences as test datasets and sequentially adding new actors to the workflow, identifying and solving error messages produced, and refining the workflow and java code until the final version was produced. first, two test users within the lab  with different programming and bioinformatics skills were employed to use the workflow and provide feedback on waters setup and usage. next, three test users outside of uc davis were asked to experiment with waters, report any bugs or problems encountered, and comment on the user manual. waters was also tested with increasingly large datasets to detect any scaling issues. testing revealed certain performance bottlenecks, both in parallelization and in memory usage, which were subsequently removed and optimized.

RESULTS
after downloading and running waters, a variety of results files are automatically generated  and are available for biological interpretation and comparison. these files and their use are discussed below. next, a discussion is presented of how the workflow can be customized and optimized for specific requirements and advanced usages. then, we discuss other types of  <dig> s rdna tool suites currently available and discuss specifically where waters fits. finally, the broad benefits of automation and its implications for the community are discussed and some of the limitations of the waters method are acknowledged.

fourteen different types of results files can be generated from one run of waters in its complete configuration. * represents the cutoffs used in otuhunter, by default  <dig> and  <dig> percent similarity, which will generate two different files at each cutoff used. abbreviations: seq, sequence; qc, quality control; otu, operational taxonomic unit.

results files automatically generated and delivered to the user
ecology statistics
one of the first questions a microbiologist may likely have about their community of interest is how many kinds of organisms do i have here? and how different is one sample from another? to answer these questions two diversity indices are calculated: chao <dig> and the shannon index. the chao <dig> diversity index  <cit>  is an ecological statistic that estimates the total number of species in a collection by taking into account the total size of the sample and the number of times a sequence was seen at least twice. this is a measure of "richness", i.e., total estimated number of organisms. a shannon-weiner index  <cit>  is also calculated, which is a measure of "evenness" that includes a measure of richness but also takes into account the relative proportion of the organisms in the collection, to convey how evenly distributed the organisms are throughout the sample.

two other global ecological results are returned: the calculation and display of rarefaction curves  and rank-abundance curves. in waters these curves are displayed as a graphical pop-up image, a saved post-script image file, and a text file that contains the x, y coordinates of the graphs. the rarefaction curve displays the increase in the number of otus as more sequences are added to the collection. the slope of the rarefaction curve indicates how well sampled a library or an environmental  variable was. for example, as the rarefaction curve begins to flatten out  along the x-axis, very few new otus are being added as new sequences are added . the rank-abundance curve plots along the x-axis, in decreasing order, the most abundant organisms in a sample and presents on the y-axis the quantity of that organism, i.e., organism rank is displayed on the x-axis and abundance on the y-axis, therefore, presenting an overview of the distribution of organisms in a sample.

library comparisons
one useful file produced by waters is the otu table . the otu table writes out the otu representatives, taxonomic description of these sequences and the total size of an otu on a per library and per metadata variable basis . the information contained in this file can be employed in many practical ways to get an overall picture of how libraries compare. for example, by dividing the abundance of each otu by the total number of high-quality reads in a library a relative proportion of each otu can be determined. these relative otu abundances can then be imported into heat-mapping software, e.g., treeview  <cit>  microarray visualization software. the heat map can then be clustered based on relative abundance and then on similarities between libraries in a manner similar to clustering gene expression patterns generated from microarray data. these relative or raw otu values or even a presence/absence view of the otus  are also formatted for use with many different statistical tests such as pca or hierarchical clustering. additionally, the otu table can be used to sum up taxonomy groups at any taxonomic level to produce taxonomy bar graphs.

two additional sets of results  are produced which are pre-formatted for use in specific software programs that allow the user to globally compare libraries. first, are the files required for phylogenetic library comparisons in unifrac  <cit> , namely a phylogenetic tree and an "environment" text file. the unifrac program allows the user to determine the statistical over- or under-representation of microbial lineages based upon their phylogeny, i.e., the unique fraction of the phylogenetic tree branch length present. second, files are formatted for use in a network-viewing program, cytoscape  <cit> . cytoscape does not take into account phylogeny, but creates a clear visual image for how similar two groups of samples are based upon the shared number of otus. for both unifrac and cytoscape files, results are generated for every similarity cutoff used in otuhunter , and they are also generated for every metadata variable comparison that the user includes.

data pruning
to assist in troubleshooting and quality control, waters returns to the user three fasta files of sequences that were removed at various steps in the workflow. a short_sequences.fas file is created that contains all sequences that were removed because they did not meet the length requirements . mallard creates a chimeras.fas file that contains sequences that were determined to be likely chimeras and thus removed. and a bad_infernal.fas file is created that contains sequences that were unalignable by infernal.

flexibility and adaptability
the basic waters workflow  contains specific default settings, but can be significantly altered and adapted in a number of different ways: e.g., the basic workflow can be changed by adding, removing, or rearranging individual actors, by changing workflow and actor parameters, and by modifying the metadata file. for runs with large amounts of data, an external mysql database can be used instead of the built-in kepler database. the following sections discuss these possibilities and the user manual gives more specific detail on how to implement and select these changes.

 <dig>  workflow design changes
many scientific workflows are, at least initially, exploratory in nature, i.e., unlike a production workflow that is used over and over again without changes to the design, exploratory workflows evolve, while the scientist is experimenting with the workflow design, testing outputs for different input datasets, parameter settings, methods used, etc. the visual programming paradigm of scientific workflows makes it fairly easy for scientists to add, remove, or substitute workflow components while exploring alternative workflow designs, in particular when a change-resilient workflow paradigm such as comad is used. conventional workflow designs often require special adaptors  and rewiring of workflows when design changes are made or when the structure of data changes. in contrast, comad can adapt to these changes via corresponding changes to its actor signatures and configuration parameters   <cit> .

beyond simple deletion and addition of existing components, workflow actors can also be exchanged to provide different methods or implementations for conceptually similar workflow steps. for example, there are two types of alignment methods, stap and infernal, infernal is the default method but can be removed and swapped out with the stap aligner. or, alternatively, the aligner can be disabled entirely if the user wishes to use their own alignment format, such as greengenes  <cit> , or has manually aligned the sequences. additionally, the phylogenetic tree methods can be switched out between raxml  <cit> , fasttree  <cit> , and quicktree  <cit> .

 <dig>  workflow and actor parameters
the second category of flexibility is to change the parameters of the overall workflow or of specific actors. for example, mallard, otuhunter, and stap each come with default parameters, but depending on the individual requirements of the user, the default settings can be changed by double-clicking on the actor and altering the parameters. mallard can be made to be more or less stringent in detecting chimeras, otuhunter can calculate otus at different percent identity cutoffs, and stap can search for taxonomy information in bacterial, archaeal, prokaryotic or eukaryotic domains of life. additionally, there is a default sequence length cutoff at the beginning of the workflow that can be changed from the default setting of  <dig> bp minimum.

 <dig>  deployment on a compute cluster
the compute-intensive actors of waters can also be executed on a linux cluster rather than locally on the scientist's desktop or laptop. deploying the analyses from these actors allows the compute time to be increased because parallel computing can be taken advantage of. the cluster-deployable actors are  the stap aligner,  the stap taxonomy assigner,  otuhunter, and  infernal. to turn on the cluster submission process, a radio button is checked and the user's account information for the cluster is entered if necessary for the user's cluster account. for extensive technical information on this process please see the online documentation wiki for more information: http://code.google.com/p/waters16s/wiki/serveractors.

 <dig>  change metadata
another way to produce new kinds of results is through the user-written metadata file. this metadata file is optional, but allows the user to describe the libraries analyzed in any combination of variables of interest. for example, many variables of interest can be added on the fly as the user becomes aware of additional experimental parameters or begins to test a new hypothesis as new variables of interest develop as a result of the original analysis. each variable column is used to create new unifrac, cytsoscape and otu table files/text. notably, because of the caching and incremental computation feature in waters, unnecessary re-computations after metadata file updates can be avoided.

 <dig>  switching to a different database
kepler has a built-in hsql database that is well-suited for smaller sequence collections of up to approximately <dig>  sequences. for analyzing larger datasets we have made waters compatible with a mysql database which will make the resulting results cache from larger runs more robust, quick and stable.

waters proof-of-principle results are biologically meaningful
to test waters we used data from a published dataset of ~ <dig>  full-length  <dig> s rdna sequences from the human colon  <cit> . the total number of otus  at the 99% similarity cutoff was comparable:  <dig> determined by waters vs.  <dig> as published. the otus were further analyzed  and show that both the otu abundance as well as the number of otus in each taxonomic group are very similar. in most taxonomic groups there is complete or near-complete numerical correspondence between the two datasets. the biggest discrepancy lies in the firmicutes clostridia group where a smaller quantity was observed in the waters results. the possible cause of this difference are unknown. the biological results  were also highly concordant with previously published results. the rarefaction analysis  indicated that the overall picture of sequence diversity was similar to published results. the unifrac analysis  indicated that the microbiota similarity was responsible for the clustering of samples together and for the separation of different individuals away from others. the topology of the phylogenetic tree  indicated that the diversity and distribution of the microbiota was also consistent with published results.

along the left are the bacterial taxonomic groups detected in the dataset. across the top are the results from waters compared to the previously published results. columns  <dig> and  <dig> provide the total abundance of all otus in that taxonomic category. columns  <dig> and  <dig> provide the number of discreet otus observed in that taxonomic group. otu abundance data for the eckburg et al. dataset can be found on page  <dig> of the original publication's supplemental material  <cit> .

waters was also used to analyze the nine small bowel transplant libraries described in hartman et al.  <cit> . these sequence collections were used to optimize and develop waters at both a small scale  <cit>  and large scale  of sequence collections. the data were analyzed using waters and the results described were derived from output files generated by waters. the results from these analyses were used to set the default parameters in waters.

advantages of using a scientific workflow approach
the automation of  <dig> s rrna analysis through a workflow system offers several advantages. first and most obviously, waters dramatically increases the efficiency of these analyses. waters saves both human time and compute-time thus allowing scientists to focus on result analysis and biological interpretation rather than on repetitive and often error-prone manual data handling tasks. waters also provides new means for the validation, "debugging", and reproducibility of results: the workflow produces log and trace files that capture exactly what operations and calculations have been performed and how derived products were obtained from their inputs.

these provenance capabilities  <cit>  provided by waters through kepler effectively turn the system into an automatic lab notebook that records details that are normally not captured and published. with increasingly powerful means to browse and query provenance in kepler  <cit>  and similar systems such as taverna and vistrails  <cit> , and with the emergence of standards for data provenance such as open provenance model , it will only become easier in the future to interpret, validate and re-analyze workflow results, both for the original experimenters and for other users.

the use of a workflow approach also increases the size of a workable dataset, and, in parallel, should greatly decrease user-mediated error. our own experience of performing large-scale analysis with self-written perl scripts strung together with shell commands has taught us that unintentional bioinformatics errors can be quite common in practice and may be easily perpetuated throughout an entire dataset. by handing over the bookkeeping to kepler and concentrating efforts on the results and interpretation, the impact of human error is minimized, particularly those borne out of minimal expertise or minimal software familiarity. ultimately, this allows for larger, more complex analysis and comparisons, which should enhance the broader field of microbial ecology. furthermore, because the analysis processes always occur in the same manner, waters could be very useful for comparing and contrasting different published datasets to each other.

tying together software written in different programming languages, requiring different inputs, returning varied outputs, and expecting different types of user interactions into one workflow program is a large technical challenge. the technical considerations to building a robust system composed of these varied and different software parts were quite high. this is one of the first examples of a wide distribution of a fully self-contained kepler workflow package  and, to the best of our knowledge, is the first comprehensive and extensible  <dig> s rdna analysis package of its kind.

known limitations
the current waters release bundles a number of external software components and is initially available and supported on mac os × . ports to other platforms are planned for the future . instructions for how to set up and use waters on other operating systems are available on the waters web site; however, they are currently not well supported or tested.

some data and metadata entry is currently file-based, i.e., users have to conform to the specific file formats to ensure the workflow executes as expected. in the future, we plan to include a more convenient form of data and metadata entry, e.g., using a configuration wizard. this will also minimize execution errors due to formatting errors in the input files.

similarly, deploying waters on a cluster requires the user to make sure the cluster-setup has been done correctly. working with distributed computing resources and similar parallel computing middleware is inherently more complex than working with a single machine. the emergence of virtual computing environments and cloud computing services should make deployment of waters on parallel platforms much more easy and fault-tolerant in the future.

using our machines and setup, we could run waters roughly around  <dig>  full-length sequences. this limit is imposed by java memory limits in kepler and in otuhunter clustering, but will vary somewhat depending on the similarity of sequences run; more similar sequences will lower the sequence limit because the clustering becomes more memory intensive. despite these limitations, waters provides end users with a new, more intuitive and user-friendly approach than has currently been available for  <dig> s rdna analysis.

related 16s rdna analysis tools
mothur  <cit> is a tool with similar aims as waters, but which uses a more conventional command-line approach instead of employing a scientific workflow system approach. the software tools it incorporates are different from those available in waters. table  <dig> shows a comparison of mothur, waters, and other tools at the time of writing:

three large websites are specifically dedicated to the purpose of  <dig> s rdna microbial ecology, greengenes  <cit> , rdp-ii  <cit> , and silva  <cit> . each website has many, but not all, of the tools required for a high quality community analysis. additionally, they all depend on web services, which inherently limit autonomy and sometimes limit the speed and the amount of data that can be processed . furthermore, these websites currently do not return data in advanced formats to be used in programs like unifrac  <cit> , cytoscape  <cit>  or heat map visualization. they also do not report ecological calculations like diversity metrics, rarefaction analysis or rank-abundance curves.

recently a new pipeline, email-based tool for short bar-coded  <dig> pyrosequencing datasets became available within rdp . the idea and concept are also similar to waters, but the input data is limited to  <dig> reads  rather than full-length reads as waters is designed for. it is included in the table for comparison purposes, but is not designed for the same type of data. table  <dig> compares waters to the available  <dig> s rrna gene analysis web services as well as stand-alone programs that serve similar functions.

in summary, waters provides the community with a comprehensive and flexible  <dig> s rdna analysis platform in a single stand-alone software package. waters can be installed on the user's local machine, but also scales to very large datasets with its support for cluster deployment of several compute-intensive steps. the waters workflow can also be customized and evolved, according to the user's needs.

future directions
as new sequencing methods continue to be developed, with decreasing costs, waters can be easily adapted for the data delivered by those "next generation" sequencing technologies. specifically, short bar-coded sequences of the variable regions from pyrosequencing require a great deal of pre-screening and quality control. in order to adapt waters to pyrosequencing reads, new actors that perform these functions need to be implemented. furthermore, otuhunter is currently the rate-limiting computation within the workflow and would either need to be optimized or replaced for datasets that exceeded roughly  <dig>  sequences or more as pyrosequencing datasets would surely do.

fortunately, though, besides additional screening and clustering, many of the downstream actors in waters would continue to be useful, and, in fact, if widely adapted, would allow for easier comparisons between methods or between datasets made with two different technologies. the beauty of the workflow system is modularity. waters grants an opportunity to expand, adapt, and change without disrupting the functional parts that already exist.

CONCLUSIONS
we present here a new, automated, workflow system to analyze  <dig> s rdna clone libraries. the system is flexible, evolvable, and modular; workflow results can be inspected and validated using the built-in data provenance sub-system, facilitating the reproducibility of results. waters should increase community-wide the transparency of results and data management as well as allow new, less-experienced users to perform analyses at a very high technical level that might otherwise be too overwhelming. waters increases efficiency and saves time; ultimately allowing the user to concentrate their efforts on more biologically-engaging questions about microbial communities rather than on repetitive bioinformatics tasks. in short, microbial ecologists do not need to be constrained by their programming abilities in order to ask penetrating comparative microbial ecology questions if they employ waters.

availability and requirements
• project name: waters

• project home page: waters.genomecenter.ucdavis.edu; user manual and problem-solving wiki also available here

• operating system: mac os  <dig>  and  <dig> 

• other requirements: java 5

• license: mit open-source

waters can be downloaded from waters.genomecenter.ucdavis.edu, which re-directs to a google code project page. the download is large  due to the dependencies of the stap application take up around  <dig> megabytes, and the self-contained installation of perl and tcl add another  <dig> megabytes. the remainder is the source code and resources that make up kepler.

on the site, a wiki contains several entries that explain how to use waters at several levels of complexity. the issues section of the site allows users to report any bugs encountered to the developer . currently, the stand-alone version of waters  is available for mac os × . windows and linux users will need to pre-install several programs before using waters and should follow the instructions on the wiki.

list of abbreviations
waters: workflow for alignment, taxonomy, and ecology of ribosomal sequences; rdna: ribosomal dna

authors' contributions
alh, jae, and bl wrote the paper; alh, sr, tm, bl, jae designed research; sr programmed waters; alh and sr tested waters. all authors read and approved the final manuscript.

