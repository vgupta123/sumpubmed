BACKGROUND
information theory  <cit> , the mathematical theory of communication, has been successfully used to address a number of important questions about sensory coding  <cit> . for example, information theoretic tools have been used to characterize the stimulus selectivity of neurons, by revealing the precise sensory features  which modulate most reliably the responses of neurons or of neural populations  <cit> . information theory has been used to investigate whether the fine temporal structure of neural activity contains important information which is lost when neural signals are averaged over longer time scales. these studies have shown that the precise timing of spikes measured with respect to the stimulus onset  <cit> , or the relative time of spikes with respect to the ongoing network fluctuations  <cit>  provides important information that cannot be extracted from the spike count. information theory has also been used to study population codes: several studies have developed measures to quantify the impact of cross-neuronal correlations in population activity  <cit>  and have applied them to the study of population coding across various sensory modalities  <cit> . another application of information theory to neuroscience is the measure of causal information transfer  <cit>  to quantify the amount of interactions between neural populations  <cit> .

information theory has been used widely for the analysis of spike trains from single neurons or from small populations  <cit> , and is now beginning to be applied systematically to other important domains of neuroscience data analysis, such as the information analysis of analog brain signals like bold fmri responses  <cit>  and local field potentials   <cit> . however, the application of information theory to analog brain signals has remained relatively limited compared to the potentials it offers. one reason that has limited a wider use of the information analysis to analog signals is that estimates of information from real neurophysiological data are prone to a systematic error  due to limited sampling  <cit> . the bias problem can be alleviated by the use of several advanced techniques  <cit>  or by combinations of them  <cit> . however, a detailed implementation and testing of these  techniques is in many cases beyond the time resources available to experimental laboratories. moreover, the performance of bias correction techniques was so far tested thoroughly primarily on spike trains, and much less on analog brain signals. since the performance of bias correction techniques depends on the statistics of the data  <cit> , it is crucial that tests and comparisons of the performance of the various techniques are also carried out on analog brain signals. clearly, the availability of a toolbox that implements several of these accurate information calculation techniques after having validated them on analog brain signals would greatly increase the size of the neuroscience community that uses information theoretic analysis.

another problem, which is particularly prominent when computing information quantities for multiple parallel recordings of neural activity, is the speed of computation. multielectrode recordings now allow the simultaneous measurement of the activity of tens to hundreds of neurons, and fmri experiments allow a broad coverage of the cerebral cortex and recording from a large number of voxels. therefore speed of calculation is paramount, especially in cases when information theory is used to shed light on the interactions between pairs or small groups of recorded regions. in fact, the number of these subgroups  increases fast with the number of recorded regions.

this article aims at meeting the demand for fast and publicly available routines for the information theoretic analysis of several types of brain signals, by accompanying and documenting the first release of the information breakdown toolbox . this toolbox can be downloaded from the url  or from the additional files provided with this article . the toolbox has several key features that make it useful to the neuroscience community and that will widen the domain of application of information theory to neuroscience. in particular  it can be used within matlab , one of the most widely used environments for the collection, preprocessing and analysis of neural data,  it is algorithmically optimized for speed of calculation  it implements several up-to-date finite sampling correction procedures  it has been thoroughly tested for the analysis of analog neural signals such as eegs and lfps  it implements the information breakdown formalisms  <cit>  that are often used to understand how different groups of neurons participate in the encoding of sensory stimuli.

definitions and meaning of neural entropies and information
before proceeding to describe the implementation of the toolbox and to discuss its use, in the following we will briefly define the basic information quantities and describe their meaning from a neuroscientific perspective.

consider an experiment in which the experimental subject is presented with ns stimuli s <dig> ... and the corresponding neural response r is recorded and quantified in a given post-stimulus time-window. the neural response can be quantified in a number of ways depending on the experimental questions to be addressed and on the experimenter's hypotheses and intuition. here, we will assume that the neural response is quantified as an array r =  of dimension l. for example, the experimenter may be recording the spiking activity of l different neurons and may be interested in spike count codes. in this case ri would be the number of spikes emitted by cell i on a given trial in the response window. alternatively, if the experimenter is recording the spiking activity of one neuron and is interested in spike timing codes, the response could be quantified by dividing the post-stimulus response window into l bins of width Δt, so that ri is the number of spikes fired in the i-th time bin  <cit> . or, the experimenter may be recording lfps from l channels and be interested in how their power carries information. in this case ri would be quantified as the lfp power in each recording channel  <cit> .

unless otherwise stated, we will assume that the neural response in each element of the response array is discrete. if the signal is analogue in nature – such as for lfp or eeg recordings – we assume it has been discretized into a sufficient number of levels to capture the most significant stimulus-related variations. for example, the power at l distinct frequencies extracted from a single-trial lfp recording will be quantized, for each frequency, into a finite number of levels  <cit> . this assumption is  necessary for the analysis and the correct functioning of the algorithms in the toolbox. the reason why it is convenient to quantify the neural response as a discrete variable is that it makes it easier to quantify the probabilities necessary for information calculation . for analog signals, the discretization can be circumvented only when there are suitable analytical models for the probability distribution , in which case particular algorithms can be applied .

having defined the response, we can quantify how well it allows us to discriminate among the different stimuli by using shannon's mutual information  <cit> :

   

the first term in the above expression is called the response entropy. it quantifies the overall variability of the response and is defined as

   

p being the probability of observing r in response to any stimulus. the second term, called noise entropy, quantifies the response variability specifically due to "noise", i.e. to trial-to-trial differences in the responses to the same stimulus. h is defined as

   

where p is the probability of observing response r when stimulus s is presented; p is the probability of presentation of stimulus s, and is defined as , ntr being the number of trials available for stimulus s and  is the total number of trials to all stimuli. mutual information quantifies how much of the information capacity provided by stimulus-evoked differences in neural activity is robust to the presence of trial-by-trial response variability. alternatively, it quantifies the reduction of uncertainty about the stimulus that can be gained from observation of a single trial of the neural response.

the mutual information has a number of important qualities that make it well suited to characterizing how a response is modulated by the stimulus. these advantages have been reviewed extensively  <cit> . here we briefly summarize a few key advantages. first, as outlined above, it quantifies the stimulus discriminability achieved from a single observation of the response, rather than from averaging responses over many observations. second, i takes into account the full stimulus-response probabilities, which include all possible effects of stimulus-induced responses and noise. thus, its computation does not require the signal to be modeled as a set of response functions plus noise and can be performed even when such decomposition is difficult. third, because information theory projects all types of neural signals onto a common scale that is meaningful in terms of stimulus knowledge, it is possible to analyze and combine the information given by different measures of neural activity  which can have very different signal to noise ratios.

the contribution of correlations between different neural responses to the transmitted information
neural signals recorded from different sites are often found to be correlated. for example, spikes emitted by nearby neurons are often synchronous: the probability of observing near-simultaneous spikes from two nearby neurons is often significantly higher than the product of the probabilities of observing the individual spikes from each neuron  <cit> . lfps recorded from different electrodes are typically correlated over considerable spatial scales  <cit> . moreover, neural signals can be correlated in time as well as in space. in fact, different temporal aspects of the neural activity from the same location are often correlated. for example, spike counts and latencies covary in some systems  <cit> , and the powers of different lfps bands recorded from the same location can also exhibit significant correlations  <cit> .

the ubiquitous presence of correlations of neural activity across both space and time has raised the question of what is the impact of this correlation upon the information about sensory stimuli carried by a combination of distributed sources of neural activity . theoretical studies have suggested that correlations can profoundly affect the information transmitted by neural populations  <cit> . it is therefore of great interest to quantify the impact of correlations on the information carried by a population of simultaneously recorded neurons. mutual information, eq. , can tell us about the total information that can be gained by simultaneously observing the l considered neural responses, but not about the specific contribution of correlations to this total value. however, a number of information-theoretic quantities have been developed to quantify how correlations affect the information carried by different neural signals . in our information toolbox we have implemented several of such approaches, which will be briefly define and summarize in the rest of this section.

different types of correlations affecting information
before we describe the information theoretic tools for quantifying the impact of correlations on coding, it is useful to briefly define the types of correlations usually considered in the studies of neural population activity.

the most commonly studied type of correlation of neural activity is what is traditionally called "noise correlation", that is the covariation in the trial-by-trial fluctuations of responses to a fixed stimulus  <cit> . because these noise covariations are measured at fixed stimulus, they ignore all effects attributable to shared stimulation. although we will stick with the well-established "noise" terminology, we point out that the name is potentially misleading since noise correlations can reflect interesting neural effects. mathematically speaking, we say that there are noise correlations if the simultaneous joint response probability p at fixed stimulus is different from the "conditionally independent" response probability

   

the conditional probability pind can be computed by taking the product of the marginal probabilities of individual elements of the response array, as in eq. , or alternatively by the empirical "shuffling" procedure described as follows. one generates a new set of shuffled responses to stimulus s by randomly permuting, for each element of the response array, the order of trials collected in response to the stimulus s considered, and then joining together the shuffled responses into a shuffled response array. this shuffling operation leaves each marginal probability p unchanged, while destroying any within-trial noise correlations. the distribution of shuffled responses to a given stimulus s is indicated by psh.

many authors further distinguish noise correlations  from "signal correlations"  <cit> , which are correlations entirely attributable to common or related stimulus preferences. signal correlations manifest themselves in similarities across stimuli between the response profiles of the individual elements of the response array. for example, neurons in different channels may all have a very similar mean response to the stimuli. there are several ways to quantify the amount of signal correlations  <cit> . we will not report them in this article, but we will only focus on quantifying their impact on the information carried by the response array .

the importance of separating noise from signal is, as revealed by theoretical studies, that signal and noise correlations have a radically different impact on the sensory information carried by neural populations . in particular, signal correlations always reduce the information, whereas noise correlations can decrease it, increase it or leave it unchanged, depending on certain conditions  <cit> .

information breakdown
we next describe and define briefly several mathematical techniques to quantify the impact of correlations of information. several different approaches have been proposed . here we present one, called the "information breakdown"  <cit> , which takes the total mutual information i and decomposes it into a number of components, each reflecting a different way into which signal and noise correlations contribute to information transmission. we decided to focus on the information breakdown formalism partly because it was developed by one of the authors of this article, and partly because it naturally includes several of the quantities proposed by other investigators  <cit> .

the information breakdown writes the total mutual information into a sum of components which are related to different ways in which correlations contribute to population coding  <cit> , as follows:

   

the meaning and mathematical expression of each of the components is summarized in figure  <dig> and is described in the following.

the linear term ilin is the sum of the information provided by each element of the response array. this is a useful reference term because if all the elements of the array were totally independent  then the total information transmitted by the response array would be equal to ilin.

the amount of synergistic information syn. the difference between i and ilin is called synergy. positive values of synergy denote the presence of synergistic interaction between elements of the response array, which make the total information greater than the sum of that provided by each element of the response array. negative values of synergy  indicate that the elements of the response array carry similar messages, and as a consequence information from the response array is less than the sum of the information provided by each individual element. the synergy can be further broken into the contributions from signal and from noise correlations, as follows.

the signal similarity component isig-sim is negative or zero and quantifies the amount of redundancy specifically due to signals correlation. we note that the negative of isig-sim equals the quantity named Δisignal which was defined in ref.  <cit> .

the noise correlation component icor quantifies the total impact of noise correlation in information encoding. originally introduced in  <cit> , it equals the difference between the information i in the presence of noise correlations and the information iind in the absence of noise correlation.  is the information obtained replacing pind for p and pind for p in the entropies entering eq. ). icor quantifies whether the presence of noise correlations increases or decreases the information available in the neural response, compared to the case where such correlations are absent but the marginal probabilities of each element of the response array are the same. icor can be further broken into two terms icor-ind and icor-dep, as follows:

   

the stimulus independent correlational term, icor-ind, reflects the contribution of stimulus-independent correlations. in general, if noise and signal correlations have opposite signs, icor-ind is positive. in this case, stimulus-independent noise correlations increase stimulus discriminability compared to what it would be if noise correlations were absent  <cit> . if, instead, noise and signal correlations have the same sign, icor-ind is negative and stimuli are less discriminable than the zero noise correlation case. in the absence of signal correlation, icor-ind is zero, whatever the strength of noise correlation.

the stimulus dependent correlational term icor-dep is a term describing the impact of stimulus modulation of noise correlation strength. icor-dep is non-negative, and is greater than zero if and only if the strength of noise correlation is modulated by the stimulus. icor-dep was first introduced in ref.  <cit>  with the name Δi. icor-dep is an upper bound to the information lost by a downstream system interpreting the neural responses without taking into account the presence of correlations  <cit> .

all quantities in the information breakdown can be expressed in terms of the six quantities h, h, hlin, hind, hind, and χ where h, h were defined above in eqs. , and:

  

the components of the information breakdown can be quantified from the above quantities as follows:

  

implementation
computing environment
our information breakdown toolbox  has been implemented in matlab  and c taking advantage of matlab's mex technology. it has been tested on several platforms  and it can be downloaded, together with a documentation for its installation and its use, from our website  <cit> .

data input/output
the main routine in the toolbox is entropy.m  which allows the computation of the fundamental quantities necessary for the computation of the terms which appear in the breakdown decomposition. this routine receives as input a matrix storing the l responses to each trial for each stimulus. the user also needs to specify the estimation method, the bias correction procedure and which entropy quantities need to be computed. these can be any of the following: h, h, hlin, hind, hind, χ and hsh, hsh .

as shown in figure  <dig>  two routines are available for the pre-processing of the input to entropy.m. buildr.m allows to build the response-matrix, to be fed to entropy.m, starting from l +  <dig> -long arrays: the first array stores a list specifying, for each trial, which stimulus was presented to the subject while the l remaining arrays specify the l corresponding recorded responses. binr.m allows to discretize continuous response-matrices – prior to calls to entropy.m with the the direct method – according to a binning method chosen among a list of available binning options. this list includes the equi-populated binning  and different types of equi-spaced discretizations. users are also given the opportunity to define their own binning strategies by easily linking their binning routine to binr.m .

finally, information.m is a wrapper around entropy.m which directly computes the breakdown terms by combining the outputs from this main function. its input is identical to that of entropy.m except for the list of possible outputs options which can be any or several of the following: i, ish, syn, synsh, ilin, isig-sim, icor, icor-sh, icor-ind, icor-dep and icor-dep-sh.

direct method
the direct method  <cit>  for computing information and entropies consists in estimating the probabilities of the discrete neural responses by simply computing the fraction of trials in which each response value is observed, and then by simply plugging into the information and entropy equations the response probabilities estimated in this way. in this subsection, we describe a novel, computationally optimized algorithm for computing the empirical estimates of the probabilities, which is at the core of our toolbox and is the principal reason for its speed.

to describe this algorithm, let's consider, as an example, the calculation of the response probability p. its direct estimator is given by

  

where c is the number of times the response r has been observed within the total number of available trials . by plugging  into equation , we obtain the direct response-entropy estimator

   

the steps required for computing  according to  are the following. first the routine has to run through all of the  available trials and compute the c values. the program must then loop through all the nr possible r responses, normalize each c by  and sum the c log c values together:

   for trial from  <dig> to 

      read r in current trial

      c → c + 1

   end loop

   for r from  <dig> to nr

      

      

   end loop

a problem with this approach comes from the rapid growing of nr as l increases. let us assume that each element in the response array can take on m different values. we have nr = ml. one can thus see that estimating the information quantities according to this two-loops method becomes prohibitive for nc and m larger than a few units. however, some simple algorithmic observation can help simplifying the problem significantly. we have

   

where ℋ = ∑r f) and f  denotes the function f = x · log x.

first of all, equation  suggests that, instead of normalizing each count c and then summing over r, we can instead first compute ℋ and then normalize. this has the advantage of reducing the number of division operations from nr to just one.

equation  tells us even more. suppose that an additional trial is provided in which the response  is observed. the total number of trials thus increases from  to  +  <dig>  we also need to update the value of c() by incrementing it by one. as a result of this change ℋ is increased by an amount  given by

  

this observation suggests that, instead of computing the final value of c we can update ℋ directly at each trial, inside the first loop, therefore skipping the second loop over the nr responses. the procedure for computing  thus becomes:

   for trial from  <dig> to 

      read r in current trial

      ℋ → ℋ + f + 1) - f)

      c → c + 1

   end loop

   normalize ℋ

where the length of the loop is determined only by the number of available trials.

the previous procedure can be extended to the direct computation of ,  and . since the argument of f is always an integer, we can store the values computed for f and use them for the computation of all four entropic quantities. in the current implementation of the toolbox, the values of f persist in memory as long as  does not change. calls to the toolbox with matrices with the same number of trials perform increasingly faster and the maximum computation speed is achieved when all values of f for x =  <dig> ...,  have been computed.

finally, let's describe the computation of  and χ. consider a very simple example in which only two responses are recorded  each of which can only take two possible values  <dig> and  <dig>  for each stimulus s we can thus build the probability arrays

  

where we used the compact notation pi = p . if we now wish to build a probability array for pind, as done for the stimulus-conditional response probabilities, we have

   

where p <dig> ⊗ p <dig> denotes the kronecker's product between the two probability arrays p <dig> and p <dig>  equation  can be extended to any l and any number of values taken by the responses

   

the number of products required to compute pind and, consequently, also the time required to compute pind,  and χ, increases rapidly together with the number, l, of responses.

since l is not known a priori but it is chosen by the user, in order to minimize the number of multiplications and of iterations while also reducing the overhead due to calls to sub-functions, we implemented eq.  by recursively partitioning the problem into half until pairwise products are reached. for example, for l =  <dig> the routine performs the products in the following order:

  

bias correction for the direct method: plug-in vs bias-corrected procedures
the direct method relies on the empirical measure of the response probabilities as histograms of the fraction of trials in which each discrete response value was observed. naturally, this procedure gives a perfect estimate of the information and entropies only if the empirical estimates of the probabilities equal the true probabilities. however, any real experiment only yields a finite number of trials from which these probabilities have to be estimated. the estimated probabilities are thus subject to statistical error and necessarily fluctuate around their true values. if we just plug the empirical probabilities into the information equations , then the finite sampling fluctuations in the probabilities will lead to a systematic error  in the estimates of entropies and information  <cit> . in some cases, the bias of the plug-in information estimate can be as big as the information value we wish to estimate. it is therefore crucial to remove this bias effectively in order to avoid serious misinterpretations of neural coding data  <cit> .

next, we describe and compare four bias correction procedures that we implemented in our toolbox. these procedures, which are among those most widely used in the literature, were selected for inclusion in our toolbox because they are applicable to any type of discretized neural response , because they are  among the most effective, and because they are guaranteed to converge to the true value of information  as the number of trials  increases to infinity.

quadratic extrapolation 
this bias correction procedure  <cit>  assumes that the bias can be accurately approximated as second order expansions in 1/, that is

   

where a and b are free parameters that depend on the stimulus-response probabilities, and are estimated by re-computing the information from fractions of the trials as follows. the dataset is first broken into two random partitions and the information quantities are computed for each sub-partition individually: the average of the two values obtained  from the two partitions provides an estimate corresponding to half of the trials. similarly, by breaking the data into four random partitions, it is possible to obtain estimates corresponding to a fourth of the trials. finally, a and b are extrapolated as parameters of the parabolic function passing through the / <dig> and / <dig> estimates.

panzeri & treves  bias correction
this correction technique computes the linear term a in the expansion  through an analytical approximation rather than from the scaling of the data of the qe procedure. this approximation depends on the number of response bins with non-zero probability of being observed which is estimated through a bayesian-like procedure. the implementations of this algorithm in our toolbox follows closely the one originally described in ref.  <cit> .

the shuffling  procedure
obtaining unbiased information estimates is particularly challenging when the response array is multidimensional , because in this case the number of possible different responses grows exponentially with l and it becomes difficult to sample experimentally the response probability. this difficulty arises because, as previously discussed, different elements of the response array are usually correlated. as a consequence, the sampling of the full probability of a response array cannot be reduced to computing the probabilities of each individual array element  as would be legitimate if the response-elements were independent. fortunately, a technique  can keep the bias introduced by correlations under control, thereby greatly improving our ability to estimate multi-dimensional information. this method  <cit>  consists of computing information i not directly through eq. , but through the following formula:

   

where hsh is the shuffle noise entropy, i.e., the noise entropy computed after randomly permuting, independently for each response, the order of trials collected in response to a stimulus ). ish has the same value of i for infinite number of trials but has a much smaller bias for finite , owing to the bias cancelation created by the entropy terms in the right hand side of eq. .

it should be noted that, if one is interested in breaking down ish ) into the information components of  <cit> , then syn, icor and icor-dep need to be re-defined as follows:

  

this three shuffled-corrected quantities, synsh, icor-sh and icor-dep-sh, converge to the same values of their uncorrected counterparts syn, icor and icor-dep, respectively, for infinite number of trials. however the bias of the shuffle-corrected quantities is much smaller when the number of trials is finite. this is especially critical for the computation of icor-dep which is by far the most biased term of the information breakdown  <cit> .

bootstrap correction
the bootstrap procedure  <cit>  consists of pairing stimuli and responses at random in order to destroy all the information that the responses carry about the stimulus. because of finite data sampling, the information computed using the bootstrapped responses may still be positive. the distribution of bootstrapped information values  can be used to build a non-parametric test of whether the information computed using the original responses is significantly different from zero. moreover, the average of the bootstrapped values instances can be used to estimate the residual bias of the information calculation, which can be then subtracted out. the bootstrap evaluation and subtraction of the residual error can be applied to any method to compute information , with or without one of the bias correction procedures described above. in our toolbox, bootstrap estimates can be computed for the quantities h, hlin, hind, hind, χ and hsh from which bootstrapped estimates of i and ish are easily obtained. the remaining quantities, h and hsh, are not affected by the bootstrapping.

gaussian method
the direct method, being based on empirically computing the probability histograms of discrete or discretized neural responses, does not make any assumption on the shape of the probability distributions. this is a characteristic which makes the direct method widely applicable to many different types of data.

an alternative approach to the direct estimation of information is to use analytical models of the probability distributions; fit these distributions to the data; and then compute the information from these probability models. this method has been applied so far relatively rarely in neuroscience . in fact this approach may prove difficult to apply to distributions of spike patterns since in this case appropriate analytical forms of probability distributions are usually not available. however, several situations exist for which it is possible to fit the response distribution to gaussian functions, especially when dealing with analog brain responses and their transformations, such as fourier transforms . the gaussian method for computing information and entropies is the one based on fitting response probabilities to gaussian functions.

under the gaussian hypothesis, the noise and response entropy and the information are given by simple functions of their variance  <cit> 

   

   

   

where |σ2| and || are the determinants of the matrices of covariance computed across trials and stimuli, and across trials to stimulus s, respectively.

note that the gaussian method – which we implemented using a straight computation of variances which are then fed into the above equations – does not necessarily require data discretization prior to the information calculation.

the advantage of the gaussian method is that it depends only on a few parameters that characterize the neural response , and is thus more data-robust, and less prone to sampling bias than the direct calculation. the potential danger with this approach is that the estimates provided by eq.  may be inaccurate if the underlying distributions are not close enough to gaussians.

although less severe than in the direct case, the upward bias of the information calculation due to limited sampling is still present when using the gaussian method. when the underlying distributions are gaussian and when no discretization is used for the responses, an exact expression for the bias of  and  can be computed. the result is as follows  <cit> :

   

   

where gbias is defined as

  

and ψ is the polygamma function .

our toolbox allows the computation of gaussian estimates without bias correction, with the analytical gaussian bias correction, eqs. , and also with the quadratic extrapolation correction qe. however, using simulated data, we found that quadratic extrapolation did not correct well for bias for the gaussian method . this can be understood by noting that eqs.  indicate that a quadratic data scaling may not necessarily describe well the bias in the gaussian case.

the gaussian method can also be used to compute the terms ilin, isig-sim and icor of the information breakdown  <cit>  by approximating, hind with hsh, the response entropy computed after shuffling the neural responses at fixed stimulus. note that in this case also hind needs to be replaced by hsh. the calculations of all these quantities is implemented in the toolbox. note that the quantity χ cannot be easily computed with the gaussian method, thereby preventing the separation of icor into icor-ind and icor-dep.

RESULTS
in the following we present several case studies and tests of the performance of our ibtb toolbox on analog neural signals, such as eegs and lfps. we emphasize that the toolbox can be effectively applied to spike trains as well as to eegs and lfps. the reason why we focus our presentation on eegs and lfps is that the very same algorithms implemented in our toolbox have been already illustrated and tested heavily on spike trains; therefore the illustration and test on eegs and lfps is more interesting. we however report that we have thoroughly tested our toolbox on spike trains. in particular, we have used our toolbox to successfully replicate a number of previously published spike train information theoretic studies from our group, reported in refs.  <cit> .

finite sampling bias corrections of information measures from analog neurophysiological signals
we start by testing the performance of bias correction procedures on simulated data. these procedures have been previously tested on simulated spike trains  <cit> , but not yet on analog neural signals. we therefore tested the bias corrections on realistically simulated lfps whose statistical properties closely matched those of real lfps recorded from v <dig> of an anaesthetized macaque in responses to hollywood color movies presented binocularly to the animal . details of the simulations procedure are reported in appendix a. our goal is to estimate the information  carried by the two-dimensional response array made of the power of the simulated lfp at frequencies  <dig> and  <dig> hz. the power in each frequency was discretized into  <dig> equipopulated bins. choosing the boundaries of the discretization bins so that each bin is equi-populated is a simple but effective way to obtain high information values even when discretizing an analog signal into a relatively small number of bins. this is because equipopulated binning maximizes the response entropy h that can be obtained with a given number of response bins  <cit> .

in order to illustrate both the origin and magnitude of the bias, it is useful to start the analysis by considering the sampling behavior of the plug-in direct estimator . figure 3a shows that the plug-in estimates of h decrease when decreasing the number of trials. that is, finite sampling makes plug-in entropy estimates biased downward. intuitively, this can be understood by noticing that entropy is a measure of variability: the smaller the number of trials the less likely we are to fully sample the full range of possible responses and the less variable the neuronal responses appear. consequently, entropy estimates are lower than their true values  <cit> .

the performance of two such procedures implemented in our toolbox  is reported in figure 3c. both corrections substantially improve the estimates of i, which, in this simulation became accurate for ntr ≈  <dig> =  <dig> /nr ≈  <dig>  to be compared with ntr/nr ≈  <dig> for pure plug-in).

finally, we considered the effect of computing information through ish  rather than through i). let us consider first the sampling behavior of the plug-in estimate of the four entropies that make up ish. because hind depends only on the marginal probabilities of the response array, it typically has very small bias . hsh has the same value of hind for infinite number of trials, but it has a much higher bias than hind for finite number of trials. in fact, figure 3a shows that the bias of hsh is approximately of the same order of magnitude as the bias of h. intuitively, this is expected because psh is sampled with the same number of trials as p and from responses with the same dimensionality  <cit> . in this simulation, the biases of hsh and h were not only similar in magnitude but actually almost identical . this, as explained in  <cit> , reflects the fact that for the data simulated here  the correlations among elements of the response array were relatively weak  <cit>  and thus hsh and h were very close both in value and sampling properties. because the biases of hsh and h almost cancel each other and since h is unbiased, the bias of ish is almost identical to that of hind. this in turn implies that the plug-in estimator of ish must have a much smaller bias than i, a fact clearly demonstrated by the results in figure 3e.

due to its intrinsically better sampling properties, ish has an advantage over i not only when using a plug-in estimation but also when using bias subtraction methods. figure 3e shows that when using pt or qe corrections, ish can be computed accurately even when using only  <dig> =  <dig> trials per stimulus /nr ≈ 2). when using an additional bootstrapping procedure to subtract the residual bias , the estimate of ish becomes almost completely unbiased, independently of the bias correction used, even with as little as  <dig> =  <dig> trials /nr ≈ 1).

it should be noted that this behavior applies to cases in which  correlations among elements of the response array are relatively weak. in conditions when the correlations among elements of the response array are very strong , then the sampling behavior of ish is still qualitatively similar to that reported here, with the main difference that in cases of stronger correlation ish tends to have a small downward  bias. this stems from the fact that in the presence of stronger correlations the bias of hsh tends to be more negative than that of h  <cit> , and was verified extensively on simulated spike trains in previous reports  <cit>  and by increasing the level of correlations in these simulated lfps .

in summary, we presented the first detailed test of bias corrections procedures  on simulated analog neural signals. these simulations  confirm that these procedures are effective also on data with statistics close to that of lfps;  show that in such case it is highly advisable to use ish as method to compute information;  indicate that evaluating and subtracting the residual bootstrap errors of ish  is particularly effective. we recommend this procedure to toolbox users interested in computing information form multidimensional lfp or eeg responses.

correlations between different frequency bands of local field potentials in primary visual cortex
in this section we illustrate the information breakdown formalism  <cit>  to study whether signal or noise correlations between the lfp powers at different frequencies made them to convey synergistic or redundant information about visual stimuli with naturalistic characteristics. for this study, we analyzed lfps recorded from the primary visual cortex of anesthetized macaques in response to a binocularly presented naturalistic color movie  <cit> . each recording site  corresponded to a well-defined v <dig> visual receptive field within the field of movie projection. from each electrode, we measured lfps as the 1– <dig> hz band-passed neural signal. each movie was  <dig> min long and was repeated  <dig> times in order to sample the probability distribution over the neural responses to each scene. full details on the experimental procedures are reported in ref.  <cit> . the correlation between the lfp activity in different frequency bands on this dataset was studied in ref.  <cit>  using only linear  correlation. here, we extend these previous results by using the information breakdown, which takes into account both linear and non-linear correlations at all orders  <cit> .

we used the information-theoretic procedure to compute how the power of lfps at these different frequencies reflects changes in the visual features appearing in the movie. we divided each movie into non-overlapping time windows of length t =  <dig>  s. each window was considered as a different "stimulus", s, and the corresponding power spectra  were considered as the neural response. from the distribution across trials of the power at each frequency and stimulus window, we computed the mutual information between the stimulus  and the joint power of the lfp at two selected frequencies f <dig> and f <dig>   to compute information, we used the direct method together with the shuffled information estimator corrected with the quadratic extrapolation bias correction and the bootstrap subtraction .

the first maximum i occurs when f <dig> and f <dig> are in the low  frequency range. a second, broader maximum is present for f <dig> and f <dig> in the high gamma range. the highest maximum, however, is obtained when combining a low power response with an high-gamma one. an interesting question is whether the lfp powers belonging to the highly informative low-frequency range and the high-gamma range carry independent or redundant information about the stimuli, and whether any potential redundancy is due to shared sources of variability  or to similarities in the tuning to different scenes of the movie . in the following, we will use the information breakdown to address this question.

let's first consider any two frequencies f <dig> and f <dig> belonging to the low frequency range. a comparison of i  and ilin  shows that i is only slightly less than ilin. thus, as made explicit in figure 4c, there is only a very small negative synergy  between low lfp frequencies. to understand the origin of this small redundancy, we used the information breakdown  <cit> . this formalism shows that low lfp frequencies have little redundancy not because they are independent, but because they share noise correlations whose effect cancel out. in particular, there is a negative stimulus-independent correlation component icor-ind  which is almost exactly compensated by a stimulus-dependent correlation component icor-dep . unlike noise correlations, signal correlations have a very little specific impact on the information carried by the two frequencies . these results suggest that the low-frequency lfp bands share a strong common source of variability and thus do not originate from entirely distinct processing pathways, even though they add up independent information about the external correlates.

we then considered the case in which f <dig> belongs to the the low frequency range while f <dig> is in the high-gamma range. in this case, the powers at f <dig> and f <dig> added up independent information about the external correlates, because the joint information i  was equal to ilin , and as a consequence there is zero synergy between  between the low and high-gamma lfp frequencies. the information breakdown  <cit>  shows that signal correlations have no impact on the information carried by the two frequencies , and that the same applies to both stimulus-dependent and stimulus-independent noise correlations . thus, low lfp frequencies and high-gamma lfps generated under naturalistic visual stimulation share neither noise nor signal correlations. they appear to be uncorrelated under naturalistic visual stimulation, and are thus likely to arise from fully decoupled neural phenomena.

we finally examined the case in which both f <dig> and f <dig> belong to the high-gamma frequency range. in this case, there is considerable negative synergy  between such frequencies. this redundancy can be attributed to signal correlation , which means that high gamma frequencies have a similar response profile to the movie scenes. the redundancy between high gamma frequencies is further enhanced by a negative effect of stimulus-independent noise correlation icor-ind, figure 4e. this can be explained by the results of a previously reported linear correlation analysis  <cit>  which suggested a presence of a small amount of positive noise correlation between high gamma frequencies that accompanies the positive signal correlation, and with the fact that a combination of signal and noise correlation with the same sign leads to a negative icor-ind.

it is interesting to note that the results obtained with the information breakdown are compatible with those obtained on the same dataset using a simpler linear signal and noise correlation  <cit> . since the information breakdown, unlike the linear correlation theory, is able to capture the impact of non-linear signal and noise correlations if they were present, the equivalence between linear correlation theory and information breakdown can be taken as strong evidence that the linear correlations individuated in  <cit>  are a sufficient description of the functional relationship between lfp responses at different frequencies.

performance of the gaussian method
in this section, we illustrate the accuracy and performance on real lfps responses of the gaussian method. we consider again the calculation of how the lfp power encodes information about naturalistic movies, and we use again the same set of lfps recorded from the primary visual cortex of an anesthetized macaque in response to a binocularly presented naturalistic color movie  <cit> , which were analyzed in the previous section. we computed the information i – about which of the  <dig>  s long movie scene in which we divided the movie was being presented – carried by the lfp power at a given frequency f. the response r was a scalar, rf, containing the power at frequency f. we estimated the information i either with the gaussian method or with the direct method.

when using the gaussian method, we first estimated the power in each stimulus window and trial using the multitaper technique. we then took the cubic root of this power; we fitted the distribution of this response to each stimulus to a gaussian; and we finally computed the information through eqs.  and  subtracting the analytic gaussian bias correction, eqs.  and . the reason for applying the cubic root transformation is that multitaper power estimates are asymptotically chi-square distributed  <cit>  thus their cubic root is approximately gaussian  <cit> . the cubic root operation, being monotonic, does not affect the underlying information values of the power, but it makes response probabilities much more gaussian and thus facilitates information estimation with the gaussian method. when using the direct method, we simply discretized these transformed power values into m equi-populated bins and computed information through eq.  and corrected it for bias using qe. the number of response bins m was varied in the range 4– <dig> .

a comparison between gaussian and direct estimates may be useful to evaluate the effect of refining the discretization of neural responses. for this dataset, we found that when using more bins to discretize responses for the direct method , the direct information values i do not change appreciably . however, when decreasing the number of bins to m =  <dig> and m =  <dig> the resulting scatterplot of i versus i was again distributed along a line  but with slopes of i =  <dig>  · i and i =  <dig>  · i, respectively. these findings suggest several conclusions. first, differences that could be observed between gaussian and direct estimates were due to loss of information caused by poor discretization when using low number of bins for the direct estimate. second, using  <dig> response bins is sufficient to capture of the information of the lfp power and using less bins leads to very moderate information losses . third, this suggests that knowledge of the second order statistics of the root-transformed power-values is sufficient for extracting the bulk of the information from the lfp power fluctuations.

to demonstrate the sampling properties and data robustness of the gaussian information estimates, we proceed as we did previously for the direct method, generating realistically simulated lfps whose statistical properties closely matched those of real v <dig> lfps . this time we considered the information about the  <dig> presented movie sequences carried by the power of either one, two or three different simulated lfp frequencies . results are reported in figure 5b. we found that if no bias correction was used, the gaussian information values were all upward biased, and the bias grew with the dimensionality of the response space . however, using the analytic bias correction in eqs.  and  eliminated the bias completely, with essentially identical accuracy for all considered dimensions.

taken together, these results indicate that the gaussian method can be an extremely accurate and useful tool for studying the information content of analog neural signals. because of its great data robustness, we strongly recommend its use on any neural signal whose response probabilities are consistent with gaussian distributions.

eegs frequencies encoding visual features in naturalistic movies
we next demonstrate the applicability of our toolbox to the analysis of single-trial eegs. we considered eegs recorded from a male volunteer with a 64-channel electrode cap while the subject was fixating the screen during the repeated presentation of a  <dig> s-long naturalistic color movie presented binocularly. full details on experimental procedures are reported in appendix b. we then used our toolbox to investigate which frequency bands, which signal features , and which electrode locations better encoded the visual features present in movies with naturalistic dynamics.

to understand which frequency bands were more effective in encoding the movie, we used a causal bandpass filter  to separate out the range of eeg fluctuations at each electrode into distinct frequency bands . we then extracted, by means of hilbert transforms of the bandpassed signal, the instantaneous phase and power of the eeg fluctuations in each electrode, frequency band, and trial and examined the time course of amplitude and phase during the movie.

to compare the reliability of phase and power of the delta-range fluctuations at different points of the movie, we discretized the power of the delta band eeg from electrode po <dig> at each time point into four equipopulated bins. we found that power was much less reliable across trials than phase . as a consequence, we also found that power carried only  <dig>  bits of information about the movie, and was thus much less informative than the delta phase from the same electrode.

having illustrated the encoding of the movie by eegs with an example recording channel and a selected eeg frequency range, we next characterized the behavior across all electrodes and over a wider range of eeg frequencies. results are plotted in figure 6e  and 6f . we found that only low frequency ranges  were informative, and that phase was far more informative than power at all electrodes. this is consistent with the attenuation properties of the skull, which is more likely to attenuate power but with relatively little introduction of phase shifts. the most informative regions were found in the right occipital parietal lobe covering the right visual cortices. it has been hypothesized that such informative low frequency fluctuations observed in visual cortex may reflect the entrainment to slowly changing informative features in the sensory signal  <cit> . to gain some insights on why eegs recorded from the right hemisphere were far more informative about the movie than those recorded from the left hemisphere, we calculated the mean pixel luminance of the movie clip over the  <dig> s stimulation window separately for left and right hemifields. we found that the mean luminance was greatest in the left visual field as compared to the right. this provides one potential reason to explain the lateralization of information about the movie observed in this subject.

this example demonstrates the capabilities of the information analysis to extract the most informative components of eeg signals even when using complex dynamic stimulation paradigms and illustrates the potentials of this toolbox for single-trial eeg analysis.

in order to allow users to familiarize with the toolbox, we have included  the entire dataset of eeg delta phases for all  <dig> channels and all trials, together with a commented script that loads the data and computes information through the appropriate calls to the toolbox . running the script contained in the afore mentioned file outputs the results plotted in top left plot of fig 6e.

comparison with other available toolboxes
other groups have developed, or are currently developing, toolboxes for the information analysis of neural responses. here we briefly discuss some of the relative features of current releases of other information theoretic toolboxes, and their complementariness.

ince and colleagues  <cit>  recently released an information theoretic toolbox for neuroscience data called pyentropy based on the python programming language  <cit> . this toolbox has the so far unique feature of including an advanced and memory-efficient algorithm for the computation of entropies which are maximal under given constraints, thereby allowing an easy calculation of many of the "maximum entropy" quantities which have received substantial attention in recent years for the study of neuronal interactions  <cit> . the choice of python as a programming language comes with several advantages provided by its open source nature, its flexibility, and its very efficient use of memory. the use of python could however be problem for most experimental neuroscience laboratories, which currently make use of matlab for preprocessing, analysing and plotting the data.

another available information theoretic toolbox for spike train analysis is the spike train analysis toolkit   <cit> . statoolkit is, like our toolbox, based on c-mex technology and like ours can be easily used in matlab by experimental neuroscience laboratories. a unique and important feature of statoolkit is the large number of estimation methods for the information carried by spike trains, including techniques such as the binless estimation  <cit>  and the metric space approach  <cit> .

with respect to the two above toolboxes, our new toolbox presents two distinctive features. first, it is the only package which has been tested heavily non only on spike trains but also on analog brain recordings such as lfps, and eegs. it also includes algorithms which are specific for these signals, such as the gaussian method information calculation and its bias correction. the second distinctive features of our toolbox is the speed of computation. this speed advantage is not only due to the c implementation, but also to the new algorithm for fast entropy calculation that we presented here. by comparing systematically the speed of our toolbox on simulated data with the speed of pyentropy  <cit> , we found that our toolbox has a speed advantage of typically an order of magnitude to the one of pyentropy. we found similar speed advantage in comparison to statoolkit.

future directions
this paper accompanies the first release of ibtb, which we will continue to be developed over the coming years. some features that we are working to implement in future releases include:

• additional bias corrections procedures. currently, we implemented some of the best known and most useful bias correction procedures for the computation of mutual information. other important corrections exist , however, which we plan to implement and include in the toolbox in the near future. additionally, starting with the next release of ibtb, users will be given the opportunity to plug-in their own custom bias correction routines linking them very easily to the main routines in the toolbox.

• additional methods. the gaussian method is one of the many analytical procedures existing for the computation of entropy and mutual information: actually, several other methods are available which take into account other probability distributions  <cit> . the modular structure of the toolbox allows to very easily add new methods to the toolbox: these will be gradually introduced with future releases.

• fmri analysis. we are currently in the process of testing and adapting our toolbox to its use with bold fmri data. although we developed  <cit>  the bias procedure used successfully in recent information analysis of fmri data some papers  <cit> , more work is needed to understand the specific problems caused by the statistics of fmri data, and how best to use information theory to detect voxels significantly tuned to the stimuli  <cit> . we plan to report thorough studies of this issues on the toolbox website as soon as possible.

CONCLUSIONS
neuroscientists can now record, simultaneously and from the same brain region, several types of complementary neural signals, such as spike, lfp, eeg or bold responses, each reflecting different and complementary aspects of neural activity at different spatial and temporal scales of organization. a current important challenge of computational neuroscience is to provide techniques to analyze and interpret these data  <cit> . we believe that the new fast information theoretic matlab toolbox presented here offers a useful technology tool to analyze these complementary brain signals and understand how the brain may combine together the information carried by aspects of neural activity at these different levels of its organization.

availability and requirements
• project name: information breakdown toolbox

• project home page: 

• operating system: tested on mac os x, windows  <dig> and  <dig> bits, linux

• programming language: matlab  and c

• other requirements: microsoft visual c++  <dig> redistributable package x <dig>  for use on windows  <dig> bit  machine. the package is freely downloadable from microsoft's website and is only required if visual c++ is not installed.

• licence: ibtb is distributed free under the condition that  it shall not be incorporated in software that is subsequently sold;  the authorship of the software shall be acknowledged and the present article shall be properly cited in any publication that uses results generated by the software;  this notice shall remain in place in each source file.

• any restriction to use by non-academics: none.

abbreviations
fmri: functional magnetic resonance imaging; lfp: local field potential; ibtb: information breakdown toolbox; eeg: electroencephalogram; bold: blood-oxygenation-level-dependent

authors' contributions
cm conceived the fast algorithms, implemented the procedures and wrote the article. kw and vs recorded the eeg data, and commented on the manuscript. nkl recorded the lfp data, and commented on the manuscript. sp supervised the project, co-implemented the procedures and co-wrote the article.

appendix a – simulation of lfp responses
we simulated the lfp power of a recording site in primary visual cortex  in response to many different movie scenes. in brief, data were simulated as follows. we selected from the dataset of  <cit>  a given example recording channel , and we computed multitaper estimates of the power at three chosen frequencies  in response to approximately 2-s-long scenes of hollywood color movies presented binocularly to the animal. the multitaper technique allows to reduce the variance of the spectral estimates while keeping the bias under control: this is achieved by means of taking the average of different direct spectra computed using tapers which are orthogonal to each other . the maximum number of averaged spectra is a free parameter  which is set by the user. here we chose k =  <dig>  thereby providing power estimates which are distributed approximately as a chi-square with  <dig> degrees of freedom. we then applied wilson and hilferty's cube-root transformation  <cit> : this transformation, being monotonic does not affect the information content of the responses while making the response-distributions to a fixed movie scene approximately gaussian . we use the same approach for simulation of multi-dimensional responses, by assuming that the joint distribution of the root-transformed power at two or three different frequencies during each fixed movie scene was a multivariate gaussian. we generated many instances of this gaussian power-responses by means of matlab's mvnrnd function using mean and standard deviation values which were computed, for each scene, from the real data. for entropy estimates computed using the direct method, the data simulated in this way have been further discretized into  <dig> equi-populated response bins.

appendix b – methods of eeg recording during presentations of short naturalistic movies
the eeg was acquired using a  <dig> channel electrode cap . electrode placement followed the international 10– <dig> system and electrodes were all referenced to a frontal central electrode . electrode impedances were kept below  <dig> kohms. horizontal and vertical eye movements were recorded using an electro-oculogram  with electrodes placed over the outer canthus of the left eye as well as below the right eye. subjects were comfortably seated in a dimly lit room. eeg recordings were digitally recorded at  <dig> hz with a bandpass of  <dig> – <dig> hz and stored for offline analysis. a small fixation cross on black background was shown in order to indicate the beginning of the trial. after  <dig> seconds of fixation, a  <dig> second movie segment  was presented, followed by  <dig> seconds of continued fixation, resulting in trials totaling  <dig> seconds of fixation. a movie clip, consisting of fast moving and colorful scenes from a commercially available movie, was presented  <dig> times. all data analysis procedures were implemented with the matlab programming language in combination with the eeglab analysis toolbox  <cit>  as described below. postprocessing was performed using the eeglab analysis software . eeg epochs  were created based on the onset of triggers recorded during the recording session. an eog artifact correction algorithm was used to remove all trials with amplitudes that exceed ±  <dig> mv. after artifact rejection,  <dig> movie presentation trials remained.

to obtain bandpassed eegs from each electrode, we bandpassed the raw eeg signal sampled at  <dig> khz with a zero-phase-shift kaiser filter with sharp transition bandwidth , very small passband ripple , high stopband attenuation , and bandwidth corresponding to the considered band . these filters were exactly equal to those used for lfps in refs  <cit> ; we refer to these references for more details.

supplementary material
additional file 1
information breakdown toolbox. this file contains the m-files and executables which constitute the information breakdown tool-box . the most up-to-date version of the code can be also obtained at the following site: .

click here for file

 additional file 2
eeg_test. this file includes the entire dataset of eeg delta phases for all  <dig> channels and all trials, together with a commented script that loads the data and computes information through the appropriate calls to the toolbox: running the script outputs the results plotted in top left plot of fig 6e.

click here for file

 acknowledgements
we thank c. kayser, a. belistki, r. ince, n. ludtke, a. mazzoni, f. montani, m. montemurro and g. notaro for many useful discussions and for testing the code, and e. molinari for useful discussions. this research was supported by the bmi project of the department of robotics, brain and cognitive sciences at the italian institute of technology, and by the max planck society.
