BACKGROUND
over the past several decades, many of the biomedical sciences have been transformed into what might be called "high-throughput" areas of study, e.g., dna mapping and sequencing, gene expression, and proteomics. in a number of cases, the rate at which data can now be generated has increased by several orders of magnitude. this scale-up has contributed to the rise of "big biology" projects of the type that could not have been realistically undertaken only a generation ago, e.g., the human genome project  <cit> . such dramatic expansions in throughput have largely been enabled by engineering innovation, e.g., hardware advancements and automation. in particular, laboratory tasks that were once performed manually are now carried out by robotic fixtures. biologists have steadily been adopting the automated and flexible manufacturing paradigms already established in industry to increase production, as well as to reduce costs and errors.

the growing trend toward automation continues to drive the urgent need for proper software support. here, we use the word "software" to mean those computational tools that  process and analyze data,  organize and store data and provide structured data handling capability, and  support the reporting, editing, arrangement, and visualization of data. this context reflects the biologist's "data → information → knowledge" paradigm  <cit> . table  <dig> lists these three classes and gives an example from each that is relevant to our own specific area of interest: dna sequencing. each class is critical to the success of any large-scale project. the categories do not imply any procedural ordering; data that move through a processing pipeline are likely to be handled repeatedly by all three classes at various points. moreover, certain tools may integrate parts of more than one of the classes. 

in the typical scenario, a research community devotes much of its attention to developing software belonging to the first group mentioned above; these applications are non-trivial in the sense that they usually involve significant algorithmic complexity. in fact, they often evolve into separate, long-term research projects in their own right. for example, although some of the earliest work in dna sequencing software focused on the problem of fragment assembly, this area is still active  <cit> . much effort is also spent on software belonging to the third group because the resulting applications directly serve end-user scientists as their primary "windows" to the data  <cit> . like their counterparts in the first group, these tools normally require significant effort to build, although their research aspect is rather more limited. developers are aided by high-level graphics languages and interfaces and designs are tailored more to end-user needs and tastes rather than specific algorithmic considerations. one can usually expect software from both groups  <dig> and  <dig> to be sophisticated and highly-refined.

in comparison, the second group is often something of a neglected middle for biological projects. this area is sometimes regarded as one of the more pedestrian hinterlands of bioniformatics, yet the underlying design issues are formidable, as we shall see below. the nucleus of group  <dig> is the "database", which in the present context goes by a number of colloquial names: laboratory information management system , workflow management system, or laboratory notebook system.  the basic intent is to store the entire set of laboratory processing data in a structured fashion, such that any desired subset of the data can readily be extracted or manipulated. of course, biological lims are not new and there is an extensive literature covering the subject .

in this paper, we report an expedient technique for modeling laboratory data and its subsequent implementation as a lims. this system has been developed over the last several years at our lab to support large-scale dna mapping and sequencing, although it is not restricted to these specific uses. the model is based on a novel combination of well-known abstraction techniques and has a number of desirable features with respect to flexibility, maintainability, and process control that we describe below.

we will begin with a review of lims as they have been applied to biological projects thus far. this will furnish both a useful survey of and primer for lims design methodologies, as well as a basis of comparison for what we propose here. it is emphasized that the classifications discussed below are not strictly formal, but are meant instead to illustrate some of the possible data modeling and abstraction techniques. we then outline what we consider to be the important modeling requirements and features of a well-designed lims. the subsequent results and discussion sections introduce our model and describe its lims implementation and use in a high-throughput environment. finally, project specifications are summarized in the methods section.

biological lims: historical summary and primer
presumably, the lims concept has existed in the form of hand-written records since the earliest days of biological inquiry. the limitations of printed information in this context are obvious: systematic search and retrieval capabilities are almost completely lacking, as is the ability to cope with a large amount of data. a slightly more sophisticated approach is to use spreadsheets and text files on a filesystem. these implement what is sometimes called the flat model of data. such resources continue to be employed as ad hoc lims by many small labs, where the amount and complexity of data are manageable and simple text-searching suffices. however, it is clear that the flat model remains entirely inadequate for high-throughput environments. flat architectures are not scalable for large numbers of records, nor do they have sophisticated search capability or sufficient resolution for data tracking. moreover, there is no ability to manage complex relationships among the data types and little flexibility to evolve as processes change. finally, there is no straightforward method of providing transactional consistency, which is critical in preventing data corruption.

it quickly became apparent to researchers that high-throughput operations called for dedicated lims. some early efforts  <cit>  used embedded tools to organize data, e.g., the berkeley database library  <cit> , but these still did not offer a general solution. the more usual approach has been to implement a data model via a relational database management system . the design described by dedhia and mccombie  <cit>  is fairly representative. basically, lab processes are identified, as are the materials, instruments, people, etc. that play roles in the processing pipeline. each of these articles is then cast in a logical model as an entity type. each type has an appropriate set of attributes and relationships between types are inferred directly from their physical manifestations. in data modeling terminology, the resulting structural description of entity types and their corresponding relationships is referred to as a schema.

a simplified portion of what such a schema might look like is shown in fig.  <dig>  here, we focus on laboratory instruments called thermocyclers and these beget an entity type of the same name. there is a primary key  called 'instrument_id' that uniquely identifies individual thermocycling instruments. other relationships provide foreign-key  constraints for additional attributes that characterize thermocyclers. for example, each one has a manufacturer, specified by 'manufacturer_id', that derives from the 'manufacturer' type. there also may be non-referentially constrained features, such as a temperature precision, e.g., ±  <dig> °c. if more attributes are later deemed important, such as a serial number or purchase date, they can be appended to the entity type.

an entire laboratory database can readily be constructed along these lines. once again, the main characteristic of this kind of model is that processes and entities lead directly to corresponding types and relationships. we shall refer to this class of model as the direct model because of the direct analogy between the physical and database realms. direct modeling figures prominently in many of the biological lims reported in the literature  <cit>  <cit> . biological database systems routinely use direct modeling, as well  <cit> .

the "direct" concept is clearly a very concrete one and this leads to certain liabilities, discussed further below. in general, a database can be improved to the degree that its design can be abstracted and several abstraction techniques are available. for example, if entity types have common subsets of attributes, the notion of inheritance can be applied. common attributes are collected in a generic type, which are then referenced by keys in other, more specific types. the latter usually implement additional attributes. fig.  <dig> shows how the basic direct model could be revised to take better advantage of inheritance. a new type called 'instrument' contains the primary key, along with attributes common to all instruments, e.g., the manufacturer. an attribute called 'instrument_type' refers to specific entity types, e.g., 'thermocycler', 'centrifuge', or 'mixer', each of which prescribes additional, instrument-specific attributes.  any number of different kinds of instruments could be appended in this way. a number of lims designs have made conspicuous use of inheritance  <cit> .

another way to improve modeling abstraction is the use of meta-data. these "data that describe data" can be used to separate the structure of laboratory data from its schematic implementation. fig.  <dig> shows the basic direct model re-cast into a form that uses meta-data. two entity types cooperatively manage instruments in a generic fashion, for example a 'thermocycler' would now be an instantiation of an 'instrument_type' entity, while an actual thermocycler would be an instantiation of an 'instrument' type. the foreign key 'instrument_type' identifies a specific instrument as a thermocycler. essentially, this approach recognizes that there are data that properly associate with 'instrument_type', implying that the 'thermocycler' type is superfluous at the data level. the latter is abstracted away from the schema altogether. new types of instruments would be treated by adding their meta-data descriptions to 'instrument_type'.

although entity-specific characteristics are managed quite naturally when using inheritance, they are less obvious with meta-data. one possibility for handling attributes is shown in fig.  <dig>  a 'feature_type' type can hold instrument-specific attributes, like the temperature precision of a thermocycler. actual feature data then reside in a bridge, in this case called 'instrument_feature', which links features to the instruments they describe. the meta-data concept is well-known, but is not often used to a degree that permits comprehensive definitions of sub-types. the emergence of dynamically typed languages for application development blends nicely with a good meta-data infrastructure, though this has apparently not been employed extensively for biological lims  <cit> . we will further discuss the uses of inheritance and meta-data below.

lims requirements and schema design
at the conceptual level, good lims designs share several characteristics: robustness, adequate capacity to represent the complexity of the data, flexibility to evolve as materials and processes change, provision for some level of process control, and facilities to enforce, or at least support various levels of data integrity-checking. how do the approaches discussed above measure-up to these requirements?

in the first category, we count issues such as the ability to handle the required volume of data and user transactions, standardization of the interface, reasonable transaction and search times, fault-tolerance and minimal down-times, etc. consequently, "robustness", as we use the term here, is essentially independent of schema design. rather, it is primarily a function of the system used to implement a given data model. the relational approach has undoubtedly been the most successful in this regard, although older storage schemes such as the hierarchical architecture can still occasionally be found  <cit> . relational systems have been widely developed and applied for decades and are quite mature. commercial rdbms, for example, have the scalability to handle enormous data sets, manage a large volume of transactions, have atomic transaction integrity, and perform with minimal down-time. moreover, sophisticated administration tools are routinely available for backups, performance tuning, etc. because high-throughput environments place a premium on such features, most biological lims have been implemented using either a commercial or a high-end, open-source rdbms  <cit>  <cit> .

consequently, the substantive design considerations revolve more around complexity, flexibility, process control, and data integrity. in a broad sense, these issues suggest maximal abstraction of how the data are modeled. as an illustration, let us pick a common laboratory scenario, accommodating a new type of instrument, and compare the basic direct model to one that uses meta-data. under the simple direct model, one would create a new entity type, along with all its attributes and relevant relationships. the resulting tables and relationships would appear in the physical database and a review would be conducted to determine what user applications would need patching to maintain compatibility with the new tables. some amount of testing and recertification of patched applications would also be expected. this is quite a labor-intensive process. in fact, goodman et al.  <cit>  have pointed out that if a schema changes very quickly, applications will always lag and could even be obsolete before modifications are finished. conversely, meta-data can be strongly leveraged in this case. the new instrument type is simply an additional datum that is appended to 'instrument_type' in fig.  <dig>  the schema and physical database do not change. moreover, the existing code infrastructure will typically handle the addition without any required changes. consequently, code review and testing would not strictly be necessary.

these observations suggest the use of an rdbms layed out according to design principles that provide a proper level of abstraction. the best designs will be able to resolve the data complexity, while assuring both data integrity and a high degree of flexibility to evolve as entities change. we propose a design methodology and lims implementation in the next section that has been formulated with these principles in mind.

RESULTS
a few main themes have guided much of our design work. procedures often change much more quickly than the types of physical articles in a biological laboratory, e.g., molecules, reagents, instruments, and personnel. in fact, the latter are essentially constant in many cases. the evolution over the last  <dig> decades of how genomic dna is sequenced adequately illustrates this point. conversely, physical articles are predominantly associated with richer sets of attributes and relationships. again, dna in its various manifestations is a good example.

following these observations, we have based our data model on several fundamental, somewhat hierarchical principles. first, the concepts of inheritance and meta-data are heavily exploited, often in combination with each other. second, event tracking is separated from the tracking of physical articles, so that each can employ the most appropriate combination of abstraction techniques. finally, the concept of an event is generalized to accommodate context-dependency. with respect to implementation, we have emphasized the object-oriented approach for the code base and a closely-coupled mapping between object classes and their counterparts in the physical database, i.e., object-relational mapping. these aspects are all described in greater detail as follows.

modeling of physical articles
many of the tracking needs in a lab revolve around "physical articles". we use this term in a broad sense to mean both tangible objects and data that represent such objects, e.g. a file of dna sequence. in general, we make conspicuous use of the notion of inheritance to model these regular entities in the lab. physical articles can often be thought of as belonging to abstract types. for example, we discussed 'instrument' above as a type that encompasses thermocyclers, centrifuges, etc. straightforward implementation of the model introduced in fig.  <dig> suffices for many such cases. additional constraints might exist, but these can sometimes be managed by trivial extension to the model. for example, one common scenario finds instances related recursively, e.g., 'person a reports to person b', which can be resolved by adding reflexive relationships.

tangibles having richer sets of relationships can usually be captured using some combination of the abstraction techniques shown in fig.  <dig>  consider that types are often related hierarchically in biological laboratories, as exemplified by dna. specifically, the abstract notion of 'dna' gives rise to many concrete sub-types, e.g. raw dna fragments, dna ligations, subclones, sequenced dna products, etc. here, entities of one sub-type beget entities of another and a single instance of a dna must fall into exactly one of these mutually-exclusive sub-types. in modeling terminology, we may say that dna encompasses both "is-a" and "has-a" relationships. the former denotes inheritance , while the latter represents a source-product relationship .

one design that resolves this situation is a straightforward extension of the inheritance concept that applies meta-data to describe the hierarchy . the abstract 'dna' type inherits a foreign key 'dna_type', which identifies appropriate concrete dna types. the 'dna_type_relationship' type specifies the sub-type hierarchies, i.e., the "has-a" relationships between the different dna types, in the form of meta-data, while 'dna_relationship' holds the instances of these relationships for actual dna. fig.  <dig> shows just two of the many possible concrete sub-types: 'genomic_sample' and 'pcr_product'. additional sub-types would attach to the abstract 'dna' type as well, e.g., ligated products, fractioned products, and sequenced products.

one of the desirable characteristics of this design is that meta-data explicitly prescribe how the sub-types are ranked. this enables applications to infer ordering information from a query rather than resorting to internal, hard-coded logic, the significance being that it again reduces maintenance at the application level dramatically. strictly speaking, the design enables, but does not actually enforce the hierarchical integrity of the data. that is, one could purposely connect a given dna instance to a parent dna instance of the wrong type. it is not practical for the rdbms to manage this sort of constraint, but there are a number of ways to address this problem. at the most rudimentary level, one could use additional procedural code at the database level, e.g., triggers, to perform the needed validation. the more elegant solution which we prefer occurs at the application programming interface  level, i.e., class methods  validate objects before allowing data to be committed to the physical database.

the above hierarchical structure arose from the transformative nature of dna into different types. however, other situations lead to similar hierarchical relationships that can be managed in a fashion much like that shown in fig.  <dig>  for example, we would expect almost all laboratories to utilize an abstract "container" type, with sub-types for trays, boxes, racks, etc. here, the hierarchy revolves around what types of containers can reside in other types of containers, e.g., a tray resides in a box resides in a rack. this nesting specification would be encoded in the data, but enforcement would again require specific handling in the api.

modeling of changes and events
recording changes and events and exactly how they occur is critical for any lims. we can frame this issue of "change management" with a simple analogy from human language. consider the common characterization of database entities as nouns. specifically, there is a table for each generic noun, with rows in each table representing proper noun instances of the generic noun and columns supplying adjectives that modify these nouns. in this context, a row is a simple atomic statement of fact, with implicit linking verbs associating the subject  with a series of applicable adjectives. consequently, a database can be viewed in a broader sense as a well-organized collection of facts that specifies the state of a system at any point in time.

perhaps the most casual technique for event management, especially common in small-scale lims, is to link event data directly to the logical entities involved in the event. an additional "adjective" is often used as a descriptor. for example, 'instrument' in fig.  <dig> might have a date field to log when a maintenance task last occurred. this feature does not track action directly. rather, it defines another variable that represents the state of the entity at the time the entity is examined. this approach is a special case of the direct-modeling practice of creating complimentary database tables dedicated to tracking specific kinds of events  <cit> .

when more detail is required, a new entity type can be defined for events occurring upon the original "noun" entity. referring to our 'instrument' example, an event instance would be created every time maintenance is performed on a instrument. according to the language analogy, we are expanding action verbs into nouns and adjectives. for instance, the event whereby alice repairs instrument  <dig> is stored as 'instrument repair event number  <dig> is done on instrument  <dig> and is done by alice'. this method has the advantage of being able to ask more complicated questions of the data. instead of defining a set of arbitrary states and explicitly recording transition into those states, a logical function can determine if an entity is in a given state by examining the event history. this is especially useful when states are not mutually exclusive. moreover, it allows for states to be defined a posteriori and re-organized as a system is re-factored.

our design combines this basic idea with the aforementioned inheritance and meta-data strategies. we define a completely generic event type, with a distinct sub-type for each kind of event which might occur. each sub-type is implemented as a sub-class in the api , where the logic for the event is actually implemented. the base class defines the infrastructure that allows any event to associate with the required physical entities and provides an interface onto which general workflow management software can be built. most event sub-types are almost entirely defined by their meta-data. in particular, none of the sub-types requires its own subclass-oriented table in the rdbms and only about 20% of the sub-types actually have any code in their api subclass. more details are given in the example below.

process control directives
a critical consideration in the design of any data-tracking system is how specific to make event type definitions. there are two extremes, neither of which is entirely suitable. on one hand, one could allow completely generic definitions, but the significant numbers of project-specific caveats, exceptions, and re-directions would make many definitions unmanageable. conversely, highly specific definitions would provide flexibility, but the resulting multiplication of the number of definitions raises similar management difficulties. instead, we designed the event tracking layer to allow the attachment of "directives" to an event. directives are instructions or parameters that govern subsequent events. they might prescribe certain parameters for a given down-stream event, veto the execution of an event entirely, or automatically prompt other events upon completion, e.g., project-specific computational analysis. a single directive can affect multiple down-stream events, and can influence a given event at multiple points in its execution. in other words, directives can function in a non-linear fashion.

the concept of the directive elegantly handles the problem of context-dependency, which routinely arises in large-scale environments. consider the example of amplification by pcr, a process performed both in the pre-finishing stage of de novo genomic sequencing  <cit>  and in the medical resequencing of patient samples  <cit> . while both pcrs occur in a physically identical fashion on the same instrument, each has its own specific constraints and reporting requirements. several problems clearly loom here, including how technicians should handle an anonymously barcoded plate and how context-free logic is separated from context-specific logic.

in the api , the directive class is abstract, requiring concrete subclasses to handle various notifications during an event's life. for example, the "project' directive has separate sub-classes for de novo and medical sequencing projects. each encapsulates the appropriate constraints, extra logic, and flow control. all directives respect encapsulation, meaning that they cannot interfere with the core execution logic, except through the same parameter interface that the application uses. this feature forms a clear boundary between an event's essential parts and its peripheral ones and ensures that independent directives do not need customized integration logic.

overview of implementation
we have followed the standard approach for lims implementation, i.e., as a multi-tiered system consisting of external applications, the api, and the physical database  <cit> . in particular, the object-relational api translates all of the database tables into perl software classes   <cit> , and integrates those classes with other non-persistent and abstract classes to form the core logic layer on which the lab operates. nearly all lab logic is in these software classes.

although the 3-tiered layout is typical, our specific implementation of the api is appreciably different from other systems due to the explicit separation in the data model between event entities and physical articles . essentially, the api is divided into two parts that reflect this separation: one for the change-oriented event object layer, and the other for the regular state-oriented object layer. these layers have equal precedence from a data access perspective. however, when change occurs, the event layer is hierarchically positioned between the application and the state-object layer. consequently, it is more accurate to speak of our system in the context of four tiers.

 <dig>  applications represent interaction with users, robots, or other external agents.

 <dig>  api events are used by applications to manage all change. each action in the lab corresponds to a logged event having explicit parameters and a known software procedure responsible for the change.

 <dig>  regular api entities, usually representing physical articles, make up the state-oriented remainder of the object layer. this level is readable by applications, mutable by the event layer, and defines the state of the lims at any given point.

 <dig>  the rdbms implements persistence.

from a programming standpoint, there are a number of interesting features native to this system. the api exploits dynamic class-loading to minimize memory usage and it maintains class meta-data to facilitate chores like dynamic generation of graphical user interfaces . multitable inheritance is also supported. access to the three physical databases , including distributed, multi-database transactions, is handled transparently by the software. the api also provides data type validation, field compression, and logical-to-physical field translation transparently. software tests ensure that resolved bugs do not recur and the test battery is run automatically every hour to verify the functionality of all production code. the system also integrates the processing algorithms we use for sequence data  <cit> . several other aspects of the implementation provide particular utility for high-throughput processing.

bar-coding
a barcode system  <cit>  supports detailed sample tracking. we barcode essentially everything, including small tubes and plates, agar plates, and all lab instruments, freezers, and reagent containers. lab personnel have barcoded badges to log their ownership of events. barcode-related information is embedded in the relational schema using a combination of the meta-data and inheritance techniques introduced in fig.  <dig>  we also have support for mapping external barcodes to unique identifiers in the lims, including two-dimensional barcodes having embedded vendor information.

container management and transfer patterns
the lims supports event tracking down to the level of an individual well in a 96-well or 384-well plate. this provides the means to direct the specific events that should occur for single samples, as well as the ability to fully reconstruct the history of any sample. we have also defined the concept of a transfer pattern, as an instruction set that prescribes how contents are to be moved between containers. for example, a pattern might specify how components in a given sector of a 384-well plate are to be re-arrayed to a 96-well plate. transfer patterns are mainly used to direct robotic fixtures for re-arraying and allow a simple identifier to map out complex movements of sample and reagent materials. like barcodes, transfer patterns are embedded in the relational schema using the abstraction techniques introduced in fig.  <dig> 

robotics integration
to the extent possible, computer systems associated with robotic fixtures in the laboratory are connected directly to the lims. many of these fixtures include hardware for direct scanning of sample barcodes. for those units without this capability, technicians manually scan sample barcodes and the lims then utilizes the appropriate sample processing parameters to drive the robot as required. in either case, the lims extracts data from the robot to be integrated back into the database. one particularly notable example is our sequence machine loading application, which communicates directly with the abi 3730xl sequencing platform . sequence traces are extracted and automatically submitted to the genbank trace archive  <cit>  within several hours after loading.

the life of an event: state transitions
events themselves can be thought of as state transition machines that respond to external cues and commands  <cit> . fig.  <dig> shows a map of the state transitions that we have found useful for our particular circumstances. the novelty here is that states fall into two categories: those that are not actually recorded in the physical database  and those that are recorded . this separation provides an extra opportunity for validating information and directives at the initiation phase of a process. specifically, at the application level, a step is created in the 'is_initiated' state, i.e., it has a reserved id from the database and can accept parameters and processing directives as attachments. if the additions are valid, the step can proceed to subsequent, recordable states, otherwise it is aborted. the remaining states are summarized as follows.

the 'is_scheduled' state denotes that a process is waiting for available resources. in particular, it serves as a flag for a scheduling system to perform job grouping, queuing, and initiation of the actual execution of the event on a computing cluster, if required. the 'is_running' state indicates that the process instance is actually executing, for example on a compute cluster. processes that occur instantaneously may skip this state. in 'is_suspended', a running process has been halted because of some irregularity and is waiting for error handling. this state will not appear often in highly fault-tolerant pipelines. similarly, 'is_pending_disposition' is a flag that indicates a process is awaiting manual redirection by lab personnel. this state will not appear often in highly-automated pipelines. the 'is_abandoned' tag announces that a process has been permanently terminated before normal completion, while 'is_completed' records that a process has fulfilled all the conditions for a normal, orderly completion. the latter does not convey any information, as to the success or failure of the process. finally, 'is_succeeded' is a sort of "super-completed" state confirming that control has successfully been transferred to a subsequent process or step.

aspects of implementing a process pipeline
an event is the most granular unit of change in our lims for which there is individual transition tracking. broader, more complicated occurrences must be broken into suitable sequences of events, commonly referred to as "workflows" or "process pipelines". accommodating a new task involves translating a laboratory protocol into distinct event types and defining meta-data such that these types are appropriately linked.

linkage actually refers to both the input-output "hooks" defined in event types and the instantiated linked-list history of sequences of actual events. in particular, any new event definition is potentially connected to other, existing definitions whose output type match the new event's input type. various constraints may be layered over this basic requirement. for instance, if containers are involved, meta-data indicate specific container types usable in the process. the net effect is that an event is implicitly part of a pipeline if it can be executed without violating any of its internal constraints.

when a new event is initialized, the prior event from which control is being transferred is determined by finding and matching values of the event parameters. the linking of these two events is recorded in the database. this might seem superfluous, given that such connections might be inferred by queries ad hoc. however, querying is often not sufficient to uniquely resolve a pipeline, e.g., when samples are used across multiple projects. the consequence of such linking is that any event has access to the entire record of upstream processing from whence it came and can access this record through any of its input parameters. furthermore, linking enables more complex constraints, such as requiring that certain precursor events have all occurred successfully before an input is accepted.

discussion
we have described the design and lims implementation of a model for laboratory data that uses a novel combination of well-known abstraction principles. here, we provide an example of its use and discuss our experiences with it at the genome sequencing center at washington university. the lims has been one of the most critical factors in our success as a large-scale sequencing facility. in particular, we have a long-standing program of progressively automating laboratory workflows, especially those that are labor-intensive, and consequently, costly and error-prone. the lims permits us to directly monitor and control laboratory activity and to efficiently distill information for effective high-level decision-making.

an example pipeline: medical resequencing
the database plays a prominent role in practically every aspect of laboratory operations. various examples of processing pipelines could be discussed, for example managing and tracking reactions for de novo sequencing projects. instead, we would like to focus on a rather newer type of pipeline for medical resequencing of patient dna. medical resequencing is becoming increasingly important in the effort to identify causal factors of various diseases. although the physical lab processes in this type of sequencing are similar to those for traditional genomic projects, the context is somewhat different. for example, instead of processing random clones, targeted regions of patient samples are sequenced. moreover, the concept of an assembly is that reads are aligned to reference sequence rather than being integrated de novo. fig.  <dig> shows an example of a direct analogy schema that could be used for a medical sequencing operation. although the design is quite intuitive, it harbors many of the liabilities that have been discussed above. specifically, it is quite rigid, gives little flexibility for evolving protocols, does not provide systematic event tracking, and is not readily extended to other contexts, e.g., traditional genomic sequencing.

the procedural layout of a typical medical sequencing operation is shown in fig.  <dig>  physical articles  and events  are represented, along with some of the specifications and constraints  specific to this type of sequencing. interactions among the various object types are complicated, demonstrating that direct designs similar to that in fig.  <dig> would be inadequate. the use of directives is especially notable in this process. for example, the project object includes a list of reference sequences to target and details which portions of those reference sequences are significant. a set of primer pairs, called a tiling path, is defined such that the required regions will be covered, and this information is also part of the project. subordinate directives are linked to the initial step in the pipeline, as well. one governs annotation of the reference sequence while another controls the tiling path algorithm. finally, additional directives control data verification according to project-specific requirements.

the project class also has code that constrains and/or extends each of the atomic lims steps, based on the project object's settings. to facilitate bulk processing, project samples are queued for replication and re-arraying into 384-well plates using a transfer pattern matched to the sample count and amplification regions. primers for the initial tiling path are ordered in parallel. upon pcr completion, the project confirms that the correct samples are used with the correct primers and that redundant pcr is not accidentally performed. it ensures that samples which require validated primer pairs are not processed until the pcr configurations have passed a validation step in the parallel pipeline. the sample-amplicon mapping used for later analysis is also updated. this information is subsequently accessed by software on-board the abi 3730×l sequencing analyzers to further direct how to process the samples. this includes specification of run length, which can depend on amplicon size, and a trace name, which encapsulates necessary data for use with down-stream snp analysis tools. although the trace analysis event performs standard processing on the traces, project directives extend processing by initiating additional analysis steps. these steps recall bases with phred  <cit> , compare reads to the reference sequence, determine success from the perspective of the project's goals, and record statistics.

the object layout encompasses the totality of the logic necessary to run the pipeline . concrete classes inherit from five highly abstracted base classes: 'dna', 'instrument', 'sequence_data', 'processing_directive', and 'event'.  in terms of actual schema implementation, the process described in figs.  <dig> and  <dig> reduces to the simple layout shown in fig.  <dig>  in particular, this diagram shows the five associated abstract entity types, along with their relationships. the 'event' type provides the essential foundation from which the other types radiate via many-many relationships.

although not shown here explicitly, each type is expanded in the database according to the various abstraction principles discussed above. for instance, 'instrument' and 'dna' are implemented according to figs.  <dig> and  <dig>  respectively. the 'event' type expands in a form something similar to that given by fig.  <dig>  in essence, the design of "events" provides full inheritance functionality, though it is implemented using meta-data. again, the advantage here is that concrete event types do not require explicit table definitions in the database. this is important for maintaining flexibility in the face of rapidly evolving protocols and the consequent reconfiguring of laboratory pipelines. notable supporting meta-data for events are input and output definitions, which can be implemented in a type slightly more general than 'feature_type' in fig.  <dig>  

comparison to other lims designs
event-tracking is clearly one of the primary functions of any lims. we have described our design above and can contrast it with some of the other established systems. in many of those cases, tracking is relegated to either defining extra, ad hoc fields in a database table or to creating additional tables for specific kinds of tasks to hold state information  <cit> . this is essentially a direct approach to handling events, as illustrated by fig.  <dig>  a number of systems take this a step further by exploiting the inheritance concept. here, a basic event specification serves as a super-class for more specific event definitions  <cit> . however, this still has the disadvantage of requiring table additions when new tasks are defined  <cit> .

a somewhat more sophisticated approach is taken by the labbase system  <cit> , which provides a special layer devoted to managing materials, steps, and states. although our software implementations are similar, labbase appears to be significantly different from our own system at the level of the data model. specifically, it maintains a form of stateful association with materials. goodman et al.  <cit>  give an example of a material-state relationship in labbase: a 'clone' is 'ready for sequencing'. this suggests that their design does not make a strict distinction between the event itself, 'sequencing', and the state of the event, 'ready'. in actuality, labbase is not a complete lims in the sense we have used the term here. rather, it is designed to work in conjunction with a workflow management system  <cit> , which shoulders much of the process management aspect.

another issue that is being increasingly appreciated in biological lims is process control. many of the established designs have little or no facility for using a database to directly steer events in the lab. such configurations frequently layer a workflow management system over the database to gain some level of control  <cit> , but this often comes in the form of altering the pipelines themselves rather than exerting a more subtle influence over a given pipeline definition.

it is well known that databases can be designed to provide certain control features  <cit>  and some recent work has moved in this direction. in particular, the magic-spp system takes an ad hoc approach, designating processes as either "high-level" or "low-level"  <cit> . organism-specific tasks and the processing of traces are cited as examples of these two classifications, respectively. each division leads to its own set of dedicated tables in the database. however, it is difficult to ascertain the extent to which this design supports multiple roles, e.g., passive or non-linear control. we have also found that the ability to apply global controls over entire projects  is quite advantageous. such control would span both of the magic-spp divisions, but it is not clear how straightforward it would be to use that system in this capacity. it also does not appear that this design is readily applicable to other, non-biological workflows, e.g., processing new lab personnel or purchase orders.

assessment of capability and performance: some statistics
our lims has been developed over the last several years and continues to be extended. the underlying database currently has about  <dig> tables and functions as an on-line transaction processing  system. many of the tables hold static data that serve as referential constraints for the data-centric tables. user applications also continue to be developed, as do the two layers of the api.

thus far, our system has been able to keep pace with the growing throughput and complexity of our laboratory operations. a few statistics are helpful in conveying some idea of the demands that these activities place on the database. every month, the genome sequencing center performs about  <dig> million sequencing reactions and finishes about  <dig> mb of sequence, primarily using our in-house bank of  <dig> abi 3730×l sequencing machines. the oltp database is currently about  <dig> terabyte in size. however, we also maintain corresponding data warehouse  and on-line analytical processing  databases  <cit> . these two implementations exist for the purposes of dedicated data-mining and large-scale analyses, allowing the primary oltp database to remain unfettered. oltp data are copied in real-time to the dw and olap databases. the dw database holds only the processed results of dna sequencing reactions: binary trace files and plain-text summary files  <cit> , including the actual nucleotide base-calls  <cit> . its schema is essentially a subset of the oltp schema. the olap database holds a slightly denormalized text-only representation of these same data and is optimized for fast, large-scale analyses using a standard "star" schema  <cit> . in particular, reports that previously took hours to generate now run in seconds. the dw database currently contains almost  <dig> terabytes of data. 

we have configured our lims to manage event pipelines for over  <dig> types of entities. most, including dna, reagents, organisms, and purchase orders, are of a physical nature. the base-class table for dna is the largest of these at about  <dig>  billion rows. other entities, like dna sequencing projects, are virtual. they do not exist in a tangible sense, but nevertheless have a well-defined operational sequence. as of this writing, there are over  <dig> defined processing events defined in our lims, of which  <dig> are currently active. about  <dig> of the latter are actually distinct. aside from scheduled maintenance, database uptime has been greater than 95%.

in our experience, one of the most important features to the end-user is the ability to efficiently navigate historical information. the system we have described here supports two complimentary avenues to do this. simple "history lists" can readily be generated starting from either a given process instance or a given material instance. for example, the predecessors of a given process can be obtained simply by traversing pointers to previous steps. conversely, the event-history of a material can be obtained directly from that material's bridge table with the main event-tracking table. more elaborate histories can easily be constructed by combining these two approaches. specifically, a given material's event history could be supplemented with a list of all the progenitor materials that participated in those events. such analysis can be useful for a number of purposes, e.g., in quality assessments and tracing and troubleshooting shipment lot numbers. if so desired, one could extend this treatment ad infinitum, finding every progenitor material and event at every level that led to the current one.

CONCLUSIONS
the importance of a robust, flexible lims for laboratory data management is only becoming more acute. processing volumes continue to grow, processes change almost fluidly, and evolving research directions dictate increasing degrees of heterogeneity in the data. the latter point is well-illustrated by the trend toward maintaining both the traditional genomic dna sequencing pipelines as well as medical/patient sequencing pipelines, simultaneously. these factors place enormous demands on a data-tracking system and it is only a slight exaggeration to say that an inferior lims can threaten a lab's very viability.

our data model and lims implementation are applicable to environments that have used, or wish to use the relational approach for data management. implementation is not limited to any particular rdbms or programming language. in particular, we were originally prompted to choose the commercially-available oracle rdbms  <cit>  because of its well-established reputation for large database projects. however, mature high-end, open-source platforms like mysql  <cit>  and postgresql  <cit>  now provide suitable alternatives. a generic version of our object-relational api has been released as free software  <cit>  and efforts are now focused on extending it to support additional rdbms platforms.

we encourage investigators to compare their existing systems to what we have reported here and to adopt any aspects that they would find to be a useful improvement.

