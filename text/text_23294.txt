BACKGROUND
experimental methods to determine the structure and function of proteins have been out-paced with the abundance of available sequence data. as such, over the past decade several computational methods have been developed to characterize the structural and functional aspects of proteins from sequence information  <cit> .

support vector machines   <cit>  along with other machine learning tools have been extensively used to successfully predict the residue-wise structural or functional properties of proteins  <cit> . the task of assigning every residue with a discrete class label or continuous value is defined as a residue annotation problem. examples of structural annotation problems include secondary structure prediction  <cit> , local structure prediction  <cit> , and contact order prediction  <cit> . examples of functional annotation problems include prediction of interacting residues  <cit>  , solvent accessible surface area estimation  <cit> , and disorder prediction  <cit> .

we have developed a general purpose protein residue annotation toolkit called svmprat. this toolkit uses a support vector machine framework and is capable of predicting both a discrete label or a continuous value. to the best of our knowledge svmprat is the first tool that is designed to allow life science researchers to quickly and efficiently train svm-based models for annotating protein residues with any desired property. the protocol for training the models, and predicting the residue-wise property is similar in nature to the methods developed for the different residue annotation problems  <cit> .

svmprat can utilize any type of sequence information associated with residues. features of the residue under consideration, as well as neighboring residues, are encoded as fixed length feature vectors. svmprat also employs a flexible sequence window encoding scheme that differentially weighs information extracted from neighboring residues based on their distance to the central residue. this flexibility is useful for some problems.

the svmprat implementation includes standard kernel functions  along with a second-order exponential kernel function shown to be effective for secondary structure prediction and pairwise local structure prediction  <cit> . the kernel functions implemented are also optimized for speed by utilizing fast vector-based operation routines within the cblas library  <cit> . svmprat is capable of learning two-level cascaded models that use predictions from the first-level model to train a second-level model. such two-level models are effective in accounting for the residue properties that are dependent on properties of near-by residues . this form of cascaded learning performs well for secondary structure prediction  <cit> . svmprat is made available as a pre-compiled binary on several different architectures and environments.

in this paper svmprat has been evaluated on a wide suite of prediction problems, which include solvent accessibility surface area estimation  <cit> , local structure alphabet prediction  <cit> , transmembrane helix segment prediction  <cit> , dna-protein interaction sites prediction  <cit> , contact order  <cit>  estimation, and disordered region prediction  <cit> . svmprat has been used in development of a transmembrane helix orientation prediction method called toptmh  <cit> , shown to be one of the best performers on a blind independent benchmark  <cit> . the svmprat framework was also used for prediction of ligand-binding sites  <cit>  and was initially prototyped for the yasspp secondary structure program  <cit> .

support vector machines are a powerful tool for classification and regression tasks. however, adapting them to the particular case of protein sequence data can be onerous. svmprat is a tool that allows svms to be applied readily to sequence data by automating the encoding process and incorporating a number of different features that are specifically designed for the problem of protein residue annotation.

implementation
svmprat approaches the protein residue annotation problem by utilizing local sequence information  around each residue in a support vector machine  framework  <cit> . svmprat uses the classification formulations to address the problem of annotating residues with discrete labels and the regression formulation for continuous values. the svmprat implementation utilizes the publicly available svmlight program  <cit> .

svmprat provides two main programs, one for the learning annotation models  and the other for the predicting labels from learned models . the svmprat-l program trains either a classification or regression model for solving the residue annotation problem. for classification problems, svmprat-l trains one-versus-rest binary classification models. when the number of unique class labels are two , svmprat-l trains only one binary classification model to differentiate between the two classes. when the number of unique class labels are greater than two , svmprat-l trains one-versus-rest models for each of the classes i.e., if there are k discrete class labels, svmprat-l trains k one-versus-rest classification models. for continuous value estimation problems , svmprat-l trains a single support vector regression  model.

the svmprat-p program assigns a discrete label or continuous value for each residue of the input sequences using the trained models produced by svmprat-l. in classification problems, svmprat-p uses the k one-versus-models to predict the likelihood of a residue to be a member of each of the k classes. svmprat-p assigns the residue the label or class which has the highest likelihood value. for regression problems, svmprat-p estimates a continuous value for each residue.

input information
the input to svmprat consists of two types of information. firstly, to train the prediction models true annotations are provided to svmprat-l. for every input sequence used for training, a separate file is provided. each line of the file contains an alphanumeric class label or a continuous value i.e., true annotation for every residue of the sequence.

secondly, svmprat can accept any general user-supplied features for prediction. for a protein, svmprat accepts any information as feature matrices. both, svmprat-l and svmprat-p accept these input feature matrices. svmprat-l uses these feature matrices in conjunction with the true annotation files to learn predictive models, whereas svmprat-p uses the input feature matrices with a model to make predictions for the residues.

a feature matrix f for a protein sequence x is of dimensions n × d, where n is the length of the protein sequence and d is the number of features or values associated with each position of the sequence. as an example, figure  <dig> shows the psi-blast derived position specific scoring matrix  of dimensions n ×  <dig>  for every residue, the pssm captures evolutionary conservation information by providing a score for each of the twenty amino acids. other examples of feature matrices include the predicted secondary structure matrices and position independent scoring matrices.

we use fi to indicate the ith row of matrix f, which corresponds to the features associated with the ith residue of x. svmprat can accept multiple types of feature matrices per sequence. when multiple types of features are considered, the lth feature matrix is specified by fl.

information encoding
when annotating a particular residue, svmprat uses features of that residue as well as information about neighboring residues. window encoding, also called wmer encoding, is employed to accomplish this. for sequence x with length n, we use xi to denote the ith residue of the sequence. given a user-supplied width w, the wmer at position i of x  is defined to be the -length subsequence of x centered at position i. that is, residues immediately before and after xi are part of wmer. the feature vectors of residues in this window, fi-w ... fi+w, are concatenated to produce the final vector representation of residue xi. if each residue has d features associated with it, the wmer encoding vector has length  × d and is referred to as wmer.

kernel functions
svmprat implements several kernel functions to capture similarity between pairs of wmers. selection of an appropriate kernel function for a problem is key to the effectiveness of support vector machine learning.

linear kernels
given a pair of wmers, wmer and wmer a linear kernel function can be defined between their feature matrices wmer and wmer, respectively as   

where ⟨·,·⟩ denotes the dot-product operation between two vectors.

some problems may require only approximate information for residue neighbors that are far away from the central residue while nearby residue neighbors are more important. for example, the secondary structure state of a residue is in general more dependent on the nearby sequence positions than the positions that are further away  <cit> . svmprat allows a window encoding shown in figure  <dig> where the positions away from the central residue are averaged to provide a coarser representation while the positions closer to the central residue provide a finer representation. this two-parameter linear window kernel is denoted  and computes the similarity between features wmer and wmer as   

the parameter w governs the size of the wmer considered in computing the kernel. rows within i ± f contribute an individual dot product to the total similarity while rows outside this range provide only aggregate information. in all cases, f is less than or equal to w and as f approaches w, the window kernel becomes a sum of the dot products. this is the most fine-grained similarity measure considered and is equivalent to the one-parameter dot product kernel that equally weighs all positions of the wmer given by equation  <dig>  thus, the two kernels  are  equivalent. specifying f to be less than w merges neighbors distant from the central residue into only a coarse contribution to the overall similarity. for f <w, distant sequence neighbors are represented by only compositional information rather than specific positions where their features occur.

exponential kernels
svmprat implements the standard radial basis kernel function , defined for some parameter γ by   

svmprat also implements the normalized second order exponential  kernel function shown to better capture pairwise information and improve accuracy for the secondary structure and local structure prediction problems  <cit> . given any base kernel function , we define  as   

which is a second-order kernel in that it computes pairwise interactions between the elements x and y. we then define  as   

which normalizes  and embeds it into an exponential space.

by setting a specific γ parameter value and using normalized unit length vectors in equation  <dig> it can be shown that the standard rbf kernel is equivalent  to a first order exponential kernel which is obtained by replacing  with only the first-order term as  in equation  <dig>  and plugging this modified  in the normalization framework of equation  <dig> 

integrating information
when multiple information in the form of different feature matrices is provided to svmprat, the kernel functions and information encoding per residue for each of the feature matrices remains the same. the final kernel fusion is accomplished using a weighted linear combination across the original base kernels. the weights for feature matrices can be set by the user.

for example, we can use the fusion of second-order exponential kernels on different features of a protein sequence. considering two sequences with k sets of feature matrices fl and gl for l =  <dig> ..., k, our fusion kernel is defined as   

where the weights ωl are supplied by the user. in most cases, these weights can be set to be equal but should be altered according to domain-specific information.

cascaded models
several prediction algorithms like phd  <cit> , psipred  <cit>  and yasspp  <cit>  developed for secondary structure prediction use a two-level cascaded prediction framework. this two-level framework trains two models, referred as the l <dig> and l <dig> models, which are connected together in a cascaded fashion. both the l <dig> and l <dig> models train k one-versus-rest binary classification models for predicting a discrete label or a single ϵ-svr regression model for estimating a continuous value. the predictions from the first-level l <dig> model are used as an input feature matrix along with the original features for training a l <dig> model  <cit> . such cascaded predictions can be accomplished within svmprat's framework in the following way. first, the entire training set is used to train a l <dig> classification/regression model using the original input features. this is followed with a n-fold cross-validation step to generate predictions for the entire training set using the fold specific trained l <dig> model. in each iteration, 1/n-th of the dataset is set aside for prediction whereas the remainder of the dataset is used for training. the predictions from the l <dig> model are then used as a new input feature along with the original features to train a l <dig> model. the user may specify any desired weighting between original features and the l <dig> model predictions according to equation  <dig>  the final result is a cascaded prediction.

efficient implementation
the runtime performance of svmprat is tied to the speed of computing the kernel function values between pairs of wmers. all the implemented kernel functions have to compute a dot product between the vector representations.

svmprat optimizes the computation time for the dot product based kernel functions given by equation  <dig> by using the optimized cblas  routines that are a part of the atlas library project  <cit> . the cblas routines provide the standard building blocks for performing vector-based and matrix-based computations. in particular, the efficient vector operations available through cblas are used within svmprat's kernel function implementations. this allows svmprat to train models and generate predictions for test cases quickly.

we ported the cblas routines to all the architectures on which svmprat was complied and provide binaries compiled with and without the cblas routines .

predictions output
for classification problems, svmprat's prediction program produces two outputs in text files. for every residue, raw prediction scores from the one-versus-rest svms are reported. in addition, each residue is assigned a class based on the maximum prediction score of the models. for regression problems, the output is a text file containing the estimated value produced by the ϵ-svr model.

model selection
svmprat provides an evaluation program called svmprat-e that allows the practitioner to determine the best set of parameters for a particular prediction problem using cross validation. for ease of use, a simple perl script is provided which invokes svmprat-e for a fixed set of parameters to determine the best kernel and window lengths.

RESULTS
svmprat has been used in two previous experimental settings with success. toptmh is a transmembrane-helix segment identification and orientation system which utilizes svmprat <cit> . it has achieved the best performance on a static independent benchmark  <cit> . the work by kauffman et al. used svmprat to predict the ligand-binding residues of a protein  <cit> . this was shown to improve the quality of homology models of the protein's binding site.

in this work, we illustrate the capabilities of svmprat on a wide range of prediction problems. these case studies illustrate the effectiveness and generality of the software for sequence annotation problems. problems involving disordered regions, dna-protein interaction sites, residue contact order, and general local structure class are covered in the subsequent sections. table  <dig> shows some characteristics of the datasets used in each problem and the reference work from which the data was derived.

#c, #seq, #res, #cv, and % denote the number of classes, sequences, residues, number of cross validation folds, and the maximum pairwise sequence identity between the sequences, respectively.  <dig> represents the regression problem.

disorder prediction
some proteins contain regions which are intrinsically disordered in that their backbone shape may vary greatly over time and external conditions. a disordered region of a protein may have multiple binding partners and hence can take part in multiple biochemical processes in the cell which make them critical in performing various functions  <cit> . disorder prediction is an example of a binary classification problem for sequence data. disordered region prediction methods like iupred  <cit> , poodle  <cit> , and dispro  <cit>  make predictions using physiochemical properties of the amino acids or evolutionary information within a machine learning tool like bi-recurrent neural networks or svms.

svmprat was used to discriminate between residues belonging to ordered versus disordered regions. we assessed the value of several feature sets on this problem as an illustration of how svmprat may combine sequence information. the feature sets were psi-blast pssms (), blosum <dig> sequence features (), and predicted secondary structure . see the material section for explanation of the different input features. the parameters w and f of the base window kernel () were varied to demonstrate their effects on prediction performance. finally, linear , radial basis function , and second order exponential  kernels were all used to show how the similarity computation in  may be further processed to improve performance.

dispro  <cit>  reports a roc score of  <dig> . the numbers in bold show the best models for a fixed w parameter, as measured by roc. , ℬ, and  represent the psi-blast profile, blosum <dig>  and yasspp scoring matrices, respectively. soe, rbf, and lin represent the three different kernels studied using the ww, f as the base kernel. * denotes the best classification results in the sub-tables, and ** denotes the best classification results achieved on this dataset. for the best model we report a q <dig> accuracy of  <dig> % with an se rate of  <dig> .

the performance of svmprat on the disorder prediction problem improved by using the , ℬ, and  feature matrices in combination rather than individually. table  <dig> shows results for the successive use of , , and  features in the soe kernel: the additional features tend to improve performance. the flexible encoding introduced by svmprat shows some merit for the disorder prediction problem. the best performing fusion kernel shows comparable performance to dispro  <cit>  that encapsulates profile, secondary structure and relative solvent accessibility information within a bi-recurrent neural network.

runtime performance of optimized kernels
we benchmarked the learning phase of svmprat on the disordered dataset comparing the runtime performance of the program compiled with and without the cblas subroutines. these results are reported in table  <dig> and were computed on a 64-bit intel xeon cpu  <dig>  ghz processor for the , , and  kernels varying the wmer size from  <dig> to  <dig>  table  <dig> also shows the number of kernel evaluations for the different models. using cblas, speedups ranging from  <dig>  to  <dig>  are achieved for disorder prediction. similar speedups were noted for other prediction problems. disorder prediction at casp8: casp is a biennial protein structure prediction competition which includes a disorder prediction category . we submitted predictions of disordered residues to the casp <dig>  the latest iteration of the competition. our mariner server  used svmprat as the backend prediction tool. casp <dig> featured  <dig> target proteins with  <dig>  residues out of which  <dig> % were disordered residues.

the runtime performance of svmprat was benchmarked for learning a classification model on a 64-bit intel xeon cpu  <dig>  ghz processor. #ker denotes the number of kernel evaluations for training the svm model. no denotes runtime in seconds when the cblas library was not used, yes denotes the runtime in seconds when the cblas library was used, and sp denotes the speedup achieved using the cblas library.

the svmprat model employed for casp <dig> was trained using profile information embedded within the soe kernel with a wmer size of  <dig>  table  <dig> gives the top performers from the disorder prediction category of casp <dig>  svmprat showed encouraging results compared to methods that are fine-tuned for disorder prediction. the blind evaluation done in casp <dig> proves the ability of svmprat to adapt readily to different prediction problems.

* - mariner used svmprat to train models for disorder prediction in participation at casp <dig> using the  kernel with w = f =  <dig>  we used the  <dig> sequences with disordered residues from the dispro  <cit>  dataset. the results are the official results from the casp organizers and were presented by dr. joel sussman at the weizmann institute of science. q <dig> denotes the 2-state accuracy for the prediction and sw is a weighted accuracy rewarding the prediction of disordered residue.

the results from the casp assessors were published recently  <cit>  and show that the top performers based on a weighted accuracy are consensus-based methods. poodle is a svm-based approach that uses two sets of cascaded classifiers trained separately for long and short disordered regions. svmprat can easily train two separate cascaded models for long and short disordered regions and thus incorporate the domain insight introduced by poodle, in an efficient and quick manner.

contact order prediction
pairs of residues are considered to be in contact if their cβ atoms are within a threshold radius, generally  <dig> Å. residue-wise contact order  <cit>  is defined as the average distance separation between contacting residues within a sphere of set threshold. contact order prediction is an example of a regression problem for sequence data: the value to be predicted is a positive integer rather than a class. to predict contact order, song and burage  <cit>  used support vector regression with a variety of sequence features including psi-blast profiles, predicted secondary structure from psipred  <cit> , amino acid composition, and molecular weight. critical random networks have also been applied to solve the problem  <cit> . we used svmprat to train ϵ-svr regression models for residue-wise contact order estimation. pssm and predicted secondary structure,  and  respectively, were used as features in the soe kernel. the window kernel parameters w and f were varied again to study their impact. evaluation was carried out using 15-fold cross validation on the dataset of song and burage  <cit> .

cc and rmse denotes the average correlation coefficient and rmse values. the numbers in bold show the best models as measured by cc for a fixed w parameter. , and  represent the psi-blast profile and yasspp scoring matrices, respectively. soe, rbf, and lin represent the three different kernels studied using the  as the base kernel. * denotes the best regression results in the sub-tables, and ** denotes the best regression results achieved on this dataset. for the best results the se rate for the cc values is  <dig> . the published results  <cit>  uses the default rbf kernel to give cc =  <dig>  and rmse =  <dig> .

protein-dna interaction site prediction
when it is known that the function of a protein is to bind to dna, it is highly desirable from an experimental point of view to know which parts of the protein are involved in the binding process. interaction is typically defined in terms of contacts between the protein and dna in their co-crystallized structure: residues within a distance threshold of the dna are considered interacting while the remaining residues are considered non-interacting. this is another example of a binary classification problem for sequence data. several researchers have presented methods to identify dna-binding residues. disis  <cit>  uses support vector machines and a radial basis function kernel with pssms, predicted secondary structure, and predicted solvent accessibility as input features while ahmad and sarai employ a neural network method with pssms as input  <cit> .

svmprat was used to train binary classification models on the disis dataset  <cit> . following disis, we performed 3-fold cross validation on our models ensuring that the sequence identity between the different folds was less than 40%. during the experiments, we found that window kernels with w = f performed the best and therefore omit other values for the parameters.

the numbers in bold show the best models for a fixed w parameter, as measured by roc. , and  represent the psi-blast profile and yasspp scoring matrices, respectively. soe, rbf, and lin represent the three different kernels studied using the  as the base kernel. * denotes the best classification results in the sub-tables, and ** denotes the best classification results achieved on this dataset. for the best model we report a q <dig> accuracy of  <dig> % with an se rate of  <dig> .

local structure alphabet prediction
the notion of local, recurring substructure in proteins has existed for many years primarily in the form of the secondary structure classifications. many local structure alphabets have been generated by careful manual analysis of structures such as the dssp alphabet  <cit> . more recently, local structure alphabets have been derived through pure computational means. one such example are the protein blocks of de brevern et al.  <cit>  which were constructed through the use of self-organizing maps. the method uses residue dihedral angles during clustering and attempts to account for order dependence between local structure elements which should improve predictability.

we chose to use the protein blocks  <cit>  as our target alphabet as it was found to be one of the best local structure alphabets according to conservation and predictability  <cit> . there are sixteen members in this alphabet which significantly increases prediction difficulty over traditional three-state secondary structure prediction.

we used a dataset consisting of  <dig> proteins derived from the scop database version  <dig> , classes a to e  <cit> . this dataset was previously used for learning profile-profile alignment scoring functions using neural networks  <cit> . to compute the true annotations, we used the three-dimensional structures associated with the proteins to assign each residue one of the protein blocks.

we used a small subset of the  <dig> proteins to tune the w and f windowing parameters with the soe kernel. we found w = f worked well on the subset and subsequently restricted the large-scale experiments to this case. three-fold cross validation was done on all  <dig> proteins for each parameter set and for both the soe and rbf kernels. table  <dig> reports the classification accuracy in terms of the q <dig> accuracy and average roc scores for different members of the protein blocks.

w = f gave the best results on testing on few sample points, and hence due to the expensive nature of this problem, we did not test it on a wide set of parameters. ** denotes the best scoring model based on the q <dig> scores. for this best model the se rate of  <dig> .

from table  <dig> we see that the soe kernel provides a small performance boost over the rbf kernel. the addition of predicted secondary structure information from yasspp  improves the q <dig> performance as would be expected for local structure prediction. our q <dig> results are very encouraging, since they are approximately 67%, whereas the prediction accuracy for a random predictor would be  <dig> % only. competitive methods for predicting protein blocks from sequence reported a q <dig> accuracy of  <dig> % in  <cit>  and  <dig> % in  <cit> .

datasets
our empirical evaluations are performed for different sequence annotation problems on previously defined datasets. table  <dig> presents information regarding the source and key features of different datasets used in our cross validation and comparative studies. we ensured that the pairwise sequence identities for the different datasets was less than 40%.

we utilized cross validation as our primary evaluation protocol. in n-fold cross validation, data are split into n sets. one of the n sets is left out while the others are used to train a model. the left out data are then predicted and the performance is noted. this process repeats with a different set left out until all n sets have been left out once. the average performance over all n-folds is reported. where possible, we used the same splits of data as have been used in previous studies to improve the comparability of our results to earlier work.

evaluation metrics
we measure the quality of the classification methods using the receiver operating characteristic  scores. the roc score is the area under the curve that plots the fraction of true positives against the fraction of false positives for different classification thresholds  <cit> . in all experiments, the roc score reported is averaged over the n folds of cross validation. when the number of classes is larger than  <dig>  we use a one versus rest roc scores and report the average across all classes.

we also compute other standard statistics defined in terms of the number of true positives , false positives , true negatives , and false negatives . these standard statistics are the following:   

for k-way classification, performance is summarized by qk, defined as   

where n is the total number residues and tpi is the number of true positives for class i.

the roc score serves as a good quality measure in the case of unbalanced class sizes where qk may be high simply by predicting the most frequent class. this is often true for binary classification problems with very few positive examples. in such cases, it is essential to observe the precision and recall values which penalize the classifiers for under-prediction as well as over-prediction. the f <dig> score is a weighted average of precision and recall lying between  <dig> and  <dig>  and is a good performance measure for different classification problems.

regression performance is assessed by the pearson correlation coefficient  and the root mean square error  between the predicted and observed true values for every protein in the datasets. the cc statistic ranges from - <dig> to + <dig> with larger values being better while rmse is larger than zero with lower values implying better predictions. the results reported are averaged across the different proteins and cross validation folds.

for the best performing models, we also report the standard error, se, of qk and cc scores, defined as   

where  is the sample standard deviation and n the number of data points. this statistic helps assess how much performance varies between proteins.

input information
position specific scoring matrices
for a sequence of length n, psi-blast  <cit>  generates a position-specific scoring matrix  referred to as . the dimensionality of  is n ×  <dig>  where the  <dig> columns of the matrix correspond to the twenty amino acids. the profiles in this study were generated using the version of the psi-blast available in ncbi's  <dig> . <dig> release of the blast package. psi-blast was run as blastpgp -j  <dig> -e  <dig>  -h  <dig>  and searched against ncbi's nr database that was downloaded in november of  <dig> .

predicted secondary structure information
we used the yasspp secondary structure prediction server  <cit>  with default parameters to generate the s feature matrix of dimensions n ×  <dig>  the th entry of this matrix represents the propensity for residue i to be in state j, where j ∈ { <dig>   <dig>  3} corresponds to the three secondary structure elements: alpha helices, beta sheets, and coil regions.

position independent scoring matrices
position independent sequence features were created for each residue by copying the residue's corresponding row of the blosum <dig> scoring matrix. this resulted in a n ×  <dig> feature matrix referred to as ℬ.

by using both pssm and blosum <dig> information, a svm learner can construct a model that is based on both position independent and position specific information. such a model is more robust to cases where psi-blast could not generate correct alignments due to lack of homology to sequences in the nr database.

CONCLUSIONS
in this work we have presented a general purpose support vector machine toolkit that builds protein sequence annotation models. dubbed svmprat, the toolkit's versatility was illustrated by testing it on several types of annotations problems. these included binary classification to identify transmembrane helices and dna-interacting residues, k-way classification to identify local structural class, and continuous predictions to estimate the residue-wise contact order. during our evaluation, we showed the ability of svmprat to utilize arbitrary sequence features such as psi-blast profiles, blosum <dig> profiles, and predicted secondary structure which may be used with several kernel functions. finally svmprat allows the incorporation of of local information at different levels of granularity through its windowing parameters. our experiments showed that this allows it to achieve better performance on some problems. svmprat's key features include:  implementation of standard kernel functions along with powerful second-order exponential kernel,  use of any type of sequence information associated with residues for annotation,  flexible window-based encoding scheme,  optimized for speed using fast solvers,  capability to learn two-level cascaded models, and  available as pre-compiled binaries for various architectures and environments.

we believe that svmprat provides practitioners with an efficient and easy-to-use tool for a wide variety of annotation problems. the results of some of these predictions can be used to assist in solving the overarching 3d structure prediction problem. in the future, we intend to use this annotation framework to predict various 1d features of a protein and effectively integrate them to provide valuable supplementary information for determining the 3d structure of proteins.

availability and requirements
• project name: svmprat

• website: http://www.cs.gmu.edu/~mlbio/svmprat

• mirror: http://bio.dtc.umn.edu/svmprat

• operating systems and architectures: 64-bit linux, 32-bit linux, 64-bit ms windows, 32-bit darwin , sun solaris sun-blade-1500-solaris

• programming language: ansi c

• additional features: compiled with/without optimized cblas

• license: gnu gpl

• restrictions for use by non-academics: yes

web interface
even though svmprat is easy to use and is available across a wide variety of platforms and architectures, we also provide biologists the functionality to predict local structure and function predictions using our web server, monster . svmprat serves as the backend for monster and can be accessed easily via the web link http://bio.dtc.umn.edu/monster.

authors' contributions
hr developed the svmprat code. hr and ck performed the experimental evaluation. gk provided support and developed the svm routines from the sv mlight package. hr and gk developed monster. all authors wrote the manuscript, read the manuscript, and approved for final submission.

