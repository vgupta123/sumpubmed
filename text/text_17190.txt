BACKGROUND
cell-to-cell variation is one of the most intriguing and important fields in today’s cell biology research. historically, the fact that cells are different from each other has been neglected, and this neglect has led to erroneous conclusions and descriptions of the system  <cit> . within the systems biology community, modelling of cells has typically been performed based on data from the average of cells, and the model has thus described an average cell. in several well-known cases, this average cell has turned out to be highly non-representative of the true underlying cellular behaviour. for instance, the prevailing view of the signalling cascade involving casp- <dig> was that the changes were described as gradual, since this was the average population behaviour; however, this population behaviour was obtained by a gradual change in the number of cells that had switched from one state to another, where the switch in each individual cell was fast  <cit> . cell-to-cell variation is also at the heart of understanding cell differentiation, which involves the important special cases of stem cell research and research on the development of tumours  <cit> .

one common example of single cell data is fluorescent recovery after photo-bleaching , which examines the time-dependent response to the bleaching of a part of a cell . this response normally follows an exponential decline/increase. the most straightforward analysis of such data is therefore to simply fit an exponential curve to the data, and evaluate the value of the exponent  <cit>  . the limitation of such an approach is that the exponent does not correspond to the velocity of any specific mechanism, but to a phenomenological description lumping many sub-processes together. a more mechanistically interpretable approach is to form a model based on prior knowledge of the underlying sub-processes. sometimes such a model is formulated using partial differential equations   <cit>  . the limitation of pdes is that a single simulation is very computationally time-consuming. therefore, pde-based models are usually utilized for forward-simulation, i.e. where simulations of different scenarios are performed, but where the model is assumed as known. another important type of modelling is known as reversed-engineering, in which parameters with mechanistic interpretation are estimated based on the data, and where conclusions can be drawn regarding mechanisms in the biological system  .
fig.  <dig> different approaches to single-cell analysis based on frap data. a the basic principle behind frap experiments: a part of a cell is bleached, and the recovery is followed. b the most common analysis of frap data: to fit an exponential function to the data. c pde simulations, where the gradients are continuous in the cytosol. d the reversed-engineering approach to frap data, to draw conclusions in terms of model rejections and estimation of parameters and predictions. e the sts approach: fit a model to each data separately, and then combine the estimations to get the distributions. f the nonmem approach: to add a postulated distribution for the parameter distributions among the population, and then fit to all the data at the same time using a joint likelihood function



such reverse-engineering approaches to research on cell-to-cell variation have typically been pursued using ordinary differential equations  and a method known as the standard two-stage approach    <cit> . sts is the typical approach used to study odes in systems biology  <cit> , applied to the problem of single-cell characterization. in other words, after a non-rejected model has been chosen, the parameters are determined in each cell separately . thereafter, the distribution of the cell population’s parameters are compared and determined . one of the problems with sts is that the data in one cell alone may be insufficient to determine the individual parameters for that particular cell accurately, i.e. that the uncertainty in each of the individual parameters are unacceptably high  <cit> . in such situations, it may be beneficial to make use of the data and information that exists also for the other cells.

such approaches, where one problem for one unit is solved in connection to the corresponding problem for all other units, have been developed in various other disciplines, under a variety of names. one such name is multi-task learning. this name is used in the machine learning community, and has been successfully applied to e.g. classification, pattern recognition, etc . however, multi-task learning approaches seem only rarely to have been applied to the task of estimating parameters in an ode  <cit> , and not at all to cell-to-cell variation studies. in contrast, the same idea has also been developed under the name mixed-effects modelling, and the sub-class known as nonlinear mixed-effects modelling   has been widely used to estimate parameters in odes  <cit> . the majority of nlme applications appear within the field of pharmacokinetics, i.e. for models that describe the uptake, breakdown, and effect of a drug in human or animal subjects.

regarding cell-to-cell variation, there are a few recent examples that make use of nlme, but there is no systematic evaluation of when and why nlme is advantageous compared to sts. one important series of papers regarding nlme and cell-to-cell variation have been published by zechner et al. the first such papers were applied to snapshot data, i.e. data were only a single data point is available for each cell . recently, this approach has been generalized to also be able to deal with time-series data  <cit> . the zechner papers focus on issues related to noise, and for instance seek to differentiate between extrinsic and intrinsic noise. presumingly for this reason, they work exclusively with continous-time markov chains , and do not present any results for odes, despite the fact that odes is the most widely used model class in the systems biology community  <cit> . in other words, the zechner papers do not explain how to use nlme to study cell-to-cell variations using odes. furthermore, the zechner papers do not demonstrate or explain why or when nlme are superior to sts. there is one conference paper on nlme-based ode-estimation of single-cell data  <cit> . this paper presents a comparison between such ode-estimation and an early version of the zechner snapshot approach  <cit> . however, also this paper  <cit>  does not explain when or why nlme should be used instead of sts. herein we present such an explanation.

more specifically, we demonstrate the occasional importance of studying cell-to-cell variation with nlme rather than using sts. based on simulated data, where the true model structure and parameter values are known , we show that for the case of uninformative data, nlme is advantageous over sts regarding parameter estimation: both kinetic and noise parameters were estimated significantly closer to the true values compared to estimating the parameters using sts. we show that this advantage seems independent of the reason for the lack-of-information in the data, and also unravel where the advantage comes from. we finally also demonstrate that nlme can be used for the analysis of real experimental frap data from the yeast saccharomyces cerevisiae.
fig.  <dig> overview of the analysis in the paper. both model  <dig> and model  <dig> are analysed with simulated data. the data comes in four forms: original, sparse, noisy and weak input signal data. the question is whether the true parameters can be obtained, and whether the non-informative cases  show different results from the informative data 



methods
the standard two stage approach
in sts, the following model structure is used 
  x˙i=fxi,ui,pi 

  yi=hxi,ui,pi 

where xi is the state vector for the i:th individual; ui is the input signal vector for individual i; pi is the parameter vector for the i:th individual; f and g are nonlinear vector functions; and yi is the vector of observations for individual i.

parameter estimation
in stage  <dig>  the parameters for each individual pi are estimated. note that in a modelling framework such as eqs. -, no information is shared between the individuals, which makes the parameter estimation problem for each individual a separate estimation problem. in stage  <dig>  the variability of the parameter estimates are calculated .

in the case of only estimating the parameters, we minimize the following cost function, based on the sum of squares of the residuals 
  costχ2=∑j=1ny∑i=1nyij−ŷij2σij <dig> 

where i and j denotes the i:th observation in the j:th state; yij is the experimental data; ŷij is the corresponding simulated output from the model; and σij is the standard deviation of the experimental measurement. in other words, the kinetic parameters p are given by 
  p^=argmincostχ <dig> 

note that this approach cannot be used to estimate σ, since in eq.  σ only appears in the denominator, and the optimum for σ therefore lies at +∞. for estimation of the noise, we therefore use the more general approach of maximizing the full log-likelihood function 
  −l=∑j=1ny∑i=1nyij−ŷij2σij2+∑j=1ny∑i=1nlogσij2+nynlog 

where 
  =arg maxl 

where σ is the vector of all σij, which now normally have their optima different from  <dig> or infinity, since they appear both in the numerator and in the denominator.

software
modelling and parameter estimation with the sts approach was performed using matlab . the simulation of the model was performed with the function spbdsimulate in the systems biology toolbox <dig>  . optimisation was done both using the built-in local optimisation algorithm lsqnonlin , and using the global simulated annealing and nonlinear simplex approach available in sbtb <dig>  all simulations and optimisations, except the ones made for the noise estimation and the jlh were done using a pc . the simulations and optimisations for the noise estimation and the jlh were done using a laptop . all matlab-files used  are available in additional file  <dig> 

the nonlinear mixed-effects approach
nlme is a general modelling approach that can be applied to analyse any type of system that can be described by eqs. -, i.e. to a system that is made up of individuals belonging to a joint population, and where the individuals’ parameter values belong to the parameter distribution of the population. this link between the parameter values among the individuals allows information to be shared between the individuals. the idea is that this information sharing may result in better estimates for both the individual parameters and for the covariance matrices. more specifically, in nlme models, the following general model structure is used 
  x˙i=fxi,ui,ϕi 

  yi=hxi,ui,ϕi,εi 

  ϕi=gΘ,ηi,zi 

  ηi∈n 

  εi∈n 

where xi, ui, and yi are, just as for sts, the state, input, and measurement vectors for individual i; where ϕi is the parameter vector for the i:th individual, which now no longer is a free variable, but instead depends on Θ, the population parameter vector describing the typical individual in the population, zi the covariates , and ηi, the random effects; and where Ω and Σ describes the covariance matrices of the random effects, ηi, and the measurement noise, εi, respectively.

parameter estimation
in nlme there are two types of parameters to estimate: the fixed effects, Θ, and the variances of the random effects, Ω and Σ.

the fixed effects describe the main trend, i.e. the typical value of the model parameters. for a single-cell model, such fixed effects could be the typical population value of a kinetic parameter.

there are two types of random effects; between-cell, η, and between-sample, ε, variability. the between-sample variabilities are related to the residuals, in that they describe the differences between the predicted, ŷ, and the observed, y, measurement values. in practice, the software packages used herein, nonmem and monolix, can handle e.g. additive 
  y=ŷ+ε 

proportional, and a combination of additive and proportional between-sample variability 
  y=ŷ·+ε <dig> 

the effect of the between-cell random effects, η, on the cell-specific parameters, ϕi, can in principle be described by any function, g, and the used software packages support both normal, log-normal, and user-specified distributions .

the marginal likelihood  of the model parameters for the data is the product of the individual marginal likelihoods of the cells, j according to 
  p=ljΘ,Σ,Ω|yj=∫pdηj=∫pyj|ηj,Θ,Σ·pηj|Ωdηj 

  lj=∫−∞+∞12πΣme−∑j=1ηjyj−ŷj2Σ12πΩe−ηj2Ωdηj=∫−∞+∞hjdηj 

where m is the number of observations per cell, i.e., m =n·ny. the parameters are estimated through minimizing - <dig> log of the likelihood . since there is no closed form solution to the marginal likelihood various approximations are available. the most commonly used approximation is the first-order conditional estimation method where l is linearised with a first order taylor expansion around the estimates of the random effects, i.e. around η. in the nonmem example , the numerical search for the minimum of the -2ll is implemented according to a modified algorithm by  <cit>  which is a derivate-free quasi-newton type minimization algorithm. the objective function value  reported by the software is proportional to -2ll. in the monolix example , the numerical search is done by a stochastic approximation of the expectation maximisation algorithm  <cit> .

software
the nlme approach has been implemented in several software packages  <cit> . in this paper, the two software packages nonmem and monolix are both used. this is done to show consistency in terms of results across different software packages, but also as a way of presenting several choices to the reader.

for the analysis with nonmem version  <dig> . <dig>  <cit>  was used. the interaction with nonmem is made through nm-tran, a language translating user-defined code and datasets into fortran <dig>  odes can be defined by the user and for this particular project a differential equation solver, for non-stiff systems, was used  together with the first-order conditional estimation  method. perl-speaks-nonmem  <dig> . <dig>  <cit>  was used for execution of models. nonmem and psn were installed on a pc and a laptop, which systems details were the same as described in the section the standard two stage approach, with the fortran compiler gnu gfortran.

for the analysis with monolix, version  <dig> . <dig> was used, as implemented in matlab version  <dig>   <cit> . importantly, for users who do not have access to matlab, a stand alone version also exists. the models are written in a language called mlxtran, which apart from having a rich library of pkpd-models also allows the users to define their own odes  <cit> . sbtb <dig> also contains functions for translating sbtb2-models into mlxtran, using the addon-package sbpop. compared to nonmem, monolix offers a more user friendly environment, including a graphical user interface. for the beginner, we therefore recommend to use monolix, and we have also developed a small tutorial, which explains how to us it for single-cell models. this tutorial, together with all scripts used to perform the analysis in this paper, is available in the additional file  <dig>  all the analysis regarding monolix was performed on the same pc as described in the section the standard two stage approach. all nonmem-files and monolix-files used  are available in additional file  <dig> 

an in-between approach: the joint likelihood function
there are two main differences between sts and nlme which both potentially could lead to improvements in the parameter estimation: i) in nlme one forms a likelihood function for the parameter estimation to the combined data set for all cells, and ii) in nlme one postulates a distribution for the variation of the parameter values across the cell population.

to analyse where the improvement in the parameter estimation originates from, we also did some analysis with an in-between approach: the joint likelihood function  approach. in jlh, we only use improvement aspect i), the single likelihood function for the combined data set, but do not postulate a joint parameter distribution. in other words, instead of eqs. -, we use the following two equations: 
  −ljlh=∑k=1nc∑j=1ny∑i=1nyijk−ŷijk2σijk2+∑k=1nc∑j=1ny∑i=1nlogσijk2+ncnynlog 

where 
  p^ <dig> p^ <dig> …σ^ <dig> σ^ <dig> …=argmaxljlhp <dig> p2…,σ <dig> σ <dig> … 

where nc denotes the number of cells.

software
the software and optimization and model formulation tools used for the jlh analysis is the same as for sts.

experimental data
the experimental procedures are further discussed in  <cit> . frap experiments were performed using yfp. we used yeast cells of the by <dig> genetic background expressing yfp under the control of the constitutively expressed act <dig> promoter . in addition, this strain expressed two extra fluorescent protein reporters: a cfp-ace <dig> fusion and a myo1-mcherry fusion, both driven by their own promoters. we used ace <dig> to locate the nucleus and determine the cells position in the cell cycle , and we used myo <dig> to confirm mother-daughter separation . in this way, we minimized cell-to-cell variation in our measurements related to cell cycle position, without disturbing the system with synchronization procedures.

we used exponentially growing cells cultured in synthetic medium . for the experiments, we attached cells to the bottom of 384-well glass bottom plates . to prevent cells from moving during the experiment, we pre-treated the wells with concanavalin a , as previously described .

photobleaching of individual nuclei was performed using an olympus ix- <dig> inverted microscope with a fv <dig> confocal module with an oil immersion olympus uplansapo 63x objective . we used an automatic z-axis control, a motorized x-y stage, a 458-488- <dig> argon laser, a  <dig> he-ne laser and photomultiplier  hamamatsu r <dig> 

for train-of-frap experiments, we imaged yfp with the  <dig> nm laser. in each cell, we repeated four times the following procedure: we first took  <dig> images, then performed a partial photobleaching , then measured the recovery for  <dig> sec . from the photobleaching step, we used a  <dig> % laser power on a small sub-area of the nucleus  during  <dig>  sec. subsequently, we imaged using  <dig> % laser power and  <dig> sec/pixel scanning speed. we set the confocal microscope pinhole to wide open .

quantification of total fluorescence in each compartment was performed in imagej, by manually drawing regions of interest  in the nucleus and the cytosol, and quantifying it using the imagej plugin “time series analyzer v2”. a typical roi was a circle of  <dig> nm in radius. we applied photobleaching and autofluorescence corrections to all images as described in  <cit> .

two case studies: a linear and a nonlinear model
let us now introduce the two examples considered in this paper. the first, model  <dig>  is a linear model, describing transport of yfp, and the second, model  <dig>  is a nonlinear model describing the osmo-regulation of yeast cells.

linear model
the state-space description of model  <dig> is given by the following four equations 
  dndt=−k1·n+k2·c 

  dcdt=k1·n−k2·c 

  yn=n+ε 

  yc=c+ε 

where n and c are the amounts of yfp in the nucleus and in the cytosol, respectively; where k <dig> and k <dig> are the transports from and to the nucleus, respectively; and where yn and yc are the two measurement signals . a sketch of the model is given in fig. 3a.
fig.  <dig> model  <dig> and the data. a sketch of model  <dig>  b example of simulated data of the nucleus under different conditions. c example of simulated data of the cytosol under different conditions



in sts estimation, data from each cell were analysed separately, potentially yielding as many k <dig> values as there were cells in the experiment. for simulation of the data using eqs. -, the following equations were used to obtain the initial conditions 
  n=pn,j·yn 

  c=pc,j·yc 

where tfrap,j is the first time point after frap j. there were four fraps, two states, and two kinetic parameters, and thus  <dig> unknown parameters in the parameter vector p  p= 

in the nlme estimation, the individual rate constants k1j and k2j were described by the following equations 
  k1j=θk1·eηk1j 

  k2j=θk2·eηk2j 

where θk <dig> and θk <dig> are the typical values of k <dig> and k <dig> in the cell population, respectively, and ηk1j and ηk2j are random effects describing the difference between the typical and individual values. ηk <dig> and ηk <dig> belongs to normal distributions with mean  <dig> and estimated variances, ωk <dig> and ωk <dig>  respectively. the estimated variances ωk <dig> and ωk <dig> can be correlated. both the variances ωki <dig> and their correlations are collected in the variance-covariance matrix Ω. thus, five parameters are needed to describe the individual k1j and k2j for the entire population: θk <dig>  θk <dig>  ωk <dig>  ωk <dig>  and the off-diagonal element in Ω. unlike in sts, this number is always five, independent of the number of analysed cells.

the initial conditions were modelled as 
  n=e·yn 

  c=e·yc 

where ηj∈nj and Θ <dig> is the standard error of the residual error.

finally, the noise is for both sts and nlme assumed to be additive, which also is how the simulated data was generated. in other words, simulations were done for different values of the parameters, additive non-correlated noise was added, and the ability of sts and nonmem to retrieve the parameter values and the standard deviation of the noise was evaluated.

the nonlinear model
model  <dig> is a model published by gennemark et al.  <cit>  . the model describes how the yeast cell reacts to an osmolarity shock by producing glycerol via activation of the protein hog <dig> . a description of all the equations can be found in the supplementary material. the model consists of  <dig> odes,  <dig> parameters, and  <dig> algebraic equations, including several nonlinearities, both products of states, and events switching between two expressions depending on the value of a state.  <dig> parameters  were optimized from the simulated data, and the remaining  <dig> parameters were kept at their literature values. the initial conditions were assumed to be known, as was the noise level. the input of the model is the addition of salt to the cells  and the measured output signal of the model is intracellular  and total  glycerol concentrations .
fig.  <dig> model  <dig> and the different inputs and data. a sketch of model  <dig>  b the two different input signals used in the analysis. c artificial data of intracellular glycerol concentration under different conditions d artificial data of total glycerol concentration under different condition



in the nlme estimation, the model parameters are described by the following equations. 
  vej=θve·eηvej 

  kp2j=θkp2·eηkp2j 

  khogj=θkhog·eηkhogj 

  tdj=θtd·eηtdj 

where θx is the typical value of x in the cell population, and ηxj is the random effect describing the difference between the typical and individual values for parameter x for cell j. these random effects  belongs to normal distributions with mean  <dig> and estimated variances . there are no covariances, i.e. the matrix Ω is set to be diagonal. both measurements of the model, intracellular and total glycerol concentrations, have additive noise 
  yic=ŷic+εic 

  ytot=ŷtot+εtot 

where index ic means intracellular, tot means total, and where εic and εtot are normally distributed, with mean  <dig> and estimated variances.

comparison of performance
the performance of sts and nlme are analyzed by comparing the relative deviation from the true parameter value  . apart from figures showing the relative deviation for each parameter for each artificial cell, the differences between sts and nlme is also accompanied by a student’s t-test. the t-test is pairwise, and tests whether the relative deviation  is significantly different between sts and nlme.
fig.  <dig> parameter estimation for model  <dig>  analysis in the case of simulated data and parameter estimation for the parameter k
 <dig> in the case of known noise for model  <dig> using data that is: a under good condition, b sparse sampling, c noisy, and d with a weak input signal respectively. the results are normalized by dividing with the known true value. the x-axes corresponds to the  <dig> simulated datasets. in comparison, the results from the parameter estimation are similar between sts and nonmem in the case of the original data, but there is a clear advantage of using nonmem when the quality of the data decreases



ethics
the experiments and data collection were carried out on saccharomyces cerevisiae, which has no associated ethical issues.

RESULTS
linear model: nlme is advantageous in cases of low-quality data
we generated frap data structured like the real experimental data  but with known true kinetic parameters, in order to determine whether there is a difference between sts’s and nlme’s ability to estimate the true parameters in a system . for model  <dig>  nlme was implemented using the software nonmem. both sts and nlme estimated the parameters using the true structural model, which for the case of nlme also included the true additive error model eqs. -. in addition, nlme were given the true form of the kinetic parameter distributions among the cell population . the standard deviation of the measurement noise was in the first part of the analysis assumed to be known.

the generated frap data were divided into four different cases: original data, sparse data, noisy data, and weak input signal data . the original data  had  <dig> observations per state, giving a total of  <dig> observations per artificial cell. the sparse data  had  <dig> observations per artificial cell . the noise of the noisy data  had a standard deviation of  <dig>  compared with  <dig> for the original data. the weak input signal data had fraps that were half of the strength of the fraps used to generate the original data. the initial conditions were the same for all time-series in the original, noisy, and sparse data, since they correspond to the same perturbation from the steady state. for the same reason, the weak input signal data had an initial condition closer to the steady state. all initial conditions were estimated from the simulated datasets eqs. -

in fig.  <dig>  the results from the estimation of the kinetic parameter k <dig> from both sts  and nlme  can be seen. from this figure, it is clear that for the case of the original data , there is no significant difference between sts and nlme in terms of their ability to estimate the true parameters: the relative deviation from the true values were  <dig>  ± <dig>  for nlme and  <dig>  ± <dig>  for sts . however, for the cases when the quality of the data is reduced in either of three different ways , there is a larger difference: sts often fails, while nlme still produces roughly the same results. these differences are also supported by a paired student’s t-test . similar results can be seen for the other kinetic parameter, k <dig>  in figure s <dig> in additional file  <dig> 

linear model: the same improvement holds for noise estimation
next we considered the case of also estimating the measurement noise. all results gave the same type of improvement as for the analysis above. we considered the case where the same noise distribution was used for all cells. in other words, for nlme there was only one additional parameter to estimate: the variance of the noise. for nlme, this means that a single value is obtained for all the cells. conversely, for sts, which considers the estimation in each individual cell as an independent problem, there were  <dig> estimated values for this new noise parameter. the results are shown in fig.  <dig>  as can be seen, nlme performed significantly better in the case of sparse data , but equally well in the case of rich data . again these conclusions are supported by a student’s t-test .
fig.  <dig> noise estimation for model  <dig>  analysis in the case of unknown variance of the measurement noise. the noise is assumed to be additive and the same in all cells. for sts, one estimate of σ is obtained from each cell . a for rich data, sts can estimate noise through the average of the values  equally good as nonmem , even though some individual cells display worse estimates. b for the case of sparse data, however, the mean from sts is significantly worse than for nonmem



the results hold for a nonlinear model
we also generated simulated data from the second, nonlinear, model. this generation used a four step input signal , simulating the addition of equal amounts of salt four times at equal time intervals. the generated data were divided into four different cases: original data, sparse data, noisy data, and weak input signal data. the sparse data  had  <dig> observations per output signal, giving a total of  <dig> observations per artificial cell. the  <dig> observations can be compared with the  <dig> observations used in the original data . the noise of the noisy data  had a standard deviation of  <dig>  compared with  <dig>  for the original data. the amplitude of the input signal in the data with a weak input signal  was one tenth of the amplitude of the input signal in the original data .the parameter estimation with nlme included the true error model and the true, lognormal, form of the parameter distributions.

for model  <dig>  nlme is implemented using monolix. figure  <dig> shows the results of model  <dig> for the parameter td. in fig. 7a, the red and blue curves largely overlap, meaning that in this case, using the original data, there is no difference between sts and nlme. in contrast, when the quality of the data is reduced in either of three different ways , sts is significantly worse than nlme. these observed differences are confirmed by a paired student’s t-test . similar results can be seen for the other three parameters, ve , kp <dig> , khog , in additional file  <dig> 

fig.  <dig> parameter estimation for model  <dig>  analysis in the case of simulated data and parameter estimation for the parameter td in the case of known noise for model  <dig> using data that is: a under good condition, b sparse sampling, c noisy, and d with a weak input signal respectively. the results are normalized by dividing with the known true value. the x-axes corresponds to the  <dig> simulated datasets. in comparison, the results from the parameter estimation are similar between sts and monolix in the case of the original data, but there is a clear advantage of using monolix when the quality of the data decreases

where does the improvement come from?
the above analysis demonstrates that nlme gives an improvement in the cases of having insufficient information in the data due to either sparsity in sample points, noise or bad input signal leading to bad excitation of the system. this leads to the natural follow-up question of where the improvement comes from. this analysis is done using the jlh approach, and this is done in two ways.

for the first jlh analysis, all parameters are kinetic or initial condition parameters, and are individual to each cell. that means that the joint likelihood function ltot, breaks down into its individual components 
  ljlh=l1+l2+… 

where li is the likelihood function considering the data available for the i:th cell only. in other words, in the first jlh analysis, jlh provides no improvement compared to sts, and all the observed improvement comes from the postulation of a joint parameter distribution across the cell population.

for the second jlh analysis, the noise distribution is shared among all cells, meaning that the total likelihood breaks down in the following way 
  ljlh=l1+l2+… 

where λ is the standard deviation of the noise. in other words, using the total likelihood function ljlh in eq.  for the estimation of all the parameters at the same time, could therefore in principle be an approach that is superior to sts.

the result of applying this second approach, in eq. , to model  <dig> is shown in fig.  <dig>  as can be seen, a jlh function  does converge faster to the truth for both parameter  and noise estimation  compared to sts . however, jlh is still not as good as nlme .
fig.  <dig> analysis of sts, nonmem, and jlh dependency of number of data sets using model  <dig>  the figure shows sts , nonmem  and the jlh approach  precision in the combined parameter and noise estimation problem, with respect to number of datasets. in the jlh approach, a joint likelihood without postulated parameter distributions has been used. a the results from the parameter estimation as the normalised sum of the absolute values of the deviation from the true parameter. b the results from the noise estimation of true noise level . estimates of both parameters and the noise from both nonmem and jlh are closer to the true values than estimates from sts. also, it is clear that nonmem converge faster towards the true parameters and noise with respect to the number of data sets, when compared to jlh



all in all, this means that the improvement of nlme comes purely from the assumption of a shared parameter distribution in cases of only estimating parameters that are unique to each cell; in contrast, the advantage of the shared distribution is combined with the additional advantage of a joint likelihood function, in cases of shared parameters across the cell population ).

application to real experimental data
to demonstrate that nlme is applicable to real data of cell-to-cell variations, we also performed a corresponding analysis for the experimental data analysed using sts and nlme . here the true parameters are not known. using all available experimental data, the estimated parameters from sts and nlme are roughly the same and they both describe the data well as can be seen in fig. 9a, c, d. however, when removing data from the full time-series, to make the data sparse, new corresponding sts parameter estimates become much more changed than the corresponding nlme estimates, fig. 9b. this is consistent with the results from the simulated data above.
fig.  <dig> analysis of sts and nonmem using real frap-data with model  <dig>  a and b are histograms of parameter k
 <dig> estimated with nonmem  and sts  using all experimental data in each cell , and only  <dig> % of the data in each cell . a the parameter distribution from sts and nonmem coincide in a lognormal distribution. b, while nonmem roughly stays in the same interval as in , the interval from sts vastly increases. c and d is a representative example of the model fit to the experimental data from one of the cells using sts and nonmem, respectively. results for parameter k
 <dig> shows similar results as for k
1




discussion
in this paper, we have answered the questions when, why, and how nlme should be applied to the problem of parameter and noise estimation based on ode analysis of single-cell data. this analysis brings clear evidence that there are important and common cases when nlme is advantageous compared to the traditional sts approach: in cases of non-informative data, nlme converges faster to the true values.

when considering the case for nlme in single-cell analysis, there are a few strengths that should be further clarified. firstly, although nlme has only recently been discovered in the analysis of single-cell data, it has a long tradition in other fields, in particular pharmacometrics. thus, there is already a rich literature of theoretical and methodological results supporting the nlme approach. in other words, the theoretical properties of the method are already well-established, and there are many associated analysis tools that can be used in future analysis of cell-to-cell variation. secondly, a specific example of such a well-established approach is covariate analysis, which has a high potential also for the study of cell-to-cell variation. covariate analysis was part of the initial incentive behind the development of nlme: to find correlations between subject-specific characteristics, such as age and weight, with the properties in the model, including subject-specific responses to drug treatments  <cit> . in the case of cell-to-cell variation, this covariate analysis translates to the identification of correlations between cell-specific characteristics such as cell-line, cell-volume, cell-type, etc, with e.g. the cell-specific kinetic parameters estimated by the model. thirdly, the rich theory behind nlme also includes estimation of the noise. such noise estimations are still quite rare in systems biology studies, but are standard in nlme estimations performed in nonmem. fourthly, the development of nlme in software packages such as nonmem and monolix was driven by challenges commonly seen with patient-specific studies, challenges that also are present in many cell-to-cell variation studies; these packages are thus well adapted for cell-to-cell variation studies. for instance, patient-specific studies often have the limitation that only a few data-points can be collected for each patient; this is a common problem also for cells. similarly, patient-specific studies are often associated with high noise-levels; high noise-level is also a problem that often becomes pronounced when considering data from individual cells. a final important advantage of nlme is the computational effort. this advantage is primarily seen when comparing nonmem and jlh of eq. . the high computational load of jlh comes from the fact that all parameters have to be estimated in one problem; the computational time in nonmem and monolix is reduced via various approximations, which have been developed over the years. nonmem is also fast because it is implemented in fortran. as an example, the computational time using  <dig> data sets was roughly  <dig> hours, and more than  <dig> hours, for nonmem and jlh, respectively.

there are naturally also limitations with nlme, and with its current implementations, when considering them in a systems biology single-cell context. for instance, pharmacometrics models are typically small, with around 3– <dig> states. today’s single-cell models are usually equally small, but other systems biology models may be substantially bigger, sometimes including hundreds of states. as single-cell omics data becomes increasingly available, this will mean that larger models probably will appear in a single-cell context as well. this will put new challenges to nlme implementations. this challenge is also put forth by the parallel developments of systems pharmacology, which links pharmacometrics models with intracellular models. one other limitation and challenge when adopting nlme to single-cell analysis is the difference in concepts and notions, and also this challenge is also put forth by systems pharmacology. for this limitation, we argue that monolix is an important alternative to the more widely used nonmem package, since monolix is based on matlab and has a user-friendly interface. finally, the methodologies considered herein have limitations in terms of their handling of noise, since they do not account for process noise. therefore, it is important to also follow the implementation of the other sub-communities of nlme, and their implementations into single-cell analysis. the perhaps most important such paper to date is the recent one by zechner et al.  <cit> .

while the method presented in the zechner paper is similar to the one we propose in the sense that both methods adapt a mixed-effects approach, there are still fundamental differences. firstly, the zechner paper only deals with models based on continuous-time markov chains. this is a class of models that is fundamentally different from the models based on odes that we use. in other words, one cannot simply put a process noise parameter to zero in their formalism and obtain the same equations and methods proposed herein. secondly, the zechner paper estimates their parameters using a bayesian inference network, which is an alternative framework, different from the frequentist approaches in nonmem and monolix. finally, we do a thorough analysis of when, why, and how nlme should be applied to single-cell problems. the zechner paper does not have the kind of convergence and comparison plots that we have , clearly demonstrating the advantage of nlme compared to sts. also, the zechner paper does not unravel the reason why this advantage is there, i.e. they do not differentiate between the different contributions of jlh and the assumption of a joint distribution across the population.

CONCLUSIONS
nlme is a widely used approach in other areas, but it has only recently and in a few papers been applied to single-cell data. no systematic comparison has clearly answered the questions when, why, and how to use it in this new situation. in this paper, we have shown that nlme should be used when the available data for each cell are not informative enough to obtain reliable parameter estimates using each cell. if the data are informative enough, nlme provides no advantage in terms of accuracy, but if the data is either too sparse, too noisy, or obtained with a too weak stimulation, nlme has a faster convergence to the true parameter values. this holds for both linear and nonlinear systems, for both simulated and experimental data, and for both parameter and noise estimation. the reason why nlme is advantageous is i) that a joint likelihood function is formed, and ii) that one assumes a shared distribution of the parameter values across the cell population. the first factor, i), only contributes if there are shared parameters, such as e.g. the noise level, across the cell population. nlme is implemented in a wide variety of software packages previously not mentioned in the single-cell literature, and we provide a small tutorial for how to use monolix - a user-friendly and stable alternative - for the analysis of single cell data. this answers the final question of how to get started.

availability of supporting data
the data sets supporting the results of this article are included within the article and its additional files.

additional files
additional file  <dig> 
supporting data. a zip-file containing the datasets used in the analysis, matlab-files, nonmem-files, and monolix-files equivalent to the ones used in the analysis, as well as a mini-tutorial of how to use monolix. 



additional file  <dig> 
equations for model  <dig>  a pdf containing the complete equations for the nonlinear model used in the second case study. 



additional file  <dig> 
supplementary figures and tables. a pdf with figures showing the results of the estimation of the parameters not shown in the article, as well as tables summarising the results of the student’s t-tests for the different parameters. 



abbreviations
ctmccontinous-time markov chain

frapfluorescent recovery after photo-bleaching

jlhjoint likelihood function

nlmenonlinear mixed-effects modelling

odeordinary differential equation

pdepartial differential equation

roiregion of interest

stsstandard two-stage approach

yfpyellow fluorescent protein

competing interests

the authors declare that they have no competing interests.

authors’ contributions

mk developed, implemented, and analysed the results for parameter estimation accuracy for model  <dig> and model  <dig>  dj developed, implemented, and analysed an early version of the results for model  <dig> and dj also developed, implemented, and analysed the presented results for the jlh-approach, noise estimation, and the experimental data. mck contributed with an expertise on nonmem. ld carried out the experimental work and together with acl provided experimental data and contributed with a biological model and interpretations. gc supervised and designed the study, and co-drafted the manuscript with mk and dj. all authors read and approved the final manuscript.

the research has been funded by list and by the swedish research council.
