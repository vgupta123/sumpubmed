BACKGROUND
the role of bioinformatics to support the life sciences has become fundamental for the collection, the management and the interpretation of large amount of biological data. the data are in most cases derived from experimental methodologies with large scale approaches, the so-called "omics" projects. international projects aimed to sequence the whole genomes of model organisms are often paralleled by initiatives for the expressed data sequencing to support gene identification and functional characterizations. moreover, because of advances in biotechnologies, ests are daily determined in the form of large datasets from many different laboratories. therefore, the analyses of expressed sequence data involve the necessity of suitable and efficient methodologies to provide high quality information for further investigations. furthermore, suitable models for the organization of information related to est data collections are fundamental to provide a preliminary environment for analyses of structural features of the data, as well as of expression maps and of functional relationships useful for the interpretation of mechanisms and of rules of gene expression processes.

there are many software available for est processing, with the purpose to clean the datasets from contaminations  <cit>  and to cluster sequences sharing identities to assemble contigs  <cit> . sequences cleaned from contaminations are usually submitted to the dbest database as they represent a fundamental source of information for the scientific community  <cit> . the results of the clustering step are useful to analyse sequence redundancy and variants as they could represent products of the same gene or of gene families. moreover, ests or contigs obtained from the clustering step are usually analysed by comparisons with biological databanks to provide preliminary functional annotations  <cit> . on the other hand, few efforts are known where all the sets of consecutive steps for est processing, clustering and annotation are integrated into a single procedure  <cit> .

expressed sequence curated databanks are worldwide available. they consist of collections built starting from dbest, using selected computational tools to solve the complex series of consecutive analyses. some of the well known efforts are the unigene database  <cit> , the tigr gene indices  <cit>  and the stack project  <cit> .

our contribution to this research is a pipeline, named parpest , for the pre-processing, clustering, assembling and preliminary annotation of ests, based on parallel computing and on automatic information storage. useful information resulting from each single step of the pipeline is integrated into a relational database and can be analysed by structured query language  calls for a "ad hoc" data-mining. we also provide a web interface with suitable pre-defined queries to the database for interactive browsing of the results that is supported by graphical views.

methods
the inputs to parpest can be raw est data provided as multi-fasta files or in genbank format. the pipeline allows pre-processing, clustering and assembling of ests into contigs and functional annotation of both raw est data and resulting contigs  using parallel computing.

the pipeline has been implemented using public software integrated by in-house developed perl scripts, on a 'beowulf class' cluster, with linux  as default operating system and the oscar  <dig>  distribution  <cit>  that provides the tools and the software packages for cluster management and parallel job executions.

the main process of the pipeline is designed to serialize and to control the parallel execution of the different steps required for the analysis and to parse into reports the collected results.

input datasets are parsed by a specific routine so that information from the genbank format or included in the fasta format could be upload into a mysql database.

sequence data are pre-processed in two steps, to clean the data and to avoid mis-clustering and/or mis-assembling. the first step requires repeatmasker  <cit>  and the ncbi's vector database  <cit>  for checking vector contaminations. in the second step, repeatmasker and repbase  <cit>  are used for filtering and masking low complexity sub-sequences and interspersed repeats. to accomplish sequence pre-processing a specific utility has been designed to distribute the tasks across the computing nodes. job assignments are managed by a pbs batch system  <cit> . job control at each step and output files integration is managed by the main process.

pace  <cit>  is the software we selected for the clustering step. for a parallel execution it requires an mpi implementation and a job scheduler server. once the whole pre-processed sequences are clustered, they are assembled into contigs using cap <dig>  <cit> . to exploit the efficiency of cap <dig> and to avoid the overhead time consuming of pbs, the main process we implemented has been designed to bundle groups of commands to be executed sequentially by each processor.

the functional annotation is performed using the mpi-blast package  <cit> . raw est data and assembled contigs are compared using blastx versus uniprot database  <cit> . the blast search is performed setting an e-value less equal than  <dig>  in case of successful matches, the five best hits are reported. when the subject accession number is reported in the gene ontology  database  <cit>  the corresponding classification is included to further describe the putative functionalities. moreover, links to the kegg database  <cit>  are provided via the enzyme  <cit>  identifier in the resulting report, for investigations on metabolic pathways.

all the results obtained from the single steps of the pipeline are recorded in a relational database and are managed through sql calls implemented in a suitable php-based web interface to allow interactive browsing of all the structural features of each est, their organization in the assembled contigs, the blast-derived annotations as well as the go classifications.

RESULTS
efficiency
the pipeline performs a parallel analysis on large amount of est data. because of distributed computing there is no execution limit for the processes, that are allocated according to available resources. the free release of pace,  <cit>  that we experienced to be limited at  <dig>  sequences, has been updated with the latest version provided by the authors who successfully tested the software with more than  <dig>  sequences . therefore, the only limiting factor for the complete execution of the pipeline is the memory space required for the database storage.

the pipeline has been tested on a cluster of  <dig> nodes single processor. in table  <dig>  execution times  are reported for  <dig> different dataset sizes  and for different node configurations . execution times are reported for the main steps of the pipeline analysis. from the table it is evident that the execution time of the pipeline is strongly dependent on mpi-blast analyses. therefore the behavior of the pipeline in terms of scalability and execution times is strongly influenced by blast comparisons .

as expected, large datasets  give the widest reduction of execution time increasing the number of nodes . the execution time for smaller datasets is almost the same when using different node configurations. this is due to the overhead time caused by the job scheduling software. a deeper evaluation of the overhead time effect is reported in figure  <dig>  which shows the average execution time per sequence using different node configurations. for increasing numbers of ests the profiles in figure  <dig> become flatter, because the average system response time becomes more stable for large data amounts, resulting in a reduced overhead effect.

we implemented the software to make it independent of the resource manager server. therefore, though we based the system on a pbs resource manager, it can be easily ported to other environment such as globus toolkit  <cit>  or sun grid engine   <cit> . therefore the current pipeline could be also implemented on to the latest grid computing environment.

database description
dbest data are organized in genbank format where organism, cloning library, development stage, tissue specificity and other information are usually available. while parsing the input file, a complete set of basic information useful to described the sequence are collected in the 'est' table of the relational database we designed .

another table is used to describe vector contaminations according to the report the main process of the pipeline automatically produces during the pre-processing step . therefore the database will include information about the ests still including vector or linker contaminating sequences. a similar approach is used to report masked regions representing low complexity subsequences or repeats as identified byrepeatmasker, using repbase as the filtering database.

clusters obtained from pace resulting in single est sequence or in contigs, are collected in the database too. a specific routine included in the main process of the pipeline performs a deep analyses on the clustered sequence to derive information on how many ests belong to a single contig, and how many contigs are produced once the sequences are clustered.

cap <dig> assemblies sequences building a multiple alignment and deriving a consensus to obtain a contig. to use only high-quality reads during assembly, cap <dig> removes automatically 5' and 3' low-quality regions . therefore, to keep information about the whole assembly process, both the complete alignment and the est trimmed regions are recorded into the database.

the table designed to organize the blast report from raw est data as well as from contig sequence analyses, can include the five most similar subject sequences and their related information. gene ontology terms related to each blast hit are recorded into the go table included in the database.

web application
the information obtained from the execution of the pipeline is stored in a mysql database that provides a datawarehouse useful for further investigations. indeed, all the information collected in the database can support biologically interesting analyses both to check the quality of the experimental results and to define structural and functional features of the data. for this purpose the database can be queried through sql calls implemented in a suitable php-based interface. we provide a pre-defined web based query system to support also non expert users. different views are possible. in particular, est browser  allows users to formulate flexible queries considering three different aspects, related to the features of the est dataset as they have been described in the input process . therefore, a single est or a group of ests can be selected by organism, clone library, tissue specificity and/or developmental stage. searches can be filtered according to sequence lengths too.

users can further select data based on the preliminary functional annotation, specifying a biological function as well as a go term or a go accession . moreover, restrictions on results obtained from the whole analytical procedure can be applied to retrieve different sets of ests . for example users can retrieve all ests containing or not vectors, presenting or not blast matches, classified as singletons or to be in a cluster.

cluster browser  is specifically dedicated to select clustered sequences through a specific identifier, as it is assigned by the software, and their structural features . information about the functional annotation of the contig can be used for retrieving too . results from specific queries are reported in graphical display, reporting among other information, the contig sequence, the ests which define the clusters and their organization as aligned by cap <dig> . this is considered useful to support analyses of transcribed variants putatively derived from the same gene or from gene families.

CONCLUSIONS
we designed the presented pipeline to perform an exhaustive analysis on est datasets. moreover, we implemented parpest to reduce execution time of the different steps required for a complete analysis by means of distributed processing and of parallelized software. though some efforts are reported in the literature where all the steps included in a est comprehensive analyses are integrated in a pipelined approach  <cit> , to our knowledge, no public available software is based on parallel computing for the whole data processing. the time efficiency is very important if we consider that est data are in continuous upgrading.

the pipeline is conceived to run on low requiring hardware components, to fulfill increasing demand, typical of the data used, and scalability at affordable costs.

our efforts has been focused to fulfill all the possible automatic analyses useful to highlight structural features of the data and to link the resulting data to biological processes with standardized annotation such as gene ontology and kegg. this is fundamental to contribute to the comprehension of transcriptional and post-transcriptional mechanisms and to derive patterns of expression, to characterize properties and relationships and uncover still unknown biological functionalities.

our goal was to set up an integrated computational platform, exploiting efficient computing, including a comprehensive informative system and ensuring flexible queries on varied fundamental aspects, also based on suitable graphical views of the results, to support exhaustive and faster investigations on challenging biological data collections.

availability
the design of the platform is conceived to provide the pipeline and its results using a user friendly web interface. upon request, users can upload genbank or fasta formatted files.

we offer free support for processing sequence collections to the academic community under specific agreements. we would welcome you to find contacts and to visit a demo version of the web interface at .

