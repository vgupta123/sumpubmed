BACKGROUND
advances in sequencing technology allow high throughput production of biological sequences in sequencing laboratories around the world. the exponential increase of genomic data extracted recently introduces the need for analysis techniques that can handle the large amount of data. this is very challenging as conventional analysis methods can be overwhelmed by volume and misled by statistical biases. it is important to develop novel tools that are time efficient and able to cope with the diversity of the data.

one of the most important tools for sequence analysis, if not the most important one, is sequence alignment which attempts to arrange biological sequences to identify regions of similarity. similarities between sequences can provide clues to discover the evolutionary relationship between species, to annotate new sequences and to compare an un-known sequence against existing sequences in a large database. there are two broad kinds of sequence alignment, namely global alignment and local alignment. global alignment attempts to match entire sequences from end to end and thus is suitable for comparing short sequences that are expected to have similar structures and functions such as proteins or genes. on the other hand, local alignment searches for conserved regions, possibly reordered, between two sequences. local alignment is thus more suitable for analysing long sequences, such as chromosomes or genomes, especially from distantly related species where significant insertions, deletions and large rearrangements may have occurred.

most existing alignment methods are inspired by the dynamic programming approach  <cit>  which attempts to examine all possible pairings of the two sequences and chooses the highest matching score alignment. this dynamic programming alignment approach has quadratic time and space complexities and hence is unattractive for handling long sequences and high volume sequence databases. to trade sensitivity for running time, heuristic search methods are often used. instead of comparing every single base of the two sequences, fasta  <cit>  and blast  <cit> , the two most popular database search tools, first search for seeds of k consecutive exact matches. seeds are then extended, by limited dynamic programming, to allow for mutations and gaps.

since  <dig> when the first genome of a free-living organism was sequenced  <cit> , a number of alignment tools capable of comparing genomes have been developed. such examples are gapped blast  <cit> , sim <dig>  <cit> , ssaha  <cit> , dialign  <cit> , mga  <cit> , mum-mer  <cit> , blastz  <cit> , chaos  <cit> , and avid  <cit> . most of these methods rely on the ideas of fasta and blast; they use different techniques for finding seeds and for extending seeds to identify conserved regions. often, seeds are located by an indexing method such as hash tables , suffix trees or suffix arrays. seeds are then extended in a fashion similar to the dynamic programming approach to form larger similar regions. many tools chain together sufficiently near seeds, and report statistically significant chains as homologues. a comprehensive review of genome wide alignment tools is presented in  <cit> .

most traditional alignment methods rely heavily on a scoring scheme that includes a substitution matrix, which describes the mutation rates between nucleotides or amino acids, and other parameters such as gap penalties. however, these methods lack a well-principled objective function to measure the performance of a set of parameters: "there is considerable disagreement among biologists about the 'right' choice of parameters"  <cit> . using a generic substitution matrix may be suitable for protein alignment as the rates of substitution in protein largely depend on the similarities between amino acid properties which are well understood. however, this is not the case in nucleotides; more than one codon can code for an amino acid and different strains show different codon preferences for a given amino acid  <cit> . it is therefore sometimes very hard to find a suitable scoring scheme for alignment of genomes, especially when little is known about the sequences. the selection of a scoring scheme would be managed easily with a reasonable objective function.

existing alignment algorithms consider sequence alignment as a variation of the edit distance problem, and perform alignment by matching characters of the two sequences. as a result, they are unable to deal with regions of low information content such as repetitive and statistically biased dna. such regions are often "masked out" before alignment  <cit> . since genomic sequences are meant to convey genetic information, a new alignment methodology that performs alignment based on the information content at each nucleotide position is proposed here. the methodology is based on information theory  <cit>  and the minimum message length  principle  <cit> . this approach considers regions that convey similar information as potential homologues. the similarity of regions can be measured by their mutual information content.

a number of information theoretic methods have been developed to compare biological sequences. the mml encoding method  <cit>  postulates that two sequences are related if compressing the two together results in a shorter code than the total code of compressing them separately. an extension of this information theoretic approach to alignment is modelling-alignment   <cit>  which incorporates population models into the alignment process and can thus estimate the information content of each nucleotide or amino acid in context, and can change matching, insertion and deletion scores accordingly. the method has been shown to significantly reduce false positives without introducing false negatives when applied to statistically biased data. however, the quadratic complexity of m-align prohibits applying it to long sequences.

this article presents xmaligner, a novel method for genomic local alignment based on information theory. as in  <cit> , our work is based on the premise that if two sequences are related, one sequence must tell something useful about the other: a predictive model can predict a sequence better if a related sequence is known. the information content of a sequence is measured by lossless compression. xmaligner makes use of the expert model compression algorithm  <cit>  for calculating the information content and mutual information content of the two sequences to be aligned. it does not require masking out of repetitive and low information regions. it has an objective function to help in selecting parameters for a good alignment. the method is shown to be practical and can handle sequences of eukaryote genome size.

method
information theory  <cit>  directly relates entropy to the transmission of a sequence under a compression model. suppose a sequence x is to be transmitted over a reliable channel where the objective is to minimise the transmitted message. the sender compresses x using a compression model and transmits the encoded message to the receiver, which decodes the compressed stream, using the same model, to recover the original message. the compression is performed by the best possible compression model. the amount of information contained in x, or the information content ℐ of x, is the amount of information actually transmitted across the channel, that is the length of the compressed message.

the transmission of x is illustrated in figure 1a. the sender uses a predictive model, which compresses each symbol of x by estimating the probability of the symbol based on observation of the preceding symbols; a good prediction results in a short code-word for the symbol. the information content of every symbol makes up the information sequence of x, which is shown in the plot below the diagram.

suppose a reference sequence y related to x is available to both parties. the sender can further reduce the transmitted message length by transmitting only the information in x that is not contained in y with the addition of references to the shared information contained in y. the receiver can recover x correctly because it also knows y. since the sender aims to send the shortest possible, recoverable message, the amount of information transmitted in this case should be no more, and probably less, than the amount of information transmitted without the reference sequence the amount of information transmitted in the presence of the reference sequence y is called the conditional information content of x given y , denoted ℐ. the sender is said to perform compression of x on the background of y. the reduction in compressed message length caused by the presence of the reference sequence is due to the shared information between the two sequences, and hence indicates the amount of mutual information of the two sequences. the mutual information of x and y is denoted as ℐ = ℐ-ℐ.

the transmission in the example above, but with a reference sequence, y , is illustrated in figure 1b. the predictive compression model now combines the information from all preceding symbols of x with the information from y to estimate the probability of each symbol of x. if x and y are truly related, the conditional information content of each symbol in x, given y , will, on average, be lower than its information content without y. the plot in the figure shows the sequence of information content of x, and the sequence of conditional information content of x given the reference sequence y. one can notice a region in x that has a related region in y - showing significantly lower conditional information content given y.

a local alignment of two sequences shows the mapping of similar regions in the two sequences and hence reveals the references to shared information contained in the sequences. the local alignment thus allows a reduction in transmission of a sequence in the presence of the other sequence as the reference sequence. this observation leads to the proposition that optimal alignment of two sequences leads to the best compression of one sequence on the background of the other. an alignment algorithm is developed based on the proposition. it uses a compression model, which makes use of a local alignment, to compress a sequence on the background of a reference sequence, and suggests the alignment that gives the best compression. the quality of an alignment can be measured by the compression.

the expert model
the alignment algorithm presented here is largely based on the expert model  compression model  <cit> . xm has been proved to be superior to other existing compression models thus giving the best estimate of the information content of sequences. in addition, its speed allows it to be applied to long sequences. importantly, the expert model allows the compression of a sequence on the background of another, and can show references to the areas where better compression is achieved. these references make up the local alignment of the two sequences.

xm is a predictive model which can be used for compression of genomic sequences as well as to measure the information content of a sequence. it compresses each symbol of a sequence x by forming the probability distribution for the symbol based on the information from all symbols seen previously. the actual symbol is then encoded with respect to the probability distribution. the information content of the symbol is the theoretical length of the encoding of the symbol: ℐ = -log2pr.

in order to form the probability distribution of a symbol, the algorithm maintains a set of experts, whose predictions of the symbol are combined into a single probability distribution. an expert is any model that can potentially provide a reasonable probability distribution for the symbol. with the availability of a reference sequence, the sender and the receiver can recruit experts that base their predictions on the reference sequence. expert opinions about the symbol are blended to give a combined prediction for the symbol. the reliability of an expert is evaluated from its past predictions. a reliable expert is given a high weight in the combination while an unreliable one has little influence on the prediction or may be even ignored.

type of experts
an expert can be anything that provides a reasonably good probability distribution for the symbol at a position in the sequence. a simple example is the order m markov expert which uses a markov model learnt from the statistics of all previous symbols to give the probability of the symbol in the context of m preceding symbols. initially, the markov expert does not have any prior knowledge of the sequence and thus gives a uniform distribution to a symbol. as the encoding proceeds, the markov expert gives the probability that a nucleotide appears in the next position as the frequency of its occurring previously. essentially, the markov expert provides the background statistical distribution of nucleotides over the sequence. different areas of a dna sequence may have differing functions and thus may have different probability distributions. to account for this, another type of expert called the local markov expert is employed. the local markov expert estimates the probability of a symbol based on the statistics from the local history rather than the entire history of the sequence.

in order to align two sequences x and y , the method attempts to compress sequence x  on the background knowledge of sequence y . it uses align experts each of which considers the region xn..xn+l in x to be aligned to a region ym..ym+l in y. an align expert estimates the probability of symbol xn+i  based on the corresponding symbol ym+i. it uses an adaptive code  <cit> , learned from its correct predictions and its mistakes in the region, to predict xn+i. two techniques are available for an align expert to learn its probability distribution for prediction. first, in the counting technique, each align expert keeps track of the number of correct and incorrect predictions, and gives the following probability to the letter at ym+i:

  pr=p=r+1w+ <dig> 

where w is the window size over which the expert reviews its performance and r is the number of correct predictions the expert has made; the remaining probability,  <dig> - p, is distributed evenly to the other letters of the alphabet. second, in the substituting technique, each align expert maintains a substitution matrix and give predictions according to the matrix.

if there is a mutation, the align expert gives a bad prediction at the position of the change, and its weight is decreased. however, subsequent correct predictions restore its influence in the combined prediction. on the other hand, when the homologous region ends, the align expert makes several mistaken predictions and its weight quickly decreases. when the weight of the expert drops to below a threshold, the expert is removed from the panel. this also happens when an insertion or a deletion occurs - the align expert is no longer able to make good predictions and is eventually excluded to make room for other align experts. though each align expert can only utilise a gap free matching region for prediction, many align experts collectively can handle larger regions that contain deletions and insertions.

proposing align experts
when a symbol of query sequence x is encoded, there are 2|y | possible align experts. this is too many to combine efficiently and anyway most are not genuine and thus would be ignored. to be efficient, the algorithm must use at most a small number of align experts at one time. the algorithm has a parameter l, which specifies the maximum number of align experts in use. when the expert panel size is less than l, the algorithm may recruit more potential align experts. since the number of experts must be small to be efficient, it is desirable that the experts proposed are those most likely to be genuine experts.

a simple method to propose potential experts is by using a hash table. the hash table associates every position in the reference sequence with the hash key composed of k symbols preceding the position. it proposes experts that suggest the current symbol is homologous to the symbols in positions in y having the same hash key. the choice of hash key size, k, and expert limit, l, is a trade-off between running time and compressibility, and hence alignment quality. generally, a small k and a large l allow xmaligner to search for repeats more thoroughly and thus give better compression at the cost of more time.

several techniques can be used to allow the hash table to propose align experts based on non-exact matching. there are two groups of nucleotides - purine  and pyrimidine . the biological properties of two nucleotides in a group are more similar than those from different groups. therefore, substitutions changing nucleotides in a group  are more common than those that change the group . in order to permit mismatches in seeds, xmaligner provides an option to use the hash table on the alphabet {purine, pyrimidine}. another technique is using gapped hash tables  <cit>  which allow selecting align experts based on matching with errors at specified positions in the hash key.

alternatively, a suffix tree or a suffix array can be used to propose align experts. these suffix structures allow selecting potential align experts based on the longest possible matching, especially for long sequences where random matches tend to be numerous. with a suffix structure, xmaligner can recruit up to l align experts from the l longest matches. suffix structures can also operate on the alphabet {purine, pyrimidine}, but cannot suggest align experts from matches with errors.

combining expert predictions
not only do experts adapt themselves based on the context of symbols they have seen, xmaligner also adaptively adjusts each expert's weight to reflect its accuracy in the given context. good experts are assigned high weights. even being nominated by the hash table, some align experts are just random matches and thus their predictions are not significantly better than the markov experts. the algorithm must be able to exclude the by-random nominees to reduce noise and to be more efficient. furthermore, a "genuine" align expert performs well only within a homologous region. beyond this, it provides random predictions and thus should also be excluded. it is important that the algorithm is able to evaluate the goodness of each expert to assign a weight accordingly, and to exclude the expert when necessary.

the core part of the expert model is the evaluation and combination of expert predictions. suppose at position n on the query sequence x, a panel of experts e is available to the compressor. expert θe gives the probability pr of symbol xn based on its observations of the preceding n -  <dig> symbols. the expert is assigned a weight wθe which reflects its reliability. the expert model performs a linear blending of experts' predictions to give the probability distribution of the symbol xn:

  pr=∑θe∈ewθepr 

in which the sum of all weights is equal to 1:

  ∑θe∈ewθe= <dig> 

a sensible way to combine experts' predictions is based on bayesian model averaging which sets an expert's weight to its posterior probability after encoding the previous n -  <dig> symbols.

  wθe=pr 

as has been shown in  <cit> , this posterior probability of θe is proportional to the product of its predictions of the n -  <dig> symbols. therefore

  wθe∝∏i=1n-1pr 

taking the negative log of the two sides in equation  <dig> gives

  -log2~-∑i=1n-1log2pr 

in other words, the negative logarithm of wθe varies linearly with the length of the encoded subsequence x <dig> .n- <dig> by expert θe. to evaluate experts on a recent history of size h, only the message length of encoding symbols xn-h..n- <dig> is used to determine the weights of experts. the final formula of wθe is

  wθe∝2-msglen 

if a symbol is part of a homologous region, the align expert of that region must predict significantly better than a markov expert. we therefore define a listen threshold, t, to determine the inclusion of an align expert. an align expert is considered reliable if the length of its encoding of the last h symbols is smaller that of the markov experts by t bits. an align expert is expected to be involved in prediction of a homologous region. beyond the region, its predictions becomes random and therefore its performance gets worse. if the align expert performance falls below the threshold, the expert is discarded to make way for others.

identifying similar regions
the main idea behind our alignment algorithm is that if two sequences are related, one will tell something new and useful about the other, that would not be known otherwise. if a region rx in the query sequence x has some biological relationship with some region ry in the reference sequence y , the similarity between rx and ry should be better than random. the align expert based on ry should perform better on rx than the markov experts whose predictions are based purely on the general statistics of sequence x. we therefore consider a region conserved if there is an align expert that predicts significantly better than the markov experts in the region due to the shared information between the region and a related region in the reference sequence. the amount of shared information, measured in bits, indicates the similarity of the two regions. the more information shared, the more similar they are. such a region is called a high-scoring segment pair .

the method identifies hsps by considering high performing align experts. each align expert is typically proposed by the hash table at some point in the query sequence during the compression process. it takes part in the compression until being discarded from the expert panel. the align expert assumes that the region it predicts is related to a region in the reference sequence, and bases its prediction on the assumption. the two regions form an hsp; the score is determined by the difference between the performance of the align expert and the markov experts.

this sub-section shows that the alignment score of an hsp  <cit>  is in fact the mutual information content of the pair. consider an align expert that aligns nucleotide xi in x to nucleotide yj in y. the alignment score is specified by the logarithm of the odds ratio of a model h which assumes the two nucleotides are homologous, and a model r assuming they are random:

  s=log2prpr 

since model r assumes that the occurrence of xi in x and yj in y are independent, the denominator of the right hand side can be expressed as pr = prpr. on the other hand, model h considers symbol xi to be related to symbol yj and hence pr = prpr by bayes's theorem. therefore,

  s=log2prprprpr=log2pr-log2pr 

pr is the probability of symbol xi estimated by the align expert upon observing yj while pr is the probability of xi estimated by the markov experts. s thus, is the mutual information of the two symbols. the alignment score of an hsp is the sum of alignment scores of all symbols in the regions. if the hsp is from two regions starting at xn and ym respectively and is l symbols long, its alignment score is

  s=∑i=0l-1-log2pr-∑i=0l-1-log2pr 

the two terms are the lengths of the compressed messages of the region xn..n+l- <dig> by the markov experts, and by the align expert, respectively. in other words, the alignment score of an hsp is the mutual information content of the two regions.

an hsp is considered a homologue if its alignment score is greater than a fraction of the information content of the region from the query sequence. specifically, xmaligner has a parameter homology ratio threshold r, and selects hsps having alignment scores

  s >r∑i=0l-1-log2pr 

as the local alignment.

once all the hsps have been selected, overlapping hsps and hsps having distances less than a certain threshold are chained together to form larger regions. more specifically, two hsps  and  where m <dig> <m <dig> are considered close if the distances between the end of hsp  and the beginning of hsp  in both sequences are less than a predefined gap. the alignment score of a chain is the sum of the alignment scores of all hsps involved. the alignment algorithm is formally described in algorithm  <dig> 

algorithm  <dig> expert model alignment algorithm

   xmaligner

   param l: limit on size of the expert panel e

   param k: size of the hash key

   param r: the ratio threshold to determine statistically significant hsps.

   param h: size of the window to evaluate experts

   param t: threshold to discard align experts

   use the hash table to index every position of the reference sequence

   e ← empty set

   for n ←  <dig> to |x| do

      while |e| <l do

         if expert θe which matches ym to xn is proposed then

            add θe into e

            set startx  ← n {the starting point of expert θe in query sequence x}

            set starty  ← m {the starting point of expert θe in reference sequence y }

         else

            break {no expert is proposed}

      end if

      end while

      set pr←∑θe∈ewθepr where wθe=2-msglen 

      msglen ← -log2pr

      for all θe ∈ e do

         msglen = -log2pr)

         update θe

         if msglen > msglen - t then

            remove θe from e

            set l ← n - startx 

            form an hsp that matches xstartx, l with ystarty, l.

            set score s←∑i-0l-1-log2pr-∑i=0l-1-log2pr

            if s>r∑i=0l-1-log2prthen

               add the hsp to a list

            end if

         end if

      end for

   end for

   chain sufficiently close hsps together

RESULTS
we ran experiments to compare the performance of xmaligner to several common genomic alignment algorithms. the criteria for selecting these algorithms was that  they can align long sequences, and  they are available to install on a workstation. the alignment algorithms selected for comparison included dialign  <cit> , chaos  <cit> , sim <dig>  <cit> , blastz  <cit>  and nucmer and promer in the mum-mer package  <cit> . experiments were run on a work station equipped with an intel dual core  <dig>  ghz cpu with  <dig> gb of memory. the machine ran linux ubuntu  <dig> .

we consider the use of genomic alignment tools in the context of identifying interesting regions in the genomes which in many cases are related to homologous regions  <cit> . we therefore evaluated the performance of each algorithm based on its ability to detect homologues. in statistics, sensitivity is defined as sn=tptp+fn and precision is defined as sp=tptp+fp, where tp is the number of true positives, fp is the number of false positives, and fn is the number of false negatives. what constitutes a true positive etc. depends on what question is asked. the literature takes two approaches:  does the method correctly identify that a segment of the query sequence is related to some segment or segments of the reference sequence?  does the method correctly identify the exact base in the reference sequence to which a base within a segment of the query sequence corresponds? clearly, both questions have their place.

we consider true positives  to be the number of homologous nucleotides that are correctly predicted as homologous , true negatives  to be the number of non-homologous nucleotides that are correctly predicted as non-homologous, false positives  to be the number of non-homologous nucleotides that are incorrectly predicted to be homologous, and false negatives  as the number of homologous nucleotides that are incorrectly predicted to be non-homologous. this definition corresponds to asking question  above.

in statistics, specificity is traditionally defined as tntn+fp. however, for alignments there are generally many fewer homologous regions, and thus homologous nucleotides, than non-homologous regions. so tn tends to be much higher than fp , making the traditional formula uninformative. consequently, the formula for sp is typically used for specificity in alignment applications  <cit> . these same definitions - sn and sp with respect to question 1) - have been used to compare tools for coding regions identification  <cit> . some work  <cit>  addressing question  above, define a quantity called alignment coverage; this happens to be equivalent to sn for question ! although this quantity does not necessarily account for the exact matching of nucleotides, it is expected to be "highly correlated with alignment sensitivity" for question  . in words, the definitions used herein are: sensitivity  is the fraction of homologous nucleotide sites covered by the alignments predicted; and specificity  is the fraction of homologous nucleotide sites predicted that are true homologues. where possible, the receiver operator characteristics  curve, plotting sensitivity against specificity, for each algorithm is presented.

simulated data
an evaluation of an alignment tool compares the homologues predicted by the tool against "true" homologues. true homologues in genomes, however, are not always reliable as they are often located by automated tools or by subjective prediction by human experts. some alignment benchmarks based on real data such as balibase  <cit>  and jareborg  <cit>  were designed based on manually curated alignments and structure protein information. these benchmarks are therefore only restricted to short sequences, and to homologues from protein coding regions. since some conserved regions are not necessarily protein coding, these benchmark data sets may cause alignment tools to report "wrong" false positives. simulated data benchmarks, such as those proposed in  <cit>  and  <cit> , are guaranteed to provide the true answers to alignment. the use of simulated data sets also allows exploring the entire spectrum of the problem space. these benchmarks, however, contain only short homologous sequences  and are only suitable for global alignment tools. they thus do not meet our goal of evaluating genome alignment tools.

we first experimented using simulated data. we generated our artificial genome benchmark data set in which homologous regions are scattered around the genomes in a random order. these homologous regions were taken from the alignment benchmark in  <cit>  for which the generation was inspired by non-coding regions from the drosophila genomes. we selected ten alignments at random from their  <dig> alignments. each alignment contains homologous sequences that were generated based on homologous non-coding regions of five species drosophila melanogaster, drosophila simulans, drosophila yakuba, drosophila ananassae and drosophila mojavensis. each sequence is  <dig> bases long. we generated five unrelated simulated genomes of length  <dig> kb, and inserted the ten homologous sequences of each species into a simulated genome at random positions. the generation resulted in five simulated genomes, each of which contains ten homologous regions.

we performed local pairwise alignment of the simulated genome containing d. melanogaster homologous sequences against each of the other four genomes. the object of the alignment was to locate the homologous sites from each genome. sites resulting from insertions were not considered homologous. the data set consists of four pairs of simulated genomes, namely, d. melanogaster - d. simulans, d. melanogaster - d. yakuba, d. melanogaster - d. ananassae and d. melanogaster - d. mojavensis in order of increasing genetic distance. in order to investigate different statistical distributions, we generated two sets with different statistical properties from these four pairs. in the first set, unrelated regions were generated from a uniform distribution . in the second set, unrelated regions were generated from a statistically biased distribution in which the frequencies of a, c, g and t are 40%, 10%, 10% and 40% respectively . in total, our benchmark contained eight pairs of simulated genomes.

the programs xmaligner, dialign  <cit> , chaos  <cit> , nucmer  <cit>  and blastz  <cit>  were applied to each pair of sequences. promer and sim <dig> were not included because they either perform alignment at the amino acid level, or rely on finding exon boundaries, whereas the data generated exhibit substitutions at the nucleotide level. furthermore, the simulated homologous regions are not actual coding regions and hence cannot sensibly be translated to protein. for each program used, we made an effort to choose the best possible parameters for a specific pair of sequences. we then varied one parameter to get different values of sensitivity and specificity of each algorithm. in particular, for chaos, we varied the score cut-off  and set the word length  to 10; for dialign, we varied the threshold ; for nucmer, we varied min cluster  and set minmatch  to  <dig> and maxgap  to 120; for blastz, we varied scoring threshold ; for xmaligner, we varied homology ratio threshold  and set the hash key size to  <dig>  all other parameters were set to their default values for dna alignment.

we performed an experiment to verify the proposition that the best alignment of two sequences leads to the best compression of a sequence on the background of the other. the experiment was performed on the four biased genome pairs. we first varied the parameters of the compression model, namely the hash table key size, the context length and the expert panel limit, so that different compression results could be obtained. the compression performance of each set of parameters is measured by the average compression of the simulated d. melanogaster genome in each pair. for each set of parameters, we varied the homology ratio threshold to obtain different sensitivity and specificity values. the roc curve for each set of model parameters is displayed in figure  <dig>  and is labelled by the compression result, in bit per symbol. the two configurations that produced the best compression results,  <dig>  bps and  <dig>  bps, also gave the best alignment performance. on the other hand, the configurations that produced the worst compression results  were inferior to other configurations set up in the experiment.

human-mouse data set
we also performed experiments on real data. we used the jareborg data set  <cit>  which contains  <dig> annotated pairs of genomic sequences from the mouse and human genomes. these sequences vary in length between  <dig> kilobases to  <dig> kilobases, with an average length of  <dig> kilobases. they contain  <dig> verified exon pairs. as exons are under stronger selective pressure, they tend to be more conserved than non-coding regions. the performance of an alignment algorithm is often evaluated by its ability to detect exons. indeed, the data set was used to evaluate alignment algorithms in several previous studies  <cit> .

for a pair from the data set, we applied each algorithm to align the mouse sequence against the human sequence, and compared the hsps detected in the mouse sequence to the annotated mouse exons. the parameters for xmaligner, chaos, dialign, nucmer and blastz were the same as in the previous experiment. for promer, we varied min cluster  and set minmatch  to  <dig> and maxgap to 30; for sim <dig>  we varied hsp threshold  and set word size  to  <dig>  the sensitivity versus specificity roc curves for these algorithms are plotted in figure  <dig>  in general, xmaligner was the most sensitive among the algorithms in the experiment. in particular, it outperformed blastz chaos and nucmer which also align sequences at the dna level. other methods, which either translate potential exons to proteins and perform alignment at the protein level  or use a built-in exon boundary detection mechanism, are more specific.

malaria parasite genomes
we used xmaligner to align the genomes of five plasmodium species, namely p. falciparum, p. knowlesi, p. vivax, p. gallinaceum and p. yoelii. the genome sequences and their annotations were obtained from plasmodb release  <dig>   <cit> . of the five species, p. falciparum and p. vivax are malaria parasites on human while p. knowlesi and p. yoelii cause malaria in monkey and rodent respectively. p. gallinaceum is a bird malaria parasite. the nucleotide compositions in these genomes are very different. the at content in the genome of p. falciparum is as high as 80% genome-wide, even 90% in introns and intergenic regions, while the at content in the p. vivax genome is just  <dig> %.

the genomes of plasmodium species exhibit an extremely difficult example of sequence alignment. the highly skewed distributions of genomes of species such as p. falciparum, especially in non-coding regions, may lead to the return of spurious matches. furthermore, in different stages of their life-cycle, plasmodium species interact with the mosquito vector and the vertebrate host. the strong evolutionary pressure from these interactions has resulted in different codon preferences among the genomes of plasmodium species. indeed, the at content of coding regions of p. falciparum is as high as 76% while the at content of coding regions of another human malaria parasite, p. vivax is only 53%, although the two species have similar metabolic pathways and their proteins share a high level of identity  <cit> .

we aligned each of the p. falciparum and p. knowlesi genomes against each of four other genomes and against the concatenation of these four genomes. the similar regions detected during alignment were compared with the exon annotation. we compared xmaligner with blastz  <cit> , promer and nucmer  <cit> , which are the only three among the chosen programs able to align such long sequences. blastz and nucmer align the sequences at the nucleotide level while promer translates potential exons to protein and aligns at the protein level. promer is generally used when the sequences are relatively divergent, which nucmer cannot handle. we varied the parameters scoring threshold  of blastz, minimum cluster  of nucmer and promer, and homology ratio threshold  of xmaligner to get several different values of sensitivity. other options are presented in table  <dig> and table  <dig> 

the alignment of one genome against another by xmaligner took about  <dig> minutes. to get high sensitivity, we performed alignment in both forward and reverse directions, and then combined both alignments. the total time for alignment of a pair of sequences therefore was about  <dig> minutes. the running time of promer was shorter, about  <dig> to  <dig> minutes for alignment one genome against another, and  <dig> minutes to align one genome against the four other genomes. nucmer is even faster, it needed only one minute for pairwise alignment and four minutes for aligning one against four genomes.

the sensitivity and specificity of exon detection of the three programs on the genomes of p. falciparum and p. knowlesi are shown in table  <dig> and table  <dig> respectively. a column with the header x/y shows the performance of aligning the genome of × against the genome of y and a column with header x/all shows the performance of aligning the genome of × against the other four genomes.

nucmer performed poorly on most cases, with the exception of aligning the p. knowlesi genome against p. vivax, these being closely related. in the alignment of distantly related genomes, nucmer obtained a sensitivity of no more than 20% in most cases. promer performed significantly better than nucmer on the data, although the matching techniques of the two algorithms are similar, except that promer performs alignment at the protein level while nucmer aligns at the nucleotide level. blastz performed better than nucmer, but was inferior to promer on aligning these sequences.

although xmaligner aligns sequences at the nucleotide level , it showed a much higher level of both sensitivity and specificity than promer in the alignment of most pairs. the only exception is the closely related pair p. knowlesi and p. vivax, where xmaligner was more sensitive but less specific. with such a close relationship, many regions other than exons also tend to be conserved. while promer translates dna to proteins for alignment, the annotation of just codons is clearly advantageous to promer's specificity.

visualisation of alignment
we have incorporated the output of xmaligner into infov toolkit  <cit>  for visualisation. when aligning a sequence x against a sequence y , xmaligner outputs the sequence of information content of x and the sequence of the conditional content of x given y , along with a list of hsps. the toolkit can read these information sequences, manipulate and display them. the annotation of the sequences can also be visualised by the toolkit.

in an earlier publication  <cit> , we performed an alignment experiment using xmaligner and infov toolkit. we downloaded the p. vivax and p. falciparum genomes from plasmodb version  <dig> . we applied xmaligner to align contig ctg <dig> from the p. vivax genome against the genomes of p. falciparum. the information content sequence of the contig and the conditional information content sequence of the contig given the p. falciparum genome were generated by xmaligner. the information content sequences were loaded into infov for viewing. the visualisation of these information sequences and the alignment is shown in figure  <dig>  the top canvas plots the two information sequences. the mutual information, obtained by taking the difference of the two information sequences, is plotted in the bottom canvas.

infov is able to display the annotations of a sequence and the hsps from an alignment. the two rows of red and blue boxes near the bottom of the viewer in figure  <dig> display the hsps from the alignment and the exon annotation of contig ctg <dig> from plasmodb version  <dig> . when a box is clicked, a pop up windows shows the relevant information of the hsp or of the annotation. users can zoom in and out to view particular areas of interest. figure  <dig> shows the view from position  <dig> to  <dig> of the contig.

during our experiment, we noticed a cluster of hsps which paired regions in contig ctg <dig> to some annotated coding regions in the genome of p. falciparum. these regions showed a high level of similarity but was not annotated in plasmodb  <dig>  version. the cluster of these region starts at position  <dig> in the ctg <dig>  and is about  <dig> bases long. its counterpart from the p. falciparum genome starts at position  <dig>  we tracked down and found that this area in the p. falciparum genome is a cluster of three genes mal7p <dig> , mal7p <dig>  and mal7p <dig> . the information of the alignment of an hsp is shown in figure  <dig>  the area was thought to be a synteny region conserved across malaria species, and contain some genes  <cit> . a later version of plasmodb  verified this finding and annotated the area as gene pvx  <dig> in the p. vivax genome.

discussion
most genomic alignment methods have four major components:  an indexing technique for locating seeds,  a method for extending seeds,  a method for assigning score to each local alignment, and  a method to evaluate the significance of an alignment. xmaligner presents novel technique for ,  and  while it can use any existing methods for  from conventional alignment approaches for to propose align experts. indeed, xmaligner provides option to use hash tables, gapped hash table, suffix trees and suffix arrays, on the standard alphabet  or on the {purine, pyrimidine} alphabet. other techniques will be implemented in the near future. most importantly, the suitability of each seeding technique can be measured by the compression objective function.

with reference to the traditional dynamic programming approach, an align expert proceeds diagonally. this is similar to gap-free extending seeds. however, there can be more than one align expert employed at any time. if there are gaps in a homologous region, some neighbouring expert would be proposed. though each align expert can suggest a gap-free hsp, the panel of experts in xmaligner can handle gaps implicitly. this also allows xmaligner not to make any assumptions about gap scores.

the matching scores in the traditional dynamic programming approach are calculated based on an information theory perspective  <cit> . indeed, an entry in the common substitution matrices such as pam  <cit>  and blosum  <cit>  represents the logarithm of the ratio of the probabilities of two hypotheses: the pair is homologous and the pair is random. these scores are calculated based on some pre-aligned data or under some evolutionary assumptions. these substitution matrices are therefore not suitable for alignment of sequences that have different properties to the data used to construct the matrices, such as sequences of biased composition. a previous attempt has been made to construct substitution matrices for such sequences by collecting pre-aligned sequences with similar composition statistics  <cit> . however, the suitability of the collected data and the reliability of the pre-alignment are called into question. we argue that it is desirable to estimate these probabilities from the sequences at hand. this calculation better reflects the information content of each symbol of the sequences to be aligned. these scores can even be estimated if the sequences are sufficiently long  <cit> .

equation  <dig> shows that the mutual information of an hsp is in fact the traditional alignment score of the hsp which is also measured by the logarithm of the odds ratio of the probability that two symbols are related and the probability that they are independent. however, xmaligner adaptively estimates these probabilities based on the context of the pair of symbols. for example, in a low information region, the information content of a more frequent symbol is lower and its alignment score is computed accordingly. unlike the "pairwise statistical significance" approach in  <cit>  which locally selects a scoring scheme from a pre-computed set, our approach estimates the scoring scheme directly from data. this mechanism of xmaligner also differs from other methodologies in dealing with biased composition data; for example in  <cit>  where the scoring scheme is derived from the standard substitution matrix by an heuristic transformation and in  <cit>  which estimates the statistical significance e-value from data. furthermore, each align expert also adaptively estimates mutation rates based on its observed data and keeps a separate scoring scheme. with the compression criterion, experts with good scoring scheme are retained while experts with unreasonable scoring scheme are discarded early. as a result, the new methodology performs better than traditional methods on statistically biased data, as demonstrated in the results section.

xmaligner might find multiple segments in the reference sequence that are strongly related to a similar segment in the query sequence. the degree of relatedness is specified by the conditional information content of the segment given each related segment on the reference sequence. this can be used as a ranking to guide further investigation of such an identified segment.

most existing alignment algorithms lack an objective function to indicate which parameters are the most suitable for the data. objective functions are very important for applications like sequence alignment because biological data are so diverse. it is very hard to anticipate which parameter values capture the essence of the data and will give the best results, especially for data that are not well studied. the objective function provided by xmaligner naturally guides parameter estimation and improves alignment quality.

CONCLUSIONS
this article presents xmaligner, a novel sequence alignment approach that matches long sequences at the information content level. it considers the information content of the nucleotide at each position during the alignment process. the information content is determined by examining the context of the nucleotide. unlike traditional alignment algorithms, xmaligner reports aligned regions from two sequences if there is significant shared information between the two regions. the approach is shown to outperform the conventional character-matching approaches, especially for distantly related sequences and sequences with statistically biased composition. the method is able to align eukaryote genomes with only a modest hardware requirement. the output from xmaligner can be integrated into a visualisation tool to aid the analysis of sequences.

we argue that, since genomic sequences are meant to carry information, aligning in terms of information content is a better approach for genomic sequence alignment. each nucleotide should be examined within its context. the approach is better suited than the conventional approaches which measure the alignment score of matching symbols entirely based on a fixed scoring scheme.

authors' contributions
mdc developed methods, performed experiments, analysed data and wrote the paper. mdc, tid and la contributed to the mathematics. tid and la supervised the work, and participated in discussions on algorithms, biology and statistics, and in the writing of the paper. all the authors read, edited and approved the final manuscript.

