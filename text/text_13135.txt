BACKGROUND
text mining helps biologists to collect disease-gene associations automatically from large volumes of biological literature. during the past decade, there was a surge of interests in automatic exploration of the biomedical literature, ranging from modest approaches such as annotating and extracting keywords from biomedical text to more ambitious attempts like natural language processing , text-based network construction and inference, and so on. these computational efforts effectively help biologists to identify the most likely disease candidates for further experimental validation. currently, the most important resource for biomedical text mining applications is the medline database developed by the national center for biotechnology information  at the national library of medicine . medline covers all aspects of biology, chemistry, and medicine, there is almost no limit to the types of information that may be recovered through careful and exhaustive mining  <cit> . to extract relevant information out of the immense amount of data, furthermore, to retrieve useful high-level knowledge from the information, text mining and machine learning have become indispensable tools in practical research.

nevertheless, the selection of controlled vocabularies and the representation schemes of terms occupy a central role in text mining and the efficiency of knowledge discovery varies greatly between different text mining models  <cit> . to address these challenges, we propose a multi-view text mining approach to retrieve information from different biomedical domain levels and combine it to identify disease relevant genes in prioritization and clustering. our notion of view is a text mining model specified by a controlled vocabulary , so the concept of multi-view text mining is featured with combining multiple controlled vocabularies to retrieve gene-centric perspectives from free text publications. it also implies that all the information is retrieved from the identical medline corpus but varies by the adopted vocabulary, so the term view denotes a domain-based perspective of the corpus. the idea of multi-view perspectives is well-known in modern mechanical design and drawing, where a mechanical component is illustrated as three or more views using the the applied geometry method developed by gaspard monge in the 1780s  <cit> . in the context of genomic data fusion, the idea of incorporating more views in analysis may be beneficial, by reducing the noise, as well as improving statistical significance and leveraging the interactions and correlations between the text mining models to obtain more refined and higher-level information  <cit> .

to retrieve multi-view information, we select multiple vocabularies on the basis of nine bio-ontologies . the text mining process  is similar to our earlier work investigating performance of a single text mining model in gene prioritization  <cit> . the novel aspect of the present approach is the combination of multiple text models in a joint framework of data fusion and dimensionality reduction. moreover, we extend the task from disease gene prioritization to clustering and propose several new algorithms for data fusion based clustering analysis. it also emphasizes our text mining approach as a general method, whose resulting high dimensional gene-by-term profiles are adoptable in a wide-range of computational tasks such as prioritization, clustering, classification analysis, and so on. furthermore, the gene-by-term profiles obtained in our approach can also be combined with biological data thus the discovery of disease associated genes is balanced between reliability and novelty.

beside the motivation of the multi-view approach, how to combine models for better performance with the multiplicity of machine learning methodologies still remains a challenge. some related work  <cit>  has proposed to incorporate text mining models using basic operators such as arithmetic average, max, min, union and so on. unfortunately, our preliminary experiment  <cit>  shows that these basic operators may work in some circumstances, but cannot guarantee the superiority of the combined model. to explore an effective methodology to integrate the multi-view data, we systematically compare the performance of several integration methods on real disease benchmark data. these integration methods not only consist of the basic operators mentioned before but also include some advanced machine learning techniques for data fusion, such as consensus functions, multiple kernel learning algorithms, and so on. moreover, to tackle the high dimensionality of text mining results, we introduce dimensionality reduction techniques in the previously developed data fusion framework. two different dimensionality reduction approaches are applied: latent semantic indexing  and vocabulary pruning with ontological structure. the former one is a well established method in information retrieval. the latter one is a specific method to select the indexed terms according to the hierarchical structure of bio-ontologies. beside the multi-view strategy, we also try some other alternative approaches: one approach is to merge different numbers of cvs as a union vocabulary for text mining; another approach is to further resolve the heterogeneities of terms in the union vocabulary by mapping them into unique concepts. the performance of these alternative approaches is also compared and the advantage of the proposed multi-view is clearly demonstrated.

we investigate two fundamental tasks in disease associated gene research: prioritization and clustering. both tasks have attracted lots of efforts in the literature, whereas their definitions and interpretations may vary by approach. their computational definitions are clarified in the method section of the present paper. genome-wide experimental methods to identify disease causing genes, such as linkage analysis and association studies, often produce large sets of candidate genes  <cit> . on one hand, computational gene prioritization methods rank the large amount of candidate disease genes according to their likeliness of being involved in a certain disease. on the other hand, clustering analysis explores the disease-gene associations by partitioning the genes based on the experimental findings described in the scientific literature. these two tasks basically share a similar assumption: in prioritization, the similarity among genes associated to the same disease is assumed to be higher than the similarity with random genes. in the case of multiple diseases, the problem can also be formulated as a clustering problem. the assumption is that the similarity of genes relevant to the same disease  is higher than the similarity of genes relevant to different diseases . thus, we expect these genes to demonstrate some "natural partitions" according to the type of diseases. therefore, we are able to evaluate the performance of prioritization task and the clustering task using the same disease benchmark data.

as mentioned, given multiple data sources, to obtain an effective combined model in the prioritization and the clustering tasks is still a non-trivial issue. the data for prioritization only contains positive samples and the clustering data are all unlabeled samples, so it is often hard to validate each individual model and select the best models for integration. furthermore, text mining is often used by biologists as an explorative method to gain first-hand knowledge about the associations of genes with disease, usually there is limited amount of prior knowledge for model evaluation. to tackle this difficulty, the integration methods proposed in this paper do not rely on evaluation of individual models. based on related work, we review and categorize several algorithms as two general strategies, ensemble learning and kernel fusion, to combine multiple models. the proposed methods are shown effective, also robust, when combining relevant and irrelevant models. the performance of combination is shown significantly better than the best individual model, moreover, is also comparable to the ideal performance obtained by combining best models only. to explain why the improvements take place, we present case studies to investigate the false positive genes in prioritization and the mis-classified genes in clustering.

methods
selection of controlled vocabularies from multiple bio-ontologies
we select vocabularies from nine bio-ontologies for text mining, among which five of them  have proven their merit in our earlier work of text based gene prioritization  <cit>  and text based cytogenetic bands mapping  <cit> . besides these five, we select four additional ontologies  because they are also frequently adopted in the identification of genetic diseases and signaling pathways, for instance, in the works of gaulton et al.  <cit> , bodenreider  <cit> , mao et al.  <cit> , smith et al.  <cit> , and melton et al.  <cit> . the nine bio-ontlogies are briefly introduced as follows.

the gene ontology
go  <cit>  provides consistent descriptions of gene and gene-product attributes in the form of three structured controlled vocabularies that each provide a specific angle of view . go is built and maintained with the explicit goal of applications in text mining and semantic matching in mind  <cit> . hence, it is an ideal source as domain-specific views in our approach. we extract all the terms in go  as the cv of go.

medical subject headings
mesh is a controlled vocabulary produced by nlm for indexing, cataloging, and searching biomedical and health-related information and documents. the descriptors or subject headings of mesh are arranged in a hierarchy. mesh covers a broad range of topics and its current version consists of  <dig> top level categories. though most of the articles in medline are already manually annotated with mesh terms, our text mining process does not rely on these annotations but indexes the medline repository automatically with the mesh descriptors .

online mendelian inheritance in man's morbid map
omim  <cit>  is a database that catalogues all the known diseases with genetic components. it contains available links between diseases and relevant genes in the human genome and provides references for further research and tools for genomic analysis of a catalogued gene. omim is composed of two mappings: the omim gene map, which presents the cytogenetic locations of genes that are described in omim; the omim morbid map, which is an alphabetical list of diseases described in omim and their corresponding cytogenetic locations. our approach retrieves the disease descriptions from the omim morbid map  as the cv.

london dysmorphology database
lddb is a database containing information over  <dig> dysmorphic and neurogenetic syndromes, which is initially developed to help experienced dysmorphologists to arrive at the correct diagnosis in difficult cases with multiple congenital anomalies  <cit> . information in the database is constantly updated and over  <dig> journals are regularly reviewed to ascertain appropriate reports. the london neurology database  is a database of genetic neurological disorders based on the same data structure and software as the lddb  <cit> . we extract the dysmorphology taxonomies from lndb  and select the vocabulary terms.

evoc
evoc  <cit>  is a set of vocabularies that unifies gene expression data by facilitating a link between the genome sequence and expression phenotype information. it was originally categorized as four orthogonal controlled vocabularies  and now extended into  <dig> orthogonal subsets subsuming the domain of human gene expression data. our approach selects the vocabulary from the evoc version  <dig> .

kegg orthology
ko is a part of the kegg suite  <cit>  of resources. kegg is known as a large pathway database and ko is developed to integrate pathway and genomic information in kegg. ko is structured as a directed acyclic graph  hierarchy of four flat levels  <cit> . the top level consists of the following five categories: metabolism, genetic information processing, environmental information processing, cellular processes and human diseases. the second level divides the five functional categories into finer sub-categories. the third level corresponds directly to the kegg pathways, and the fourth level consists of the leaf nodes, which are the functional terms. in literature, ko has been used as an alternative controlled vocabulary of go for automated annotation and pathway identification  <cit> . the ko based controlled vocabulary in our approach is selected on the version due to december  <dig> 

mammalian phenotype ontology
mpo  <cit>  contains annotations of mammalian phenotypes in the context of mutations, quantitative trait loci and strains which was initially used in mouse genome database and rat genome database to represent phenotypic data. because mouse is the premier model organism for the study of human biology and disease, in the caesar  <cit>  system, mpo has also been used as a controlled vocabulary for text mining based gene prioritization of human diseases. the mpo based controlled vocabulary in our approach is selected on the version due to december  <dig> 

systematized nomenclature of medicine-clinical terms
snomed is a huge and comprehensive clinical terminology, originally created by the college of american pathologists and, now owned, maintained, and distributed by the international health terminology standards development organization . snomed is a very "fine-grained" collection of descriptions about care and treatment of patients, covering areas like diseases, operations, treatments, drugs, and healthcare administration. snomed has been investigated as an ontological resource for biomedical text mining  <cit>  and also has been used in patient-based similarity metric construction  <cit> . we select the cv on the snomed  obtained from the unified medical language system  of nlm.

universal protein knowledgebase
uniprotkb  <cit>  is a repository for the collection of functional information on proteins with annotations developed by european bioinformatics institute . annotations in uniprotkb are manually created and combined with non-redundant protein sequence database, which brings together experimental results, computed features and scientific conclusions. mottaz et al.  <cit>  design a mapping procedure to link the uniprot human protein entries and corresponding omim entries to the mesh disease terminology. the vocabulary applied in our approach is selected on uniprot release  <dig>  .

the terms extracted from these bio-ontologies are stored as bag-of-words and preprocessed for text mining. the preprocessing includes transformation to lower case, segmentation of long phrases, and stemming. after preprocessing, these vocabularies are fed into a java program based on apache java lucene api to index the titles and abstracts of medline publications relevant to human genes.

vocabularies selected from subsets of ontologies
as mentioned, in some "fine-grained" bio-ontologies the concepts and terminologies are labeled in multiple hierarchies, denoted as sub-ontologies, to represent domain concepts at various levels of specificity. for instance, go consists of three sub-ontologies: biological process, cellular component and molecular function. mesh descriptors are arranged in  <dig> hierarchical trees at the top level. in snomed, the medical terminologies are composed of  <dig> higher level hierarchies. evoc ontology contains  <dig> orthogonal vocabulary subsets, whose terms contained are strictly non-overlapping. to investigate whether more specific vocabularies can improve the effectiveness of the text mining model, we select terms from the sub-ontologies of go, mesh, snomed, and evoc and compose the corresponding subset cvs. considering the main objective as disease-associated gene identification, only the most relevant sub-ontologies  are selected. other sub-ontologies either have very few terms or have no relation with the topic of disease gene identification so they are not selected. to distinguish the gene-by-term profiles obtained from subset cvs from those obtained from complete cvs, we denote the former ones as subset cv profiles and the latter ones as complete cv profiles.

merging and mapping of controlled vocabularies
the strategy of incorporating multiple cvs may be alternatively achieved by merging terms of several vocabularies together. to investigate this, we merge the terms of all  <dig> complete cvs as a union of vocabulary and denote the corresponding gene-by-term text mining result as "merge- <dig> profile". furthermore, we notice the lexical variants across multiple ontologies: a concept may be represented as different terms due to the diversities of professional expressions. for example, the mesh term "denticles" is expressed as "dental pulp stones" in omim and as "pulp stone" in snomed. to resolve these variants, we refer to the umls metathesaurus to map terminological variants as unified concepts. we download the concept names and sources file  from umls metathesaurus, which provides the mapping of atoms  to unified concepts. in text mining, we build a synonym vocabulary to map and aggregate the occurrences of various synonym terms as the occurrences of the unified concepts. the obtained results are gene-by-concept profile whose features are the unique and permanent concept identifiers defined by umls metathesaurus. among the nine vocabularies adopted in our approach, only four of them  are included in umls and resolved in the mrconso.rrf file. therefore, to fairly compare the effect of concept mapping, we also create "merge-4" gene-by-term profile using the union of the four vocabularies in indexing. then, we map the lexical variants as concepts and the result is denoted as "concept-4" profile. moreover, to create a naive baseline, we also index the medline corpus without using any controlled vocabulary. all the terms appeared in the corpus are segmented as single words and the results are expressed by these vast amount of words, denoted as "no-voc profile". we didn't consider the phrases of multiple words because there would be an immense number of combinations.

text mining
we create the gene-by-term profiles according to the mapping of genes and publications in entrez generif. a subset of medline literature repository  that consists of  <dig>  human gene-related publications is selected for text mining. in the first step all these  <dig>  medline documents are indexed and the doc-by-term  vectors are constructed. in the second step, we averagely combine the document-by-term  vectors as gene-by-term  vectors according to the generif mapping. the detail of the text mining process is presented in our earlier work  <cit> . table  <dig> lists all the cvs applied in our approach. table  <dig> illustrates the overlapping terms among the nine complete cvs.

the versions of bio-ontologies and medline repository adopted in the indexing process are mentioned in the text. the number of indexed terms of controlled vocabularies reported in this table are counted on indexing results of human related publications only so their numbers are smaller than those in our earlier work  <cit> , which were counted on all species appeared in generif. the number of terms in cv are counted on the vocabularies independently from the indexing process. the numbers of terms of merge- <dig>  merge- <dig> and concept- <dig> are counted on text mining results of all species occurring in generif.

the upper triangle matrix shows the numbers of overlapping terms among vocabularies independent from indexing. the lower triangle matrix shows the numbers of overlapping indexed terms. the second horizontal row  are the numbers of the terms in vocabularies independent from indexing. the second vertical column  are the numbers of the indexed terms.

preliminary result shows that the weighting scheme of terms also influences the performance of gene-by-term data in biological validation  <cit> . when the same vocabulary and the ranking algorithm are applied in prioritization, the idf representation generally outperforms the tf-idf and the binary representations. therefore, in this article all the term profiles are represented in the idf weighting scheme.

dimensionality reduction of gene-by-term data by latent semantic indexing
we have introduced the subset cvs method to reduce the number of terms expressing the genes. alternatively, we also apply lsi to reduce the number of term features. on the one hand, the information expressed on vast numbers of terms is mapped to a smaller number of latent factors so the irrelevant information is reduced. on the other hand, we expect that lsi does not compromise the information required for prioritization and clustering. in implementation, we use the matlab function svds to solve the eigenvalue problem of the sparse gene-by-term matrix of the whole human genome . to calculate the total variance on this huge matrix is very computational expensive, so we sort the eigenvalues obtained by the sparse eigenvalue decomposition. to determine the number of latent factors, we select the dimension where the corresponding smallest eigenvalue is less than  <dig> % of the sum of all eigenvalues.

algorithms and evaluation of gene prioritization task
the computational definition of gene prioritization is mentioned in our earlier work  <cit> . we briefly introduce it here for completeness. genes that are known relevant to the same disease are constructed as a disease-specific training set. a prioritization model is first built on this training set, then that model is used to rank a test set of candidate genes according to their similarity to the model. the performance is evaluated by checking the positions of the real relevant genes in the ranking of a test set. a perfect prioritization should rank the gene with the strongest causal link to the biomedical concept, represented by the training set, at the highest position . the interval between the real position of that gene and the top is similar to the error. for a prioritization model, minimizing this error is equal to improving the ranking position of the most relevant gene and in turn it reduces the number of irrelevant genes to be investigated in biological experimental validation. so a model with smaller error is more efficient and accurate to find disease relevant genes and that error is also used as a performance indicator for model comparison  <cit> . the ranking of candidate genes is usually based on scores. assuming a larger score represents a higher similarity towards the prioritization model, in benchmark study, one can label the real relevant genes as class "+1" and other irrelevant genes as class "-1" and plot the receiver operating characteristic  curves to compare different models by the values of area under curve . the error of prioritization is thus equivalent to  <dig> minus the auc value.

the methods to combine models for prioritization are roughly classified as two approaches: ensemble of rankings and fusion of sources .

ensemble ranking
in ensemble ranking, the prioritization is first carried on each individual model and then multiple ranking results are combined. since our main objective is to integrate the models, we use the same algorithm, standard correlation   <cit> , as the base method to obtain ranking results on individual models. using other algorithms, the results after model integration may not be significantly different and the computational complexity is more likely to be higher than the standard correlation algorithm.

the ranking results are integrated either as ranking orders, ranking ratios  or ranking scores. to compare them, we implement three integration algorithms. two of them are basic operators to calculate the average or maximal value of multiple ranking scores. the third one is based on order statistics to combine ranking orders, as implemented in the endeavour system  <cit> . the order statistics is formulated as follows: for each gene, a q statistics is calculated from all rank ratios using the joint cumulative distribution of an n-dimensional order statistic as previously done by stuart et al.  <cit> , given by   

where ri is the rank ratio for data source i, n is the number of data models. to reduce the complexity, aerts et al.  <cit>  implement a much faster alternative formula to approximate the integration, given by:   

where   

where ri is the rank ratio for data source i, rn-k+ <dig> is the rank ratio for data source n - k +  <dig> and v <dig> =  <dig> 

the q statistics for randomly and uniformly drawn rank ratios is found approximately distributed according to a beta distribution when n ≤  <dig>  and a gamma distribution for n >  <dig>  according to the cumulative distribution, we obtain p-value for the q value computed as   <cit>  for each gene. thus the original n rankings are combined into a ranking of p-values computed for each gene.

kernel fusion for prioritization
the kernel fusion method for gene prioritization is proposed by de bie et al.  <cit>  as a one-class svm  problem, where the kernels derived from multiple sources are combined in a weighted convex form, given by   

where kj is the j-th centered kernel matrix of training genes plus candidate genes, μj is the weighting coefficient on kj, rj is a regularization parameter controlling the complexity of kernels and is set as the trace value of kj and n is the number of data sources. the objective is to maximize the margin between the origin point and the separating hyperplane defined by the integrated model. the problem is solved as a quadratic constraint linear programming  problem, given by      

where ν is the regularization parameter penalizing part of the training data as outliers, m is the number of training genes, αi are the dual variables, gj is the kernel matrix of training genes taken from kj, t is the dummy variable for optimization. in this qclp formulation, the weighting coefficients μj are corresponding to the dual variables bounded by the constraints in  and their sum is equal to  <dig> 

we apply this 1-svm method to combine kernels derived from the multi-view data for gene prioritization. because the dimensionality of gene-by-term profile is high, we only use linear function to construct the kernel matrices. one of the main features of the 1-svm is the sparsity of its solutions, whose dominant coefficient may be assigned on one or two kernels. this property is useful to distinguish a small amount of relevant sources from a large number of irrelevant sources in data fusion. however, in biomedical applications, the data sources are usually preprocessed and have high relevances w.r.t. the problem. sparse solution may be too selective, in this case, to thoroughly combine the redundant and complementary information in these data sources. to balance the effect of sparse coefficients  and non-sparse coefficients in model generalization, we try  <dig> different values of the regularization parameter μmin to restrict the optimization process as a lower bound of coefficient assigned on each kernel. when μmin =  <dig>  there is no lower bound and the optimization procedure will probably result in the sparse coefficients. when μmin =  <dig> /n, each kernel is insured to have a minimum contribution in data fusion. when μmin = 1/n, the kernels are averagely combined.

after solving a and μj in , the function applied to prioritize candidate genes is given by   

where Ω is optimal combination of kernel defined in , kj are values of kj where x denotes a candidate gene and xi denotes the i-th training gene. the score obtained from  ranges from - <dig> to + <dig> and larger score represents stronger similarity towards the prioritization model. in our implementation, the qclp problem in  is solved by sedumi  <dig>   <cit> .

evaluation of prioritization
the prioritization result is evaluated by leave-one-out  method  <cit> . in each experiment, given a disease gene set which contains k genes, one gene, termed the "defector" gene, is deleted from the set of training genes and added to  <dig> randomly selected test genes . we use the remaining k -  <dig> genes  to build our prioritization model. then, we prioritize the test set which contains  <dig> genes by the trained model and determine the ranking of that defector gene in test data. the prioritization performance is thus measured by the error .

algorithms and evaluations of gene clustering task
as mentioned, the gene-disease associations can be alternatively investigated by segregating the genes into different groups. the within-grouping and between-grouping of genes give useful evidence about their functions and processes in the biological side. clustering analysis is a fundamental technique to gain this insight and it has been the workhorse for a wide range of applications, such as microarray expression data analysis, protein interaction network analysis, and many others. the clustering of disease relevant genes belongs to another paradigm than prioritization. firstly, the gene profiles are collected, appropriately preprocessed and poured into a representation. secondly, a distance measure should be chosen to quantify the relationship between genes, which is usually selected as euclidean distance or mahalanobis distance. thirdly, clustering algorithms are applied on the distance matrix of genes to segregate them into different partitions. fourthly, the assessment of cluster quality is done based on the data that generated the partitions  or using external information   <cit> . the first type of assessment is also known as the internal validation and the second type is called the external validation. the statistical assessment of the data fusion based clustering is still an ongoing issue because most of the internal validation indices  are data dependent, which makes it difficult to evaluate the clustering results among inconsistent indices. our approach mainly focuses on the biological assessment of clustering and the conceptual scheme of our clustering approach is illustrated in figure  <dig> 

analogous to the prioritization task, the methods of data fusion based clustering can also be categorized in two approaches: ensemble clustering and kernel fusion.

ensemble clustering
in ensemble clustering, we apply k-means clustering using euclidean distance as the "base clustering algorithm" on a single data source to obtain the partition; then we combine the multiple partitions as a consolidate partition via consensus functions. we also tried other candidate algorithms  and other distance measures , although we observe some discrepancies of performance on individual gene-by-term data, the difference after multi-view integration is not significant. in literature, various consensus functions have been proposed for ensemble clustering. we select  <dig> popular ones and compare them in our approach.

cspa, hgpa, and mcla
strehl and ghosh  <cit>  formulate the optimal consensus as the partition that shares the most information with the partitions to combine, as measured by the average normalized mutual information. they use three heuristic consensus algorithms based on graph partitioning, called cluster based similarity partition algorithm , hyper graph partitioning algorithm  and meta clustering algorithm  to obtain the combined partition.

qmi
topchy et al.  <cit>  formulate the combination of partitions as a categorical clustering problem. in their approach, a category utility function is adopted to evaluate the quality of a "median partition" as a summary of the ensemble. they prove that maximizing this category utility function implies the same clustering ensemble criterion as maximizing the generalized mutual information based on quadratic entropy . furthermore, the maximization of the category utility function is equivalent to the square error based clustering criterion when the number of clusters is fixed. the final consensus partition is obtained by applying the k-means algorithm on the feature space transformed by the category utility function.

eacal
fred and jain  <cit>  introduce the concept of evidence accumulation clustering  that maps the individual data partitions as a clustering ensemble by constructing a co-association matrix. the entries of the co-association matrix are interpreted as votes on the pairwise co-occurrences of objects, which is computed as the number of occurrences each pair of objects appears in the same cluster of an individual partition. then the final consensus partition is obtained by applying single linkage  and average linkage  algorithms on the co-association matrix. according to their experiments, average linkage performs better than single linkage so in this paper we apply evidence accumulation clustering with average linkage  as the representative algorithm for comparison.

adacvote
ayad and kamel  <cit>  propose an adaptive cumulative voting  method to compute an empirical probability distribution summarizing the ensemble. the goal of this ensemble is to minimize the average squared distance between the mapped partitions and the combined partition. the cumulative voting method seeks an adaptive reference partition and incrementally updates it by averaging other partitions to relax the dependence of the combined partition on the selected reference. in the adacvote they proposed, the partitions are combined in the decreasing order of their entropies.

kernel fusion for clustering
an alternative approach to combine multi-view data for clustering is achieved by fusing the similarity matrices  <cit> , as known as the kernel fusion approach. kernel fusion integrates data before clustering , whereas ensemble clustering aggregates partitions after clustering . we implement  <dig> kernel fusion algorithms and cross-compare their performance. in the present paper, the kernel matrices are all constructed by linear functions because the text mining data is in very high dimension.

hierarchical clustering
we average the kernels of multi-view data and transform it into a distance matrix in hilbert space, given by  <cit> :   

when the kernel mapping ϕ  is based on a linear function and x and z are data vectors normalized by the norm, dϕ  boils down to the euclidean distance between x and z. when ϕ  is based on nonlinear mapping , the distance dϕ  does not have direct interpretation in the original space of x and z. given the distance matrix, we can apply linkage methods  and obtain the hierarchical clustering result in hilbert space.

okkc
optimized data fusion for kernel k-means clustering  is proposed in our earlier work  <cit>  as a weighted kernel fusion algorithm for clustering. in okkc, the convex kernel combination is optimized for clustering by maximizing the between cluster mahalanobis distance in hilbert space. okkc consists of two iteration steps: kernel k-means clustering and kernel fusion. the clustering step obtains class labels of the data; the kernel fusion step optimizes the coefficient of kernels with the given labels. these two steps iterate until convergence. in our implementation, the clustering step is based on girolami's kernel k-means clustering algorithm  <cit> . the kernel fusion step is solved by a quadratic constraint quadratic programming  problem, given by   

where βj is the dual variable of the objective of clustering  discriminating the j-th class with other classes, λ is the regularization parameter smoothing the covariance matrix defined in the mahalanobis distance, lj is the vector of labels discriminating the j-th class with other classes, k is the number of classes, t is the dummy variable for optimization, ki is the centered kernel matrix of the i-th data source, ri is the trace of ki. analogue to the 1-svm algorithm in prioritization, we adjust the lowerbound of the coefficients assigned on different kernels via a regularization parameter μmin. two values are set for μmin:  <dig> and 1/n. when μmin = 1/n, the okkc algorithm is equivalent to the girolami's kernel k-means clustering applied on averagely combined kernels. the qcqp problem in  is solved by mosek toolbox  <cit> . the computational burden of qcqp problem can be much simplified as semi-infinite linear programming  formulation, which is inspired by the work of sonnenburg et al.  <cit>  and ye et al.  <cit> . the technical discussion of this simplification is irrelevant to the topic of this paper, so we use the conventional qcqp formulation here.

evaluation of clustering
as explained before, we assess the clustering performance biologically by labeled disease benchmark data. two external validations, rand index   <cit>  and normalized mutual information   <cit> , are applied and their definitions are given as follows. given a set of n genes x = {x <dig>  ..., xn } and two partitions to compare,  = {c <dig>  ..., cn} and   = {p <dig>  ..., pn}. in our problem,  and  are respectively the cluster indicators and the disease labels of the set of genes x. we refer that  a, the number of pairs of genes in x that are in the same set in  and in the same set in ;  b, the number of pairs of genes in x that are in different sets in  and in different sets in ;  c, the number of pairs of genes in x that are in the same set in  and in different sets in ;  d, the number of pairs of genes in x that are in different sets in  and in the same set in .

ri is defined as   

for binary class problem, the ri value ranges from  <dig> to  <dig> and the value of random partitions is  <dig> .

nmi is defined as   

where m is the mutual information between the indicators, e() and e() are the entropies of the indicators and the labels. for a balanced clustering problem, if the indicators and the labels are independent, the nmi value approaches  <dig> 

benchmark data set of disease genes
we validate the clustering results with the human disease benchmark data set of endeavour  <cit> , which consists of  <dig> relevant genes from  <dig> diseases. genes from the same disease are constructed as a disease-specific training set used to evaluate the prioritization and clustering performance. for prioritization, we perform  <dig> rankings  on  <dig> candidate human genes randomly selected from the human genomic. the prioritization is repeated  <dig> times  and the average error value is reported as the final result. for clustering, we enumerate all the paired combinations of the  <dig> diseases and result in  <dig> binary clustering tasks. in each paired combination, the overlapping genes  are removed for each clustering task. all the  <dig> tasks are repeated  <dig> times and the mean value of ri and nmi of all tasks in all repetitions is reported as the final result. the schema of the clustering evaluation is depicted in figure  <dig> 

RESULTS
multi-view performs better than single view
in prioritization and clustering, integration of multi-view data obtained significantly better performance than the best single view data. as presented in table  <dig>  the best performance in prioritization was obtained by combining  <dig> complete cv profiles as kernels in the lsi reduced dimensionality . this error was only half of the best single cv profile  without integration. in clustering, as shown in table  <dig>  three algorithms obtained significant performance. ward linkage applied on average combination of kernels showed the best performance . eacal  and okkc without regularization  also performed better than the best single view data .

the experiments are repeated  <dig> times on random candidate gene sets and the standard deviations are all smaller than  <dig> .

comparing the results of single view data, we found that the performance varies strongly by the choice of cv. the largest error in prioritization  was more than  <dig> times larger than the smallest one . the discrepancy on clustering was also significant. in short, single view text model was shown fragile w.r.t. the uncertainty, which is consistent with the result in our earlier work  <cit> .

the main finding of this approach was that combining multiple cvs in text mining improves the performance of disease gene prioritization and clustering. it is often practically difficult to predict the best candidate cv, thus the main advantage of our approach was its robustness w.r.t. to the uncertainty because the overall performance does not solely depend on any single cv and is more likely to be near-optimal.

combining lsi with data fusion improves prioritization performance
as discussed, we applied two different methods  to reduce the dimensionality of gene-by-term profiles. in prioritization, the effect of lsi on single view data is shown in figure  <dig>  lsi almost reduced the error of prioritization on all cvs . the best performance was obtained by mesh-lsi .

the comparison of subset cvs with the complete cv profiles is illustrated in figure  <dig>  for each cv, the three best subset cv profiles are shown. on snomed and mesh, the presented subset cv profiles outperformed the complete cv. however, subset cv profiles didn't work well on go and evoc. this is probably because snomed and mesh contain vast amounts of terms, so even when split into different subsets, many redundant terms still remain. the improvement may also be ascribed to the specific descriptions about diseases and phenotypes in the subset cvs . on the contrary, the number of terms in go and evoc is relatively small. in particular, the sub-ontologies of evoc are orthogonal with each other so there are fewer redundant terms. after splitting, the semantic effect preserved in the ontological descriptions is lost and the performance decreases. despite the variation of performance, the idea of splitting complete cv into subset cvs might be useful to handle textual data containing immense amount of term features. when the term number is huge, statistical feature selection techniques might not be reliable and efficient, therefore, to prune the terms with their underlying ontological structure is an efficient strategy.

our main interest was to integrate text models in the reduced dimensionality. we firstly combined the  <dig> lsi profiles, next combined all subset cv profiles. though some cv subset profiles were observed performing better than the complete cv, as the rationale of machine learning, we should not only select the best model by validation results. therefore, we integrated all the  <dig> subset cvs  with the  <dig> other complete cvs . result of these two approaches are compared in figure  <dig>  without lsi, the 1-svm data fusion reduced the error from  <dig>  to  <dig> . when coupling lsi with 1-svm, the error was further reduced from  <dig>  to  <dig> . considering the cost and effort of validating the false positive genes in lab experiments, the improvement from  <dig>  to  <dig>  is quite meaningful because it means that when prioritizing  <dig> candidate genes, our proposed method saves the effort of validating  <dig> false positive genes. our result is also comparable to the performance of the existing systems. in the endeavour system  <cit> , the same disease benchmark dataset and evaluation method are implemented whereas the main differences are two aspects: firstly, endeavour combines one gene-by-term data  with nine biological data sources and there is no dimensionality reduction applied. second, endeavour applies order statistics to integrate the rankings obtained from multiple data sources. the performance obtained in our approach is much better than endeavour . moreover, our result also improves over the work of de bie et al.  <cit> . in their approach, they use the same data sources as endeavour and apply the 1-svm for model integration . in our approach, the data fusion method and the disease gene benchmark data are exactly the same, the improvement is therefore attributed to multi-view strategy and the lsi dimensionality reduction.

dimensionality reduction of gene-by-term profiles for clustering
the same lsi profiles and subset cv profiles for prioritization are also used in clustering task. as shown in figure  <dig> and figure  <dig>  some lsi profiles were slightly better, others were slightly worse than the complete profiles. some subset cvs performed better than the complete cv. in particular, snomed situation and mesh diseases outperformed significantly the complete cvs.

analogue to the prioritization task, we integrated  <dig> complete cvs,  <dig> lsi profiles, and  <dig> subset cvs for clustering and evaluated the performance. as shown in table  <dig> and figure  <dig>  the best result was obtained by combing  <dig> complete cvs with ward linkage, okkc , and eacal. other comparing methods did not obtain better results than the best single cv.

for integration of  <dig> lsi and  <dig> subset cvs, only the best three results are shown.

multi-view approach is better than merging vocabularies
our multi-view approach is featured by the "integration after splitting" process. as discussed in the method section, an alternative approach is to merge the multiple cvs as a union cv for text mining. to evaluate this, we applied the merge- <dig>  merge- <dig>  novoc, and concept- <dig> profiles in the prioritization and clustering tasks. the roc curves of prioritization are illustrated in figure  <dig>  the evaluations of clustering are listed in table  <dig>  obviously, our "integration after splitting" strategy performed significantly better than the comparing methods.

the merge- <dig>  merge- <dig> and concept- <dig> profiles were clustered by k-means in  <dig> random repetitions and the mean values and deviations of evaluations are shown in the table. the novoc profile was only evaluated once by k-means because of the extremely high dimension and the computational burden.

effectiveness of multi-view demonstrated on various numbers of views
to further demonstrate the effectiveness of multi-view text mining, we evaluated the performance on various numbers of views. the number was increased from  <dig> to  <dig> and three different strategies were adopted to add the views. firstly, we simulated a random strategy by enumerating all the combinations of views from the number of  <dig> to  <dig>  the combinations of  <dig> out of  <dig> views is ,  <dig> out of  <dig> is , and so on. we calculated the average performance of all combinations for each number of views. in the second and the third experiment, the views were added by two different heuristic rules. we ranked the performance of the nine views: in prioritization, the order from high to low was lddb, evoc, mpo, go, mesh, snomed, omim, uniprot, and ko. in clustering, the order was lddb, omim, uniprot, evoc, mpo, mesh, snomed, go, and ko. the second strategy combined best views first and increases the number from  <dig> to  <dig>  in the third strategy, the irrelevant views were integrated first. the results obtained by these three strategies are presented in figure  <dig>  the performance of the random strategy increases steadily with the number of views involved in integration. in the best view first strategy, the performance increased and reached the ideal performance, then started to decrease when more irrelevant views are involved. the ideal performance of prioritization was obtained by combining the five best views  by the 1-svm method applied on averagely combined kernel. the generic integration method  did not perform well on high dimensional gene-by-term data. the ideal performance of clustering was obtained by combining the four best views  using the ward linkage method. the performance of integrating all cvs was comparable to the ideal performance, which shows that the proposed multi-view approach is quite robust to the irrelevant views. furthermore, the merit in practical explorative analysis is that the near-optimal result can be obtained without evaluating each individual model. in the third strategy, because the combination starts from the irrelevant views first, the performance was not comparable to the random or the ideal case. nevertheless, as shown in figure  <dig>  the performance of the multi-view approach was always better than the best single view involved in integration. collectively, this experiment clearly illustrated that the multi-view approach is a promising and reliable strategy for disease gene identification.

effectiveness of multi-view demonstrated on disease examples
to understand why the improvements took place when combining the multiple views, we presented two case studies. the first example was taken from the prioritization of mtm <dig>  a gene relevant to myopathy disease. in the disease benchmark data set, myopathy contains  <dig> relevant genes so we built the disease model using the other  <dig> genes and left mtm <dig> out with  <dig> random candidate genes for validation. to compare the rankings, only in this experiment, the  <dig> random candidate genes were kept identical in different views. in table  <dig>  we list the ranking positions of mtm <dig> and the "false positive genes" ranked before it. on lddb, three "false positive genes"  were ranked higher than mtm <dig>  to investigate the terms causing this, we sorted the terms by their correlation scores. the correlation scores were computed between the gene-by-term profile of the candidate gene and the prioritization model . the real relevant gene mtm <dig> was highly correlated on terms like "muscle, muscle weak, skeletal, hypotonia, growth, lipid", and so on. c3orf <dig> was ranked at the top because of the high correlation on terms such as "skeletal, muscle, heart", and so on. hdac <dig> was ranked second on terms like "muscle, heart, calcium, growth", and some others. ctnfr was ranked as the third due to terms such as "muscle, heart, muscle weak, growth", and so on. nevertheless, according to our knowledge none of these three genes  is actually known to cause any disease. escarceller et al.  <cit>  show that c3orf <dig> seems to be enhanced in heart and skeletal muscle, but there is no evidence about its relevance to the disease. hdac <dig> is found in the work of little et al. "as a specific downstream sbstrate of camkiideltab in cardiac cells and have broad applications for the signaling pathways leading to cardiac hypertrophy and heart failure"  <cit> . in the papers of glenisson et al.  <cit>  and cohen et al.  <cit> , hdac <dig> is found to have a role in muscle, which means it might be a good candidate but it has not been directly proved as a relevant gene. for cntfr, it has been found that in heterozygotic mice inactivation of the cntfr leads to a slight muscle weakness  <cit> . in the papers of roth et al.  <cit>  and de mars et al.  <cit> , cntfr is shown related to muscular strength in human. collectively, although there is no evidence that these  <dig> genes found by lddb are disease causing factors, the prioritization result was still meaningful because they have similar correlated terms as the real disease gene mtm <dig>  especially, hdac <dig> and cnfr seem to be nice candidates to muscular disorder. though lddb ranked  <dig> "false positive" genes higher than the real disease relevant gene, evoc and go ranked the real gene mtm <dig> as the top candidate. on evoc, the most important correlated terms were "muscle, sever, disorder", and so on. on go, the highly correlated terms were "muscle, mutation, family, sever", and others. in multi-view result obtained using the 1-svm , the ranking of lddb was complemented by evoc and go thus mtm <dig> was ranked as the top gene.

the second example was taken from the clustering task of genes relevant to breast cancer and muscular dystrophy, where each disease contains  <dig> non-overlapping genes, as shown in table  <dig>  we list the confusion tables and mis-partitioned genes of each single view in table  <dig>  as illustrated, single views all produced some mis-partitioned genes. in the multi-view approach , all the genes were correctly partitioned to the correct disease labels.

discussion
the merit of our approach lies in the conjunction of data fusion methods with dimensionality reduction techniques and its application on biomedical text mining to solve the gene prioritization and clustering problems.

the issue of model integration has already been investigated in several text mining applications. the notion of multi-view has been proposed by bickel and scheffer  <cit>  in web document clustering analysis to combine intrinsic view  and extrinsic view  of web pages. we refer the term multi-view to denote the gene-by-term profiles represented by different cvs. neveol et al.  <cit>  combine three different methods  to identify mesh main heading/subheading pairs from medical text. chun et al.  <cit>  develop an integrative system to extract disease-gene relations from medline. jimeno et al.  <cit>  combine three methods  to recognize disease names on a corpus of annotated sentences. gaulton et al.  <cit>  adopt  <dig> different ontologies and  <dig> data sources in the caesar system to annotate human genes as disease associated candidates. when annotating multiple data sources with different relevant terms from ontologies, each gene can get multiple scores of relevance with the input text query. caesar combines the scores using  <dig> basic methods: maximum, sum, normalized average, and a transformed score penalized by the number of genes annotated in a given data source. our approach differs from caesar by exploiting all the relevant medline abstracts so the gene-by-term profiles are retrieved from vast amounts of free-text information in literature. we have shown that these profiles can solve different text-based computational problems such as prioritization and clustering. yamakawa et al.  <cit>  combine  <dig> different sources  to create gene list annotated with go terms. then, a decomposition method  is applied to extract multiple aspects information from the target gene list, resulting in several bipartite graphs describing the relationships between small subset of genes and the go terms. their approach has the same motivation as ours, that is, to obtain more refined characteristics of genes separated in different aspects . they however haven't shown how multi-aspect gene annotations can improve the effectiveness of biomedical knowledge discovery.

in the past decade, many computational approaches have been developed to prioritize disease candidate genes using textual data  <cit> . to obtain thorough evidences for complex genetic diseases, many efforts focus on combining computational models learned from multiple data sources  <cit> . in particular, de bie et al.  <cit>  formulate gene prioritization as a novelty detection task and combine textual data with biological data in a kernel optimization framework. we used the same 1-svm method to integrate multi-view text data and further intervene data fusion with dimensionality reduction. we have shown that it leads to better performance in prioritization.

clustering by multiple  data sources is an ongoing topic with many interests. recently, wolf et al.  <cit>  have investigated the memory persistence  of bacteria by observing a strain of bacillus subtilis at different experimental conditions and developmental times. these multiple observations are then analyzed by clustering to quantify the mutual information the bacterium "remembers" at different stages. some other approaches address consensus clustering to combine multiple partitions generated on a single dataset, for instance, the analysis of microarray data by monti et al.  <cit>  and yu et al.  <cit> . asur et al.  <cit>  adopt consensus clustering methods to combine matrices generated by  <dig> different types of measurements  to cluster protein-protein interaction networks. lange and buhmann  <cit>  merge similarity matrices and phrase multi-source clustering as a non-negative matrix factorization problem. the kernel fusion problem for clustering is connected to many active works in machine learning and optimization, for instance, the framework of linear kernel fusion for binary supervised learning task proposed by lanckriet et al.  <cit>  and bach et al.  <cit>  and its extension to multi-classes problem proposed by ye et al.  <cit> . sonnenburg et al. simplify the computational burden of kernel fusion by semi-infinite programming   <cit> . on the basis of kernel fusion, chen et al.  <cit>  propose a clustering algorithm called nonlinear adaptive distance metric learning as an analogue of lanckriet et al.'s statistical framework for clustering. yu et al.  propose a clustering algorithm, okkc, for heterogeneous data fusion and combine text mining data and bibliometrics data to explore the structure mapping of journal sets  <cit> . in this paper, we systematically evaluate and compare  <dig> representative algorithms from two main approaches, ensemble clustering and kernel fusion, to combine the multi-view data. our experimental result shows that ward linkage, okkc, and eacal perform better than other methods. the number of disease genes in our benchmark data is imbalanced, which may partially affect the evaluation of clustering results .

the interpretation of text based prioritization is limited by lsi, whose latent factors cannot be easily attributed to the terms affecting the prioritization. when combining multi-view data by k-means and ensemble algorithms , to estimate the optimal cluster numbers is also difficult because the number of clusters is predefined. the statistical evaluations of clustering quality which is used to indicate the optimal cluster number on single data set are not always reliable for data fusion because they may differ in heterogeneous data sources. to circumvent this problem, one may relax the k-means clustering as a spectral clustering  <cit>  thus the optimal cluster number can be investigated from the eigenspectrum. to estimate the optimal cluster number in hierarchical clustering is easier, because it can be estimated by checking the dendrogram. another limitation in our clustering approach is the ignorance of overlapping genes despite of the fact that a gene may be biologically relevant to several topics . therefore, how to apply "soft clustering" techniques to obtain partitions containing overlapping genes will be the main topic of our future work. the notion of multi-view text mining has the potential of incorporating models varied by other parameter. for example, instead of using curated generif as the mapping of genes to publications, one can detect gene names expressed in the text automatically by natural language processing  and create new gene profiles according to this mapping. one can also retrieve the relationships of genes from literature, or refer to interaction networks and produce new view specified about relationships of genes. combining these views will undoubtedly lead to significant and thorough insight about the associations between diseases and genes.

CONCLUSIONS
we have presented the approach of combining multi-view text mining models to obtain precise identification of disease relevant genes. these views were specified by multiple controlled vocabularies derived from different bio-ontologies. using these vocabularies, we have indexed the medline titles and abstracts relevant to generif and have obtained a series of gene-by-term profiles. to demonstrate the effectiveness of our approach, we have combined these profiles and evaluated them on two fundamental problems: gene prioritization and clustering. experimental results have shown that the performance obtained on the multi-view approach is significantly better than the single-view data. nonetheless, the selection of the appropriate integration algorithm was nontrivial. we have cross-compared  <dig> algorithms in prioritization and  <dig> algorithms in clustering on a disease benchmark data set containing  <dig> diseases. in prioritization, the combination of the 1-svm with lsi performed the best; in clustering, the ward linkage applied on the uniform combination of kernels performed better than other methods.

second, we have integrated dimensionality reduction of individual data source in the data fusion framework. to tackle the very high dimensionality of text mining data, we have applied lsi, a popular reduction technique in information retrieval, on gene-by-term profiles. alternatively, we have also pruned the vocabularies according to the hierarchical structures of the bio-ontologies where they were derived. in this way, the gene-by-term profiles specified by a complete cv have been further separated as several subset cv profiles. in some experiments, the lsi and the subset cv profiles have obtained better performance than the complete cv.

third, we have substantiated the rationale of the proposed "integration after splitting" by comparing three other methods such as vocabulary integration, concept mapping, and no vocabulary indexing. experiments and validation results have clearly indicated that the proposed multi-view approach is a promising strategy.

authors' contributions
all authors conceived the project and design. sy performed text mining, programmed the algorithms, analyzed the data and wrote the paper. lct collected the disease benchmark dataset, programmed the order statistics method for prioritization model integration, and investigated the false positive genes in prioritization. bdm and ym are the promoters of sy and lct. all authors read and approved the manuscript.

supplementary material
additional file 1
discussion about the effect of class imbalance in clustering evaluation. extended discussion about the data and the result.

click here for file

 acknowledgements
the work was supported by research council kul: goa ambiorics, coe ef/05/ <dig> symbiosys, prometa, several phd/postdoc and fellow grants; fwo: phd/postdoc grants, projects g. <dig>  , g. <dig>  , g. <dig>  , g. <dig>  , g. <dig>  , g. <dig>  , research communities ; iwt: phd grants, gbou-mcknow-e, gbou-ana , tadbioscope-it, silicos; sbo-bioframe, sbo-moka, tbmendometriosis,tbm-iota <dig>  o&o-dsquare; belgian federal science policy office: iuap p6/ <dig>  ; eu-rtd: ernsi: european research network on system identification; fp6-noe biopattern; fp6-ip e-tumours, fp6-mc-est bioptrain, fp6-strep strokemap.
