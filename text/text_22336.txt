BACKGROUND
transcriptome data are fast becoming an important and cost effective component of phylogenetic studies  <cit> . the rapid fall in sequencing prices has contributed to the growing number of phylogenetic studies that integrate data from genomes and transcriptomes, often referred to as “phylogenomic” analyses. there is wide recognition that adding data from a larger number of genes is necessary to address many open phylogenetic questions, though of course additional gene sequences alone will not be sufficient to resolve them all  <cit> .

the primary impediments to wider adoption and further improvement of phylogenomic methods are the complexity of the analyses and the lack of integrated tools to conduct them. each phylogenomic study requires many steps, the vast majority of which concern matrix construction rather than phylogenetic analysis itself. these steps include raw data filtering, assembly, identification of ribosomal rna, selection of transcript splice variants, translation, identification of homologous sequences, identification of orthologous sequences, sequence alignment, phylogenetic analysis, and summary of results. implementing a phylogenomic analysis is not just a matter of executing available tools for each of these steps. among other challenges, results must be summarized across multiple steps, detailed records must be kept of all analysis steps, data files often need to be reformatted between analyses, and computational load must be balanced according to the available resources.

because phylogenomic studies are complex and have been manual or semi-automated, they are difficult to implement and explicitly describe, and require extensive technical effort to reproduce. these problems can make it difficult to evaluate results, integrate data across studies, expand analyses, or test the impact of alternative analysis approaches. in addition, manual analyses often include many subjective decisions that may impact the final results.

some higher-level pipelines have addressed subsets of phylogenomic analyses. these tools include partigene  <cit> , a pipeline to aid in the assembly and annotation of sanger transcriptome data collected across a diversity of species, and scafos  <cit> , a semi-automated tool for the curation of super matrices from previously assembled transcriptomes. no existing tool, however, can execute a full phylogenomic analysis of modern sequence data.

we addressed these needs by developing agalma, an automated phylogenomics workflow. using agalma, an investigator can conduct complete phylogenomic analyses, from raw sequence reads to preliminary phylogenetic trees, with a small number of high-level commands. the results are accompanied by detailed reports that integrate diagnostic information across data sets and analysis steps. in a first pass with agalma, the investigator conducts the analysis under default settings. the investigator then consults the reports to consider how best to optimize the analyses, and easily re-executes them with updated settings to produce new matrices and preliminary trees. the investigator can then analyze the optimized matrices with external phylogenetic inference tools not already included within agalma to explore other factors, such as model sensitivity.

implementation
we built agalma with biolite  <cit> , a generic bioinformatics pipeline framework. biolite profiles memory and cpu use, tracks provenance of all data and analyses, and centralizes diagnostic reporting. agalma is a modular workflow comprised of helper scripts and a series of pipelines. each pipeline is made up of stages that call internal analysis functions  and wrappers from the biolite python module. the wrappers invoke command-line tools, which include external bioinformatics tools such as the bowtie <dig> aligner  <cit>  and trinity assembler  <cit> , as well as several c++ tools from biolite. all calls to these external programs are logged, as are the version numbers of the programs and a hash of the binary so that it is possible to detect differences in compiler settings. this information about calls to external programs is available to the user via the agalma diagnostics command. we provide automated installation tools for agalma, biolite, and all required third party software. as a result, complete installation of agalma and all dependencies requires only a handful of commands on os x and ubuntu, and the scripts facilitate installation on many other unix compliant systems as well. installation details are provided in the readme and install files included with biolite and agalma.

all interactions with agalma are with the command agalma. the command agalma assemble, for example, provides access to the assemble pipeline. these commands include built-in help via the -h option, which provides details on all arguments and the parameters for external packages that are exposed to the user through agalma. the command agalma -h provides help for the top-level wrapper. the command agalma assemble -h, for example, provides help for the assemble command. additional details regarding the interface can be found in the readme file.

the first step for analyzing each data set, whether it consists of raw reads to be assembled or of previously assembled gene predictions, is to catalog the data. this creates a database entry that includes sample metadata and the paths to the data files. agalma has built-in support for transcriptome assembly of pair-end illumina data only in fastq format. when analyzing public data, raw reads and associated metadata can be imported directly from the ncbi sequence read archives  using the command sra import. this command downloads the reads for a given sra accession number , converts them into fastq format, and populates the catalog with the corresponding data paths and metadata. when previously assembled gene predictions are used, the assembly files in fasta format can be directly catalogued.

there are several distinct tasks subsequent to cataloging the data–sequence assembly, loading the genes into the database, and phylogenetic analysis. these tasks are described in detail in the readme and tutorial files provided with agalma, and are briefly summarized below.

assembly
the pipeline transcriptome runs an assembly from read filtering through assembly and preliminary annotation. in a typical analysis, transcriptome would be run once for each species for which raw illumina transcriptome data are available. the transcriptome pipeline executes the following sub-pipelines, which can also be run individually: 

•sanitize filters raw paired-end illumina data and randomizes the order of reads  to facilitate subsequent subsetting. reads are discarded if they do not meet a specified mean quality threshold, if they contain illumina adapter sequences, or if the base composition is highly skewed . this pipeline also generates fastqc  <cit>  summaries of read quality.

•insert_size uses subassemblies and bowtie <dig> mapping to estimate the mean and variance of the insert size . this information provides important feedback on the success of sample preparation, and is also used in some downstream analysis steps.

•remove_rrna removes most reads that are derived from ribosomal rna . removing rrna in advance of assembly can reduce both the number of chimeric transcripts and the time required for assembly. this pipeline first assembles multiple subsets of reads. a range of subset sizes is used since the optimal number of reads for assembling a particular rrna transcript depends upon multiple factors, including the fraction of reads that originate from rrna and the uniformity of coverage across the rrna transcripts . rrna transcripts are then identified by blast comparison of these subassemblies to an included dataset of known rrna sequences. the entire set of reads is then compared to the rrna transcripts that are identified from the subassemblies, and any reads that bowtie <dig> maps to them are removed. a plot of the distribution of reads across exemplar rrna transcripts is shown to help evaluate rrna assembly success. the top hit in the ncbi nt database is also provided as an independent check on sample identity and to help spot potential contaminants. the fraction of reads that derive from rrna is also reported to aid in improving library preparations.

•assemble filters the reads at a higher quality threshold and assembles them. assemblies can be conducted under multiple protocols . this pipeline can also assemble multiple subsets of different numbers of reads, which provides perspective on how sequencing effort impacts assembly results. the default assembler is trinity  <cit> . the wrapper we have included in agalma for running trinity makes two improvements over the wrapper script that comes with trinity. first, we have added a filter in between the chrysalis and butterfly stages to remove components that are smaller than the minimum transcript length parameter passed to butterfly, since running butterfly on these components will not yield a transcript. for the five assemblies in our test data set, this reduces the number of butterfly commands from roughly  <dig>  to  <dig> . second, we have replaced the parafly utility that is used for concurrent execution of the butterfly commands with the gnu parallel tool  <cit>  because it has better parallel efficiency. parafly executes the commands concurrently, but in blocks, so that the time to execute a block is the runtime of the slowest individual command. the runtimes can vary greatly because of variance in transcript length and complexity. in contrast, parallel load balances the commands across the available processors.

•postassemble uses blastn against the rrna reference sequences to identify rrna transcripts  and screens against the ncbi univec database to identify vector contaminants . the longest likely coding region per transcripts is identified and annotated using blastp against the ncbi swissprot database. reads are mapped to the assembly and all transcripts quantified using rsem  <cit> , and the splice variant with the highest expression is selected as the exemplar transcript for each gene for downstream analyses. it also produces a coverage map that helps assess the distribution of sequencing effort across genes. finally, it uses blastx to compare all the transcripts against the ncbi swissprot database to establish which are similar to previously known proteins.

following these steps, the investigator can inspect the assembled data directly or load them into the agalma database to prepare them for phylogenetic analysis.

load genes into the local agalma database
subsequent phylogenetic analyses require that all gene sequences to be considered are loaded into the local agalma database. the load command takes care of this process. in a typical analysis, load is executed once for each dataset that has been assembled by the transcriptome pipeline described above, and once for each set of gene predictions from external sources .

phylogenetic analysis
once assemblies for multiple species are loaded into the local agalma database, the user carries out a phylogenomic analysis by consecutively executing the following pipelines: 

•homologize allows the user to specify which datasets to include in a particular phylogenetic analysis. it then uses an all-by-all tblastx search to build a graph with edges representing hits above a stringent threshold, and breaks the graph into connected components corresponding to clusters of homologous sequences with the markov clustering algorithm  tool  <cit> .

•multalign applies sampling and length filters to each cluster of homologous sequences, and uses another all-by-all tblastx within each cluster to trim sequence ends that have no similarity to other sequences in the cluster . the sequences of each cluster are aligned using macse  <cit> , a translation-aware multiple-sequence aligner that accounts for frameshifts and stop codons. multiple sequence alignments are then cleaned with gblocks  <cit> . optionally, the alignments can be concatenated together to form a supermatrix.

•genetree uses raxml  <cit>  to build a maximum likelihood phylogenetic tree for each cluster of homologous sequences. gene trees can be filtered according to mean bootstrap support, which eliminates genes that have little phylogenetic signal  <cit>  and reduces overall computational burden. this filter can be applied prior to running treeprune , which has the added advantage of restricting ortholog selection to only well-supported gene trees. all options available in raxml can be passed as optional arguments. if the input is a supermatrix consisting of concatenated orthologs, it builds a species tree.

•treeprune identifies orthologs according to the topology of gene phylogenies, using a new implementation of the method introduced in a previous phylogenomic study  <cit> . it uses dendropy  <cit>  to prune each gene tree into maximally inclusive subtrees with no more than one sequence per taxon. each of these subtrees is considered as a set of putative orthologs. the pruned subtrees are re-entered as clusters into agalma’s database.

after treeprune, the user can build a supermatrix and a preliminary maximum likelihood species tree with raxml. these steps, which include rerunning multalign and genetree on the orthologs, are detailed in the agalma tutorial file. agalma produces a partitions file that describes which regions of the supermatrix correspond to which genes. the user can then proceed with more extensive phylogenetic analyses of the supermatrix using external phylogenetic inference software of their choice . as the alignments for each gene are also provided, the investigator can also apply promising new approaches that simultaneously infer gene trees and species trees  <cit> .

test data and analyses
a small set of test data is provided with agalma. it consists of  <dig>  100bp illumina read pairs for the siphonophore hippopodius hippopus, a subset of 72- <dig> gene sequences assembled for each of five siphonophores, and a subset of  <dig> gene predictions from the nematostella vectensis genome assembly. these data were chosen because they run relatively quickly and enable testing of most commonly used features.

these test data serve several purposes. they allow a user to validate that agalma is working correctly, and users are strongly encouraged to run this test with the command agalma test right after installation. this test analysis takes about  <dig> minutes on a 2-core  <dig> ghz computer. the test data also serve as the foundation for the example analysis described in the tutorial file. for the developer, the agalma test command serves as a regression test to check if changes break existing features. we routinely run this test in the course of adding and refactoring code. the test data also serve as a minimal case study for developing new features without needing to first download and curate data.

RESULTS
in addition to the small test data sets included with agalma, here we present an example analysis of larger data sets from seven species. though most phylogenetic analyses would include more taxa than this simple example case, the size of the dataset for each species is typical for contemporary phylogenomic analyses. this seven-taxon data set consists of new raw illumina reads for five species of siphonophores, abylopsis tetragona, agalma elegans, nanomia bijuga. physalia physalis, craseoa sp., and gene predictions for two outgroup taxa, nematostella vectensis <cit>  and hydra magnipapillata <cit> , produced by previous genome projects. mrna for the five siphonophore samples was isolated with dynabeads mrna direct  and prepared for sequencing with the truseq rna sample prep kit . the sample preparation was modified by including a size selection step  prior to pcr amplification. analyses were conducted with agalma at git commit dc549d <dig> and biolite at git commit 025fe65e .

we deposited the new data in a public archive  prior to running the final analysis. a git repository of the scripts we used to execute the example analyses is available at https://bitbucket.org/caseywdunn/dunnhowisonzapata <dig>  these scripts download the data from the public archives, execute the analyses, and generate the analysis reports. all of the figures presented here are taken from the analysis reports generated by agalma. this illustrates how a fully reproducible and open phylogenomic analysis can be implemented and communicated with agalma. these scripts can be used as they are to repeat the analyses. they could also be modified to try alternative analysis strategies on these same data, or they could be adapted to run similar analyses on different data.

assembly
the tabular assembly report  summarizes assembly statistics across samples, and links to more detailed assembly reports for each sample. for the example analysis, this summary indicates, among other things, that the fraction of rrna in each library ranged from  <dig> % to  <dig> % and the insert sizes were on average  <dig> bp long. the detailed assembly reports have extensive diagnostics that pertain to sample quality and the success of library preparation. as an example, figure  <dig> shows several of the plots from the detailed assembly report for agalma elegans, the siphonophore after which our tool is named. the distribution of sequencing effort across transcripts  and the size distribution of transcripts  are typical for de novo illumina transcriptome assemblies.

phylogenetic analyses
the agalma phylogeny report includes a plot of the number of sequences considered at each stage of the analysis . in the example analysis, the step that removed the most sequences was the first step of homology evaluation in the homologize pipeline . this reduction is due to low similarity to other sequences and clusters with poor taxon sampling. the next major reduction occurred in cluster refinement in multalign. this reduction is largely due to the elimination of clusters that failed the taxon sampling criteria, and reflects uncertainty regarding the homology of some sequences and sparse sampling of some homologs. the next major reduction in the number of genes occurred in treeprune. these reductions are due to both uncertainty regarding orthology and poor sampling of some ortholog groups. the preliminary species tree for the example analysis  is congruent with previous analyses of siphonophore relationships  <cit> .

resource utilization
phylogenomic analyses are computationally intensive. detailed information about resource utilization helps investigators plan resources for projects and balance computational load more efficiently. it is also critical for the optimization of the analyses, and can help guide design decisions. for each analysis, agalma produces a resource utilization plot that displays the time and maximum memory used by external executables . the peak memory use, and the longest-running step, in the transcriptome pipeline was the call to trinity during the assemble stage.

CONCLUSIONS
a distinction is sometimes drawn between manual approaches that enable close user inspection of data and results, and automated approaches that isolate the user from their results. this is a false dichotomy–automating analyses and examining the results closely are not mutually exclusive. automated analyses with detailed diagnostics provide the best of both worlds. the user has a very detailed perspective on their analysis, and the added efficiencies of automation leave the investigator with far more time to assess these results. automation also allow improvements made in the context of one study to be applied to other studies much more effectively.

for a study to be fully reproducible, both the data and the analysis must be described explicitly and unambiguously. the best description of an analysis is the code that was used to execute the analysis. by automating phylogenomic analyses from data download through matrix construction and preliminary phylogenetic trees, agalma enables fully reproducible phylogenomic studies. this will allow reviewers and readers to reproduce an entire analysis exactly as it was run by the authors, without needing to re-curate the same dataset or rewrite the analysis code.

there are alternative approaches to many of the steps in a phylogenomic analysis presented here. there are, for example, multiple tools that identify orthologs according to different methods and criteria  <cit> . agalma is a general framework and can be expanded to include these additional methods, and directly compare them in the context of a complete workflow that is consistent in all other regards.

availability and requirements
project name: agalma

project home page:https://bitbucket.org/caseywdunn/agalma

operating system: linux, mac os x

programming language: python

other requirements: biolite, and the dependencies listed in the biolite install file

license: gnu

competing interests
the authors declare that they have no competing interests.

author’s contributions
cwd and mh conceived of the original design of agalma, and together with fz made most design decisions. mh, cwd, and fz implemented and tested the software and wrote the manuscript. mh did most of the coding. mh and fz conducted the computational analyses. all authors read and approved the final manuscript.

supplementary material
additional file 1
html report for assembly of the sample data sets. the html report for the assembly of the test data sets from raw reads. the tabular report  provides an overview across the five assemblies for the ingroup taxa, and includes links  to detailed reports for the assembly of each species. fasta files for the annotated transcripts have been removed from the report to reduce file size.

click here for file

 additional file 2
html report for phylogenetic analyses. the html report for the phylogenetic analysis of the sample data.

click here for file

 acknowledgements
steve haddock provided several of the specimens, and freya goetz prepared all material for sequencing. thanks to deb goodwin and the students aboard ssv robert c. seamans for collecting the physalia physalis specimens, and to erik zettler and the sea education association for facilitating this opportunity. thanks also to christian sardet for assisting with collecting the hippopodius hippopus. freya goetz prepared the two siphonophore samples for sequencing. stephen smith and nicholas sinnott-armstrong made important contributions to early planning of agalma. stefan siebert assisted with curation of the test dataset and provided suggestions on features. richard nguyen provided html and css styling for the reports. thanks to steve haddock, warren francis, lingsheng dong, adrian reich, vanessa gonzalez, and other early-adopters for feedback on previous versions of agalma. rebecca helm provided helpful comments on an earlier version of this manuscript. this research was conducted using computational resources and services at the center for computation and visualization, brown university. agalma was developed with the support of the us national science foundation .
