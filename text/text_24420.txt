BACKGROUND
retrieving a gene regulatory network from experimental measurements and biological prior knowledge is a central issue in computational biology. the dna micro-array technique allows to measure expression levels of hundreds of genes in parallel, and many approaches to identify network structure from micro-array experiments have been proposed. models include dynamical systems based on ordinary differential equations   <cit> , bayesian networks  <cit> , or boolean networks  <cit> . we focus on the ode setting, where one or few expression levels are perturbed by external means, such as rna interference  <cit> , gene toggle switches   <cit> , or using diploid heterozygotes, and the network structure is inferred from changes in the system response. so far only few studies investigate the possibility of designing experiments actively. in an active setting, experimental design is used to choose an order of perturbations  such that maximum novel information about the underlying network is obtained in each experiment. multi-gene perturbations are becoming increasingly popular, yielding more informative data, and automated data-driven design technologies are required to deal with the combinatorial number of choices which can be opaque even for a human expert.

identifying  ode systems from observations and experimental design are well developed within the control community  <cit> . however, in the systems biology context, only very few measurements are available compared to the dimension of the system , and experiments leading to such observations are severely restricted. biological measurements are noisy, and time resolution is low, so that in practice only steady states of a system may be accurately measurable. on the other hand, there are no real-time requirements in biological control applications, and more advanced models and analysis can be used. a large body of biological knowledge can be used to counter the small number of observations, for example by specifying a prior distribution within a bayesian method. the standard system identification and experimental design solutions of control theory may therefore not be well-suited for biology.

we propose a full bayesian framework for network recovery and optimal experimental design. given many observed genes and rather few noisy measurements, the recovery problem is highly under-determined, and a prior distribution encoding biological knowledge about the connectivity matrix does have a large impact. one of the key assumptions is network sparsity, which holds true for all known regulatory networks. we adopt the linear model frequently used in the ode setting  <cit> , but use a sparsity-enforcing prior on the network matrix. the sparse linear model is the basis of the lasso  <cit> , previously applied to the gene network problem in  <cit> . however, they simply estimate the single network maximizing the posterior probability from passively acquired data, and do not address experimental design. we closely approximate the bayesian posterior distribution over connectivity matrices, allowing us to compute established design criteria such as the information gain, which cannot be done using maximum a posteriori  estimation. the posterior distribution cannot be computed in closed form, and obtaining an accurate approximation efficiently is challenging. we apply a novel variant of the recent expectation propagation algorithm towards this end.

many other approaches for sparse network recovery have been proposed. in  <cit> , the space of possible networks  is scanned for the sparsest solution. a sparse bayesian model is proposed in  <cit> , see also  <cit> . while there is some work on experimental design for boolean networks  <cit>  and bayesian causal networks  <cit> , none of the above mentioned methods have been used towards this goal. experimental design remains fairly unexplored in the sparse ode setting, with the notable exception of  <cit> . we compare our approach to theirs, finding our method to perform recovery with significantly less experiments and running much faster. our method is more robust to observation noise frequently present for biological experiments, and somewhat more transparent and in line with statistical practice. finally, their method consists of a combinatorial search and is therefore only applicable to networks with uniformly small in-degree, an assumption invalid for many known regulatory networks, e.g.  <cit> .

RESULTS
algorithm
our model
we start with the common linearized ode model: expression levels x ∈ ℝn of n measured genes at time t are modeled by the stochastic dynamical system

 dx = f)dt - udt + dw. 

here, f: ℝn → ℝn describes the non-linear system dynamics, u is a user-applied disturbance, and dw  is white noise. with u ≡  <dig>  we assume that the system settles in a steady state, and we linearize the system around that point. in this setting, a perturbation experiment consists of applying a constant u ≡ u, then measuring the difference x between new and undisturbed steady state. under the linearity assumption, we have that

 u = ax + ε, 

where a is the system matrix with entries aij, the non-zero aij describing the gene regulatory network. the noise εis assumed to be i.i.d. gaussian with variance σ <dig>  we focus on steady state differences, as in  <cit> . time course measurements are modelled linearly in  <cit> , and our method can easily be formulated in their setup as well. we assume that the disturbances u do not drive the system out of the linearity region around the unperturbed steady state. while this seems a fairly strong assumption, our simulation experiments show that effective network recovery is possible even if it is partly violated.

our contribution to this standard linear regression formulation is a bayesian model, incorporating prior information about a, namely its sparsity. the unknown matrix a is inferred via a posterior distribution, rather than merely estimated, allowing us to perform experimental design within a statistically optimal framework.

observations are denoted x = t, u = t, and the bayesian posterior is

 p ∝ pp, 

where the likelihood is p=∏j=1mn, owing to .

note that typically m < n, certainly in early stages of experimental design, and u = xa has no unique solution. in this situation, the encoding of knowledge in the prior p is of large importance. true biological networks are known to be sparsely connected, so we would expect sparse network matrices a. the prior should force as many entries of a close to zero as possible, at the expense of allowing for fairly large values of a few components. it should be a sparsity prior.

we employ a laplace prior distribution

 p=∏i,jp,p=τ2e−τ|aij|. 

it is instructive to compare the laplace against the gaussian distribution, which is commonly used as prior in the linear model. the laplace puts much more weight close to zero than the gaussian, while still having higher probabilities for large values. the implications are depicted in figure  <dig>  see also  <cit> . in fact, the gaussian prior is used with the linear model mostly for convenience, since the posterior is gaussian again and can be computed easily  <cit> . even within our framework, computations with a gaussian prior are significantly more efficient than with a laplace. however, our results prove that theoretical arguments in favour of the laplace prior do have real practical weight, in that the computational advantages with the gaussian are paid for by a much worse predictive accuracy, and identification needs significantly more measurements than for the laplace.

the bi-separation characteristic of the laplace prior into few large and many small parameters  is embodied even more strongly in other sparsity priors, such as "spike-and-slab" , student-t, or distributions based on α-norms, ‖x‖αα=∑i|xi|α, with α <  <dig>  see also figure  <dig>  however, among these only the laplace distribution is log-concave, i.e. has a log-concave density function, leading to a posterior whose log density is a concave function, thus has a single local maximum. this simplifies accurate inference computations significantly. for a non-log-concave prior, posteriors are usually multi-modal, spreading their mass among many isolated bumps, and the inference problem is in general at least as hard as the combinatorial problem of testing all possible sparse graphs. for such posteriors, all known methods for approximate bayesian inference tend to either perform poorly or require an excessive amount of time. furthermore, they tend to be algorithmically unstable, and the approximation quality is hard to assess. robustness of the inference approximation is important for experimental design, since decisions should not be based on numerical instability artefacts of the method, but on the data alone. these points motivate our choice of a laplace sparsity prior.

note that the laplace prior does not imply any strict constraints on the graph structure, i.e. the sparsity pattern of a, in contrast to other combinatorial approaches which can be run affordably only after placing hard constraints on the in-degree of all network nodes  <cit> . the laplace prior p and the resulting posterior have densities, so that the probability of a matrix a having entries exactly equal to zero vanishes. sparsity priors with point masses on zero have been used in statistics, but approximate bayesian inference for such is very hard in general . we predict discrete network graphs from our posterior as follows. for a small threshold δe, we take aij to represent an edge i ← j iff |aij| > δe. moreover, the marginal posterior probability of {|aij| > δ e} is used to rank potential edges i ← j.

the posterior for the sparse linear model with laplace prior does not fall into any standard multivariate distribution family, and it is not known how to do computations with it analytically. on the other hand, experimental design requires a good approximation to the posterior, which can be updated efficiently in order to score an experiment. denote the observations  obtained so far by d. from  and , we see that the posterior factorizes w.r.t. rows of a, in that p=∏ip, where ai,t. is the i-th row of a. the factors are joint distributions over n variables. we noted above that these factors are log-concave, and thus have a single local maximum and convex upper level sets . these features motivate approximating them by gaussian factors, so that a posterior approximation is obtained as q = ∏i q with multivariate gaussians q. the approximate inference method we use is a novel variant of expectation propagation   <cit> . our approach deals correctly with very underdetermined models , where previous ep variants would fail due to severe numerical instability. details are provided in the methods section, see also  <cit> .

experimental design
in our setup, an experiment consists of applying a constant disturbance u to the system, then measuring the new steady state. with current technology, such an experiment is expensive and time-consuming, especially if u is to be controlled fairly accurately. the goal of sequential experimental design is to choose the next experiment among a set of candidates , with the aim of decreasing the uncertainty in a using as few experiments as possible. a successful design methodology allows to obtain the same conclusion with less cost and time, compared to doing experiments at random or even following an exhaustive coverage. to this end, an information value score is computed for each candidate, and the maximizer is chosen.

different costs of experiments can be considered by multiplying the information value score with the costs. however, note that if the costs are extremely different, experiment design is often not necessary since the costs alone determine what should be done next.

a straightforward choice of an information value score is the expected decrease in uncertainty. in general, experimental design thus cannot be done without a representation of uncertainty in a, and the bayesian framework maintains such a representation at its core, namely the posterior. methods based solely on maximum likelihood or maximum a posteriori estimation  fail to represent uncertainties. denote the current posterior by q = q. if  is the outcome of an experiment, let q' = q'}) be the posterior including the additional observation. different information value scores have been proposed for experimental design, see  <cit>  for an overview. a measure for the amount of uncertainty in q is the differential entropy eq , so a convenient score would be the entropy difference eq  - eq' . a related score is the information gain s = d = eq' . here, d is the relative entropy , a common measure for the "cost"  of replacing q' by q. the inclusion of a new experiment leads precisely to the replacement q → q', so the information gain is well-motivated in our setup. while scores such as information gain or entropy difference are hard to compute for general distributions q, q', this can be done straightforwardly for gaussians. if q = n, q' = n and a = ai,t., the information gain is

 12tΣ−1), 

with m = -1Σ, which can be computed very efficiently in our framework.

the outcome  of an experiment is of course not completely known before it is performed. the central idea of bayesian sequential design is to compute the distribution over outcomes of the experiment, based on all observations so far, with which to average the score s. thus, some experimental candidate e is represented by a distribution qe over . in the setting of this paper, u* is completely known, say u* = u for candidate e, although in an extended setting, e might only specify a distribution over u*. given u* = u, qe=i{u∗=u}q, which can be sampled from easily: first, draw a ~ q, then x* = a- <dig>  ε* ~ n. in general, the information value for candidate e is given as s=eqe, which specializes to s|d)=s=eq] in our setup here.

testing
in the literature, there are some small networks with known dynamics, e.g. the drosophila segment polarity network  <cit> . however, a thorough evaluation of our method requires significantly larger systems for which the dynamics are known, so that disturbance experiments can be simulated, and the predictions of our method can be verified. we are not aware of such models having been established for real biological networks yet, the dream project  <cit>  aims at providing such data in the future. we therefore concentrate on realistic "in-silico" models, applying our method to many randomly generated instances with different structures and dynamics in order to obtain a robust evaluation and comparison.

we simulate the whole network identification process. first, we generate a biologically inspired ground-truth network together with parameters for a numerical simulator of nonlinear dynamics. we feed our method with a number of candidate perturbations {u*}, among which it can choose the experiments to be done. if some u* is selected, the corresponding x* is obtained from the simulator, and  is included into the posterior as new observation. we score the current posterior q against the true network after each inclusion, comparing our method against variants in different settings. free hyperparameters  are selected individually for each of the methods to be compared . we also compare against the experimental design method proposed in  <cit> , and finally show results on the real, but small drosophila segment polarity network  <cit> .

network simulation
common computational models of sparse regulatory networks often build on the scale-free or the small-world assumption  <cit> . in small world networks the average path length is much shorter than in a uniform random network. we sample such small-world networks with n =  <dig> nodes , see figure  <dig> for an example. further details about network generation and properties are given in additional file  <dig> 

for a given network structure, we sample plausible interaction dynamics using hill-type kinetics, inspired by the model in  <cit> . the non-linear function in  is

 fi=−vdixidi+xi+vsi∏j∈ai1+aijnij1+nij∏j∈ℐi11+nij, 

where ai are the activating  parents of gene i. the parameters in  and the way they are randomly sampled are described in additional file  <dig>  proposed system equations are subject to the condition, that the model produces dynamics with a reasonable stable steady state.

each observation  consists of a constant disturbance u and its effect x, being the difference between a new  and the old  steady state. disturbance candidates were restricted to a small number r of non-zero entries, since experimental techniques for disturbing many genes in parallel by tightly controlled amounts are not yet available. all non-zero uj are in {±ν}, where the sign is random, so ||u|| is the same for all u. we measure ||u|| in units given by the average relative change in steady state when such disturbances u are applied. we use a pool of  <dig> randomly generated candidates. the sde simulator can be used with different levels of noise, measured in terms of the signal-to-noise ratio , i.e. the ratio of ||u|| and the standard deviation of the resulting ε in .

all results are averaged over  <dig> runs with independently drawn networks. in the comparative plots presented below, the different methods all see the same data in each run.

evaluation criterion
the output from a regulatory network identification method most relevant to a practitioner is a ranking of all possible links, ordered by the probability that they are true edges. with this in mind, we choose the following evaluation score, based on roc analysis.

at any time, our method provides a posterior q, of which at present we only use the marginal distributions q. we produce a ranking of the edges according to the posterior probabilities q, where δe =  <dig>  in all experiments. δe was calibrated against average component sizes |aij|, which are roughly given through the dominant time scales in the dynamical system. the predicted rankings are robust against moderate changes of δe.

in a standard roc analysis, the true positive rate  is plotted as a function of the false positive rate , and the area under this curve  is measured. this is not useful in our setting, because only very small fprs are acceptable at all . our iauc score is obtained by computing auc only up to a number of fp equal to the number of edges in the true network, normalized to lie in  <cit> . for n =  <dig>  the "baseline" of outputting a random edge ranking has an expected iauc of  <dig> . furthermore, on average about 25% of the true edges are "undetectable" by any method using the linearized ode assumption: although present in the nonlinear system, their entries aij are very close to zero, and they do not contribute to the dynamics within the linearization region. such edges were excluded from the computation of iauc, for all competing methods.

discussion
in figure  <dig>  we present reconstruction curves for our method versus competing techniques, lacking novelties of our approach . very clearly, optimal design helps to save on costly and time-consuming experiments. the effect is more pronounced for the laplace than for the gaussian prior. the former is a better prior for the task, and it is well known that the advantage of designed versus random experiments scales with the appropriateness of the model. in this case, the iauc level  <dig>  is attained after  <dig> experiments with designed disturbances, yet only after  <dig> measurements with randomly chosen ones, thus saving 30% of the experiments.

in general, the model with laplace prior does significantly better than with a gaussian one . the difference is most pronounced at times when significantly less than n experiments have been done and the linear system  is strongly under-determined. this confirms our arguments in favour of the laplace prior.

the systematic underperformance of the most direct variant ld of our method, up to about n/ <dig> observations, is not yet completely understood. one should be aware that aggressive experimental design based on very little knowledge can perform worse than a random choice. this is a variant of the well-known "explore-exploit" trade-off  <cit> , which can be countered by either specifying prior knowledge more explicitly, or by doing a set of random inclusions  before starting the active design . this is done in the lm variant.

in figure  <dig>  experimental design is compared to the random experiment choice setting, both with a laplace prior. in the left panel, we vary the number r of non-zero entries in the disturbances u. recall that large r are in fact unrealistic in experimental techniques available today, but may well become accessible in the future. the less constraints there are on u, the more information one may obtain about a in each experiment, and the better our method performs. this is in line with linear systems theory, where persistent excitations  <cit>   are known to be most effective for exploring a system. the edge of experimental design is diminished with larger r. this is plausible, in that the informativeness of each u increases strongly with more non-zeros, thus the relative differences between u's are smaller. experimental design can outperform random choices only if there are clear advantages in doing certain experiments over others.

the middle panel in figure  <dig> explores effects of different sizes ||u||, i.e. different perturbation strengths . for larger ||u||, the real non-linear dynamics deviate more and more from the linearized ones, thus decreasing recovery performance above about 5%. on the other hand, larger ||u|| would result in a better snr for each experiment, given that non-linear effects could be modelled as well. this is not yet done in our method, but these shortcomings are shared by all other methods relying on a linearization assumption. it is, however, encouraging that our method is quite robust to the fact that even at smaller ||u||, the residuals ε behave distinctly non-gaussian .

the right panel in figure  <dig> shows how increasing stochastic noise in  influences network recovery. we keep r =  <dig> and set ||u|| to generate steady state deviations of 1%. good performance is obtained at snrs beyond  <dig>  with a snr of  <dig>  one cannot expect any decent recovery with less than n measurements. at all snrs shown, the network was recovered eventually with more and more experiments, but this is probably not an option one has in current biological practice.

comparison to tegnér et.al
the method proposed in  <cit>  is state-of-the-art for experimental design applied to gene network recovery, and in this section, we compare our method against theirs. their approach can be interpreted in bayesian terms as well, this is detailed in additional file  <dig> 

in contrast to our method, they discretize the space of possible matrices a. observations are used to sieve out candidates which are not "consistent" with all measurements so far. they have to restrict the maximum node in-degree for each gene to  <dig> in order to arrive at a procedure of reasonable cost. to our knowledge, the code used in  <cit>  has not been released. we implemented it, following all details in their paper carefully . in general, the diagonal of a  is assumed to be known in  <cit> . for the comparison, we modified our method to accept a fixed known diag a and changed the iauc score not to depend on self-edges.

results of a direct comparison are shown in figure  <dig>  with and without the proposed optimal design methods. due to the high resource requirements of the method in  <cit> , we use networks of size n =  <dig> , restricted to in-degrees at most  <dig>  in general, our method performs much better in recovering the true network. this difference is robust even to significant changes in the ground truth simulator. we find that their method is very sensitive to measurement and system noise, or to violations of the linearization assumption, whereas our technique is markedly more robust w.r.t. all these. we give some arguments why this might be the case. firstly, their "consistency" sieve of a candidates in light of measurements is impractical. after every experiment a number of inconsistent a is rejected from consideration, and noisy experiments may well lead to a wrong decision. any future evidence for such a rejected solution is, however, not considered any more. at the same time, an experiment does not help to discriminate between matrices which are still consistent afterwards. another severe problem with their approach lies in the discretization of a entries. a histogram of values of aij from our simulator reveals a very non-uniform  distribution: many values close to zero, but also a substantial number of quite large values. at the very least, their quantization would have to be chosen non-uniformly and adaptively, such that each bin has about equal mass under this distribution. however, it is quite likely that the best quantization depends on details of the true system which are not known a priori. statistics with continuous variables, as we employ, is a classical way of avoiding such quantization issues. furthermore, our laplace prior seems to capture features of the aij distribution favourably.

in table  <dig>  we compare running times. even though they restrict the node in-degree to  <dig>  which is often unrealistic for known biological networks  <cit> , the required running times are orders of magnitude larger than for our method. also, their memory requirements are huge, so that networks sizes beyond n =  <dig> could not be dealt with on a unit with  <dig> gb ram. both are clearly consequences of their quantization approach, which we circumvent completely by applying a continuous model. the asymptotic running time for a naive implementation of our method is o , independent of the true network structure, but this can be reduced to o as discussed in the methods section.

in minutes;  <dig> ghz opteron processor,  <dig>  gb ram. *: we allowed  <dig> gb ram for  <cit> , but this failed due to even higher demand for n >  <dig> 

drosophila segment polarity network
in  <cit> , von dassow et.al. describe a realistic model of the drosophila segment polarity network. we tested our algorithm on a single cell submodule, using the equations and parameters as described in , who also used this model.

so far, we modelled only mrna levels. however, the drosophila network also contains  <dig> proteins which play an important role in the regulatory network. since proteins are hard to control and to observe, we treat them as unobserved variables and focus on identifying the effective network between the genes. a link i → j between genes i ≠ j in the effective network represents one or more interactions of the form i → p <dig> → ⋯ → pq → j, where p <dig>  ..., pq, q ≥  <dig> are intermediate proteins, but not genes. in the methods section, we give a mathematical proof that any method working on the observed part of the system only, such as ours, in fact focusses on identifying the effective network, given that the linearized ode assumption is applied to the complete system. this is reassuring, since all regulatory networks between genes are nothing else but effective networks of larger partially unobserved systems.

as shown in figure  <dig>  the network contains  <dig> inter-gene regulatory pathways, apart from the self-links that are dominated by the respective self-decay rates. three of the inter-gene links are functionally weak . we simulated single gene perturbation experiments with an ordering chosen by our algorithm . after each experiment we ranked potential edges according to their probability. resulting ranks after  <dig>   <dig>   <dig> experiments for the true network edges are shown in figure  <dig>  all significant network edges are recovered after  <dig> experiments . even weak links are assigned low ranks compared to a maximal rank  <dig>  which places them amoung the first that would have to be examined more closely.

CONCLUSIONS
we have presented a bayesian method for identifying gene regulatory networks from micro-array measurements in perturbation experiments , and shown how to use optimal design in order to reconstruct networks with a minimum number of such experiments. the approach proves robust and efficient in a realistic non-linear simulation setting. our main improvements over previous work consist of employing a laplace prior instead of a simpler gaussian one, encoding the key property of sparse connectivity of regulatory networks within the model, and of actively designing rather than randomly choosing experiments. both features are shown to lead to significant improvements. when it comes to experimental design, our method outperforms the most prominent instance of previous work significantly, both in higher recovery performance and in smaller resource requirements. our application of the recent expectation propagation technique to the under-determined sparse linear model is novel, and variants may be useful for other models in bioinformatics.

in this paper, we have focussed on modelling mrna levels, which can be measured easily and cost-effectively. however, protein and metabolite concentrations also play important roles in any regulatory pathway, and a concise ode explanation of a system can probably not be formulated if they are ignored. our method allows to treat these as unobserved variables and to identifying effective networks between the genes. however, if the additional variables can be directly measured, they can easily be treated explicitly within our method, by simply extending the state variable x.

throughout the paper we have assumed that u* is known for an experiment, i.e. the disturbance levels of the r targeted genes can be controlled or at least predicted in advance, before the experiment is actually done. for example, a study trying to model the efficacy of rnai experiments is given in  <cit> . in the context of experiment design, we can only hope to compute the expected decrease in uncertainty for a specific experiment, and thus rank potential experiments according to their expected value, if the experimental outcome is predictable to some degree. in our method, the outcome x* for a given u* is inferred through the current posterior, i.e. the information gain from  is averaged over q. this can be extended to uncertain u*, if distributions qe specific to each experiment e can be specified. for experimental biology, this means that not only do we need experimental techniques which deliver quantitative measurements, but furthermore the parameters distinguishing between different experiments  either have to be fairly tightly controlled , or their range of outcome has to be characterized well by a mathematical model.

in general, biological prior knowledge about the  regulatory network may already be available before any experiments are done. in fact, in the presence of many genes n, it is typically not affordable to do on the order of n disturbance experiments, which are required for complete network identification in the absence of specific prior knowledge  experiments are required only in  <cit> , but we cannot confirm such a surprisingly fast scaling based on our experiments, even when using their method). within our method, such prior knowledge can be incorporated if it can be formulated in terms of the system matrix a. no interaction i ← j is encoded as aij =  <dig>  an activating influence i ← j as aij >  <dig>  these types of knowledge can be included in our method, as is discussed in the methods section.

there are several other setups of formulating the network recovery problem in terms of a sparse linear model. time-course mrna measurements with unknown, yet time-constant disturbances u are used in  <cit>  and  <cit> . relative rather than absolute changes in expression levels are employed in  <cit> . within all these setups, our general efficient bayesian framework for the sparse linear model could be beneficial, and could lead to improvements due to the laplace sparsity prior.

the linearized ode assumption is frequently done  <cit> , yet it is certainly problematic. for disturbances which change steady state expression levels by more than about 5%, our simulator showed a behavior which cannot directly be captured by a linearized approach. but such perturbation levels may be necessary to achieve a useful snr in the presence of typically high measurement noise. an important point for future work is the extension of the model by simple non-linear effects of relevance to biological systems. for example, our model can directly be extended to higher-order taylor expansions of non-linear dynamics, since these are still linear in the parameters.

