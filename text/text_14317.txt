BACKGROUND
mapreduce  <cit>  is an exciting new paradigm for designing parallel applications. it was popularized by google to support the parallel and distributed execution of data intensive applications. to process petabytes of data, google executes thousands of mapreduce applications per day. there is interest within the bioinformatics community to harness the power of mapreduce to develop parallel applications to process large datasets of genomic data. for example, cloudburst  <cit> , a mapreduce application for sequence analysis, has recently been released. in this paper, we study whether mapreduce can be used to develop efficient parallel phylogenetic applications for multi-core platforms.

we develop a new algorithm called mrsrf  for computing the all-pairs robinson-foulds distance between t evolutionary trees on multi-core computing platforms. the rf distance is a popular measure for computing the differences in evolutionary relationships between t phylogenetic trees of interest. there are several applications for using rf matrices such as visualizing collections of trees  <cit>  and clustering tree collections  <cit> .

the novelty of our work centers around using mapreduce in a non-standard way. typical uses of the mapreduce framework reduce the final output into a smaller representation than the initial input. one of the interesting aspects of the all-pairs rf distance problem is that the output size  is much larger than the input size . under the all-pairs rf problem, we are significantly expanding the data. for k total cores, how they are partitioned among the n nodes , where each node consists of c computing cores, has a significant impact on performance since cores on the same node share resources such as memory bandwidth. for example, with  <dig> total cores, a  <dig> nodes by  <dig> cores  cluster configuration outperforms  <dig> ×  <dig>   <dig> ×  <dig>  and  <dig> ×  <dig> configurations in our experiments. hence, multiple configurations should be tested in order to attain optimum performance on a multi-core platform.

we ran our experiments on  <dig>  and  <dig>  biological tree collections consisting of  <dig> and  <dig> taxa, respectively. mrsrf was implemented using phoenix  <cit> , a mapreduce implementation for shared memory multi-core platforms, and openmpi  <cit> . our results show that mrsrf is a promising methodology for parallelizing the all-pairs rf distance problem. in our experiments, mrsrf shows good overall speedup. on  <dig> cores, mrsrf is over  <dig> times faster than the best-performing sequential algorithm, which is also mrsrf run on a single core. for  <dig> cores, it is  <dig> times faster than the serial version of mrsrf. speedup resulted from allowing the underlying mapreduce runtime system to schedule communication on the multi-core system, which greatly simplifies mrsrf's implementation.

a common trend in phylogenetics is encapsulating the result into a single consensus tree, where the assumption is the information discarded is less important than the information retained. however, many of the trees may contain elements of the "true" evolutionary tree and their relationships should not be ignored. hence, we show how to use rf matrices to improve the summarization of a phylogenetic analysis. overall, our results provide evidence that large computations involving phylogenetic trees can take advantage of the mapreduce framework to design high-performance phylogenetic applications.

methods
mapreduce
mapreduce  <cit>  is a popular parallel model that automates parallel computation largely in the background, making it easier to develop a parallel program. popularized by google in  <dig>  it has since been used for a variety of diverse applications such as distributed sort and grep, google web indexing, and data processing by large companies such as amazon, yahoo! and facebook. the central features of the mapreduce framework are two functions: map() and reduce(). the map() function produces a set of intermediate key/value pairs. the reduce() function accepts an intermediate key and a set of values and merges them together. both the map() and reduce() functions are written serially by the programmer. the underlying mapreduce framework takes care of scheduling these functions on the multi-core system.

as each mapper outputs  pairs, these pairs are merged to form keys with associated lists of values. in the reduce phase, each instance of the reduce function  takes as input a key and associated list of values. in figure  <dig>  the fifth reducer takes in as input the key fish and the associated list 1-1-1- <dig>  as "fish" occurred four times in the input file. each reducer takes its list of values, sums all of its member's values, and outputs this sum of values with the key.

phoenix: a mapreduce library
the underlying mapreduce framework of mrsrf is based on phoenix  <cit> , a multi-core mapreduce approach. phoenix is a threads-based implementation of mapreduce designed specifically for multi-core systems where all computing cores have access to shared memory. it dynamically schedules map and reduce tasks to available compute cores. hadoop  <cit>  is the most popular framework for developing mapreduce applications. we developed mrsrf implementations in both phoenix and hadoop. for our rf matrix application, we were able to achieve significantly better performance using phoenix. consider a n × c multi-core cluster configuration, where n represents the number of physical nodes  of the cluster and c is the number of computing cores per node. each of the cores on a node share access to memory. phoenix works on a  <dig> × c configuration. we augment phoenix with openmpi in order to use distributed-memory clusters, where n ≥  <dig>  our mrsrf implementation is available publicly from the web  <cit> .

robinson-foulds distance
the robinson-foulds  distance  <cit>  is one of the most common methods used to compare the topological differences between two trees. for tree t on n taxa, removing an edge  splits tree t into two independent sets, s <dig> and s <dig>  each of the n taxa belong to either s <dig> or s <dig>  consider bipartition b in tree t. we can represent this bipartition as s1|s <dig>  where si contains the names of the taxa in that set. the rf distance computes the topological distance between two trees by comparing their set of bipartitions. let  define the set of bipartitions found in tree t. the rf distance between trees ti and tj is:  

in this paper, we develop a multi-core algorithm to compute the t × t rf matrix for a collection of t trees. entry  in the rf matrix represents the rf distance between trees ti and tj. finally, our results shows the rf rates instead of rf distances. the rf rate is obtained by normalizing the rf distance by the number of internal edges and multiplying by  <dig>  for n taxa, there are n -  <dig> internal edges in a binary tree. hence the maximum rf distance between two trees is n -  <dig>  which results in an rf rate of 100%. thus, the rf rate varies between 0% and 100% signifying that the two trees ti and tj are identical and maximally different, respectively.

hashrf
our mrsrf algorithm for multi-core platforms is based on the hashrf algorithm  <cit> , a fast, sequential algorithm for computing an all-to-all rf matrix to compare t trees on n taxa. for a bipartition b, hashrf uses a global hash table h to store that bipartition along with the identities of the trees  that contain that bipartition. hashrf uses two uniform hash functions h <dig> and h <dig>  where the h <dig> value represents the hash table location for storing the bipartition b and h <dig> provides a shortened bipartition identity  for this bipartition. moreover, for each , h2) pair, a list of trees  containing bipartition b is also stored in the hash table h. to compute the rf matrix m, each index  of the hash table h is visited. at h , each h <dig> value  is visited and its list of tree identities  are extracted. for each pair of trees ti and tj in the list of tids, entry m  is incremented by one to compute a similarity matrix. once the hash table has been traversed, entry m  is subtracted from n -  <dig>  the maximum rf distance, to produce the rf matrix. the worst-case running time of hashrf is o.

mrsrf: computing a t × t rf matrix
we introduce mrsrf , a multi-core all-to-all rf distance matrix algorithm using the mapreduce framework. the design of mrsrf is motivated by the hashrf algorithm. moreover, in mrsrf, bipartitions are analogous to words in the mapreduce word count example. mrsrf takes as input a tree file containing t trees and a n × c cluster configuration. the number of cluster nodes specifies the number of physical machines that executes the code. the number of cores is the number of cpus within each node. for serial execution, n =  <dig> and c =  <dig>  if instead one wanted to run mrsrf on  <dig> machines each containing  <dig> cpus, the respective n and c values would be  <dig> and  <dig> 

there are two main steps to our mrsrf algorithm. first, we organize the n nodes into a grid in order to partition the t input trees among the nodes. phoenix, the underlying mapreduce library, automatically distributes the input for a single node amongst its c cores. that is, it works for  <dig> × c cluster configurations. as a result, we manually partition the input among the n nodes. if n is a perfect square, then we assume the nodes are organized into a  grid. if n is not a perfect square, let i = ⌊⌋. if n mod i =  <dig>  then we assume a n/i × i grid of nodes. otherwise, we decrement i until it divides n evenly. for n =  <dig>  the n nodes are partitioned into a  <dig> ×  <dig> grid . if n =  <dig>  we obtain a  <dig> ×  <dig> grid. the size of the input tree file has no bearing on how the n nodes are organized into a grid.

secondly, once the nodes are organized in a grid using openmpi, the mrsrf algorithm is executed on each node to compute a p × q submatrix. for example, consider node n <dig> in figure  <dig>  let  and  represent the row and column trees, respectively, in the submatrix. for node n <dig>   = {tt/ <dig>  ..., tt-1} and  = {t <dig>  ..., tt/ <dig> - 1}. hence, the size of node n2's submatrix is p × q, where p = || and q = ||.

once each node is finished computing its p × q submatrix, the final t × t rf matrix is the concatenation of the n submatrices.

mrsrf: computing a p × q rf submatrix
the heart of our mrsrf algorithm lies in the subprogram mrsrf, which runs independently on each of the n nodes. each node has access to the input file containing the t trees and is responsible for retrieving the appropriate trees for its  and  sets. a node knows which trees belongs to its  and  sets based on its identifier within the node grid. in figure  <dig>   = {tt/ <dig>  ..., tt-1} and  = {t <dig>  ..., tt/ <dig> - 1} on node n <dig>  if the number of computing cores on node n <dig> is eight, then the trees associated with sets  and  will each be split into four files, yielding a total of eight input files for the eight cores. under mapreduce, these eight files will be automatically assigned to the cores on node n <dig> 

under mrsrf the trees in  are compared to the trees in . if , all trees are compared to each other. node ni's submatrix is created in parallel using two mapreduce phases as described below.

phase  <dig> of the mrsrf algorithm
the first map stage
similarly to hashrf, the first mapreduce phase is responsible for generating the global hash table. that is, every bipartition is given a unique identifier . its values are the tree identities  that contain that bipartition. the number of mappers correspond to the number of cores utilized on a particular node. each mapper sends its trees to hashbase to create a local hash table. hashbase is our name for a modification to hashrf that outputs a hash table from its input trees. each line from the hash table that is provided to mrsrf from hashbase consists of a bipartition bi and the associated list of tree ids that were found to share it. in addition, all the bipartitions that are found are given a marker to denote which input file created it. this is to ensure that bipartitions shared within a tree file are not compared to each other. the bipartition and its list of tree ids form a  pair, which is emitted as an intermediate for the reduce stage.

in figure  <dig>  there are two input files, each containing two trees each. trees in the first file are only compared to trees contained in the second file. in the figure, we assume there are only two mappers, where each mapper is responsible for handling one of the two input files. each mapper creates a local hash table based on the trees that it receives by using a marker of "1"  or "2"  to keep track of trees and their bipartitions from the  and  sets, respectively. each mapper then emits its marked hash table to the reduce stage. for example, in figure  <dig>  the first mapper emits the following  pairs: ), ), and ). these  pairs are processed in an intermediate stage, where each reducer processes all of the values associated with a particular key.

the first reduce stage
once the map stage completes, each of the r reducers takes as input a ) pair with bipartition bi as the key and a list of tree id lists as the value. there will be at most m lists of tree ids for each bipartition. each reducer then combines these o lists in a manner such that the trees from file  <dig> are separated from the trees from file  <dig> to form a single line in the global hash table. each row of the global hash table represents a unique bipartition among the t trees. continuing with our example from figure  <dig>  the first reducer processes the lists associated with keys abd|ce, and abc|de. thus, the first reducer receives the list of lists ) and , ) and outputs the final  pairs of ), and ), respectively. the symbol || denotes a partition that separates trees from the first input file with trees from the second input file.

phase  <dig> of the mrsrf algorithm
the second map stage
each of the m mappers receives an equal portion of the global hash table based on the total number of comparisons required to process the  pairs. in figure  <dig>  key  has as its list of values . two total comparisons will be done since t <dig> is compared to t <dig> and t <dig>  in general, if there are u tree ids on the left side of || and v trees on the right side, then uv total comparisons are required. each mapper then computes a local similarity matrix from its portion of the hash table. in the second reduce stage, this similarity matrix will be converted into a rf  matrix.

consider figure  <dig>  the first mapper has two rows of the global hash table assigned to it. next, it computes a p × q similarity matrix from those hash table elements. in our example, the resulting similarity matrix is of size  <dig> ×  <dig>  to do this, it compares the tree ids in the first partition of a row to the tree ids located in the second partition of a row, and increments the local similarity matrix accordingly. for example, for the hash table row ), the first mapper increments by one the locations  and  in its local similarity matrix. rows that do not contain elements in both of its partitions are discarded. therefore, in figure  <dig>  the hash table row ) is discarded. each increment to entry  in the similarity matrix represents that the pair of trees ti and tj share a bipartition.

once, a mapper has finished processing its hash table, it emits its similarity matrix for processing in the reduce stage. the key is the row id and the value is the contents of that row id. thus, in figure  <dig>  the first mapper emits the  pairs ), and ).

the second reduce stage
here, the input is a similarity matrix row identifier, i, and a list of rows that contain the local similarity scores found by each of the mappers. for a particular key, the number of rows within the list of rows received by a reducer is equal to the number of mappers, m. in figure  <dig>  the first reducer receives the following  pair for similarity identifier, t0: , ). the reducer sums up the columns of each of the lists to produce ). to produce the rf distance for row ti, each column in the final similarity matrix is subtracted from n -  <dig>  the maximum possible rf distance where n represents the number of taxa in the t trees of interest. together, the output from the reduce stage yields a final submatrix. for each node ni, the resulting submatrix is written to a file. these files can then be combined to form a final rf matrix, or be kept in their partitioned form for easier handling.

analysis of mrsrf
mrsrf is where all of the computation for the mrsrf algorithm lies. at least one node will require o time to obtain the trees for its  and  sets. the first map phase of the mrsrf algorithm, which is based on hashrf's first phase, requires , where n is the number of taxa and m is the number of mappers. o) is the total number of bipartitions that must be processed across the p + q trees and inserted into the hash table. suppose b unique bipartitions are found. in the worst case, a bipartition has a length of p + q, which re ects the fact that it appears in all p + q trees. hence, the complexity is  for the first reduce phase, where r is the number of reducers. for the second phase of the mrsrf algorithm, in the worst case, each mapper requires  to produce its local similarity matrix. each reducer requires  time. hence, if p and q are large enough, phase  <dig> is more time-consuming than phase  <dig> in the mrsrf algorithm. our analysis does not incorporate communication costs as there is not an explicit model of communication for the mapreduce framework.

biological trees
below, we describe the biological trees used in this study were obtained from two recent bayesian analysis.

 <dig>   <dig>  trees obtained from a bayesian analysis of an alignment of  <dig> taxa  with  <dig>  aligned sites  <cit> . two independent runs consisting of  <dig> million generations  were performed with four independent chains in mrbayes using the gtr+i+Γ model.

 <dig>   <dig>  trees obtained from an analysis of a three-gene,  <dig> taxa  dataset with  <dig>  aligned characters, which is one of the largest bayesian analysis done to date  <cit> . twelve runs, each with four chains, ran for at least  <dig> million generations in mrbayes using the gtr+i+Γ model. trees were sampled every  <dig>  generations.

implementation and platform
all experiments were run on a multi-core cluster with configurations ranging from  <dig> to  <dig> nodes. each node consists of a poweredge  <dig> 1u server, with two intel xeon e <dig>  <dig>  ghz quad-core processors, resulting in a total of eight cores. each node also consists of  <dig> gb ddr <dig>  <dig> mhz fully-buffered dram and  <dig> gb of hard-disk. the nodes are connected together with a gigabit ethernet switch. we modified the phoenix runtime system  to work on 64-bit linux platforms, as the cluster runs the centos  <dig>  64-bit operating system on all nodes. hashrf and hashrf  <cit>  are written in c++ and mrsrf and phoenix are implemented in c. all programs are compiled with gcc  <dig> . <dig> with the - <dig> compiler option.

RESULTS
establishing the fastest sequential algorithm
we evaluate the performance of mrsrf on our computational platform as we vary the number of cores, the number of nodes, and the problem size of interest. first, we establish the fastest sequential algorithm in order to compute the speedup of our approach. speedup is defined as  where, t <dig> is the time required by the fastest sequential program and tn × c is the time required by mrsrf run on n nodes and c cores. previous experiments established hashrf and hashrf as the fastest sequential algorithms for computing the rf matrix  <cit> .

multi-core performance of mrsrf
cluster configurations
to test how much a factor architecture is to speedup, we used different system configurations to measure performance. let n denote the number of nodes used, and c denote the number of cores used on each node. for any n × c configuration, there are nc total cores being utilized. thus, for  <dig> total cores to be used, we run our algorithm using  <dig> ×  <dig>   <dig> ×  <dig>   <dig> ×  <dig> and  <dig> ×  <dig> configurations. each curve denotes the number of cores utilized per node. therefore, if c =  <dig> and the total number of cores is  <dig>  then this data point reflects a  <dig> ×  <dig> configuration. likewise, if c =  <dig> and the total number of cores is  <dig>  then the data point reflects a  <dig> ×  <dig> configuration.

 <dig> taxa trees
 <dig> taxa trees
we see a similar trend in architectural performance in the  <dig> taxa case, thus underscoring the importance of managing resource contention and communication overhead in relation to performance. however, with the increased bipartitions present in the  <dig> taxa case, we see an markedly increased amount of speedup, with a maximum amount of speedup of  <dig>  attainable with  <dig> cores using a  <dig> ×  <dig> cluster configuration. the maximum speedup corresponds to an average running time of  <dig>  seconds. in comparison, it took the serial execution of mrsrf an average of  <dig>  seconds to compute the rf matrix, while it took hashrf and hashrf an average of  <dig>  and  <dig>  seconds respectively. our results show that mrsrf is a very scalable approach for computing the all-to-all rf matrix, with performance increasing with large problem sizes. figure  <dig> shows that phase  <dig> of the mrsrf approach exhibits linear speedup. overall speedup of mrsrf increases  when phase  <dig>  of mrsrf dominates the computation time. once again, the differences in speedup that we observe with different n × c configurations suggest that multiple cluster configurations should be run to achieve the maximum speedup.

rf matrix application: visually summarizing tree collections
the fundamental question we address here is "what do the gathered trees tell us about the bayesian analyses that produced them?" to answer this, we partitioned our t × t rf matrix based on the mrbayes run that generated the tree. figure  <dig> shows a heatmap of our  <dig>   <dig> ×  <dig>   <dig> rf matrix broken up into a  <dig> ×  <dig> matrix, where each entry  shows the average rf rate between the trees from run i and run j of mrbayes. for this dataset, two mrbayes runs were used to create the entire collection of  <dig>   <dig> trees, where each run consisted of  <dig>   <dig> trees. figure  <dig> shows the  <dig>   <dig> ×  <dig>   <dig> rf matrix broken up into a  <dig> ×  <dig> matrix. for this dataset, twelve mrbayes runs were used to create the entire collection of  <dig>   <dig> trees, where each run consisted of  <dig>   <dig> to  <dig>   <dig> trees.

in figures  <dig> and  <dig>  heatmap cell  is colored according to how similar  the trees are across runs i and j. hot regions, colored in shades of red, denote highly similar trees. cool regions, colored in shades of green, denote dissimilar trees. cell  in figure  <dig> shows an average rf rate of about 20% while  show an average rf rate of around 24%. in figure  <dig>  cell  shows an average rf rate of about 18% while these runs compared to themselves  and cells ) show higher levels of similarity with an average rf rate of around 11%. finally, the histogram in the color key represent the number of cells with a particular rf value.

hierarchical clustering, using the hclust function in r, is used to cluster highly similar cells in the heatmap to each other. for the  <dig> taxa tree collection, the trees from one run are not similar to trees in the other run suggesting that two different summaries are required to encapsulate the evolutionary relationships among the trees . for the  <dig> taxa trees, the heatmap  shows regions of high similarity among the trees within a run, and regions of dissimilarity across runs. the clustering also shows that runs  <dig>   <dig>   <dig> exhibit trees with high levels of similarity among them. one conclusion is that these runs converged to similar areas of tree space in the phylogenetic search and the trees from those runs can be summarized by a single tree . this is also true for runs  <dig>   <dig>   <dig>  and  <dig>  the clustering of the other runs  and  have lower levels of similarity.

overall, the clusterings in figures  <dig> and  <dig> suggest that there exist several well-supported partitions of the trees and that each partition should be summarized separately in order to minimize information loss. moreover, the data suggests that the various bayesian runs among the  <dig> and  <dig> taxa trees did not converge to the same place in tree space. one of the greatest benefits of convergence is reliability of the trees found by a phylogenetic heuristic. hence, rf matrices could be used as a method to detect convergence between runs.

CONCLUSIONS
in this paper, we evaluate the applicability of the mapreduce framework for developing multi-core phylogenetic applications. we design a new algorithm called mrsrf for computing the all-to-all rf matrix using the mapreduce framework. an open-source implementation of our mrsrf algorithm is available from the web  <cit> . one of the novelties of computing an rf matrix in a mapreduce context is that the size of the input  is much smaller than the size of the output . our results show that we achieve a significant speedup using mrsrf over the fastest sequential algorithm. on our largest problem size , we attain a maximum speedup of  <dig>  on  <dig> cores. our results suggest that mrsrf is a very scalable approach with increased performance resulting from larger collections of trees. furthermore, our results show the usefulness of running an algorithm on multiple n × c multi-core cluster configurations to ensure that the best performance is attained. finally, we show an application for rf matrices related to summarizing a collection of trees and detecting convergence of a phylogenetic analysis.

additionally, our results suggest that the storing the hash table is a better alternative to storing the large rf matrices, which are not sparse. since the hash table contains a list of shared, unique bipartitions, it is much smaller than the final t × t rf matrix. for example, storing the hash table of the  <dig>   <dig> ×  <dig>   <dig> rf matrix takes only  <dig> mb of storage compared to the  <dig>  gb necessary to store the full matrix . given the speed of the mrsrf, especially in phase  <dig>  one could store the hash table and compute the resulting t × t rf matrix on the fly as needed.

overall, our results show that mapreduce is an exciting approach for developing multi-core phylogenetic applications. future work includes studying the performance of mrsrf on larger clusters and tree collections. finally, we intend to design additional mapreduce phylogenetic applications—especially as it relates to reconstructing more accurate phylogenetic trees efficiently.

competing interests
the authors declare that they have no competing interests.

authors' contributions
sm and tw both designed and implemented the mrsrf algorithm. sm created all of the figures. both authors contributed to writing the manuscript and have approved its final contents.

