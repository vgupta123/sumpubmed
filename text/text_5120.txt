BACKGROUND
chromatin immunoprecipitation on tiling arrays  has been widely used to study genome-wide binding sites of transcription factors  <cit>  and other protein complexes  <cit> , as well as histone modifications  <cit> . to ensure data quality, chip-chip experiments are generally performed in replicates , and their reproducibility is used to determine whether the data are of sufficient quality and whether further experiments or validations are necessary. a number of factors impact the reproducibility of chip-chip data, including uniformity of cross-linking and sonication, specificity of the antibody, efficiency of chromatin immunoprecipitation, quality of the probes, and biological variability.

to assess the reproducibility of chip-chip data, a correlation coefficient is most often used to compare the replicates at the probe level  <cit> . the pearson correlation coefficient  is the most common, but the non-parametric spearman or kendall correlation coefficient can also be used. however, while the simplicity of the standard correlation coefficient is appealing, it suffers from a serious problem: the statistic depends not only on the reproducibility of the data but also on the amount of dna-protein binding or post-translational modification  present in that experiment, as we will demonstrate . that is, when a large amount of signal is present, the correlation coefficient tends to be higher than when a small amount of signal is present. in the latter case, the background signal contributes significantly and leads to a lower coefficient, even though pcc is generally sensitive to the outlying values. this lack of robustness to the amount of signal is an undesirable property, as it means that correlation coefficients cannot be properly compared between different experiments, except in the likely event that the amount of binding is exactly the same. a pcc of  <dig>  in one experiment does not indicate the same data quality or reproducibility as  <dig>  in another experiment if the amount of binding is different between the two experiments. this phenomenon is especially true for two-color chip-chip experiments, in which log-ratios rather than absolute intensities are used to measure correlation. when correlation is computed on the absolute intensities, the low values contribute relatively little to the statistic because their variability is small compared to the range of intensities. when a control channel or experiment is used to generate log-ratios, however, these background probes have large variability around zero and impact the correlation substantially.

another way to compare replicates is after further processing of the data, after regions  of binding or modification are identified. while we have often employed this approach in our previous analysis  <cit> , this is not a simple process. for instance, the noise level in each replicate may be different and so the thresholds for identification of enriched regions must be defined in each case. depending on how these thresholds are defined, the results can vary substantially. furthermore, the agreement among the identified clusters is difficult to define precisely. one can measure, for instance, the proportion of enriched clusters that overlap above a given percentage, at the expense of introducing another parameter. but, other complications also arise, such as a cluster in one experiment that encompasses multiple clusters on another array, making it unclear how to best compute the proportion. one can also choose to measure the fraction of overlap in base-pairs, but this quantity becomes biased in favor of data sets that have broad clusters  rather than sharp marks . these complications may be avoided in many situations if a more robust criterion for data quality and reproducibility exists. in the end, however, neither point-wise nor cluster-wise comparison is perfect; the overall decision on the quality should depend on several statistics including these two.

in this work, we propose the quantized correlation coefficient  that is more robust to the amount of signal present. we first use simulated data to measure the extent to which the pcc is influenced by the amount of signal in the data . this is done at varying levels of enrichment over background, which we refer to as the signal-to-noise ratio . then we describe qcc and compare it to the pcc. we also describe the methodological issues for this statistic and use real data to further demonstrate its usefulness.

RESULTS
quantized correlation coefficient 
the pearson correlation coefficient between two vectors, , i =  <dig>  ..., n,   

fails to be robust at different signal amounts because the background noise can have substantial influence on the statistic when the amount of binding is low. the main idea behind the proposed quantization and merging procedure is first to identify the probes containing background noise with an iterative step and then to reduce their impact on the correlation coefficient. this involves quantization-the process of constraining a continuous set of values by a discrete set-and hence the new statistic is called the quantized correlation coefficient.

 <dig>  initial quantization: for each data set, all the probe-level data are binned into b <dig> groups of equal size  based on the signal quantiles, starting with  <dig> for the probes with smallest intensities. the probes in each group are assigned as their new values integers  <dig>   <dig>  ..., b <dig> that correspond to their group number. for instance, with b <dig> =  <dig>  all probes that were in the 3rd decile in terms of intensity now have the new values  <dig>  b <dig> is a user-specified, initial number of bins for quantization. the choice of this parameter is discussed later.

 <dig>  merging of two neighboring groups: given b bins, there are b -  <dig> possible ways to merge two consecutive bins. for each possible merging, reassign the probes with  <dig>   <dig>  ..., b -  <dig> based on the new group configurations and calculate their corresponding pcc. after pcc is computed for every b -  <dig> possible configurations, choose the merging that most improves the correlation, and update the group assignment and probe values.

 <dig>  loop: repeat the above merging procedure until the correlation coefficient no longer improves. this defines the final groupings and values of the data points on which the correlation is computed.

in part, this method resembles the spearman correlation coefficient, which is the pccs applied on the ranks of the original data. because the correlation is computed on the ranks, qcc assesses how well an arbitrary monotonic function captures the relationship between the two replicates without any distributional assumptions. it also assumes that the consecutive ranks are equidistant  and thus suppresses the effect of outliers in the data. on the other hand, whereas the spearman correlation utilize the ranks of all data points equally in the computation, qcc collapses the the noisy part of the data  and reduces its contribution.

analysis of simulated chip-chip data
we first study the impact of signal amount on the pcc. we generated synthetic chip-chip data with  <dig>  total number of probes , with the probes divided into background and signal groups. we assume that the snr is fixed for each simulation. gaussian noise is added to each measurement and, without loss of generality, the standard deviation is set to  <dig>  thus, signal coverage  and signal intensity relative to noise level  are the only two parameters for each chip-chip simulation.

fig. 1a and 1b show two simulated sets of experiments with two replicates each, generated with the same snr but with different amounts of binding signal . because they have the same snr, an ideal statistic for reproducibility should be the same in both cases. however, the pccs are  <dig>  and  <dig> , respectively, showing that the pcc depends heavily on signal coverage. fig. 1c shows the correlation coefficients at varying amounts of signal, for five different snrs. the correlation coefficient correctly orders the experiments according to their snrs at a given signal amount. but within a single snr, it varies substantially. how this fluctuation can lead to misleading assessment of reproducibility is clear. the two replicates that have snr of  <dig> and coverage of 15%, for instance, have a higher correlation coefficient between them than the two replicates with snr of  <dig> but coverage of 5%. the coefficients do plateau after 30% or so, but the signal amount in most real experiments are much smaller. for instance, even if a transcription factor binds at  <dig>  places in the human genome with  <dig> kb resolution of binding on the array, this corresponds to only 1%.

we applied our quantization and merging method to see whether the new coefficient is more stable. in fig.  <dig>  we compare qcc with the pearson and spearman correlation coefficients in the simulated chip-chip replicate data at different snrs. in all three cases, although qcc  is not completely flat, it is substantially more robust across different signal coverages. for snr of  <dig> , the pearson and spearman correlation coefficients fluctuate by a factor of  <dig> and  <dig>  respectively, while qcc changes by 10-20%. for higher snr, qcc is even more stable. although it is not possible to determine whether one value of correlation is more correct or less biased than another, qcc is more robust to the signal amount. it also appears to reflect the quality of the data more accurately: when the signal coverage is low for a given snr, noise in the background prevails in pcc but has less impact for qcc. that qcc is consistently higher than the other two does not indicate that it is less biased, but qcc is closer to the coefficient one would obtain from the signal part of the data . in addition, qcc is still sensitive to data quality and increases with higher snr. this is the case for the pearson coefficient but not for the spearman coefficient.

as the initial number of probe bins is the only parameter we need to specify in the quantization and merging procedure, we investigate its effect on qcc. fig.  <dig> shows the results, which suggest that qcc quickly stabilizes when initial number of bins is large enough, e.g.,  <dig>  we have used  <dig> bins in the simulations above based on this result. the only exception would be for the cases with very high snrs and low coverage, but the computation is fast and a larger bin number can be easily used.

analysis of msl complex chip-chip data
dosage compensation is a process in which organisms attempt to equalize x-linked gene expression when the x-chromosome copy numbers are different between males and females. without this process, gene products between x and autosomes would not be balanced in each sex. in mammals, this is achieved by x-inactivation, which silences gene expression from one of the two x chromosomes in females. in drosophila, the opposite strategy of up-regulating the single x chromosome by two-fold is employed. how the entire male x chromosome is regulated on such a large scale is a fascinating question, which we have studied extensively. while some aspects of this process are still not clear, a key step involves the drosophila male-specific lethal  complex. previously, we have used custom nimblegen tiling arrays  to examine the binding of this complex on the x chromosome as well as on an autosome of similar size  that serves as an internal control. our experiments and analysis identified precise locations on the x chromosome on which the msl complex binds to start the process of up-regulation of the x-linked genes  <cit> .

the specific binding of the complex on the x chromosome makes this data set an ideal one for studying methodological questions in chip-chip data analysis, as we know that all binding signals must come from the x chromosome  <cit> . in fig. 4a, the scatterplot between two chip-chip replicates is shown for the binding of the msl complex . fig. 4a shows the probe values on the x chromosome and the autosome in different colors. it appears that nearly all the high signals come from red points corresponding to the x chromosome and that they correlate fairly well. profiles along the chromosomal positions also indicate that the binding sites are well reproduced in this data set. however, the standard pcc at the probe level is only  <dig> . although there is no consensus definition of what a 'high' or 'low' correlation coefficient is,  <dig>  appears to be lower than what one may expect from the signal part of the data. this happens because the binding signal in this data set is very small  as in most chip-chip data and the background noise dominates in the computation of the coefficient.

we applied our proposed quantization and merging method with  <dig> initial number of bins to this data set. fig. 4b shows the final configuration . as expected, almost all the probes from the 2l autosome are grouped into the background part, and the x-specific signal probes are grouped separately. the resulting correlation increases from  <dig>  to  <dig> . we believe this is a better indicator for these chip-chip data quality and reproducibility. when we compute the pcc based only on the intersection of the top 10% probes from each replicate, it is  <dig> , revealing that the signal portion is well correlated between replicates and that the reproducibility measured by qcc is a reasonable estimate. that qcc is not simply a blind inflation of correlation coefficient can be examined by the qcc for mock data. when we test qcc for the mock data, which were available only for the wildtype msl complex  <cit> , the qcc is  <dig>  while the pcc is  <dig>  .

comparison to cluster-level reproducibility
we have already described in the introduction some of the complications that arise if reproducibility is measured at the cluster level, including how to identify clusters accurately and how to measure overlap between two sets of clusters. nonetheless, we carry out one direct comparison between computing reproducibility at the probe level versus at the cluster level, both using simulated data and real data, to get a rough idea between the two types of comparisons.

numerous peak callers  have been proposed and their performance varies widely depending on the type of binding profiles  and platforms. for our simulated data, we set the size of every cluster as  <dig> probes and add noise of varying magnitude to each probe, for a variable number of clusters. then a region spanning a minimum of  <dig> consecutive probes  above a quantile threshold is then called as a cluster. this very simple simulation ensures that the peak calling is done accurately and there is little variation coming from that step. to determine concordance, the probes identified as part of a peak is replaced with  <dig> and the rest are replaced with  <dig>  and the pearson correlation coefficient is computed between the replicates. in fig.  <dig>  the results of this simulation is shown on top of the plots shown in fig.  <dig>  we see that the cluster-level reproducibility, if the clusters can be accurately identified, has a desirable property of not being strongly influenced by the amount of binding signal. this can be seen by the blue lines that are relatively flat, similar to the green qcc lines, at varying signal-to-noise levels. we also see that cluster-level reproducibility is more sensitive in that it has a greater range of values for the three signal-to-noise ratios.

for real data, we compared ctcf chip-chip data in drosophila s <dig> cells produced in two laboratories , as part of the model organism encyclopedia of dna elements  project http://intermine.modencode.org. fig. 6a shows the scatterplot between the two datasets for the probes on the x chromosome and fig. 6b shows the quantized view with jittering, as was done in fig. 4b. the pcc is  <dig>  while the qcc is  <dig> . we computed cluster-level concordance at varying quantile thresholds  and the maximum correlation was  <dig> , which was a little closer to qcc than to pcc.

overall, the probe-level and cluster-level reproducibility measures are complementary and both types of analysis can assist in determine the data quality. qcc is much simpler to compute and is not burdened with the dependence on the quality of the peak-calling algorithm or the threshold  used to define peaks. for instance, a low fdr will select only the most prominent peaks, which tend to be quite reproducible; a higher fdr will result in inclusion of less prominent, lower enrichment peaks that are less reproducible. thus, for many data sets, the user will get a different impression on the quality of the data, depending on the fdr value used. on the other hand, any subsequent analysis of chip-chip data will involve characterization of the peaks, such as searching for a shared motif or co-binding patterns with other factors. thus, it is reasonable to consider the reproducibility of data at the cluster-level.

CONCLUSIONS
we have shown that standard correlation coefficients do not properly reflect the agreement between chip-chip replicates, especially when amount of binding is low. our proposed procedure groups the background probes and suppresses their contribution to the correlation so that reproducibility can be better captured. this procedure requires no a priori knowledge about the features, such as enrichment/depletion, high/low coverage, and noise level. the initial number of bins is the only parameter involved but we have shown that the results are stable beyond a sufficiently large number.

as mentioned in background, there are other ways to measure reproducibility in chip-chip data. in particular, a correlation analysis approach discussed here completely ignores the signal locations, and accounting for this by comparing replicates after identification of enriched regions is an alternate approach that should be done at the same time. other essential steps in quality control are to visualize the data in a genome browser and to compare the enrichment at specific loci with qpcr data. nonetheless, a simple correlation analysis is often done in practice and it is therefore important to address the pitfalls in this approach. again, we do not claim that qcc is more accurate than pcc for a given set of data. such a statement is difficult to make in general and the maximization steps we take could be overcompensating the pcc estimate. instead, we claim that it has the desirable property of robustness against the varying fraction of signal we observe in real experiments and that it accurately reflects snr in the data.

qcc was developed in the context of chip-chip data, but it can be directly applied to other applications in which estimation of the similarity or reproducibility between any two sets of measurements is needed, as long as data can be ordered and binned into subgroups. as illustrated in this paper, qcc is most useful when a large part of the data consists of background noise and its amount is a confounding factor in interpretation of the standard correlation coefficients. in genome-wide array measurements, this occurs frequently. in many gene expression studies, the majority of the genes may not be expressed or expressed at low levels; in many array comparative genomic hybridizations , only a small fraction of the genomic dna may display copy number aberrations. quality control of these experiments requires a consistent measure of reproducibility of replicate experiments and qcc is one possible measure.

