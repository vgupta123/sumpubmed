BACKGROUND
with the digitalization of much of the biomedical literature, automated processing of journal publications has become increasingly important in biomedical research. biomedical researchers struggle to keep abreast of the exponentially growing literature, due to not only its sheer scale but also to the expanding range of disciplines and journals relevant to a typical research question. biomedical publications, like most texts, are fraught with synonymy, polysemy, ambiguity, and complexity. transformation of these texts into formal representations of the contained knowledge makes possible the application of sophisticated computational methods that assist researchers and advance science. substantial progress in biomedical natural-language processing , particularly in the tasks of information retrieval, concept recognition, and information extraction  <cit>  raises the possibility of creating formal representations for the entire biomedical literature.

development of formal ontologies for the representation of domain-specific knowledge has also made substantial progress  <cit> . among the most ambitious of these efforts are the open biomedical ontologies , a set of ontologies whose domains include anatomy, biological processes and functions, cells and cellular components, chemicals, phenotypes and diseases, and experiments and procedures. these ontologies are largely constructed in a community-driven approach, and their developers commit to a common set of attributes including openness, shared syntax, clear versioning, demarcated content, and clear definition  <cit> . millions of genes, gene products, and biomedical data sets have been annotated with ontological terms, and these annotations are widely used as the basis for high-throughput data analysis. in particular, calculations of enrichment of gene ontology  terms in sets of differentially expressed genes are widely used  <cit> , and more sophisticated uses of formal knowledge representations in data analysis are beginning to be published .

manually annotated, or “gold-standard”, corpora are increasingly important for the development of sophisticated nlp systems, both as training data and for evaluative purposes. use of manually annotated biomedical corpora in nlp research has consistently led to improved results. in a study by tomanek et al., the accuracy of tokenization of a test set of biomedical text increased from  <dig> % when their tool was trained on a corpus that was tokenized using newspaper language patterns to  <dig> % when their tool was trained on a corpus whose tokenization was biomedically motivated  <cit> . kulick et al. showed that accuracy of part-of-speech annotation of biomedical text increased from  <dig> % to  <dig> % on test abstracts when their tagger was retrained after the training corpus was manually checked and corrected  <cit> , and coden et al. found that adding a small biomedical annotated corpus to a large general-english one increased accuracy of part-of-speech tagging of biomedical text from 87% to 92%  <cit> . lease and charniak demonstrated large reductions in unknown word rates and large increases in accuracy of part-of-speech tagging and parsing when their systems were trained with a biomedical corpus as compared to only general-english and/or business texts  <cit> . it was shown by roberts et al. that the best results in recognition of clinical concepts  in biomedical text, ranging from 10% below to 11% above the interannotator-agreement scores for the gold-standard test set, were obtained with the inclusion of statistical models trained on a manually annotated corpus as compared to dictionary-based concept recognition solely  <cit> . craven and kumlein found generally higher levels of precision of extracted biomedical assertions  for naïve-bayes-model-based systems trained on a corpus of abstracts in which such assertions were manually annotated, as compared to a basic sentence-cooccurrence-based method  <cit> .

in recognition of the importance of such corpora, the colorado richly annotated full-text  corpus, a collection of  <dig> full-length, open-access biomedical journal articles selected from the regular annotation stream of a major bioinformatics resource, has been manually annotated to indicate references to concepts from multiple ontologies and terminologies. specifically, it contains annotations indicating all mentions in each full-length article of the concepts from nine prominent ontologies and terminologies: the cell type ontology   <cit> , the chemical entities of biological interest ontology   <cit> , the ncbi taxonomy   <cit> , the protein ontology , the sequence ontology   <cit> , the entries of the entrez gene database   <cit> , and the three subontologies of the go, i.e., those representing biological processes , molecular functions , and cellular components   <cit> .

the first public release of the craft corpus includes the annotations for  <dig> of the  <dig> articles, reserving two sets of  <dig> articles for future text-mining competitions ., this corpus is among the largest gold-standard annotated biomedical corpora, and unlike most others, the journal articles that comprise the documents of the corpus are marked up in their entirety and range over a wide range of disciplines, including genetics, biochemistry and molecular biology, cell biology, developmental biology, and even computational biology. the scale of conceptual markup is also among the largest of comparable corpora. while most other annotated corpora use small annotation schemas, typically comprised of a few to several dozen classes, all of the conceptual markup in the craft corpus relies on large ontologies and terminologies nearly in their entirety, creating an unprecedentedly rich semantic resource. since we have been guided by marking up textual mentions with their directly corresponding ontological and terminological concepts, these mentions are marked up without loss of knowledge.. all of the concept annotations of all terminologies used were created using a single set of guidelines, making clear which spans of text are to be marked up and what the span boundaries should be, which has resulted in high interannotator agreement. along with the syntactic  <cit>  and coreferential  <cit>  annotations that have been created for the same set of journal articles, the concept annotations of the craft corpus have the potential to significantly advance biomedical text mining by providing a high-quality gold standard for bionlp systems.

following this brief introduction, we will present the salient statistics for the conceptual markup of the corpus in the form of counts of concept annotations and of unique annotated concepts for each of the vocabularies used, as well as the formats in which this markup is being released. this is followed by an in-depth comparison of the concept annotations of the craft corpus to those of other publicly available manually annotated gold-standard biomedical corpora as well as several other relevant projects, along with a discussion of the aspects of our concept annotations that we claim are prominent factors in their being a significant contribution to the bionlp community. ongoing and future work is then briefly described, followed by our conclusions. the primary text of our paper ends with methodology with regard to corpus assembly, terminology selection, creation of annotation guidelines, and creation of the conceptual markup. finally, as supplementary material, we provide an extensive presentation of our concept-annotation guidelines and a spreadsheet of our interannotator-agreement statistics in detail.

RESULTS
the articles in the craft corpus have been completely marked up with the full sets of conceptsa  of nine biomedical ontologies and terminologies. here we present annotation-count and interannotator-agreement statistics for the conceptual annotation. in a companion paper  <cit>  we present the syntactic annotation  of the craft corpus and studies in which it was used to train high-performing models, providing indirect evidence of its high quality.

concept annotation statistics
awe are still in the process of reviewing and editing the go bp & mf annotations for the official  <dig>  version release; therefore, the statistics for these will likely change. we will update annotation statistics on the project web site as needed.

bwe have calculated statistics for the go cc project both with and without the annotations of cell , as these account for over half of the annotations of this project. in addition to skewing these statistics, since this is such a trivial concept that is also being annotated in the cl project, users may wish to exclude these annotations for training and evaluation of systems.

cin addition to the hundreds of thousands of organism entries, the ncbi taxonomy also has a small taxonomy of types of biological taxa . for the ncbi taxonomy pass, there are also a small number of annotations of the mentions of these taxonomic concepts in the articles; however, we have excluded these in these statistics.

dfor the so statistics, the independent_continuant annotations  were excluded from the analysis.

ethe averages of the total number of annotations per article and of unique concepts per article were calculated simply by adding up the averages for each terminological annotation pass.

counts of annotations and of average, median, minimum, and maximum counts of annotations per article for the  <dig> articles constituting the initial public release of the craft corpus.

counts of unique mentioned concepts and of average, median, minimum, and maximum counts of unique mentioned concepts per article for the  <dig> articles constituting the initial public release of the craft corpus.

interannotator-agreement statistics
figures  <dig> and  <dig> illustrate that the use of our concept annotation guidelines  has enabled consistently high interannotator agreement after a short initial period of working with a newly encountered ontology. our annotators, who are domain experts, not knowledge engineers , were able to quickly reach and with occasional exception remain at a 90 + % iaa level for all of the terminological annotation passes except for the challenging go bp & mf passb. oscillations in these figures are partly explained by the fact that an annotator may make the same type of error many times in a given article, which can strongly affect iaa statistics. for example, a given article often has many mentions of some concept, and two annotators might consistently annotate these mentions differently, leading to a considerable drop in iaa. for example, the large drop seen in the eighth data point for the cl project is almost wholly attributable to the consistently discrepant annotation of the several dozen mentions of polymorphonuclear leukocytes/pmns in one article.  and the other with cl:mature neutrophil , one of its subclasses.) in addition to figures  <dig> and  <dig> within this paper, we have included a spreadsheet of the precise iaa statistics for all of the annotation passes as supplementary material .

this degree of iaa is impressive, given that the annotation schemas  are very large  as compared to the typical textual annotation project, which uses a schema of no more than dozens of classes. furthermore, a very strict standard of matching was used in the calculation of these iaa statistics: a given pair of annotations was considered a match only if they used the exact same class/term and specified the exact same text span. for many of the mismatches , the given pair of annotations used closely related classes  and/or had only slightly different text spans; thus, even a slight relaxing of the matching criteria would result in even higher iaa figures.

as presented in the methodology section, most of these data points are single-blind statistics, in which the lead semantic annotator inspected the markup of the annotators, edited  markup with which he disagreed, and calculated the agreement between the original markup and the edited version. we have also annotated a small number of articles in a double-blind fashion, including the last three articles of the corpus  annotated with the bp and mf branches of the go, which resulted in iaas of  <dig> %,  <dig> %, and  <dig> %, in concordance with previous data points, as can be seen in this figure. these  data suggest that the single-blind iaas are unlikely to be biased by a considerable amount.

a primary reason we have not performed double-blind annotation for the majority of the articles of the corpus is simply a lack of resources; as it is, the creation of this semantic markup has entailed tens of thousands of person-hours over more than four years of effort. additionally, for the creation of a gold-standard annotated corpus, we have found it crucial for a knowledge engineer to thoroughly review the markup using the concepts of these ontologies. it is necessary not only to have a broad familiarity with the ontologies being used but also to have a deeper understanding of their representational subtleties and assumptions that domain experts may very well never even consider much less make sense of by themselves. for example, the chebi ontology contains a subgraph for amino acids as discrete molecules , another for those as amino-acid residues within peptide chains , and another for the molecules' derived substituent groups , and using these correctly and consistently at first may not be at all obvious; indeed, the annotator using chebi to mark up the articles of the craft corpus incorrectly used the molecular terms for many of the residue mentions  before the semantic lead observed this and provided guidance. additionally, many ontologies have their own idiosyncrasies and often even ambiguous or dubious modeling, and expertise in knowledge engineering is very valuable in deciding how to deal with these representational issues with regard to their use in text annotation  <cit> . the craft corpus was constructed on the assumption that a consensus set of annotations resulting from single-blind evaluation by a competent knowledge engineer of ontology-based markup created by a domain expert would result in a higher-quality annotated corpus than a consensus set of annotations resulting from double-blind annotation by multiple domain experts. the soundness of this approach is supported by recent studies by dligach et al. that have shown that single-annotating more data as opposed to double annotation is more cost-effective in improving system performance  <cit>  and that double annotation can be greatly reduced without loss of system performance  <cit> .

distribution formats
the craft corpus concept annotations have been released in multiple formats to promote ease of use and community uptake. the release includes both the full-text documents and sets of conceptual and syntactic annotation artifacts accompanying each document. all craft corpus documents are drawn from the pubmed central  open access subset  <cit> , licensed to allow redistribution. the craft corpus release includes two versions of each document: an xml version provided by pmc and a more human-readable plain-text version derived from the xml, the latter of which was used by the annotators to generate concept markup. the character offsets referenced by the annotations are therefore relative to the plain-text documents and not the original xml; inclusion of the xml format in the distribution is for provenance only.

we have provided these concept annotations in both the xml-based genia project markup language   <cit> , and in the w3c-standard resource description framework . craft corpus annotations in gpml will allow quick uptake by users with software based on the well-known genia corpus, and the version in rdf will open the corpus up to the semantic web community. it is important to note that because discontinuous annotations  cannot be unambiguously represented in gpml, we have excluded all such annotations from this version; thus, the set of concept annotations in the gpml version of the craft corpus is be regarded as incomplete. an xml-based format produced by the knowtator annotation tool  <cit> , which is also widely used in the text annotation community, is provided as well.

to facilitate use in nlp generally, we have also released craft tools for the unstructured information management architecture   <cit> , a popular open-source middleware layer for text processing. the craft corpus documents and annotations have been bundled as serialized uima common analysis structures , and we provide a uima collection-reader component for them.

for browsing and exploratory purposes, we plan to integrate the craft corpus with two online resources: u-compare  <cit>  and massachusetts general hospital’s document metadata organizer  application, formerly called the swan annotation tool  <cit> . u-compare is a web-based nlp system relying on uima that allows users to set up and run text-processing pipelines using a variety of tools over a variety of corpora. domeo is a web-enabled annotation tool designed for markup of scientific articles with ontology terms and will allow an easy introduction to the corpus by facilitating browsing of its data. furthermore, we have already integrated the craft corpus with brat   <cit> , with which our concept annotations can be browsed online at http://compbio.ucdenver.edu/craft. note, however, that brat cannot render discontinuous annotations  properly. for each discontinuous annotation, the shortest span of text encompassing all of the text spans is instead highlighted. however, in brat, if the cursor hovers over an icon indicating an annotation, a window pops up specifying the annotated text, the concept with which this text is annotated, and the terminology of which this concept is a member; for discontinuous annotations, the correct disconnected spans of text are displayed as the last line in this popup window.

the corpus, annotation guidelines, and other associated resources are freely available at http://bionlp-corpora.sourceforge.net/craft/index.shtml.

discussion
craft can be compared to a number of previously released corpora. we focus primarily on biomedical corpora, as these are obviously most directly related: abgene  <cit> , bioinfer  <cit> , the clef corpus  <cit> , the fetchprot corpus  <cit> , the fourth i2b2/va challenge corpus  <cit> , genetag  <cit> , genia  <cit> , grec  <cit> , the iti txm ppi and te corpora  <cit> , medpost  <cit> , the pennbioie oncology and cyp v <dig>  corpora  <cit> , and the yapex corpus  <cit> . though it bills itself as a “silver standard”, due to its vast scale we also compare craft to the output of the collaborative annotation of a large biomedical corpus , an effort at constructing a very large biomedical corpus through “harmonization” of the automatically generated annotations of five systems  <cit> . finally, though it focuses on newswire articles, we also compare our corpus to ontonotes release  <dig>  here, as it is analogously a large-scale manually created corpus project with multiple types of semantic and syntactic annotation  <cit> . table  <dig> summarizes some criteria by which we compare craft to other corpora.

comparison of corpora in terms of total numbers of words/tokens is summarized in table  <dig>  the full corpus contains ~ <dig>  tokens, and the initial release contains more than  <dig> ; they are larger than nearly all gold-standard annotated corpora , including genetag, ontonotes, genia, the pennbioie oncology and cyp corpora, the medpost corpus, and bioinfer. the only corpora larger than ours by this criterion is the silver-standard calbc corpus, with ~ <dig> , <dig> tokens, and the gold-standard iti txm ppi and te corpora, with ~ <dig> , <dig> and ~ <dig> , <dig> tokens, respectively; however, the counts of the iti txm corpora include all versions of the subset of documents that were multiply annotated , and, as discussed later, not all sections of the component documents of these corpora were annotated.

fbioinfer has ~ <dig>  tokens total, and ~ <dig>  excluding punctuation.

gbioinfer has ~ <dig>  named-entity annotations and ~ <dig>  annotations of what are termed relationships but that might more properly be conceptualized as process or state classes and thus are included here, totaling ~ <dig>  concept annotations.

hin the calbc corpus, ncbi taxonomy and umls concepts were respectively used to mark up species and disease mentions.

1the clef corpus is composed of many types of medical documents:  <dig> entire patient records  and  <dig> each of clinical narratives, histopathology reports, and imaging reports.

jthe annotators of the iti txm corpora attempted to assign entrez gene ids to gene annotations and refseq ids to annotations of proteins, mrnas, and cdnas .

kthe annotators of the iti txm corpora used chebi, mesh, and ncbi taxonomy concepts for drug, tissue, and sequence mentions.

lin ontonotes, the  <dig> most frequent polysemous verbs and  <dig>  most frequent polysemous nouns have been annotated with the appropriate senses of wordnet  <dig> , so the size of the schema  likely numbers in the thousands; however, they note that this is different from their ontological annotation, for which only approximately  <dig> concept types are being used to subsume the annotated word senses.

min addition to ~ <dig>  annotated verbs, ontonotes has an unstated but presumably large count of annotated nouns.

a summary of counts of words/tokens, of counts and types of component documents, of domains, and of counts of concept annotations for the craft corpus and related corpora.

corpora can also be compared on the size of the documents annotated, also summarized in table  <dig>  most of the corpora surveyed here are composed of relatively short documents. among the shortest are those documents that are individual sentences, which compose the genetag, the abgene corpus, and bioinfer corpora. most comparable corpora are composed of documents of several sentences to a paragraph, typically publication abstracts, e.g., the calbc corpus, genia, the pennbioie oncology and cyp corpora, grec, and the yapex corpus, as well as those composed of discharge summaries, e.g., the fourth i2b2/va challenge corpus. the clef corpus is composed of a number of different types of moderately sized medical documents, and the ontonotes corpus contains  <dig>  multiparagraph newswire documents. the longest documents of these surveyed corpora are full-length biomedical articles, e.g., the iti txm ppi and te corpora, the fetchprot corpus, and the craft corpus. in the biomedical domain, having access to full-length articles is increasingly seen as important for concept-identification and information-extraction efforts  <cit> .

another point of comparison of annotated corpora is in terms of their respective domain, also summarized in table  <dig>  the corpora surveyed are within the biomedical domain, with the exception of ontonotes, which covers english and chinese newswire text. the clef corpus and the i2b2/va challenge corpus contain clinical documents, which are relatively rare due to issues of patient confidentiality of medical records. the remainder of the corpora discussed here are composed of sentences, abstracts, or full-length articles culled from medline. however, most of these are further narrowed to one or several relatively specific biomedical domains. in addition to requiring open licensing, the articles of the craft corpus were selected for their being evidential sources for one or more go and/or mp annotations of mouse genes or gene products. apart from focusing on the laboratory mouse , the articles have no predefined constraints within the biomedical domain, and the corpus includes articles ranging over the disciplines of genetics, biochemistry and molecular biology, cell biology, developmental biology, and even computational biology. while our corpus does not include examples of articles that do not support go and/or mp annotations of mouse genes/gene products, e.g., clinical studies, it otherwise reflects a broad overview of the biomedical literature. compared to other publicly available corpora, craft is a less biased sample of the biomedical literature, and it is reasonable to expect that training and testing nlp systems on craft is more likely to produce generalizable results than those trained on narrower domains. at the same time, since our corpus primarily concentrates on mouse biology, we expect our corpus to exhibit some bias toward mammalian systems.

one of the most important aspects of the semantic markup of corpora is the total number of concept annotations, for which we have provided statistics in table  <dig>  the full corpus contains over  <dig>  annotations to terms from ontologies and other controlled terminologies; the initial release contains nearly  <dig>  such annotations. this is among the most extensive concept markup of the corpora discussed here for which we have been able to find such counts, including the iti txm ppi and te corpora, genia, and ontonotes, and it is considerably larger than that of most corresponding previously released corpora, including genetag, bioinfer, the abgene corpus, grec, the clef corpus, the yapex corpus, and the fetchprot corpus. the only corpus with amounts of concept markup considerably larger than ours  is the silver-standard calbc corpus.

a significant difference between the craft corpus and many other corpora is in the size and richness of the annotation schemas used, i.e., the concepts that are targeted for tagging in the text, also summarized in table  <dig>  some corpora, including the iti txm corpora, the fetchprot corpus, and the calbc corpus, used large biomedical databases for portions of their entity annotation, though most were done in a limited fashion.; furthermore, though such databases represent large numbers of biological entities, the records are flat sets of entities rather than concepts that themselves are embedded in a rich semantic structure. there has been a small amount of corpus annotation with large vocabularies with at least hierarchical structure, among these the iti txm corpora and the calbc corpus, though these are limited in various ways as well. ontonotes, the grec, and bioinfer use custom-made schemas whose sizes number in the hundreds, while most annotated corpora rely on very small concept schemas. in the craft corpus, all concept annotation relies on extensive schemas; apart from drawing from the ~ <dig> , <dig> records of the entrez gene database, these schemas draw from ontologies in the open biomedical ontologies library, ranging from the ~ <dig> classes of the cell type ontology to the ~ <dig>  concepts of the ncbi taxonomy. the initial 67-article release of the craft corpus contains over  <dig>  distinct concepts from these terminologies. furthermore, the annotation of relationships among these concepts  will result in the creation of a large number of more complex concepts defined in terms of these explicitly annotated concepts in the vein of anonymous owl classes formally defined in terms of primitive  classes  <cit> . analogous to research done in calculating the information content of go terms by analyzing their use in annotations of genes/gene products in model-organism databases   <cit> , the information content of biomedical concepts can be calculated by analyzing their use in annotations of textual mentions in biomedical documents .

a crucial difference between the craft corpus and many other gold-standard annotated biomedical corpora is that markup of concepts requires semantic identity. by this we mean that every annotation in craft is tagged with a term from an ontology or controlled vocabulary such that the text selected for the annotation is essentially semantically equivalent to the term; that is, each piece of annotated text, in its context, has the same meaning as the formal concept used to annotate it. in many other corpora, text is marked up even if the concept denoted is more specific than the concept used to annotate it; this approach is sometimes referred to as marking up all mentions “within the domain of” the given annotation class. for example, given a schema with a cell class , most corpora would annotate a mention of the word “erythrocyte” to that class. this results in semantic loss: it is not the case that the annotated text means the same thing as the associated semantic class. the size of the annotation schemas and the principle of semantic identity make assertions involving annotated concepts more valuable. for example, if the goal is to identify specific proteins expressed in specific cell types, annotations to generic categories such as “protein” or “cell” are not adequate.

though it may sound straightforward to mark up all mentions of a given annotation class, it is often difficult and can seem subjective. tateisi et al. have reported on the difficulty of distinguishing the names of substances from general descriptions of the substances in the construction of genia  <cit> , and there was relatively low agreement on what qualified as, e.g., activators, repressors, and transcription factors in the grec  <cit> . this is even more difficult when it involves identifying precise text spans for annotation. our annotators found that evaluating whether a span of text is semantically equivalent to a given term is easier than attempting to evaluate whether a piece of text refers to a concept that is subsumed by a more general schema class but not explicitly represented. it is for this reason that we emphasize annotation to an ontology/terminology rather than to a domain. domain boundaries are often ill-defined, which makes it difficult to evaluate whether a piece of text refers to a concept that “should be” in some ontology; thus, we annotate only to what actually is in an ontology, not to some abstract idea of its domain. for example, if the ontology being used to annotate the corpus contains a concept representing vesicles but nothing more specific than this, a textual mention of “microvesicle” would not be annotated, even though it is a type of vesicle; this is because this mention refers to a concept more specific than the vesicle concept . in other cases, a portion of a mention to a concept missing from an ontology can be marked up; for example, for the text “mutant vesicles”, “vesicles” by itself is tagged with the vesicle concept. we regard such an approach as a strength, as only text that directly corresponds to concepts represented in the terminology is selected. although experts might use such texts to make suggestions of new concepts to ontology curators, such activity was in general beyond the scope of the annotation work itself. however, we expect that the craft corpus could be exploited by ontology curators to find such missing concepts.

the craft corpus is distinguished by the quality and applicability of the schemas  used for annotation. many other corpora rely on concept schemas custom-made for their specific projects, often with representational idiosyncrasies; such schemas are not widely reusable for other purposes. some corpora, such as the grec and the event subset of genia, use schemas based, at least in part, on subsets of established external resources. the craft corpus is unique in that it relies on well-established, independently curated resources in their entirety. eight of these resources are formal biomedical ontologies developed within the sphere of the open biomedical ontologies  movement and are dedicated to faithfully representing the concepts within their respective domains, including five in the obo foundry that conform to an additional set of ontological principles. by predominantly annotating to widely used, high-quality terminologies, the craft corpus builds on years of careful knowledge representation work and is semantically consistent with a wide variety of other efforts that exploit these community resources.

in addition to using community-curated resources in our scheme, craft also annotates every mention of nearlyc every concept that appears in the texts. although such an approach seems intuitive , it is not used in a number of corpora. tanabe et al. have written that “one fundamental problem in corpus annotation is the definition of what constitutes an entity to be tagged” and cited the complex guidelines of the muc- <dig> named entity task as evidence  <cit> . in bioinfer, the focus is the annotation of relationships among genes, proteins, and rnas, and entities are only annotated if they are relevant to this focus and if they are named entities—a term itself with much baggage, however, if the arguments of primary events are other events or qualities that recursively have genes, proteins and/or rnas as arguments, these secondary events or qualities are annotated as “extended named entities”, but they are annotated only in such cases. in the pennbioie oncology corpus, a gene is only annotated if there is an associated variation event, and in the i2b2/va challenge corpus, only concepts lexicalized as complete noun phrases are annotated; e.g., “diabetes” is annotated in “she developed diabetes” but not in “she takes diabetes medication”.

the span selection guidelines for the concept annotations of the craft corpus also provide important advantages. given an initial anchor word as the basis for an annotation, the rules for deciding which adjacent words can be considered for inclusion in an annotation and which cannot are precise and purely syntax-based, and the decision as to whether to include one or more modifiers or modifying phrases rests solely on whether their inclusion would result in a direct semantic match to a concept in the terminology being used. unlike some other corpora , annotations in craft can be discontinuous, i.e., can be composed of two or more nonadjacent spans of text, though these must still abide by the same span-selection guidelines. use of discontinuous annotations allows us to ensure that only text that is semantically identical to a concept is marked, regardless of internal interruptions. in some corpora, there are unclear guidelines  for the text spans associated with an annotation. for example, in genia, “the inclusion of qualifiers is left to the experts sic judgment” for the task of entity annotation  <cit> , and in the i2b2/va challenge corpus, “p to one prepositional phrase following a markable concept can be included if the phrase does not contain a markable concept and either indicates an organ/body part or can be rearranged to eliminate the phrase”  <cit> . the craft specifications minimize subjective selections, and increase interannotator agreement on spans. craft text span-selection guidelines are quite extensive , but our biomedical-domain-expert concept annotators with no previous experience with formal linguistics were able to quickly learn them.

finally, few corpora have attempted to capture semantic ambiguity in concept annotations. the most prominent way in which craft represents concept ambiguity is in cases in which a given span of text could be referring to two  represented concepts, none of which subsumes another, and we have not been able to definitively decide among these. this occurs most frequently among the entrez gene annotations, in which many mentions of genes/gene products not grammatically modified with their organismal sources are multiply annotated with the entrez gene ids of the species-specific genes/gene products to which these mentions could plausibly refer. similar to genia, this multiple-concept annotation explicitly indicates that these cases could not be reliably disambiguated by human annotators and therefore are likely to be particularly difficult for computational systems. explicitly representing this ambiguity allows for more sophisticated scoring mechanisms in the evaluation of automatic concept annotation; for example, a maximum score could be given if a system assigned both insertion concepts to the aforementioned example and a partial score for an assignment of only one of these concepts. . however, we have attempted to avoid such multiple annotation by instead singly annotating such mentions according to improvised guidelines for specific markup issues . for example, some nominalizations  may refer either to a process  or to the resulting entity , both of which are represented in the so, and it is often not possible to distinguish among these with certainty; we have annotated such mentions as the resulting sequences except those that can only  be referring to the corresponding processes. a simpler case involves a text span that might refer to a concept or to another concept that it subsumes. in such a case, only the more general concept is used; for example, mus refers both to a organismal-taxonomic genus and to one of its subgenera, so a given mention would only be annotated with the genus; the rationale for this decision is that it is generally not safe to assume that the more specific concept is the one being mentioned.

ongoing and future work
in addition to the conceptual annotation that is described here and the syntactic annotation that we describe in a companion article  <cit> , there are multiple ongoing projects that add additional layers of annotation to the craft corpus data, all of which will be made available in future releases of the corpus:

· we have begun work on assertional annotation of the corpus, i.e., the markup of assertions among the annotated concepts by linking them via relations. we have encountered many difficult aspects in this task, which may be challenging to accomplish as consistently as the concept annotation. we seek to create this assertional markup using a methodology such that the annotations will be able to be programmatically translated into formal knowledge representations that can be stored and queried in an rdf knowledge base  <cit> .

· an extensive project is nearly complete to mark all coreference in the corpus. the two relations of coref  and appos  are marked. the guidelines for this portion of the work were adapted from the ontonotes guidelines, with the major difference that we did not utilize the category of generics. as we have discussed in relation to the guideline selection process for this task  <cit> , we maintain that in the biomedical domain, in which everything mentioned, including abstract concepts such as data, belongs in the domain of an ontology, the notion of genericity does not apply.

· discourse annotation on the sentence level, using the cisp/art schema  <cit> , is nearly complete. an early result of this work has been the finding that sequences of rhetorical moves can be characterized by finite state machines.

· the contents of all parentheses are being annotated with respect to a schema of twenty categories, including citations, data values, p-values, figure/table pointers, list elements, and others. we have previously presented the annotation procedure and the use cases for the various categories in the schema, as well as a classifier for determining category membership of contents of parentheses  <cit> .

· as a primary criterion in the selection of articles for the corpus was their use as evidential sources for ontological annotations of mouse genes/gene products in the mouse genome database , we have marked up the specific sentences within these articles upon which these annotations are based. motivated by a growing need for semiautomatic assistance in the curation of data in model-organism databases, we intend for this to serve as a gold standard for the training of systems to identify relevant evidential sentences in the biomedical literature.

furthermore, in the future, we intend to periodically update the annotations using current versions of the obos as well as correct errors that we find or are brought to our attention.

CONCLUSIONS
the concept annotation of the craft corpus, a collection of  <dig> full-length, open-access biomedical journal articles, is designed to serve as a high-quality gold standard for the training and testing of advanced biomedical nlp systems. in our corpus, we have created annotations for all mentions of nearly all concepts from nine prominent biomedical ontologies and terminologies, consistently created based on one set of guidelines. craft displays consistently high interannotator agreement, as evaluated by single-blind review by the lead semantic annotator of the primary annotators’ markup.

at approximately  <dig>  tokens in the initial 67-article release and  <dig>  tokens in the full set, the craft corpus is among the largest gold-standard annotated biomedical corpora, and unlike most others, the journal articles that comprise the documents of the corpus cover a wide range of biomedical disciplines. additionally, with a total concept annotation count of nearly  <dig>  in the initially released 67-article subset and of over  <dig>  in the full collection, the scale of our conceptual markup is also among the largest of all comparable corpora. along with the syntactic and coreferential annotations that have been created for the same set of journal articles, the concept annotations of the craft corpus have the potential to significantly advance biomedical text mining by providing a high-quality gold standard for nlp systems.

