BACKGROUND
whole genome sequencing remains a time- and cost-prohibitive method to screen patients for rare and causal genetic variation. thus, a current strategy involves targeted amplification of only the genomic regions of interest with subsequent focused sequencing. while technological advances have increased the throughput of sequencing platforms, the target enrichment step remains low-throughput and rate-limiting. increasingly, multiplexed microfluidic pcr assays have been employed to increase throughput of the target enrichment reactions in a rapid and cost-effective manner  <cit> . as an example, the  <dig>  access array system  is a chip-based platform that uses microfluidics to reduce sample requirements and increase scale, enabling multiple pcr amplification reactions to occur simultaneously. with  <dig> dna inlets and  <dig> primer inlets, each chip permits  <dig> simultaneous amplification products. to further increase the scale, reactions have been multiplexed with  <dig> primer pairs per inlet, resulting in a total of  <dig>  unique amplification products. thus, multiplex microfluidic pcr assays permit target enrichment at an increased scale, which can then be integrated with next generation sequencing  technologies, where the replicated pcr products are sequenced.

a primary goal in data analysis is to maximize the accuracy of variant calling from the targeted sequence data generated. false variant calls can arise from numerous sources, such as errors in base calling, or artifacts that occur when aligning samples’ reads to the reference genome  <cit> . in addition, multiplexed microfluidic pcr assays have unique factors that can increase potential for false variant calling. first, the polymerase can incorporate incorrect nucleotides during target amplification. the subsequent pcr product, now containing an error, can itself be used as the target for additional rounds of amplification. if the product accumulates in sufficient quantity, the product will be sequenced. this altered sequencing read is now a false positive finding, a technical artifact masquerading as a variant. second, the process of target amplification for focused sequencing inherently creates multiple duplicate sequencing reads. compared to whole genome sequencing – where duplicate reads can be removed and the presence of unique sequencing reads increases confidence in variant calling – the presence of duplicates reads that cannot be removed in targeted sequencing strategies can confound the analysis. together, these limitations necessitate intense scrutiny of the sequence data to minimize false variant calling.

following multiplexed microfluidic pcr amplification and subsequent sequencing, a common approach to data analysis involves using commercial software for sequence alignment and variant calling  <cit> . subsequently, filtering strategies have been developed to improve accuracy of variant calling. while developing a filter, halbritter and colleagues studied the distributions of allele balance  and alternative allele depths for heterozygotes  <cit> . they suggested that an allele balance of 20 %--a ratio of alternative alleles to total alleles of > <dig> —be required as a filtering mechanism. in addition, they recommended requiring that the alternative allele appear in at least  <dig> reads . after discovering variants meeting this criteria, manual inspection of the alignments is necessary to distinguish between primer artifacts and true variation. even following careful and labor-intensive filtering strategies, many identified variants are subsequently not confirmed by sanger sequencing.

thus, there is a need to develop a more accurate and more fully automated variant calling pipelines when working with microfluidic pcr based ngs studies. in this paper, we describe targeted sequencing support vector machine , an open source pipeline based on genome analysis toolkit   <cit>  that seeks to achieve this goal. our pipeline is a synthesis of the ideas from previous studies calling variants from microfluidic pcr coupled with ngs  <cit> , many suggestions from gatk’s best practice filter, and gotcloud’s  <cit>  svm filtering strategy. our pipeline trims adapters from reads, aligns to the reference genome, performs variant calling, and uses a novel svm which models allele balance and features summarized by gatk to perform variant quality filtering. tarsvm is further improved by incorporating knowledge from the  <dig> genomes  <cit>  and the exome aggregation consortium   to maximize the number of variants to learn the svm’s decision boundary. using two independent cohorts of participants with kidney phenotypes, we constructed this pipeline and compared it to existing and alternative methods. tarsvm was then validated using sanger sequencing on a third cohort. the software is available on github at https://github.com/christopher-gillies/targetspecificgatksequencingpipeline.

RESULTS
overview
in this study, our goal was to develop a more efficient and accurate filtering pipeline to identify true variants identified by microfluidic pcr followed by next generation sequencing . to achieve this, we used two existing ngs datasets from cohorts of subjects with kidney phenotypes who had undergone targeted diagnostic screening of disease-associated gene panels using microfluidic pcr + ngs. ngs-based sequence reads from these two existing datasets had previously evaluated using the clc genomics workbench™  including quality control via manual inspection of the alignments. variants called with this approach that were also predicted to be pathogenic , then underwent sanger sequencing for confirmation. variants confirmed by sanger sequencing were used as a gold standard to analyze predictions from existing methods and our new approach. we then validated our approach using a third cohort with kidney disease who had existing sequencing data. additionally, to obtain an external evaluation of the sensitivity of different methods, we used an exome array based technology as a gold standard for which to compare. all analyses were performed on the variant call format  file generated from our pipeline  except where noted otherwise.

terminology
in the following discussion, we assume that the ground truth is the set of variants that underwent sanger sequencing. the below statements also apply if the exome array dataset was used as the ground truth. please note that we use tarsvm as a placeholder for any of the filtering methods that we evaluate.

the set of variants that underwent sanger sequencing has two subgroups  sites validated by sanger sequencing and  sites not validated by sanger sequencing.a site that is found in the sanger sequencing data and passed tarsvm is a true positive .

a site that is validated in the sanger sequencing data but not identified by tarsvm is a false negative .

a site not validated in sanger sequencing data and not identified by tarsvm is a true negative .

finally, a site that is not validated in the sanger sequencing data, but identified by tarsvm is a false positive .

sensitivity = tp/, the proportion of sanger validated sites identified by tarsvm.

specificity = tn/, the proportion of sites that were not validated via sanger that were also rejected by tarsvm.

false discovery rate = fp/, the proportion of sites identified by tarsvm and underwent sanger sequencing that were not validated by sanger.

accuracy = as /, the proportion of all sites that underwent sanger sequencing that tarsvm predicted correctly.



dataset descriptions
the first cohort  was comprised of  <dig> subjects,  <dig> with nephrotic syndrome  enrolled in either the nephrotic syndrome study network   <cit>  or c-probe and ancestry matched  <dig> population controls from the  <dig> genomes project. in this cohort,  <dig> genes implicated in monogenic ns were amplified using fluidigm access array and sequenced using illumina ngs instruments. the neptune cases and 1000g controls have been previously described  <cit> . the second cohort  consisted of  <dig> subjects sequenced across  <dig> genes associated with congenital anomalies of kidney and urinary tract . to validate our svm based-filtering, we applied our approach to a third cohort  comprising  <dig> south african subjects with ns sequenced across  <dig> genes implicated in monogenic ns nephrotic syndrome in the same manner as the ns cohort.

next generation sequencing statistics
in the ns cohort, 95 % of samples had a mean depth greater than  <dig>  and 80 % of samples had a mean depth greater than  <dig>  in the cakut cohort, 95 % of samples had a mean depth of at least  <dig>  and 80 % of samples had a mean depth of at least  <dig>  in the sa cohort, 95 % of the subjects had a mean depth greater than  <dig>  and 80 % of subjects had mean depth of at least  <dig> 

accuracy of clc-based filtering strategy
a variant filtering strategy using clc genomics workbench™  paired with manual inspection of potential variants sites yielded a high false discovery rate . specifically, in the ns cohort,  this filtering strategy identified  <dig> sites that harboring missense, splice variants, or stop gained variants classified as putatively pathogenic and which thus were subsequently were sanger sequenced. of these  <dig> variants,  <dig>  were validated with sanger sequencing and  <dig>  failed validation. overall there was a 42 % fdr. the vast majority of variants were found in the heterozygous state in a single individual . however for the rare cases where multiple subjects had a variant at a site, we defined a site as true if at least one variant was sanger confirmed at that site. we also limited our analysis to single nucleotide variants, because of the relatively few indels identified.

of the  <dig> cakut subjects,  <dig> were excluded because of low mean depth. the clc-workbench-based analysis identified  <dig> rare sites of variation that were putatively pathogenic. of these,  <dig> were validated with sanger sequencing . thus, the fdr for the cakut cohort was 57 %.

sensitivity of tarsvm
we first sought to determine whether our svm-based strategy was sensitive to detect polymorphic sites  and sites that have a non-reference allele on exactly one chromosome within the target geneset. to do this, we took advantage of the fact that  <dig>  ns cohort participants had been previously genotyped using the illumina human exome- <dig> v1a . in the exome chip array, there were  <dig> potential variant sites among the  <dig> genes. in our  <dig> ns subjects,  <dig> of these sites were monomorphic  and  <dig> were polymorphic. of the  <dig> polymorphic sites,  <dig> had a non-reference allele count of one. we used this independent genotyping dataset as a gold standard so that we could effectively benchmark our method’s sensitivity. variants with a non-reference allele count of one are rare. specifically, 69 % of the variants have an allele frequency in the exac database less than 10^- <dig> .

we evaluated three strategies for their power to detect polymorphic  and singleton sites . the three strategies were “no filter”, the “default genotype filter”, and “tarsvm” . the microfluidic pcr platform, without any filter, identified 95 % of the polymorphic sites on the exome chip. the default genotype filter identified 93 % of sites, and tarsvm identified 92 % of all sites. when focusing on singletons, the microfluidic pcr platform with no filter, default, and tarsvm identified  <dig>   <dig>  and 89 % of singletons, respectively. thus applying tarsvm to microfluidic pcr-derived ngs data has acceptable sensitivity to detect rare variants.fig.  <dig> sensitivity of detecting variant sites in exome chip dataset of  <dig> subjects. panel  shows the sensitivity of detecting sites  an allele count greater than one in the exome chip dataset of  <dig> subjects using microfluidic pcr for the same subjects. panel  shows a similar sensitivity analysis, except it is limited to sites with an allele count of exactly one . please note that the “no filter” bar is an upper bound for all methods except gotcloud svm. the overall conclusion from these two plots is that tarsvm’s sensitivity is very close to the most sensitive methods for common and rare variants



accuracy of tarsvm: comparison to alternative methods
we next assessed the sensitivity and specificity of our method in comparison to five other filtering strategies . we obtained existing sequence data from microfluidic ngs performed on ns and cakut cohort. variants had been called using clc-software based method with manual alignment inspection. called variants from ngs data were confirmed by sanger sequencing. we considered these sanger-confirmed. we considered these sanger variant calls as the gold standard against which we would compare the different methods.fig.  <dig> sensitivity and specificity of filters using sanger sequenced variants as gold standard for ns cohort. panel  shows the sensitivity of six different filters to detect  <dig> variants sequenced using sanger. the default genotype filter has the highest sensitivity, which is not surprising because it was the principal norm for determining whether or not a variant should undergo sanger sequencing from the clc genomics workbench™. tarsvm is nearly as sensitive as the default genotype filter. panel  displays the specificity of six filters. tarsvm is significantly more specific than the default genotype filter

fig.  <dig> accuracy and fdr of filters using sanger sequenced variants as gold standard for ns cohort. panel  shows the accuracy of six different filters for  <dig> variants sequenced using sanger. the svm filter is more accurate than other filter methods. panel  illustrates the decreased false discovery rate of tarsvm as compared to other filters

fig.  <dig> sensitivity and specificity of filters using sanger sequenced variants as gold standard for cakut cohort. panel  shows the sensitivity of six different filters for  <dig> variants sequenced using sanger. the default genotype filter has the highest sensitivity, and tarsvm has comparable sensitivity with other methods. panel  displays the specificity of six filters. tarsvm is substantially more specific than other filters

fig.  <dig> accuracy and fdr of filters using sanger sequenced variants as gold standard for cakut cohort. panel  shows the accuracy of six different filters for  <dig> variants sequenced using sanger. tarsvm is more accurate than other filter methods. panel  illustrates the decreased false discovery rate of tarsvm as compared to other filters



the filtering methods we compared were variant quality score recalibration   <cit> , gotcloud’s svm  <cit> , gatk’s best practice hard filter, the default variant quality filter  <cit> , the stringent genotype quality  <cit> , and tarsvm . first we investigated the sensitivity and specificity using the ns cohort . tarsvm was  <dig>  % sensitive and  <dig>  % specific. only the default genotype filter had higher sensitivity  than tarsvm. while tarsvm was the most specific filter, the false positive rate was still substantial at  <dig>  %.

figure  <dig> shows the accuracy and false discovery rate for the same analysis. tarsvm’s accuracy was  <dig>  %, which is  <dig>  % higher than the stringent genotype filter . furthermore, it reduces the false discovery rate from  <dig>  %  to  <dig>  % . figures  <dig> and  <dig> display the same comparison using the cakut cohort. the performance of all methods decreased on the cakut data. tarsvm’s sensitivity was about  <dig>  times lower than the most sensitive filter , but its specificity was  <dig>  times higher than the second most specific filter . tarsvm’s accuracy was  <dig>  times higher than the second most accurate filter . overall, the accuracy of tarsvm was  <dig>  %. tarsvm’s false discovery rate  was  <dig>  times lower than the stringent genotype filter’s fdr.

reduction in number of variants to validate using sanger sequencing
we next sought to determine the magnitude by which utilizing tarsvm would reduce our pool of variants that would subsequently need to undergo sanger sequencing. we first applied either the default genotype or tarsvm quality filter to the variants called by gatk coming from the ns and cakut cohorts. with these high-quality variants called, we then applied a previously described “default” variant pathogenicity filter to both cohorts, which utilized a combination of allele frequency thresholds and functional prediction scores to protein-altering variants  <cit> .

table  <dig> displays a summary of applying the pathogenicity filter to the variant calls sets emerging from the default genotype filter and tarsvm. overall there were  <dig> variants in the ns cohort called by both the default and tarsvm. tarsvm filter called  <dig> variants not called by the default genotype filter, and the default filter called  <dig> variants uniquely. then a previously described pathogenicity filter was applied  <cit> . as compared to the default genotype filter, tarsvm reduced the number of variants required to validate by 22 % . in the cakut cohort, filters were in consensus for  <dig> variants. tarsvm called  <dig> variants not called by the default genotype filter, and the default filter called  <dig> variants uniquely. in this cohort, when applying the same pathogenicity filter, tarsvm reduced the number of variants required for validation by 32 % .table  <dig> reduction in number of variants to validate with sanger sequencing for the ns and cakut cohorts

the first column describes the cohort for which the row corresponds. the second column identifies the variant quality filter applied to the dataset. the total variants column refers to the total variants that were called by gatk. the next column shows the number of variants passing a particular variant quality filter for a specific cohort. eligible variants referrers to all missense and loss of function variants considered for the analysis, excluding frame shift mutations that are considered in the pathogenicity filter. the final column for the pathogenicity filter column displays the number of variants passing having an allele frequency of less than 1 % across all population in the exome variant server, and the variant was either loss of function or predicted to be deleterious by two of mutationtaster, polyphen <dig>  and sift



validation of svm filter
we next applied our pipeline to the ngs data from the 75-member sa cohort. after applying tarsvm, we then applied the same pathogenicity filter as used above, which identified  <dig> rare variants for sanger sequencing. of these  <dig> variants,  <dig> were validated with sanger sequencing, which gives tarsvm a fdr of 8 %. tarsvm filter also correctly predicted common variants implicated in apol1-related kidney disease  <cit> , tarsvm filter thus had an overall fdr of 5 % . an additional three variants were also identified when performing the sanger sequencing, and tarsvm correctly predicted two of these variants. thus, the overall accuracy was  <dig> of  <dig> or 91 %.

discussion
we used a support vector machine approach to develop tarsvm, a fully automated variant quality filtering pipeline that can be applied to targeted next generation sequencing data resulting from microfluidic pcr technology. existing variant quality filtering strategies designed for whole genome sequencing such as the svm-based approach in gotcloud’s alignment pipeline  <cit> , variant quality score recalibration   <cit> , or gatk’s best practice hard filter. gotcloud and vqsr both use machine learning based methods to model true and false variants, and gatk’s hard filter uses specific thresholds for various variant quality metrics to discriminate true from false variants. these approaches do not model microfluidic specific problems well, thus they perform insufficiently on microfluidic pcr data . our method achieved similar sensitivity to other variant read quality filtering methods. importantly, tarsvm substantially increases the specificity as compared to other methods.

in terms of the variant filtration tarsvm is most similar to gotcloud in that they both use hard filters to identify negative training examples and use known variants to identify positive training examples. after selecting the training examples, both methods use an svm to learn a decision boundary and finally classify all the variants as true or false using this decision boundary. while both algorithms appear similar at this level of abstraction, at a more detailed level of analysis, the differences are manifold and explain the reasons why tarsvm outperforms gotcloud using microfluidic pcr data.

in microfluidic pcr data, a small number of genes are targeted, thus there is a risk of not having enough sites to adequately learn the svm model. tarsvm addresses this risk by incorporating information from exac and dbsnp into the positive training example selection step. gotcloud does not use this information, which is not a problem on the scale of whole genome/exome sequencing. in terms of negative training example selection, tarsvm uses the normalized allele dosage test   and mean alternative allele depth across heterozygotes to improve the identification of low quality sites. in addition, tarsvm also uses sites that are not labeled “pass” in exac and the 1000g as negative training examples when there is some additional evidence that the site is low quality. tarsvm does not include strand bias scores in the filtration process either which is important for microfluidic pcr since only one strand is being amplified. additionally, the thresholds for filtering have been tuned primarily using prior information for microfluidic pcr. the nad and alternative allele depth are also included as features in the model fitting process, which improves the ability of the svm to learn a versatile decision boundary. finally, tarsvm applies a genotype-level filter to remove very low quality variant calls .

tarsvm represents a useful step forward in a number of ways.  we have created an open source, automated pipeline that transforms raw sequence reads to a filtered vcf file. the pipeline trims adaptors, aligns to the reference genome, and performs variant calling following gatk’s best practice recommendations wherever possible.  tarsvm improves positive and negative training example selection over gotcloud for microfluidic pcr data.  we created the nad feature improves the classification of sensitivity and specificity of detecting variants both for hard filtering and for learning the svm.  tarsvm the overall improves the overall accuracy of variant calling. by incorporating knowledge from the  <dig> genomes and exome aggregation consortium and modeling microfluidic-pcr biases such as allele balance, tarsvm more accurately models true and false variants in this application. thus tarsvm obtains an improved decision boundary, which distinguishes between true and false variants.  by fully automating the process of transforming raw sequence reads to a filtered vcf, the tarsvm pipeline removes the dependency of having a user manually reject alignment artifacts as compared the clc-workbench approach. specifically, the pipeline’s read trimming also removes adapters from reads where at least six bases match the adapter starting from the end of the read. this trimming step dramatically reduces the need to manually inspect variants contaminated by adapter sequence.  by aligning to the whole genome reduces alignment artifacts because a read can align to its correct genomic coordinates rather than a region of high similarity as compared to the clc-workbench based approach.  finally, since tarsvm has a higher specificity than other methods, false variants are not carried forward for consideration for potential pathogenicity. this reduces the time and costs associated with sanger sequencing confirmation. additional file 3: supplemental note # <dig> and additional file 4: supplemental note # <dig> provide more details on the impact of these unique aspects on the performance of tarsvm.

in the cakut cohort, there was a decreased accuracy of variant calling for all filtering methods tested. we believe this is largely due to the increase number of primers used per inlet on the microfluidic pcr platform. in the cakut cohort,  <dig> primers per inlet were used rather than the  <dig> primers per inlet for the ns cohort. we posit that unintended interactions between primers within each reaction chamber increased the introduction and amplification of false variants. it is possible that sample size may be affecting the results, but it seems more plausible that the interacting primers are the main cause.

when comparing the call sets emerging from tarsvm and default genotype filters, we found that tarsvm call set is not simply a subset of the default filter. thus, the default filter may not be capturing all true variants, resulting in false negatives. because we used variants that were manually inspected and passed the default genotype filter via clc genomics workbench™, it is not surprising the sensitivity we calculated was higher for the default genotype filter . but because of these false negatives, tarsvm’s true sensitivity may not be lower than the default genotype filter. in fact, when performing validation on the sa cohort, we did not begin with a predefined set of sanger sequencing variants. in this study, the default genotype filter had sensitivity for the sa cohort of 90 % , whereas tarsvm’s sensitivity was 95 % .

one limitation of this study is that it only investigated the effectiveness of our pipeline on data emerging from the fluidigm’s access array. however, our approach should work for other high-depth targeted sequencing technologies where duplicate reads cannot be removed. the utility of this pipeline is also currently limited to assays where duplicate reads are expected. one possible future direction for this pipeline would be to expand support for duplicate removal, because this will expand support beyond microfluidic pcr-based approaches for targeted rare variant detection   <cit> .

CONCLUSIONS
overall tarsvm represents a synthesis of knowledge from high quantity variant call sets and previous filtering strategies. tarsvm achieves similar sensitivity, but substantially improved specificity over other filtering methods for microfluidic pcr data. at the same time, tarsvm is not a panacea for all microfluidic pcr or ngs-based issues. the fdr in the ns cohort improved from  <dig> to 19 % for our svm approach, and for the cakut cohort the fdr improved from  <dig> to 37 % fdr with our svm approach. while these fdrs are much improved, the fdr estimates derived here illustrate the need to continue to use sanger sequencing to validate variants derived from microfluidic pcr data. however, by automating the process and decreasing the false positive rate, the tarsvm pipeline reduces both sequencing and analytic costs and increases confidence in the variants called in this manner. finally, it is available as an open source pipeline for read trimming, aligning, variant calling, and variant quality filtering on github at https://github.com/christopher-gillies/targetspecificgatksequencingpipeline.

