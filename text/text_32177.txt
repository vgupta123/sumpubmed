BACKGROUND
research projects in modern molecular biology rely on increasingly complex combinations of computational methods to handle the data that is produced in the life science laboratories. a variety of bioinformatics databases, algorithms and tools is available for specific analysis tasks. their combination to solve a specific biological question defines more or less complex analysis workflows or processes. software systems that facilitate their systematic development and automation  <cit>  have found a great popularity in the community.

more than in other domains the heterogeneous services world in bioinformatics demands for a methodology to classify and relate resources in a both human and machine accessible manner. the semantic web  <cit> , which is meant to address exactly this challenge, is currently one of the most ambitious projects in computer science. collective efforts have already lead to a basis of standards for semantic service descriptions and meta-information.

most importantly, the world wide web consortium  set up a number of working groups addressing different technological aspects of the semantic web vision. among their outcomes are the semantic annotations for wsdl  recommendation  <cit> , the resource description framework  specification  <cit> , and the web ontology language   <cit> . while sawsdl is designed to equip single entities with predicates, rdf and the more powerful owl formally define relationships between the resources of a domain.

without a reasonably large set of semantically annotated  services, it is, however, difficult to evaluate the semantic web technologies with significant results and develop practical software for the client side. on the other hand, providers are not willing to put effort in annotating their services as long as they can not be confident which technologies will finally become established. community initiatives like the semantic web services  challenge  <cit>  or the semantic service selection contest   <cit>  address this problem. they provide collections of services, domain information and concrete scenarios that the different participants, being developers of methodologies for different semantic web aspects, have to deal with. in the scope of the s <dig> contest, opossum  <cit> , an "online portal to collect and share sws descriptions"  <cit> , was set up. it aims at collecting, sharing, editing, and comparing sws descriptions within a community infrastructure in order to collaboratively evaluate and improve sws formalisms. as of march  <dig>  however, opossum does not list any bioinformatics services.

an example of a knowledge base particularly capturing bioinformatics data types and services are the constantly evolving namespace, object and service ontologies of the biomoby service registry  <cit> . biomoby's aim is to "achieve a shared syntax, shared semantic, and discovery infrastructure suitable for bioinformat-ics"  <cit>  as a part of the semantic web. originating from the early 2000s, the  <dig>  moby-s spec-ifications, however, do not adhere to the semantic web standards that have been developed in the last years. consequently, the s-moby branch of the project came into being to migrate to common technologies. it has recently been merged into the sswap   <cit>  project, which aims at providing life science knowledge using standard rdf/owl technology. sswap provides a number of own ontologies, but also incorporates third-party domain knowledge like the moby-s object and service ontologies.

generally, the development of ontologies in the bioinformatics community is already very promising. projects like the gene ontology   <cit>  and the open biomedical ontologies   <cit>  have already become widely used and also, for instance, incorporated by the sswap project. the majority of publicly available ontologies in the bioinformatics domain is, however, designed for the classification of scientific terms and the description of actual data sets, and not for  descriptions of service interfaces and data types.

the lack of properly semantically annotated services has evidently already been recognized by the community, as different projects are commencing to address the issue. for instance, major service providers like the european bioinformatics institute  plan to extend their service infrastructure to provide meta-information conforming to semantic web standards. other initiatives aim at setting up stand-alone collections of service uris and corresponding annotations, without influencing the service infrastructures as such.

while the provision of semantically annotated services is mainly the service providers' task, on the client side software is needed that fully utilizes the available semantic information in order to provide helpful tools to the in silico researcher. the challenge for user-side software is to abstract from the underlying semantic web technology again and provide the achievements in an intuitive fashion.

a simple but useful feature building upon semantic information about services is the categorization of services according to different criteria. a corresponding functionality has already been available in the meanwhile discontinued biospice dashboard, where it was possible to arrange services by location, provider, function, or i/o type . the biospice project is a meanwhile abandoned initiative that focused on the development of computational models for intracellular processes. besides the provision of mere "access to the most current computational tools for biologists"  <cit> , the work also aimed at integrating the services into a convenient graphical user environment, called the biospice dashboard. thus, the need of multi-faceted service classification has been recognized several years ago, but until present services hardly provide appropriate meta-information.

more advanced examples of utilizing semantic information about services are, for instance, available in the scope of the sws challenge  <cit> . among others, projects like swe-et   <cit>  and wsmx  <cit>  participate in the challenge, adressing both discovery and mediation scenarios for semantic web services. however, these solutions demand quite some technical understanding from the user, which hampers the uptake by a larger biological user community.

as an example from the bioinformatics domain, the biomoby project provides a simple composition functionality for its services.  <cit> . with the moby-s web service browser  <cit>  it is, e.g., possible to search for an appropriate next service, while in addition the sequence of actually executed tools is recorded and stored as a taverna  <cit>  workflow. a substantial drawback of this approach is, however, its restriction to the services that are registered in the respective platform.

in this paper, we present our approach to semantics-based service composition in the bio-jeti platform  <cit> . by integration of automatic service composition functionality into an intuitive, graphical process management framework, we are able to maintain the usability of the latter for semantically aware workflow development. furthermore, we can integrate services and domain knowledge from any kind of heterogeneous resource at any location, and are not restricted to any semantically annotated services of a particular platform.

this manuscript is structured as follows: in the next section, results and discussion, we discuss two examples that we developed in bio-jeti with the help of a semantics-aware workflow synthesis method and model checking: a simple phylogenetic analysis workflow and a more sophisticicated, highly customized phylogenetic analysis process based on blast and clustalw. subsequently, the conclusion deals with directives for the future development of our approach. finally, the methods section describes the applied techniques in greater detail.

RESULTS
the approach to semantics-based service composition that we present in this paper builds upon the bio-jeti  <cit>  framework for model-based, graphical design, execution and management of bioinformatics analysis processes. it has been used in a number of different bioinformatics projects  <cit>  and is continuously evolving as new service libraries and service and software technologies become established. technically, bio-jeti uses the jabc modeling framework  <cit>  as an intuitive, graphical user interface and the jeti electronic tool integration platform  <cit>  for dealing with remote services. using the jabc technology, process models, called service logic graphs  are constructed graphically by placing process building blocks, called service independent building blocks , on a canvas and connecting them according to the flow of control. figure  <dig> shows a screenshot of the graphical user interface of the jabc. slgs are directly executable by an interpreter component, and they can be compiled into a variety of target languages via the genesys code generation framework  <cit> . as figure  <dig>  illustrates, genesys provides the means for transforming slgs into native, stand-alone programm code  as well as into other workflow languages .

workflow development in bio-jeti is already supported by several plugins of the jabc framework, for instance providing functionality for component validation or step-wise execution of the process model for debugging purposes. now we are going to exploit further jabc technology, such as model checking and workflow synthesis, in order to enable bio-jeti to support the development of processes in terms of service semantics.

model checking  <cit>  can be used for reasoning about properties of process models. this can help to detect problems like undefined data identifiers, missing computations, or type mismatches. solving these problems might require the introduction of further computational steps, for instance a series of conversion services in case of a data type mismatch. the approach here is to automate the creation of such process parts via workflow synthesis methodology  <cit>  that allows for the automatic creation of  workflows according to high-level, logical specifications. figure  <dig>  illustrates the relationship between our specification language sltl  and the actual bio-jeti workflow models, the slgs: provided with a logical specification of the process and semantically annotated services, the workflow synthesis algorithm generates linear sequences of services, which can be further edited and combined into complex process models on the slg level.

for the study that we present in this paper we used a sib collection offering various remote and local services. examples for contained remote services are the data retrieval services provided by the ebi   <cit> , sequence analysis algorithms offered by bibiserv   <cit> , web services hosted by the ddbj   <cit> , and some tools of the emboss suite  <cit> . on the local side, there are specialized components such as visualizer for phylogenetic trees  <cit>  and more generic ones like sibs that realize user interaction or functionality for file management. table  <dig> lists the fragment of the library that is relevant for our examples.

in the jabc, the sibs are displayed to the user in a taxonomic view, classified according to their position in the file system  or to any other useful criterion, like the provider or the kind of service. the sibs have user-level documentation, explaining what the underlying tool or algorithm does, that is derived directly from the provider's service descriptions. in addition, the sibs provide information about their input and output types via a specific interface. this is already an integral part of the semantic information that helps to systematically survey large sib libraries and it is used by our process synthesis and model checking methods. it is, in addition, possible to add arbitrary annotations to the sib instances and by doing so providing further  information that is taken into account by our formal methodologies.

the knowledge base that is needed for the process synthesis consists, furthermore, of service and type taxonomies that classify the services and types, respectively. taxonomies are simple ontologies that relate entities in terms of is-a and has-a relations. these classifications provide sufficient information for our synthesis methodologies.

we assume simple taxonomies for our examples, which have the generic owl type thing at the root. going downwards, classifications are introduced, for instance refining the generic type into integers and strings, whereas the latter is further distinguished into alignments, trees, sequences, tool outputs, and so on. figure  <dig> shows the service taxonomy for the services that we use in our examples, edited in the onted ontology editor plugin of the jabc. the corresponding type taxonomy classifying the involved data types is given in figure  <dig> 

the basic input and output information for the services is defined in terms of the data types contained in the type taxonomy. table  <dig> lists the set of data types that is relevant for our examples. the services are characterized by input-output-pairs of types, where the input or output may well be empty , respectively. services may also provide multiple possible transformations and thus achieve polymorphism. for instance, bibiserv's clustalw can process sequences in fasta or in sequenceml format, and produces a fasta or alignmentml output, accordingly.

example 1: a simple phylogenetic analysis workflow
when developing bioinformatics analysis workflows, users often have a clear idea about the inputs and final results, while their conception of the process that actually produces the desired outputs is only vague. figure  <dig>  shows a stub for a workflow: the start sib  is an input dialog for a nucleic or amino acid sequence, which is followed by a sib running a blast query with the sequence having been input in order to find homologous sequences. the workflow ends by invoking archaeopteryx to display a phylogenetic tree . the configuration of the sibs is sound at the component level, as the local checker plugin  confirms. however, there are errors regarding the correct configuration of the model as a whole, as the required input type for archaeopteryx, some phylogenetic tree format, is not produced previously in the process. this is detected by our model checker gear , that checks a temporal formula covering the following constraint :

  

an experienced bioinformatician might be aware of the problem immediately, due to his familiarity with the involved tools. this is, however, only a small workflow. an automatic, semantically supported detection of misconfigurations and modeling errors unfolds its full potential when processes become more complex, and it is not feasible for the in silico researcher to dive into the documentations of all services or to explore their behaviour by trial-and-error executions.

once detected, there are different ways to fix the problem. one can look for replacements for one of the involved sibs that essentially compute the same results, but provide them in a data format that fits in the surrounding process. another approach, assuming that the user has chosen these services for good reason, is to search for a sequence of additional services that resolve the mismatch and insert them into the process. such data mediation sub-workflows are usually linear. they can consist of type conversions that simply adapt the involved data, or also of real computational services when the match can not be realized so easily.

as a means for resolving the violation of property *, the example process model stub implies a process specification adequate as input for our workflow synthesis algorithm . in a high-level formulation, it reads:

  

utilizing the semantically annotated sib collection and domain information from above, and computing the shortest service combination that satisfies the specification, our synthesis algorithm proposes the following simple four-step workflow for the above query :

 <dig>  extract the ids of the hits from the blast result .

 <dig>  turn the matches into a comma-separated list.

 <dig>  call dbfetch .

 <dig>  run emma .

the generated sequence of sibs can now be inserted into the process stub and all parameters configured appropriately. as figure  <dig>  shows, neither the local nor the model checking does reveal errors any more. the process is now ready for execution. figure  <dig> illustrates the corresponding runtime behaviour: the workflow starts by asking the user for a query sequence, then performs a similarity search, data retrieval and sequence analysis before it finally displays the resulting phylogenetic tree.

example 2: blast-clustalw workflow
a simple phylogenetic analysis like in the previous example is an often recurring element of complex in silico experiments. in many cases, however, a customized, more specific processing of intermediate results is required, like in the blast-clustalw workflow  <cit>  that is one of the ddbj's sample workflows for the web api for bioinformatics  <cit> . it is the archetype for our second example.

the blast-clustalw workflow  <cit>  has the same inputs and outputs as the simple phylogenetic workflow from example 1: it finds homologuous sequences for an input dna sequence via blast and computes a hypothesis about the phylogenetic relationship of the obtained sequences . the proposed analysis procedure consists of four major computation steps :

 <dig>  call the blast web service to search the ddbj database for homologues of a nucleic acid sequence. the input is a 16s rna sequence in fasta format, the output lists the database ids of the similar sequences and basic information about the local alignment, e.g. its range within the sequences.

 <dig>  call the getentry web service with a database id from the blast output to retrieve the corresponding database entry.

 <dig>  extract accession number, organism name and sequence from the database entry. trim the sequence to the relevant region using the start and end positions of the local alignment that are available from the blast result.

 <dig>  call the clustalw web service to compute a global alignment and a phylogenetic tree for the prepared sequences.

due to the loop that is required for repeating steps  <dig> and  <dig> a certain number of times, this process can not be created completely by our current synthesis algorithm, which is restricted to produce linear sequences of services. it is, however, possible to predefine a sparse process model in which the looping behaviour and other crucial parts are manually predefined, and to subsequently fill in linear parts of the process automatically.

at this state of the process, the local checking of the components detects no errors, but the model checker reveals problems : as in the previous example, the sib archaeopteryx uses a variable tree, which is not defined before. moreover, the sibs extract organism and extract sequence use a variable ddbjentry, which is defined with an incompatible type. details on the model checking procedure can be found in the methods section.

to resolve the first problem, we proceed similar as in example  <dig>  by providing the synthesis algorithm with a temporal formula that asks for a sequence of services that takes a set of sequences as input  and produces a phylogenetic tree . as figure  <dig>  shows, a single call to emma is one of the  sequences that fulfils this request.

the second problem is the presence of a type ddbjfasta where the type ddbjentry is expected. to solve this mismatch, we ask our synthesis algorithm for a way to derive the latter from the former. it returns with an empty result , which means that our sib collection can not provide an appropriate sequence of services. we exclude the type ddbjfasta and the sib getfasta_ddbjentry, by which is it produced, and try our luck with the type ddbjaccession, which has been defined last, as starting point for the synthesis. the answer is a service sequence consisting of the sib getddbjentry , by which we can now substitute the improper data retrieval sib from above.

the bottom of figure  <dig> shows the completely assembled process. we omit to demonstrate its execution behaviour, as it is very similar to that of example  <dig> 

discussion and perspectives
by means of two examples, the previous sections demonstrated the local checking, model checking and workflow synthesis methodology that is currently available in the jabc framework and thus part of bio-jeti. the local checker plugin provides domain-independent functionality and is already conveniently integrated in the framework. we are now working on a user-friendly integration of the domain-specific model checking and synthesis techniques, especially with regard to the bioinformatics application domain. this ongoing work spans three dimensions, which are discussed in the following sections: domain modeling, model checking, and model synthesis.

domain modeling
this dimension is the heart of making information technology available to biologists, as it enables them to express their problems in their own language terms – on the basis of adequately designed ontologies. it raises the issue where the domain knowledge ideally comes from. it is, of course, possible for each user to define custom service and type taxonomies, allowing for exactly the generalization and refinement that is required for the special case. however, as the tools and algorithms that are used are mostly third-party services, it is desirable to automatically retrieve domain information from a public knowledge repository as well. therefore we plan to incorporate knowledge from different publicly available ontologies, like biomoby  <cit>  and sswap  <cit> , and to integrate it into the service and type taxonomies for use by our synthesis methodology.

it is, of course, also necessary that the services themselves are equipped with meta-information in terms of these ontologies. again, we are looking at biomoby with interest: numerous institutions have registered their web services at moby central, describing functionality and data types in pre-defined structures using a common terminology. although biomoby does not yet use standardized description formalisms like sawsdl, it is already clear that there is semantic information available that we can use as predicates for automatic service classification.

furthermore it will be interesting to consider the incorporation of more content-oriented ontologies like the gene ontology  <cit>  or the obo   <cit>  into our process development framework. this would allow the software to not only support the process development on a technical level, but also in terms of the underlying biological and experimental questions. additional sources of information, like the provenance ontologies of  <cit>  could be also easily exploited by our synthesis and verification methods.

model checking
this dimension is meant to systematically and automatically provide biologists with the required it knowledge in a seamless way, similar to a spell checker which hints at orthographical mistakes – perhaps already indicating a proposal for correction. immediate concrete examples of detectable issues are :

• missing resources: a process step is missing, so that a required resource is not fetched/produced.

• mismatching data types: a certain service is not able to work on the data format provided by its predecessor.

however, this is only a first step. based on adequate domain modeling, made explicit via ontologies/taxonomies, model checking can capture semantic properties to guarantee not only the executability of the biological analysis process but also a good deal of its purpose, and rules of best practice, like:

• all experimental data will eventually be stored in the project repository.

• unexpected analysis results will always lead to an alert.

• chargeable services will not be called before permission is given by the user.

on a more technical side, model checking allows us also to apply the mature process analysis methodology that has been established in programming language compilers in the last decades  <cit>  and has shown to be realizable via model checking  <cit> . by providing a predefined set of desirable process properties to the model checker we plan to achieve a thorough monitoring of safety and liveness properties within the framework. similar to the built-in code checks that most integrated  development environments provide, this would help bio-jeti users to avoid the most common mistakes at process design time. in addition, the list of verified properties is extendable by the user, and can thus be easily adapted to specific requirements of the application domain.

model synthesis
this dimension can be seen as a step beyond model checking: the biologist does not have to care about data types at all – the synthesis automatically makes the match by inserting required transformation programs. this is similar to a spell checker which automatically corrects the text, thus freeing the writer from dealing with orthography at all. 

the potential of this technology goes even further: ultimately, biologists will be able to specify their requests in a very sparse way, e.g. by just giving the essential corner stones, and the synthesis will complete this request to a running process. in our text writing analogy, this might look like a mechanism that automatically generates syntactically and intentionally correct text from text fragments according to predefined rules that capture syntax and intention. for instance, the fragments "ten cars", " <dig> euro for shipping", "19% value added tax", "four days" and "mercedes", may be sufficient to synthesize a letter in which a logistics company offers its services to mercedes according to a specific request.

back to biology, the fragments "dna sequences", "phylogenetic tree", and "visualization", may automatically lead to a process that fetches ebi sequence data, sends them in adequate form to a tool that is able to produce a phylogenetic tree, and then transfers the result to an adequate viewer. typically there are many processes that solve such a request. thus our synthesis algorithm provides the choice of producing a default solution according to a predefined heuristics, or to propose sets of alternative solutions for the biologist to select.

CONCLUSIONS
we demonstrated by means of two examples how semantic web technology together with an adequate domain modelling frees in silico researchers from dealing with interfaces, types, and inconsistencies. in bio-jeti, bioinformatics services can be graphically combined to complex services without worrying about details of their interfaces or about type mismatches of the composition. these issues are taken care of at the semantic level by bio-jeti's model checking and synthesis features. whenever possible, they automatically resolve type mismatches in the considered service setting. otherwise, they graphically indicate impossible/incorrect service combinations. in the latter case, the workflow developer may either modify his service composition using semantically similar services, or ask for help in developing the missing mediator that correctly bridges the detected type gap. newly developed mediators should then be adequately annotated semantically, and added to the service library for later reuse in similar situations.

in the first example we developed a simple phylogenetic analysis workflow. the model checker detected a sib trying to access a data item that has not been defined previously in the workflow, which indicates that necessary computation steps are missing. we used the synthesis algorithm to generate the sequence of these missing steps.

the second example dealt with a more complex phylogenetic analysis workflow, involving several local steps processing intermediate data. here, the model checker did not only detect missing computations, but also a type mismatch that lead to an incorrect process model. again, the synthesis algorithm was used to find an appropriate intermediate sequence of services and an alternative to the erroneous part of the workflow, respectively.

we believe that our model checking and synthesis technologies have great potential with respect to making highly heterogeneous services accessible to in silico researchers that need to design and manage complex bioinformatics analysis processes. our approach aims at lowering the required technical knowledge according to the "easy for the many, difficult for the few" paradigm  <cit> . after an adequate domain modeling, including the definition of the semantic rules to be checked by the model checker or to be exploited during model synthesis, biologists should ultimately be able to profitably and efficiently work with a world-wide distributed collection of tools and data, using their own domain language. this goal differentiates us from other workflow development frameworks like kepler  <cit>  or triana  <cit> , which can be seen as middleware systems that facilitate the development of grid applications in a workflow-oriented fashion. they require quite some technical knowledge. in kepler, for instance, the workflow design involves choosing an appropriate director for the execution, depending on, e.g., whether the workflow depends on time, requires multiple threads or distributed execution, or performs simple transformations. these aspects have to be taken into account for efficient execution of complex computiations, but not necessarily when dealing with the actual composition of services. this way, these frameworks address a bioinformatics user, and not the biologists themselves.

we believe that bio-jeti's control flow-oriented approach is suitable for adressing non-it personnel: it allows them to continue to think in "dos" and "dont's", and steps and sequences of action in their own terms at their level of domain knowledge. in contrast, dataflow-oriented tools like kepler  <cit> , taverna  <cit> , or triana  <cit>  require their users to change the perspective to a resource point of view, which, in fact, requires implicit  knowlegde to profitably use them.

the challenge for us is now to integrate the available semantic information and the semantically aware technologies into our process development framework in the most user-convenient way. one central issue is to find an appropriate level of abstraction from the underlying technology: we would like to provide a set of general, pre-defined analyses and synthesis patterns, but at the same time give experienced users a way to add specialized specifications. another issue is how to integrate semantic information about the application domain and its services into this  automated workflow development process, since such knowledge is essential to achieve adequate results.

on the one hand, this requires predicates characterizing the single services, i.e. their function and their input/output behaviour. on the other hand, taxonomies or ontologies are required which provide the domain knowledge against which the services  are classified. the majority of this information has to be delivered by the tool and database providers, covering semantics of services as well as semantics of data. the convenience on the client side will increase as the semantic web spreads and new standards become established.

