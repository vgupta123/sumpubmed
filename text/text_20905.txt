BACKGROUND
quantitative measures derived from microscopy images are basic to enhancing our understanding of biological processes. with the rapid growth in emerging imaging technologies and high throughput bioimaging, robust image processing methods are critically needed in such quantitative analysis. while there is a large amount of literature concerning basic image processing methods, there exists currently no proper guidance for an end-user to choose a small subset of methods that are likely to be effective in a given application scenario. this is particularly true for segmentation and tracking, where literally hundreds of new methods are proposed each year. in most of these cases experimental results are provided on a very limited set of data, often coming from different domains, making it more difficult to judge their usability. the lack of well defined data sets that allow a fair comparison of different basic methods is a major bottleneck for progress in bioimage analysis. this is the main motivation in building the biosegmentation benchmark infrastructure and dataset collection for biological image analysis applications. in particular, we have collected datasets of different modalities and scales and carefully generated manual ground truth that could be of significant help not only to researchers in biological image analysis but also to the image processing community in general. by having a standardized set of data with associated ground truth, we believe that rapid progress can be made not only in identifying the appropriate methods for a particular task but also in facilitating the development of new and more robust methods.

in this paper we focus specifically on a benchmark dataset for image segmentation and tracking. typical challenges in developing robust bioimage analysis methods include low signal to noise ratio, complex changes in object morphology and the diversity of imaging techniques . given this diversity in imaging methods and bioimage samples, it is now well recognized that there is a clear need for validating new image analysis methods, see for example  <cit> .

benchmarks can be invaluable tools for both image processing specialists and scientists. the developers of the algorithms can use such benchmarks to evaluate the performance, reliability and accuracy of newly developed methods. the benchmark provides them with a well established problem set. further, the workload involved in validation can be reduced significantly by providing access to other analysis and evaluation methods  <cit> .

there have been several successful benchmarking efforts in image analysis and computer vision, such as the face recognition dataset  <cit> , the berkeley  segmentation dataset for natural images  <cit>  and the object caltech   <dig> dataset  <cit> . in medicine, databases with macrobiological structures such as mammogram and magnetic resonance images  <cit> , and clinical data  <cit>  have also been developed. in biology, there have been some efforts in creating microbiological image databases such as the cell centered database  <cit>  and the mouse retina database  <cit> . the protein classification benchmark collection  <cit>  was created in order to collect a standard datasets on which the performance of machine learning methods can be compared. finally, the broad bioimage benchmark collection  <cit>  consists of microscopy image sets with associated ground truth, such as cell counts, foreground/background and object outlines.

in addition to the above datasets, there have been few organized competitions in computer vision. these include the face recognition grand challenge   <cit> , face recognition vendor test   <dig>  <cit> , and the iris challenge evaluation  <dig>  <cit> . data and evaluation results of iris recognition competition are available in  <cit>  and benchmarking change analysis algorithms in lung ct in  <cit> . results from frgc and frvt  <dig> challenges documented two orders of magnitude improvement in the performance of face recognition under full-frontal, controlled conditions over the last  <dig> years. similarly, researchers have reported a significant improvement in object recognition performance over the caltech  <dig> and caltech  <dig> datasets over the last few years. this further supports our earlier observation that good benchmark datasets with ground truth information can act as catalysts in the development of robust image analysis methods.

a preliminary version of this dataset was presented in a conference publication at the international conference image processing' <dig>  <cit> . this work expands on  <cit>  by providing detailed descriptions on segmentation algorithms and performance metrics. in addition a new 3d image dataset is included. also, we describe a web-accessible infrastructure that we have developed recently for testing the algorithms. a flexible metadata model  is described that is used to exchange data and results for performance evaluation. this infrastructure lowers the burden of choosing datasets for testing algorithms, re-implementing analysis methods and developing evaluation metrics for comparison.

in the following, we describe both our benchmark infrastructure and comprehensive benchmark datasets for evaluating microscopy image analysis methods. the benchmark includes images with well defined ground truth. we provide representative datasets of microbiological structures whose scales range from the subcellular level  to the tissue level . the collections are obtained through collaborations with domain scientists in molecular, cellular, and developmental biology and in medicine. the datasets highlight some of the current challenges at these varying spatial scales for image analysis . for each of the datasets, we refer the reader to associated analysis methods that are currently integrated or available at our website  <cit> . we also suggest performance metrics that are used to evaluate the analysis. finally, we also describe our online benchmark infrastructure  <cit>  which can be used to download the data, upload and analyze the performance of different methods, and software that can be downloaded by researchers for self-evaluation of the analysis methods.

RESULTS
our biosegmentation benchmark  <cit>  consists of:

• image datasets at different scales;

• ground truth, manually verified results ;

• analysis methods, mostly segmentation methods, cell counting and tracking algorithms;

• evaluation methods, image analysis performance measurement;

• web-accessible infrastructure.

identifiable objects in the image datasets range from nanometers to microns: images at subcellular, cellular and tissue level are available . at the subcellular level, we focus on microtubules. at the cellular level, we provide a wide range of data. at the tissue level, many retinal images have been collected. ground truth is extracted from part of the datasets and is manually verified by domain experts. to guarantee a fair comparison of algorithm performance, we split the ground truth into training and testing datasets.

example implementations of image analysis tools are included for comparing newly developed algorithms. these include detection and tracking at subcellular level; cell counting and segmentation to quantify cellular structures at cellular level and layer segmentation at the tissue level. researchers can compare the performance of their algorithms through established evaluation metrics such as precision and recall measures. furthermore, scientists can use this benchmark as a resource for finding the best analysis methods available. all these data and tools are available through a web accessible infrastructure  <cit> .

subcellular level
at the subcellular level, the structures within a cell have a typical size of less than  <dig> μm. cells consist of organelles that are adapted and/or specialized for carrying out one or more vital functions and large assemblies of macromolecules that carry out particular and specialized functions. such cell structures, which are not formally organelles, include microtubules. our example dataset at subcellular level consists of time sequence images of microtubules under different experimental conditions. the image analysis challenges at this scale and with the fluorescence imaging acquisition method are typical for in-vivo subcellular imaging in that the analysis methods need to cope with high clutter and low signal to noise ratio.

microtubule dataset
microtubules are a core component of the cellular cytoskeleton and can function as conveyer belts inside the cells. researchers believe microtubules play an important role in the study of alzheimer's and in certain cancers  <cit> . they move vesicles, granules, organelles like mitochondria, and chromosomes via special attachment proteins. structurally they are linear polymers of tubulin which is a globular protein. microtubule growth and shortening, otherwise known as microtubule dynamics, are studied extensively by biologists  <cit> . understanding the dynamics of the microtubules under different experimental conditions is important in the study of the above mentioned conditions. traditionally, microtubule dynamics are obtained by manual microtubule tracking  <cit>  . the tracking of the microtubule free ends  allows biologists to compute the growth and shortening statistics which in turn are related to the presence of key proteins such as tau and its interaction with various drugs.

the microtubule dataset  is obtained by transmitted light microscopy at the feinstein/wilson laboratory at university of california, santa barbara . the microtubule dataset includes  <dig> traces which consists of ground truth for both microtubule tip location and microtubule bodies.

microtubule tracking
the manual measurements of these microtubules are very labor intensive and time consuming. to obtain an automatic quantitative description of behavior under different experimental conditions, tracing algorithms have been implemented. due to the limitations in biological sample preparation and inconsistent staining, typical images in live cell studies are noisy and cluttered, making automatic microtubule tracing difficult. our benchmark implementation includes an automatic method that employs arc-emission hidden markov model for extracting curvilinear structures from live cell fluorescence images  <cit> .

evaluation
we propose the following three measurements to evaluate microtubule tracing :

• microtubule tip distance, ϵt: tip distance error is the euclidean distance between the ground truth tip to the analysis trace tip,

• microtubule trace body distance, ϵd: trace distance error is the average distance from all the points on the ground truth to all the points on the trace,

• microtubule length errors, ϵl: length difference is simply the difference between the length of the ground truth and the trace.

tracking failure occurs when the above errors are larger than corresponding thresholds:

   

   

   

where the thresholds τt, τb and τl, are empirically set by biologists.

for our tracing algorithm described in  <cit>  the failures rate is on average less than 9%. examples of failure and successful tracking are shown in figure  <dig> 

cellular level
the cell is the structural and functional unit of all known living organisms. a typical cell size is  <dig> μm. image processing challenges at the cellular level include large variations in cell phenotype, staining intensity variation within and across the images, and occlusions. cells can grow, reproduce, move and die during many processes, such as wound healing, the immune response and cancer metastasis. one of the common tasks is to count the number of cells or nuclei, particularly in histological sections, and characterize various cell attributes, such as cell density, morphology, size and smoothness of the boundary. in our example datasets, we use cell counting as a feature for estimating cell density in 2d retinal and 3d arabidopsis images, and cell segmentation for studying cell morphology in breast cancer and kidney histopathological images.

photoreceptors in retinal images
the vertebrate retina is a light sensitive part inside the inner layer of the eye. the photoreceptor cells, rods and cones, receive light and transform it into image-forming signals which are transmitted through the optic nerve to the brain. the vertebrate retina is a highly structured tissue and consists of distinct layers as depicted in the cross section figure  <dig>  the decreasing number of photoreceptor nuclei in the outer nuclear layer  is one of the characteristics of retina degeneration. the number of photoreceptors in the onl can decrease due to injury or disease and this may eve result in blindness. the number of photoreceptors in a given section is often used as a measure of effectiveness of a treatment following retinal detachment  <cit> . images are typically collected using a laser scanning confocal microscope from tissue sections.

the retinal dataset  is collected at the fisher's laboratory at ucsb to study retinal degeneration and our benchmark data consists of  <dig> laser scanning confocal images of normal and 3-day detached feline retinas . for each image, the ground truth consists of an onl binary mask and the corresponding manual cell count in the onl layer provided by three different experts for the same image as depicted in figure  <dig> 

2d cell nuclei detection
as mentioned above, of particular interest to this collection is detection of cell nuclei. we have implemented in the benchmark system a 2d nuclei detector based on a laplacian of gaussian blob detector, see  <cit>  for more details on the method itself.

evaluation
common ways of evaluating cell/nuclei counting take into account the mismatched counts between detected and ground truth nuclei, and/or the displacement of detected nuclei. in 2d analysis evaluation only the counts are available in the ground truth.

a simple object count evaluation method was proposed in  <cit> . the error e in the cell count is measured as the ratio between manual counts  and the result of automated detector:

   

where n is the number of images in the dataset, ndi and  are the number of nuclei automatically detected and the average of manual counting, respectively.

our nuclei detector  <cit>  applied to the 2d retina dataset gives an error of  <dig> % for the nuclei count within the onl retina layer.

cell nuclei in 3d plant images 
in plants, meristems are regions of cells capable of division and growth. the live 3d imaging of the arabidopsis meristem has been recently applied in order to analyze the cell lineage and the cell fate during active growth of the shoot meristem  <cit> . this technique helps to understand the genetic control of the meristem size. again, cell counts are often used to quantify this process. however, this is an extremely time consuming and laborious task given that a 3d stack consists of approximately  <dig> cells.

currently we have  <dig> stacks of images and one annotated arabidopsis laser scanning confocal microscope image generously provided by meyerwitz's laboratory at caltech. the annotated image contains  <dig> channels with nuclear and membrane stains and is shown in figure  <dig>  the ground truth consists of manually detected nuclei centroids as shown in figure  <dig> 

3d cell nuclei detection
the 3d nuclei detection  <cit>  extends our earlier work on 2d detection based on a laplacian of gaussian blob detector and it is also integrated into our benchmark. figure  <dig> shows arabidopsis's nuclei automatically detected in the selected region of the image shown in figure  <dig> 

evaluation
in 3d, the ground truth contains both counts and positions of the nuclei centroids. thus, we include in this evaluation metric also the nuclei positions available in the 3d ground truth. we take into consideration the distance of the detected nuclei from the ground truth. the distance error, ed, normalizes the detection error and gives the overall error  as:

   

where false positives  are objects detected in the test image but not present in the ground truth, false negatives  are objects that were not detected in the test image but are present in the ground truth. gt and nd denote, respectively, the ground truth  and automatically detected nuclei coumt. the mean distance () is the mean of all the distances between the detected nuclei locations and their corresponding ground truth locations,  is the standard deviation of these distances, and dmax is the max of all the distances. note that g3d is normalized to  <cit>  and  <dig> represents the worst case. to quantify the performance of our automatic 3d nuclei detection algorithm  <cit> , we compared its output with ground truth manually annotated by two experts and obtained a g3d of  <dig> . when we compared one expert ground truth to the other one we obtained a g3d of  <dig> .

breast cancer cells
the utility of determining nuclear features for correct cancer diagnosis has been well established in medical studies. scientists extract a variety of features from nuclei in histopathology imagery of cancer specimens, including size, shape, radiometric properties, texture, and chromatin-specific features. histopathological images are stained since most cells are essentially transparent, with little or no intrinsic pigment. certain special stains, which bind selectively to particular components, are used to identify biological structures such as cells. routine histology uses the stain combination of hematoxylin and eosin, commonly referred to as h&e. in those images, the first step is manual cell segmentation for subsequent classification into benign and malignant cells.

in our benchmark dataset there are  <dig> h&e stained histopathology images used in breast cancer cell detection from david rimm's laboratory at yale. the ground truth is obtained for  <dig> images including both benign and malignant cells and is described in table  <dig> 

breast cancer cell segmentation
in  <cit>  a system has been developed which extracts nuclei in histopathology imagery of breast cancer specimens  based on watershed using as initial condition the results of the 2d cell nuclei detection method mentioned earlier  <cit> .

evaluation
the following metric is defined for the segmentation evaluation of cell nuclei  <cit> . for the cell segmentation, the size of regions missed, of extraneous regions and their shape are penalized, assuming roughly elliptical shapes for the cells. the metric is given by:

   

where n is the number of ground truth nuclei; nd is the number of nuclei detected by the segmentation algorithm; the weight α <dig> can be thought of as the penalty for an over-segmented nucleus; sr is the number of segmented regions overlapping the ground truth nucleus; δsr =  <dig> is the upper limit for number of segmented regions; pm is the number of pixels missing; gt is the number of pixels in the ground truth; qspm is the "quadrant sum" of the pixels missed; α <dig> can be thought of as the penalty regions of pixels missed, penalizing both size and shape; ep the number of excess pixel; α <dig> is thus the penalty for size and shape of excess pixel regions, and is related to the degree of under-segmentation of the nucleus; qsep is "quadrant sum" of the excess pixels; the term with α <dig> =  <dig> is simply the detection rate; er as the number of excess segmented regions and δer is the fraction of total ground truth nuclei that we will allow as excess regions; α <dig> is the penalty for excess segmented regions. the metric takes value p ⊂  <cit>  and  <dig> represents the worst segmentation scenario. for a detailed explanation of the metric the reader is referred to  <cit> . our cell segmenter using the seeds from the 2d cell nuclei segmentation gets a score of p =  <dig> .

additional dataset
in addition to the above dataset we also have images of kidney cells and ground truth corresponding to kidney cell segmentation, but we do not have any associated analysis or evaluation methods for this data. this data was collected by feinsten's lab at ucsb to study alzheimer's disease. usually manual segmentation provides a reliable alive/dead cell ratio which will test the hypothesis that tau  confers an acute hypersensitivity of microtubules to soluble, oligomeric amyloid-beta and that taxol, a microtubule-stabilizing drug, provides neuroprotective effects. because tau is not endogenously expressed, tau effects are easier to study in kidney cancer cells. kidney cells can easily be transfected with tau. to quantify this phenomenon, cos <dig> cells  are collected through confocal microscopy imaging at the feinsten's lab at ucsb.

in the dataset  the images are of both wild-type cos <dig> cells  and tau transfected cos <dig> cells and these cells are imaged at  <dig> different time points after treatment . ground truth has also been collected for  <dig> images of this dataset and is represented by binary masks.

tissue level
our tissue level benchmark dataset comes from retinal cross-sections. as discussed in the previous section confocal microscope images of retinas taken during detachment experiments are critical components for understanding the structural and cellular changes of a retina in response to disease or injury. during retinal detachment, layers undergo several changes in morphology. thus, it is crucial to have a reliable map of the retinal layers. four major layers of the retina are usually segmented manually to quantify the number of cells in them: the ganglion cell layer , the inner nuclear layer , the outer nuclear layer , and the outer segments , as depicted in figure  <dig>  hundreds of retinal images  <cit>  and layer ground truth are part of the benchmark, see table  <dig>  retinal layer segmentation is a challenging problem due to the heterogeneity in retinal images stained with different antibodies.

retinal layer segmentation
three automatic retinal layer segmentation methods are integrated into the benchmark. one is a variational segmentation approach based on pairwise pixel similarities  <cit> . this method exploits prior information  and a dissimilarity measure between the pixels of the reference image and the pixels of the image to be segmented. the second segmentation algorithm uses parametric active contour to detect the boundaries between layers  <cit> . the third method  <cit>  is a segmentation based on non rigid registration using thin plate splines, which assumes a non rigid transformation of the layer boundaries of a training retinal image in order to segment the test image. an example of the segmentation results and ground truth are respectively shown in figures  <dig>   and  <dig> 

evaluation
for boundary evaluation, the distance between the ground truth boundary pixels and computed boundaries for each layer is computed. for layer evaluation, several measures are implemented: precision  is the ratio between true positive and automatically detected pixels; recall or sensitivity  is the ratio between true positive pixels and ground truth; f-measure  is the harmonic mean between precision and recall for each layer defined as:

   

the weighted f-measure  is the sum of the f-measure scores for each layer i in proportion to their area ai of the total area atot:

   

this modified f measure allows for weighting more segmentation errors in larger layers.

our best performing method  <cit>  gives a f-measure around 88% when applied on the dataset. the average distance between boundary pixels in the computed and ground truth data using the boundary detection method from  <cit> , averaged over all experimental conditions, is  <dig>  pixels.

CONCLUSIONS
benchmark datasets often have a strong positive effect in advancing the state of the art of image analysis algorithms.

the benefits of benchmarking include a stronger consensus on the research goals, collaboration between laboratories, transparency of research results, and rapid technical progress. our benchmark provides unique, publicly available datasets as well as image analysis tools and evaluation methods. the benchmark infrastructure avoids the burden of choosing datasets for testing algorithms, reimplementing analysis methods and evaluation metrics for comparison.

we hope that our benchmark will help researchers to validate, test and improve their algorithms, as well as provide biologists a guidance of algorithms' limitations and capabilities. the benchmark datasets and methods are available online  <cit> . analysis results can be uploaded directly and automatically evaluated. the benchmark data that we describe is by no means complete, given the complexity and diversity of the bio-samples and imaging modalities. by making this infrastructure easily accessible to the community, we hope that the collections and analysis methids will grow over time. users are encouraged to submit datasets, associated ground truth, and analysis results for evaluation. moreover, user contributed analysis  algorithms and evaluation methods will be integrated upon request.

availability and requirements
it is generally acknowledged that making fair comparisons between different methods is quite difficult, particularly in the absence of well established datasets. in addition, even if such datasets are available, often the researchers are left to implement other methods on their own in order to make such comparisons. newly developed algorithms are often tested against relatively limited datasets. keeping these limitations in mind, we have been developing the ucsb bisque infrastructure  <cit>  whose primary goal is to facilitate a tighter integration of datasets and analysis methods.

in the following, we outline the bisque  <cit>  components that are relevant to the benchmarking effort. all of the working modules and datasets are available from our bioimage informatics website  <cit> . from the website users can download the different datasets discussed in this paper and associated ground truth data. each dataset includes a complete set of original images to process, a document in xml format and an example of ground truth. the xml structure follows bisque standards  <cit>  and examples for metadata and graphical annotations are presented in the appendix. a complete description and formatting of metadata for each dataset is given in  <cit> .

two evaluation options are available to the users :

• matlab code for evaluation is available for download. users can download this code and self evaluate the performance of the analysis methods. the evaluation requires that the results are stored in a certain format and a detailed description of the file formats are also provided on bisque,

• users can also evaluate the performance using our web-based evaluation module. in order to use the web-based evaluation the users must first register into bisque. they need to first upload their results, one image at the time in the correct format to the web-based evaluator and the evaluation results will be automatically displayed on the web site. this option will also allow the result to be stored on our benchmark website and made available to the registered users.

abbreviations
caltech: california institute of technology; cos: cv- <dig>  in origin, and carrying the sv <dig> genetic material; ct: computed tomography; frgc: face recognition grand challenge; frvt: face recognition vendor test; gcl: ganglion cell layer; h&e: hematoxylin eosin; inl: inner nuclear layer; is: inner segments; mri: magnetic resonance imaging; ml: multilayer; onl: outer nuclear layer; opl: outer photoreceptor layer; os: outer segments; ucsb: university of california santa barbara.

authors' contributions
edg developed the initial benchmark, web site and tested the image analysis code. she also developed the different evaluation measures. bo developed the matlab toolbox to access the bisque database system. bo, df and kk conceived and developed the flexible database infrastructure and access tools used by the benchmark. bm conceived the project and coordinated the research and development activities. bm and kk refined the the manuscript drafted by edg, ob and df. all the authors read and approved the manuscript.

appendix
flexible data model: the benchmark uses a flexible metadata model based on tag documents to store and process ground truth and resultant data. a tag is a named field with an associated value. the tags themselves may be nested, and include values that are strings, numbers, other documents or list. for example, the 3d cell counting document has:

<benchmark type="3d nuclei"> <image name=" <dig>  tiff " src =" <dig>  tiff ">

   <gobject name="my_algorithm " type="nucleidetector3d_automatic">

      <gobject name="1" type="point">

         <vertex x="0" y="1020" z="0"/>

      </gobject >

      <gobject name="2" type="point">

         <vertex x="1048" y="941" z="4"/>

      </gobject >

      <gobject name="3" type="point">

         <vertex x="871" y="1046" z="2"/>

      </gobject >

      ...

   </gobject >

</image> </benchmark>

the benchmark uses also an extensible metadata format for graphical annotations that has a number of graphical primitives and can be extended by new object types and object properties. graphical annotations are termed gobject s and are used, to represent ground truth objects in the dataset. the following is an example gobject description for microtubules:

<gobject name="gt " type="mt_gt" >

<tag name="expert " value="expert_name"/>

<tag name="tube_id " value ="1"/>

   <polyline type="polyline " name="polyline">

      <vertex x=" <dig> " y=" <dig> " index="0" t =" <dig> "/>

      <vertex x=" <dig> " y=" <dig> " index="1" t =" <dig> "/>

</polyline >

<polyline type="polylin e " name="polyline">

      <vertex x=" <dig> " y=" <dig> " index="0" t =" <dig> "/>

      <vertex x=" <dig> " y=" <dig> " index="1" t =" <dig> "/>

</polyline >

...

