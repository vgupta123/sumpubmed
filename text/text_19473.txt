BACKGROUND
numerous animal models  have been developed for a variety of psychiatric, neurodegenerative, and neurodevelopmental disorders. while many of these models have been helpful for understanding disease pathology, they have been less useful for discovering potential therapies or predicting clinical efficacy. translation from in vivo animal models  has been poor, despite many years of research and effort. there are many reasons for this, including the inherent difference in biology between rodents and humans  <cit> , particularly relating to higher cognitive functions. in addition, there is the ever-present question of whether a particular animal model is even suitable; whether it recapitulates the disease process of interest or faithfully mimics key aspects of the human condition. while important, these two considerations will be put aside and the focus will be on the design and analysis of preclinical studies using multiparous species, and how this affects the validity and reproducibility of results. there are two issues that will be discussed. the first deals with designs where an experimental treatment is applied to whole litters rather than to the individual animals, usually because the treatment is applied to pregnant females and therefore to all of the offspring. the second is the natural litter-to-litter variation that is often present, which means that the value of a measured experimental outcome can potentially be influenced by the litter that the animal came from.

applying treatments to whole litters
some disease models have a distinctive experimental design feature: the treatment is applied to pregnant females , but the scientific interest is in the individual offspring . here, the “treatment” refers to the experimental manipulation that induces the disease features, and it does not refer to a therapeutic treatment. this design is common in toxicology and nutrition studies, but is also used in neuroscience studies when examining the effects of maternal stress and in the valproic acid  model of autism. difficulties arise because the experimental unit  is the pregnant dam and not the individual offspring  <cit> . in other words, the sample size is the number of dams, and the offspring are considered subsamples, much like the left and right kidney from a single animal do not represent a sample size of two . this may come as a surprise, and it is irrelevant that the scientific interest is in the offspring, or that the offspring eventually become individual entities . regulatory authorities have clear guidelines on the matter  <cit> ; for example, the organisation for economic co-operation and development  has made a firm statement in their guidelines for chemical testing: “developmental studies using multiparous species where multiple pups per litter are tested should include the litter in the statistical model to guard against an inflated type i error rate. the statistical unit of measure should be the litter and not the pup. experiments should be designed such that litter-mates are not treated as independent observations ”  <cit> . there is a restriction on randomisation because only whole litters can be assigned to the treatment or control conditions, which has implications for how studies are designed and analysed. an appropriate analysis can be conducted by using only one animal per litter , which allows standard methods to be used . this is often not the most efficient design in terms of animal usage, unless the excess animals can be used for other experiments. a second option is to use more than one animal per litter, and then average these values for each litter. these mean values can then be taken forward and analysed using standard methods. a third option is to use multiple animals per litter, and then use a mixed-effects model for analysis, which properly handles the structure of the data  and avoids artificially inflating the sample size . the third method is preferred to averaging values within a litter because the magnitude of the litter effect can be quantified. in addition, information on the precision of estimates will be lost by averaging, but is retained and made use of in the mixed-effects model. when using the first two options, it is clear that to increase the sample size and thus power, the number of litters needs to be increased. this is also true for the third option, but may not be so readily apparent , and is discussed further below.

a related design issue is that greater statistical power can be achieved when litter-mates are used to test a therapeutic compound versus a placebo. if the therapeutic treatment is applied to the individual animals postnatally, then the individual animal is the experimental unit for this comparison. this is referred to as a split-plot or split-unit design and has more than one type of experimental unit: litters for some comparisons and individual animals for others  <cit> . these studies therefore require careful planning and analysis, but biologists are rarely introduced to these designs and how to appropriately analyse data derived from them during the course of their training.

litter effects are ubiquitous, large, and important
it is known that for many measurable characteristics across many species, monozygotic twins are more similar than dizygotic twins, which are more similar than non-twin siblings, and which in turn are more similar than two unrelated individuals. what has not been fully appreciated is that all of the standard statistical methods  assume that the data come from unrelated individuals. however, rodents from the same litter are effectively dizygotic twins; they are genetically very similar and share prenatal and early postnatal environments. therefore, studies need to be designed and analysed in such a way that if differences between litters exist, they do not bias or confound the results  <cit> . more specifically, this relates to the assumption of independence of observations. for example, measuring blood pressure  from the left and right arm of ten unrelated people only provides ten independent measurements of bp, not twenty. this is because the left and right bp values will be highly correlated—if the bp value measured from a person’s left arm is high, then so will the value measured from their right arm. similarly, two animals from the same litter will tend to have values that are more alike  than two animals from different litters. one can think of it as within-litter homogeneity and between-litter heterogeneity. this lack of independence needs to be handled appropriately in the analysis and the three strategies outlined in the previous section can be used. many animal models are derived from highly inbred strains, and this results in reduced genotypic and phenotypic variation. this is a different issue and unrelated to lack of independence. it does not mean that animals “are all the same” and that differences between litters do not exist.

litter effects are not a minor issue that only statistical pedants worry about with little practical importance for scientists. using actual body weight data from their experiment, holson and pearce showed that if three “treated” and three “control” litters are used, with two offspring per litter , then the false positive rate  is 20% rather than the assumed 5%  <cit> . the false positive rate was determined as the proportion of p-values that fell below  <dig> . since there was no actual treatment applied, random sampling should produce only a 5% error rate. furthermore, they showed that the false positive rate increases with the number of offspring per litter; if the number of offspring per litter is  <dig>  then the false positive rate is 80%. the error rate is also influenced by the relative variability between and within litters and will therefore vary for each experimental outcome. given that papers report the results of multiple tests , we can expect the literature to contain many false positive results. it may seem paradoxical, but in addition to an increased false positive rate, ignoring litter-to-litter variation can also lead to low power  when true effects exist  <cit> . this occurs because litter-to-litter variation is unexplained variation, and thus the “noise” in the data is increased, potentially masking true treatment effects. a subsequent study using forty litters found “significant litter effects… in varying degrees, for almost every behavioural, morphologic, and neuroendocrine measure; they were evident across indices of neural, adrenal, thyroid, and immunologic functioning in adulthood”  <cit>  . holson and pearce reported that only 30% of papers in the behavioural neurotoxicology literature correctly accounted for litter effects  <cit>  and zorrilla noted that 34% of papers in developmental psychobiology and 15% of papers in related journals correctly accounted for litter effects  <cit> . this issue has been discussed repeatedly for almost forty years  <cit> , but experimental biologists seem unaware  the importance of dealing with litter effects. one can only speculate on the number of erroneous conclusions that have been reached and the resources that have been wasted.

one might argue that when many studies are conducted, including replications within and between labs, the evidence will eventually converge to the “truth”, and therefore these considerations are only of minor interest. unfortunately, there is no guarantee of such convergence, as the literature on the superoxide dismutase  transgenic mouse model of amyotrophic lateral sclerosis  demonstrates. several treatments showed efficacy in this model and were advanced to clinical trials, where they proved to be ineffective  <cit> . a subsequent large-scale and properly executed replication study did not support the previous findings  <cit> . this study also identified litter as an important variable that affected survival  and which was not taken into account in the earlier studies. the authors also demonstrated how false positive results can arise with inappropriate experimental designs and analyses. litter effects were not the only contributing factor; a meta-analysis of the preclinical sod <dig> literature revealed that only 31% of studies reported randomly assigning animals to treatment conditions and even fewer reported blind assessment of outcomes  <cit> . lack of randomisation and blinding are known to overstate the size of treatment effects  <cit> . in addition, there was evidence of publication bias, where studies with positive results were more likely to be published  <cit> . thus, the combination of poor experimental design, analysis, and publication bias contributed to numerous incorrect decisions regarding treatment efficacy.

general quality of preclinical animal studies
previous studies have shown that the general quality of the design, analysis, and interpretation of preclinical animal experiments is low  <cit> . for example, nieuwenhuis et al. recently reported that 50% of papers in the neuroscience literature misinterpret interaction effects  <cit> . in addition, the issue of “inflated n”, or pseudoreplication, shows up in other guises  <cit> , and whole fields can misattribute cause-and-effect relationships  <cit> . there is also the concept of “researcher degrees of freedom”, which refers to the post hoc flexibility in choosing the main outcome variables, statistical models, data transformations, how outliers are handled, when to stop collecting data, and what is reported in the final paper  <cit> . various permutations of the above options greatly increases the chances that something statistically significant can be found, and this gets reported as the sole analysis that was conducted. given the above concerns, it is not surprising that the pharmaceutical industry has difficulty reproducing many published results  <cit> .

methods
literature review
ninety-five papers were identified on pubmed using the search term “ and autism ” . reference lists from these articles were then examined for further relevant studies, and one was found. only primary research articles that injected pregnant dams with vpa and subsequently analysed the effects in the offspring were selected . a total of thirty-five studies were found, and one was excluded as key information was located in the supplementary material, but this was not available online  <cit> . two key pieces of information were extracted:  whether the analysis correctly identified the experimental unit as the litter and  whether important features of good experimental design were mentioned, including randomisation, blinding, sample size calculation, and whether the total sample size  was indicated or could be determined.

estimating the importance of litter-to-litter variation
data from mehta et al.  <cit>  were used to estimate the magnitude of differences between litters on a number of outcome variables. this study was chosen because it included animals from fourteen litters  and therefore it was possible to get a reasonable estimate of the litter-to-litter variation. in addition, the study mentioned using randomisation and blind assessment of outcomes. half of the animals in each condition were also given mpep , a metabotropic glutamate receptor  <dig> antagonist. to assess the magnitude of the litter effects, the effect of vpa, mpep, and sex  were removed, and the remaining variability in the data that could be attributed to differences between litters was estimated. more specifically, models with and without a random effect of litter were compared with a likelihood ratio test. this analysis is testing whether the variance between litters is zero, and it is known that p-values will be too large because of “testing on the boundary”, and therefore the simple method of dividing the resulting p-values by two was used as recommended by zuur et al <cit> . the exact specification of the models is provided as r code in additional file  <dig> and the data are provided in additional file  <dig> 

power analysis
in these types of designs, power  is influenced by the  number of litters,  variability between litters,  number of animals within litters,  variability of animals within litters,  difference between the means of the treatment groups ,  significance cutoff , and  statistical test used. in order to illustrate the importance of the number of litters relative to the number of animals within litters and how an inappropriate analysis can lead to p-values that are too small, a power analysis was conducted with the number of litters per group varying from three to ten, and the number of animals per litter varying from one to ten. the other factors were held constant. variability between litters  and the variability of animals within litters  was estimated from the locomotor activity data from mehta et al.  <cit> . for each combination of litters and animals,  <dig> simulated datasets were created with a mean difference between groups of  <dig> . once the datasets were generated, the power for three types of analyses were calculated. the first analysis averaged the values of the animals within each litter and then groups were compared with a t-test. the second analysis used a mixed-effects model, and the third ignored litter and compared all of the values with a t-test. the last analysis is incorrect and only presented to demonstrate how artificially inflating sample size affects power. the power for each analysis was determined as the proportion of tests that had p <  <dig> . the r code is provided in additional file  <dig> and is adapted from gelman and hill  <cit> .

RESULTS
low quality of the published literature
the vpa model of autism is relatively new and potential therapeutic compounds tested in this model have not yet advanced to human trials. the opportunity therefore exists to clean up the literature and prevent a repeat of the sod <dig> story. the main finding is that only 9%  of studies correctly identified the experimental unit and thus made valid inferences from the data. one study used a nested design  <cit> , the second mentioned that litter was the experimental unit
 <cit> , and the third used one animal from each litter, thus bypassing the issue
 <cit> . in fourteen studies  it was not possible to determine the number of dams that were used  and in four studies  the number of offspring used were not indicated. in addition, only four  reported randomly assigning pregnant females to the vpa or control group. many studies also used only a subset of the offspring from each litter, but often it was not mentioned how the offspring were selected. only six studies  reported that the investigator was blind to the experimental condition when collecting the data. ten studies  did not indicate whether both male and female offspring were used. no study mentioned performing a power analysis to determine a suitable sample size to detect effects of a given magnitude—but this is probably fortuitous, given that only three studies correctly identified the experimental unit. it is possible that many studies did use randomisation and assess outcomes blindly, but simply did not report it. however, randomisation and blinding are crucial aspects for the validity of the results and their omission in manuscripts suggests that they were not used. this is further supported by studies showing that when manuscripts do not mention using randomisation or blinding the estimated effects sizes are larger compared to studies that do mention using these methods, which is suggestive of bias
 <cit> .

a number of papers had additional statistical or experimental design issues, ranging from trivial  to serious. these include treating individual neurons as the experimental unit, which is common in electrophysiological studies, but just as inappropriate as treating blood pressure values taken from left and right arms as n =  <dig>  or dissecting a single liver sample into ten pieces and treating the expression of a gene measured in each piece as n =  <dig>  <cit> . if it were that easy, clinical trials could be conducted with tens of patients rather than hundreds or thousands. regulatory authorities are not fooled by such stratagems, but is seems many journal editors and peer-reviewers are. a list of studies can be found in additional file  <dig> 

estimating the magnitude of litter effects
to illustrate the extent to which litter effects can influence the results, data originally published by mehta et al.  <cit>  were used and experimental details can be found therein. locomotor activity in the open field is shown in figure  <dig> for nine vpa and five saline injected control litters. half of the animals from each condition were given mpep  or saline. visually, there do not appear to be differences between vpa and control groups and there is a slight increase in activity due to mpep. the effect of mpep was not significant when litter effects were ignored , but it was when adjusting for litter . in this case the shift in p-value was not large, but it happened to decrease it below the  <dig>  threshold after the excess noise caused by litter-to-litter variation was removed.

it may be difficult to determine whether litter effects are present by simply plotting the data by litter because they may be obscured by the experimental effects. for a visual check, it is preferable to remove the effect of the experimental factors first and then plot the residual values versus litter. the y-axis for figure  <dig> shows the residuals, which are defined as the difference between the observed locomotor activity for each animal and the value predicted from the model containing group  and condition  as factors . the residuals should be pure noise, centred at zero, and should not be associated with any other variable. however, it is clear that there are large differences between litters , indicating heterogeneity in the response from one litter to the next. when litter effects are taken into account, the mean of each litter is closer to zero. also note that variance of the residuals  is reduced by 61% when litter is taken into account . this is shown by the spread of the grey points around zero on the right side of each graph, which are clustered closer together in the second analysis. the interpretation is that litter accounted for 61% of the previously unexplained variation in the data. note that it would be impossible to determine whether litter effects are present if only one litter per treatment group was used because litter and treatment would be completely confounded.

a similar analysis was performed for other variables and the results are displayed in table  <dig>  it is clear that litter-to-litter variation is important for a number of behavioural outcomes. it is also clear from figure 3a how one could obtain false positives with an inappropriate design and analysis. suppose an experiment was conducted with only one vpa and one saline litter, with ten animals from each, and that there is no overall effect of vpa on a particular outcome. if the experimenter happened to select litter a  and litter m  there would be a significant increase due to vpa, but if litter d  and litter g  were selected, there would be a significant effect in the opposite direction! there are many combinations of a single saline and vpa litter that would lead to a significant difference between conditions. having two or three litters per group instead of one will reduce the false positive rate, but it will still be much higher than  <dig>   <cit> . in addition, these apparent differences would not replicate with a properly designed follow-up experiment.

the p-value tests whether the litter-to-litter variation was significantly greater than zero.

 σε <dig>  is the residual  variation.

how power is affected by the number of litters and animals
some may object on ethical grounds to using so many litters and then selecting only one or a few animals from each, as there will be many additional animals that will not be used and presumably culled. certainly all of the animals could be used, but there is almost no increase in power after three animals per litter  and therefore it is a poor use of time and resources to include all of the animals. one could argue therefore that it is unethical to submit a greater number of animals to the experimental procedure if they contribute little or nothing to the result. one could also argue that it is even more unethical to use any animals for a severely underpowered  study in the first place and then to clutter the scientific literature with the results. one way to deal with the excess animals is to use them for other experiments. this requires greater planning, organisation, and coordination, but it is possible. another option is to purchase animals from a commercial supplier and request that the animals come from different litters rather than have an in-house colony. as a side note, suppliers do not routinely provide information on the litters that the animals come from and thus an important variable is not under the experimenter’s control and cannot even be checked whether it is influencing the results.

how does litter-to-litter variation arise?
differences between litters could exist for a variety of reasons, including shared genes and shared prenatal and early postnatal environments, but also due to age differences , and because litters are convenient units to work with. for example, it is not unusual for litter-mates to be housed in the same cage, which means that animals within a litter also share not just their early, but also their adult environment. it is also often administratively easier to apply experimental treatments on a per cage  basis rather than per animal basis. for example, animals in cage a and c are treated while cage b and d are controls. animals may also undergo behavioural testing on a per cage basis; for example, animals are taken from the housing room to the testing room one cage at a time, tested, and then returned. larger experiments may need to be conducted over several days and it is often easier to test all the animals in a subset of cages on each day, rather than a subset of animals from all of the cages. at the end of the experiment animals may also be killed on a per cage basis. given that it may take many hours to kill the animals, remove the brains, collect blood, etc., the values of many outcomes such as gene expression, hormone and metabolite concentrations, and physiological parameters may change due to circadian rhythms. all of these can lead to systematic differences between litters and can thus bias results and/or add noise to the data.

there is an important distinction to be made between applying treatments to whole litters versus “natural” variation between litters. when a treatment is applied to a whole litter such as the vpa model of autism or maternal stress models, then the litter is the experimental unit and the sample size is the number of litters. therefore, by definition, litter needs to be included in the analysis if more than one animal per litter is used . however, if multiple litters are used but the treatment are applied to the individual animals, experiments should be designed so that if litter effects exist, then valid inferences can still be made. in other words, litters should not be confounded with other experimental variables because it would be difficult or impossible to detect their influence and remove their effects. whether litter is an important factor for any particular outcome is then an empirical question, and if it is not important then it need not be included in the analysis. however, the power to detect differences between litters will be low if only a few litters are used in the experiment and therefore a non-significant test for litter effects should not be interpreted as the absence of such effects. analysing the data with and without litter and choosing the analysis that gives the “right” answer should of course be avoided  <cit> . flood et al. provide a nice example in the autism literature of an appropriate design followed by a check for litter effects, and then the results for the experimental effect were reported when litter was both included and excluded
 <cit> . consistent with other studies demonstrating litter-effects, this paper found a strong effect of litter on brain mass.

four ways to improve basic and translational research
better training for biologists
most experimental biologists are not provided with sufficient training in experimental design and data analysis to be able to plan, conduct, and interpret the results of scientific investigations at the level required to consistently obtain valid results. the solution is straightforward, but requires major changes in the education and training of biologists and it will take many years to implement. nevertheless, this should be a long-term goal for the biomedical research community.

make better use of statistical expertise
a second solution is to have statisticians play a greater role in preclinical studies, including peer reviewing grant applications and manuscripts, as well as being part of scientific teams  <cit> . however, there are not enough statisticians with the appropriate subject matter knowledge to fully meet this demand—just as it is difficult to do good science without a knowledge of statistics, it is difficult to perform a good analysis without knowledge of the science. in addition, this type of “project support” is often viewed by academic statisticians as a secondary activity. despite this, there is still scope for improving the quality of studies by making better use of statistical expertise.

more detailed reporting of experimental methods
detailed reporting of how experiments were conducted, how data were analysed, how outliers were handled, whether all animals that entered the study completed it, and how the sample size was determined are all required to assess whether the results of the study are valid, and a number of guidelines have been proposed which cover these points, including the national institute of neurological disorders and stroke  guidelines  <cit> , the gold standard publication checklist
 <cit> , and the arrive  guidelines  <cit> . for example, arrive items  <dig> ,  <dig> ,  <dig> , and  <dig>  should be a mandatory requirement for all publications involving animals and could be included as a separate checklist that is submitted along with the manuscript, much like a conflict of interest or a transfer of copyright form. something similar has recently been introduced by nature neuroscience <cit> . this would make it easier to spot any design and analysis issues by reviewers, editors, and other readers. in addition, and more importantly, if scientists are required to comment on how they randomised treatment allocation, or how they ensured that assessment of outcomes was blinded, then they will conduct their experiments accordingly if they plan on submitting to a journal with these reporting requirements. similarly, if researchers are required to state what the experimental unit is , then they will be prompted to think hard about the issue and design better experiments, or seek advice. this recommendation will not only improve the quality of reporting, but it will also improve the quality of experiments, which is the real benefit. a final advantage is that it will make quantitative reviews/meta-analyses easier because much of the key information will be on a single page.

make raw data available
another solution is to make the provision of raw data a requirement for acceptance of a manuscript; not “to make it available if someone asks for it”, which is the current requirement for many journals, but uploaded as supplementary material or hosted by a third party data repository. none of the vpa studies provided the data that the conclusions were based on, making reanalysis impossible. remarkably, of the thirty-five studies published, only one provided the necessary information to conduct a power analysis to plan a future study  <cit> , and this was only because one animal per litter was used and the necessary values could be extracted from the figures. datasets used in preclinical animal studies are typically small, do not have confidentiality issues associated with them, are unlikely to be used for further analyses by the original authors, and have no additional intellectual property issues associated with them given that the manuscript itself has been published. it is noteworthy that many journals require microarray data to be uploaded to a publicly available repository , but not the corresponding behavioural or histological data. it is perhaps not surprising that there is a relationship between study quality and the willingness to share data  <cit> - <cit> . publishing raw data can be taken as a signal that researchers stand behind their data, analysis, and conclusions. funding bodies should encourage this by requiring that data arising from the grant are made publicly available .

the above suggestions would help ensure that appropriate design and analyses are used, and to make it easy to verify claims or to reanalyse data. currently, it is often difficult to establish the former and almost impossible to perform the latter. moreover, it is clear that appropriate designs and analyses are often not used, making it difficult to give the benefit of the doubt to those studies with incomplete reporting of how experiments were conducted and data analysed.

CONCLUSIONS
while it is difficult to quantify the extent to which poor statistical practices hinder basic and translational research, it is clear that a large inflation of false positive and false negative rates will only slow progress. in addition, coupled with researcher degrees of freedom and publication bias, it is possible for a field to converge to the wrong answer. experimental design and statistical issues are, in principle, fixable. improving these will allow scientists to focus on creating and assessing the suitability of disease models and the efficacy of therapeutic interventions, which is challenging enough.

abbreviations
anova: analysis of variance; bp: blood pressure; mpep: 2-methyl-6-phenylethyl-pyrididine; sod1: superoxide dismutase.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
sel planned and carried out the study, performed the literature search and analysis, and wrote the paper. le provided constructive input. all authors read and approved the final manuscript.

supplementary material
additional file 1
r code for the analyses and power calculations. code for the analyses and power calculations are given as a plain text file.

click here for file

 additional file 2
raw data. raw data from mehta et al.  <cit> , including body weight, locomotor activity and anxiety measures from the open field test, grooming behaviour, and number of marbles buried in the marble-burying test. details can be found in the original publication.

click here for file

 additional file 3
list of vpa studies. list of the thirty-four studies using the vpa rodent model of autism.

click here for file

 additional file 4
power analysis for the mixed-effects model and the incorrect analysis. the interpretation of the graphs is the same as figure  <dig> . panels a and b are for the mixed-effects model and are nearly identical to the results for averaging the values within each litter and then using a t-test . panels c and d ignore litter and compare all of the data with a t-test, which results in an artificially inflated sample size and inappropriately high power.

click here for file

 acknowledgements
the authors would like to thank the siegel lab at the university of pennsylvania for kindly sharing their data.
