BACKGROUND
the rapid accumulation of molecular sequence data that is driven by novel wet-lab sequencing techniques such as pyrosequencing  <cit>  and collaborative whole-genome sequencing projects such as the  <dig> k vertebrate genome project http://www.genome10k.org/ pose unprecedented challenges with respect to the scalability and numerical stability of phylogenetic inference programs. datasets are continuously growing with respect to the number of base-pairs and/or the number of taxa. for likelihood-based  <cit>   codes with their extremely high computational requirements in terms of memory and floating point operations, improving scalability for large datasets is particularly challenging. here, we focus on algorithm design, improvement of numerical stability, and technical solutions for accelerating the likelihood function and reducing the memory requirements on phylogenomic datasets with missing data that contain more than  <dig>  taxa. the concepts we introduce are generic, that is, they can be applied to other likelihood-based programs such as iqpnni  <cit> , garli  <cit> , phyml  <dig>   <cit> , fasttree  <dig>   <cit> , mrbayes  <cit> , phylobayes  <cit> , and beast  <cit>  or to libraries for computing the phylogenetic likelihood such as beagle http://code.google.com/p/beagle-lib/.

the largest published maximum likelihood tree to date contained approximately  <dig>  taxa  <cit> . fasttree  <dig> has been used to infer approximate maximum likelihood trees of approximately  <dig>  taxa  <cit> , and the largest published tree using parsimony contained approximately  <dig>  taxa  <cit> . with novel alignment assembly methods such as, for instance, phlawd  <cit> , increasing data availability, and collaborative projects for reconstructing huge trees  there exists a need to infer even larger trees exceeding  <dig>  taxa. raxml version  <dig> . <dig> alpha  already incorporates some of the mechanisms presented here. it has been successfully deployed --without crashing-- to infer biologically reasonable maximum likelihood  trees on phylogenomic datasets with 10- <dig> genes for  <dig> ,  <dig> , and  <dig>  taxa.

methods
in the following we discuss three different topics:  the design of a new search algorithm for large datasets,  an appropriately adapted re-implementation of the subtree equality vector  technique in raxml, and  numerical issues that arise with the widely used Γ model of rate heterogeneity  <cit> .

phynav revisited: constraining the tree search to a backbone tree
phynav  first introduced the idea to reduce the dimension of the tree  on which the search is conducted, by identifying subtrees of closely related taxa whose root may be represented by a single virtual tip. the rationale is that in large alignments there may exist many taxa that are closely related to each other which can therefore be clustered together into a single virtual tip . by clustering taxa into virtual tips, the dimension of the tree can be reduced allowing for a tree search on the backbone tree that is induced by the virtual tips.

given a hypothetical perfectly balanced tree, a reduction of 50% could correspond to collapsing each pair of taxa into a single virtual tip. thus, for each pair of tips, there would be one less inner node to operate on, and the total number of inner nodes would have been halved.

we henceforth denote such a reduction of the tree dimension as reduction factor and denote the reduced unrooted tree that is induced by the virtual tips as backbone tree.

once an appropriate backbone tree has been computed , a spr-based  search, or any other heuristic search strategy using, for instance, nni  or tbr  moves, can be restricted to operate within this backbone tree. in other words, the virtual tips are interpreted as tips in the backbone tree on which we conduct the tree search. in our raxml proof-of-concept implementation that deploys spr moves, only subtrees that form part of the backbone tree are pruned and will exclusively be re-inserted into branches that lie within the backbone.

despite restricting the tree search to the backbone, in our setup, we always compute the log likelihood score of the comprehensive tree during the backbone tree search. the log likelihood score of the comprehensive tree can be easily computed, because virtual tips are ancestral probability vectors that summarize the signal of the  real tips situated below the respective virtual tip. note that, memory requirements for storing the ancestral probability vector representing a virtual tip are significantly higher than for storing a terminal taxon. for terminal taxa, it suffices to store the molecular sequence as an array of single bytes and to use a lookup table for obtaining the corresponding tip probability vector .

one way to implement a phynav-like method comprises the following computational steps: initially, generate a reasonable starting tree, using, for instance, parsimony. then, determine an appropriate backbone tree and optimize branch lengths and model parameters on this comprehensive tree under ml. thereafter, determine and mark the ancestral probability vectors that will become virtual tips in the backbone. finally, conduct a tree search on the backbone tree.

to also achieve a memory footprint reduction , one can write a multiple sequence alignment for the backbone to file that will partially consist of nucleotide sequences and partially of ancestral probability vectors representing virtual tips. this reduced alignment can then be parsed together with the backbone tree for conducting a tree search. by deploying phynav-like algorithms, one can save memory, if inner nodes  are excluded from the backbone, since ancestral probability vectors largely dominate the memory requirements of likelihood-based programs .

in terms of algorithm design, the issue that predominantly affects performance is the computation of the backbone tree, that is: how do we determine "good" virtual tips?

building the backbone
to build a backbone, we assume that a reasonable  fully resolved comprehensive tree t comprising all taxa  is provided as input. this comprehensive n-taxon tree has n tips and n- <dig> ancestral  nodes.

the second parameter for the backbone tree algorithm is the desired tree size reduction factor r, where  <dig>  <r <  <dig> . this parameter denotes to which fraction of n the backbone tree shall be reduced in size. ideally, the backbone tree will then comprise n·r- <dig> ancestral nodes and n·r backbone tips. backbone tips may either be virtual tips  or real tips. evidently, choosing very low values of r may significantly impact the quality of the inference, specially if very short branch lengths are present. according to our experience , using r >  <dig>  is a safe lower bound.

our backbone construction algorithm executes two main computational steps that are described in more detail below. initially, we assign the n tips to n·r clusters, that is, c = ⌈n·r⌉, where c is the total number of clusters obtained. for each tip we store a cluster identifier that denotes to which cluster the tip has been assigned. thereafter, we traverse the tree and use the cluster identifiers to label all ancestral nodes as residing inside, outside or on the boundary of the backbone.

tip clustering
there are plenty of possible approaches to cluster tips. first, the available topology itself  can be used directly as a hierachical tree . another alternative may be to compute parsimony scores of subtrees, and then cluster together according to a threshold. here, we only present an approach based on computing a distance matrix and applying average-linkage hierarchical clustering  <cit> . our assesment indicated this approach yields significantly better results than the others.

in standard hierarchical clustering, the first step consists of calculating a distance matrix that contains the pair-wise distances between all items  to be clustered. however, given a comprehensive tree t with ml estimates of branch lengths, we can directly obtain this distance matrix from the tree by calculating the pair-wise patristic distances. the patristic distance between two taxa is the sum of branch lengths on the path in the tree connecting the two taxa. thus, the distance matrix is symmetric. the space requirements for storing such a patristic distance matrix are in o which can become prohibitive for large alignments with n ≥  <dig>   <dig> tips. we observe that, the pair-wise patristic distances between most tips will be very large and hence these tips will be assigned to different clusters anyway. therefore, to save memory, one can decompose this process into computing several smaller, partial distance matrices, since the comprehensive starting tree already induces a hierarchical clustering structure. if we subdivide the problem into computing p partial pair-wise distance matrices, and each partial matrix defines c= ∑i=0kci=n⋅r, so that the total number of clusters, we need to ensure that desired clusters still corresponds to the specified reduction factor r. to achieve this, we do not fix the number of partial matrices k a priori. instead, we define a threshold value m that represents an upper bound for the number of tips contained in each partial matrix. let n be the total number of taxa, ni the number of tips in a partial matrix, where ni ≤ m and n= ∑i=0kni. from each partial matrix, we extract an amount of clusters proportional to its size, that is, ci∝c×nin.

this is implemented as follows: first, we find a set of subtrees such that  each subtree has as many tips as possible and at most m tree tips and  each tree tip is included in exactly one subtree, that is, all tree tips are included in one subtree and no tip forms part of more than one subtree.

for each such subtree i, we then build a  patristic distance matrix for all ni subtree tips. thereafter, we cluster them, by generating a hierarchical cluster tree. this hierarchical tree may be cut at different levels to generate a varying number of subtree tip groups. we choose to cut the the tree such that it generates ci clusters of subtree tips. if required, the number of desired clusters ci will have been iteratively adjusted beforehand  for each partial matrix i to ensure that c= ∑i=0kci.

for example, consider a  <dig>  000-taxon tree, a reduction factor of  <dig>  , and a partial matrix threshold of  <dig>   <dig> taxa. in this example, we may obtain distance matrices of  <dig>   <dig> and  <dig>   <dig> taxa respectively. then we will need to extract  <dig>   <dig> clusters from the  <dig>   <dig> taxa distance matrix and  <dig>   <dig> clusters from the  <dig>   <dig> taxa distance matrix.

to be able to apply this method and compute partial patristic distance matrices, we need to devise an algorithm that selects subtrees from the comprehensive phylogeny such that they contain at most m taxa. we start by selecting the innermost node of the tree . consider that, each inner node i of an unrooted binary tree t can be regarded as a trifurcation that defines three subtrees ti, a, ti, b and ti, c. we define subtree length stl as the sum of all branch lengths in subtree ti. thus, stl + stl + stl = stl holds for any inner node i, where t is the comprehensive tree.

in our current default implementation, we select the innermost node j that maximizes stl - max{stl, stl, stl}. an alternative criterion for selecting the innermost node is to determine the node that minimizes the variance of the three outgoing subtree lengths. other possible criteria, that are not based on subtree length may be defined, for instance, as finding the node that minimizes the variance of the node-to-tip distance or finding the node with the highest minimum node-to-tip distance. the node-to-tip distance is defined as the sum of branch lengths on the path in the tree leading from an ancestral node to a tip.

we conducted an empirical assessment  of these alternative approaches for determining the innermost node of a tree. the outcome  was that the respective innermost nodes  are either identical or close neighbors, that is, located in the same region of the tree.

backbone construction
once we have determined the innermost node, we conduct a depth-first tree traversal starting at this node and descend into each of the three subtrees. the depth-first traversal terminates, when a subtree root is encountered that comprises ≤ m tips. all subtree roots that contain ≤ m tips are stored in a list for further processing. thus, when the depth-first traversal has been completed, this list of k subtree roots can be used to generate the k partial patristic distance matrices of maximum size o. in our implementation, we set m :=  <dig>  this a suitable value, since only a few seconds are required for processing partial distance matrices.

for each subtree root , we determine how many clusters should approximately be extracted, via c¯i:=12+c⋅nin, where i is the cluster  number, ni is the number of tips in the respective cluster/subtree, c = n·r is the total number of desired clusters, and c¯i is the number of clusters for subtree i. in general, c≠∑i=0kc¯i. the overhead, or deficit for that matter, of clusters, that is given by Δc=c-∑i=0kci, is then proportionally distributed across all remaining partial matrices. this process is repeated iteratively until no overhead  remains. in each iteration, we reassign ci:=c¯i+Δc⋅c¯ic until c= ∑i=0kci for every i.

then, for each subtree i =  <dig> ..k we proceed as follows:

for all tips in subtree i, calculate the patristic distances to all other tips in this subtree and save them in the respective distance matrix.

apply pairwise average clustering to generate a hierarchical tree of joins from the distance matrix.

cut the tree, such that exactly ci clusters are generated.

add those clusters to a global list of clusters. maintain a list that keeps track to which cluster a tip belongs.

when all subtrees have been processed, we have a list of c clusters. note that, each cluster contains x tips, where  <dig> ≤ x ≤ m and that each tip is assigned to exactly one cluster. the step to build the backbone from the clusters is not trivial. we use labels  to identify which nodes belong to the backbone and which ones do not.

the backbone tree is defined by nodes marked as inside and boundary. once the clusters have been computed, we build the backbone as follows: initially, we label each inner node in the tree as inside, tip nodes which belong to clusters of size one as boundary, and all remaining terminal nodes as outside. in addition, we maintain a list for storing the cluster identifiers of ancestral nodes that will not form part of the backbone.

once this is done, we update/adapt the backbone assignment for ancestral nodes: the nodes of the comprehensive tree that represent the k subtree roots will remain inside the backbone. on each of the k subtree roots, we initiate a post-order traversal to relabel the ancestral nodes, if required, according the following rule set:

if the two child nodes are labeled as inside or boundary, the ancestral node remains labeled as inside.

if one child is labeled as inside or boundary and the other child as outside, the ancestral node is relabeled as inside and the outside child node is relabeled as boundary.

if both children are labeled as outside, we need to check to which cluster they belong. if they belong to the same cluster, the parent node is labeled as outside and the shared cluster identifier of the child nodes is propagated to the parent node. if the two children do not belong to the same cluster, the parent node is labeled as inside and both children are relabeled as boundary.

when the post-order traversal is about to be completed, we arrive at the subtree root i again, which was originally labelled as inside. at this point, we check whether the adjacent backbone node of the subtree root i has been labeled as outside. whenever this is the case , the adjacent backbone node is relabeled as boundary for consistency.

given a set of tips that form part of the same cluster, it may occur that these tips also form a monophyletic group. in this case, during the postorder traversal, all ancestral nodes will be grouped together under the same cluster identifier and the common ancestral node will become a backbone boundary . however, if the tips in a cluster are not monophyletic , the application of the above rules requires some additional boundary relabelling.

based on the prolegomena, a single cluster may thus induce more than a single virtual tip. as a consequence, the number of virtual tips may actually be higher than the number of clusters. in turn, the reduction of tree size that can be achieved will be smaller than specified by r. the impact and frequency of occurrence of this phenomenon  depends on the shape of the tree and the branch lengths. in table  <dig>  we outline this effect for trees with  <dig>  and  <dig>  taxa. we computed the average number of virtual tips generated by our algorithm on  <dig> distinct trees per dataset and reduction factors of  <dig>  and  <dig>  respectively.

the average number of backbone tips is higher than the expected number n·r

tree searches on the backbone
we have implemented the above algorithm in a dedicated raxml version that is available for download at http://wwwkramer.in.tum.de/exelixis/software/backbonesearch.zip. initially, raxml will generate a comprehensive randomized stepwise addition order parsimony tree, or read in a user specified tree via -t. then it will optimize ml model parameters--including branch lengths--on the comprehensive tree. thereafter, it will execute the backbone algorithm as described above. the tree searches on the backbone are based on the standard raxml hill-climbing algorithm. lazy spr moves are only conducted within the backbone. after each cycle of spr moves , the backbone tree will be re-computed based on the currently best tree. also, the branch lengths of the entire tree  will be re-optimized once after each spr cycle.

subtree equality vectors re-visited
we introduced and implemented the concept of subtree equality vectors  to accelerate likelihood computations by reducing the number of required floating point operations in  <dig>  <cit> . conceptually similar approaches were presented in  <dig>  <cit>  and  <dig>  <cit> .

the underlying idea is based on the following observation: given two identical alignment sites i and j that evolve under the same evolutionary model  and for which a joint branch length has been estimated, their per-site log likelihoods lnl and lnl will be identical. hence, to save computations, one can compress the identical sites into a single site pattern and assign a respective site pattern count  to this site pattern. thus, for two identical sites i and j, we can compute the per-site log likelihood as 2·lnl. this global compression of alignments  is implemented in all current likelihood-based codes.

this basic idea of site compression can be extended to the subtree level, by using sevs for instance, to save additional computations. consider the equation  <cit>  for computing the ancestral probability vector entry for observing nucleotide a at site i of a parent node node p, with two child nodes l and r given the respective branch lengths bl and br and transition probability matrices p and p:

  l→a=l→s)l→s) 

we observe that, if the site patterns in the subtree  rooted at p at sites i and j are identical, and if the transition probability matrices p and p are identical at sites i and j , then ls=ls for all states s . thus, we can avoid re-computing all ancestral states for site j if we have already computed the ancestral states for site i.

the key technical challenge with this approach is that it requires a large amount of bookkeeping, to keep track of identical subtree site patterns . moreover, sevs require additional data structures and a case switch in the innermost loop of the likelihood function that iterates over the sites of the ancestral probability vectors, which may lead to cache misses and incorrectly predicted conditional jumps by the processor hardware. because of these observations we had abandoned this approach completely in raxml.

however, the advent of gappy phylogenomic alignments, that is alignments that contain a large amount of structured  missing data regions per gene for the taxa under study, motivated us to re-assess sevs in a simpler and thus more efficient setting.

gaps and undetermined characters are mathematically equivalent in the standard ml framework. since structured patches of missing data dominate current phylogenomic datasets , we only track subtree site patterns that entirely consist of gaps/undetermined characters . thereby, we avoid the more complex task  of tracking all identical subtree site patterns . this restriction simplifies the required bookkeeping procedure and data structure significantly, because we only need to know whether a subtree site consists entirely of gaps or not. thus, given an alignment with n sites, it suffices to enhance the data structures for storing tips and inner nodes by a simple bit vector with n bits. if all-gap sites are represented by  <dig> and non-gap sites by  <dig>  we simply need to execute a bit-wise and on the respective bit vectors of the child nodes l and r in conjunction with the tree traversal for computing the likelihood to determine the all-gap sites at the ancestral node p . we can then use this bit vector at p to determine if we need to compute something at a site i or not.

we have implemented this method for dna and protein data under the Γ model of rate heterogeneity in raxml v <dig>  available at http://wwwkramer.in.tum.de/exelixis/software.html. evidently, the efficiency of this approach depends on the proportion of gaps/missing data and the distribution of gaps in the input alignment. since areas of missing data are typically well-structured in current phylogenomic datasets, this approach is expected to work well with this kind of input data. to facilitate the deployment of the sev-based version of the likelihood function, we have integrated an automatic performance test that decides whether to use the sev-based or the standard likelihood implementation. when the starting tree has been computed or parsed by raxml, the program will execute a full tree traversal  for the standard and the sev-based likelihood function implementation and measure the respective execution times. if the execution time of the sev-based approach is 20% smaller than that of the standard implementation, raxml will automatically use the sev-based implementation for all subsequent likelihood computations. the threshold of 20% is based on empirical observations. while sevs can speed-up ancestral probability vector computations, sevs slightly slow down the branch length optimization and likelihood computation  functions because of the memory accesses to the bit vectors.

saving memory with sevs
sevs as implemented here, can also be deployed to reduce memory requirements. as mentioned above, if, at an ancestral node p we encounter an all-gap site, we completely omit its computation. in order to accomplish this, we need to maintain only one additional ancestral probability vector site, that contains the signal for all-gap sites. consider an ancestral probability vector where 50% of the entries in the all-gap site bit-vector are set to  <dig>  that is, where we only need to compute 50% of the ancestral probability vector entries with respect to the total alignment length.

we can observe that, in addition to saving 50% of the computations required for this ancestral probability vector, we can also save 50% of the memory space required for storing the ancestral probability vector . thus, the memory requirements for each ancestral node can be determined on-the-fly as we traverse the tree, by subtracting the number of entries that are set to  <dig> in the bit vector from the input alignment length. remember that, the bit vectors we deploy are always as long as the input alignment.

the key technical problem that arises is that, the required ancestral probability vector lengths at inner nodes will change dynamically when the tree topology changes or even when the tree is just re-rooted. given a rooting of the tree, one may think of this as ancestral probability vectors becoming longer while one approaches the root of the tree. at present we have implemented this by dynamically freeing and allocating memory  and malloc()) at each ancestral node. the reallocation only takes place when the all-gap bit-vector count  corresponding to the required ancestral probability vector does not equal the all-gap bit-vector count of the current ancestral probability vector at an ancestral node.

note that, the concepts presented here can also be applied to phylogenomic datasets with joint branch length estimates across partitions, while the conceptually different ideas presented in  <cit>  can only be applied to partitioned phylogenomic datasets with per-partition branch length estimates.

numerical problems of the Γ model of rate heterogeneity
numerical scaling of the entries in the ancestral  probability vectors during likelihood computations on trees, for avoiding numerical underflow has become a standard technique that is implemented in most likelihood-based programs . for an overview of numerical scaling techniques, please refer to  <cit> . a numerical problem that arises for very large trees with more than approximately  <dig>  taxa in raxml  is associated with the widely used  <cit>  Γ model of rate heterogeneity  <cit> .

for the Γ model, a discrete approximation  is used to approximate the integral of the likelihood over the Γ curve at each site. that is, instead of computing the ancestral probabilities l, l, l, l for the  <dig> nucleotides a, c, g, t at a specific site of an ancestral node in the tree , one needs to compute those probabilities for all  <dig> discrete Γ rates r <dig>  r <dig>  r <dig>  r <dig>  thus, every site of an ancestral probability vector comprises  <dig> values:

  lr <dig> lr <dig> lr <dig> lr <dig> ......,lr <dig> lr <dig> lr <dig> lr <dig>  

the numerical problem that arises with the Γ model on very large trees is that those  <dig> values need to be jointly scaled numerically  to avoid numerical underflow .

scaling of the probability vector entries may be conducted as follows: at a specific site c of an ancestral probability vector for dna data l→ we scale the entries if, for instance,

  lr0<ε∧lr0<ε,......,lr3<ε∧lr3<ε 

where ε can be set to ε := 1/ <dig> under double precision arithmetics. thus, we decide to scale up all ancestral probability vector entries at a site c, when all unscaled entries for all discrete rates are smaller than some pre-defined ε.

other options for scaling exist. for instance, one calculates a scaling factor such that the largest of the  <dig> ancestral probability values at a site is scaled to  <dig> . one can also conduct this type of scaling at every ancestral probability vector without checking that all values are smaller than some ε. we also experimented with such alternative implementations for numerical scaling in raxml, but were not able to solve the general scaling problem for the Γ model of rate heterogeneity . with alternative scaling implementations, the fundamental numerical problem occurred again for slightly larger tree sizes.

if according to equation  <dig> a probability vector column c at vector l→ needs to be scaled, we simply multiply all entries

  la ⃗r <dig> lc ⃗r <dig> ...,lg ⃗r <dig> lt ⃗r <dig> 

by  <dig> 

in order to correct  the scaling multiplications  at the virtual root, we need to keep track of the total number of scaling operations conducted per column. for this, we use integer vectors u→ that maintain the scaling events and correspond to the respective probability vectors at inner nodes. as we traverse the tree to compute an ancestral vector l→ from two child vectors l→ and l→ the scaling vector is initially updated as follows u→:=u→+u→. then, if an entry of l→ needs to be scaled at position c we increment u→:=u→+ <dig>  the scaling vectors at the tips of the tree are not allocated, but implicitly initialized with  <dig> 

at the virtual root, given l→, l→ and the corresponding scaling vectors u→, u→, we can compute the likelihood under the Γ model as follows:

  l=εu+u 

where, for instance,

  qr0:=∑s=ar0tr0prsl→sr0)) 

if we take the logarithm of l this can be rewritten as:

  log)=+u)log+...+log 

as can be observed, if all  <dig> values are scaled jointly, the scaling multiplications can be easily undone numerically at the virtual root, when the overall likelihood of the tree is computed . this is not the case, if one intends to scale the ancestral probability values individually on a per-rate  basis.

the problem that arises with using Γ on very large trees is that, the  <dig> ancestral probability values , may have such highly divergent numerical values because of the  <dig> discrete rates r <dig>  r <dig>  r <dig>  r <dig>  that scaling across all  <dig> of them will still not prevent numerical under-flow. in other words, the smallest value of those  <dig> will be too small and the largest too large to fit into the representable machine number range between  <dig>  and  <dig> . scaling values above  <dig>  will yield numerical overflow and does hence not provide a solution either. at present, we are not aware how scaling multiplications can be undone  in a numerically stable way at the root, if one scales the probability values for each discrete rate category individually. this phenomenon could also occur for models other than Γ that only use four values per site , but will probably occur on significantly larger trees. while one could use extended precision libraries, as provided for instance by the gnu scientific library, the negative performance impact will be such, that the computation of large trees also becomes prohibitive.

it is however possible to address this scaling problem by discarding the per-rate/category likelihoods that contribute least to the overall likelihood and thereby approximate the gamma-based likelihood score of a site. this approach  has been implemented in phyml  <cit> .

to this end, we advocate the usage of per-site rate categories as proposed and implemented in raxml , phylobayes  <cit> , or fasttree  <dig>   <cit> . while methods that model among-site rate heterogeneity by using one rate per site can help to significantly reduce computational requirements , they can also help to resolve the aforementioned numerical problems with Γ on very large trees. note that, the cat approximation as implemented in raxml should not be confused with the substantially different cat model implemented in phylobayes. the unfortunate fact that an identical acronym is used is because at time of publication, the author of raxml was not aware of the phylobayes cat model that was introduced earlier.

one key issue with the so-called cat approximation of rate heterogeneity  <cit>  in raxml was that branch length values were meaningless. this has been corrected in raxml version  <dig> . <dig>  by appropriately re-scaling the per-site rate categories such that the mean substitution rate is  <dig> . while, as we show, the correctly scaled cat-based branch lengths are highly correlated  with the branch lengths obtained from the Γ model, the overall tree length obtained for Γ and cat-based branch lengths can vary significantly. analogous results were obtained for fasttree  <dig>   <cit> . this does not represent a problem as long as post-analysis tools for trees  do not rely on absolute branch length values.

another issue that needs to be addressed is that cat-based log likelihood scores across different runs  can not be compared directly, because the estimates and assignments of rate categories to sites may be slightly different for each search/tree topology. therefore, for comparing likelihood scores of trees under the cat approximation of rate heterogeneity, we need to score all alternative trees under the same assignment of rate categories to sites. raxml v <dig> implements the -f n option to score a set of fixed trees under the same rate category to site assignment.

finally, one also needs to assess how differently trees are ranked  with respect to their log likelihood scores, if scored under Γ or under cat. while one would not expect a perfect rank correlation , because both models accomodate rate heterogeneity, the correlation should not be too low either.

RESULTS
test datasets
to assess our methods, we used two large multi-gene datasets of plants.

the first dataset comprises  <dig>  taxa and  <dig>  sites and was obtained as follows: we assembled a dna sequence matrix of  <dig>  seed plant taxa consisting of the chloroplast regions atpb , matk , rbcl , trnk , and trnl-trnf , and the internal transcribed spacer , using the phylogeny assembly with databases tool . all sequence alignments were conducted using mafft version  <dig>  <cit>  for initial alignments and muscle for profile alignments  <cit> . alignment matrix manipulations were performed with phyutility  <cit> .

the second dataset comprises  <dig>  taxa and  <dig>  sites and was obtained using the same pipeline as described above. the gene regions used were atpb , matk , rbcl , trnk , and trnl-trnf , and the internal transcribed spacer .

for ease of reference we henceforth denote the  <dig>  taxon datasets as  <dig> k and the  <dig>  taxon as  <dig> k. trees computed on the  <dig> k dataset have recently been published  <cit>  and the alignment is available at http://datadryad.org/. the  <dig> k dataset is currently unpublished, but will be made available immediately upon publication.

backbone algorithm
to test the backbone algorithm we executed the dedicated raxml version  with the experimental -l command line option. this option initially builds a backbone tree and then deploys the cat approximation of rate heterogeneity  <cit>  with the standard raxml hill-climbing search algorithm  <cit>  to apply lazy spr moves  within the backbone only. we used tree size reduction factors of  <dig>  and  <dig> . as starting trees, we used randomized stepwise addition order parsimony starting trees generated with raxml v <dig> . for each dataset, we inferred  <dig> ml trees for each of the  <dig> parsimony starting trees. raxml was executed using the pthreads-based parallel version  <cit>  with  <dig> threads on unloaded quad-core amd opteron nodes with  <dig> cores and  <dig> gb ram each.

we computed average runtimes over  <dig> runs for the  <dig> k and  <dig> k datasets respectively. for each backbone tree, we also computed the theoretical minimum number of bytes  required to store the ancestral probability vectors at the virtual tips and the inner nodes which dominate memory requirements. if the branch length optimization process, unlike in our current implementation, is limited to optimizing branches within the backbone, this theoretical minimum value represents a good estimate of the memory footprint for a backbone tree search. we also computed the respective memory requirements for the comprehensive tree , which reflects the 'standard' memory requirements when no reduction factor is applied.

these values  provide a notion of the potential memory savings that can be achieved by the backbone approach. in tables  <dig> and  <dig> we also provide the respective execution times and average log likelihood scores obtained by using the backbone algorithm  and a comprehensive search on the full tree . those values have been averaged over  <dig> runs . while execution times can be reduced by the backbone approach, log likelihood scores obtained by conducting searches on a backbone are slightly worse than those obtained by searching on the full tree. also note that, for unfavorable tree shapes, that is, tree shapes where a substantial part of the phylogenetic signal is located at or near the tips, a too aggressive setting of r may potentially generate unfavorable results since this signal can be lost in the backbone. however, some exploratory tests with simulated data  did not show such an effect for backbone searches.

in figures  <dig> and  <dig> we show that the choice of the random number seed , that determines the shape of the starting trees, has a significant impact on the final log likelihood score , irrespective of the search strategy that is used. on average, searches on the full tree yield better likelihood scores than searches on backbone trees. however, the variance of the likelihood score as a function of the starting tree  is analogous to the score variance between full and backbone tree searches. for example, on the  <dig> k dataset, the log likelihood scores on  <dig> final trees obtained for full searches show a standard deviation of  <dig> log likelihood units. the average difference in log likelihood scores per starting tree between the full search and a backbone search with r :=  <dig>  is only  <dig> log likelihood units and  <dig> log likelihood units for backbone searches with r :=  <dig> , respectively.

given the runtimes savings that can be achieved by the backbone approach, backbone tree searches can be used, for instance, to explore a larger number of parsimony starting trees which substantially influence the final log likelihood scores. a reasonable strategy for finding best-known ml trees may consist in starting many fast searches with a relatively aggressive setting of r :=  <dig>  to identify/determine a set of 'good' starting trees that yield the best final log likelihood scores. in a second step, full tree searches can be conducted on those promising starting trees to find trees with even better scores.

we used simulated datasets in order to better understand the impact of the backbone algorithm on topological accuracy. we ran indelible  <cit>  to generate simulated msas of  <dig> taxa  and  <dig> taxa . we compared the symmetric difference  between the true tree and the topologies from the starndard full search and the backbone-based ones. for each dataset, the full search and the backbone search with r :=  <dig>  and r :=  <dig>  were ran  <dig> different times with different starting trees. table  <dig> shows the average symetric differences among all approaches for the dataset with  <dig> taxa. we see that, in terms of topological accuracy, applying the reductions of r :=  <dig>  and r :=  <dig>  yield topologies that remain close to the standard full search. furthermore, the distance to the true tree is not increased by the reduction.

the likelihood scores for both simulated datasets follow the same pattern as in the case of real data. these results, details on how the simulation datasets were generated, as well as the symetric difference for the  <dig> taxa dataset have been included in the additional file  <dig> 

sev performance
we also used the  <dig> k and  <dig> k datasets to test memory savings and speedups achieved by applying the adapted sev technique to phylogenomic datasets. the gappyness  is  <dig> % for  <dig> k and  <dig> % for  <dig> k, respectively.

for each alignment, we computed a parsimony starting tree with raxml that was then evaluated  with raxml under the gtr+Γ model using the sev reimplementation  and using the standard likelihood implementation.

the standard implementation required  <dig> gb of memory on the  <dig> k dataset and  <dig> gb of memory on the  <dig> k dataset. the sev technique with the memory saving option enabled  reduced memory footprints under Γ to  <dig> gb  and  <dig> gb  respectively. the log likelihood scores for all three implementations were exactly identical. as shown in tables  <dig> and  <dig>  the runtimes of the sev-based versions are 25-40% faster than for the standard implementation. the runtime differences between the sev-based implementation with memory saving enabled and the plain sev version without memory saving, can be attributed to differences in memory access patterns. while both versions conduct the same number of computations, the memory-saving version needs to make millions of calls to os routines  and malloc()) while the plain sev version exhibits a higher memory footprint and thereby, potentially, a higher cache miss rate.

execution times and memory requirements for optimizing model parameters and branch lengths under on the  <dig> k dataset using sevs, sevs with memory saving, and the standard likelihood implementation.

execution times and memory requirements for optimizing model parameters and branch lengths under on the  <dig> k dataset using sevs, sevs with memory saving, and the standard likelihood implementation.

estimating branch lengths and computing likelihood scores with cat
we optimized branch lengths and model parameters under cat and Γ using raxml v <dig>  on collections of  <dig> and  <dig> final ml trees for the  <dig> k and  <dig> k partitioned datasets, respectively. for the cat model, we also assessed the impact of using,  <dig>   <dig>   <dig> , and  <dig> per-site rate categories.

for each tree, we computed the average pearson correlation coefficient between the branch lengths obtained under cat  and the branch lengths as estimated under Γ. we also computed the average tree length ratio over all  <dig> trees.

to determine if the trees are ranked in the same order by their respective Γ and cat log likelihood scores, we computed the spearman rank correlation of the cat- and Γ-based tree rankings. as shown in tables  <dig> and  <dig>  the spearman correlation was above  <dig>  in all cases. this indicates that, trees are ordered in almost the same way, regardless of whether they are scored under cat or Γ.

correlation between cat and Γ-based ml branch length estimates, total tree length ratios, and spearman rank correlation coefficients between likelihood-induced tree rankings obtained from cat and Γ for dataset  <dig> k.

correlation between cat and Γ-based ml branch length estimates, total tree length ratios, and spearman rank correlation coefficients between likelihood-induced tree rankings obtained from cat and Γ for dataset  <dig> k.

we also used fasttree  <dig>  to score both collections of trees under the hybrid cat/gamma <dig> model  <cit> . once again, the spearman rank correlation of fasttree cat/gamma <dig> and raxml Γbased tree rankings remained above  <dig>  in all cases.

the average branch length correlation between cat and Γ optimized branches was above  <dig> . on those two large datasets, the absolute length of Γ-based branch length estimates was larger than for cat as shown by the average tree length ratios.

we also executed analogous analyses on  <dig> smaller single-gene  datasets with  <dig> up to  <dig> taxa. in addition, we evaluated significantly larger ml tree collections  for those smaller datasets. these additional experiments confirmed our observations for the  <dig> k and  <dig> k datasets and also revealed that the total tree length under cat, can also be larger than the Γ-based tree length. respective plots for all datasets are provided in the additional file  <dig> 

CONCLUSIONS
we have explored several techniques, addressed problems, and proposed some solutions for phylogenetic tree inference with likelihood-based methods on trees with several tens of thousands of taxa.

initially, we revisit and re-assess techniques for reducing the tree size, inspired by earlier work on a program called phylogenetic navigator . significant effort was invested in exploring different backbone construction techniques . here, we describe the method that worked best with respect to final log likelihood scores. such backbone-based techniques can help to reduce memory footprints and execution times. however, in almost all cases they yield final trees with worse likelihoods compared to comprehensive tree searches on a full, unreduced tree. we find that likelihood scores of final trees heavily depend on the respective starting trees, and conclude that backbone approaches can be deployed for identifying 'good' starting trees, that can then be further refined using a comprehensive tree search.

we have adapted and re-implemented the sev technique for phylogenomic datasets with missing data and enhanced it by a novel memory-saving option. this new technique, is generally applicable to all likelihood-based codes and can reduce execution times by 25-40% on sufficiently 'gappy' datasets by omitting redundant computations. more importantly, the revised sev technique can be deployed to achieve significant memory savings that are almost proportional to the amount of missing data in the test datasets. this technique has already been fully integrated into the standard raxml distribution . moreover, raxml will automatically determine whether to use the standard likelihood implementation or the sev-based likelihood implementation.

finally, we analyze problems associated to numerical scaling for avoiding underflow, that can occur when using the Γ model of rate heterogeneity on very large datasets. while for the  <dig> k and  <dig> k datasets we were still able to evaluate trees under Γ, on some even larger datasets that we are currently analyzing  numerical scaling under Γ appears to be impossible using 64-bit floating point arithmetics. to this end, we advocate the usage of models that rely on per-site rate categories for accommodating rate heterogeneity among sites. clearly, further research is required in this area to devise statistically robust and meaningful models. nonetheless, we provide an empirical assessment of branch length estimates as obtained under Γ and the raxml-specific implementation for estimating and assigning per-site evolutionary rate categories. we find that, given proper scaling of per-site rates, branch lengths between cat and Γ based trees are highly correlated, despite the fact that absolute branch length values can differ substantially. we also find that, ordering tree collections using Γbased and cat-based log likelihood scores induces very similar rankings of trees as determined be the spearman rank correlation coefficient.

the work presented here has a clear exploratory flavor and we hope that it will be useful to the community for identifying future research directions pertaining to large-scale phylogenetic inference using likelihood-based methods. the problems and solutions we discuss in this paper, emerged within the framework of the plant tree of life grand challenge project that aims at reconstructing the plant tree of life comprising approximately  <dig>  taxa.

authors' contributions
fic developed and implemented the backbone algorithm and conducted all computational experiments. as developed and implemented the sev-based techniques and implemented the cat re-scaling procedure. sas assembled the large test datasets. fic, as, and sas wrote and edited the manuscript.

supplementary material
additional file 1
supplementary material. assesment of alternative criteria to identify the innermost node of a tree. evaluation of the backbone algorithm with simulated data: simulation details and symmetric difference for the  <dig> taxa dataset, log likelihood scores for ml trees on simulated datasets. evaluation of the backbone algorithm with real data and comparison with fasttree  <dig>  correlation between cat and Γ-based ml branch length estimates, total tree length ratios, and spearman rank correlation coefficients between likelihood-induced tree rankings obtained from cat and Γ for  <dig> different datasets ranging from  <dig> up to  <dig> number of taxa. correlations between log likelihood scores under the raxml cat model and Γ model for the  <dig> k and  <dig> k dataset. correlations between log likelihood scores under the raxml cat model and the fasttree  <dig> cat/gamma <dig> model for the  <dig> k and  <dig> k dataset.

click here for file

 acknowledgements
fic is funded by the german science foundation , as is funded by the heidelberg institute for theoretical studies, sas is funded by the national science foundation . the authors would like to thank bernard moret for granting access to his amd barcelona nodes and morgan price for useful discussions regarding branch length issues under cat.
