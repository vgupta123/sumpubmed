BACKGROUND
the main goal of genome-wide association studies  is to identify the complex phenotype associated loci. a great success of the gwas leads us to move our focus on the application to personal genomics and clinical practice such as diagnosis, disease risk prediction and prevention.

in personal genomics, one can genotype their own genome using direct-to-consumer  genotyping service provided by personal genomics companies such as 23andme  and decodeme . personal genotyping will be followed by genome analysis and annotation for providing brief summary of genetic effects on various phenotypes of an individual. moreover, snpedia, a wiki based snp database, can be used to expand the information regarding genetic effect of snps  <cit> . personal genome data will be the baseline information for personal medical treatment and scientific research as the number of gwas and genomics studies are growing. personal genomics has already played an important role for scientific research. gwas using personal genome data successfully have unveiled novel loci for common traits and parkinson's disease  <cit> .

in a clinical aspect, genetic analysis has provided valuable information for clinical treatment. for example, mutation analysis of brca <dig> and brca <dig> in breast cancer is valuable for clinical treatment  <cit> . genotype information of vkorc <dig> and cyp2c <dig> can potentially be used as clinical information for estimating individual warfarin dose  <cit> . in genome-wide scale, ashley et al. studied clinical usefulness of genome information  <cit> .

the efforts described above are mainly focusing on interpretation of genomic information using previously identified phenotype associations. for diagnosis and clinical treatment, however, a more accurate phenotype prediction model is required. previous studies have performed the disease prediction using phenotype-associated snps  <cit> . however, the prediction using previously known disease associated loci has not been quite successful  <cit> . for example, the prediction performance for type  <dig> diabetes and coronary heart disease using known associated loci have area under the curve  ranging from  <dig>  ~  <dig>   <cit> . there are three main reasons of this poor predictability  <cit> . firstly, a limited number of previously known susceptibility variants were shown to explain only a small proportion of phenotypic variation  <cit> . secondly, in previous studies, relatively simple statistical approaches were applied to gwas data to explore disease associated loci. additive mode of genetic inheritance and regression model were used, while not considering the complex relationships of interactions between multiple loci contributing to disease risk. finally, genetic effect of variants would vary across phenotypes. previous studies reported a wide range of heritability ranging from  <dig>  to  <dig>   <cit> . a certain phenotype would be a result from the interaction of genetic and environmental effects.

there are two approaches to improve the poor predictability of phenotype based on the genetic variants. the first approach is to identify additional phenotype-associated loci and causal variants. a large scale genome-wide meta-analysis comprising tens of thousands individuals and next-generation sequencing technology are expected to unveil the hidden phenotype related loci. these methods would find more phenotype-associated loci and causal variants. although this approach is promising, it requires a relatively high cost. the second is to develop a more accurate and reliable prediction method. in general, the prediction procedure consists of two steps: feature selection and prediction. most previous efforts on disease prediction have largely focused on improving the performance of the prediction methods.

moreover, most studies used a set of snps selected by p-values of simple linear regression model  <cit> . low prediction performance of previous studies may partly be due to a failure to include genetic variants with complex relationship with phenotype. recently, wei et al. reported that prediction performance is varied by the number of variants used  <cit> , demonstrating that selection of number of variants for the prediction is important to predict the risk of phenotypes. single snp analysis may not be adequate for identifying multiple causal variants and predicting risk of disease  <cit> . however, only a few studies discussed about the variable selection methods on gwas data despite the importance of feature selection  <cit> .

in this study, we investigated the effect of feature selection on the performance of the prediction methods more thoroughly. in particular, we considered the following methods for feature selection and prediction: logistic regression, linear discriminant analysis, regularized regression analysis, support vector machine, and random forest. for these models we studied the effect of feature selection on the performance of prediction and suggested an optimal number of features for improving the predictability of phenotypes. our investigation was based on the analysis of gwa dataset of  <dig>  kare samples, for predicting smoking behaviors.

methods
data
dataset was obtained from the korea association resource  project as a part of korea genome epidemiology study . briefly,  <dig>  samples were genotyped using affymetrix genome-wide human snp array  <dig> . after quality control for samples and snps,  <dig>  samples and  <dig>  snps remained for subsequent analysis. the detailed information has been described in the previous studies  <cit> . in this analysis, we only used male samples for predicting smoking behaviors, because there are insufficient numbers of female smokers. among  <dig>  males, there were  <dig> individuals of non-smokers,  <dig>  individuals of former and  <dig>  individuals of current smokers. for number of cigarettes smoked per day  among smokers, kare provides samples of  <dig> ,  <dig>  ,  <dig> , and  <dig> . given the smoking status, we defined three dichotomous traits such as smoking initiation , cpd <dig> , and smoking cessation  . smoking behavior phenotypes are summarized in table  <dig>  the association results between the smoking behavior phenotypes and the snps have been reported in the previous studies  <cit> .

cpd10: binary phenotype of nicotine dependence  defined as < <dig> cigarettes/day and > <dig> cigarettes/day

for multiple snp analysis, we often encounter information loss due to missing values. therefore, we imputed missing genotypes using fastphase software  <cit>  and obtained complete genotype data for multiple snps analysis. in particular, we imputed  <dig>  samples with fastphase using options -t  <dig>  -k  <dig> and -c  <dig> 

feature selection and risk prediction
stronger statistical significance of snp in the association analysis does not always assure a better disease risk prediction  <cit> . moreover, most of previous prediction studies selected features based on the p-values from simple linear regression analysis. the emphasis on linear relationship between genotype and phenotype would omit the variants having complex relationship with phenotype. for studying the effect of feature selection on prediction performance, we used five statistical methods including logistic regression, linear discriminant analysis, penalized regression and data mining methods including support vector machine and random forest.

for feature selection, we used the scores computed by each prediction method to rank and select features. considering complex relationship between variants and phenotype, simultaneous variable selection using the whole chromosome would be the most appropriate approach. due to the immense amount of computation, however, we could not perform the analysis using the whole snps at the same time. alternatively, the two step approach was used for the feature selection as suggested by xu et al.  <cit> . first, all snps were partitioned into  <dig> chromosomal subsets. from each subset, feature selection was performed. second, all snps were ordered based on their scores and  <dig>  snps were selected for the additional joint feature selection. in this way, the snps showing the strongest association with the trait were selected for the subsequent prediction analysis. this two step approach was performed for each prediction method. we selected  <dig>  snps due to limitation of computing resource in our laboratory.

for phenotype prediction, we used the same five prediction methods which were used for the feature selection step to observe the effect of combination of different methods in feature selection and prediction.

logistic regression
a logistic regression model is one of most widely used methods in the analysis of genomic data. let yibe as a binary variable standing for the disease status , and xijdefines as additive snp value  according to the number of minor allele) for the jth snp.

for feature selection, single snp logistic regression  analysis was conducted.

 logpr1-pr=β0+β1xij 

where pr is the probability of subjects being cases .β <dig> and β <dig> are the coefficients of intercept and snp, respectively.

multiple logistic regression  was used for prediction.

 logpr1-pr=β0+ ∑j=1kβjxij 

where k is the number of the selected snps in the feature selection step. β <dig> and βj's are the intercept and effect sizes of snps, respectively.

elastic-net analysis
one caveat of using lr model in gwas is that linkage disequilibrium  dependency of input markers may make the parameter estimation unstable  <cit> . to address this issue, we imposed elastic-net regularization on the lr model building  <cit> . elastic-net regularization uses ridge and lasso penalties simultaneously to take advantages of both regularization methods. thus, it provides shrinkage and automatic variable selection and can handle more efficiently with the severe multicollinearity that often exists in gwa analysis. elastic-net regularization would perform better than lasso in gwa analysis, in which multicollinearity persistently exists due to linkage disequilibrium among nearby snps  <cit> . elastic-net regularization is particularly useful when the number of highly correlated predictor variables is much larger than the sample size. elastic-net regularization solves the following problem:

 minβ, 

where xi=t and β=t. elastic-net penalty is defined as pα= ∑|β|+α∑β <dig> where α is a weight of a value between  <dig> to  <dig>  cross validation  is generally employed to find the best values of λ and α, which minimize mean-squared prediction error  <cit> . based on the result of en, we selected snps with non-zero coefficients

linear discriminant analysis
linear discriminant analysis  is used to find linear combinations of features which characterize or discriminate two or more classes. lda is simple and fast. it often produces models with accuracy comparable to more complex methods  <cit> . lda is the classifier that separates the two or more classes by determining the projection matrix that maximizes the ratio of between-class covariance to within-class covariance  <cit> .

linear discriminant function is

 l=xtΣ-1-12tΣ-1+logp0p <dig> 

where μ <dig> and μ <dig> are means of the controls and cases, ∑ is common covariance matrix.

feature selection is based on ranking snps by correlation-adjusted t  scores  <cit> . the cat score measures the individual contribution of each single feature to separate two groups, after removing the effect of all other genes.

support vector machine
support vector machine  is a data mining approach for classification, regression, and other learning tasks  <cit> , which shows empirically good performance and successful applications in many fields such as bioinformatics, text and image recognition. no assumptions are required about the underlying model. svm finds an optimal hyperplane separating cases and controls and this process is based on large margin separation and kernel functions  <cit> .

for function

 f=<ω,Φ>+b 

where Φ  is a mapping function of × to a high dimensional space, svm find ω  and b such that minw,ξ12ωtω+c ∑ξi all {}, under the constraint yi≥1-ξiand ξi≥ <dig> for all i.

we used the radial-basis function as a kernel function.

 k=exp-∥xi-x′i∥22σ <dig> 

for snp selection, svm-rfe  algorithm for the variable selection task algorithm  <cit>  is used. we used r statistics package e <dig>  for model building, we adopted default options including the radial kernel.

random forest
random forest   <cit>  is a classification algorithm using sets of random decision trees which are generated by a bootstrap sampling for decision and voting. random subset of the variables is selected as the candidate at each split. rf has been widely used in pattern recognition and bioinformatics such as identification of gene and gene-gene interaction  <cit> . for feature selection, rf importance scores were used to rank and select snps. rf importance score is a measure for the relative contribution of a feature to the model.

cross-validation
the best strategy for performance comparison would be to use a separate validation set for accessing the performance of prediction models. however, the limited access to genomic data and phenotype information is often an obstacle to acquire a separate validation set. the second best strategy would be k-fold cross-validation comprising training data to fit the model and test data to measure the prediction performance.

in general, the whole data are split into k equal-sized subsets. typically, selecting k is depending on the user's choice, usually  <dig> or  <dig> are recommended. for building prediction model, k- <dig> sets among k subsets are used as training set and the rest is used as a test set for measuring performance. this procedure iterates k times. specifically, for i =  <dig> ,...,k the overall prediction performance is calculated from the k estimates of prediction performance  <cit> . in this study,  <dig> fold cross-validation was used to estimate the predictive performance for each prediction method.

prediction performance measure
for measuring prediction performance, we used the auc of the receiver operating characteristic  curve that is most widely used method for measuring prediction performance  <cit> . roc shows the relationship between sensitivity  and 1-specificity  at all possible threshold values. the auc score is used as the indicator of discrimination power for binary traits. the score is ranging from  <dig>  to  <dig> and higher score represents better discriminatory power.

RESULTS
we compared the performance of the prediction methods such as logistic regression , lda, svm, elastic net  and random forest . after the initial feature selection, we measured auc of each prediction algorithm by increasing the number of features  used for the prediction. performance comparison was based on 10-fold cv comprising 90% of samples for training set and rest 10% of samples for test set. auc was calculated by averaging the performance scores of ten trials of cross validation. figure  <dig> is the graph of prediction performance by various feature selection method varying the number of snps used in prediction for cpd <dig>  overall, the performance improves, as the number of features used for the prediction increases. lr and lda, however, showed decreasing tendency in their performance when more than 300~ <dig> variants were used for prediction. although an increasing trend of performance was observed over the number of features, a quite large number of features would not be required to achieve the best performance. for example, the performance of each algorithm increases rapidly until the number of snps reaches  <dig> and then increase slowly, as shown in figure  <dig> 

tables  <dig>   <dig> and  <dig> are the results of performance comparison with various feature selection methods for cpd <dig>  si and sc, respectively. each column provides information about the performance of feature selection for a given prediction method, while each row does about the performance of prediction methods for a given feature selection method. in each column, the best performance among feature selection methods for a given prediction method is marked as underlined. in each row, the best performance of prediction methods for a given feature selection method is boldfaced.

in each column, the best results are shown as underlined. in each row, the best results are boldfaced.

in each column, the best results are shown as underlined. in each row, the best results are boldfaced.

in each column, the best results are shown as underlined. in each row, the best results are boldfaced. a. # of snps with non-zero coefficient.

for feature selection, the performance of svm and en showed better results than lda, rf and simple lr methods. in overall, svm showed the best performance for feature selection. svm was the best with  <dig> and  <dig> snps for cpd <dig> and sc, while en showed the best performance with  <dig> snps for sc . as expected, simple lr and lda did not perform well enough to explain complex relationship between snps and phenotypes. however, lr was not the worst in feature selection for si and sc phenotypes. for si phenotype, en and lr were the best with  <dig> and  <dig> snps, respectively .

these results imply that one feature selection method would not be always the best for various phenotypes. thus, the prediction method seems to be carefully selected depending on the phenotypes.

for prediction, svm outperformed other prediction algorithms for any feature selection method. although svm showed the best performance in overall, it was not the best method when a relatively small number of features were used. for example, lr, lda and en methods outperformed the svm algorithm with features smaller than  <dig> for all phenotypes. with less than  <dig> snps, en was the best prediction method while svm was the best if over  <dig> snps were used for the prediction. for si and sc phenotype, the results were similar to those of cpd <dig> 

we investigated the best combination of feature selection and prediction algorithm. the best combination of feature selection and prediction algorithm with  <dig> snps were svm-en , en-lda  and en-en . the best combinations for more than  <dig> snps, however, were svm-svm , lr-svm , and svm-svm .

in the current study, we examined the performance of prediction methods in combination of feature selection and prediction algorithm. all prediction methods showed the best performances when around 400~ <dig> features were used. for prediction method, svm with radial kernel outperformed the other methods regardless of algorithms used in the feature selection.

discussion
in this study, the performance of prediction methods was compared in combination of various feature selection and prediction algorithms. also, the effect of the number of snps on prediction performance was investigated. in earlier study of wei et al.  <cit>  and kooperberg et al.  <cit> , limited studies of the prediction performance were presented. wei et al. used svm and logistic regression while kooperberg et al. used logistic regression and penalized regression such as lasso and elastic net. further, they have not discussed about the effect of various feature selection methods. although kooperberg et al. suggested penalized regression based feature selection during cross-validation using subset of data, they did not compare their results to other statistical methods such as svm, lda and rf which are used in our study. our study thoroughly performed the comparison analysis of combinations of feature selection and prediction algorithms including logistic regression, en, lda, svm and rf. complex relationship between phenotype and genetic variants was considered by using various statistical methods. therefore, our study is particularly valuable in the context of comprehensive comparison analysis for improving prediction performance by the various condition of the number of features, feature selection, and prediction algorithm. in addition, it is noteworthy that the feature selection in our study was conducted on genome-wide scale. this is an important difference in that the previous studies have a limitation in the feature selection, because it used the p-values from the simple linear regression method  <cit> .

the performance of prediction method via auc varied by phenotypes. the difference in magnitude of the prediction performance is dependent on the proportion of genetic effect on phenotypes  <cit> . for example, prediction performance for cpd <dig> was the best among smoking phenotypes. this is consistent with the level of heritability of smoking phenotypes. the estimated heritability was known to be about  <dig>  and  <dig>  for cpd <dig> and si, respectively  <cit> . based on these results, we may improve the prediction model by including the clinical variables for those of phenotypes with relatively low heritability.

overall, svm outperformed all other prediction methods in feature selection and prediction. note that svm is the most complex prediction algorithm. thus, the good performance of svm implies that complex relationship of genetic effect on phenotypes should be taken into consideration for selecting features and building a prediction model. however, it is also important to emphasize that svm was not the best if a relatively small number of features were used for phenotype prediction. therefore, feature selection and prediction algorithms should be carefully selected depending on the phenotypes. we first expected that prediction performance would be the best if the same algorithm is used for feature selection and prediction. however, our results indicated that a certain prediction methods did not provide the best fit for the features selected by the same algorithm. set operations, like union, intersection, or majority voting, can be applied to the feature selection process. for example, a union set of all selected features from different methods can be used for prediction. alternatively, the common features from two or more selection methods, defined as an intersection set, can be used for prediction. majority voting approach which chooses the features selected by more than half of the methods can also be used. however, our application of set operations to kare data did not improve the prediction results much . a further systematic study on set operations is desirable.

for measuring the prediction performance, we adopted 10-fold cross-validation. since we did within-study cross-validation, our performance measures may be overestimated. independent genotype data may be required for complete assessment of prediction performance comparison. the ultimate goal of genome study is clinical practice based on personal genome. in this context, our prediction analysis using genomic information is important in order to understand human genome and apply it to clinical studies.

CONCLUSIONS
in this study, we performed comprehensive comparison analysis for improving prediction performance by the various condition of the number of features and combination of feature selection and prediction algorithm including logistic regression, en, lda, svm and rf. overall, svm outperformed other methods in feature selection and model prediction. with less than  <dig> snps, en was the best prediction method while svm was the best if over  <dig> snps were used for the prediction.

competing interests
the authors declare that they have no competing interests.

authors' contributions
dky and tsp designed the study and dky, yjk and tsp carried out statistical analysis. tsp coordinated the study. dky, yjk and tsp wrote the manuscript. all authors read and approved the final manuscript.

