BACKGROUND
in phylogenetics, simulations have been widely used to study the robustness of inference methods  <cit>  and have been involved in parametric bootstrapping  <cit> . for instance, simulations have shown that maximum likelihood methods often more accurately reconstructed the evolution of an alignment than distance or parsimony methods  <cit> , but could also fail in conditions where compositional biases  or rate heterogeneity along branches  were too intense  <cit> . similarly, simulations have been used to compare topologies with respect to an alignment  <cit> , or to assess the fit of a model to a particular data set  <cit> . in this last case, a model has a good fit to a particular data set if the alignments it generates have properties similar to the properties of the real alignment. both for investigating reconstruction methods and for parametric bootstrapping, it is highly desirable that simulation methods model as precisely as possible the conditions that shaped biological sequences through evolution. however, widely-used simulation programs cannot be easily tuned to precisely reproduce the peculiar evolution of a particular data set. noticeably, non-homogeneity cannot be simulated by seq-gen  <cit>  or paml  <cit> , even if these phenomena are all known to affect the evolution of many data sets  <cit> .

the ability to estimate parameters of sequence evolution with realistic models, and then computationally evolve sequences using these fitted parameters is crucial to better characterize the behavior of reconstruction methods in realistic settings.

here we introduce extensions to the bio++ package  <cit>  that permit first to estimate parameters of evolution on a specific data set in a maximum likelihood framework, and second to simulate the evolution of sequences using these estimated parameters. importantly, nearly any combination of non-homogeneous  and heterotachous models of evolution can be fitted to data, so that simulations may mimic very precisely the evolution of a data set. such a flexibility should enable one to probe how robust methods of phylogenetic tree or ancestral state reconstruction are to more realistic evolutionary conditions. moreover, it offers the possibility to compare a large variety of models by assessing through parametric bootstrapping their respective ability to reproduce a given characteristic of interest, measured on a real data set.

implementation
molecular phylogenetic methods are used by a wide range of biologists, from bioinformaticians willing to characterize and improve models of sequence evolution to molecular biologists trying to grasp the particular evolutionary history of their gene of interest. these different types of users have different needs: the former may benefit from easy-to-assemble, high-level object-oriented code to conduct phylogenetic analysis, while the latter likes user-friendly interfaces. however, both demand programs able to run the most recent models of evolution. the newly introduced extensions are available in two flavors that might fit different users' needs:  as classes in the bio++ phylogenetic library, including a special class called substitutionmodelset which implements the relationships between models, parameters and branches, and  through the bppml and bppseqgen programs, which can respectively adjust these models to a data set and simulate data from these models. these programs share a common syntax for model specification and are hence fully inter-operational and easy-to-use.

the substitutionmodelset class
the bio++ libraries  <cit>  provide data structures and algorithms dedicated to analysis of nucleotide, codon and amino acid sequences, phylogenetics and molecular evolution, and are designed in an object-oriented way. these include classes for storing phylogenetic trees, computing likelihood under various models of substitution, and estimating parameters. the likelihood classes take as input a phylogenetic tree and a substitution model, and were extended to allow the computation under non-homogeneous models . this support is achieved through the addition of parameters for the rooting of the tree, since the likelihood may not be independent of the root position with a non-homogeneous model  <cit> , and through a new class named substitutionmodelset. the substitutionmodelset class essentially associates a substitution model with each branch of the phylogenetic tree, and links each substitution model to a list of corresponding parameters . it also provides a series of methods for the developer to set up the general model, to assign parameters to substitution models and substitution models to branches.

substitution models can be totally independent of each other, or can share any number of parameters. virtually any non-homogeneous model can thus be set up, provided the alignment is not a mix of nucleotide, amino-acid or codon sequences. all models available in bio++ can be used with this class , including heterotachous models  and any rates across sites model . the developer can also use the substitutionmodelset class with his own substitution model through the bio++ substitutionmodel interface. the substitutionmodelset class can be used in conjunction with other bio++ classes to reconstruct ancestral states or to map substitutions, and hence allows to perform these analyses in the general non-homogeneous case.

estimating parameters
estimation of numerical parameters is performed using a modified newton-raphson optimization algorithm, commonly used in phylogenetics  <cit> , and therefore requires computing derivatives with respect to parameters of the model. because the use of the cross derivatives leads to numerical instabilities in the optimization , they are set to zero in the hessian matrix. derivatives regarding branch lengths are computed analytically, whereas derivatives regarding the rates across sites distribution are computed numerically. although the substitution model derivatives can be computed analytically in the homogeneous case as well as in galtier and gouy's model, they are difficult to compute analytically in the more general case, and are consequently computed numerically in bio++. to prevent convergence issues due to erroneous derivative values we use, in the last optimization steps, powell's multi-dimensions algorithm, which does not rely on parameter derivatives  <cit> .

a general file format to describe non-homogeneous models
we introduced a new user-intuitive property file format to describe non-homogeneous substitution models. this format is an extension of paml or nhml property file formats, and uses a syntax of the kind

property_name = property_value

a parser that automatically instantiates the appropriate substitutionmodelset object is included in the bio++ libraries and is used by all programs in the bio++ programs suite. moreover, the same format is used for the input file of the programs and for their output, so that the output of one program  can easily be used as the input of another one . figure  <dig> shows how the models in figure  <dig> are coded using this format. the core part of the description is the "model" property, which is associated to one or several nodes of the phylogenetic tree through node identifiers. these node identifiers can be obtained from the programs in the bio++ program suite, or set by the user in his own program.

the bppml and bppseqgen programs
parameter estimation and simulation procedures are available as dedicated classes in the bio++ phylogenetic library, and can hence be used in any c++ program. however, for users who would rely on appropriate software rather than program their own tools, the bio++ program suite was designed. these programs, including bppml  and bppseqgen  are command line driven and fully parametrized using property files, as introduced above. they can thus easily be pipelined with scripting languages as bash, python or perl. in addition to the bppml and bppseqgen programs, the bio++ program suite also contains programs for distance-based phylogenetic reconstruction, sequence file format conversion and tree manipulation.

RESULTS
our new general non-homogeneous model implementation was applied to boussau and gouy's data set of concatenated small and large subunit ribosomal rna sequences and tree  <cit> . this data set contains  <dig> sequences and  <dig> complete sites. we first compare computation time, memory usage and parameter estimation for various models and software. we then show how the general non-homogeneous model introduced here can be used to study model fit through parametric bootstrapping.

in this section, we use the following model notations:

h homogeneous model, using a tamura  <dig> substitution model  <cit> .

nh <dig> one-theta-per-branch non-homogeneous model  <cit> . this model uses tamura's  <dig> substitution model, with one θ  per branch in the tree, whereas κ  is shared by all branches.

nh <dig> one-theta-per-kingdom non-homogeneous model. in this general model, we allowed each kingdom  to have its own equilibrium g+c content, while sharing the same transitions/transversions ratio.

nh <dig> same as nh <dig>  but in addition the thermophilic bacteria on one hand, and the eukaryote g+c-rich genus giardia on the other hand were allowed to have their own equilibrium g+c content.

nh <dig> one-kappa-per-branch non-homogeneous model. this model has one κ per branch in the tree, whereas θ is shared by all branches.

performance
we compared the likelihood of our implementation with the nhml  <cit>  and phyml  <cit>  programs . several models have been tested: kimura two parameters  for the homogeneous case, and tamura  <dig>  derived models for the non-homogeneous cases, with constant rate, gamma distributed rates , gamma  + invariant and galtier's  <dig> site-specific rate variation model . on all tested models, the optimization algorithm in bio++, while using numerical derivatives, leads to similar or better likelihood values than other programs, although at the price of an increase in computational time. however this increase is not sufficient to prevent the use of complex models on data sets of usual sizes, as it takes a little bit more than an hour and a quarter to optimize parameters with the richest models on a data set containing  <dig> sequences. it is also noteworthy that the bio++ implementation requires less memory than other programs. this is partly explained by differences in the algorithms used to compute the likelihood  <cit> . the phyml programs, including nhphyml, use a double-recursive algorithm  <cit> , which saves a lot of computation when exploring the space of tree topologies but results in a three fold increase in memory usage compared to the simple-recursive algorithm. because no tree space exploration was involved, bppml computations used the simple-recursive algorithm. if desired, however, bio++ also offers the double-recursive algorithm.

time is shown as hours:minutes:seconds. numbers in bold font correspond to the best performance for each comparison. memory corresponds to the maximum memory usage during the program execution in megabytes. h: homogeneous case, with a k <dig> substitution model, nh1: theta per branch model, with a t <dig> substitution model, nh3: clade-specific and g+c-rich species theta model, see methods. the phyml program was used for the h model, and nhphyml for the nh <dig> model.

the convergence of the optimization algorithm was assessed by two methods, using the nh <dig> model. first, we used  <dig> distinct randomly chosen initial sets of parameter values and the rna data set . we found that the estimated values obtained in each run were the same for all parameters up to the 5th decimal. second, we simulated  <dig> data sets using the nh <dig> model with a gamma + invariant rate distribution, with parameter values estimated from the real data set and the same number of sites. these parameters were then re-estimated for each simulated data set using random initial values. the results are displayed on figure  <dig>  and show that the parameter values are recovered without bias and with a good precision. the only exception is the proportion of invariant sites which is slightly overestimated. these results also validate the simulation procedure.

example of application: parametric bootstrap and bowker's test for non-homogeneity
as most phylogenetic reconstruction models are homogeneous, they do not properly model the evolution of homologous sequences that vary widely in their compositions. analyzing compositionally heterogeneous data sets with homogeneous models of sequence evolution may therefore lead to incorrect inferences, provided the heterogeneity is large enough. several tests have been developed to assess the amount of heterogeneity present in a data set .

estimating the amount of compositional heterogeneity in a data set
most commonly, a matrix is assembled that contains compositions in all characters for all sequences, and this matrix is analyzed through χ <dig> statistics  <cit> . however, this approach usually does not distinguish between constant and variable sites, and therefore may underestimate the true amount of heterogeneity in a data set  <cit> .

recently, ababneh et al.  <cit>  re-introduced bowker's pairwise test  <cit>  for symmetry. given two aligned sequences s <dig> and s <dig> on a given alphabet of size n and characters x{ <dig> ...n}, it compares the numbers of substitutions between xi in s <dig> and xj in s <dig>  {i,j} ∈ , with the numbers of substitutions between xj in s <dig> and xi in s <dig>  if these pairs of numbers are equal for all {i, j} ∈ , the two sequences may have evolved according to two identical processes. otherwise, the two processes were necessarily different.

bowker's test therefore permits to assess whether compositional differences have accumulated between two sequences through non-homogeneous evolution. to apply it to more than two sequences, rodriguez-ezpeleta et al.  <cit>  computed all pairwise bowker's tests in their alignment and computed the median value; one could also have counted the number of bowker's tests that are significant at a 5% threshold according to a χ <dig> table.

however, none of these tests permit to estimate if the amount of heterogeneity that they detect in a given data set is sufficient to bias inferences made using homogeneous models, although this is likely the question an average user would like to answer.

assessment of the fit of evolutionary models with respect to compositional heterogeneity
here, we describe a method to reveal the ability of evolutionary models to account for the compositional heterogeneity in a sequence alignment, which we measure using the median of all bowker's pairwise statistics, or the number of significant bowker's pairwise tests . this method is tree-based, and uses parametric bootstrapping  <cit> . in this respect, it is similar to the method recently introduced in  <cit>  in the bayesian setting. our approach requires  <dig> steps to estimate the fit of a model m to a data set d.

 <dig>  compute the compositional heterogeneity measure h for the data set d.

 <dig>  estimate the parameters of model m based on the data set d according to the maximum likelihood criterion.

 <dig>  simulate a large number of data sets d' using the model m previously estimated.

 <dig>  compute the compositional heterogeneity measure h' for each alignment d'.

 <dig>  compare the measure h obtained on data set d to measures h' obtained on data sets d'. if h is outside 95% of the distribution of h', the model does not properly reproduce the heterogeneity of data set d.

using such an approach, any model can be compared with others with respect to their ability to handle the compositional heterogeneity of a given data set: the closest the distribution of h' is from h, the highest is the fit. ideally, the distribution of measures h' obtained on the parametric bootstrap replicates of a good model should be centered around the value obtained for the real alignment h, with a very low variance. if one neglects potential problems linked with over-parametrization, the inferences of the best model should be preferentially trusted compared to a model that fails to account for an important feature of a data set. overall, our approach can be used for model selection, although contrary to criteria such as aic or bic  <cit>  this approach does not take into account the number of parameters; more importantly, it can also be used for estimating model adequacy.

application to an rrna data set
our approach to assess the composition-wise fit of evolutionary models to a data set was applied to an alignment containing ribosomal rna sequences from archaea, bacteria and eukaryotes  <cit> . first, several homogeneous and non-homogeneous models were fitted to the data set, using a tamura  <dig> model of substitution with a four classes gamma + invariant distribution of rates across sites. then,  <dig>  artificial data sets were simulated in each case using these estimated parameters. eventually, the real data set and the simulated data sets were compared with respect to their compositional heterogeneity: models able to simulate data sets with similar amounts of heterogeneity as the real data set appropriately account for this specific aspect of the data.

results are shown in figure  <dig> and table  <dig>  both the number of significant bowker's tests and the median of their values give similar results. for instance, both indices find that the real data set shows significantly more heterogeneity than the distributions of data sets simulated under the homogeneous model of sequence evolution . the homogeneous model therefore lacks parameters useful to account for this particular feature of the data. allowing different transition/transversion rates for each branch as in model nh <dig> does not solve this problem, as the obtained bootstrapped distribution also significantly underestimates the heterogeneity in the real data . it is noteworthy, however, that the likelihood ratio test finds that this model describes the data significantly better than the homogeneous one, whereas the aic and bic criteria do not. on the contrary, the nh <dig> model simulated sequences distribution surrounds the value obtained on the real data set . this suggests that galtier and gouy's modeling  <cit>  properly accounts for the heterogeneity in rrna data sets, and that there may be no point in using more parameter-rich models such as yang and roberts'  <cit>  on these molecules. the results even suggest that nh <dig> might be slightly prone to over-estimating the amount of heterogeneity. for instance, the median bowker's test value for simulated data sets are most often higher than the value obtained on the real data set. nh1's behavior may be explained by over-parametrization: it is likely that during sequence evolution, not all branches witnessed significant shifts in mutational parameters or selection pressures. to investigate further the impact of the number of parameters on model fit, two other models were tested: nh <dig>  in which different equilibrium g+c contents are associated to each kingdom, and nh <dig>  which further adds two equilibrium g+c contents, one for the hyperthermophilic  bacteria, and one for the g+c rich eukaryote giardia. hyperthermophilic  archaea were not considered separately from the others as nearly all archaea in our data set were thermophilic or hyperthermophilic. the nh <dig> model seems to lack useful parameters to properly account for the heterogeneity in the real data set, as its simulated data sets are less heterogeneous than the real one . the nh <dig> model improves upon nh <dig> as its bootstrapped distribution is more centered upon the observed value, which is no longer rejected . however, the observed value is still on the right side of the null-distribution, and it is very likely that the correct parametrization lays between nh <dig>  too rich with its  <dig> equilibrium g+c contents, and nh <dig>  maybe too poor with its  <dig> equilibrium g+c contents. however, as nh <dig> provides a fit nearly as good as nh <dig> with a much lower amount of parameters, the best model may well have less than a dozen equilibrium g+c contents. interestingly, bowker's tests are in agreement with the bayesian information criterion  and favor the nh <dig> model. conversely, akaike's information criterion  and the likelihood ratio test  favor the more parameter-rich model nh <dig>  obviously, although a few works already addressed this issue in the bayesian framework  <cit> , automatic ways to explore and choose among heterogeneous models in a maximum likelihood framework are much needed. all the tools required for such a project are now available in the bio++ libraries.

comparison of the various non-homogeneous models with the homogeneous case, using different criteria. k is the number of parameters and lnl is the log likelihood of each model. the akaike's information criterion  of each model is defined as 2k - 2·lnl, and the lowest value, corresponding to the best model according to this criterion is in bold font. the bayesian information criterion  is computed as k· ln - 2·lnl, n =  <dig> being the number of observations. the lowest value is in bold font. the likelihood ratio test  allows to compare nested models only, and is defined as minus two times the logarithm of the ratio of likelihoods. all lrt are significant at the  <dig> % level. this ratio follows a χ <dig> distribution with the number of additional parameters as the degrees of freedom. the last two columns show the p-values of the two bowker's test introduced in this paper.

CONCLUSIONS
bio++ is a growing set of libraries designed for sequence, phylogenetic and molecular evolution analyzes. in this article extensions allowing to implement a wide variety of non-homogeneous models of sequence evolution were introduced. combined with support for rates across sites and heterotachous models of evolution, and with routines for optimizing parameters and tree topology in the maximum likelihood framework, they provide a comprehensive platform for phylogenetic studies, either for bioinformaticians willing to develop their own software, or for biologists characterizing the evolution of a particular set of sequences using the bppml and bppseggen programs. whilst being a generalist program implementing a large variety of models, bppml was shown to be of a similar quality as programs dedicated to particular homogeneous or non-homogeneous models of evolution, achieving higher likelihood scores with smaller memory requirements while conserving reasonable running-times. its joint use with bppseqgen permits to precisely study the evolution of a particular data set through parametric bootstrapping, and may be used to generate realistic artificial data sets to study the robustness of phylogenetic reconstruction methods in the presence of heterogeneity and heterotachy. further developments may involve methods to optimize the number of models necessary to account for the heterogeneity in a data set, or methods to explore the space of tree topologies with a broad range of non-homogeneous models of sequence evolution.

