BACKGROUND
increasing necessity for sustainable manufacturing processes is driving a trend to replace the traditional methods of chemical synthesis by biotechnological approaches, in order to produce a number of valuable products, such as pharmaceuticals, fuels and food ingredients. this, however, implies that the microorganisms' metabolism usually needs to be modified to comply with industrial purposes, rather then to follow their natural aims like, for example, the maximization of biomass growth.

in the last few years, within the field of metabolic engineering, a number of tools have been developed in order to introduce genetic modifications capable of achieving the production of the desired products  <cit> . however, these have still been based mostly on qualitative or intuitive design principles and scarcely on effective mathematical models that can accurately predict cellular behaviour.

a number of attempts have been made to model the whole cell behaviour  <cit> , but these models are still incomplete due to the lack of kinetic and regulatory information.

nevertheless, it is possible to predict cellular metabolism, under some assumptions, namely considering steady-state conditions and imposing a number of constraints over the rates of reactions.

this is the way followed by the flux balance analysis  approach  <cit> , where the flux over a particular reaction is typically optimised using linear programming, resulting in a value for the fluxes of all reactions in the cell. the most usual approach, under this framework, is to define a reaction for biomass production and to consider this as the objective function, thus assuming that the microbes have evolved towards optimal growth  <cit> .

using this technique, it is possible to predict the behaviour of a microorganism, both in its wild type and mutant forms, under a number of environmental conditions. a bi-level optimization problem can then be formulated, by adding a layer that searches for the best mutant that can be obtained by simply deleting a few genes from the wild type. the idea is to force the microorganisms to produce the desired product by selected gene deletions. therefore, the underlying optimization problem consists in reaching an optimal subset of gene deletions to maximize an objective function related with the production of a given compound.

a first approach to tackle this problem was the optknock algorithm  <cit> , where mixed integer linear programming  is used to reach an optimum solution. an alternative approach was proposed by the optgene algorithm  <cit>  that considers the application of evolutionary algorithms  in this context. since eas are a meta-heuristic optimization method, they are capable of providing solutions in a reasonable amount of time, although this solution might not be the optimal one. still its application in the context of the yeast s. cerevisiae allowed the optimization of an industrially important non-linear objective function related with productivity in several processes such as the production of succinic acid or vanillin.

optgene proposed eas with two alternative solution representation schemes: binary or integer. the binary representation is closer to the natural evolution of microbial genomes, but is more complex and leads to solutions with a larger number of knockouts. the integer representation allowed for a more compact genome in the ea, encoding only for the gene deletions. however, one of the major limitations of this representation in optgene is the need to define a priori the number of gene knockouts, a parameter that remains fixed throughout the ea's evolution.

in this work, the authors propose a set-based representation that considers a variant with variable-sized solutions . this allows for the consideration of solutions with a different number of knockouts  during the optimization process, avoiding the trial and error approach for determining the optimum number of knockouts in a particular problem.

under this new framework, two optimization algorithms were developed: simulated annealing  and set-based evolutionary algorithms . both search for the optimum set size in parallel with the search for the optimum set of gene deletions.

although the proof of principle of the applicability of meta-heuristics to the problem of microbial strain design has already been achieved  <cit> , a thorough validation based on the collection of sufficient data to perform statistical analysis was needed.

therefore, in this paper, we present the results obtained by the application of the two novel methodologies to four case studies where s. cerevisiae and e. coli are the target microorganisms. in these cases, the objective function is related to the production of succinic and lactic acid, respectively. in the in silico experiments, the proposed sa and seas, and also variants with fixed size solutions were compared.

for each experiment, the algorithms were run  <dig> times allowing a sufficient number of function evaluations in each run, and the results obtained allowed not only to perform statistical analysis and a valid comparison between the approaches, but also to obtain a close to optimum family of solutions that were analyzed resembling their biological significance.

RESULTS
solution representation and evaluation
the optimization problem addressed in this work consists in selecting, from a set of genes in a microbe's genome, a subset to be deleted in order to maximize a given objective function, related to the microorganism's metabolism. the first issue to address, when developing an algorithm to tackle this task, is the encoding of the solutions.

in this work, a novel set-based representation is proposed, where only gene deletions are represented in the solution. each solution consists of a set of integer values representing the genes that will be knocked out. therefore, if the set contains the value i, this means that the gene corresponding to the i-th reaction in the microbe's metabolic model will be deleted. each element of the set is, therefore, an integer with a value between  <dig> and the total number of reactions, n, and no repeated elements are allowed. two variants of this representation can be defined, considering either fixed or variable sized sets.

the solutions are evaluated by taking all values in the set, and forcing the fluxes of the reactions encoded by those genes to the value  <dig>  thus adding new constraints to the metabolic model. the process proceeds with the simulation of the mutant. in this work, this is achieved using fba  but other methods can be considered at this stage . the output of this step is the set of values for the fluxes over all the reactions, some of which are then used to compute the fitness value, given by an appropriate objective or fitness function.

in this work, the adopted fitness function is the biomass-product coupled yield   <cit> , given by:

 bpcy= pgs 

where p stands for the flux representing the excreted product; g for the organism's growth rate  and s for the substrate intake flux.

besides optimising for the production of the desired product, this objective function also allows to select for mutants that exhibit high growth rates, i.e., that are likely to exhibit a high productivity, an important industrial aim. the overall process of decoding and evaluating a solution is depicted in figure  <dig> 

evolutionary algorithms
evolutionary algorithms   <cit>  are a popular family of optimization methods, inspired in the biological evolution through natural selection. these methods work by evolving a population, i.e. a set of individuals that encode solutions to a target problem in an artificial chromosome. each individual is evaluated through a fitness function that assigns it a numerical value, corresponding to the quality of the encoded solution. new individuals  are created by the application of reproduction operators to selected parents and, since the pool of parents is taken from the previous population using probabilities, eas are stochastic in nature.

the proposed set-based ea  uses the set-based representation and defines two reproduction operators: a crossover and a mutation. the crossover operator is inspired on traditional uniform crossover operators  <cit>  and works as follows: the genes that are present in both parent sets are kept in both offspring; the genes that are present in only one of the parents are sent to one of the offspring, selected randomly with equal probabilities. the mutation operator is a random mutation that replaces an element of the set by another, randomly generated in the allowed range .

in seas, a minimum and a maximum value for the set size are defined. if these values are equal, the search only goes through sets of a given cardinality. the operators comply with this constraint by creating solutions always of the same size. in the case of the crossover, this implies that, when selecting the destination of the genes that are present in only one parent, if an offspring reaches the maximum number of elements in the set, the remaining genes go to the other offspring.

if the maximum and minimum values of the set sizes are different, variable-sized sets can be encoded and compete within the same population. in this case, two additional mutation operators are defined in order to create solutions with a distinct size:

• grow: consists in the introduction of a number of new elements into the set, whose values are randomly generated in the available range, avoiding duplicates.

• shrink: a number of randomly selected elements are removed from the set.

in both cases the limits on the set size are strictly obeyed. the grow and shrink mutation operators are each used with a probability of 5% each, meaning that 10% of the new individuals are created in this way. the remaining ones are bred by the aforementioned crossover and mutation operators with equal probabilities. in the experiments reported in this work, when a variable size sea is used, the minimum size is set to  <dig> and the maximum size is set to n, thus not restricting the possible range of solutions.

sea uses a selection procedure that consists in converting the fitness value into a linear ranking of the individuals in the population, and then applying a roulette wheel  <cit>  scheme. in each generation, 50% of the individuals are kept from the previous generation, and 50% are bred by the application of the reproduction operators. an elitism value of  <dig> is used, allowing the best individual of the population to be always kept.

an initial population is randomly created and the termination criterion is based on a fixed number of generations . in the variable size seas, the size of the sets encoded in the initial individuals is randomly set to a value between  <dig> and  <dig> 

simulated annealing
simulated annealing   <cit>  is an optimization algorithm where a single solution evolves by successive small changes  to achieve an approximation to the global optimum. better solutions are always accepted and local optima are avoided by the fact that sa allows worse solutions to replace the current one with a certain probability that decreases over time. this probability is controlled by the value of a parameter, denoted as temperature given the fact that sa is loosely inspired by the annealing process used in many different areas  where the system initially at a high temperature, is slowly cooled so that the system at any time is approximately in thermodynamic equilibrium.

in optimization, the current state is a solution to the problem and its fitness value states for the system's energy. the current solution is represented using similar encoding schemes as the ones used in the eas. this allows sa to be applied for instance to problems with binary, integer or real variables. in this work, we developed an sa with the set-based representation previously explained in the context of the sea.

at each step, the algorithm works by creating a new solution from the current one, using mutation operators. the sa variant developed in this work allows the use of a combination of mutation operators, similar to the ones described for the eas, each with a given probability. when a new solution is created and evaluated, the difference between the previous and the new fitness values is computed . a better solution is always accepted, while a worse one is only accepted with a probability given by the boltzmann factor:

 p=e−Δet 

the temperature t is initialized to t <dig> and it is decreased according to a given cooling schedule that represents how this value decreases along the algorithm. the entire process is repeated until the temperature is sufficiently low. for each temperature value, a number of iterations are performed, sufficient to give a good sampling statistics.

the main configuration parameters of the sa are the initial and final temperatures, the number of iterations performed at each temperature and the cooling schedule used. the choice of these parameters is of paramount importance to the performance of the algorithm. if the initial temperature is too low or the cooling schedule is not slow enough, the optimization process may become stuck in a local optimum. on the other hand, if the initial temperature is too high, the cooling is too slow or the number of iterations per temperature is too high, the algorithm wastes a potentially large amount of computational time searching for solutions.

the cooling schedule used in this work is among the most popular ones, where the temperature decreases exponentially, according to the following equation:

 tn+ <dig> = αtn 

where α is a scale parameter .

to ensure that the cooling schedule is sufficiently slow, the parameter α should be given values close to the unity. the choice of initial  and final temperatures  is problem dependent and its definition poses serious problems. indeed, it is easier to think in terms of the objective function values  than in terms of values for the temperature. thus, the following auxiliary parameters were defined:

• Δe <dig> – the difference in energy that corresponds to an acceptance probability of worse solutions of 50%, at the beginning of the run;

• Δef – the difference in energy that corresponds to an acceptance probability of worse solutions of 50%, at the end of the run;

• trials – the number of iterations per distinct temperature value;

• nfes – the number of function evaluations.

using these parameters, the initial temperature, the final temperature and the scale parameter were computed using the following equations:

 t0=−Δe0log⁡ <dig> tf=−Δeflog⁡ <dig> α=exp⁡) 

the advantage of using Δe <dig> and Δef is that it allows the user who approximately knows the fitness landscape of the problem to automatically define the temperatures by reasoning over the values of the objective function. furthermore, by supplying the number of function evaluations instead of the scale parameter α enables the comparison with other optimization approaches.

as in the ea, two variants of this representation can be defined, considering fixed or variable sized sets. in the fixed-size alternative, the previously defined random mutation operator is used. in variable-sized representations, the two additional mutation operators  are also used, each with a probability of 25%, meaning that half of the new individuals are created in this way.

in brief, the sa algorithm searches for the optimal set of gene deletions by exploring the whole search space  in a stochastic manner, where the probability of accepting a non-optimal search direction is high in the beginning and very low or zero in the end. the probability of accepting a non-optimal direction allows the algorithm to avoid the local optimal solutions. thus, the algorithm can find combinations of gene deletions which individually may not necessarily lead to the improved production.

an overview of the major steps in the sea and sa algorithms is provided in figure  <dig> 

greedy algorithm
a greedy algorithm was devised to provide a baseline comparison with the more elaborate optimization approaches. this algorithm tries to explore the search space efficiently, by combining local search  and exhaustive search. the exhaustive search starts with the wild type and proceeds with an increasing number of knockouts. when a solution that improves over the wild type one is found, local search is used to recursively improve it by adding knockouts. when no further improvements can be obtained with this local search procedure, exhaustive search is resumed. the details on the implementation of this algorithm are given in the methods section.

pre-processing and post-processing
typically, in microbial genome-scale models, the number of variables  is quite high  and therefore the search space is very hard to address by the optimization algorithms. thus, every operation that gives a contribution to reduce this number, without compromising the quality of the solutions, greatly improves the convergence of the methods used.

in this work, a number of operations were implemented in order to reduce the search space, being described in the methods section.

case studies
four case studies were used to test the aforementioned algorithms. the first considers s. cerevisiae and the aim is to produce succinic acid, while the remaining considers e. coli and the production of lactic acid  and succinic acid. all use glucose as the main substrate.

succinic acid is a chemical used as feedstock for the synthesis of a wide range of other chemicals with several industrial applications. succinic acid and its derivatives have been used as common chemicals to synthesize polymers, as additives and flavouring agents in foods, supplements for pharmaceuticals, or surfactants. currently, it is mostly produced through petrochemical processes that can be expensive and have significant environmental impacts. succinic acid, therefore, represents an important case study for identifying metabolic engineering strategies  <cit> . in fact, the knockout solutions that lead to an improved phenotype regarding the production of succinic acid are not straightforward to identify since they involve a considerable number of interacting reactions.

lactic acid and its derivatives have been used in a wide range of food-processing and industrial applications like meat preservation, cosmetics, oral and health care products and baked goods. additionally, as lactate can be easily converted to readily biodegradable polyesters, it is emerging as a potential material for producing environmentally friendly plastics from sugars  <cit> .

several microorganisms have been used to commercially produce lactic acid  <cit> , such as lactobacillus strains. however, those bacteria also have undesirable traits, such as a requirement for amino acids and vitamins which complicates acid recovery. e. coli has many advantageous characteristics as a production host, such as rapid growth under aerobic and anaerobic conditions and simple nutritional requirements. moreover, well-established protocols for genetic manipulation and a large knowledge on this microbe's physiology enable the development of e. coli as a host for production of optically pure d- or l-lactate by metabolic engineering  <cit> .

although reported work have been focused on the anaerobic production of lactic acid, it is in principle possible to develop aerobic processes, since the carbon overflow in e. coli towards acetic acid in aerobic conditions can be diverted to the production of lactate. the main advantage of such a process is that, since e. coli reproduces much faster in aerobic conditions, it should be possible to improve the productivities when compared with anaerobic processes.

the genome-scale stoichiometric models used for s. cerevisiae and e. coli were developed by  <cit> . the details of each model are given in table  <dig> 

the details on the original models are given, together with the statistics of the models after the pre-processing steps.

experiments
a systematic set of experiments was conducted to evaluate the performance of the proposed sea and sa algorithms. these were applied to the four case studies, using both their fixed and variable size variants. in the fixed-size case, several alternatives for the cardinality of the set  were considered, being used the following values for k:  <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig>  the experimental setup is given in the methods section.

the full set of results can be found in the additional files  <dig> to  <dig>  including a set of statistics calculated over the  <dig> runs performed in each scenario, the set of knockouts in the overall best solutions and also the frequencies of occurrence of each gene deletion in the results of each optimization scenario. the analysis of these results will be conducted in the next section.

discussion
reaching the optimum solution size
in figure  <dig>  a summary of the main statistics regarding the objective function  obtained for both algorithms  is plotted. the plots make clear the improvement of the results when the value of k increases , until an optimum level is reached.

the variable size alternatives are normally able to find results of the same quality of the best values of k, therefore being able to automatically discover good values for this parameter. therefore, the use of the variable size alternative allows the user to save a considerable amount of time in computation.

also, one important feature in this problem is the ability to find good solutions with the minimum number of gene deletions, since this will make easier its implementation in the lab. in table  <dig>  the average size of the best solutions found by the variable size variants of the sea/sa are shown. it is important to note that the size of the solutions for each algorithm is computed from the best solutions found in each run that undergo a simplification process .

this table shows the average number of knockouts in the solutions obtained by the variable size variants of sea and sa. only the best solutions found over the  <dig> runs are considered.

from the values on this table and the results shown in figure  <dig>  it is clear that the variable size variants do not return solutions with very large sets of knockouts, when compared to the fixed size approaches. indeed, it seems that these solutions only "grow" during the evolution in sea/sa if the new knockouts provide fitter solutions.

comparison between the performance of the algorithms
to provide a baseline result for the comparison between the algorithms, the greedy algorithm described before was applied to the four case studies. the termination criterion was to perform  <dig> million function evaluations . the results are given in table  <dig> 

this table shows the results obtained by the greedy algorithm. the first column shows the details on the case study ; the second column states the value of the objective function  and the last column the number of knockouts, both obtained for the best solution found.

the results confirm that the optimization problems are quite difficult to solve, since in most case studies the greedy algorithm cannot find good results. the exception is the e. coli, lactate, anaerobic case study that seems an easier task. the difficulty of the case studies is also visible in the small number of runs where the best solutions are found, both by the sea and the sa . this fact leads to an important conclusion regarding the use of these stochastic methods: it is normally necessary to run sea or sa multiple times to guarantee that a good solution is achieved.

regarding the comparison of the sea/sa with the greedy algorithm, both the sea and sa perform a quite efficient exploration of the search space, since although conducting only 1% of the number of solution evaluations, they are able to obtain much better results.

comparing the performance of the sea and sa algorithms, they seem to be at a very similar level, in most cases with overlapping confidence intervals. when comparing the variable size variants of both algorithms, the sa seems to be more reliable, showing good results in all case studies and smaller variability across the  <dig> runs.

two additional features that are important when comparing meta-heuristic optimization algorithms are the computational effort required and the convergence of the algorithm to a good solution. the computational burden of the alternatives compared  is approximately the same, since the major computational effort is devoted to fitness evaluation and the same number of solutions is evaluated in each case. a typical run of each algorithm for the case studies presented will take approximately one hour in a regular pc.

regarding the convergence of the algorithms, a plot of the evolution of the objective function along the generations of the sa and sea is given in figure  <dig>  the case study regarding e. coli production of succinic acid is taken as an illustrative example and the variable size variants were selected. it is clear from this plot that the sa converges faster than the sea, obtaining high quality results earlier in the runs. this is the case also in the remaining case studies, although the results are not shown. thus, sa allows a reduction in the computation time needed to achieve a useful solution.

an additional analysis was performed in order to better understand the reasons why the sa seems to perform better. the first was a study of one of the best solutions found for the e. coli, succinate case study and its partial solutions . the aim was to understand how the algorithms build the final solution from smaller ones, along evolution, using mutation and/or crossover operators. the result of this analysis is displayed in figure  <dig> 

a look into the figure shows that in most cases the solutions with more knockouts can be obtained by adding one or two knockouts to smaller solutions. this leads to the insight that the crossover operator is probably redundant in this process. to test this hypothesis, an ea only with mutation operators was tested . the results for this algorithm are given in additional file  <dig>  and its analysis shows that they are quite similar to the ones obtained by the sea with crossover. thus, the crossover operator does not seem to be profitable in terms of the ea's performance, at least in these case studies. this is probably due to the lack of building blocks  that can be combined to achieve better solutions. to study if there are case studies where this could happen remains as interesting further work.

it should be noticed that, although eas have performed worse, their application in this field should still be considered as, when compared with sa, they allow an easier parallelization of the computation given their population based nature. this represents an important advantage when algorithms with heavy computational demands are compared.

analysis of the solutions
a thorough analysis of the solutions obtained is out of the scope of this paper, but it should be pointed out that some of the knockouts obtained are unlikely to have a biological meaning . the very large number of knockouts required for the best solutions for succinate production in both yeast and e. coli are un-realistic to realize in reality .

this table shows the list of knockouts in the best solution found in all the runs of both algorithms. in the rows marked with *, other solutions with the same bpct exist; the one with less knockouts was selected 

furthermore, some of the knockouts obtained are related with fluxes associated with transport reactions that are not necessarily enzyme catalyzed or belong for example to nucleotide pathways. although these reactions are not included in the set of essential genes validated experimentally  <cit> , it is possible that the removal of the corresponding genes will originate non-viable mutants or that the mutation will not contribute to increase the production of the target compound. for most of the cases, however, even in silico, such mutations contribute very slightly to the objective function value. these results emphasise the need of analysing the solutions in the context of additional biological information before lab implementation. nevertheless, analysis of the connection between these mutations and the objective function may help to gain insight into the bottlenecks for the production of the desired compounds.

interestingly, if the solutions obtained are compared with real implementation of knockout mutants, as is the case of succinate production with e. coli, for which there are several works published, for example  <cit> , where a penta-mutant is described, the best solutions obtained by the algorithms are quite different from the ones implemented, although in some cases the reactions to be eliminated belong to the same pathways. comparing our results with the work described in  <cit> , the penta-mutant described there was not reached by our algorithms, a situation that is easily explained by the fact that one of the genes deleted  is de facto an essential gene in the stoichiometric model of e. coli used, since its deletion leads to zero growth. furthermore, the simulation of the tetra-mutant obtained  only produces succinate in silico if the oxygen uptake is constrained, which is consistent with what is described in  <cit>  regarding pyruvate accumulation for this mutant. in fact, the strategy used in  <cit>  is partly based on diverting the overflow in glycolysis from acetate to succinate. since the stoichiometric model does not accurately predict the overflow phenomenon in e. coli, it is difficult to obtain such strategies using our algorithms. this analysis emphasises the importance of using reliable stoichiometric models with these algorithms. although for e. coli the model used has been validated for many different situations, there are clearly still some discrepancies between simulation results and reality, namely concerning essential genes. it would be interesting to know if the mutants obtained in silico based on our approaches perform better or worse than the ones based on empirical knowledge of the metabolic pathways. some of the obtained knockouts point to changes in the citrate cycle, either by the deletion of succinate dehydrogenase or fumarase, which is also consistent with some of the approaches used in  <cit> . in the case of yeast, the suggested strategies also include deletion of succinate dehydrogenase, which is the main succinate consuming reaction under aerobic conditions. since yeast can grow without flux through this reaction, the coupling of the objective function  to the growth is achieved by suggesting the deletion of other pathways. for example, deletion of thr <dig> necessitates production of threonine  via the glyoxylate cycle where succinic acid is produced.

looking again at additional file  <dig>  the fact that among the  <dig> runs multiple solutions are found with close to optimum objective function values is an interesting feature, meaning that, especially for "difficult" case studies, there are many combinations of knockouts that give good solutions. more generally, it has been experimentally shown that many different and non-intuitive combinations of genetic modifications can lead to product enhancement . many of the effects of genetic changes on the desired objective function are due to kinetic and regulatory effects apart from the stoichiometry which is the area of focus in this study.

interestingly, our results point out that even stoichiometric model leads to several distinct solutions. this variety is due to the large possible operating space of the cellular metabolic pathways  <cit> . the number of alternative solutions will thus be a function of the number of elementary flux modes that span the desired range of design and biological objective functions. since, in general, it is difficult to account for the kinetic and regulatory information in the genome-scale models, the variety of the solutions generated from our approach can serve as a compendium of hypotheses that can be subsequently manually screened based on the available regulatory and kinetic data about the systems under investigation. regulatory and kinetic constraints are difficult to explicitly incorporate into the genome-scale models due to their non-linear nature and lack of reliable estimates of in vivo kinetic parameters and metabolite concentrations. the productivity and flux data available in literature for certain mutants can nevertheless be used to impose additional constraints on the models. the ability of the here proposed algorithms to rapidly and effectively search large solutions space provide us new opportunities to handle more complex problems where some of the available regulatory information can be incorporated into the fitness function.

we also here note that the strategies generated by our algorithm inherently exploit the robustness of the network in face of multiple knock-outs  <cit> . the alternative strategies  can be classified into two broad categories: i) strategies where two sets of deletion targets that are interchangeable due to their close  biochemical relation, and, ii) biochemically different strategies leading to similar objective function values.

one clear example from the first category is sdh3_ <dig> and sdh3_ <dig> targets found for the improvement of succinate productivity in yeast. these two are biochemically coupled reactions. in fact, both of these reactions are catalyzed by the same sdh <dig> protein complex. similar example is two reactions in the respiratory chain, frd <dig>  and nadh <dig>  which were found as part of two different deletion strategies for the improvement of lactate in e. coli under anaerobic conditions. for the same objective function under aerobic conditions, acetate kinase  and phosphotransacetylase  distinguishes two solutions obtained for  <dig> deletion search. the product of both of these reactions is acetyl phosphate, albeit obtained via different substrates.

one example of the second category  is illustrated in two proposed gene deletion sets:  and  identified in the succinic acid case study for e. coli. apart from sucd <dig>  the rest of the genes span different parts of metabolism: glycine and serine metabolism, oxidative phosphorylation and pentose phosphate cycle in the first set; and threonine and lysine metabolism and glycolysis in the second set.

CONCLUSIONS
the development of efficient and accurate modelling and optimization methods in metabolic engineering has a considerable impact in biotechnology, leading to substantial economical gains in areas such as the production of pharmaceuticals, fuels and food ingredients.

in this work, a contribution to this arena was provided by the development of evolutionary algorithms and simulated annealing that are able of reaching near optimal sets of gene deletions in a microbial strain, in order to maximize the production of a given product. an important novel feature of this work was the introduction of set-based representations that made use of variable size sets of gene deletions. this allows the automatic definition of the optimum number of gene deletions, in parallel with the search for the best knockouts.

a systematic statistical validation of the algorithms was conducted, where those were tested, in several variants, in four case studies that dealt with the production of succinic and lactic acid by the bacterium e. coli and the yeast s. cerevisiae.

a number of features can be introduced and/or improved in this work. these include other algorithms for simulation and distinct objective functions. regarding the former, an alternative algorithm for simulating mutants' phenotype is the moma algorithm that was proposed by  <cit> , where it is assumed that knockout metabolic fluxes undergo a minimal redistribution with respect to the flux configuration of the wild type. it would also be very interesting to consider an objective function capable of taking into account the number of knockouts of a given solution and the cost of its experimental implementation.

one other area of future work is the development of multi-objective optimization algorithms that are able to provide in a single run, not only a single solution but rather a whole set of distinct trade-offs between the two goals: maximizing biomass and maximizing the desired product.

