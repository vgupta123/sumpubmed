BACKGROUND
novel genomic research methods have enabled researchers to perform high-throughput experiments that results in massive amounts of experimental data. combining data from different experiments allows researchers to validate their results and to gain a better understanding of the biological questions being studied. in cross-species studies, data derived from experiments performed on different organisms are combined to find universal themes. in cross-platform studies, common biological questions are studied using different research platforms and technologies. the ability to combine experimental data is particularly useful when extended to combine data available on public data repositories such as sequence, expression, and literature databases.

the desire to perform large-scale studies that combine experimental results from various sources has led to compendium studies that combine cross-species/cross-platform data in order to obtain a larger perspective on biological questions. unfortunately, for several reasons, the combining of experimental results is anything but a trivial task. these reasons can be divided into biological and technical challenges. the biological challenges include variances between species, different experimental design/conditions, and lack of knowledge of the underlying biological processes. the technical challenges are caused by differences in how experimental data is stored, presented, and managed. because of the lack of standards, different research equipment, software, and databases identify and structure data in different and unique ways that make it a challenge to combine data obtained from these heterogeneous sources.

to address these technical challenges and to enable researchers to automate the integration of the genomic data derived from heterogeneous sources, without the need for using complex programming and scripting tools, we have developed a user-friendly web-based software resource called cropper. cropper can be used to combine datasets from different genomic research platforms such as microarrays, biological databases, and experiments performed on different species. when performing the combining process, associated data can be brought along. this facilitates the import of the resulting dataset by the user into the desired statistical/analytical program for further analysis. cropper uses the latest genomic information with respect to identifiers and orthologous genes retrieved from the ensembl  <cit>  database.

implementation
cropper is developed using perl version  <dig> . <dig>  bioperl version  <dig>   <cit>  and ensembl database api written in perl  <cit> . cropper runs on a web-server which also acts as an application server. users can use the web-interface to input their datasets and related parameters to the application server. the application server processes the data, and if required, queries the database server containing installation of the ensembl-database for information about data identifiers, orthologous genes, and gene annotations. information about how the identifiers are linked and how the orthologue predictions have been performed can be found from the ensembl-website.

cropper can be used to combine genomic datasets obtained from various heterogeneous sources. as an input, cropper takes datasets as delimited text-files. the delimited text-files can have any kind of column structure, but should include a column with an identifier for each data row. all the external database identifiers found from the ensembl database can be used, including identifiers for major biological databases  and technology providers .

the user can select the structure of a result file and choose a metagene identifier to be created for each data row. a metagene identifier is a common identifier automatically created by cropper that groups together different identifiers originating from a single gene . for example a gene and a protein coded by an orthologous gene in another species will have a common metagene identifier. the concept of the metagene identifiers is shown in detail in figure  <dig> 

RESULTS
cropper can be used to combine genomic data obtained from heterogeneous sources. because cropper uses the ensembl-database for information about data identifiers and orthologous genes, the number of different possible sources of data is enormous. these sources include major technology providers and databases, and therefore cropper can be used to perform cross-platform studies using data from these sources. the current ensembl-build  contains genomic information from  <dig> different species  including major model organisms. this allows cropper to be used for cross-species studies.

using cropper is a straightforward process which is divided into two parts; processing of individual dataset files and combining the processed files. when processing individual files, users can choose to annotate and re-structure their dataset, and additionally add a metagene identifier for each data row. after processing the individual files and adding metagene identifiers, the processed datasets can then be directly used for analysis in suitable 3rd party analysis software, or alternatively combined with cropper using the common metagene identifiers. the combining process produces a result dataset that the users can then import to the analysis software of their choice. the flow of processing and combining datasets using cropper is presented in figure  <dig> 

performing an example compendium study
to demonstrate how cropper can be used in cross-species/cross-platform compendium studies, we performed a small-scale compendium study. the datasets used were; mouse full powerblot western array dataset used for proteomic analysis after rasagiline treatment , affymetrix genechip human genome focus array dataset used for gene expression profiling of parkinsonian substantia nigra pars compacta  <cit> , affymetrix genechip c. elegans genome array dataset used for identification of gene expression changes in transgenic c. elegans overexpressing human mutant a53t α-synuclein  <cit>  and affymetrix genechip human genome 133a set used for gene expression profiling of mptp-lesioned macaque model of parkinson's disease  <cit> . the reasons for selecting these datasets were the common focus of the studies , the wide variety of covered species  and differences in used platforms . the question we wanted to study was: are there common themes between the human disease state and animal disease models? moreover, what are the themes that can be found from genes with altered expression in animal models, but not in the humans?

cropper was successfully used to assign metagene identifiers to the datasets and then to combine the datasets in to a single result dataset, which was used in further analyses . z-transformation  <cit>  was used to normalise the data by calculating z-ratios for the difference between control and treatment data in each of the original study cases .

cropper was used to combine datasets from four distinct experiments. metagene identifiers were assigned to each data row. inspection of the gene descriptions reveals that it is likely that combining has been successfully and metagene identifiers have been assigned to a common group of genes and gene products across the different species and technology platforms.

the genes that were present in the human dataset  were clustered into  <dig> clusters using a self-organizing map  with genespring  <dig>   as presented in figure  <dig>  a new gene list was created from the clusters in which the expression levels greatly varied between the conditions . from this list, those also regulated in the human disease state  were considered to be the most likely candidates in the disease models. the profiles of these  <dig> genes are marked in green in the figure  <dig>  in addition, the genes which were regulated in any of the animal data sets by a z-ratio of ±  <dig>  but not in the human parkinson's disease sample   are marked in red in figure  <dig>  these two lists of genes were inspected for the enriched kegg-pathways by using david  <cit>  with the whole human genome as a background list and for the enriched go-terms by using generator  <cit>  with the present human genes from the combined dataset as a background list. the results from different phases of the analysis are presented in the additional file  <dig> 

the biological themes discovered from the lists of regulated genes suggest that biological hypotheses with explanatory power can be generated using the metagene approach. the results also suggest that combining datasets from different studies provides a valuable tool for validating results, as the human dataset was used to filter out genes not detected in the human disease state. this makes it possible to detect genes from the animal experiments that are most likely to be involved in the actual human disease, supporting a selection process of candidate genes based not only on statistical power, but also on the biological differences between species. in the analysed data, the clustering of genes based on the go-terms revealed that the transport proteins, molecular biosynthesis mechanism, and the neurofilaments are good candidates for studies in most of the animal models for neurodegeneration. calmodulin and calcium related modulatory mechanisms are also detected in the animal models. downstream data extraction can be performed by export of the combined data to view enriched terms from the kegg pathway , that then can be used to create new hypotheses for the further studies.

discussion and 
CONCLUSIONS
researchers performing experiments using novel genomic research methods often face the challenge of combining their experimental results with results derived from different heterogeneous sources, such as experiments conducted using different technologies, different model organisms or results retrieved from public databases. we have developed a web-based software program called cropper that automates this task.

several compendium studies that combine data from different sources have been published, but it is common that the actual combination and integration of the data has been done using custom-made software programs and scripts that are useful only for the data used for the specific study  <cit> . this has resulted in the need of complete end-user programs that biologists can use to combine their datasets. different resources have been developed to address this challenge  <cit> , but are hindered by several limitations. these limitations include focusing on a single  of species/technology, requiring a strict pre-defined format on datasets, not allowing customisation of the result file or including actual experimental data in the dataset. cropper differs from these resources by giving users a good flexibility on how to import and export data and broad coverage of all the major databases, technologies and species, making cropper useful for a very wide variety of users.

one of the main strengths of cropper is that it uses the ensembl-database, therefore ensuring that all the major data sources and species are covered, and that the data is always up-to-date. what distinguishes cropper from the data mining tools provided by ensembl  <cit> , is that cropper is specially designed for automated data integration, implementing the original metagene approach, therefore allowing users to combine numerous distinct datasets, bring the experimental data along, and not requiring other software or programming tools to facilitate the combining. this is in addition to its ease of use to help biologists combine their datasets and to gain increased power for their research, which could not be obtained by direct usage of ensembl data mining tools. moreover, users do not need to be familiar with ensembl or their data mining tools to use cropper.

when performing compendium studies, the researcher should pay attention to the steps taken in data combining. for example it should be clear that when cross-linking gene datasets to protein datasets, accuracy is lost. it is also lost when combining datasets derived from different technologies. for example, when combining data from cdna and oligonucleotide microarrays, the actual experimental measurements are very rarely directly comparable. it should be noted that cropper only combines the related data rows, but does not alter the experimental data in any way. therefore it is likely that depending on the type of the study, different methods and tools are required to make the data comparable. in many cases, the z-ratio method presented here will work, but other methods can also be used. many statistical and computational methods and software are publicly or commercially available towards this end. when performing compendium studies, the combining of data elements is usually the first and most difficult task and using cropper helps researchers to overcome this major bottle-neck in the integration of genomic data.

availability and requirements
project name: cropper

project homepage: 

operating system: platform independent

programming language: perl

other requirements:

license: free for academic use

any restrictions to use by non-academics: license needed

authors' contributions
jp designed and developed the methodology and software and drafted the manuscript. ms performed the compendium study and helped to draft the manuscript. gw conceived the study, participated in its design and coordination and helped to draft the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
combined data used for the example analysis. combined data from the four different neurodegenerative studies, used for the example compendium study. data presented with z-ratios  and the genes regulated in human and/or in other organisms are presented.

click here for file

 additional file 2
the biological themes and patterns detected in the combined data. first worksheet contains the enriched kegg pathways, number of genes with association to a pathway, and the p-value for the statistical significance. second and third worksheet contain the enriched go terms detected in the combined data, clustered into  <dig>   <dig> and  <dig> clusters based on the associated go terms.

click here for file

 acknowledgements
jp was financially supported by the a.i. virtanen institute graduate school and academy of finland. ms was supported by grants from the university of kuopio. gw was supported by the academy of finland and university of kuopio. the authors thank suvi vartiainen, jani kekäläinen, petri pehkonen and petri törönen for discussions and helpful comments. the support and development staff of the ensembl project are acknowledged for their assistance during the project.
