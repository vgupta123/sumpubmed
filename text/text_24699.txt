BACKGROUND
single-nucleotide polymorphisms  serve as markers for mapping disease-associated genetic variants. it has been well known that snp profiles are associated with a variety of diseases  <cit> . high-throughput genotyping technologies have been used to assay hundreds of thousands of snps in the human genome. many single-locus based methods  <cit>  have been proposed and many susceptibility determinants have been identified  <cit> . however, these identified snps seem to be insufficient in explaining the genetic contributions to complex diseases  <cit> . researchers start to suspect that the causality of many common disease are more related with gene-gene interactions rather than with single genetic variations  <cit> . for many common complex diseases, some snps have shown little main effects while their interactions are significantly associated with disease traits  <cit> . consequently, detecting gene-gene interactions is a topic of current interest in gwas  <cit> . many methods have recently been proposed to identify interaction patterns associated with diseases, including mdr  <cit> , cpm  <cit> , rpm  <cit> , bgta  <cit> , snpruler  <cit> , lasso  <cit> , hapforest  <cit> , boost  <cit> , plink  <cit> , beam  <cit> , snpharvester  <cit>  and intersnp  <cit> . however, a key issue of applying most of these methods in gwas is the computational burden  <cit> . for example, to find pairwise interactions from  <dig>  snps, we need  <dig>  ×  <dig> statistical tests in total. to address this issue, screening approaches  <cit>  have been proposed. the whole process of detecting gene-gene interactions is then divided into three stages:

• screening: evaluate the importance of each snp and assign it a score. those snps with scores lower than the given threshold are removed without further consideration. often a small portion of snps remains. this stage is often accomplished by fast algorithms using heuristics to reduce the search space of the next stage. one example is the popular screening method tuning relieff  <cit> .

• modeling: search for the best combination of snps in the remaining snps. the exhaustive search can be used in this stage because the number of remaining snps is small, e.g., the popular modeling methods mdr and cpm. during the search process, the importance of a snp combination is often measured by its prediction accuracy . thus, the best snp combination and its corresponding interaction pattern can be identified in term of prediction accuracy.

• testing: assess the significance of interaction patterns by hypothesis testing.

hypothesis testing employed in the testing stage is also referred to as “feature assessment” in  <cit> . a critical issue in feature assessment is to choose an appropriate null distribution for hypothesis testing. an inappropriate null distribution may lead to an over-optimistic result  or an over-conservative result   <cit> . figure  <dig> gives a toy example. in this figure, null distribution  <dig> follows the χ <dig> distribution with the degree of freedom df =  <dig>  denoted as , and null distribution  <dig> follows . if the true null distribution is , then using  for hypothesis testing will give many false positive results.

in this paper, we show through simulations that both screening and modeling may change the null distribution used in hypothesis testing. however, many methods  neglect the rightward shift of the original null distribution  caused by screening and modeling. they inevitably suffer from the higher false positive rate. we have also noticed that some methods  <cit>  modify the test statistics, which causes the leftward shift of the original null distribution. if we still stick to the theoretical null distribution in hypothesis testing, we may produce conservative results . to address this issue, we suggest to use the permutation test and testing on the independent data set. the permutation test uses the re-sampling method to estimate the changed null distribution for hypothesis testing. testing on the independent data set can reserve the theoretical null distribution. through simulation experiments and the experiment on a real genome-wide data set from an amd study, we demonstrate that the appropriate choice of null distributions leads to more reliable results.

RESULTS
simulation study of the inappropriate choice of null distributions
the huge number of snps in gwas poses a heavy computational burden for detecting gene-gene interactions. the exhaustive search of all pairwise interactions and further using cross-validation to evaluate them  become impractical in gwas. to make it computationally feasible, a screening method is applied to the whole data set to pre-select a small subset of snps. then the exhaustive search can be applied to identify the most likely disease-associated snps. at last, hypothesis testing is conducted on identified snps. the importance of hypothesis testing is briefly discussed in the discussion section. here we use mdr and some efficient screening methods  <cit>  as examples to show that null distributions are affected by these methods throughout this paper, we use the latest mdr software  to perform all experiments. it also implements various screening methods, such as relieff, tuning relieff, and surfstar. we first show that the modeling process of mdr changes the null distribution in its search process. then we show that mdr coupled with some screening methods further changes the null distribution.

the null distribution of mdr
mdr is a popular non-parametric approach for detecting all possible k-way  combinations of snps that interact to influence disease traits. mdr runs 10-fold cross-validations, and uses the prediction errors and the consistencies to search for the optimal set of k-way interactions. for each of the selected k-way interactions, mdr constructs a  <dig> ×  <dig> contingency table by partitioning the samples into the high-risk and low-risk groups. then mdr conducts hypothesis testing based on this  <dig> ×  <dig> contingency table. table  <dig> illustrates how this is done by mdr. the authors claimed that mdr could reduce the k dimensional model into a  <dig> dimensional model  <cit> . therefore, the current mdr software conducts the statistical test on a  <dig> ×  <dig> contingency table using  as its null distribution. this statement is biased because the higher dimensional space has to be browsed in order to construct the  <dig> ×  <dig> continency table  <cit> . here we use simulation experiments to show the correct null distribution in hypothesis testing for mdr.

suppose the threshold of the odds ratio in mdr is specified to be τor =  <dig>  then the  <dig> ×  <dig> contingency table is obtained by mdr in the following way: for the cells whose odd-ratio is higher than τor will be considered as high-risk. for case genotypes, the bold numbers indicate these genotypes are considered as high-risk. their summation equals to  <dig> as shown in bold font in the mdr table. for control genotypes, the italic numbers also indicate these genotypes are considered as high-risk. their summation equals to  <dig> as shown in italic font in the mdr table. for the low-risk cells, this is done in the same way. statistical tests can be conducted based on the  <dig> ×  <dig> contingency table: χ <dig> =  <dig>  for the pearson χ <dig> test and χ <dig> =  <dig>  for the likelihood ratio test.

we design two scenarios with three settings of d-way interactions , one showing the true null distributions of mdr  and another showing the change of null distributions of mdr . for each scenario of this experiment, we generate  <dig> null data sets, each of which contains  <dig>  samples.

in the first scenario, we generate data sets containing two, three and four snps for the settings d =  <dig>   <dig>   <dig>  respectively. all snps are generated using the hardy-weinberg principle with minor allele frequencies uniformly distributed in . by doing so, the mdr model can be directly fitted without search. let us take d =  <dig> as an example. for each null data set, we first obtain a genotype contingency table as shown in table  <dig>  and then collapse it into a  <dig> ×  <dig> contingency table. next we conduct the statistical test . the histogram of the statistics forms the null distribution. for d =  <dig> , this can be done in the same way. the histograms of these null distributions obtained from  <dig> null data sets are shown in the upper panel of figure  <dig>  we observe that the null distributions of mdr  follow the χ <dig> distributions. the estimated degrees of freedom of the χ <dig> distributions are df =  <dig> , df =  <dig>  and df =  <dig>  for d =  <dig>  d =  <dig> and d =  <dig>  respectively. the non-interger degree of freedom is well defined, see  <cit> . this clearly indicates that  is not an appropriate null distribution for mdr.

in the second scenario, we generate the data sets containing  <dig> snps for all three cases. these snps are generated in the same way as in the first scenario. in this scenario, mdr first searches for the best d*-way interactions using 10-fold cross-validation and then conducts the testing. the results are shown in the lower panel of figure  <dig>  compared with the first scenario, the null distributions change when mdr searches for the best d*-way interactions  in d =  <dig> dimensions. we see that the null distributions do not strictly follow χ <dig> distributions. we use χ <dig> distributions to approximate them, and obtain their degrees of freedom as df =  <dig> , df =  <dig>  and df =  <dig> , respectively.

in summary, our result shows the following facts:

• the null distribution in mdr does not follow  even when search process is not involved.

• the null distribution in mdr further changes when the search process is involved.

therefore, hypothesis testing using  as the null distribution in mdr will give many false positive results.

the null distribution for mdr combined with screening methods
mdr works well for small studies with  <dig> or less snps. in gwas, it is not practical to use this exhaustive search method. therefore, many screening methods have been proposed to reduce the number of snps before mdr is applied to detect d*-way interactions. these screening methods include relieff  <cit> , tuning relieff   <cit> , surf  <cit> , surfstar  <cit> . the reader is referred to  <cit>  for a recent review on these screening methods.

the issue here is that after these screening methods are applied, the null distributions are further changed. to show the effect of screening methods on the null distribution, we generate  <dig> null data sets. each data set contains  <dig>  samples and each sample contains l =  <dig>  snps. all snps are generated using the hardy-weinberg principle with minor allele frequencies uniformly distributed in . different screening methods, such as relieff, turf, surfstar, are first applied to reduce l =  <dig>  snps to d =  <dig> snps. after that, mdr is applied to find the best d*-way interactions . figure  <dig> presents the experiment results. the lower panel of figure  <dig> serves as the reference distributions. it is obvious that all three screening methods further change the null distributions.

simulation study of the suggested solutions
two solutions are suggested in the method section. the first one is the permutation test and the second is testing on the independent data set. the permutation test works well to estimate null distributions in practice. although the computational cost of this method is high, it is still feasible to be applied in gwas when an efficient screening method is available. it is unnecessary to show its performance in simulation studies since it is a standard way of calibrating the null distribution in hypothesis testing  <cit> . we demonstrate its performance using a real genome-wide data set from an amd study in the next section. here we show that the null distribution does not change when testing on an independent data set. we generate  <dig> null data sets. each data set contains  <dig>  samples and each sample has l =  <dig>  snps. each data set is partitioned into three subsets as nearly equal as possible: d, d, d. the screening method relieff is first applied to d, and the number of snps is reduced from l =  <dig>  to d =  <dig> snps. the indices of the remaining  <dig> snps are collected in a <dig>  after that, stepwise logistic regression  is applied to d to find the best d*-way interactions  among the snps in a <dig>  at last, the likelihood ratio test is applied to the identified snp set in hypothesis testing: it uses the difference between the deviance of the full d*-way logistic regression interaction model and the deviance of the null logistic regression model. here we first use lr rather than mdr, because the null distributions of lr are known analytically. for  <dig>   <dig>  4-way full logistic regression interaction models, the null distributions follow χ <dig> distributions with df =  <dig>   <dig>   <dig>  respectively. the experiment results are present in the upper panel of figure  <dig>  we can see that the obtained null distributions of logistic regression models match the theoretical null distributions well for  <dig>  3-way interaction models. for the 4-way interaction model, the estimated null distribution has a smaller degree of freedom than the theoretical null distribution. this is because about  <dig> samples are not enough to accurately estimate the large degree of freedom of the theoretical null distribution . this is a disadvantage of using an independent data set for testing. when there are not enough samples, testing on an independent data set may have a lower power if its theoretical null distribution is used .

to see the effect of mdr using the independent data set in hypothesis testing, we similarly divide each null data set into three subsets: d, d, d. relieff is applied to d and then mdr is applied to d. at last, χ <dig> tests are conducted on d using the  <dig> ×  <dig> contingency tables obtained by the mdr method. the result is shown in the lower panel of figure  <dig>  we can see that the obtained null distributions of mdr agree with the distributions shown in the upper panel of figure  <dig>  this result clearly shows that the null distributions are not changed by screening and modeling when testing on the independent data set.

real data analysis using the suggested solutions: an experiment on the aged-related macular degeneration  data set
we use the amd data from  <cit>  as a real example. the amd study genotyped  <dig>  snps on  <dig> cases and  <dig> controls. after applying quality control to the amd data set, we have  <dig>  qualified snps. two significant loci, rs <dig> and rs <dig>  were reported in  <cit>  based on the allelic association test with degree of freedom df =  <dig> 

first, we use the latest mdr software to analyze this data set. relieff is applied to reduce the number of snps in amd data from l =  <dig>   <dig> to d =  <dig>  after that, mdr is applied to the remaining  <dig> snps to search for the best model. the result is given in table  <dig>  the amd data set is available at http://bioinformatics.ust.hk/nulldistramd.zip. our analysis result given in table  <dig> can be freely reproduced. following the typical selection procedure of mdr, the four-way interaction among snps  is considered as the best one. if the effect of screening and modeling on changing the null distribution is not taken into account, then the hypothesis testing on m <dig> using the null distribution  ) will give a p-value of  <dig>  × 10– <dig>  which is a significant result.


                              m

                              m

                              m

                              m
the result obtained by using mdr after screening. relieff is used to reduce the number of snps from l =  <dig>   <dig> to d =  <dig>  after that, mdr is applied to the remaining  <dig> snps to search for the best model. the fourth-way interaction model m <dig> is considered as the best model because it gives high cv accuracy and cv consistency. here cv stands for cross-validation. the χ2-value of  <dig>  will correspond to a p-value of  <dig>  × 10– <dig> against the null distribution  ), indicating a significant hypothesis testing result. but the permutation test gives it a p-value of  <dig>  , indicating a non-significant result.

second, we use the permutation test to re-examine the above result. we conducted b =  <dig> permutations to the amd data set. the details are given in the method section. the result is shown in figure  <dig>  the p-value obtained by the permutation test for model m <dig> is  <dig> , which is far from being significant. more importantly, the permutation result shows that model m <dig> is significant with p-value  <dig> . this result is consistent with that in the original paper  <cit> .

third, we apply testing on the independent data set. the whole data set is partitioned into three groups . reileff, mdr and hypothesis testing are sequentially applied to them, as described in the method section. finally, no significant features are reported. this seems different from the result of the permutation test. the reason is that testing on the independent data set has a lower power than the permutation test. the number of samples of the amd data set is very small . after a nearly equal partition of the data set, only  <dig> samples can be used in hypothesis testing. thus no significant results can be detected . since the permutation test often has a higher power, the permutation test is preferred when the computational cost is affordable.

discussion
the importance of hypothesis testing in feature assessment
it is important to note that hypothesis testing plays a key role in feature assessment  <cit> . model selection is a closely related topic, which aims to identify the best model in term of the prediction accuracy  <cit> . analytical methods such as akaike information criterion , bayesian information criterion  can be applied for model selection. efficient sample re-use methods such as cross-validation and bootstrapping can be applied here as well. however, to statistically quantify the importance of selected features, feature assessment is preferred. instead of considering prediction accuracy, feature assessment makes use of hypothesis testing to statistically assess the significance of features. to characterize the performance of hypothesis testing, different measures have been defined, e.g., the family wise error rate  and the false discovery rate   <cit> . the bonferroni correction and the benjamini-hochberg method  <cit>  can be used for controlling fwer and fdr, respectively.

as pointed out by efron  <cit> , the choice of null distribution is critical in hypothesis testing. the empirical null distribution may not match the theoretical null distribution due to reasons such as inappropriate assumptions or correlation across features and samples. this paper shows that screening  and modeling  can also change the null distribution. if their effect is not taken into account in hypothesis testing, the resulting feature assessment will be unreliable.

related work
we have shown that inappropriate choice of null distributions will give misleading results of hypothesis testing. one example is that mdr combined with tuning relieff  <cit>  will give over-optimistic results. alternatively, some other methods  <cit> , which modify the test statistic but stick to the theoretical null distribution, may produce conservative results. for example, machini et. al  <cit>  proposed a two-stage method for detecting interactions in genome-wide scale. in the first stage, a single-locus-based test was performed. those snps with significant p-values  were selected. the selected set of snps was denoted as i <dig> . in the second stage, for each pair of snps l and m , the log likelihood ratio statistic r was calculated for the full interaction model. they defined a new statistic r′ = r – , where kl and km were the single-locus χ <dig> values for snps l and m. the significance of this statistic was assessed against  distribution, where df =  <dig> is the degrees of freedom of the full model fitted at the two snps. they showed that their method was conservative in term of the false positive rate. in fact, the modified statistic r′ is a shrunken version of r, but hypothesis testing is performed using the degree of freedom of r, where l, m ∈ { <dig> …, l}, l ≠ m. thus, this method will be too conservative to detect interesting interactions.

CONCLUSIONS
gwas have identified many genomic regions associated with complex diseases. however, some previously reported results are based on an inappropriate choice of null distributions, which will produce many false positive results. in this work, we have illustrated that both screening and modeling can change the null distribution used in hypothesis testing. this causes unreliable significance assessment. we have suggested two solutions to address this issue. one is to use the permutation test and another is to use the independent data set for testing. both solutions can help to appropriately choose null distributions, while the permutation test has a higher power with more computational cost.

