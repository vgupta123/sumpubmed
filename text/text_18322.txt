BACKGROUND
choosing an appropriate model of molecular evolution  is an important part of phylogenetics, and can affect the accuracy of phylogenetic trees, divergence dates, and model parameters  <cit> . one of the most important aspects of model selection is to find a model that can account for variation in the substitution process among the sites of the alignment. this variation may include differences in rates of evolution, base frequencies, and substitution patterns, and the challenge is to account for all such variation found in any given dataset. there are many different ways to approach this problem, of which the simplest and most widely used is partitioning. in the broadest sense, partitioning involves estimating independent models of molecular evolution for different groups of sites in an alignment. these groups of sites are often user-defined , for example based on genes and codon positions  <cit> . it is also increasingly common to refine user-defined partitioning scheme by combining similar data blocks algorithmically  <cit> . a vast number of phylogenetic studies have used partitioned models of molecular evolution, and it is widely appreciated that partitioning often leads to large improvements of the fit of the model to the data . many studies also report that partitioning has improved phylogenetic inference, including the estimation of tree topologies, branch lengths, and divergence dates  <cit> .

partitioning is one of many methods to account for variation in substitution processes among sites. some approaches automatically assign sites to different substitution models , and others estimate more than one model of molecular evolution for each site . many of these methods are better and more elegant than the form of partitioning we focus on here, because they do not rely on user-defined data blocks and can more effectively scale to the true variation in substitution processes present in the data. however, partitioning remains the most widely-used method to account for variation in rates and patterns of substitution among sites  <cit> . its enduring popularity is part historical contingency and part practical: many of the superior methods are more recent and have not yet become widely adopted, and partitioning is implemented in many popular phylogenetic inference programs. most importantly for this study, partitioning is still the most practical method with which to account for variation in rates and patterns of substitution in very large datasets. because of this, it is important that we work to ensure that partitioned models of molecular evolution are as accurate as possible, particularly when they are applied to large datasets, and that is the focus of this study.

it is important to note that all of the commonly used methods to account for variation in substitution patterns among sites  assume that sequences evolved under a stationary, reversible, and homogeneous process. these assumptions are necessary to make the methods efficient enough to allow for searches of phylogenetic tree space, although they are far from guaranteed to hold for empirical datasets . it is possible to relax these assumptions, but the computational cost of doing so is extremely high and precludes effective tree searches in all but the simplest cases. so for the time being it is necessary to make these assumptions in order to estimate tree topologies from very large datasets.

the biggest challenge in partitioning is to select the most appropriate partitioning scheme for a given alignment, i.e. to divide the alignment into groups of sites that account for variation in patterns of molecular evolution, while avoiding over- or under-parameterisation  <cit> . to select a partitioning scheme, phylogeneticists typically start by grouping together putatively similar sites in an alignment into homogenous data blocks, using a priori knowledge of the variation in patterns of molecular evolution among sites  <cit> . the challenge is then to find an optimal partitioning scheme by combining sufficiently similar data blocks, which is usually done by finding the combination of data blocks that minimises a metric such as the corrected akaike’s information criterion  or the bayesian information criterion   <cit> . for smaller datasets of up to about  <dig> initial data blocks, this optimisation step can be achieved automatically using a greedy heuristic search algorithm implemented in the software partitionfinder  <cit> . however, recent reductions in dna sequencing costs mean that it is now routine to produce very large ‘phylogenomic’ datasets which can contain hundreds or thousands of loci  <cit> . current methods  <cit>  are not computationally efficient enough to optimise partitioning schemes for these datasets. for example, the greedy algorithm implemented in partitionfinder would have to analyse almost  <dig> million subsets of sites to estimate the optimal partitioning scheme for a sequence alignment of  <dig> protein-coding loci, which is well beyond the bounds of practicality. this is a problem, because we have no methods to optimise partitioning schemes for the largest, and potentially most useful, datasets in phylogenetics.

hierarchical clustering is a statistical method that has some attractive properties for optimising partitioning schemes for phylogenomic datasets. to use hierarchical clustering to optimise partitioning schemes, molecular evolutionary parameters  are first estimated for each initial data block, and data blocks are then combined based on the similarity of their parameter estimates. hierarchical clustering and related methods  have been used to select partitioning schemes in a number of previous studies with datasets of various sizes  <cit> . hierarchical clustering is far more computationally efficient than the greedy algorithm implemented in partitionfinder: if n is the number of data blocks specified by the user, hierarchical clustering is o, while the greedy algorithm is o. for example, with an alignment of  <dig> protein-coding genes, the strict hierarchical clustering approach we describe below requires the analysis of only  <dig> subsets of sites , which is more than  <dig> orders of magnitude more efficient than existing approaches.

one drawback of hierarchical clustering is that a-priori decisions have to be made about the best way to determine the ‘similarity’ of different data blocks. researchers typically estimate up to four parameter categories for each data block:  a parameter to describe the overall substitution rate of that data block ;  one or more parameters to describe the relative rates at which nucleotides replace each other  model, known as the rate matrix);  parameters to describe the proportions of nucleotides or amino acids in the data block ; and  one or two parameters to describe the distribution of substitution rates among sites . in principle, data block similarity can be defined using any combination of these parameters. however, different studies have used different parameter combinations, and there has been no attempt to systematically understand the best way to define the similarity of different data blocks when estimating partitioning schemes  <cit> .

in this study, we set out to investigate the performance of hierarchical clustering approaches for optimising partitioning schemes for phylogenomic datasets. we first developed a generalised strict hierarchical clustering method that allows the user to define relative importance of different model parameters when defining the similarity of subsets. we found that the choice of weighting scheme can have very large effects on the performance of the algorithm, and that regardless of the weighting scheme strict hierarchical clustering always performed substantially worse than the existing greedy algorithm. to remedy this, we developed a new method, which we call relaxed hierarchical clustering, that incorporates many of the benefits of strict hierarchical clustering while avoiding many of its disadvantages. we show that relaxed hierarchical clustering outperforms strict hierarchical clustering on all of the datasets that we examined. the computational demands of this method can be scaled to the dataset and computational resources available. it is therefore a pragmatic approach to estimating best-fit partitioning schemes on phylogenomic datasets, where more rigorous methods are computationally infeasible.

we have implemented all of the methods described in this study in the open-source software partitionfinder, which is available for download from http://www.robertlanfear.com/partitionfinder. the partitionfinder source code is available from https://github.com/brettc/partitionfinder/.

methods
terminology
following previous studies  <cit> , we define a “data block” as a user-specified set of sites in an alignment. a data block may consist of a contiguous set of sites , or a non-contiguous set . a “subset” is a collection of one or more data blocks. therefore, all data blocks are also subsets, but the converse is not true. a “partitioning scheme” is a collection of subsets that includes all data blocks once and only once. we do not use the term “partition” because it has conflicting meanings in phylogenetics and set theory – in phylogenetics a “partition” is used colloquially to denote what we call a “subset” here, whereas in set theory it defines what we call a “partitioning scheme”  <cit> .

strict hierarchical clustering algorithm
we developed a strict hierarchical clustering algorithm inspired by a popular previous implementation  <cit> , with some improvements. this algorithm is extremely efficient – given a set of n initial data blocks it creates a set of n partitioning schemes with between  <dig> and n subsets, and then selects the best partitioning scheme from this set. the algorithm has seven steps, which we summarise here and describe in more detail below:

 <dig>  estimate a phylogenetic tree topology from the sequence alignment;

 <dig>  start with a partitioning scheme that has all user-defined data blocks assigned to independent subsets;

 <dig>  calculate the ml model parameters and log likelihood of each subset in the current partitioning scheme;

 <dig>  calculate the similarity of all pairs of subsets in the current partitioning scheme;

 <dig>  create a new partitioning scheme by combining the two most similar subsets in the current partitioning scheme;

 <dig>  return to step  <dig>  until a partitioning scheme with all sites combined into a single subset is created ;

 <dig>  choose the best-fit partitioning scheme based on information theoretic metrics.

in principle this algorithm could be applied to dna or amino acid alignments, but for simplicity we focus only on dna alignments in this study.

all ml calculations in this algorithm are performed with a modified version of raxml  <cit>  available at https://github.com/brettc/standard-raxml because raxml is the most widely-used and computationally efficient software for analysing extremely large alignments. we substantially modified the partitionfinder code  to enable it to perform model selection and partitioning scheme selection by calling raxml, and parsing the output produced by raxml.

in step  <dig> of the strict hierarchical clustering algorithm, we estimate a maximum parsimony  starting topology in raxml which is then fixed for the rest of the analysis. fixing the topology is crucial in increasing computational efficiency when searching for best-fit partitioning schemes  <cit> . although mp is known to perform poorly relative to maximum likelihood  when estimating phylogenetic trees, previous studies have shown that any non-random tree topology is adequate for accurate model selection  <cit> . nevertheless, our implementation of this algorithm in partitionfinder allows users to specify a starting tree topology calculated using any method, so that datasets for which mp has known issues may still be analysed rigorously.

in step  <dig> we calculate the log likelihood and parameters of a gtr + g model on each new subset of sites using raxml. a new subset of sites is defined as a subset that the algorithm has not yet encountered. the log likelihood and ml parameters of each subset are then stored in memory so that they do not have to be recalculated in subsequent iterations of the algorithm. we use the gtr + g model rather than the gtr + i + g model because the ‘i’ parameter, which describes the proportion of invariant sites, is not independent from the ‘g’ parameter, which describes the gamma distribution of rates across sites, making it impossible to estimate both parameters accurately  <cit> . this dependency between ‘g’ and ‘i’ compromises attempts to infer the similarity of subsets using their parameter estimates . in principle, however, step  <dig> could include the selection of the best model of molecular evolution for any given subset.

in step  <dig> we calculate the similarity of subsets based on their ml model parameters. to do this, we group parameters into four categories and use a distance metric that allows users to specify the relative importance of different parameter categories. the four parameter categories are:  the overall rate of evolution of the subset, calculated as the sum of the maximum likelihood branch lengths for that subset;  the  <dig> parameters of the general time reversible  model;  the four base frequencies; and  the alpha parameter that describes the gamma distribution of rates across sites. the parameters from categories  and  are not independent of each other, but we include both because we do not have prior information on which parameters are more important, or which may be most useful for optimising partitioning schemes. to calculate the similarity of all pairs of subsets, we first calculate a pairwise euclidean distance matrix for each of the four parameter categories. we then normalise each distance matrix so that the maximum distance is one, and then scale each matrix by a user-specified weight . the similarity of a pair of subsets is then calculated as the sum of the distances across the four matrices, which gives the manhattan or city block distance between a pair of subsets. in this approach, the user-specified weights have a natural interpretation as the relative importance of different parameters in defining subset similarity.

this approach to calculating subset similarity has a number of advantages over previous methods. many previous approaches have used fewer than the four categories we define to calculate subset similarity, and most have implicitly assumed that all parameter categories are equally important in determining subset similarity  <cit> . in contrast, our method allows for any combination of parameter categories to be specified, and for the relative importance of each category to be specified. for example, a parameter category can be excluded from similarity estimates by setting its weight to zero. similarly, a parameter category can be defined as tenfold less important than other categories by setting its weight to  <dig> , and the weights of the other categories to  <dig>  another limitation of previous clustering approaches is that they have estimated the parameters of larger subsets directly from the parameter estimates of subsets they contain  <cit> . this approach is problematic because it is difficult to predict how the information in two smaller subsets will combine to determine the parameters of the larger subset, and simply averaging the ml parameters of the smaller subsets is unlikely to produce parameters close to the ml parameters for the larger subset. furthermore, error in the parameter estimates of the smaller subsets may limit their accuracy in the first place  <cit> . our approach circumvents these problems by calculating ml parameter estimates for every subset that is analysed, including subsets that were created by merging together two smaller subsets. this approach ensures that the hierarchical clustering procedure is as accurate as possible, given the limitations of estimating model parameters from finite datasets.

in each iteration of our algorithm, we find the most similar pair of subsets from the focal partitioning scheme , and then merge these subsets to create a new subset and a new partitioning scheme . in this manner, the algorithm iteratively merges subsets to create a set of n partitioning schemes from n initial data blocks. these n schemes contain from  <dig> to n subsets. the final step of the algorithm  simply involves comparing the information theoretic score  of all n partitioning schemes, and choosing the scheme with the best score. choosing the best partitioning scheme does not involve any further ml calculations, because the log likelihood of each partitioning scheme can be calculated from the sum of the log likelihoods of the subsets contained in that scheme  <cit> .

relaxed hierarchical clustering algorithm
the strict hierarchical clustering algorithm is computationally efficient, but it has some obvious drawbacks. first, it can merge subsets that make the information theoretic score of a partitioning scheme worse, rather than better. this is because there is no guarantee that any given measure of ‘similarity’ will translate into an improvement in the information theoretic score. second, even if a given similarity measure does translate into robust improvements in the information theoretic score, the algorithm may be misled when the accuracy of ml parameter estimates is limited, as can be the case with small subsets  <cit> .

to overcome these limitations, we propose a relaxed hierarchical clustering algorithm. this algorithm has eight steps:

1- <dig>  identical to strict hierarchical clustering

 <dig>  select the top p% of most similar subset pairs;

 <dig>  create s new partitioning schemes, each of which includes one of the subset pairs from step 5;

 <dig>  choose the partitioning scheme from step  <dig> with the best information-theoretic score ;

 <dig>  return to step  <dig>  until no further improvements in the information theoretic score are found;

steps 1– <dig> proceed precisely as in the strict hierarchical clustering algorithm. in step  <dig> we create a ranked list of all possible subset pairs from the current partitioning scheme, where the rank is defined by the similarity of the subsets. we then use a user-defined percentage, p , to choose the s most similar subsets pairs. in step  <dig> we create a new partitioning scheme for each of the s subset pairs, by merging the two subsets in each pair and calculating the new log-likelihood and maximum likelihood parameter estimates. in step  <dig>  we calculate the information theoretic score  of each of the s new partitioning schemes, and select the partitioning scheme with the best score. the algorithm then iterates  until no further improvements in the information theoretic score can be found.

the key difference between the relaxed and strict hierarchical clustering algorithms is the ability to set the parameter p, which controls the thoroughness of the heuristic search algorithm. when p is set to 0%, the relaxed clustering algorithm will behave similarly to the strict hierarchical clustering algorithm, and only evaluate the single partitioning scheme that includes the most similar pair of subsets . when p is set to 100%, the relaxed clustering algorithm will behave similarly to the existing greedy algorithm in partitionfinder  <cit> , and evaluate all possible subset pairs at each iteration of the algorithm. larger values of p will take more computational time, but are also likely to produce better solutions because they will search the space of partitioning schemes more thoroughly. in preliminary analyses we observed that even very small values of p  can often lead to the discovery of partitioning schemes that dramatically outperform those found by the strict hierarchical clustering algorithm.

datasets
as described above, we expect both of the new methods we describe here to perform worse than the greedy algorithm implemented in partitionfinder, simply because they are less thorough heuristic searches. the true utility of the new methods is to find partitioning schemes for datasets that are too large to analyse with existing methods  <cit> . nevertheless, to properly assess the new algorithms described here, it is necessary to compare them to existing approaches. because of this, we focussed our analyses on data sets to which we could apply both the new and existing methods.

we used  <dig> publicly available datasets  to compare the clustering methods to existing approaches. these datasets comprise a range of different sequence types , and come from a range of different taxa. the largest dataset comes from a phylogenomic study of birds , and comprises  <dig> taxa,  <dig> sites, and  <dig> data blocks. this dataset is close to the upper size limit of datasets that can be analysed using the greedy algorithm implemented in partitionfinder  <dig> . <dig>  <cit> , so represents the practical limit of datasets that we can include in this study. in two cases  we reduced the number of taxa in the original dataset, by removing the taxa with the most gaps, in order that we could analyze the dataset using the greedy algorithm in partitionfinder. precise details of the taxa we removed are provided in the figshare repository associated with this article . removing taxa does not reduce the complexity of the task of selecting partitioning schemes, but simply reduces the computational burden of analysing each subset. note that this is done to provide a suitable set of test datasets for comparing new and old methods, and we do not mean to imply that partitioning schemes estimated from reduced-taxon datasets should be used on the full-taxon dataset. all of the datasets, as well as the associated input files for partitionfinder, are available from figshare , and references for the datasets and the studies that they are associated with are provided in table  <dig> 

the original study describing each dataset is referenced, the dataset itself is also referenced where it is archived under a separate doi.

analyses
we exhaustively compared the two new algorithms to existing methods using the largest dataset in this study . based on the results of these analyses, we compared the two new algorithms to existing methods across the ten datasets described in table  <dig>  the analyses were run in partitionfinder version  <dig> . <dig> with the following settings common to all analyses: we used the raxml version of partitionfinder developed for this study , because the older phyml version of partitionfinder is not computationally efficient enough to analyse the very large datasets that are the focus of this study ; all analyses were performed twice – once with model selection performed under the aicc, and once under the bic; all branch lengths were set to ‘linked’ in all analyses, meaning that relative branch lengths were estimated at the start of the analysis using a gtr + g model in raxml, and that these relative branch lengths were then fixed for the rest of the analysis, with each subset afforded its own rate multiplier to account for differences in rates of evolution between subsets  <cit> ; only the gtr + g model of evolution was considered . we do not consider analyses using the aic, because the aicc should be preferred to the aic in all cases  <cit> .

we note that the approach we have implemented here, using a rate multiplier and a single set of molecular branch lengths, does not allow for heterotachy , although this is known to be an important source of variation in patterns of substitution  <cit> . in principle, our approach can account for heterotachy by allowing each subset to have an independent set of branch lengths, and this can be achieved in partitionfinder by setting ‘branchlengths’ option to ‘unlinked’. however, in practice this way of accounting for heterotachy adds so many parameters to the overall model that it is inferior to using a rate multiplier. a better approach is to use a covarion model or a mixture of branch lengths  <cit> , but since our focus here is producing partitioning schemes for very large datasets that can be subsequently analysed in raxml, and since neither of these models is available in raxml, we do not consider them further here.

for every analysis, we recorded:  the best partitioning scheme and it’s information theoretic score ;  the information theoretic score of each partitioning scheme visited by each algorithm during the heuristic search; and  the time taken to complete the analysis on a desktop a mac pro with  <dig>  <dig>  ghz quad-core intel xeon processors and 32gb ram. the details of the absolute computational times are not important, but a comparison of the analysis times is informative  because it allows us to empirically compare the computational efficiency of the different methods.

analyses using the phylogenomic bird dataset
for the phylogenomic dataset from birds we first removed all sites in the alignment that were removed by the original authors  <cit> , and then defined data blocks based on each intron, and each codon position in each exon. this resulted in a total of  <dig> data blocks. we then performed a total of  <dig>  searches for partitioning schemes on this dataset, described below.

we performed  <dig> searches for optimal partitioning schemes using the greedy algorithm  <cit> : one with the aicc, and one with the bic.

we performed  <dig> searches for optimal partitioning schemes using the strict hierarchical clustering algorithm described above. the  <dig> searches comprise  <dig> searches using the bic and  <dig> using the aicc, where each search used one of  <dig> distinct clustering weights . the clustering weights are defined by a vector of four numbers that specify the relative importance of four parameter categories . analysing  <dig> sets of weights allows us to empirically compare the performance of different weighting schemes, and to determine the relative importance of the different parameter categories when searching for partitioning schemes, as well as the variation in the algorithm’s performance under different weighting schemes. the first  <dig> sets of weights comprise all possible combinations of setting at least one weight to  <dig> , and other weights to  <dig>  . these represent  <dig> of the  <dig> corners of a four dimensional hypercube, and allow us to compare the  <dig> cases where either all parameter categories are given equal weight  or where one or more parameters are given zero weight . the other  <dig> points were chosen using latin hypercube sampling in the ‘lhs’ package, version  <dig>  in r  <cit> . this procedure ensures that the sampled points are relatively evenly distributed in four-dimensional space, and is a more efficient way of sampling high-dimensional space than using a grid-based sampling scheme.

we performed  <dig>  searches for optimal partitioning schemes using the relaxed clustering algorithm described above. these  <dig>  searches comprised  <dig> searches using the aicc, and  <dig> using the bic, each of which was performed with  <dig> different clustering weights, and at  <dig> different values of the parameter p. the  <dig> weighting schemes we used were identical to those used above, and the values of the parameter p  that we used were 1%, 2%, 5%, 10%, and 20%.

the results of all  <dig> analyses presented here are available at figshare .

analyses across all datasets
based on the results of our analyses of the phylogenomic bird dataset, we set some pragmatic default values for the clustering weights and the p parameter . we then analysed the performance of the greedy algorithm, the strict hierarchical clustering algorithm, and the relaxed hierarchical clustering algorithm across all  <dig> datasets in table  <dig> using these default settings. we compared both the computational time and the performance of all three algorithms across all  <dig> published datasets. this involved a total of  <dig> analyses:  <dig> datasets,  <dig> information theoretic scores , and  <dig> algorithms . details of all of the datasets are given in table  <dig>  input files for partitionfinder, and results of these analyses are available from figshare .

RESULTS
all three algorithms we discuss in this paper start with a user-defined set of data blocks, and progressively merge data blocks to improve the information-theoretic score of the partitioning scheme. better algorithms will lead to larger improvements in the information theoretic score. we discuss algorithm performance below in two ways: in terms of the amount  that they improve the score of the partitioning scheme relative to the starting scheme which has each data block assigned to an independent subset; and in terms of the percentage improvement that an algorithm achieves relative to the existing greedy algorithm in partitionfinder. thus, a good algorithm will score highly on both counts.

strict hierarchical clustering
the strict hierarchical clustering algorithm performed substantially worse than the greedy algorithm on the phylogenomic bird dataset . this was the case regardless of the way in which subset similarity was defined, or whether partitioning schemes were selected using the aicc or the bic . the greedy algorithm improved the aicc and bic scores of the partitioning scheme by  <dig> and  <dig> units respectively. across all  <dig> different sets of clustering weights analysed, the best-scoring partitioning schemes found by the strict hierarchical clustering algorithm improved the aicc and bic scores by  <dig> and  <dig> units respectively . these improvements represent 22% and 72% of the potential improvement in aicc and bic scores estimated from the greedy algorithm.

the performance of the strict hierarchical clustering algorithm also varied substantially depending on the way in which subset similarity was defined. across all  <dig> different sets of clustering weights analysed, the worst-scoring partitioning schemes found by the strict hierarchical clustering algorithm improved the aicc and bic scores by  <dig> and  <dig> units respectively . these improvements represent 2% and 7% of the potential improvement in aicc and bic scores estimated from the greedy algorithm. the mean improvement in aicc and bic scores across all  <dig> different sets of clustering weights was 8% and 51% of the potential improvement in aicc and bic scores.

the weights used to define subset similarity have a complex relationship to the performance of the strict hierarchical clustering algorithm. figure  <dig> shows that the performance of the strict hierarchical clustering algorithm was better when the weights given to the overall rate of a subset and the alpha parameter were higher, and when the weight given to the base frequencies of a subset was lower. however, all of these relationships show substantial variation. furthermore, the set of weights that resulted in the best partitioning scheme  differed depending on whether the aicc or the bic was used to evaluate partitioning schemes, and would be very difficult to predict from first principles. one of the clearest results from this analysis is that grouping together subsets based on their base frequencies always led to worse performance for this dataset . this suggests that base frequencies can provide misleading information on subset similarity. this is likely to be most severe when subsets are small and base frequencies are estimated from limited data, which in turn will be most problematic at the start of the algorithm.

these results suggest that in most practical cases , the strict hierarchical clustering algorithm is likely to perform very poorly. although some methods of defining subset similarity performed better than others, our results suggest that there is no one method of defining subset similarity that works well for the duration of the algorithm. this is likely to be because the parameters of molecular evolution that we are able to measure  are not sufficient to determine whether clustering a given pair of subsets will result in an improvement of the aicc or bic scores. as a result, the algorithm often clusters together subsets that result in a worsening of the aicc or bic score. this problem is compounded by the fact that it is difficult to predict, either from first principles or empirical tests , the best way to define subset similarity given the parameters that we can measure.

performance of the relaxed hierarchical clustering algorithm
the relaxed hierarchical clustering algorithm performed better than the strict hierarchical clustering algorithm, and its performance approached that of the existing greedy algorithm . when 20% of all possible partitioning schemes were examined, the best-scoring partitioning schemes found by the relaxed hierarchical clustering algorithm improved the aicc and bic scores by  <dig> and  <dig> units respectively . these improvements represent 93% and 97% of the potential improvement in aicc and bic scores estimated from the greedy algorithm.

the performance of the relaxed hierarchical clustering algorithm improved as the percentage of schemes examined was increased . when 1% of all possible partitioning schemes were examined the mean improvement in aicc and bic scores was  <dig> and  <dig> units respectively. these improvements represent 46% and 68% of the potential improvement in aicc and bic scores estimated from the greedy algorithm. these improvements increased with the percentage of all possible partitioning schemes that were examined, rising to >80% when 10% of schemes were examined, and >90% when 20% of schemes were examined. in figure  <dig>  this is demonstrated by the aicc and bic scores from the relaxed clustering algorithm approaching those from the greedy algorithm as p increases. concomitant with this improvement, dependence of the relaxed hierarchical clustering algorithm on the way in which subset similarity is defined decreased as the percentage of schemes examined increased .

these results suggest that although our estimates of subset similarity are highly imperfect, they do contain information that can be used to help optimise partitioning schemes more efficiently. unlike the strict hierarchical clustering algorithm, the relaxed hierarchical clustering algorithm does not rely solely on the estimated similarity of subsets in order to decide whether to cluster them together. instead, it considers a collection of the most similar pairs of subsets and then chooses the pair that gives the largest improvement in the aicc or bic score. this approach circumvents the limitation of the strict hierarchical clustering method by reducing the reliance of the algorithm on the estimates of subset similarity.

the performance of the strict and relaxed clustering algorithms on  <dig> datasets
to ensure that the results we obtained on the phylogenomic dataset of birds were not idiosyncratic to a single dataset, we compared the strict and relaxed clustering algorithms to each other and to the greedy algorithm on a collection of  <dig> datasets . in these analyses, we defined subset similarity based solely on the overall substitution rate , based on our analyses of the phylogenomic dataset of birds , and on the results of previous phylogenomic studies that have relied on overall substitution rates to combine subsets in partitioning schemes . we fixed the proportion of partitioning schemes analysed by the relaxed clustering algorithm to 10% , based on the observation that for the phylogenomic dataset of birds this cutoff represented a good balance between computational efficiency and performance. for the same reasons, we defined default settings in partitionfinder such that subset similarity is based solely on the overall substitution rate , and the proportion of partitioning schemes analysed by the relaxed clustering algorithm is 10% . while it is possible that these parameters are idiosyncratic to the phylogenomic bird dataset, our results below suggest that they produce broadly similar results across all of the datasets we have analysed. furthermore, using a single set of parameters in the analyses of  <dig> datasets more accurately reflects the likely behaviour of the end users of these algorithms, who are unlikely to run thousands of analyses to determine the best parameters for partitioning scheme selection. thus, using a single set of parameters represents the most useful basis for comparing the three algorithms. we provide recommendations for the use of each of these algorithms, based on the results of all of our analyses, in the conclusions section at the end of this article.

the relaxed clustering algorithm found better partitioning schemes than the strict clustering algorithm on all  <dig> of the datasets we examined . for the relaxed clustering algorithm, the mean improvement in aicc and bic scores across all  <dig> datasets was 80% and 88% of the potential improvement estimated from the greedy algorithm respectively . for the strict clustering algorithm, the mean improvement in aicc and bic scores was 7% and 55% of the potential improvement estimated from the greedy algorithm .

the greedy algorithm performed best in all cases, as expected, and the aicc/bic score is shown for each run with that algorithm. the relaxed clustering algorithm typically performed almost as well as the greedy algorithm, and always performed better than the strict clustering algorithm. Δaicc or Δbic scores are shown for the clustering algorithms, and represent the difference in aicc or bic score from the greedy algorithm.

the computational efficiency of the strict and relaxed clustering algorithms on  <dig> datasets
both the relaxed clustering algorithm and the strict clustering algorithm took less computational time than the greedy algorithm, but the identity of the fastest algorithm depended on the size of the dataset . the relaxed clustering algorithm was the fastest method for 6/ <dig> datasets when using the aicc, and for 4/ <dig> datasets when using the bic . the datasets for which the relaxed clustering algorithm was faster tended to be those with smaller numbers of data blocks. across all datasets and information theoretic scores, the relaxed clustering algorithm finished in 11% of the time it took the greedy algorithm to finish, and the strict clustering algorithm finished in 9% of the time it took the greedy algorithm to finish. but for the two largest datasets that we analysed  <cit> , the relaxed clustering algorithm finished in 9% of the time it took the greedy algorithm to finish, and the strict clustering algorithm finished in 2% of the time it took the greedy algorithm to finish .

the two clustering algorithms are roughly an order of magnitude faster than the greedy algorithm. analyses were conducted on a mac pro with  <dig>  <dig> ghz quad-core intel xeon processors and 32 gb ram.

these differences in the speed of the strict and relaxed clustering algorithms result from two effects: search space and stopping conditions. the relaxed clustering algorithm analyses many more partitioning schemes than the strict clustering algorithm, which tends to make it slower. however, the relaxed clustering algorithm stops when the information theoretic score stops improving, whereas the strict clustering algorithm always computes the likelihood of n partitioning schemes for a dataset with n data blocks. the interplay of these two effects determines which algorithm will be quicker on any given dataset. although the fastest algorithm depends to some extent on the number of data blocks in the optimal partitioning scheme, a general rule of thumb is that the strict clustering algorithm will be quicker on very large datasets, but will produce poorer results.

CONCLUSIONS
partitioning is an important part of many phylogenetic analyses, and can dramatically improve the fit of models to data for almost all datasets. this is particularly true of very large datasets, which contain more genomic regions, and thus more variation in rates and patterns of molecular evolution than smaller datasets. as the analysis of very large datasets becomes more common, methods to infer partitioning schemes need to keep pace so that we can make the best possible inferences from the datasets we have.

in this study, we compared three methods for estimating partitioning schemes: an existing greedy algorithm  <cit> ; a strict hierarchical clustering method which extends the work of li et al.  <cit> ; and a relaxed hierarchical clustering method which we developed here. our results allow us to make clear recommendations for those wishing to estimate partitioning schemes.

preference should always be given to using the greedy algorithm in partitionfinder over the two algorithms developed here  <cit> . the substantial improvements made to partitionfinder for this study now permit the greedy algorithm to analyse datasets that include up to  <dig> data blocks on a desktop computer . many datasets being collected today, however, contain hundreds or thousands of loci  <cit> . in these cases, it would be computationally infeasible to use the greedy algorithm to select partitioning schemes, and where possible the relaxed hierarchical clustering algorithm should be used instead.

when using the relaxed hierarchical clustering algorithm, the percentage of schemes analysed at each step of the algorithm  should be set as high as practically possible. determining what is practical for a given dataset on a given computer may require some trial and error, but we suggest first running the analysis using the default setting of 10%. if this run finishes quickly, the percentage should be increased and the analysis re-run. if it runs too slowly, the analysis can be cancelled and re-started with a smaller percentage. subsequent runs will be much faster than the initial run, because partitionfinder saves and reloads the results of previous analyses. determining whether a given percentage of schemes analysed will produce a partitioning scheme of a similar score to the greedy algorithm may be possible by examining the results of at least three runs of the relaxed clustering algorithm using different percentages . this is because as the percentage of schemes analysed is increased, the results of the relaxed clustering algorithm will asymptotically approach those of the greedy algorithm . finally, if the percentage of schemes analysed is very low, then it may be prudent to perform more than one run with different sets of clustering weights.

the strict hierarchical clustering algorithm should be used only if an analysis using the relaxed hierarchical clustering algorithm is computationally infeasible. the strict hierarchical clustering algorithm is still likely to provide large improvements in the fit of the model to the data when compared to not attempting to optimise the partitioning scheme, but it may be sensible to try a number of different methods of defining subset similarity in order to ensure the best possible results . for example, one option would be to optimise partitioning schemes under all possible combinations of setting at least one weight to  <dig> , and other weights to  <dig> . the best-fit partitioning scheme could then be chosen from the set of  <dig> estimated partitioning schemes. for simplicity, this set of  <dig> weights can be found in the figshare repository that accompanies this paper .

in the future, it would be interesting to explore more complex partitioned models of molecular evolution. for example, our study considers only partitioning schemes in which each subset of sites has an independent model of molecular evolution from all other subsets. this decision was results from the practical consideration that this is the only partitioned model available in raxml, the primary software for analysing extremely large phylogenomic datasets. however, the most recent version of other maximum-likelihood phylogenetic software, phyml  <cit> , allows for different subsets to share any number of parameters with any number of other subsets. this hugely increases the number of possible partitioning schemes, and in particular it allows for complex models of heterotachy to be estimated. as a result, this approach is likely to allow for partitioned models that dramatically improve on those we can currently estimate using partitionfinder. however, searching among the space of these possible partitioned models, and estimating the optimal model for any given dataset, remains an unsolved problem.

availability of supporting data
the data sets supporting the results of this article are available in the figshare repository, http://dx.doi.org/ <dig> /m <dig> figshare. <dig> <cit> . this repository contains all of the datasets from table  <dig>  as well as the results of all analyses and the r script used to produce the figures in this manuscript.

all of the methods we have developed and described here of the latest version of partitionfinder are available from: http://www.robertlanfear.com/partitionfinder.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
rl and bc conceived the study. rl, bc, cm, dk, as developed, coded, and tested the methods. rl collated and analysed the empirical datasets. rl wrote the manuscript. all authors read and approved the final manuscript.

