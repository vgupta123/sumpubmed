BACKGROUND
dynamic programming is an elementary and widely used programming technique. introductory textbooks on algorithms usually contain a section devoted to dynamic programming, where simple problems like matrix chain multiplication, polygon triangulation or string comparison are commonly used for the exposition. this programming technique is mainly taught by example. once designed, all dynamic programming algorithms look similar: they are cast in terms of recurrences between table entries that store solutions to intermediate problems, from which the overall solution is constructed via a more or less sophisticated case analysis. however, the simplicity of these small programming examples is deceiving, as this style of programming provides no abstraction mechanisms, and hence it does not scale up well to more sophisticated problems.

in biological sequence analysis, dynamic programming algorithms are used on a great variety of problems, such as protein homology search, gene structure prediction, motif search, analysis of repetitive genomic elements, rna secondary structure prediction, or interpretation of data from mass spectrometry  <cit> . the higher sophistication of these problems is reflected in a large number of recurrences – sometimes filling several pages – using more complicated case distinctions, numerous tables and elaborate scoring schemes. hence, implementing a novel dynamic programming algorithm is a cumbersome task and requires extensive testing, while the resulting programs are difficult to re-use on related problems.

however, these difficulties are alleviated somewhat by a certain programming discipline: we may organize a dynamic programming algorithm such that the typical dynamic programming recurrences describe the problem decomposition and case analysis, but are completely separated from the intended optimization objective. neither the initialization values for trivial problems, nor the scoring and objective functions, nor the required number of answers, not even the data type of the result must be visible in the recurrences. in this setting, one can simply exchange one encapsulated scoring scheme by another – including ones that do not solve optimizations problems, but compute other types of useful information about the search space. the new technique proposed here is a "product" operation on scoring schemes, creating a new scheme from two given ones. this product uses a non-trivial way to combine the two objective functions. given some standard scoring schemes and the product operation, we can perform a remarkable variety of applications, such as optimizations under multiple objective functions, alternative solutions and backtracing, holistic search space analysis, ambiguity checking, and more, without additional programming effort, and without creating a need of debugging.

overview
we set the stage for our exposition with a condensed review of the "algebraic" approach to dynamic programming. we also introduce the individual scoring schemes that will be used in products later. we then introduce and discuss our definition of the product operation. from there, we proceed with a series of examples demonstrating the versatile use of products. the new product operation has been implemented and made available via the bielefeld bioinformatics server  <cit> , where the reader may run the examples presented in this paper, as well as his or her own ones. in our own, real-world programming projects, the product operation has become indispensable, and we report from our experience in the implementation of several recent bioinformatics tools.

algebraic dynamic programming by example
for our presentation, we need to give a short review of the concepts underlying the algebraic style of dynamic programming : trees, signatures, tree grammars, and evaluation algebras. we strive to avoid formalism as far as possible, and give an exemplified introduction here, sufficient for our present concerns. see  <cit>  for a complete presentation of the adp method. as a running example, we use the rna secondary structure prediction problem. we start with a simple approach resulting in an adp variant of nussinov's algorithm  <cit>  and move on to a more elaborate example to permit the demonstration of our new concepts. nothing of the new ideas presented here is specific to the rna folding problem. products can be applied to all problems within the scope of algebraic dynamic programming, including pairwise problems like sequence alignment  <cit> .

rna secondary structure prediction
while today the prediction of rna 3d structure is inaccessible to computational methods, its secondary structure, given by the set of paired bases, can be predicted quite reliably. figure  <dig> gives examples of typical elements found in rna secondary structure, called stacking regions , bulge loops, internal loops, hairpin loops and multiple loops.

the first approach to structure prediction was proposed by nussinov in  <dig> and was based on the idea of maximizing the number of base pairs  <cit> . today's algorithms are typically based on energy minimization.

adp methodology
when designing a dynamic programming algorithm in algebraic style, we need to specify four constituents:

• alphabet: how is the input sequence given?

• search space: what are the elements of the search space and how can they be represented?

• scoring: given an element of the search space, how do we score it?

• objective: given a number of scores, which are the ones we are interested in?

in the following, we will work through these steps for the rna secondary structure prediction problem.

alphabet
the input rna sequence is a string over  = {a, c, g, u}.  is called the alphabet and * denotes the set of sequences over  of arbitrary length. ε denotes the empty string. in the following, we denote the input sequence with w ∈ *.

search space
given the input sequence w, the search space is the set of all possible secondary structures the sequence w can form. in the adp terminology, the elements of the search space for a given input sequence are called candidates. our next task is to decide how to represent such candidates. two possible ways are shown in figure  <dig>  the first variant is the well-known dot-bracket notation, where pairs of matching parentheses are used to denote pairing bases. the second variant, the tree representation, is the one we use in the algebraic approach.

such a tree representation of candidates is quite commonly used in rna structure analysis, but not so in other applications of dynamic programming. to appreciate the scope of the adp method, it is important to see that such a representation exists for any application of dynamic programming .

in our example, the trees are constructed using four different node labels. each label represents a different situation, which we want to distinguish in the search space and in the eventual scoring of such a candidate. a node labeled pair represents the paring of two bases in the input sequence. the remaining nodes right, split and nil represent unpaired, branching and empty structures. it is easy to see that each tree is a suitable representation of the corresponding dot-bracket string. also note that in each of the example trees, the original sequence can be retrieved by collecting the leaves in a counter-clockwise fashion. this is what we call the yield of the tree. the yield function y maps candidate trees back onto their corresponding sequences.

the next important concept is the notion of the signature. the signature describes the interface to the scoring functions needed in our algorithm. we can derive the signature for our current example by simply interpreting each of the candidate trees' node labels as a function declaration:



the symbol ans is the abstract result domain. in the following, Σ denotes the signature, tΣ the set of trees over the signature Σ.

with the concepts of yield and signature we are now prepared to give a first definition of the search space: given an input sequence w and a signature Σ, the search space p is the subset of trees from tΣ, whose yield equals w. more formally, p = {t ∈ tΣ|y = w}.

this would suffice as a very rough description of the search space. in general, we want to impose more restrictions on it, for example, we want to make sure, that the operator pair is only used in combination with valid base pairs. for this purpose we introduce the notion of tree grammar. figure  <dig> shows grammar nussinov <dig>  origin of our two example trees. this grammar consists of only one nonterminal, s, and one production with four alternatives, one for each of the four function symbols that label the nodes. z denotes the axiom of the grammar. the symbols base and empty are terminal symbols, representing an arbitrary base and the empty sequence. the symbol basepairing is a syntactic predicate that guarantees that only valid base pairs can form a pair-node.

our refined definition of the search space is the following: given a tree grammar  over Σ and  and a sequence w ∈ *, the language described by  is  = {t|t ∈ tΣ, t can be derived from the axiom via the rules of }. the search space spawned by w is .

from the language theoretic viewpoint,  is the set of all parses of the sequence w for grammar .

the method we use for constructing the search space is called yield parsing, a solved problem that need not concern us here.

scoring
given an element of the search space as a tree t ∈ , we need to score this element. in our example we are only interested in counting base pairs, so scoring is very simple: the score of a tree is the number of pair-nodes in t. for the two candidates of figure  <dig> we obtain scores of  <dig>  and  <dig> . to implement this, we provide definitions for the functions that make up our signature Σ:



in mathematics, the interpretation of a signature by a concrete value set and functions operating thereon is called an algebra. hence, scoring schemes are algebras in adp. our first example is the algebra bpmax for maximizing the number of base pairs. the subscript bpmax attached to the function names indicates, that these definitions are interpretations of the function under this algebra. in the following, we will omit these subscripts.

the flexibility of the algebraic approach lies in the fact that we don't have to stop with definition of one algebra. simply define another algebra and get other results for the same search space. we will introduce a variety of algebras for our second, more elaborate example in section in-depth search space analysis.

objective
the tree grammar describes the search space, the algebra the scoring of solution candidates. still missing is our optimization objective. for this purpose we add an objective function h to the algebra which chooses one or more elements from a list of candidate scores. an algebra together with an objective function forms an evaluation algebra. thus algebra bpmax becomes:



a given candidate t can be evaluated in many different algebras; we use the notation ε to indicate the value obtained from t under evaluation with algebra ε.

given that yield parsing constructs the search space for a given input, all that is left to do is to evaluate the candidates in a given algebra, and make our choice via the objective function h. for example, candidates t <dig> and t <dig> of figure  <dig> are evaluated by algebra bpmax in the following way:



definition  <dig> 

• an adp problem is specified by a signature Σ over , a tree grammar over Σ, and a Σ-evaluation algebra ε with objective function h.

• an adp problem instance is posed by a string w ∈ *. its search space is the set of all its parses, .

• solving an adp problem is computing h{ε | t ∈ } in polynomial time and space with respect to |w|.

in general, bellman's principle of optimality  <cit>  must be satisfied in order to achieve polynomial efficiency.

definition  <dig>  an evaluation algebra satisfies bellman's principle, if for each k-ary function f in Σ and all answer lists z <dig> ..., zk, the objective function h satisfies

h | x <dig> ← z <dig> ..., xk ← zk]) = h | x <dig> ← h,..., xk ← h])

as well as

h = h ++ h )

where ++ denotes list concatenation, and ← denotes list membership.

bellman's principle, when satisfied, allows the following implementation: as the trees that constitute the search space are constructed by the yield parser in a bottom up fashion, rather than building them explicitly as elements of tΣ, for each function symbol f the evaluation function fε is called. thus, the yield parser computes not trees, but their evaluations. to reduce their number  the objective function may be applied at an intermediate step where a list of alternative answers has been computed. due to bellman's principle, the recursive intermediate applications of the objective function do not affect the final result.

as an example, consider the following two candidates  in the search space for sequence aucg:



since algebra bpmax satisfies bellman's principle, we can apply the objective function h at intermediate steps inside the evaluation of candidates t <dig> and t4:



given grammar and signature, the traditional dynamic programming recurrences can be derived mechanically to implement the yield parser. in the sequel, we shall use the name of a grammar as the name of the function that solves the dynamic programming problem at hand. naturally, it takes two arguments, the evaluation algebra and the input sequence.

in-depth search space analysis
note that the case analysis in the nussinov algorithm is redundant – even the sequence ' aa' is assigned the two trees right  'a' and split  , which actually denote the same structure.

in order to study also suboptimal solutions, a non-redundant algorithm was presented in  <cit> . figure  <dig> shows the grammar wuchty <dig>  here the signature has  <dig> function symbols, each one modeling a particular structure element, plus the list constructors  to collect sequences of components in a unique way. nonterminal symbol strong is used to capture structures without isolated  base pairs, as "lonely pairs" are known to be energetically unstable. purging them from the search space decreases the number of candidates considerably. this grammar, because of its non-redundancy, can also be used to study combinatorics, such as the expected number of feasible structures of a sequence of length n. this algorithm, as implemented in rnasubopt  <cit> , is widely used for structure prediction via energy minimization. the thermodynamic model is too elaborate to be presented here, and we will stick with base pair maximization as our optimization objective for the sake of this presentation. figure  <dig> shows four evaluation algebras that we will use with grammar wuchty <dig>  we illustrate their use via the following examples, where g denotes the application of grammar g and algebra a to input w. table  <dig> summarizes all results for an example sequence.

wuchty98: the enumeration algebra enum yields unevaluated terms. by convention, function symbols are capitalized in the output. since the objective function is identity, this call enumerates all candidates in the search space spawned by w. this is mainly useful in program debugging, as it allows us to inspect the search space actually traversed by our program.

wuchty98: the pretty-printing algebra pretty yields a dot-bracket string representation of the same structures as the above.

wuchty98: the base pair maximization algebra is bpmax, such that this call yields the maximal number of base pairs that a structure for w can attain. here the objective function is maximization, and it can be easily shown to satisfy bellman's principle. similarly for grammar nussinov <dig> 

wuchty98: the counting algebra count has as its objective function summation, and εcount =  <dig> for all candidates t. hence, summing over all candidate scores gives the number of candidates.

however, the evaluation functions are carefully written such that they satisfy bellman's principle. thus,  == wuchty <dig>  where the right-hand side is polynomial to compute, while the left-hand side typically is exponential due to the large number of answers returned by wuchty <dig>  technically, the result of wuchty <dig> is a singleton list, hence the .

nussinov78: this computes  the number of structures considered by the nussinov algorithm, which, in contrast to the above, is much larger than the size of the search space.

these examples show analyses achieved by individual algebras. we now turn to what can be done by their combination.

RESULTS
in this section we first introduce and discuss our definition of the product operation. from there, we proceed with a series of examples demonstrating its usage.

the product operation on evaluation algebras
we define the product operation as follows:

definition  <dig>  let m and n be evaluation algebras over Σ. their product m***n is an evaluation algebra over Σ and has the functions

fm***n...) = , fn) for each f in Σ,

and the objective function



above, ∈ denotes set membership and hence ignores duplicates. in contrast, ← denotes list membership and respects duplicates. implementing set membership may require some extra filtering effort, but when the objective function hm, which computes l, does not produce duplicates anyway, it comes for free. we illustrate the application of the product operation to algebras bpmax and count:



here, each function calculates a pair of two result values. the first is the result of algebra bpmax, the second is the result of algebra count. the interesting part is the objective function h. it receives the list of pairs as input, with each pair consisting of the candidate's scores for the first and for the second algebra. in the first step the objective function of algebra bpmax  is applied to the list of the first pair elements. the result is stored in l. in this example, l holds only one element, namely the maximum base pair score of the input list. in general, l holds many elements or may be empty. for each element of l, a new intermediate list is constructed that consists of all corresponding right pair elements of the input list. this intermediate list is then applied to the objective function of the second algebra . finally, the result of h is constructed by combination of all elements from l with their corresponding result for the second algebra stored in r. this computes the optimal number of base pairs, together with the number of candidate structures that achieve it.

one should not rely on intuition alone for understanding what m***n actually computes. for any tree grammar  and product algebra m***n, their combined meaning is well defined by definition  <dig>  and the view that a complete list of all candidates is determined first, with hm***n applied only once in the end, is very helpful for understanding. but does  actually do what it means? the implementation works correctly if and only if bellman's principle is satisfied by m***n, which is not implied when it holds for m and n individually! hence, use of product algebras is subject to the following proof obligation: prove that m***n satisfies bellman's principle .

alas, we have traded the need of writing and debugging code for a proof obligation. fortunately, there is a theorem that covers many important cases:

theorem  <dig>  for any algebras m and n, and answer list x,  is a permutation of x.

 if hm and hn minimize with respect to some order relations ≤m and ≤n, then hm***n minimizes with respect to the lexicographic ordering .

 if both m and n minimize as above and both satisfy bellman's principle, then so does m***n.

proof.  according to definition  <dig>  the elements of x are merely re-grouped according to their first component. for this to hold, it is essential that l is treated as a set.  follows directly from definition  <dig>   in the case of minimization, bellman's principle is equivalent to  monotonicity of the functions fm and fn with respect to ≤m and ≤n, and this carries over to the combined functions  and the lexicographic ordering ).     □

in the above proof, strict monotonicity is required only if we ask for multiple optimal, or for the k best solutions rather than a single, optimal one  <cit> .

theorem  <dig>  justifies our peculiar treatment of the list l as a set. it states that no elements of the search space are lost or get duplicated by the combination of two algebras. theorem  <dig>  say that *** behaves as expected in the case of optimizing evaluation algebras. this is very useful, but not too surprising. a surprising variety of applications arises when *** is used with algebras that do not do optimization, like enum, count, and pretty.

the proof obligation is met for all the applications studied below. a case where the proof fails is, for example, wuchty <dig>  which consequently delivers no meaningful result.

implementing the product operation
the algebraic style of dynamic programming can be practiced within any decent programming language. it is mainly a discipline of structuring our dynamic programming algorithms, the perfect separation of problem decomposition and scoring being the main achievement. when using the ascii notation for tree grammars proposed in  <cit> , the grammars can be compiled into executable code. otherwise, one has to derive the explicit recurrences and implement the corresponding yield parser. care must be taken to keep the implementation of the recurrences independent of the result data type, such that they can be run with different algebras, including arbitrary products.

all this given, the extra effort for using product algebras is small. it amounts to implementing the defining equations for the functions of m***n generically, i.e. for arbitrary evaluation algebras m and n over the common signature Σ. in a language which supports functions embedded in data structures, this is one line per evaluation function, witnessed by our implementation in haskell . in other languages, abstract classes  or templates  can be used. it is essential to provide a generic product implementation. otherwise, each new algebra combination must be hand-coded, which is not difficult to do, but tedious and error-prone, and hence necessitates debugging. a generic product, once tested, guarantees absence of errors for all combinations.

efficiency discussion
before we turn to the uses of ***, a word on computational efficiency seems appropriate. our approach requires to structure programs in a certain way. this induces a small  overhead in space and time. for example, we must generically return a list of results, even with analyses that return only a single optimal value. normally, each evaluation function is in o, and when h returns a single answer, asymptotic efficiency is solely determined by the tree grammar  <cit> . this asymptotic efficiency remains unaffected when we use a product algebra. each table entry now holds a pair of answers, each of size o. things change when we employ objective functions that produce multiple results, as the size of the desired output can become exponential in n, and then it dominates the overall computational effort. for example, the optimal base pair score may be associated with a large number of co-optimal candidates, especially when the grammar is ambiguous. thus, if using *** makes our programs slower , it is not because of an intrinsic effect of the product operation, but because we decide to do more costly analyses by looking deeper into the search space.

the only exception to this rule is the situation where objective function hm produces duplicate values, which must be filtered out, as described with definition  <dig>  in this case, a non-constant factor proportional to the length of answer lists is incurred.

the concrete effect of using product algebras on cpu time and space is difficult to measure, as the product algebra runs a more sophisticated analysis than either single one. for an estimation, we measure the  use of the same algebra twice. we compute wuchty <dig> and compare to wuchty <dig>  the outcome is shown in table  <dig>  for input lengths from  <dig> to  <dig> bases, the product algebra uses  <dig> % to  <dig> % more space and is  <dig> % to  <dig> % slower than the single algebra.

applications of product algebras
we now turn to applications of product algebras. table  <dig> summarizes all results of the analyses discussed in the sequel, for a fixed example rna sequence.

application 1: backtracing and co-optimal solutions
often, we want not only the optimal answer value, but also a candidate which achieves the optimum. we may ask if there are several optimal candidates. if so, we may want to see them all, maybe even including some near-optimal candidates. the traditional technique is to store a table of intermediate answers and backtrace through the optimizing decisions made  <cit> . this backtracing can become quite intricate to program if we ask for more than one candidate. we can answer these questions without additional programming efforts using products:

wuchty <dig> computes the optimal number of base pairs, together with the number of candidate structures that achieve it.

wuchty <dig> computes the optimal number of base pairs, together with all structures for w that achieve this maximum, in their representation as terms from tΣ.

wuchty <dig> does the same as the previous call, producing the string representation of structures.

wuchty <dig> w) does both of the above.

to verify all these statements, apply definition  <dig>  or visit the adp web site and run your own examples. it is a nontrivial consequence of definition  <dig> that the above product algebras in fact give all co-optimal solutions. should only a single one be desired, we would use enum or pretty with a modified objective function h that retains only one element.

note that our substitution of backtracing by a "forward" computation does not affect asymptotic runtime efficiency. with bpmax***enum, for example, the algebra stores in each table entry the optimal base pair count, together with the top node of the optimal candidate and pointers to its immediate substructures, which reside in other table entries. hence, even if there should be an exponential number of co-optimal answers, they are represented in polynomial space, because subtrees are shared. should the user decide to have them all printed, exponential effort is incurred, just as with a backtracing implementation.

application 2: holistic search space analysis
abstract shapes were recently proposed in  <cit>  as a means to obtain a holistic view of an rna molecule's folding space, avoiding the explicit enumeration of a large number of structures. bypassing all relevant mathematics, let us just say that an rna shape is an object that captures structural features, like the nesting pattern of stacking regions, but not their size. we visualize shapes by strings alike dot-bracket notation, such as _], where _ denotes an unpaired region and  denotes a complete helix of arbitrary length. this is achieved by the following shape algebra. here, function shmerge appends two strings and merges adjacent characters for unpaired regions . the function nub eliminates duplicates from its input list.



together with a creative use of ***, the shape algebra allows us to analyze the number of possible shapes, the size of their membership, and the  optimality of members, and so on. let bpmax be bpmax with an objective function that retains the best k answers .

wuchty <dig> computes all the shapes in the search space spawned by w, and the number of structures that map onto each shape.

wuchty98***shape,w) computes the best k base pair scores, together with their candidates' shapes.

wuchty98***,w)

computes base pairs and shapes as above, plus the number of structures that achieve this number of base pairs in the given shape.

wuchty <dig> computes for each shape the maximum number of base pairs among all structures of this shape.

application 3: optimization under lexicographic orderings
theorem  <dig> is useful in practice as one can test different objectives independently and then combine them in one operation. a simple case of using two orderings would be the following: assume we have a case with a large number of co-optimal solutions. let pretty' be pretty with h = min.

wuchty <dig> computes among several optimal structures the one which comes alphabetically first according to its string representation.

of course, there are more meaningful uses of a primary and a secondary optimization objective. for lack of space, we refrain from introducing another optimizing algebra here.

application 4: testing ambiguity
dynamic programming algorithms can often be written in a simpler way if we do not care whether the same solution is considered many times during the optimization. this does not affect the overall optimum. a dynamic programming algorithm is then called redundant or ambiguous. in such a case, the computation of a list of near-optimal solutions is cumbersome, as it contains duplicates whose number often has an exponential growth pattern. also, search space statistics become problematic – for example, the counting algebra speaks about the algorithm rather than the problem space, as it counts evaluated, but not necessarily distinct solutions. tree grammars with a suitable probabilistic evaluation algebra implement stochastic context free grammars   <cit> . the frequently used statistical scoring schemes, when trying to find the answer of maximal probability , are fooled by the presence of redundant solutions. in principle, it is clear how to control ambiguity  <cit> . one needs to show unambiguity of the tree grammar in the language theoretic sense , and the existence of an injective mapping from tΣ to a canonical model of the search space. however, the proofs involved are not trivial. rather, one would like to implement a check for ambiguity that is applicable for any given tree grammar, but this may be difficult or even impossible, as the problem is closely related to ambiguity of context free languages, which is known to be formally undecidable.

recently, dowell and eddy showed that ambiguity really matters in practice for scfg design, and they suggest a procedure for ambiguity testing  <cit> . this test uses a combination of viterbi and inside algorithms to check whether the  candidate returned by viterbi has an alternative derivation. a more complete test is the following, and due to the use of ***, it requires no implementation effort:

the required homomorphism from the search space to the canonical model may be coded as another evaluation algebra. in fact, if we choose the dot-bracket string representation as the canonical model, algebra pretty does exactly this. we can test for ambiguity by testing injectivity of pretty – by calling wuchty <dig> on a number of inputs w. if any count larger than  <dig> shows up in the results, we have proved ambiguity. this test is strictly stronger than the one by dowell and eddy, which detects ambiguity only if it occurs with the  "near-optimal" predictions. this and other test methods are studied in detail in  <cit> .

limitations of the product operation
the above applications demonstrate the considerable versatility of the algebra product. in particular, since a product algebra is an algebra itself, we can work with algebra triples, quadruples, and so on. all of these will be combined in the same fashion, and here we reach the limits of the product operation. the given definition of *** is not the only way needed to combine two algebras. in abstract shape analysis  <cit> , we use three algebras mfe , shape and pretty. a shape representative structure is the structure of minimal free energy within the shape. similarly to the above, wuchty <dig> w) computes the representatives of all shapes. however, computing only the k best shape representatives requires minimization within and across shapes, which neither mfe***shape nor shape***mfe can achieve. hence, a hand-coded combination of the three algebras is necessary for this particular analysis.

CONCLUSIONS
we hope to have demonstrated that the evaluation algebra product as introduced here adds a significant amount of flexibility to dynamic programming. we have shown how ten meaningful analyses with quite diverse objectives can be obtained by using different products of a few simple algebras. the techniques we have shown here pertain not just to rna folding problems, but to all dynamic programming algorithms that can be formulated in the algebraic style.

the benefits from using a particular coding discipline do not come for free. there is some learning effort required to adapt to a systematic approach and to abandon traditional coding habits. after that, the discipline pays back by making programmers more productive. yet, the pay-off is hard to quantify. we therefore conclude with a subjective summary of our experience as bioinformatics toolsmiths. after training a generation of students on the concepts presented here, we have enjoyed a boost in programming productivity. four bioinformatics tools have been developed using this approach -pknotsrg  <cit> , rnashapes  <cit> , rnahybrid  <cit>  and rnacast  <cit> . the "largest" example is the pseudoknot folding grammar, which uses  <dig> nonterminal symbols and a 140-fold case distinction. the techniques described here have helped to master such complexity in several ways:

• the abstract formulation of dynamic programming recurrences in the form of grammars makes it easy to communicate and refine algorithmic ideas.

• writing non-ambiguous grammars for optimization problems allows us to use the same algorithm for mathematical analysis of the search space.

• ambiguity checking ensures us that such analyses are correct, that probabilistic analyses are not fooled by redundant recurrences, and that near-optimal solutions when reported do not contain duplicates.

• enum algebras allow for algorithm introspection – we can obtain a protocol of all solution candidates the program has seen, a quite effective program validation aid.

• using pretty and enum algebras in products frees us from the tedious programming of backtracing routines.

• the use of the product operation allows us the create new analyses essentially with three key strokes – and a proof obligation that must be met.

this has created a good testbed for the development of new algorithmic ideas, which can immediately be put to practice.

