BACKGROUND
ad-hoc information retrieval
information retrieval on the biomedical literature indexed by medline  <cit>  is most often carried out using ad-hoc retrieval. the pubmed  <cit>  boolean search engine is the most widely used medline retrieval system. other interfaces to searching medline include relevance ranking systems such as relemed  <cit>  and systems such as ebimed  <cit>  that perform information extraction and clustering on results. certain web search engines such as google scholar  <cit>  also index much of the same literature as medline. alternatives to ordinary queries include the related articles feature of pubmed  <cit> , which returns the medline records most similar to a given record of interest, and the etblast  <cit>  search engine which ranks medline abstracts by their similarity to a given paragraph of text.

supervised learning for database curation
ad-hoc retrieval in general has proven inefficient for the task of identifying articles relevant to databases that require manual curation of entries from biomedical literature, such as the pharmacogenetics knowledgebase   <cit> , and for constructing corpora for automated text mining systems such as textpresso  <cit> . it is difficult to design an expert boolean query  that recalls most of the relevant documents without retrieving many irrelevant documents at the same time, when there are many document features that potentially indicate relevance.

the case of many relevant features is, however, effectively handled using supervised learning, in which a text classifier is inductively trained from labelled examples  <cit> . several databases have therefore used supervised learning to filter medline for relevant documents  <cit> , a recent example being the immune epitope database   <cit> . iedb researchers first used a sensitive pubmed query several pages in length to obtain a medline subset of  <dig>  records. the components of the query had previously been used by iedb curators, whose manual relevance judgements formed a "gold standard" training corpus of  <dig>  relevant and  <dig>  irrelevant documents. different classifier algorithms and document representations were evaluated under cross validation, and their performance compared using the area under the receiver operating characteristic  curve  <cit> . the best of the trained classifiers is to be applied to future results of the sensitive query to reduce the number of documents that have to be manually reviewed.

supervised learning has also been used to identify medline records relevant to the biomolecular interaction network database  <cit> , the acp journal club for evidence based medicine  <cit> , the textpresso resource  <cit> , and the database of interacting proteins   <cit> . classification may also be performed on full-text articles as in the trec  <dig> genomics track  <cit> , and cohen  <cit>  provides a general-purpose classifier for the task. most classifiers have been developed for filtering sets of a few thousand medline records, but it is possible to classify larger subsets of medline and even the whole medline database. a small number of methods have been developed for larger data sets, including an ad-hoc scoring method that has been tested on a stem cell subset of medline  <cit> , the pharmgkb curation filter  <cit> , and the pubfinder  <cit>  web application derived from the dip curation filter  <cit> . however, tasks submitted to the pubfinder site in mid- <dig> are still processing and the maintainers are unreachable. in some cases, text mining for relationships between named entities is used instead of supervised learning to judge relevance – for example in the more recent curation filter developed for the dip  <cit> . the most closely related articles  <cit>  to individual articles in a collection have also been used to update a bibliography  <cit>  or a database  <cit> .

comparison of information retrieval approaches
approaches to retrieving relevant medline records for database curation have included ad-hoc retrieval , related article search, and supervised learning. pure boolean retrieval systems like pubmed return  all documents that satisfy the logical conditions specified in the query. the vector space models used by web search engines rank documents by similarity to the query, and probabilistic retrieval models rank documents by decreasing probability of relevance to the topics in the query  <cit> . related article search retrieves documents by their similarity to a query document, which can be accomplished by using the document as a query string in a ranking ad-hoc retrieval system tuned for long queries  <cit> . overlap in citation lists has also been used as a benchmark for relatedness  <cit> . the method used in pubmed related articles  <cit>  directly evaluates the similarity between a pair of documents over all topics  using a probabilistic model. supervised learning trains a document classifier from labelled examples, framing the problem of medline retrieval as a problem of classifying documents into the categories of "relevant" and "irrelevant". classifiers may either produce ranked outputs or make hard judgements like a boolean query  <cit> . statistical classifiers, such as the naïve bayes classifier used here, use the same probability ranking principle as probabilistic ad-hoc retrieval systems  <cit> . ranked classifier results may loosely be considered to contain documents closely related to the relevant examples as a whole.

overview of mscanner
we have developed mscanner, a classifier of medline records that uses supervised learning to identify relevant records in a non-domain-specific manner. the user provides only relevant citations as training examples, with the rest of medline approximating the irrelevant examples for training purposes. most classifiers are developed for particular databases, a limitation that we address by demonstrating effectiveness in multiple domains and providing facilities to evaluate the classifier on new inputs. we make it easier to use text classification by providing a web interface and operating on all of medline instead of a medline subset. to attain the high speeds necessary for online use, we used an optimised implementation of a naïve bayes classifier, and a compact document representation derived from two feature spaces in the medline record metadata, namely the medical subject headings  and the journal of publication . the choice of the mesh feature space is informed by a previous study  <cit> , in which classification using mesh features performed well on pharmgkb citations. we describe the use of the classifier, present example cross validation results, and evaluate the classifier on a gold standard data set derived from an expert pubmed query.

RESULTS
web interface workflow
the web interface, shown in figure  <dig>  takes as input a list of pubmed ids representing the relevant training examples. in the case of a database curated from published literature, the pubmed ids can be extracted from line-of-evidence annotations in the database itself. an existing domain-specific text mining corpus or bibliography may also serve as input. the classifier is then trained, in order to calculate support scores for each distinct term in the feature space . it uses the input corpus to estimate term frequencies in relevant articles, and the remainder of medline to estimate term frequencies in irrelevant articles. the remainder of medline provides a reasonable approximation of the term frequencies in irrelevant articles, provided the frequency of relevant articles in medline is low. the trained classifier then ranks each article in medline by score  and returns articles scoring greater than  <dig>  subject to an upper limit on the number of results.

the support scores for feature occurrence, for retrieval using pg <dig>  "r" denotes |r ∩ fi = 1| and "r¯" denotes |r¯ ∩ fi = 1|, which are the number of example and rest-of-medline articles with each feature. p and p. more specialised options include the estimated fraction of relevant articles in medline , and the minimum score to classify an article as relevant. higher estimated prevalence produces more results by raising the prior probability of relevance , while higher prediction thresholds return fewer results, for greater overall precision at the cost of recall.

cross validation protocol
the web interface provides a 10-fold cross validation function. the input examples are used as the relevant corpus, and up to  <dig>  pubmed ids are selected at random from the remainder of medline to approximate an irrelevant corpus. in each round of cross validation, 90% of the data is used to estimate term frequencies, and the trained classifier is used to calculate article scores for the remaining 10%. graphs derived from the cross validated scores include article score distributions, the roc curve  <cit>  and the curve of precision as a function of recall. metrics include area under roc and average precision  <cit> .

below, we applied cross validation to training examples from three topics  and one control corpus, to illustrate different use cases. the pg <dig> corpus consists of  <dig>  pharmacogenetics articles, for the use case of curating a domain-specific database. the aidsbio corpus consists of  <dig>  articles about aids and bioethics, for the case of approximating a complex query or extending a text mining corpus. the radiology corpus consists of  <dig> articles focusing on splenic imaging, for the case of extending a personal bibliography. the control corpus consists of  <dig>  randomly selected citations, and exists to demonstrate worst-case performance when the input has the same term distribution as medline. we derived the irrelevant corpus for each topic from a single corpus, medline100k, of  <dig>  random medline records. for each topic, we create the irrelevant corpus by taking medline100k and subtracting any overlap with the relevant training examples. this differs from the web interface, which generates an independent irrelevant corpus every time it is used. a summary of the cross validation statistics for the sample topics is presented in table  <dig> 

summary of the cross validation training sets and performance metrics. prevalence is the fraction of the data that is relevant, and break-even is point where cross validation precision equals recall.

distributions of article scores
the article score distributions for relevant and irrelevant documents for each topic are shown in figure  <dig>  we have marked with a vertical line the score threshold that would result in equal precision and recall. the areas above the threshold represent the true and false positive rates, while areas below the threshold represent true and false negative rates  <cit> . the low prevalence of relevant documents in medline for a given topic of interest places stringent requirements on acceptable false positive rates when the classifier is applied to all of medline. for example, a score threshold capturing 90% of relevant articles and 1% of irrelevant articles yields only 8% precision if relevant articles occur at a rate of one in a thousand. for our sample topics, the article score distributions for aidsbio and radiology were better separated from their irrelevant corpus than for pg <dig>  as expected, the distribution of the control corpus overlapped entirely with the irrelevant articles, indicating no ability to distinguish the control articles from medline.

receiver operating characteristic
the roc curve  <cit>  for each topic is shown in figure  <dig>  we summarise the roc using the area under curve  statistic, representing the probability that a randomly selected relevant article will be ranked above a randomly selected irrelevant article. we calculated the standard error of the auc using the tabular method of hanley  <cit> . worst-case performance was obtained for the control corpus, as expected, with equal true and false positive rates and  <dig>  falling within the standard error of the auc. in the theoretical best case, all relevant articles would be retrieved before any false positives occur . the auc for pg <dig> in table  <dig>  was significantly lower than the auc for aidsbio  and radiology , which did not differ significantly. the lower auc for pg <dig>  and the poorer separation of its score distribution from medline background, may be because pharmacogenetics articles discuss the interaction of a drug and a gene , which may not always be represented in the mesh feature space.

precision under cross validation
we evaluated cross validation precision at different levels of recall in figure  <dig>  where the precision represents the fraction of articles above the prediction threshold that were relevant. to summarise the curve we evaluated precision at each point where a relevant article occurred and averaged over the relevant articles  <cit> . the averaged precisions for aidsbio, radiology and pg <dig> in table  <dig> were  <dig> ,  <dig>  and  <dig>  respectively. as an overall summary, the mean of averaged precisions   <cit>  across the three topics was  <dig> . in additional file  <dig> we provide 11-point interpolated precision curves for these topics and for the iedb tasks below, to facilitate future comparisons to our results. as expected for the control corpus, precision at all thresholds was roughly equal to the 9% prevalence of relevant articles in the data. aidsbio and radiology had comparable roc areas, but the averaged precision for radiology was much lower than for aidsbio. this is because prevalence  is much lower for radiology than aidsbio:  <dig> % vs  <dig> % in table  <dig>  for a given recall and false positive rate, precision depends non-linearly on the ratio of relevant to irrelevant documents, while roc is independent of that ratio  <cit> .

performance in a retrieval situation
to evaluate classification performance in a retrieval situation we compared the performance of mscanner to the performance of an expert pubmed query that was used to identify articles for the immune epitope database . we made use of the  <dig>  results of a sensitive expert query that had been manually split into  <dig>  relevant and  <dig>  irrelevant articles for the purpose of training the iedb classifier  <cit> . mesh terms were available for  <dig>  of the articles, of which  <dig>  were relevant and  <dig>  irrelevant. the final data set is provided in additional file  <dig>  to create training and testing corpora, we first restricted medline to the  <dig>  records completed in  <dig>  a year within the date ranges of all components of the iedb query. for relevant training examples we used the  <dig>  relevant iedb results from before  <dig>  and we approximated irrelevant training examples using the whole of  <dig> medline. we then used the trained classifier to rank the articles in  <dig> medline.

we compared precision and recall as a function of rank for mscanner and the iedb boolean query in figure  <dig>  for the task of retrieving iedb-relevant citations from  <dig> medline. the iedb query had  <dig>  results in  <dig> medline, of which  <dig>  had been judged relevant and  <dig>  irrelevant, for  <dig> % precision and 100% recall . since the iedb query results were unranked, we assumed constant precision for plotting its curves. up until about  <dig> results, mscanner recall and precision are above those of the iedb query. at  <dig>  results, mscanner's relative recall was 57% and its precision was  <dig> %. precisions after retrieving n results are as follows: p <dig> = 50%, p <dig> = 44%, p <dig> = 49%, p <dig> = 44% and p <dig> = 37%.

performance/speed trade-off
we also compared mscanner to the iedb classifier on its cross validation data, to evaluate the trade-off between performance and speed. the iedb uses a naïve bayes classifier with word features derived from a concatenation of abstract, authors, title, journal and mesh, followed by an information gain feature selection step and extraction of domain-specific features . using cross-validation to calculate scores for the collection of  <dig>  documents, the iedb classifier obtained an area under roc curve of  <dig> , with a classification speed  of  <dig>  articles per  <dig> seconds. mscanner, using whole mesh terms and issn features, obtained an area under roc of  <dig>  ±  <dig> , with a classification speed of approximately  <dig> million articles per  <dig> seconds. however, the prior we used for frequency of term occurrence  is designed for training data where the prevalence of relevant examples is low. the prevalence of  <dig>  in the iedb data is much higher than the prevalences in table  <dig>  and using the laplace prior here would improve the roc area to  <dig>  ±  <dig>  but degrade performance in cross validation against medline100k. the remaining difference in roc between mscanner and the iedb classifier reflects information from the abstract and domain-specific features not captured by the mesh feature space. all roc auc values on the iedb data are much lower than in the sample cross validation topics. this is because it is more difficult to distinguish between relevant and irrelevant articles among the closely related articles resulting from an expert query, than to distinguish relevant articles from the rest of medline.

discussion
uses of supervised learning for medline retrieval
supervised learning has already been applied to the problem of database curation and the development of text mining resources. however, using a web service like mscanner to perform supervised learning is a simple operation compared to constructing a boolean filter, gold standard training set, and custom-built classifier. mscanner may supplement existing workflows that use a pre-filter query by detecting relevant articles inadvertently excluded by the filter. another possibility is using mscanner in place of a filter query when one is unavailable, and confirming relevance by passing on the results to a stronger classifier or an information extraction method such as that used by the database of interacting proteins  <cit> . supervised learning can also be used in other scenarios where relevant training examples are readily available and the presence of many relevant features hinders ad-hoc retrieval. for example, individual researchers could leverage the documents in a personal bibliography to identify additional articles relevant to their research interests.

performance evaluation
mscanner's performance varies by topic, depending on the degree to which features are enriched or depleted in relevant articles compared to medline. the relative performance on different corpora also depends on the evaluation metric used. for example, roc performance on pg <dig> shows lower overall ability to distinguish pharmacogenetics articles from medline, but the right hand sub-plot of figure  <dig> shows higher recall at low false positive rates on pg <dig> than aidsbio. besides the topic itself, the size of the training set can also influence performance. for the complex topics curated by databases, many relevant examples may be needed to obtain good coverage of terms indicating relevance. narrower topics, such as the radiology corpus, require fewer training examples to obtain good estimates of the frequencies of important terms. too few training examples, however, will result in poor estimates of term frequencies , degrading performance. the use of a random set of medline articles as the set of irrelevant articles in training  can also influence performance in cross validation. it can inflate the false positive rate to some extent because it contains relevant articles that are not part of the relevant training set.

the score distributions for the control corpus  were somewhat anomalous, with multiple narrow modes. this is due to the larger irrelevant corpus derived from medline100k containing low-frequency features not present in the control corpus. each iteration of training therefore yielded many rare features with scores around - <dig> to - <dig>  the four narrow peaks correspond to the chance presence of  <dig>   <dig>   <dig> or  <dig> of those features, which were influential because other features scored close to zero. in non-random corpora , the other non-zero features dominate to produce broader unimodal distributions. removing features unique to medline100k reduced the control distribution to the expected single narrow peak between - <dig> and + <dig> 

document representations
we represented medline records as binary feature vectors derived from mesh terms and journal issns. these are separate feature spaces: a mesh term and issn consisting of the same string would be not be considered the same feature. medline provides each mesh term in a record as a descriptor in association with zero or more qualifiers, as in "nevirapine/administration & dosage". to reduce the dimensionality of the feature space we treat the descriptor and qualifier as separate features. we detected  <dig>  distinct mesh features in use, and  <dig>  issn features, for an average of  <dig>  features per record. the  <dig> mesh vocabulary comprises  <dig>  descriptors and  <dig> qualifiers. of the journals, about  <dig>  are monitored by pubmed and the rest are represented by a only few records each. an advantage of the mesh and issn feature spaces is that they allow a compact document representation using 16-bit features, which increases classification speed. mesh is also a controlled vocabulary, and so does not have word sense ambiguities like free text. however the vocabulary does not cover all concepts, and covers some areas of biology and medicine  more densely than others. also, not every article has all relevant mesh terms assigned, and there is a tendency for certain terms to be assigned to articles that just discuss the topic, such as articles "about dental research" rather than dental research articles themselves  <cit> .

performance can be improved by adding an additional space of binary features derived from the title and abstract of the document. not relying solely on mesh features would also enable classification of medline records that have not been assigned mesh descriptors yet. the additional features would, however, reduce classification speed due to larger document representations, introduce redundancy with the mesh feature space, and require a feature selection step. the iedb classifier  <cit>  avoids redundancy by concatenating the abstract with the mesh terms and using a single feature space of text words. binary features should model short abstracts relatively well, although performance on longer texts is known to benefit from considering multiple occurrences of terms  <cit> .

mesh annotations and journal issns are domain-specific resources in the biomedical literature. the articles cited by a given article  are another domain-specific resource that may prove useful in retrieval tasks, in addition to their uses in navigating the citation network. for example, the overlap in citation lists has been used as a benchmark for article relatedness  <cit> . in supervised learning, it may be possible to incorporate the number of co-citations between a document and relevant articles, or to use the citing of an article as a binary feature.

CONCLUSIONS
mscanner inductively learns topics of interest from example citations, with the aim of retrieving a large number of topical citations more effectively than with boolean queries. it represents an advance on previous tools for medline classification by performing well across a range of topics and input sizes, by making available implementation source code, and by operating on all of medline fast enough to use over a web interface. as a non-domain-specific classifier, it has a facility for performing cross validation to obtain roc and precision statistics on new inputs. mscanner should be useful as a filter for database curation where a sensitive filter query and customised classifier are not already available, and in general for constructing large bibliographies, text mining corpora and other domain-specific medline subsets.

