BACKGROUND
drug discovery and development is a lengthy process that begins with the identification of potential drug targets and ends after testing in clinical trials. the targets are generally identified through basic science studies as being critical components affected in a disease. once a target protein has been identified, the goal is to identify drug-like compounds that either increase or decrease its activity. high throughput screening  and high content screening  are frequently used to ascertain the effects of many compounds on a target. however, even with automation, screening a large experimental space can be expensive . one approach to reducing the need for experimentation is to generate a model for compound effects in silico, a process referred to as virtual screening. there are two common methods  <cit> . during a quantitative structure activity relationship  analysis, molecules are checked for the presence or absence of specific structural elements. the vector describing a molecule is referred to as a “fingerprint.” qsar methods have been used to make predictions about the activity of compounds on target proteins  <cit> . molecular docking is an alternative method that requires knowledge of the structure of both target and compound  <cit> . computer simulations are run in which the target and compound are forced into contact and the interaction energy between the target and compound molecule estimated. these methods take into consideration features of the target protein and potential drugs. beyond virtual screening, efforts have also been made to apply machine learning techniques to the wealth of information available in the pubchem database, paying particular attention to the gross imbalance of active to inactive compounds  <cit>  in efforts to make accurate predictions of the effects of compounds on targets.

these predictive studies consider the effects of many compounds on one  of targets in order to identify promising compounds for further development. however, it is not uncommon in drug development for previously unknown effects to be discovered after significant investment in a potential drug, resulting in relatively high attrition rates in later phases or even after drug release  <cit> . these side effects are not discovered earlier because screening is for desired effects of compounds on a single target protein without considering whether compounds have undesired effects on other targets. this suggests that early drug screening should consider a larger portion of the compound-target effect space  <cit> . ideally, we would have knowledge of the whole experimental space of compounds and targets  and columns for each compound ).

by having knowledge of all effects of all compounds, much more informed decisions could be made about which compounds to advance through the development process . however, measuring the full matrix would require on the order of  <dig> measurements, the cost of which would be prohibitive. an alternative is clearly needed.

as with single targets, predictive modeling methods for the larger space have also been described. chemogenomic approaches have been developed that concurrently consider the similarity of compounds and the similarity of ligands to make predictions for unknown associations between proteins and compounds  <cit> . furthermore, methods have been developed that allow for the identification of compounds with a desired effect profile across multiple targets by using evolutionary methods to generate compounds to be tested  <cit> . these polypharmacological methods make predictions for the effects of compounds across multiple targets. using text mining methods, clinical outcome records have also been analyzed to predict effects  <cit> . inverse docking methods have been developed as well which start with a compound and measure the interaction energy between the compound and multiple proteins  <cit> .

building any of these predictive models requires data for at least a subset of all possible experiments. this is typically all data currently available, or new results for a human-specified subset thought to be representative. in approaches like those mentioned above, machine learning methods are then used to predict results for a large set of compounds, and a small number of these are tested. in most cases, the process stops after this, and selected compounds are advanced to further development. however, the process can be made iterative, so that information from the additional experiments may be used to improve the model, make new predictions and select more experiments to execute  <cit> . this type of approach is referred to as active learning in the machine learning literature. in active learning, rather than being chosen in advance, experiments are iteratively selected to most improve the accuracy of the predictive model. in the context of drug development, this should require fewer experiments to make accurate predictions  allowing for more effective decisions and reduced late-phase attrition  <cit> . while active learning is widely used in some fields, there have been only limited applications to biological problems  <cit> .

active learning consists of three phases performed in a loop . a campaign of experiments can be initialized either using prior results from literature or databases or by randomly selecting a batch of experiments from an experimental space.  a model is generated to represent the currently available data.  from that model, experiments are selected for execution that are expected to improve the model.  the set of experiments is executed and the resulting data are combined with previously collected experimental data. the loop then continues from step  <dig> until either a desired accuracy of predictions is achieved or a specified budget has been exhausted. there have been limited previous applications of active learning to the drug discovery process. in these efforts, compound activity was considered to be binary  and effort was focused on only a single target  <cit> .

the most important difference of the work described here from previous approaches is our emphasis on active machine learning to simultaneously model the effects of many compounds on many targets. to demonstrate the utility of active learning for drug discovery in the context of multi-target modeling, we combined two modeling approaches to make predictions about activities for large numbers of combinations of compounds and targets. our model uses features developed for virtual screening to describe compounds, and features from sequence analysis to describe target proteins. as a part of this effort, we did not endeavor to make the most accurate predictive model possible. rather, we investigated the utility of applying active learning in combination with predictive models in order to efficiently discover active compound-target pairs. in tests using data from the pubchem database, we found that active compound-target pairs could be discovered as much as twenty-four times faster using active learning than by random selection of experiments. the algorithms we describe are also computationally efficient, making application to very large experimental spaces practical.

RESULTS
dataset
to evaluate our proposed approaches, we chose to use existing experimental results for assays on many targets and many compounds. we therefore began by assembling a large set of compound effect scores from pubchem . in total, compound activity scores for  <dig> assays were assembled. of these assays,  <dig> were from in vitro assays and  <dig> were from in vivo assays. of the  <dig>  compounds in pubchem across the  <dig> assays, an average of 30% had a reported activity score for a given assay.  of these, we created a dataset of all assay data for  <dig>  randomly-chosen compounds, resulting in a system with  <dig>  million possible experiments . all combinations of target and compound with scores above  <dig> or below − <dig> were marked as hits.  information on the assays, compounds and their respective features, can be found in the additional files  <dig> and  <dig> 

model definition
as an initial approach to constructing a predictive model, we explored using linear combinations of features. given the large numbers of features involved, lasso regression  <cit>  was used because it allows for efficient feature selection for linear regression models. we note that while the assay scores may be non-linearly related to true activity, and while estimates of true activity may be obtained by further manipulation or testing, we expect them to be good approximate predictors of which combinations of compounds and targets will show high activity.

three approaches to prediction of the assay scores were used. the first approach used compound features only  to predict the activity of each compound in a given assay . using lasso regression, compound features were selected that were strongly indicative of the activity of a compound on a single target. a regression model was learned for each individual target allowing for the selection of compound features unique to a target . the second approach used protein features only  to predict the effect on each target of a given compound. when considering all experiments which involved a single compound, lasso regression allowed us to select features of the target protein which were indicative of the likelihood for a target to be affected by that single compound . the third approach made a combined compound-target  prediction by averaging the two predictions for each compound-target combination .

evaluating model performance
we first sought to determine how accurately these models could predict target-compound hits as a function of how much training data was available. to do this, we randomly sampled a sequence of experiments in batches of  <dig> experiments until 3% of the experimental space had been sampled . as each experiment was sampled, we combined it with all previous experiments from that sequence to train a model and evaluated its ability to predict hits for all remaining data.

a receiver-operator characteristic  curve was calculated for each of these models by varying the classification threshold to predict a hit . finally, the area under the roc curve was calculated for each set of predictions. this process was repeated ten times for each of the three prediction approaches described above . the means and standard errors of the area under the roc curve for the ten trials for each prediction approach are shown in figure  <dig>  two methods can be considered to generate random predictions for comparison to these results. the simplest would be to randomly choose predictions from the set of all scores for all assays. because these are globally random, the area under the roc curve is clearly expected to be  <dig> . all of our methods perform better than this. a more demanding baseline was therefore used, in which scores were randomly chosen from those for all compounds for a given target. the predictions from this sort of random predictor are expected to be more accurate than randomizing across all observations , thus it is a more stringent standard for comparison. predictions using cfo or using cct performed better than random by this standard . this is despite the fact that less than  <dig> % of the combinations were active according to our definition.

we also considered which features were more informative than others. to make a single set of predictions across the entire space of  <dig>  compounds and  <dig> targets requires the training of  <dig>  lasso regression models. the final models trained at 3% of the experimental space  were analyzed and the proportion of models where the coefficient for each feature was non-zero was calculated. to determine the magnitude of the effect of a feature on prediction, the mean absolute coefficient for each feature  was calculated. for targets, the most frequently selected features  were the amino acid compositions. for compounds, the most frequently selected feature was “group iia ” and the feature with the largest absolute coefficient was “4 m ring”. further details on other features are provided in the additional file  <dig> 

we also were interested in how applicable a trained model would be to a new target or a new compound. we utilized the same random sampling approach described above. however, for each of the ten trials, the experimental results were held out for a unique 10% of all targets or compounds and a roc curve was calculated for only the held out experiments. the result of this process is that when targets are held out entirely, only pfo models can be used and likewise when compounds are held out entirely, only cfo models can be used. the results  show that when holding out entire compounds, relatively accurate predictions can be made about activities from the remainder. as expected from the results in figure  <dig>  the predicted activities for held out targets are much less accurate. both, however, perform better than random prediction . the results confirm that the regression approach can capture important information about compound effects, even when no information about a compound is provided during training. the fact that scores could be predicted better for new compounds than for new targets may be due to the fact that data was available for many more compounds than targets .

active learning simulation
given that our modeling approach performed better than random at predicting relative activity scores, we next determined whether it could be used to successfully drive an active learning process . for this, simulations were run for an experimental space of all  <dig> assays  and all  <dig>  compounds. for this experimental space, rank scores from actual experiments executed were available in pubchem for  <dig> , <dig> experiments out of  <dig> , <dig> possible experiments. experiments selected during simulations were restricted to those for which results were available; requests from an active learner for other experiments were skipped.

to initialize a simulation, all experimental results were hidden from the active learner as if they had never been executed. a set of  <dig> experiments were selected randomly for “execution.” during the execution phase , results from selected experiments were “revealed” and used for training of a predictive model . a new batch of experiments was then selected using one of a number of active learning methods . finally, the data for the selected experiments were added to the pool of previously selected data and the loop continued until 3% of the possible experimental space was explored. each round consisted of the selection of  <dig> experiments. ten separate simulations were run for each experiment selection method, each starting out with a different set of initial experiments. at each round, the discoveries  were counted, and the mean count and associated standard error recorded as a function of the fraction of experimental space so far explored.

we first considered a greedy active learning approach in which unobserved experiments that had the greatest predicted effect  were selected for measurement in the next round. this greedy approach was used in combination with cct, single regression with predictions from compound features for each protein target  and single regression with predictions from protein target features for each compound . for comparison, a random selection method was also included. as shown in figure  <dig>  the greedy cct method performed better than the other methods using less sophisticated predictive models. after exploration of 3% of the experimental space, an average of approximately 38% of possible discoveries were made. results for the single regression approaches are also shown. as might be expected from the results in figure  <dig>  results for prediction from target features only are nearly the same as for random selection. results using cfo are much better, but cct performs even better. this may be considered surprising given that cfo performed better than cct when measuring the accuracy of predictions based on the area under the roc curve in figure  <dig>  however, the primary reason is that at high thresholds, predictions using cct had a higher true positive rate than those of cfo giving the results in figure  <dig>  for the entire set of predictions across many thresholds, cfo predictions performed better as shown in figure  <dig> 

the rate of discovery for the greedy method using cct decreased as the simulations progressed. exploration of the experimental space with the greedy algorithm was limited to regions of the feature space which were predicted to have large activities. we considered the possibility that this limited the system’s ability to learn a better model, and that this could be overcome by acquiring data in regions where few observations have been made or where the model predictions were uncertain. therefore, a “density-based” approach was also tested which selected experiments so as to explore the experimental space efficiently without regard to predicted values or experimental results. in this approach experiments were tested which were most similar to the unobserved experiments and least similar to observed experiments  <cit> . a variation on this idea, diversity sampling, was also tested, along with uncertainty sampling in which experiments with the highest uncertainty of their prediction were selected. results for these approaches are shown in additional file  <dig>  the uncertainty-based selection method performed much better than random but not as well as cct with greedy sampling. density-based and diversity-based sampling performed similarly to random selection. these three classical active learning methods are generally designed to select experiments for execution which will yield the most accurate model, while the results in additional file  <dig> are for finding hits. we therefore considered the accuracies of the models for each method by calculating the area under the roc curve . as shown in additional file  <dig>  all selection methods, except for uncertainty sampling, resulted in an initial peak accuracy followed by a slight, gradual reduction in the accuracy of the models. the better performance of uncertainty sampling compared to cct with greedy sampling is consistent with the opposite result in additional file  <dig>  this is because uncertainty sampling does not prefer finding hits over non-hits.

because uncertainty, diversity and density-based selection methods were designed to select experiments which would yield a more accurate predictive model, we also tested hybrids of greedy cct with each of these methods. these hybrid methods were designed to concurrently improve the predictive model and confirm predictions generated by the increasingly accurate predictive model. the hybrids with density and diversity performed worse than greedy cct by itself  but the hybrid with uncertainty sampling performed slightly better .

we also considered the possibility that the decrease in rate of learning for greedy cct was due to excessive testing of a given target for new discoveries after all of them have already been revealed. to address this possibility, we developed a modified approach  in which only information from a given number of previous rounds was used in the model generation and active learning process. any requests from the active learner for experiments previously selected and subsequently hidden were skipped. as shown in figure  <dig>  limiting memory to only the previous  <dig> or  <dig> rounds yields significant improvement in the discovery rate. almost 60% of discoveries were made after only 3% of the experimental space was explored. we also found that limiting memory in the context of hybrid uncertainty methods also improved the quality of the predictive model as measured by the area under the roc curve in figure  <dig> 

for reasons of computational time, we restricted our analysis to  <dig>  compounds. it was therefore of interest to estimate how performance might change if more compounds were included. as a preliminary indication of this, we performed simulations for smaller sets of compounds. the results  show that the learning rate is significantly worse for  <dig>  compounds than for  <dig> , but that it is not much different for  <dig>  than  <dig> . this suggests performance for larger sets might be as good or better.

discussion and 
CONCLUSIONS
we have described a pipeline for executing experiments driven by an active learning system and demonstrated that it can result in the rapid discovery of compounds which affect target proteins using a set of heterogeneous assays. we found that the selection of experiments based only on predictions calculated using compound features  performed significantly better than the selection of experiments based only on predictions from target features . decent performance of the prediction models using compound features is to be expected given past results with qsar approaches to modeling compound activity on a given target. the comparatively poor performance of the protein models could be a result of multiple issues: poor features, limited data, and heterogeneous data sources. the system included only features that could be calculated from sequence information, and it is likely that this feature set could be improved by the inclusion of features calculated from protein structural information. some assays utilized in this study included high content screening assays in which living cells were imaged to measure the effects of compounds. these types of experiments are inherently more complex than simple binding assays and may have been poorly represented by features only describing a single target protein within the complex system. both types of models performed better than random prediction and combining them yielded accurate models that could be utilized to rapidly make new discoveries. previously, ensembles of predictors have yielded good results and the performance of these combined models may be caused by the same effect. importantly, the addition of memory limitations to these models further improves the discovery rate. in this experiment, only information from  <dig> assays was used. as information from more assays becomes available, predictive models are expected to improve.

there are at least five factors to be considered in applying active learning approaches to problems such as compound screening. first, whether to use a priori measures of similarity between compounds or targets must be decided. the advantage of using them is that predictions can be made even before any data are acquired, but the disadvantage is that they may be biased towards previously explored compounds or targets. in separate work, we have described approaches for using modeling and active learning without such features  <cit> . second, the method for choosing experiments to perform should reflect the goals of the campaign. as we have illustrated here, uncertainty sampling can be used to learn an accurate predictive model very efficiently. however, when the goal is not to learn an accurate predictive model of the whole space, but rather something such as just finding hits, we have also illustrated how hybrid experimentation selection methods can prove very beneficial. with hybrid methods, a portion of the experiments are chosen so as to learn an accurate predictive model and the remainder of the experiments are chosen to take advantage of the improved predictive model to accomplish the desired goal. further, we have shown that limiting the memory of the active learning system can result in further improvements in efficiency by avoiding exploration of areas of the experimental space in which most relevant information has already been discovered. third, computational complexity is an important consideration in practical use of active learning methods. methods that model the entire space at once are theoretically preferable  <cit> , but they can require prohibitively extensive computation for problems with thousands of targets and millions of compounds. in such cases, the methods we have described here can provide a faster alternative. fourth, the logistics of the types of experimentation to be undertaken need to be considered. for example, in this study with the batch size we chose, 3% of the experimental space would have required approximately  <dig> rounds of experimentation. for some types of experimentation, a large number of small rounds may not be practical and thus larger batch sizes could be used in fewer rounds.

finally, the primary goal of the active learning process is to reduce the experimentation required to complete an objective. in order for those reductions to be realized one needs to determine when to stop running experiments. this is an ongoing area of study, but progress has been made in our prior work  <cit> . in the current study, we observed that the discovery was high as the first  <dig> % of the experimental space was explored and then decreased . to explore whether the learning would continue or would plateau, we continued the simulations past 3% for the best method . the learning rate continued at a rate about 2– <dig> times as fast as for random sampling and did not reach a plateau . extrapolating the learning rate predicts that it would find all hits after sampling approximately 20% of the experimental space.

it is worth noting that while simultaneous consideration of multiple targets and multiple compounds may increase the number of experiments needed to find a compound that affects a single target, it may be expected to decrease the average number of experiments per target when used to simultaneously conduct campaigns for multiple targets.

the selection of an appropriate batch size is an important consideration for the utilization of an active learning system. if there is a significant setup cost for a set of experiments , then larger batches are preferable. if on the other hand, setup costs are low and a short time is required to execute the experiments relative to computational time to update the model, a smaller batch size would be preferable.

in conclusion, the work presented here provides a practical, scalable approach to the specific problem of learning a combined model for the effects of many compounds on many targets and demonstrates that the model can be combined with active machine learning methods to dramatically reduce the number of experiments needed to find compounds with desired target effects. many variations on the approaches described here can be considered, including different predictive models, different feature sets and different active learning algorithms . an exhaustive evaluation of these variations is beyond the scope of this paper, but we have firmly established that significant improvement in learning rates can be achieved. we believe active learning will be particularly important as drug development efforts increasingly consider variation among cell types and among individuals. the size of this experimental space clearly precludes exhaustive experimentation. the paradigm of exploring combinatorial experimental spaces through active learning is also widely applicable in biomedical research beyond drug discovery. this includes any study that seeks to determine the effects of large numbers of perturbations  on large numbers of molecular, cellular or histological behaviors . as the size of the experimental space grows, exhaustive experimentation becomes more impractical and active learning may be expected to provide even greater benefit.

