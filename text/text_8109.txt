BACKGROUND
biological systems are mainly composed of genes that encode the molecular machines that execute the functions of life and networks of regulatory interactions specifying how genes are expressed, with both operating on multiple, hierarchical levels of organization  <cit> . systems biology aims at understanding how such systems are organized by combining experimental data with mathematical modeling and computer-aided analysis techniques  <cit> .

the modeling and simulation of biochemical networks  has recently received a great deal of attention  <cit> . the modeling framework selected depends both on the properties of the studied system and the modeling goals. lauffenburger et al.  <cit>  organized the models in terms of three main groups, depending on their level of detail: deterministic, probabilistic and statistical.

currently, the most typical approach to representing biochemical networks is through a set of coupled deterministic ordinary differential equations intended to describe the network and the production and consumption rates for the individual species involved in the network  <cit> . the conceptual framework selected for the construction of rate equations enables models to be further classified as generalized mass-action-based models and power-law models  <cit> .

unfortunately, with model details come parameters, and most parameters are generally unknown, thereby hampering the possibility for obtaining quantitative predictions. modern experimental techniques, such as time-resolved fluorescence spectroscopy or mass-spectrometry-based techniques, can be used to obtain time-series data for the biological system under consideration. the goal of model identification is then to estimate the non-measurable parameters so as to reproduce, insofar as is possible, the experimental data. although apparently simple, non-linear model identification is usually a very challenging task, due to the usual lack of identifiability, either practical or, in the worst case, structural. in fact, several authors have reported difficulties in assessing unique and meaningful values for the parameters from given sets of experimental data since broad ranges of parameter values result in similar model predictions .

this problem has motivated the development of iterative procedures for model identification, such as those proposed by feng and rabitz  <cit> , who, using a closed-loop strategy, attempted to estimate how to stimulate and how to observe a system for identification purposes. kremling et al.  <cit>  and gadkar et al.  <cit>  suggested alternative identification procedures that involve some type of experimental design, to either calculate stimuli profiles or to select species whose concentration measurements would maximally benefit model calibration and/or model discrimination.

it is important to note, however, that, in most cases, only a limited number of components in the network can be measured, usually far fewer components than incorporated in the model; only specific stimuli are available, and the system may only be stimulated in very specific ways ; the number of sampling times is usually rather limited, and finally, the experimental data are subject to substantial experimental noise. these constraints, together with the dynamic and typically non-linear character of the models under consideration result in identifiability problems, i.e. in the impossibility of providing a unique solution for the parameters.

our research describes a novel general iterative identification procedure, extending the one originally outlined in balsa-canto et al.  <cit> , that addresses model identification under these typical constraints and which aims to reduce the effects of the lack of identifiability.

with this aim in mind, the iterative identification procedure presented here involves the following steps:

• analysis of structural identifiability. this step, which is often disregarded, evaluates whether the parameters may be assigned unique values from a given pair model and observables, under ideal experimental conditions, and assesses - when this is possible - the reformulation of a given model or the implementation of an iterative procedure for model calibration.

• global ranking of parameters. this step helps decide which parameters are the most relevant to model output. in the case of lack of structural identifiability, global ranking may be used to make decisions as to reformulate the model or which parameters to estimate.

• model calibration using global optimization methods. the model calibration problem can be formulated as a non-linear optimization problem. unfortunately, since it is usually the case that several sub-optimal solutions are possible, the use of global optimization methods is necessary to somehow guarantee that the best possible solution is located.

• practical identifiability analysis. complementary to the structural identifiability test, the practical identifiability analysis enables an evaluation of the possibility of assigning unique values to the parameters from a given set of experimental data or experimental scheme, subject to experimental noise. in this paper we distinguish between two types of practical identifiability analyses: firstly, the expected quality of a given experimental scheme is analyzed a priori using what we call the expected uncertainty of the parameters; and secondly, the quality of the parameter estimates for a given set of experimental data using robust confidence intervals is analyzed a posteriori.

• optimal experimental design via dynamic optimization. the purpose of this step is to design dynamic experiments with the aim of maximizing data quality and quantity  for the purpose of model calibration.

to illustrate the difficulties that may be faced when identifying a nonlinear dynamic biological model and the performance of the proposed identification procedure we consider the mathematical model that describes the nf-κb regulatory module proposed by lipniacki et al.  <cit> .

methods
model building
a mathematical model has three important functions: first, it helps to better understand the biological phenomenon studied; secondly, it enables experiments to be specifically designed to make predictions of certain characteristics of the biological system that can then be experimentally verified; and finally, it summarizes the current body of knowledge in a format that can be easily communicated. devising such a model involves a number of steps , commencing with a definition of its purpose and finishing with a preliminary working model.

the purpose of the model will condition the selection of the modeling framework and the information that should be included in the model. only elements that might have an impact on the questions to be addressed by the model should be included. in this regard, account should be taken of the fact that reaction models can only include a small subset of all reactions taking place within a cell. thus, assumptions must be made about the extent to which the species included in the model evolve independently of the species excluded from the model, and also about the species that are crucial for the purpose of the model. at this stage it is possible to define the network architecture and decide which type of modeling framework may be the most appropriate 

in the next step, an initial mathematical model structure  is proposed. new experimental information must then be used to verify hypotheses, and to discriminate, if possible, among different model alternatives. the candidates will often depend on a number of unknown non-measurable parameters that can be computed by means of experimental data fitting .

this crucial step provides the mathematical structure with the capacity to reproduce a given data set, make predictions and discriminate among different model candidates.

the last step is validation, which essentially means reconciling model predictions with any new data observed. this process is likely to reveal defects, in which case a new model structure and/or new  experiment is planned and implemented. this process is repeated iteratively until validation is considered to be complete and satisfactory.

note that the success of this model-building loop relies on being able to perform experiments under a sufficient number of conditions to extract a rich ensemble of dynamic responses, to accurately measure such responses and to iterate in order to improve the predictive capabilities of the model without a significant cost.

since model identification is a task that consumes large amounts of experimental data, an iterative identification procedure is proposed which is intended to accurately compute model unknowns while reducing experimental cost.

optimal identification procedure
the proposed iterative identification procedure is depicted in figure  <dig> 

if there are several model candidates two extra steps should be included in the loop, one to analyze structural distinguishability among candidates and the other to design experiments for model discrimination  <cit> .

mathematical model formulation
we will assume a biological system described by the vector of state variables x ∈ x ⊂ , which is the unique solution of the set of nonlinear ordinary differential equations:   

where  corresponds to the external factors and θ ∈ Θ ⊂  is the vector of model parameters where Θ is the feasible parameter space.

moreover, given an experimental scheme, with ne experiments,  observables per experiment e and  sampling times per experiment e and observable o, ye, o ∈ y ⊂  will regard the vector of  discrete time measurements, as follows:   

where  regards the sth sampling time for observable o in experiment e. thus every experimental  data will be denoted as  and similarly, the corresponding model predictions will be denoted as .

structural identifiability analysis
once the structure of the state-space representation, eqns. -, has been established, the structural identifiability problem is concerned with the possibility of calculating a unique solution for the parameters while assuming perfect data . structural identifiability is thus related to the model structure and possibly to the type of stimulation and independent of the parameter values.

there are, at least, two obvious reasons to asses structural identifiability: first, the model parameters have a biological meaning, and we are interested in knowing whether it is at all possible to determine their values from experimental data; second, is related with the problems that a numerical optimization approach may find when trying to solve an unidentifiable model.

there are a few methods for testing the structural identifiability of nonlinear models  <cit> : the similarity transformation approach  <cit> , differential algebra methods  <cit>  and power series approaches  <cit> . unfortunately there is no method amenable to every model, thus at some point we have to face the selection of one of the possibilities. all of them present limitations related to the non-linearity and the size of the system under consideration, meaning by size the number of state variables, the number of parameters and the number of observables. probably the most easy to apply, provided one uses a symbolic manipulation software, are the power series expansions methods. in this regard two possibilities have been developed: the taylor series and the generating series.

details of the taylor series approach can be found in  <cit> . the approach is based on the fact that observations are unique analytic functions of time and so all their derivatives with respect to time should also be unique. it is thus possible to represent the observables by the corresponding maclaurin series expansion and it is the uniqueness of this representation that will guarantee the structural identifiability of the system. the idea is to establish a system of non-linear algebraic equations on the parameters, based on the calculation of the taylor series coefficients, and to check whether the system has a unique solution. the generating series approach  <cit>  allows to extend the analysis to the entire class of bounded and measurable stimuli. in this case the series is generated with respect to the stimuli domain. the method requires the model to be linear in the stimuli as follows:   

the observables can be expanded in series with respect to time and stimuli in such a way that the coefficients of this series are g and the lie derivatives:   

where lfg is the lie derivative of g along the vector field f, given by:   

with fj the jth component of f.

if s regards the vector of all the coefficients of the series, a sufficient condition for the model to be identifiable is that there exists a unique solution for θ from s, similarly to the taylor series method. note also that power series approaches assume that all the information on the progress of the observables is contained in the germ, i.e. the infinite set of power series coefficients evaluated at t = 0+. if the derivatives are zero, then the germ is said not to be informative and no conclusions about identifiability can be directly drawn. later observations  could give more information and restrict the set of feasible values of θ. probably the major drawback of the power series approaches is that the necessary number of power series coefficients is usually unknown. for the taylor series approach an upper limit has been shown for bilinear and polynomial systems  <cit> . additionally margaria et al.   <cit>  showed that for the combination of the taylor series and the differential algebra approaches, nx +  <dig> derivatives are sufficient for the case of rational systems with one observable. however there are not bounds for a general non-linear system. in addition, solving the non-linear system of equations resulting from the power series approaches is usually not a trivial task, particularly when the number of parameters is large and the number of observables is reduced. we therefore propose using the following identifiability tableaus to easily visualize the possible structural identifiability problems.

the tableau represents the non-zero elements of the jacobian of the series coefficients with respect to the parameters. it consists of a table with as many columns as parameters and with as many rows as non-zero series coefficients, in principle, infinite, as shown in figure  <dig> 

if the jacobian is rank deficient, i.e. the tableau presents empty columns, the corresponding parameters may be unidentifiable. note that since the number of series coefficients may be infinite, unidentiability may not be fully guaranteed unless higher order series coefficients are demonstrated to be zero.

if the rank of the jacobian coincides with the number of parameters, then it will be possible to, at least, locally identify the parameters. in this situation a careful inspection of the tableau will help to decide on an iterative procedure for solving the system of equations, as follows:

• the number of non-zero coefficients is usually much larger than the number of parameters. in practice this means that we should select the first nθ rows that guarantee the jacobian rank condition. the tableau helps to easily detect the necessary coefficients and to generate a "minimum" tableau.

• a unique non-zero element in a given row of the minimum tableau means that the corresponding parameter is structurally identifiable. if any, the parameters in this situation can be computed as functions of the power series coefficients and can be then eliminated from the "minimum" tableau to generate a "reduced" tableau. subsequent reductions may lead to the appearance of new unique non-zero elements and so on. thus all possible "reduced" tableaus should be built first.

• once no more reductions are possible, one should try to solve the remaining equations. since it is often the case that not all remaining power series coefficients depend on all parameters, the tableau will help to decide on how to select the equations to solve for particular parameters.

• if several meaningful solutions exist for a given set of parameters, then the model is said to be locally identifiable.

if the model turns out not to be completely identifiable, identifiability may be recovered by extending the set of observables, however this may not be accessible in practice. alternatively one may consider fixing some parameters  <cit>  or to reformulate the model.

global ranking of parameters
observables will depend differently on different parameters and this may be used to rank the parameters in order of their relative influence on model predictions. such influence may be quantified by the use of parametric sensitivities.

local parametric sensitivities for a given experiment e, observable o and at a sampling time  are defined as follows:   

they may be numerically computed by using the direct decoupled method within a backward differentiation formulae  based approach, as implemented in e.g. odessa  <cit> .

the corresponding relative sensitivities, , can be used to asses the individual local parameter influence or importance, that is to establish a ranking of parameters. brun and reichert   <cit>  suggested several importance factors, that may be generalized for the case of having several observables and experiments  <cit> .

of course, the values of the parameters are not known a priori, and even when optimally computed, optimal values are subject to uncertainty depending on the type of experiments and the presence of experimental noise. consequently, the ranking for a given value of the parameters may be of limited value. alternatively, one may compute ranking for a sufficiently large number of parameter vectors in the feasible parameter space.

the simplest approach is to apply a monte carlo sampling. by sampling repeatedly from the assumed joint-probability density function of the parameters and by evaluating the sensitivities for each sample, the distribution of sensitivity values, along with the mean and other characteristics, can be estimated. this approach yields reasonable results if the number of samples is quite large, requiring a great computational effort.

an alternative that can yield more precise estimates is latin hypercube sampling . this method selects nlhs different values for each of the parameters, which it does by dividing the range of each parameter into nlhs non-overlapping intervals on the basis of equal probability. next, from each interval one value for the parameters is selected at random with respect to the probability density in the interval.

the nlhs values thus obtained for the first parameter are then paired in a random manner  with the nlhs values for the second and successive parameters. this method allows the overall parameter space to be explored without requiring an excessively large number of samples. the importance factors will then read:   

where nd = nlhsnenons, δmsqr and δmabs quantify how sensitive a model is to a given parameter considering δmabs interactions between parameters. δmax and δmin indicate the presence of outliers and provide information about the sign. δmean provides information about the sign of the averaged effect a change in a parameter has on the model output.

ordering the parameters according to these criteria, preferably in decreasing order, results in a parameter importance ranking. this information may be useful to decide on reformulating the model or to fix the less relevant parameters to improve either structural or practical identifiability.

note that the summations will, in general, hide the different effects from the different experiments and observables unless they are in the same order of magnitude. similar analyses may be performed for experiments and observables, thus providing information on the parameters that are more relevant to a particular observable in a particular type of experiment.

model calibration
given the measurements, the aim of model calibration or parameter identification is to estimate some or all of the parameters θ in order to minimize the distance among data and model predictions. the maximum-likelihood principle yields an appropriate cost function to quantify such distance, which, for the case of gaussian noise with known or constant variance, reads as the widely used weighted least-squares function:   

where  collects the information related to a given measurement experimental noise.

parameter identification is then formulated as a non-linear optimization problem, where the decision variables are the parameters and the objective is to minimize j subject to the system dynamics in eqns. - and also, possibly, to some algebraic constraints that define the feasible region Θ.

this problem has recently received a great deal of attention in the literature. jaqaman and danuser presented a guide for model calibration in the context of biological systems  <cit>  noting that there are two major issues in minimizing 13: first, the presence of local minima and second, the lack of practical identifiability.

to deal with first difficulty several authors have proposed the use of global optimization methods  <cit> , since most of the model calibration problems related to biological models have several sub-optimal solutions. recently suggested, in addition, was the use of sequential hybrid global-local methods  <cit>  to enhance efficiency, particularly for highly multimodal and large scale systems.

practical identifiability analysis
as already mentioned in the introduction, practical identifiability analysis enables an evaluation of the possibility of assigning unique values to parameters from a given set of experimental data or experimental scheme subject to experimental noise. we distinguish between practical identifiability a priori, which anticipates the quality of the selected experimental scheme in terms of what we will call the expected uncertainty of the parameters, and practical identifiability a posteriori, which assesses the quality of the parameter estimates after model calibration in terms of the confidence region.

it is important to note that the major difference between the two analyses is that, a priori, we have to assume a maximum experimental error, whereas, a posteriori, since the experimental data are already available, the experimental error may be estimated either through experimental data manipulation  or after model calibration using the residuals   <cit> .

possibly the simplest approach to perform such analyses given a set of simulated  or real  experimental data is to draw contours of the cost j by pairs of parameters. this will help detect typical practical identifiability problems, such as strong correlation between parameters, the lack of identifiability for some parameters when the contours extend to infinity, or the presence of sub-optimal solutions.

to quantify the expected uncertainty of the parameters and/or the confidence region, we rely on a monte carlo-based sampling method  <cit> . the underlying idea is to simulate the possibility of performing hundreds of replicates of the same experimental scheme for a given experimental error. the model calibration problem is solved for each replicate and the cloud of solutions is recorded in a matrix. note that, in order to avoid convergence to local solutions, an efficient global optimization method is required. the cloud of solutions is assumed to correspond to, or to be fully contained in, a hyper-ellipsoid. principal component analysis applied to the  <dig>  -  <dig>  interquartile range of the cloud or matrix of solutions then provides information on hyper-ellipsoid eccentricity  and pseudo-volume . the analysis of the histograms of the parameter solutions provides the mean value of the parameters  and either maximum expected uncertainty  or the confidence intervals  for the parameters . see details in  <cit> .

the obtained expected uncertainty of the parameters will allow the different experimental designs to be compared a priori, i.e. without performing any experiment. the richest experiment, in terms of the quantity and quality of information, will be the one with the best compromise between pseudo-volume and eccentricity.

the confidence intervals obtained for the parameters will enable a decision to be made on the need to perform further experiments to improve the quality of the parameter estimates and, thus, the predictive capabilities of the model.

optimal experimental design
a crucial aspect of experimental data is data quantity and quality. as mentioned in the previous section, a given set of data may result in practical identifiability problems. this is why data generation and modeling have to be implemented as parallel and interactive processes, thereby avoiding the generation of data that may eventually turn out to be unsuited for modeling.

in addition, the use of model-based  experimentation can greatly reduce the effort and cost of biological experiments, and simultaneously facilitate the understanding of complex biological systems  <cit> .

the model identification loop is complemented with an optimal experimental design step. the aim is to calculate the best scheme of measurements in order to maximize the richness  of the information provided by the experiments while minimizing, or at least, reducing, the experimental burden  <cit> .

the richness of the experimental information may be quantified by the use of the fisher information matrix   <cit> , which for the case of gaussian known or constant variance reads as follows:   

where e represents the expectation for a given value of the parameters μ presumably close to the optimal solution θ*.

the optimal experimental design is then formulated and solved as a general dynamic optimization problem, see details in  <cit> , that computes the time-varying stimuli profile, sampling times, experiments duration and  initial conditions so as to maximize a scalar measure of the fisher information matrix subject to the system dynamics  and to other algebraic constraints associated with experimental limitations.

regarding the selection of the scalar measure of the ℱ, several alternatives exist all of them related to the eigenvalues of the ℱ and thus related to the shape and size of the associated hyper-ellipsoid. the most popular are probably the d-optimality and e-optimality criteria, the former corresponding to the maximization of the determinant of the ℱ and the latter corresponding to the maximization of the minimum eigenvalue. from previous studies  <cit>  it may be concluded that the e-optimality criterion offers the best quantity-quality compromise for the information, particularly for cases where the parameters are highly correlated or the sensitivities with respect to the parameters are highly uneven; otherwise d-optimality may be more successful.

RESULTS
the nf-κb regulatory module
nf-κb is implicated in several common diseases, especially those with inflammatory or auto immune components, such as septic shock, cancer, arthritis, diabetes and atherosclerosis  <cit> . mathematical models connected to experimental data have played a key role in revealing forms of regulation of nf-κb signaling and the underlying molecular mechanisms. commencing with the original model proposed by hoffmann et al.  <cit> , several models have been proposed that include additional feedback loops, cross-talk with other pathways and nf-κb oscillations, as detailed in the recent reviews by lipniacki and kimmel,  <cit>  and cheong et al.,  <cit> .

the model considered in this work was proposed by lipniacki et al.  <cit> . this model presents several modifications with respect to the original by hoffmann et al.  <cit> . basically, while the original model accounts for the interplay among three isoforms of the inhibitory proteins iκbα, iκbβ and iκbϵ, lipniacki et al. consider the inhibitory roles of iκbα and a <dig>  incorporate new assumptions about the ikk activation and introduce the nuclear-cytoplasmic volume ratio.

the model involves two compartment kinetics of the activators ikk and nf-κb, the inhibitors a <dig> and iκbα and their complexes. it is assumed that ikk exists in any one of three forms: neutral , active  or inactive . in the presence of an extracellular signal such as tnf, ikk is transformed into its active  form. in this form it is capable of phosphorylating iκbα, and this leads to its degradation. in resting cells, the unphosphorylated iκbα binds to nf-κb and sequesters it in an inactive form in the cytoplasm. as a result, degradation of iκbα releases the second activator, nf-κb. the free nf-κb enters the nucleus and upregulates transcription of the two inhibitors iκbα and a <dig> and of a large number of other genes including the control gene cgen. the newly synthesized iκbα again inhibits nf-κb, while a <dig> inhibits ikk by catalyzing its transformation into another inactive form in which it is no longer capable of phosphorylating iκbα.

the scheme of the pathway is illustrated in figure  <dig>  the corresponding mathematical model consists of  <dig> non-linear ordinary differential equations with  <dig> parameters as follows  <cit> :  

where ikkn represents the cytoplasmic concentration of neutral form of ikk kinase; ikka, the cytoplasmic concentration of active form of ikk; ikki, the cytoplasmic concentration of inactive ikk; iκbα, the cytoplasmic concentration of iκbα; iκbαn, the nuclear concentration of iκbα; iκbαt, the concentration of iκbα mrna transcripts calculated per cytoplasmic volume v; , the cytoplasmic concentration of complexes ikka and iκbα, equivalent notation is used for all the complexes; tr is a logical variable representing the presence or absence of signal; kv is the ratio of cytoplasmic to nuclear volumes.

results/discussion
in their paper, lipniacki et al.  fixed some of the model parameters by using values from the literature. to fit the unknown parameters, they used experimental data from previous works by lee et al.  <cit>  and hoffmann et al.  <cit> :   

lipniacki et al. concluded that several different sets of parameters are capable of reproducing the data. this lack of identifiability may originate either in the structure of the model and observables selected  or in the type of experiments performed and the experimental noise . our aim was to determine the origin of the problem and to use the model identification loop presented here to improve the quality of the parameter estimates.

structural identifiability analysis
to perform the analysis we take into account that lee et al.  <cit>  considered wild-type cells subject to a persistent tnf signal and collected data for a <dig> mrna , total ikk , activated ikk , total cytoplasmic iκbα ), iκbα mrna  and free nuclear nf-κb , and also that hoffmann et al.  <cit>  measured the responses of the free nuclear nf-κb  and the cytoplasmic iκbα ) in wild-type cells under persistent and pulse-wise tnf stimulation. it should be noted here that, due to the additive character of the weighted least-squares function  and the fisher information matrix , we will regard an experiment as the combination of the measurements corresponding to all observables under a given stimulation even if they may not be measured simultaneously in practice.

the following is assumed:

• only the concentrations measured by lee et al.  <cit>  and hoffman et al.  <cit>  are at our disposal.

• initial conditions correspond to those for wild type cells after resting.

• the tnf stimulus is activated.

• only the set θ in eqn. are considered all the other parameters are assumed to be fixed, see details in table  <dig> 

the size of the model under consideration, the number of observables and the number of parameters make the application of the similarity transformation and the differential algebra approaches rather complex, thus the power series expansions will be used here.

in a first approximation to the structural identifiability problem the taylor series approach was applied. from the analysis of the resultant tableau it is possible to asses that i <dig>  k <dig>  c3a and i1a are structurally identifiable. unfortunately the complexity of the remaining equations prevents to draw clear conclusions for the rest of parameters.

the application of the generating series approach resulted, as expected, in a simpler system of equations. in fact it was possible to obtain as many coefficients as necessary to guarantee full rank jacobian, the corresponding  tableau is presented in the additional file 1: supplemental figure s <dig>  following the approach described before we obtained the minimum and the reduced tableaus  to demonstrate that the model is structurally identifiable . details are presented in the additional file  <dig>  figure  <dig> shows a summary of the steps followed with the minimum tableau to solve the algebraic set of equations on the parameters. since the parameters are structurally identifiable the origin of the difficulties found by lipniacki et al.  must be the lack of practical identifiability. in many practical situations this lack of identifiability originates in the lack of sensitivity of the observables with respect to the parameters. this can be assessed by performing a global sensitivity analysis and a ranking of parameters.

ranking of parameters
the parameters were ranked globally considering three different experimental schemes for wild-type cells. the first experiment corresponded to a persistent tnf stimulation and the second and third experiments corresponded to  <dig> h and  <dig> h pulse-wise tnf stimulations. since it is often argued that ranking will depend on the range of parameters selected, several different tests had to be performed.

however, deciding the range of parameters is often a quite difficult task. in practice large bounds are defined so as to somehow guarantee that the real solution will lie within. unfortunately, this approach often results in very large flat areas in the search space that make calibration extremely difficult. in addition, global analyses may lead to wrong conclusions, since the probability of considering sets of parameters that are far from the real sets increases rapidly. whenever possible, one should use knowledge about the system to define reasonable bounds.

for this particular example we selected a reference parameter vector  taking into account the fact that the behavior of the experimental data is oscillatory under persistent tnf activation:  

the reference was then used to select different bounds for the parameters. three different tests were performed: i) within the range (), where  corresponds to the reference value of the ith parameter in the set θ; ii) within the range () and iii) within the range (), i.e. considering that we may have underestimated, in a maximum of two, the order of magnitude of the parameters with respect to the reference. we remark that a sample of  <dig> elements was used for every case.

results obtained for all cases for the criterion δmsqr are presented in figure  <dig> together with the mean value over all ranges. from the ranking it may be concluded that the observables are significantly sensitive to c3a, c4a, kprod and kdeg and almost insensitive to e2a, t <dig> and t <dig>  indicating possible practical identifiability problems.

in general, different ranking criteria may lead to different conclusions. in this example all criteria drive same results regarding the lack of influence of e2a, t <dig> and t <dig> .

as already mentioned before, the summations over experiments and observables may hide some relevant information. for example, from figure  <dig> it is not possible to asses the effect of using pulse-wise stimulation or what are the parameters that are more relevant to the different observables evolution. to analyze this information we considered the sensitivities for the range ()  in more detail. results are depicted in figure  <dig> 

from the figures it may be concluded that certain observables become more sensitive to certain parameters under short pulse-wise stimulation . this is the case, for example, when looking at the sensitivities with respect to c3a, c4a or i <dig>  note that only the measurements of total cytoplasmic iκbα provides information about i <dig> and i1a and also the fact that we obtain almost no information about t <dig>  t <dig> and e2a.

it is important to underline that for the case of i <dig>  experiments under sustained stimulation appear not to be relevant whereas the model becomes more sensitive to c <dig> or k <dig> under sustained stimulation. it can thus be expected that using an experimental scheme combining a sustained stimulation experiment with  pulse-wise stimulation experiments will increase overall sensitivity and thus improve identifiability properties.

taking into account the results the vector of parameters θ is partitioned into two new vectors θκ and  as follows:   

the components of θκ will be now considered in the next steps of the identification loop, the components in  will remain fixed to a nominal value since their presence for model calibration will be a clear source of practical identifiability problems.

practical identifiability analysis
to establish a basis for comparison we first consider the problem as addressed by lipniacki et al., i.e. with all parameters in set θ and the experimental scheme available from lee et al.  <cit>  and hoffmann et al.  <cit> , to be referred to henceforth as es <dig>  the results obtained for the identifiability analysis will be considered as reference .

for this purpose we can perform a battery of hundreds of in silico experiments  under such experimental conditions, getting experimental data with zero-mean gaussian noise with unknown varying variance but with a maximum corresponding to 10%.

to perform the quantitative analysis according to the monte carlo approach the model calibration problem was solved for all sets of data by using the recently developed global optimization method based on scatter search  and with bounds for the parameters of ().

results obtained justify the fact addressed by lipniacki et al ., the origin of multiple equivalent solutions is the poor practical identifiability originated in the lack of influence of some parameters in the available observables.

if we compare the results with the ones obtained considering only the set θκ, table  <dig> shows a significant improvement regarding both the μ value, the relative distance to the nominal and the expected uncertainties. the following should be remarked:

c3a and c4a can be already be appropriately estimated. the μ value is less than a 1% relative distance to the nominal  value. in addition the expected uncertainties are less than a 10% which is in the order of the experimental error. as a consequence c3a and c4a can be removed from the subsequent steps in the identification procedure for the remaining parameters, denoted as , μ value is within the 5% of the nominal but the uncertainties for most of the parameters are over the 20% and over the 50% for kprod and kdeg. taking a look at the eccentricity values by pairs of parameters we will found out that in fact kprod and kdeg are the most correlated pair with an eccentricity value of  <dig> .

optimal experimental design
in order to improve the identifiability properties of  we considered a parallel-sequential optimal experimental design, in such a way that the information reported by the experimental scheme es <dig> was taken into account by introducing the experiments in the fisher information matrix . new experiments were designed within the following experimental constraints:

• initial conditions correspond to those for wild type cells after resting.

• the tnf stimulus is activated and may be pulse-wise. in order to make the experiments more easily implementable in practice a maximum of two pulses is allowed.

• the maximum number of sampling times will be  <dig> and they may be optimally located.

• the experimental noise corresponds to a maximum variance of the 10%.

• the reference value for the parameters in the ℱ  corresponds to the μes <dig> .

regarding the ℱ based criteria for optimal experimental design, the d- and e-optimality criteria are the usually preferred ones. for this particular example, and attending to the eccentricity values corresponding to es <dig>  e-optimality seemed to be the most suitable, since this promotes the simultaneous reduction of the expected uncertainty and the eccentricity.

the new experiment consists of performing two pulses and  <dig> optimally located sampling times . detailed analysis of the identifiability properties are incorporated in the additional file 1: supplemental tables s <dig> and s <dig> showing how the addition of the optimally designed experiment led the mean value μes <dig> to practically coincide, less than 1% relative error, with the nominal θ* value. in addition the expected uncertainty has substantially improved as compared to the expected uncertainties found for the experimental scheme es <dig>  it should be remarked that now the worst case is of around the 32% whereas for es <dig> it was of around 60%, in addition the maximum eccentricity, which again corresponds to the pair kprod - kdeg, has been substantially reduced, to a value of  <dig> .

the estimations of k <dig>  i <dig> and i1a are now satisfactory with less than  <dig> % error with respect to the nominal value and expected uncertainties of around the 10%. next step is to compute a new optimal experimental design for the remaining parameters by using μes <dig> as a reference.

underlined values represent the worst value for the given experimental scheme. bold face values represent the best value achieved for each parameter at the end of the identification procedure.

CONCLUSIONS
it has been largely recognized that solving the solution of parameter identification problems becomes harder with the size of the problem, particularly when the ratio between the number of observables and experimental data and the number of parameters is low, since these induce multimodality and lack of structural and/or practical identifiability.

this research describes an iterative identification procedure for non-linear dynamic biological models that is intended to improve parameter identification, i.e. to reduce the dimensionality of the problem when possible and to improve identifiability properties, and therefore to avoid premature  conclusions about the explanatory and predictive capabilities of a particular model. the procedure involves the following steps: structural and practical identifiability analysis, global ranking of parameters, parameter estimation using efficient global optimization techniques and optimal experimental design.

as an illustrative example, we considered parameter estimation of the model describing the nf-κb module proposed by lipniacki et al.  <cit> . using the identifiability tableau based on the generating series coefficients, the possibility of simultaneously estimating the entire set of parameters was revealed. with the support of the global ranking of parameters we were able to predict the insensitivity of the observables to some of the parameters and the consequent lack of practical identifiability. after fixing such parameters we proceeded throughout the identification procedure. the practical identifiability analysis for the available experimental schemes indicated high correlation between some pairs of parameters in the subset and large expected uncertainties for the parameters. the final stage was to design two new optimal experiments that were able to substantially improve the quality of the parameter estimates. this case study clearly reveals the usefulness of the proposed identification procedure to improve efficiency and robustness during model development in systems biology.

the methodology described here has been implemented in a software toolbox, amigo, which is available from the authors upon request.

authors' contributions
ebc and jrb contributed to the conception and design of the work. ebc implemented the iterative identification procedure, performed the computations and drafted the manuscript. aaa and jrb gave valuable advises and helped to draft the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
further details on the application of the identification procedure to the mathematical model of the nf-κb regulatory module. additional file  <dig> presents further details on the analysis of the structural identifiability, the ranking of parameters, the optimal experimental design and the corresponding identifiability analysis for the of nkκb example.

click here for file

 acknowledgements
this work was supported by the spanish micinn project "multisysbio", ref. dpi2008-06880-c03- <dig> 
