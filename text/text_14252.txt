BACKGROUND
a large number of proteins are known to bind dna in a location-specific manner. these include transcription factors, replication factors and chromatin components. it is widely accepted that individual proteins do not usually act in isolation but form multi-protein effector complexes on dna. when the binding sites of the individual proteins within a complex are determined by genome-wide high-throughput assays, these complexes are revealed as regions where the binding sites of multiple proteins are clustered. when evaluating the apparent co-localisation of the binding sites of a pair of proteins it is necessary to determine whether they genuinely co-localise or whether the observations could have arisen by chance. many methods have been proposed for assessing the statistical significance of such clusters . the classical statistical methods rely on obtaining the null distribution for a statistic that correlates with the phenomenon of interest. the two key design decisions are therefore the choice of how the statistic is computed and how the null distribution is obtained. we will discuss how we have addressed these questions when considering the merit of a particular test and when devising an improved test.

choice of statistic
general considerations
some latitude exists in the choice of statistic for a co-occurrence test. with co-occurrence, we are primarily interested in different dna-binding factors being located closely together more frequently than might be expected 'by chance', the latter being determined by the null distribution. a useful statistic is expected to be increased by the proximity of the factors and the number of clusters of factors in the genome. using the term heterogeneous overlap to refer to overlap between the binding sites of two different factors, one can readily envisage as useful statistics either counting the number of pairs of locations where heterogeneous overlaps occur, or the total number of bases of heterogeneous overlap. with the former, it is the presence or otherwise of an overlap that determines whether to increment the statistic while the latter weights overlaps according to the degree they are overlapped.

we desire a general framework for a statistic that is applicable to different policies for scoring co-occurrence. additionally, we also wish to divorce the specific scoring policy from the statistical test itself. a co-occurrence matrix can satisfy both objectives.

co-occurrence matrix
we will use the term binding profile to refer to the list of locations bound directly or otherwise by a specific dna-associated factor . let  and  be the binding profiles of a pair of dafs, a and b, respectively and define their joint set of locations as .

further, let there be a co-occurrence matrix, c with elements, cij, such that the co-occurrence statistic can be defined as  

i.e. the co-occurrence statistic is the sum of the scores of every pairwise combination of locations where one is drawn from those bound by a and the other from one bound by b .

the co-occurrence matrix embodies all information relevant to the calculation of the statistic. cij is the contribution to the statistic when location i is bound by a and location j is bound by b. that c is defined in general terms allows its use with a wide range of different approaches to scoring co-occurrence.

for example, if the co-occurrence metric is whether the features overlap irrespective of its extent, cij is set to unity when an locations i and j overlap and to zero otherwise. alternatively, if the number of bases overlapped between two locations is the metric, cij is set to the number of bases overlapped between locations i and j. other scoring strategies can be readily considered and implemented via a co-occurrence matrix in a similar manner. the use of a matrix removes subsequently irrelevant details such as the actual base coordinates and the specific overlap scoring metric from further consideration since it incorporates their entire effect on the calculation of the statistic. computation of the statistic can therefore be performed during resampling solely by reference to the co-occurrence matrix.

choice of null distribution
the difference between statistical methods frequently reduces to the manner in which the null distribution is obtained. with parametric methods, the observed data is used to parameterise one of the classical statistical distributions which is then used as the null distribution. non-parametric methods use resampling approaches to approximate the null distribution.

chromatin components naturally bind in a highly non-uniform manner and the null distribution needs to account for this appropriately. indeed, failure to account for so called bursty sequence effects can result in many false positives  <cit> . in addition, co-occurrence tests must be cognisant of the provenance of the binding site data. binding profiles arise from experiments and the mapped sites reflect the actual physical state of chromatin. the contribution of the underlying sequence is therefore already entirely accounted for in the binding profile. in contrast, attempts to identify clustering of sequence-specific proteins by the co-occurrence of their binding site motifs try to reconstruct the chromatin state from sequence: in this case, the binding-site motifs specified by the position-weight matrices  and the underlying sequence composition are pertinent to the specification of the null distribution: for example, two similar pwms will have a large number of co-occurring hits anyway. so while both sources of data generate sets of locations that can be analysed for co-occurrence, the null distributions are likely to be very different. here, our interest is strictly confined to the simpler case of binding profiles alone and our method is unlikely to be applicable to motif co-occurrence data. a survey of the motif co-occurrence problem can be obtained from papers cited in  <cit> .

tests based around resampling strategies have been previously explored as a means of accounting for variation in binding site distribution  <cit> . as they rely on randomising the observed binding sites for the factors, they can be expected to automatically account for binding site non-uniformity. in the following sections, we will demonstrate that the utility of these methods is highly dependent on the resampling scheme used.

resampling strategies rely on sampling a pool of locations to infer the null distribution. the pool can be constituted in two ways. first, it can comprise only the binding profiles of the pair of factors being examined for co-occurrence. alternatively, it could also be constituted by pooling the binding profiles of a large number of factors, some or all of which are to be investigated for pair-wise co-occurrence. all methods we discuss except for the fl method of haiminen et al use the former approach.

resampling-based tests
resampling methods operate by randomly sampling the observed data to construct a null distribution. when evaluating the statistical significance of co-occurrences between a pair of binding profiles containing m and n locations respectively, their constituent sites are combined to form a joint pool of locations. further pairs of binding profiles containing m and n locations respectively are then repeatedly sampled from this joint pool and scored for co-occurrence to estimate the null distribution of the test statistic.

we can describe resampling formally with two sampling operators f and g that draw n elements from x. then, at each iteration, two new lists of binding sites,  and , the same size as the originals are generated:-  

and the distribution of  is used as the null distribution.  and  are the numbers of elements in  and .

in the following sections, we report the results of our investigations into three different resampling strategies, including the previously described permutation test approach.

permutation test
in this approach, a joint pool of all binding sites is created and  sites randomly assigned to daf a. the remaining sites are then assigned to daf b ).

let ρ and  be operators for sampling with replacement and without replacement respectively. the permutation test can then be formally described in set notation as:-  

independent resampling
instead of permuting the joint pool of locations, this approach reconstitutes  and  independently ), i.e.  

note that the same location can be assigned to both factors with independent sampling.

hybrid method
the approach here is to split the joint locations into two pools. the first pool comprises locations at which co-occurrence has been observed in the dataset. the second pool comprises the remaining singleton locations.

during resampling, locations in the first pool may either remain unassigned or allocated to one factor only ). thus, factors co-occur in this pool only when they are assigned to a pair of overlapped locations. locations in the second pool are freely assigned to the factors. but since the locations in this pool are singletons, overlaps arise only when both factors are assigned to the same location.

a formal statement of this strategy proceeds along the following lines. the locations in  are partitioned into sets of heterogeneously overlapped and singleton locations. we can define the set of singleton locations with reference to the co-occurrence matrix by  

and the set of heterogeneously overlapped locations is then  

the resampling is then performed by:-  

multiple factor methods
haiminen et al proposed and analysed the performance of several interesting approaches that can be applied where the binding sites of many factors are simultaneously known on the same sequence extents, a situation that arises frequently with high-throughput datasets  <cit> .

two approaches of theirs are of particular interest. in the fl approach, the joint pool comprises all binding sites for all factors in the dataset. permutation is performed by assigning  sites to daf a from this pool. a further  sites are then assigned to daf b from the remaining unassigned members of the pool. the null distribution is the distribution of co-occurrence scores obtained by this permutation method. in the fl method, the binding sites of the first factor, daf a, remain unchanged while those of the second factor are permuted as described above.

the fl but not the fl approach will be included in our analysis. we are not entirely at ease with treating one of a pair of factors differently from the other nor with the possibility that a co-occurrence may be significant when one factor is fixed but not when the other is fixed.

desired behaviour of a co-occurrence significance test
it is important to form some expectation of the p-values that could arise from a sensible method for assessing factor co-occurrence. first, greater significance would be expected when a larger proportion of sites co-occur. for example, one might expect  <dig> co-occurrences where each factor only bound to  <dig> sites to yield a lower p-value than if each factor bound to  <dig> sites. in addition, given the same proportion of co-occurring sites, a greater significance is expected if the total number of sites is larger.

RESULTS
a simple model system
a particularly simple co-occurrence model is one with m paired co-occurrences out of a total of  and  binding sites for dafs a and b respectively.

the behaviour of different approaches to implementing permutation tests will initially be investigated on this analytically tractable system. unless otherwise stated, the statistic used is the number of co-occurrences as described above and a co-occurrence is deemed to occur if two locations bound by different factors overlapped each other. as we note above, the specific overlap definition used has little influence on the validity or otherwise of the approaches.

permutation test
this approach appears to be similar to that reported by hannenhalli and levy  <cit> .

permutation test: synthetic data
when a simulation was conducted with m =  <dig> and na = nb =  <dig>  i.e. wherein all  <dig> sites bound by a and b were paired,  <dig> co-occurrences were observed in  <dig> resamplings yielding a p-value of  <dig> . when the total number of sites bound was increased to na = nb =  <dig> without change to the number of co-occurrences,  <dig> co-occurrences were observed which yields an estimated p-value of  <dig> . an apparent anomaly arises therefore where the p-value is only modestly changed when the proportion of co-occurring sites changes from  <dig> in  <dig> to  <dig> in  <dig>  more surprisingly, the p-value actually falls with this change. what is the source of this anomaly? a better understanding can be gained from examining the mathematics behind this approach.

the p-value for the simple model can be derived analytically. the observed number of co-occurrences in this case is also the highest value it could attain - no permutation can cause a co-occurrence at any of the singleton locations since each of these can only be assigned to one of the factors. the p-value is therefore the probability of exactly m paired co-occurrences given the values of na and nb. this probability is, in turn, the product of the probability of having exactly m sites of each factor assigned to the overlapped pairs, pm and the probability of them being arranged as pairs, ppair. the former is described by the hypergeometric distribution  

the latter is the product of selecting m pairs starting with m of each factor. the probability of each pair is described by the hypergeometric distribution from the number of sites left to assign. on simplifying  

for any given ratio of na to nb, pm changes little with increasing n = na + nb. for example, with na = nb =  <dig>  the probability of assigning  <dig> sites to each factor is  <dig> . with na= nb =  <dig>  it is  <dig> . the p-value is therefore dominated by ppair, i.e. it is the number of overlaps that determines the p-value: the proportion of sites overlapped has little influence on it. for example, ppair for  <dig> overlaps is  <dig>  which is the upper limit for p-value irrespective of the total number of sites. with  <dig> overlaps, ppair =  <dig>  × 10- <dig> which guarantees statistical significance on almost any reasonable threshold irrespective of how small a proportion of the total number of sites bound that might be. this does not accord with what might be deemed reasonable behaviour from a co-occurrence test.

permutation test: other implications
as previously noted, the permutation test approach appears to have been used by hannenhalli and levy  <cit>  albeit in a different statistical framework. we were interested to determine whether their framework circumvents the drawbacks we identified with a permutation test approach assessed with the hypergeometric distribution.

hannenhalli and levy studied the co-occurrence of hits generated by a pair of position-weight matrices  and counted the number of times hits from different pwms were within some threshold distance of each other. they then computed a co-localization index :-  

where nij is the number of co-occurrences observed between two pwms, i and j, and rijis the number observed after a permutation. repeated permutations of the dataset can be expected to yield differing values of rij and it is not clear how the value of rij used above is selected. further, it should also be noted that both the log-ratio and the ratio have been used to define ci  <cit> . to get an idea as to how the distribution of ci varies with the fraction of sites overlapped, we simulated  <dig> resamplings of a simple model having  <dig> paired overlaps and a varying number of singleton sites . it is clear that the distribution of ci varies little with the fraction of co-occurring sites and is almost wholly determined by the absolute number of overlapped sites. hannenhalli and levy  <cit>  commented on the high frequency of co-occurring transcription factor binding and the paradox that higher-order co-occurrences were not enriched amongst factors showing highly-significant pairwise co-occurrences. while co-occurring pwm pairs will be expected to yield many overlapped pairs and therefore generate high ci scores, it could be that some of the highly scoring pwm pairs had large number of hits such that a small fraction of overlaps is suficient to yield enough overlaps to give a high ci thereby causing those factors to be incorrectly identified as co-occurring pairs.

independent resampling
a major reason for the highly significant p-values observed with the permutation test approach is that the observed score is maximal and there are so many ways in which m sites of each factor can be arranged to result in fewer overlaps. instead of permuting the available sites, if two independent draws of size na and nb are made, it is possible for the same location to be assigned to both factors ). the observed score is then no longer maximal since overlaps can potentially occur at any location. further, when both locations involved with the observed overlaps are assigned to both factors, four overlaps can be scored instead of the single overlap observed ). this change in sampling procedure has the effect of reducing statistical significance since the observed score can be matched or exceeded in many different ways.

in contrast to the permutation test approach, which leads to overly significant p-values, statistical significance was never achieved with independent resampling. for example, using synthetic data with  <dig> locations, all presenting as overlapped pairs, a surprisingly high value of  <dig>  is obtained. this arises because whenever a pair of overlapped locations has each of its locations assigned to both factors, it increases the score by four. this is four times that obtained when two isolated locations, each bound by a different factor, overlap each other.

hybrid approach
the behaviour of this model is expected to be intermediate between permutation test and independent draws in that the first pool is treated with the former strategy while the second is treated with the latter.

hybrid approach: synthetic data
with a profile consisting of paired overlaps only, as with the earlier example with  <dig> pairs , the result is identical to that obtained from the permutation test strategy, i.e. the p-value is approximately  <dig> . however, merely adding  <dig> singletons of each factor to the model raises the p-value to approximately  <dig> . we now compare other scenarios against this baseline example. with more pairs, the p-value becomes more tolerant of singletons. for example, with m =  <dig>  adding  <dig> singletons to each factor yields a p-value of around  <dig>  . we expect that for the same proportion of overlapped locations, the p-value should fall with an increase in the total number of locations. adding ten locations to each factor with m =  <dig> gives the same proportion of overlapped locations as the baseline but yields a p-value of around  <dig>  . both these behaviours are concordant with the desired behaviour of an appropriate co-occurrence test.

the asymptotic behaviour of a statistic shyb computed by the hybrid method can be considered for the case where a binary scoring is used. at one extreme, when there are no singleton sites, the method is entirely equivalent to the permutation test and should yield significant p-values provided the number of sites overlapped is large enough. at the other extreme, when a negligible fraction of the locations are overlapped, the distribution of shyb can be expected to approach that where na as and nb bs bind a total of na + nb sites. the probability mass function  of shyb for this limiting case is  

where  <dig> ≤ k ≤ min. .

as the average value of shyb is then nanb/, we may expect that statistical significance will only be achieved if the observed number of overlaps considerably exceeds this.

hybrid approach: polycomb/trithorax data
the hybrid technique was tested on data published by schuettengruber et al  <cit> . the authors published chip-chip  <cit>  binding profiles for polycomb and trithorax proteins , other chromatin proteins, polyhomeotic, pleiohomeotic, pleiohomeotic-like, dorsal switch protein- <dig>  gaga factor  as well as histone marks  in drosophila melanogaster. in all, ten chromatin components bound over  <dig> locations were examined.

in this test, the presence of co-occurrence was investigated for all possible pairs of binding profiles. of the  <dig> potential pairs,  <dig> highly-significant co-occurring pairs  were identified. these were pc-me3k <dig>  pc-ph, me3k27-ph, me3k4-pho, me3k4-phol, pho-phol, me3k4-trx-n, dsp1-gaf, dsp1-pho, phol-trx-n and pho-trx-n.

the co-occurrences divided the factors nicely into two clusters  with a single co-occurrence linking the clusters . the first cluster includes the two polycomb recruitment complex i  proteins  while the pleiohomeotic recruitment complex  proteins  localise with the other cluster. it was particularly pleasing to detect a ph-pho co-occurrence since these are similar proteins that bind the same target sequence in vitro but have somewhat different binding profiles in vivo  <cit> . unlike phol, which only interacts with the prc <dig> component esc  <cit> , pho is known to interact directly with the prc <dig> components pc and ph as well as with esc  <cit> .

trithorax is known to be cleaved into two independently acting fragments, trx-n and trx-c  <cit> . a very weak score for the ph-trx-c co-occurrence was again consistent with the finding that while trx-n is associated with me3k4-marked sites, trx-c is associated with prc <dig> complexes  <cit> .

we were interested in how the p-value behaved with different degrees of overlap between factors and in particular where the threshold between statistically significant and insignificant overlap lay . it is noteworthy that this method does not only detect cases where both sets overlap extensively, i.e. where percentage overlaps in both cases exceed 50%, but also detects occasions where one factor almost wholly co-occurs with a subset of binding sites of the other factor. our hybrid method appears to detect co-occurrences broadly consistent with known interactions. a summary of results is shown in table  <dig> with the p-values obtained by each of the methods described in this paper. note that our earlier objections to the permutation test and the independent resampling approach have been amply confirmed when applied to this dataset.

pilot runs of  <dig> resamplings were performed to filter out clearly non-co-occurring pairs. thereafter,  <dig> resamplings were used for candidate pairs when testing the hybrid method. tests using the permutation test approach were restricted to  <dig> resamplings. pairs with significant p-values by the hybrid method are indicated in bold and italics.

fl method
an experimental implementation of the fl method  <cit>  was used for the following studies.

fl method: synthetic data
where the dataset consists of only the pair of factors of interest the fl method is identical to the permutation test. however, this is not the context for which the fl method is intended and we examine the effect of additional binding site data from further factors. consider two factors present as  <dig> co-occurring pairs. the p-value obtained over  <dig> trials is, as expected from our foregoing permutation test analysis, very low: <  <dig> × 10- <dig>  the presence of other factors with a very similar pattern of binding can be examined by adding further pairs of factors with the same binding sites as the first pair. the p-value escalates rapidly on doing so, to  <dig> ,  <dig>  and  <dig>  on adding one, two and three replicates respectively. if the effect of the presence of a factor with a disjoint set of binding sites can be readily seen by adding to the four-replicate set, a further factor binding to  <dig> novel non-overlapping sites. the p-value then declines precipitiously from  <dig>  to very significant score of ≈  <dig> × 10- <dig>  the synthetic data analysis presented above illustrates that the fl method is potentially sensitive to the choice of factors in the dataset, which can be expected given that the null distribution is obtained from combining all binding sites in the dataset. where factors co-occur frequently, the statistical significance of an observed co-occurrence is reduced accordingly. correspondingly, where that is not the case, an observed co-occurrence is more readily deemed statistically significant.

fl method: polycomb/trithorax data
to further investigate the potential of the fl method, it was applied to the same polycomb/trithorax data previously examined with the hybrid method. a summary of the results from this method are juxtaposed with the results of all other methods in table  <dig>  in general, with this dataset, the fl method reported more significantly co-occurring pairs than our own hybrid method . all pairs scored as co-occurring by the hybrid method were also co-occurring by the fl method indicating that the fl method was either more sensitive/less conservative than the hybrid method.

given our concerns over the sensitivity of the fl method to the choice of factors in a dataset, we also determined the effect of removing each factor in turn from the dataset and repeating the analysis. in general, the fl method was found to be quite robust to these exclusions. the results remained unchanged when any one of five factors were excluded . exclusion of one of dsp <dig>  ph, pho, phol or trxc caused the gaf-me3k <dig> co-occurrence to become statistically significant. in addition excluding pho also caused the phol-trxc co-occurrence to be deemed statistically significant. the suppressor of hairy-wing insulator protein, su, binds at  <dig> sites in the drosophila genome, almost all of which are distinct from those of other known chromatin components and its addition to this dataset may be expected to raise the statistical significance of co-occurrences between other factors. this was indeed observed, with both the gaf-me3k <dig> and phol-trxc co-occurrences already mentioned above becoming significant, as well as three previously non-significant co-occurrences, gaf-trxn, me3k27-pho and trxc-trxn. it is noteworthy that while sites in the su binding profile only comprise around 15% of the total original number of locations, they were enough to drive a further five candidate co-occurring pairs to significance.

performance
as with other resampling methods, our hybrid method test is computationally expensive, especially when low p-value thresholds are required. the current implementation is written in pure r and is single-threaded. the user time for analysing the pc-me3k <dig> co-occurrence data with  <dig> resamplings was  <dig> s on a core  <dig> quad xeon t <dig> . to improve compute times we exploited the inherent parallelism in the resampling process, utilising the r interface to a message-passing interface  implementation, rmpi  <cit> . when run on a dual core  <dig> quad xeon t <dig> with six slave processes, the elapsed time for the pc-me3k <dig> task fell to  <dig> s, a reduction of  <dig> -fold which amounts to an acceptable  <dig> -fold per slave process. further speedup may be possible by reimplementing parts of the code in a lower-level language.

discussion
our analysis of four different approaches to the use of permutation tests showed the p-values obtained to be highly sensitive to the manner in which the resampling is done. two aspects are of concern. first, the independent sampling approach cannot be useful in any practical context as it is incapable of yielding significant p-values under any practical conditions. it readily explains why it has not been previously encountered. of greater concern is the permutation test approach. this method readily yields highly significant p-values with our statistical framework even in cases that should not warrant it. the permutation test approach has been previously used for co-occurrence analysis and we urge caution be used when interpreting data obtained in this way. the fl and hybrid methods were found to be effective on a real-world dataset with the latter being more conservative on this dataset: all pairs scored as significantly co-occurring with the latter were similarly scored with the former.

it is difficult to determine which method is more appropriate. clustering of factors represent a multi-way co-occurrence which may not be adequately detected/rejected by the presence or otherwise of a pairwise co-occurrence. the use of the fl method is sensitive to the selection of factors used to determine the null distribution and some thought should go into selecting appropriate factors to include in the dataset. in particular, a data mining exercise screening large numbers of binding profiles for co-occurrences might favour the inclusion of the hybrid method in the test repertoire because of its insensitivity to factor selection. given the limited data available to validate the relative performance of the methods with regard to selectivity and sensitivity, it may be advantageous to use both. generally, resampling tests are limited in the range of p-values they can yield by the number of resamplings performed and the use of two methods with different sensitivities may allow the very high confidence co-occurrences to be differentiated from the weaker ones. to conclude, we report the development of a hybrid approach based on pragmatic grounds that we believe has utility. our evaluation, using a comprehensive real-world data set, indicates that the derived co-occurrence data appear reasonable. indeed, we believe our approach is conservative in that it assumes that all singletons are capable of generating a co-occurrence. while the method may be computationally expensive, it is no more so than other techniques relying on resampling.

an initial release of the cooccur package implementing our method accompanies this paper .

CONCLUSIONS
we have proposed a hybrid approach to sampling in a permutation test for detecting pairwise co-occurrences of factor binding. we have also demonstrated that the method is applicable to real-world data and yields results consistent with previous expectations. it is likely to be useful for analysis of data from high-throughput genome-wide screens of factor binding.

