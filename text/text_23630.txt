BACKGROUND
every cell is packed with solutions to the problems its ancestors faced. these solutions are embodied in biological macromolecules – rna and proteins – which produce energy from nutrients, neutralize external stressors, coordinate cell division, defend cells against invaders, and so on. most of us think of these solutions as extremely rarefied: they would be difficult to find in the space of possible nucleotide or amino acid sequences, because they occupy exceptionally small regions in this space. their discovery by living things was hard-won, through innumerable generations of mutation and natural selection. despite this common wisdom motivated by stringent functional constraints on biological molecules, we have little rigorous, quantitative understanding of how abundant or rare the molecular structures of biological molecules are. the fundamental reason is that our ability to characterize genotypes  still vastly exceeds our ability to characterize phenotypes . while it is simple to determine the nucleotide sequence of a gene and even of entire genomes, the prediction of the structure of individual proteins or rna molecules, let alone of their integrated behavior, is a major challenge.

if they are extremely rare, functional phenotypes may be very difficult to find in a blind evolutionary search. however, the significance of phenotype rarity does not end with this observation. the descendants of biological macromolecules may give rise to molecules with new phenotypes and functions – evolutionary innovations. the ease with which they do is also called their evolvability  <cit> . some molecules have been extremely prolific in this regard – highly evolvable – whereas others have been less so. the rarity of a molecule may affect its propensity to evolve new structures and functions. to see why, it is useful to consider that such molecules are usually part of large networks of genotypes  <cit> . most known structures of protein and rna molecules are adopted not by one sequence, but by large sets of sequences. many or all of these sequences can be connected in sequence space through series of nucleotide or amino acid changes that traverse a large fraction of this space, yet leave the structure and function of the molecule unchanged. such sets of sequences are often referred to as neutral sets or neutral networks  <cit> . specifically, a neutral set is a set of sequences with the same phenotype. a neutral set is called a neutral network if all sequences in it can be connected through series of single mutations that do not leave this set. this distinction maintains the generality of our framework. however, for the rna phenotypes we study neutral sets are almost always connected, so the two terms can be used interchangeably. the size of a neutral set is a measure of a phenotype's rarity in sequence space. the greater this size, the easier it should be to find the phenotype in an evolutionary search. we will refer to phenotypes with large neutral sets as abundant or frequent phenotypes.

evolutionary innovations arise when mutations that explore variants of a functional phenotype strike a molecule with a new and useful function. a large neutral network can be of advantage in this process, because the immediate neighborhood of a large neutral network in sequence space contains many more phenotypic variants than that of a smaller neutral network. through neutral evolution on large neutral networks, molecules can thus get access to many molecular variants. this is why high abundance of a phenotype can be argued to be beneficial for evolutionary innovation  <cit> . recent evolutionary work on protein structures shows that abundant protein phenotypes have indeed evolved greater functional diversity  <cit> . other factors, such as neutral network topology may also play a role in evolutionary innovation  <cit> .

these observations motivate the need for approaches to estimate the abundance of phenotypes in sequence space. we here show how to solve this problem for a computationally accessible molecular phenotype, the secondary structure of rna molecules. rna secondary structure is required for the biological function of many rna molecules  <cit> . it is thus an important phenotype in its own right. because algorithms to predict rna secondary structure from an rna sequence are available  <cit>  secondary structure is an important computational model to understand the relationship between rna genotypes and phenotypes  <cit> . the computational challenge to estimate whether an rna phenotype is frequent or rare, i.e., whether it is adopted by many or few sequences, is formidable. for example, even for sequences of length l =  <dig> one has to estimate numbers smaller than 10- <dig> . below, we discuss the details of the method we developed, which is based on a nested sampling of genotypes. we then apply this method to multiple biological and random rna sequences. the results demonstrate that biological rna structures have a large number of sequences that fold into them, much larger than for random phenotypes. this number of sequences may also be moderately larger than for structures produced from random genotypes.

methods
software for structure prediction and inverse folding
for our analyses, we used the vienna rna package , including the routines fold, which determines the minimum free energy  structure of a sequence, and inverse_fold, which creates sequences folding into a given minimum free energy structure, using a guided random walk through sequence space that begins with a randomly chosen sequence. we also used the utility bp_distance which calculates the base-pair distance of two arbitrary structures.

sampling neutral sets
in the literature, heuristic sampling of neutral sets has been performed by using the inverse_fold routine implemented in the vienna rna package . to test this approach, we studied its statistical bias by comparing its results to results obtained by random sampling of compatible sequences – sequences where only bases capable of pairing occur in the structure's stacks. we found that inverse_fold does not sample the desired set of sequences uniformly: sequences that are on the "boundary" of this set are sampled more frequently. we thus refrain from using this approach when uniform random sampling is necessary. for our study, the routine inverse_fold is used only in the initialization of the monte carlo approach which quickly loses the memory of this initial choice.

error estimates
to estimate a neutral set size for any given structure s, we first carry out a very long run of the nested monte carlo procedure  to estimate the size of the neutral set. we measure the quantities oi = χd that are the averages in each of the fractional intervals  of the total run. that is, each oi is computed from a fraction of 2% of the total run. this gives  <dig> estimates of neutral set size, where the global average is the actual estimated size . we then use the  <dig> values of oi to obtain the error in this estimate. as in all markov chain monte carlo methods, the  <dig> values oi are not independent. to address this problem, we apply the jackknife method  <cit>  which is a general way to compute errors even for correlated and non-normally distributed data. in this method, if one has k samples oi, one first computes k averages mi of these samples, omitting for each average the single value oi. the resulting set of values  has some standard deviation σ. the jackknife error estimate is given by k−1σ. it is that value we report as the error bar on the neutral set size estimates.

in a similar vein, we obtained error estimates for p-values as follows, again using the jackknife method. a structure's p-value is determined from expression  for a sample of m phenotypes. for each i =  <dig>  ..., m, we remove the i-th structure from this sample and recompute the p-value according to  with this altered sample. if σ is the standard deviation of these m estimates, then the jackknife procedure specifies m−1σ as the error of these estimates; this is the error we quote in table  <dig> 

RESULTS
to determine neutral network sizes, one can in principle enumerate all sequences and the structures they fold into, or one can sample by ''brute force'' many sequences from sequence space, and estimate the fraction of sequences with a structure of interest. if one focuses only on sequences compatible with a given structure – sequences where only bases capable of pairing occur in the structure's stacks – then these approaches are practical for single structures up to l ≈  <dig>  however, for our work we need to do this kind of calculation for thousands of structures, and for neutral set sizes that may exceed  <dig>  we thus need a more sophisticated approach. in what follows we describe a method that leads to reliable estimates for much larger l. in addition, this method achieves uniform sampling regardless of whether sequences adopting a given structure fall into one neutral network, or into multiple, disjoint neutral networks. in a second part, we explain how this approach can be used to quantify whether a structure's neutral set is atypically large or small.

part 1: a nested monte carlo approach to estimate the size of a neutral set
we are given a discrete space of 4l genotypes , where l is sequence length. we would like to determine the number of genotypes in this space that have a given "target" phenotype  s*. to this end, we have developed a monte carlo sampling approach. it builds on the metropolis algorithm  <cit>  that can sample connected spaces according to any predefined probability measure. however, the sampling of a set does not yield an estimate of its size. we overcome this shortcoming by considering nested sampling. our approach only assumes that there exists a distance metric d among all phenotypes, or at least a measure of distance between any phenotype s and the target s*.  in practice, d will be an integer, ranging from  <dig> to some integer dmax; d =  <dig> if and only if s = s*. we will call v the number of genotypes whose phenotype s satisfies d ≤ d, and we will refer to the set of these genotypes also as v. our quantity of interest is v, the size of s*'s neutral set, be it connected or not. to compute this quantity, we use the identity

  v=vvvv…vvv 

because v = 4l is known, v can be estimated from the estimates of all the ratios v/v. these ratios can be estimated using the metropolis algorithm by sampling uniformly the space v, and measuring the average of the indicator  function χd of v

  χd=1 if g∈vχd=0 if g∉v 

note that by construction the sets v are nested, either like russian dolls, or in more complicated ways, since each set need not be connected. the innermost set, v, is the ultimate set of interest and its size is the desired neutral set size; all the other sets are just of use to connect v to the known quantity v = 4l.

to estimate the ratio v/v we sample v uniformly using the metropolis algorithm. specifically, we begin with an arbitrary genotype g <dig> in v, and produce a  chain, g <dig>  g <dig> ... gk ..., of genotypes. to obtain gk+ <dig> from gk, a random nucleotide in gk is changed, producing a mutated genotype g'; if g' ∈ v, then gk+ <dig> = g', otherwise gk+ <dig> = gk. at sufficiently large k, the distribution of gk is uniform in v, allowing for unbiased statistical estimates of χd. to be precise, at this stage the sampling is uniform but restricted to the connected component of v that contains g <dig> 

as described so far, nested sampling estimates the ratios v/v by performing independent monte carlo simulations for each d, but the algorithm is sound only if each set v is connected. to guarantee soundness even when this is not the case, we estimate all the ratios in  simultaneously, introducing genotype ''swaps'' similar to those used in the exchange monte carlo approach  <cit> . specifically, we first initialize the monte carlo procedure by establishing as many sequences in v as there are ratios to estimate in . it is simplest to initialize all these sequences to the same element of the neutral set which we assume to be non empty. whether this initial sequence is from an unbiased  distribution does not matter. we thus use inverse_fold  <cit>  to establish such a sequence. each of these sequences will then start a random walk that will be used to estimate one of the ratios of . at each round of the nested monte carlo, there are now two steps. the first is a mutation step, in which each random ''walker'' is mutated as described above according to the metropolis rule; it is thus confined to the set v used to estimate the ratio v/v . the second step consists of a swap of two sequences: genotype  <dig> in v is exchanged with genotype  <dig> in v, if and only if genotype  <dig> also lies in v, and if genotype  <dig> also lies in v. that a genotype lies in two sets is possible, because the sets are nested. just as in exchange monte carlo, one can prove that the detailed balance condition upon with the success of the metropolis algorithm rests  <cit>  is still satisfied with this procedure; thus the desired fractions can still be computed in the same way as for the simple sampling previously described.

while it may seem that this generalized monte carlo method is simply a parallel version of our initial sampling, the introduction of swaps has two important benefits. first, as in all markov chains, the successive sequences of genotypes generated in the metropolis algorithm are correlated. this correlation leads to statistical errors and thus is undesirable. the random swaps reduce this correlation and thus lead to greater computational power. second, ergodicity – uniform sampling regardless of whether the sets v are connected or not – is guaranteed by the modified monte carlo algorithm. the reason is as follows. the detailed balance condition for each walker ensures that all genotypes which can be reached are necessarily sampled with equal probability. now in the largest volume v , the random walk is ergodic, simply because the entire sequence space v is connected. through swaps, walkers can reach any genotype in v, so that the random walk in v is also ergodic. by recurrence, one can see that the random walk in v is ergodic for all d.

we note that the sampling scheme from  can also be generalized to other nested sets of volumes that do not use successive values of d as in . greater efficiency could be obtained by adapting the choice of d-values: having too many fractions to estimate in  leads to excessive computational cost, while too few fractions lead to poor sampling and large sampling variance. in our application to rna molecules below, we found that the simplest procedure, of using all d up to dmax was adequate. note also that our approach will work not only for rna genotypes, but for any genotype space  as long as a distance metric between phenotypes exists. our software to estimate neutral network sizes is available at .

part 2: evaluating the abundance of secondary structures
we now have described how to estimate the neutral set size of an individual structure.

one of our goals is to find out whether biological structures have neutral sets that are atypical in size. since evolvability arguments suggest that these sizes might be large, we shall ask whether biological neutral network sizes are much larger than those of typical structures. specifically, we wish to test the null hypothesis h <dig> that a given secondary structure of a biological rna molecule has an associated neutral set whose size could have been drawn at random from the distribution of all neutral set sizes, i.e., from randomly chosen phenotypes.  this task requires us to estimate neutral set sizes for many different structures. however, already for moderate length l, there is an astronomical number of structures, and we thus cannot enumerate them exhaustively. we here demonstrate the theoretical foundation of an enhanced sampling method that allows us to estimate the comparative abundance of a phenotype.

neutral set sizes ns follow some distribution p, defined as the probability that ns equals some integer x . although this distribution is discrete, there are so many different structures that a continuous notation with a corresponding probability density ρ is appropriate. note that ∫0∞ρdx= <dig>  we would reject h <dig> if, for a specific phenotype s* and its neutral set size ns*,

  p=∫ns*∞ρdx< <dig>  

i.e., we integrate over the right tail of the distribution, thus performing a one-tailed test. if  holds for a neutral set, we call the set atypically large at a confidence level of  <dig> , but this threshold can of course be reduced if a more conservative test is needed.

we next demonstrate an intimate link between the p-value and the rank histogram of neutral set sizes, which will lead us to a sampling scheme to estimate small p-values.

for very short sequences, one can calculate p-values by exhaustive enumeration of sequences and structures. consider, for example figure  <dig>  which shows all  <dig> rna secondary structures for l =  <dig> for which there exists at least one sequence folding into the structure.  in this case with l =  <dig>  each neutral set  size is unique. in the figure, the structures are rank-ordered with the largest neutral network size  to the right. for any given structure, we can immediately evaluate whether  holds by verifying whether it is among the 5% of phenotypes with lowest rank. more precisely, if n is the total number of structures, and r is the rank of a given structure s*, then the associated p-value can also be thought of as a "relative rank" p = r: = r/n. ties, where two or more structures have the same neutral set size, can be resolved by assigning these structures successive ranks. note that the most abundant, lowest ranked structure in figure  <dig> corresponds to the unfolded "structure". because that structure is of no interest for our work, we shall not include it in our figures or data sets hereafter.

for large l, such rank histograms cannot be computed, because the number of structures scales exponentially with l, so it is not generally possible to identify all structures. our sampling approach avoids this problem, thereby allowing the estimation of p-values at much larger l. the key point is that the  rank of a structure is not necessary, we only need an estimate of its relative rank, and that can be obtained as follows. first, we generate m random structures, where each structure is obtained with equal probability, compute their neutral set sizes, and then sort these sizes. for the second step, consider a structure s* of neutral set size ns*. its  rank r is unknown, but its relative rank r/n can be estimated as r'/m where r' is the number of structures in the sample of size m that have neutral sets at least as large as ns*. the associated estimate of p is then simply r'/m.

a complication to this sampling approach comes from the requirement of random  sampling of phenotypes. for rna secondary structures, phenotypes could be sampled by random assignments of allowed base pairings  <cit> , but in other systems, such phenotypic sampling may not be straightforward. in addition, some phenotypes may have empty neutral sets, i.e., ns =  <dig>  in which case the phenotype is not "designable"  <cit> . undesignable phenotypes are of limited biological interest, but certain knowledge that a phenotype is undesignable is hard to come by. to overcome this challenge, and to avoid undesignable phenotypes, one can perform random sampling of genotypes instead. however, in this approach, the computation of the p-value has to be modified because one does not sample phenotypes uniformly, but only genotypes. in effect, each phenotype s is chosen with a probability ns/∑sns that is linearly proportional to the size ns of its neutral set. in such a sampling, phenotypes with large neutral sets arise more frequently than those with small neutral sets. in this sense, the sampling of phenotypes is biased . incidentally, this bias focuses the sampling on structures with large neutral sets, which allows us to estimate small p-values with a small statistical error. we next explain how to calculate the p-values of equation  with this sampling scheme.

returning to our continuous notation, denote by μdx the probability of obtaining a neutral set size in the interval  through this random genotype sampling. the linear dependence of the sampling probability on neutral set size leads to the following expression for μ:

  μ=xρ∫0∞yρdy 

the denominator is a normalization constant which ensures that μ is a proper probability density. equation  can be rewritten as

  p=∫ns*∞μρμdx 

it follows from  that

  ρμ=1x∫0∞yρdy 

taking advantage of  to modify  yields

  p=∫ns*∞μ1x∫0∞yρdydx=1xdx)dy) 

we can determine the value of the rightmost integral by setting ns* to zero, because equation  shows that in this case p =  <dig>  we then obtain

  ∫0∞yρdy=1∫0∞μ1xdx 

finally, substituting  into  gives

  p=∫ns*∞μ1xdx∫0∞μ1xdx 

in sum, we can use  to estimate p by sampling genotypes at random ). in practical terms, in order to estimate the p-value of any  structure s* of interest, we first determine the neutral set sizes  for m structures obtained from a sample of m random sequences, using the nested monte carlo approach. we then estimate the structure's neutral set size ns*. finally, we estimate the p-value of s* as

  p≈∑{i|nsi>ns*}1nsi∑i=11nsi 

here, summation in the numerator extends over all structures in the sample whose neutral set is greater than that of s*.

analogous p-values can be estimated for related hypotheses. for example, beyond testing for anomalously small neutral set sizes, one can ask whether the neutral network of a particular phenotype is significantly larger than neutral networks associated with the phenotypes of random genotypes; to test this hypothesis, no reweighting is necessary and p is simply given by the fraction of random genotypes that have neutral sets larger than s*. additional file  <dig> shows a comparison of our procedure with an exact enumeration method that is tractable for very short sequences.

algorithm performance
the nested monte carlo approach overcomes the difficulty of measuring the tiny fraction v/v by replacing it with the problem of measuring the series of larger fractions v/v. the cost paid is the need to follow dmax random walkers rather than just one such walker. for our rna application, this cost is dominated by the cost of folding sequences. in the vienna package  <cit> , the time to fold a sequence of l bases grows as l <dig>  this is to be compared with the time to implement a random mutation ) or to implement a swap ). it is thus no surprise then that the nested monte carlo procedure consumes nearly all its cpu time within the folding routine. in an individual run, at least  <dig> × l mutations are carried out. on today's standard desktop workstations  it takes approximately  <dig> minutes to compute the neutral network size to within 2% when l =  <dig>  about  <dig> minutes when l =  <dig>  and more than  <dig> hours when l =  <dig>  the longer the run, the more precise the estimate becomes.

we have the choice of sampling the whole space of genotypes, or of imposing any additional constraint on the genotypes, as long as v  is unaffected and the restricted v can be computed. we thus implemented in our software tool the ability to impose the constraint of working only with "compatible" sequences. we here use this ability. specifically, we force those bases which are paired in v to always be "compatible" i.e., the pairs a-c, a-g and c-u are not allowed. this constraint leads to a smaller sampling space in our nested monte carlo approach, and thus to a smaller statistical error.

since the sampling is performed via a markov chain, the successive genotypes are highly correlated, because they differ by only one mutation. one can observe these correlations very clearly via the distance between a genotype at mutation/swap cycle t and the genotype at cycle t+τ. these correlations are expected to persist on a time scale that is on the order of the number l of bases of the sequence. the reason is that each base should be mutated at least once, if the distances are to decorrelate completely; the inset of additional file  <dig> validates this expectation. clearly, a monte carlo run must be much longer than this decorrelation time, and even in that situation the statistical error analysis requires some care. for illustration, we display in additional file  <dig> the estimator of v as a function of the cycle number, using window averages. the signal is clearly noisy and on this time scale the short term memory  is invisible.

application to biological rna sequences
we next applied the nested monte carlo algorithm to  <dig> sequences of length  <dig> ≤ l ≤  <dig> in the functional rna database frnadb . the database does not provide curated structures, so we used the secondary structures predicted by the vienna package  <cit> . only computational limitations prevented us from studying a larger data set or a data set of longer sequences. we determined both neutral set sizes and p-values for secondary structures, where a structure's p-value is, as defined above, the fraction of structures with a larger neutral set. table  <dig> shows one representative from each functional category in this data set. it is evident that even for the relatively short sequences considered here, neutral set sizes are enormous. for example, there are more than  <dig> sequences forming the predicted structure of a snrna of pyrococcus abyssi . however, even the collection of all these sequences constitutes only a small fraction  of the vast sequence space. the computation of the p-value of this structure gives  <dig>  × 10- <dig>  this means that fewer than one in  <dig>   structures have a larger network than this structure. similar p-values arise for the other structures in table  <dig>  we note that the error estimates of both neutral set sizes and p-values are generally substantially smaller than the estimates themselves, that is, the relative error is small. in the supplementary material , we give the neutral network sizes for all  <dig> sequences examined.

an analogous analysis can be performed by comparing the neutral network sizes of biological structures to neutral network sizes of random genotypes. random genotypes adopt phenotypes whose neutral network sizes are larger than that of random phenotypes, because each phenotype is produced with a probability proportional to the size of its neutral network . in this analysis, we find that for random sequences of length l =  <dig>  the associated median neutral network size is  <dig>  ×  <dig> while the 90th percentile is  <dig>  ×  <dig>  thus the biological sequences we studied have larger neutral networks than random sequences, but the difference is less dramatic than for random phenotypes, and our sample sizes are too small to make statistical conclusions.

because of the different sizes of sequence spaces for different l, rank histograms like that of figure  <dig> cannot be produced for sequences mixing different lengths. however, p-values can be compared for such sequences, because their meaning is length-independent. figure 2b shows a histogram of logarithmically transformed p-values for all  <dig>  structures examined here. again, this larger data set also shows that biological structures have atypically large neutral networks when compared to random structures. the median p-value for all  <dig> structures is  <dig>  × 10- <dig>  with a 10th and 90th percentile of  <dig>  × 10- <dig> and  <dig>  × 10- <dig>  in sum, fewer than one in  <dig>  randomly chosen structures have more associated sequences than the typical biological rna structure in our data set. only one out of  <dig> structures has a p-value of greater than  <dig> , and only four have a p-value greater than  <dig> . figure 2c shows, for the same  <dig> structures, a histogram of neutral network sizes, expressed as fractions of sequence space. as in the above examples, the neutral networks of even such highly abundant structures span only a tiny fraction of sequence space. . this can be understood from the fact that even the set of sequences compatible with a secondary structure, which contains the neutral network, encompasses only a tiny fraction of sequence space  <cit> .

the mutational robustness of a sequence is the fraction of its neighbors that are neutral , or, equivalently, the fraction of mutations that leave a sequence's structure unchanged  <cit> . similarly, we can define the mutational robustness rμ of a structure as the mean mutational robustness of the sequences belonging to its neutral network. figure  <dig> shows how rμ depends on neutral network size. for the  <dig> biological rna sequences we examined, mutational robustness increases  with increasing logarithm of the neutral network size, ns. if we focus on structures of a given length l, this association is even stronger . the partial correlation coefficient between the two quantities  is r =  <dig>  . we also observe that as neutral network size increases by eight orders of magnitude , mutational robustness increases only modestly, i.e., by a factor of approximately two.

finally, given the computational cost of our nested monte carlo approach, it is reasonable to ask whether there are good indicators of neutral network size that are more easily computed. possibly the simplest candidate indicator is the number of paired bases in a structure. in line with the simple expectation that each base has a certain probability of being paired in a random structure, one finds empirically that the mean number of paired bases grows linearly with l. similarly, the entropy of a structure, defined thermodynamically as the logarithm of the neutral network size ns, is expected to grow linearly with l. as a consequence, to compare indicators of neutral network size across structures of different length l, it is useful to compare these quantities to their mean or median values. for that reason, we consider the association between log/l and the fraction of paired bases. we find a significant negative association . the more paired bases a structure has, the smaller is thus its neutral network. however, this association explains less than 40% of the variance of neutral network size . we note that omitting the length-normalization of neutral network size or the number of paired bases leads to even lower associations. previous work, partly based on artificial random graphs, partly based on genotype-phenotype maps of short sequences, points to reasons why such indicators have limited value  <cit> . it also indicates that the minimum free energy itself may be an indicator of the biological origin of a structure  <cit> .

recently, an easily computed contiguity statistic of neutral network sizes was proposed  <cit> . this indicator adds a structure's total bases in stem-loops to the number of paired bases, and divides this sum by the number of stacks. we find that this indicator is positively associated with neutral network size , an association that decreases if neutral network size is not length-normalized. the association explains a fraction r <dig> =  <dig>  of the variance. our observations above suggest that the biological rna phenotypes we examined differ very significantly from random phenotypes, which raises the possibility that the previous indicators may work better or worse for random rna sequences. we find that this is in fact the case. for example, in a random sample of  <dig> sequences of length  <dig>  spearman's r = - <dig>  for numbers of paired bases and log-transformed neutral network size, and spearman's r =  <dig>  for the contiguity statistic and log-transformed neutral network size . however, the fractions of explained variances are less than r <dig> =  <dig>  and r <dig> =  <dig> , respectively. in sum, rapidly computed indicators of neutral network size exist, but these indicators leave the majority of neutral network size variance unexplained.

discussion
the method we presented to compute neutral set sizes makes direct estimation of astronomically large neutral set sizes possible for the first time, but this ability comes at a cost. with currently available computational resources, the method can accurately estimate neutral set sizes for individual rna molecules up to length l =  <dig>  if one wants to estimate the relative abundance of an rna phenotype, this size reduces to l =  <dig> because one needs to estimate relative ranks from a sufficiently large sample of genotypes in the same sequence space, as we did in figure 2a. many functional rna molecules are substantially longer than that, so computational cost is currently a limitation.

in earlier work, an rna structure was called frequent if its associated neutral set had a size greater than that of the average neutral set  <cit> . using our notation, such a frequent structure has a p-value of p <  <dig> . the  <dig> biological structures from the functional rna database  <cit>  that we examined here are vastly more abundant than that. their median p-value of  <dig>  × 10- <dig> means that fewer than 1/p ≅  <dig>  structures are more abundant than the average biological structure. despite their atypically large neutral sets, these networks occupy only a very small  fraction of  <dig>  × 10- <dig> of sequence space. these observations show that a structure may both occupy a tiny fraction of sequence space, and have a huge neutral set. the reason is simply that sequence space is unfathomably large, and has enough space for an astronomical number of structures with enormous neutral sets. being atypically abundant and occupying a small fraction of sequence space are thus no contradictions. this would hold even more so for sequences longer than those we were able to study. when comparing neutral network sizes of biological structures to structures adopted by random genotypes, we found the biological structures to have somewhat larger neutral network sizes, but our sample sizes were too small to draw statistically sound conclusions.

why are structures of biological molecules not atypically rare? consider an evolutionary search in sequence space that is successful only if it discovers a sequence with a desirable structure, a structure that can be involved in some biological function beneficial to the organism. if both a rare and a frequent structure can satisfy these constraints, then the search will most likely find the frequent structure first. in other words, the abundance of biological structures suggests that solutions to problems that organisms face will be more readily found among abundant structures.

a high abundance of biological structures – if true generally – would have implications for the ability to find new structural variants starting from any one structure s. rare structures s have small neutral networks. their immediate neighborhood-defined as all sequences that differ by one nucleotide from a sequence on the network – will contain few structures different from s. in contrast, abundant structures have large neutral networks, in whose neighborhood many structural variants reside. if we accept that some small fraction of such variants may be novel structures beneficial to the organism – evolutionary innovations – then abundant structures may have an advantage in discovering such innovations, simply because they have access to more structural variants. a large neutral network may thus facilitate the production of useful phenotypic variation  <cit> .  in addition, it has been shown that populations of rna molecules which evolve under the influence of mutation and selection to maintain their structure, can spread more rapidly on a large neutral network. they thus gain access to a greater amount of structural variants in their immediate neighborhood  <cit> . all in all, structural abundance can facilitate the production of structural variation, as can other factors  <cit> .

our observations on average mutational robustness of rna sequences also speak to the importance of neutral network size. rna sequences with extremely high mutational robustness have few new structures in their neighborhoods  <cit> . one might thus think that rna phenotypes with large neutral networks would show such extremely high robustness. however, for the  <dig> structures we analyzed here, mutational robustness is modest and varies by a factor of less than two , whereas the corresponding neutral network sizes vary by more than fourteen orders of magnitude . a similar observation has been made previously in studies of random graphs that can be used as models for the rna genotype-phenotype relationship  <cit> . it suggests that a modestly reduced number of neighbors with different structures in large neutral networks is much more than compensated for by the vastly increased neutral network size observed in abundant structures.

some caveats to our findings are in order. first, while they suggest that many biological rna sequences may have abundant structures, it is clear that there are biological rna structures that are rare. the most prominent example is the simple stem-loop , which, unadorned by other structural elements, is a frequent regulatory motif, for example in translational regulation  <cit> . because its many paired bases constrain its sequence severely, it is a rare structure. second, although for some regulatory rna molecules, only the secondary structure may be important, many rna molecules may evolve under substantial additional constraints. consider, for example, the hammerhead ribozyme  <cit> , where some mutations that leave the secondary structure intact may completely abolish its biochemical activity; or the telomerase rna, whose interaction with telomerase is critical for telomerase function  <cit> . for such rna molecules, the set of mutations that do not abolish rna function will be substantially smaller than the set of mutations that preserve the secondary structure. however, even in that case, structures with a larger neutral network to begin with may tolerate more sequence change. third, for reasons of computational limitations, we have considered only a small sample of rna structures. the most prominent known functional rna structures are much longer than those we could study here, and it is an open question whether the same observations will hold for longer sequences. we hope that the method we propose here will help answer this question.

CONCLUSIONS
we here presented a method to estimate the size of the set of genotypes that adopt a given phenotype, and to estimate the size of this set relative to other such sets.

because the method is based on the nested monte carlo approach, it can estimate neutral set sizes even where these sets are disconnected. although we applied the method to rna molecules, the method is general and can be applied to different systems, such as proteins or biological networks, provided that two prerequisites are met. first, for the study system it must be possible to determine a phenotype from a given genotype. the number of genotype-phenotype maps where this is possible is increasing, and includes not only molecular phenotypes , but also phenotypes adopted by genetic networks  <cit> . second, a notion of distance among different phenotypes must exist. this is generally not a problem because such measures can be readily defined for phenotypes as different as protein structures and gene expression patterns. the overall computational framework may also be of use in other disciplines such as computer science or engineering.

competing interests
the authors declare that they have no competing interests.

authors' contributions
aw conceived the project, tj and ocm designed the algorithm, and tj implemented it; all authors analyzed the data and contributed to writing the manuscript. the authors have read and approved the final manuscript.

supplementary material
additional file 1
the effect of our sampling procedure. the horizontal axis shows neutral network sizes, the vertical axis shows p-values determined in two different ways, for all  <dig> structures adopted by sequences of length  <dig>  for molecules this short, all sequences can be enumerated, and neutral network sizes, as well as p-values can thus be determined exactly . grey, open circles with error bars indicate estimates obtained for m =  <dig> sequences through the nested monte carlo method with our sampling procedure. as discussed in the main text, the biased sampling procedure preferentially identifies structures with large neutral networks. this is reflected in the higher accuracy of our estimates for large neutral network sizes , which are most relevant to the analysis of biological rna molecules .

click here for file

 additional file 2
cycle to cycle correlations in the markov chain procedure. variation in estimated neutral network sizes during  <dig>  mutation/exchange cycles for a  <dig> nt hammerhead structure ")))).......))))...)))))))" involved in the self-cleavage of peach latent mosaic viroid. data is plotted every  <dig> cycles and shows that correlations arise only on short time scales. the horizontal line indicates the mean of  <dig>  ×  <dig> over the entire window shown. the inset shows the autocorrelation function c of genotype distances at cycle t and t+τ:  <dig> cycles is enough to lose memory of the preceding genotype. thus, the markov chain explores efficiently all genotype space.

click here for file

 additional file 3
the  <dig> biological rna molecules used in this study. none.

click here for file

 acknowledgements
this work was supported by the sixth european research framework . aw was supported through snf grant 315200- <dig> 
