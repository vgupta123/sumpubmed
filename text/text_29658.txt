BACKGROUND
one of the central objectives in molecular biology is to characterize all cellular processes controlling gene regulation. the complex interaction between dna chromatin structure and transcription factors is one of these key processes. the basic unit of chromatin structure is the nucleosome, which is composed of ≈  <dig> base pairs of dna wrapped around a protein complex of eight histones. loosely speaking, the more compact the chromatin, the harder it is for transcription factors and other dna binding proteins to access dna and trigger transcription. thus, to elucidate the role of interactions between chromatin and transcription factors, it is crucial to determine the location of all nucleosomes along the chromosomes.

several experimental techniques are available to produce genome-wide nucleosome maps. for instance, one can isolate the portions of dna that are free of nucleosomes or enrich for genomic regions that are bound to histones. the latter can be achieved via micrococcal nuclease digestion   <cit> , which can be combined with chromatin immunoprecipitation  to enrich for a particular subset of nucleosomes , typically followed by high-throughput sequencing . in this work, we assume that the sequencing data is either mnase-seq or chip-seq, which are currently the most popular approaches to study the locations of nucleosomes and histone modifications.

an analysis of the literature reveals that the majority of nucleosome maps have so far been produced from single-end reads . as a consequence, nearly all computational methods available assume that the input data are single-end reads. nucleosome positioning from single-end reads is, however, more computationally challenging and much less precise than if paired-end data was available. paired-end reads allow one to determine both ends of nucleosome-enriched dna fragments, whereas with single-end reads one either obtains one "boundary" or the other. in the latter case, the problem of associating a peak in the forward strand with the correct peak in the negative strand can be difficult, in particular for complex nucleosome configurations.

existing methods for single-end reads either rely on the assumption that nucleosome-enriched dna fragments are expected to be of a size compatible with the nucleosome , or use probabilistic models to estimate these sizes from the data. from our experience, the first approach can lead to poor results because there is no fragment size that will work equally well for all nucleosomes in the genome. while one would expect nucleosome-enriched dna fragments to be about  <dig> bp, in mnase-seq the digestion process can either leave nucleosome-free dna in the sample, or "over-digest" the ends of nucleosome-bound dna. furthermore, the rate of digestion is sequence-dependent  <cit> , so nucleosomes in different genomic locations can end up with different dna fragment sizes.

despite these challenges, the majority of so-called "peak-calling" approaches usually rely on the assumption that the data is derived from nucleosome-sized dna fragments and consist of following steps:  a nucleosome occupancy score function is obtained from mapping nucleosome-enriched reads to the reference genome, followed by counting, smoothing and normalization;  candidate nucleosomes are placed according to the peaks of the score function;  the final set of nucleosomes is selected to satisfy additional constraints . to compute the occupancy score, different techniques have been proposed, ranging from simply computing the number of reads covering each genomic location, to sophisticated statistics to estimate the false discovery rate. for instance, nucler  <cit>  uses the raw coverage with extensive "profile cleaning" based on the fourier transform, whereas nseq  <cit>  employs a triangle statistic based on read counts within a sliding window.

a second group of methods is based on probabilistic models. our tool normal  <cit>  uses a modified gaussian mixture model to infer nucleosome-enriched fragment sizes. the parametric probabilistic model allows to deal with the problem of overlapping and complex configurations of nucleosomes. developed in parallel with normal, ping  <cit>  employs a similar probabilistic model. both tools provide a clear advantage over algorithms that rely on the user to provide estimated dna fragment sizes.

finally, a distinct group of positioning methods depend on the availability of a control track , e.g., nucleofinder  <cit> , while others have been designed to perform differential nucleosome positioning, e.g., danpos  <cit>  and dinup  <cit> .

in this work, we focus on the problem of determining nucleosome positions based on the availability of paired-end reads . to the best of our knowledge, nucpossimulator  <cit>  is the only published tool specifically designed to take advantage of paired-end reads: to place nucleosomes it solves the optimization problem of selecting the subset of peaks which maximizes the total score, under the constraint that these peaks are located at the expected nucleosome distance from each other. our tool puffin  instead uses a novel multi-resolution approach: while its algorithm is relatively simple, our approach introduces some novel ideas that have the potential to be useful in other domains of genome analysis.

methods
our method consists of three steps:  we build a set of nucleosome profiles and nucleosome "landscapes";  we detect candidate nucleosome locations on each profile;  we select a "consensus" set of nucleosomes that satisfies non-overlapping constraints. we discuss these steps in detail in the next subsections.

computing nucleosome profiles and nucleosome landscapes
we first map sequenced reads to the reference genome and then compute a nucleosome profile that represents the likelihood that a genomic location is occupied by a nucleosome. candidate nucleosomes are detected at the peaks of the nucleosome profile. in order to reduce false positives, profiles have to be cleaned from their high frequency component. choosing the best smoothing method  is, however, not easy. for instance, in  <cit>  the authors show that the kernel density estimation method  <cit>  works significantly better than moving average-based smoothing. the choice of kernel parameters is also important: too much smoothing can merge adjacent peaks, too little can leave noisy artifacts that can be interpreted as peaks and thus introduce spurious nucleosomes. to address the challenges of choosing the "right" kernel and smoothing parameters, we follow an alternative  procedure to construct nucleosome profiles.

first, we replace each mapped paired-end read i with a function fiαwi distributed as a gaussian with mean µi and standard deviation αwi, i.e.,

 fiαwix=1αwi2πe-x-μi2αwi <dig> 

where µi is the genomic center location of read i, wi is the length of read i , and α is a smoothing parameter. replacing each mapped read with a gaussian distribution allows us to model probabilistically the uncertainty in the paired-end mapping. for instance, when the left and right mate are mapped far from each other, the mass of the gaussian will be distributed on a longer interval because of its large variance. if instead the left and right mate are close to each other, the gaussian will have its mass concentrated at the center of the read, indicating a higher confidence in the nucleosome position.

then, we compute the nucleosome profile sα as the weighted sum of functions fiα for all the mapped reads in the input

 sα= ∑i=1nβifiαwi 

where n is the number of mapped reads in input, and βi is the weight of the read i. if we had employed a uniform weighting scheme βi=1n, paired-end reads with very short insert would dominate the profiles. to reduce the effects of short dna fragments, we use a non-uniform weighting scheme. for paired-end reads that are shorter than 146bp, we assign a penalty factor γ < <dig>  such that the shorter the read is, the less the weight is ). additionally, one could use the weights βi to account for sequence quality of individual reads, mappability biases, etc.

as said, parameter α controls the smoothness of function sα. the bigger α is, the smoother sα  is, , and vice versa. when α is large, we capture nucleosome binding preferences at a lower resolution scale; when α is small we can detect nucleosomes at a high resolution scale . in the limit α →  <dig>  function sα→∑i=1nχ, where χ= <dig> x= <dig> x≠ <dig> is the indicator function. in this case, s <dig> represents how many read centers cover location x in the genome.

one might think that one could obtain the same profiles by computing the read coverage function smoothed by a gaussian kernel. there is, however, a significant difference: the size of each mapped read independently influences the shape of sα , while in the case of kernel smoothing the impact of read sizes becomes less and less important as the smoothing strength increases.

since we do not know the appropriate value for α for the data, in this step we generate a family of functions for several choices of α. formally, we create a set of m functions sαkk= <dig> ,…,m=sα <dig> sα <dig> …,sαm, where α <dig> < α <dig> < . . . < αm are m distinct choices for α. the value m is hard-coded in our implementation .

the set of functions sαkk= <dig> ,…,m enables our algorithm to detect candidate locations for nucleosomes at different resolution scales, thus eliminating the need to specify in advance the parameters for the range of nucleosome-enriched fragments. in other words, our algorithm can "adapt" to the local properties of the input data by processing the same location at different resolutions .

finally, we compute a set of nucleosome landscapes nαkk= <dig> ,…,mby normalizing each function sαk by the lowest resolution function sa, as follows

 nαk=logsak+εsa+ε 

where ε > <dig> is a small constant to avoid a division by zero, and a >maxk =  <dig> ,...,m αi. in our implementation we pick α to range from  <dig>  to  <dig>  and a =  <dig> . since mappability biases affect each function sαk, we can effectively reduce these biases by taking the log ratio of high-resolution and low-resolution function. another reason to carry out this normalization step is to reduce the differences in the peak heights.

to illustrate the multi-resolution approach in our algorithm, we created a small synthetic dataset with four nucleosomes shown in figure  <dig>  panel a shows the raw coverage obtained by mapping synthetic paired-end reads to the reference genome. observe that nucleosomes i,iii and iv are strongly positioned, while nucleosome ii is "fuzzy". fuzzy nucleosomes are quite common and occur when a subset of the cells in the sample has a nucleosome at one location, while in the other subset the same nucleosome is slightly shifted. nucleosome i is isolated, while nucleosomes iii and iv are located very close to each other. panel b shows the family of functions sαk for three choices of α; panel c illustrates the set of nucleosome landscape functions nαk. observe in figure 1c that the transformation amplifies candidate peaks in areas with low coverage and reduces the amplitude of peaks in regions with high coverage.

detecting candidate nucleosomes
by construction, a nucleosome landscape nαk represents a non-parametric distribution of nucleosomes at resolution αk . the presence of a peak in any nucleosome landscape indicates a candidate nucleosome. the reads that form corresponding peak belong to that candidate.

a peak is defined by a pair  where q is the center of the peak and s is the width of the peak. we say that  is a peak for function n when n  is local maximum for n and s = minz  where z is any local minimum for function n .

detecting peaks on each function nαk can be easily computed in linear time along the length of the genome. as a result, for every choice of αk, k =  <dig>   <dig>  . . . , m we have a set of peaks {pk <dig> pk <dig> …,pkl}, where pkj is a pair  representing the peak, and l is the number of peaks.

peaks are however not guaranteed to have a symmetric shape. we therefore recompute the location of every nucleosome candidate as the centroid location of its read midpoints. this additional step ensures that candidate nucleosome locations properly represent the corresponding input reads.

building the final solution
we now explain how to build the final set of non-overlapping nucleosomes from the family of peak sets pαkk= <dig> …,m. we say that two peaks  and  overlap if |q <dig> − q2| < <dig> . observe that by construction, the number of peaks detected at lower resolution  will be smaller than or equal to the number of peaks detected at higher resolution, i.e., |pα| ≤ |pβ| when α > β. as we increase the smoothing parameter α, the total number of peaks decreases: while some peaks are preserved, others are merged. in other words, for every peak in pα we can find at least one corresponding peak in pβ if α > β.

based on this observation, we build the final set of non-overlapping nucleosomes c as follows. given a family of peak sets pαkk= <dig> …,m where α <dig> < α <dig> < . . . < αm, we process each peak set pαk in increasing order for α. we add a peak p from the current set pαk to the final solution c if p does not overlap with any other peak in the set pαk and if p does not overlap with any other peak already in c. a sketch of the algorithm can be found in figure  <dig> 

let us consider again our example in figure  <dig>  detected peaks are marked with circles in panel c. the algorithm first processes the set of peaks on the blue function . since there are no peaks on that curve that are located at a distance greater than 146bp from each other, the final set c remains empty. next, the algorithm processes the green curve : here there are three peaks that satisfy the non-overlapping constraint. thus, the algorithm adds those peaks  to c. then, the algorithm considers the red curve : all four peaks are non-overlapping with each other, however only one peak  can be added to c. as a result, the final solution c consists a set of four peaks that match the original nucleosomes. observe that strongly positioned nucleosomes i, iii and iv are detected earlier in the algorithm  than fuzzy nucleosome ii .

running time
to compute a set of profile functions sα we use a precomputed set of curves fiαw for every choice α and w in a predefined range. as a result, it takes Θ operations, where n is the number of reads and m is the number of curves. in our implementation we used m =  <dig> choices of equally distributed values for α ∈ .

finding peaks on each curve sα takes Θ time, where l is the length of the processed region. thus, the total time to find candidate nucleosomes  is Θ). building the resulting set of non-overlapping nucleosomes is determined by the number of candidates that is at most Θ. given that m is predefined, it follows that the total running time is linear in the region size and number of input reads.

experimental 
RESULTS
to evaluate the performance of puffin, we performed extensive benchmarking against nucpossimulator, template filtering and normal. nucpossimulator is the only published tool designed to deal with paired-end reads  <cit> . as said, it solves the optimization problem of selecting the subset of peaks which maximizes the total score, under the constraint that these peaks are located at the expected nucleosome distance from each other. template filtering is one of the first algorithms developed to infer the size of the fragments from single-end reads  <cit> . normal uses a modified gaussian model to cluster input single-reads such that every cluster represents a nucleosome  <cit> . some of the recently published tools that use a control sample to solve the nucleosome positioning problem, e.g., danpos and nucleofinder , are not included in this comparison.

we used default parameters for each tool except for the following provisions. for template filtering and normal we set to zero the allowed overlap between adjacent nucleosomes to allow for a fair comparison with puffin and nucpossimulator.

arguably the major challenge for nucleosome position inference is that the true positions of nucleosomes are unknown. the lack of a "ground-truth" makes it very hard to benchmark existing computational methods. for this reason we made extensive use of synthetic data, as explained next.

results on synthetic data
we started by producing a small dataset of reads corresponding to dna-enriched fragments for only one nucleosome . this allowed us to investigate the behavior of these various tools in the scenario of low sequence coverage in a region containing a fuzzy nucleosome. nucleosome i is centered at 300bp and the paired-end reads of size 146bp were generated with midpoints distributed according to gaussian with mean  <dig>  and standard deviation  <dig>  to simulate a low coverage scenario, we generated only twenty sequence reads . puffin, template filtering and normal report one nucleosome located at 308bp, 311bp and 292bp, respectively, while nucpossimulator reports two nucleosomes positioned at 221bp and 369bp. the slight difference of the reported locations for the first three tools could be explained by the small sample size that is insufficient to recover the true location. interestingly, the first two methods, which are based on peak-detection, produced a similar close right shift, while the nucleosome detected by normal showed a small left shift. nucpossimulator detected two distinct nucleosomes, probably because the objective of this tool is to maximize the total score of reported nucleosomes. we believe that maximizing this quantity has the undesirable effect to over-report nucleosomes . decreasing the smoothing parameter in nucpossimulator from  <dig>   to  <dig>  reduces the output to a single nucleosome, again demonstrating how the choice of smoothing parameters can have significant effects on the results.

next, we performed a more realistic comparison on in silico reads for larger synthetic nucleosome maps. we used the nucleosome map generator syntheticnucmap from nucler  <cit> . this tool allows users to specify the number of well-positioned and fuzzy nucleosomes, as well as the variance for the location of synthetic reads and the coverage level. well-positioned nucleosomes are placed along the chromosome regularly spaced with a fixed linker size . for fuzzy nucleosomes, locations are picked at random and independently from other nucleosomes already on the chromosome. as a consequence, fuzzy nucleosomes can overlap with other nucleosomes. for the variance parameter we choose  <dig> bases for well-positioned and  <dig> bases for fuzzy nucleosomes.

our objective was to investigate the accuracy of nucleosome detection as a function of the fraction of fuzzy nucleosomes: we expected the detection problem to become increasingly harder as the number of fuzzy/overlapping nucleosomes increases. for each percentage level of fuzzy nucleosomes  we generated ten datasets of synthetic reads for a map containing  <dig>  synthetic nucleosomes. to build these datasets, we used the following command: syntheticnucmap, wp.var =  <dig>  fuz.num=, fuz.var =  <dig>  max.cover =  <dig>  nuc.len =  <dig>  lin.len = 20), where r controls the fraction of fuzzy nucleosomes . for each group of ten datasets we measured the number of reported nucleosomes and the accuracy of each tool, and reported the average and standard deviation over the ten sets. to measure the accuracy, we calculated the distances between the true nucleosome location and the center of the corresponding detected nucleosome. results in figure  <dig> show that puffin reports nucleosome positions more accurately in datasets with larger proportions of fuzzy nucleosomes. in addition, figure  <dig> shows the average number of nucleosomes detected by the various tools for increasing percentages of fuzzy nucleosome . first observe that although each dataset is expected to have synthetic reads for exactly  <dig>  nucleosomes, this is only true for datasets with no fuzzy nucleosomes. since fuzzy nucleosomes may overlap other nucleosomes, we expect to detect a decreasing numbers of nucleosomes as the percentage of fuzzy nucleosomes increases . also observe in figure  <dig> that in datasets with more than 20% of fuzzy nucleosomes, nucpossimulator detects the highest number of nucleosomes compared to other tools. however, as we demonstrated earlier in figure  <dig>  nucpossimulator can over-report nucleosomes. to explore whether this was true on these larger datasets, we computed the distribution of distances between adjacent nucleosomes . in the group of datasets with no fuzzy nucleosomes, both nucpossimulator and puffin have strong peak at around 167bp location and 334bp. this is expected, because all nucleosomes are well-positioned and are located at multiples of 167bp. however, as we increase the percentage of fuzzy nucleosomes in the datasets, nucpossimulator reports more and more nucleosomes exactly  <dig> bp apart from each other, which suggests that its strategy to maximize the total score for reported nucleosomes has the effect of reporting too many nucleosomes.

to eliminate the effects of over-reporting in nucpossimulator, we discarded from the counts nucleosomes that are located  <dig> bases or less from each other, such that every pair of tightly placed nucleosomes is count as one nucleosome. in figure  <dig>  curves marked "filtered" shows the results of this cleaning step. observe that the number of nucleosomes reported by nucpossimulator drops significantly, while only a small number of puffin nucleosomes are affected. in fact, using this cleaning step, puffin reports a larger numbers of nucleosomes than nucpossimulator. all together, these experimental results on synthetic data show that puffin generates more accurate nucleosome maps, without over-reporting nucleosomes.

results on real data
for the comparison of nucleosome positioning tools, we used a publicly available dataset for s. cerevisiae  and our dataset for p. falciparum . all datasets contain paired-end reads produced by an illumina sequencing instrument. reads were mapped to their corresponding reference genomes using bowtie <dig>  <cit>  with --very-fast-local --no-discordant flags. we removed reads that were not mapped uniquely or had a distance between the left and right mates smaller than 40bp or bigger than  <dig> bp.

experimental results are summarized in table  <dig>  which include the number of reported nucleosomes and the execution time. nucleosome positioning in s. cerevisiae is extensively studied and the majority of the tools perform well on this organism. also, nucleosomes in yeast are well-positioned and not many overlaps are present. the results in table  <dig> show that the number of nucleosome reported in yeast by these tools are quite similar, except for nucpossimulator that reports a significantly larger number. these results possibly again suggest the over-reporting behavior of this tool.

our previous work  <cit>  has demonstrated that the p. falciparum genome has a greater complexity of nucleosomes configurations. as expected, experimental results show much greater variance in the number of nucleosomes in the malaria dataset reported by the various tools. puffin reports a similar number of nucleosomes compared to nucpossimulator, but significantly higher numbers than normal and template filtering, indicating that our method is capable to resolve complex configurations of nucleosomes.

the execution time of puffin is higher than normal and template filtering on both datasets, but shorter than nucpossimulator on p. falciparum and higher on s. cerevisiae datasets. our implementation of puffin is currently written in python, while the other tools use either java or c/c++. we believe that speed of our tool could be easily improved by one order of magnitude by implementing it in c/c++ .

to investigate the sensitivity of the tools on the quantity of the input data , we performed an experiment in which an increasing fractions of the input reads were discarded. specifically, we sampled the p. falciparum dataset by randomly selecting a given fraction of the input reads  and ran the four tools on the resulting datasets. subsamples have 7x, 14x,. . . , 63x fold coverage. figure  <dig> shows that the performance of puffin degrades monotonically as the quantity of the data decreases, while nucpossimulator remains more stable over a larger range of input data. we therefore recommend to use sequence data with a minimum of 30-fold for the analysis of nucleosome positions if puffin is used.

CONCLUSIONS
we described a novel method to solve the nucleosome positioning problem when paired-end data is available. our method employs a multi-resolution strategy that circumvents a smoothing step that usually requires user-defined parameters to set the strength of the smoothing and type of kernel to be used. experimental results show that our method more accurately detects nucleosome positions as compared to existing software tools, in particular when complex nucleosome configurations are present in the data.

competing interests
the authors declare that they have no competing interests.

authors' contributions
klr and sl conceived and supervised the project; ap designed, implemented, tested and debugged puffin; emb generated experimental nucleosome positioning datasets in p. falciparum; ap performed all computational analyses; ap, sl, and emb wrote the manuscript. all authors read and approved the final manuscript.

