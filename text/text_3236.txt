BACKGROUND
metabolomics is a newly established omics-discipline widely used in systems biology. by targeting metabolites as substrates, intermediates and products of metabolic pathways, it has been successfully applied to explain observed phenotypes  and to monitor changes in cells in response to stimuli  <cit> . while targeted metabolomics focuses on a chosen set of metabolites  <cit> , non-targeted studies aim at the simultaneous and relative quantification of a wide breadth of metabolites in the system investigated . the latter approach demands multi-parallel analytical technology, including ultrahigh resolution mass spectrometry  in direct infusion  and/or linked to chromatography or electrophoresis, as well as nuclear magnetic resonance , in order to achieve complete experimental coverage  <cit> . the spectra obtained from the different samples generated from each of these platforms are usually aligned in an intensity matrix whose rows correspond to samples and columns of overlapping chemical signals. this matrix allows the simultaneous study of mass spectra.

previous studies have used various statistical learning methods on such data matrices to reveal differences between classes of samples and to isolate chemical signals specific to a certain class or trend  <cit> . in the context of non-targeted metabolomics, the reliability of these multivariate methods might suffer from the curse of the dimensionality problem  <cit> . this problem arises when datasets contain too many sparse variables  and very few samples . making a statistical model conform closely to such datasets with a limited number of training samples could result in loss of predictive power . from another angle, since non-targeted techniques capture inegligible chemical noise and experimental bias, it may be difficult for a mathematical model to properly isolate the structure of interest  <cit> . therefore applying statistical learning requires intensive method selection and validation work .

indeed, it is recommended to apply various learning algorithms in the same study to improve the reliability of the information extracted  <cit> . one common way of doing this is to use unsupervised learning  prior to supervised methods , since basic data structure is revealed through simple dimension reduction, unbiased by the target information. the goal of such a non-hypothesis driven technique is to detect underlying structures relevant to the information expected, or to unnoticed subgroups, bias and noise  <cit> . it allows better understanding of how the non-targeted approach reflects each link of a biological experiment.

in our study, an unsupervised learning algorithm, i.e. independent component analysis , is applied to enlarge the feature discovery in comparison to classical principal component analysis . currently, the concept of ica is widely used in high-dimensional data analysis such as signal processing of biomedical imaging  <cit>  and transcriptomics research  <cit> . recently several applications in targeted  <cit>  and low-resolution non-targeted metabolomics have achieved the goal of feature extraction  and functional investigation  <cit> . to apply ica we assume that the data observed x  are linear combinations of unknown fundamental factors or sources s, independent of each other . matrix a describes the linear combination. the sources are estimated by searching statistical components that are as independent as possible. compared to pca, ica as a linear method could provide three potential benefits for non-targeted metabolomics:fig.  <dig> matrix decomposition in non-targeted metabolomics. a
x is an aligned data matrix from mass spectra of all the samples studied. the goal of ica is to decompose x to a matrix s which contains independent sources and matrix a describes the linear mixture of theses source. b one row of x: the mass spectrum of one studied sample. c one column of x: aligned mass peaks for an annotated compound. d one independent source is plotted against another. the distribution of samples can be seen in the space described by these two sources. e represents the contribution of metabolites to these sources 

more meaningful components would be extracted by optimizing independence condition instead of variance maximization in pca  <cit> .

independence conditions detected by ica involve both orthogonality  and higher-order independence , while classical pca only ensures orthogonality between components. therefore ica could potentially extract additional information from the dataset.

since non-targeted metabolomics data usually contain huge numbers of variables and only a few samples, certain techniques using nonlinear mapping could result in computational complexity and overfitting  <cit> . another drawback of such techniques is the difficulty of mapping the extracted component back in the data space. as a method based on simple linear hypothesis, ica not only reduces the risk of overfitting but also allows the reconstruction of data in the original space.



however, major concern with ica algorithms is stochasticity. most ica algorithms try to solve gradient-descent-based optimization problems such as the maximization of the non-gaussianity of source s , minimization of mutual information  <cit>  and maximum likelihood estimation  <cit> . the randomness due to the fact that the objective function can only be optimized  locally depending on the starting point of the search . thus, outputs will not be same in different runs of algorithms if the algorithm input is randomized. the curse of dimensionality makes the situation more complicated in the case of high-dimensional signal space as in non-targeted metabolomics data: it is extremely unlikely that the local minima obtained from one algorithm run will be the desired global minima and they should be interpreted with great caution.

a parameter free, bayesian, noisy ica algorithm has recently been developed to model the stochasticity in targeted metabolomics  <cit> . by applying prior distributions to a, s and noise Γ, bayesian ica estimates the posterior distribution of s iteratively through a mean-field-based approach  <cit> , then a & Γ using a maximum a posteriori  estimator. the algorithm also suggests an optimal component selection strategy based on the bayesian information criterion . however, tests of this algorithm on non-targeted datasets present several uncertainties: firstly, it is hard to decide on the types of priors for a and Γ in a non-targeted study since the dataset reflects the complexity of the study and has multiple manifolds; besides, the performance of the mean-field-based approach is doubtful if it cannot be compared with a full monte carlo sampling ; in addition, bic maximization is usually impossible for high dimensional datasets with a reasonable amount of components.

therefore we developed a heuristic method based on the fastica algorithm and hierarchical clustering. the method, named metica is based on the icasso algorithm used in medical imaging studies  <cit> . we start with data pre-processing, including centering and dimension reduction, for which a classical pca was used  <cit> . the fastica algorithm is run many times on the pca score matrix with m different inputs, generating many estimated components. close estimates give birth to a cluster. the reliability of the fastica algorithm can be reflected by the quality of clustering. moreover, as with any statistical method, it is necessary to analyze the statistical reliability  of the components obtained. in fact, a relatively small sample size can easily induce estimation errors  <cit> . bootstrapping original datasets and examining the spread of the sources estimated might identify these uncertainties. both reliability studies would help to decide the optimal number of components. in addition to the adaptation of the icasso algorithm in non-targeted metabolomics, the novelty in the present study is the dual evaluation of algorithmic and statistical reliability for model validation. another novelty is the automatic ordering of extracted ics based on statistical reliability instead of only on kurtosis, as is done in other studies  <cit> . finally, our metica could be used for routine validation and interpretation of ica in non-targeted metabolomics.

methods
metabolomics data acquisition and pre-treatment
non-targeted metabolomics data were obtained from a di-ms platform: a bruker solarix ion cyclotron resonance fourier transform mass spectrometer  equipped with a  <dig> tesla superconducting magnet  and an apolo ii esi source  in negative ionization mode. mass spectra of each sample were acquired with a time domain of  <dig> mega words over a mass range of m/z  <dig> to  <dig> . the technique has ultrahigh resolution  and high mass accuracy . after de-adduction and charge state deconvolution, mass peaks were calibrated internally according to endogenous abundant metabolites in dataanalysis  <dig>   and extracted at a signal-to-noise ratio  of  <dig>  the peaks extracted were aligned within a 1 ppm window and generated a data matrix. each row represents the intensity of one mass signal in each sample . masses found in less than 10 % of samples were not considered during further data analysis and other absent masses were set at zero intensity in the sample concerned. we applied the software netcalc developed in-house to remove potential spectral noise and isotope peaks. this software also unambiguously annotates the elemental formula assigned to the aligned m/z based on a mass difference network  <cit> . the annotation process is considered as an unsupervised filtration that reduces data size and reveals an underlying biochemical network structure inside the data set. our ica algorithm is applied on this filtered data matrix.

biological studies
we applied the non-targeted approach followed by the ica algorithm in a comparative study of metabolic footprinting of randomly-selected yeast strains. the goal is to detect underlying yeast phenotype subgroups based solely on their exo-metabolome in wine  <cit> . to reach this goal, fifteen commercial saccharomyces strains  were chosen to perform alcoholic fermentation  triplicates in the same chardonnay grape must. the strains chosen were different in species  and in origin  to ensure phenotype diversity. we kept the fermentation conditions consistent  between strains and replicates. at the end of af , methanolic extracts of  <dig> samples were studied on the icr/ft-ms platform with the method described in the section "metabolomics data acquisition and pre-treatment". we randomized the order of strains for the fermentation experiment and for the non-targeted study. the resulting data matrix "yeast-experimental.txt"  had n =  <dig> rows  and p =  <dig> columns . prior knowledge about yeast strains according to the yeast producer, including basic genetic traits, fermentation behaviors and wine characteristics, will be used for component interpretation and method validation.

application of metica algorithm
we provide a concise overview of metica for non-targeted metabolomics . the algorithm was mainly implemented in r version  <dig> . <dig> fig.  <dig> each step of metica




pca-denoising
pca is done by a singular value decomposition  of the centered data matrix x¯. the denoised matrix xd is obtained by xd=x*k, where k is the k first pcs of loading matrix, obtained from the prcomp function in the script metica_fastica.r . working on xd preserves 90 % of the relevant information and reduces the potential noise given by 10 % of variance.

fastica algorithm
the functions ica.r.def  and ica.r.par  from the r package fastica, version  <dig> - <dig> , were applied to the denoised matrix xd . the goal of the fastica algorithm is to very rapidly estimate w or the demixing matrix. based on a fixed-point iteration schema  <cit> , w^ is estimated to maximize the approximated negentropy under the constraint of orthogonormality. the estimated source is calculated by s^ = xd ∗ w^. several rules concerning input parameters are followed while running the algorithms multiple times on xd:the number of ics is set to be the same as the number of pcs chosen for denoising.

the hyperbolic logcosh function is fixed for negentropy approximation as a good general purpose contrast function  <cit> .

the script metica_fastica.r contains two methods of extracting more than one ic: ica.r.def  and ica.r.par . 'deflation' avoids potential local minima  <cit> , while 'parallel' has the power to minimize mutual information between sources  <cit> . therefore each method is responsible for half of the runs.

the matrix w <dig>  which is the initial point of each run, is arbitrarily sampled from a gaussian distribution . other random distributions were tested and no big changes were observed for extracted components.



dissimilarity matrix
the pipeline presented in fig.  <dig> is achieved in metica_source_generator.r and metica_cluster_generator.r . each run of fastica generates an estimated source matrix sl^ containing k components. these k components can be similar to a certain extent. if we combine these sl^ in a large estimated matrix s^ , the similarity between the components from different runs can be described by spearman’s correlation coefficient. in order to perform further clustering analysis, each coefficient rij is transformed into distance or dissimilarity by dij=1-|rij| according to  <cit>  .

hierarchical clustering
an agglomerative hierarchical clustering analysis  is performed on the dissimilarity matrix d with r function hclust . the results display a tree-like dendrogram  for the hierarchical data structure: more similar components agglomerate to form a cluster and multiple clusters form a larger as a function of inter-cluster distance  <cit> . an average-link  agglomeration method was chosen as in the original algorithm, icasso  <cit> . based on the hierarchical data structure, it is possible to obtain a reasonable number of clusters by cutting the dendogram at certain dissimilarity levels . in this way, all k*m components are partitioned into a certain number of groups. compact and well-separated clusters reveal the convergence of the fastica algorithm. the representative points or 'centrotype' of each cluster is the point that has the minimum sum of distances to other points in the cluster . these points are considered as convergence points of fastica and deserve further study. therefore it is crucial to decide on the number of partitions providing the highest-quality clusters in terms of algorithmic convergence and statistical significance. some validation strategies will be presented in the results and discussion section.

production of simulated data
to confirm the power of the metica algorithm, a simulated data sx was generated to mimic the real non-targeted metabolomics data. the visual illustration of this process is in  and the function used was in metica_simulated_generator.r . from the centered yeast metabolic footprinting data x¯, a multivariate gaussian background noise n was created to have the same covariance as x¯. in parallel, we performed a simple pca and used non-gaussian pcs  to reconstruct a matrix, rx. the simulated sx is the sum of i∗n and rx, wherein i is a real number controlling the level of noise. the simulated data for i =  <dig>  was stored in yeast-simulated.txt .

RESULTS
diagnostics of simulated and experimental data
the fastica algorithm is based on the maximization of negentropy, an exact measure of non-gaussianity. it is equivalent to the minimization of mutual information, or searching independent components  <cit> . the algorithm only works when the dataset is derived from non-gaussian sources and thus contains non-gaussian features. therefore we measured the non-gaussianity of each mass using kurtosis . the distribution of kurtosis for the experimental data showed a significant amount of super-gaussian  and sub-gaussian  variables, while the background matrix n mainly contained gaussian variables . the simulated matrix sx contained a large number of super-gaussian variables, knowing that two super-gaussian pcs  were used for generation . since both experimental and simulated datasets displayed non-gaussian features, we were able to apply metica to these datasets.

performance of metica on simulated data
the metica was first tested on simulated data. the performance was evaluated based on whether the algorithm was able to retrieve the signals  used for generation. different combinations of non-gaussian pcs were used to generate the simulated data and evaluate the algorithm. the following is a simple example from different sxs generated by pc <dig>  and pc <dig>  with three levels of noise . we applied metica to sx in the way described in the previous section. the objective here was to find the optimal number of partitions for metica estimated sources. with this number, we expected to obtain high-quality clusters from hca, with two of them representing the pcs used for generation. our strategy started with the visualization of all the estimated sources  after projection onto a 2d space. a reliable projection should preserve the distance between estimated sources and hierarchical clusters should only contain neighboring points. according to our tests, curvilinear component analysis  outperformed multidimensional scaling  and the self-organizing map  for this purpose. in fact, cca preserved the distance better and gave more explicit visual separations between clusters. in order to examine the hca results in the 2d space, the executable program metica_cca.exe  assigned randomly different colors to the sources belonging to different clusters. we could monitor cluster splitting by increasing the number of clusters  until we obtained compact, well-separated clusters . apart from visual monitoring, we applied a quality measure to decide the optimal number of partitions. the index is simply the ratio between the average within-clusters distance and the between-clusters distance . the smaller the index is, the more compact and better separated the clusters seem to be on the 2d space. at the beginning this index decreases as a function of the number of clusters. from a certain point, it tends to be stable or increases, meaning that adding another cluster does not much improve the data modeling. the decision regarding the optimal number of clusters via this index is consistent with visual monitoring .fig.  <dig> feature extraction from simulated data. a, b, c, distribution of estimated metica sources  when projected on a 2d cca space. sources belonging to the same hierarchical cluster have the same color. d the sample distribution on pc <dig> and pc <dig> used for sx generation: samples  corresponding to fermentation triplicates of the same strain are connected to their gravity center . e the sample distribution of the centrotypes of the red cluster and blue cluster. for any background noise level tested, the centrotypes of these two clusters carry the same strain rankings as pc <dig> and pc15



after the optimal number of clusters was chosen, centrotypes of clusters were verified by comparing to components used for data generation . for all three noise levels tested, pc <dig> and pc <dig> can be described by the centrotypes of red and blue cluster, respectively . in other words, metica was able to retrieve both pcs from the simulated data at different levels of noise. however, we needed  <dig> clusters at noise level i =  <dig>  instead of  <dig> clusters at i =  <dig>  and  <dig> clusters at at i =  <dig> , proving that metica could start to extract sources from the background noise.

in brief, the performance of metica on simulated data confirmed that we could effectively study the fastica convergence via hca, cca and the cluster quality index. more clusters were needed to extract underlying components when the data contained stronger noise.

algorithmic reliability of metica on experimental data
the same validation strategy was applied to the experimental data as to the simulated data. we evaluated the algorithm convergence from  <dig> ics  estimated in each of m =  <dig> fastica runs. our quality index decreased until the number of clusters reached c =  <dig> and it increased afterwards. the optimal number c =  <dig> was confirmed visually . the matrix oc  contained the centrotypes of all the clusters.fig.  <dig> selection of optimal cluster number. a the evolution of the geometric index average inner/between cluster distance as a function of number of clusters. the index is smallest at c= <dig>  meaning the most compact and well-separated clusters. b the distribution of clusters  on the 2d space of cca. it provides a visual confirmation for c




statistical reliability of metica on experimental data
metica revealed the convergence of fastica on non-targeted metabolomics data. however, some of the convergences observed might only haven been due to a few particular samples. therefore it is important to evaluate the statistical significance of each centrotype obtained. however, as an unsupervised method, ica could not be validated via prediction error since no target information could be used. once again, as an optimization-based component analysis, cross-validation  methods widely used in pca validation  <cit>  are inappropriate or too time-consuming. in fact, to start each cv run, datasets must be divided into two groups and the whole metica procedure has to be run on one of them . accordingly it is necessary to validate the convergence for each cv run.

therefore we instead applied a sophisticated bootstrapping validation. bootstrapping means random sampling with replacement. in general, bootstrapping is considered as a slight modification of the dataset without changing its size. bootstrapping validation is widely used for model selection in machine learning problems , especially when strict mathematical formulations are not available. in our case, the statistical significance of metica components was barely evaluated mathematically. therefore we tried to find a score that described the stability of metica components subjected to bootstrapping. it was expected that components distorted by particular samples would be very sensitive to these slight modifications, while statistically significant components were expected to remain stable. the validation was implemented in the script metica_bootstrap.r  for yeast exo-metabolome data as follows: from the original x  we generated b =  <dig> bootstrapped data: x <dig>  x <dig> … xb by replacing  <dig> rows of x each time. then, we fixed the algorithm input, the demixing matrix w <dig> and ran fastica once on  <dig> bootstrapped datasets with ’parallel’ extraction and the other  <dig> with 'deflation' extraction. we extracted from each bootstrapped dataset k estimated sources  to ensure r <dig> > 90 % and we did likewise in each fastica run for the original data .

the  <dig> centrotypes oc <dig>  oc2… oc <dig> from the original dataset were compared with these k estimated sources. the most correlated source s^ba′ was considered to be aligned to centrotype oca. the absolute spearman’s correlation coefficient ρa between oca and s^ba′ was the score of oca for the particular bootstrapped data. the higher the score was, the closer the estimated source was to the centrotype. the sum of scores h=∑ρa from all the bootstrapped data was our final similarity score for centrotype oca. it measured how similar metica centrotypes were to estimated sources of bootstrapped data, in other words, the stability of centrotypes after bootstrapping. the math input is as follows: h=∑b=1bmaxj=1…kρoca,sbj 

the h score implies the statistical reliability of centrotypes given a fixed demixing matrix w <dig>  however, such a score might depend on the fastica input. therefore the scoring is repeated with fixed bootstrapped datasets but  <dig> randomized w <dig>  finally, for each centrotype, we obtained a distribution of h. we used the median h^ of the distribution as an exact score of the centrotype. the dispersity shows how trustworthy the score estimate is. our empirical experiment showed that the distribution was quite weakly dispersed . the visual illustration of the whole scoring process is in .

the centrotype scoring leads to another possibility for deciding on the number of clusters. after the number of clusters was determined, we could evaluate the h^ of each centrotype after which we obtained a score distribution of all the centrotypes for the particular number of clusters. therefore we could monitor the h^ for all the centrotypes as a function of the number of clusters  and select the optimal number based on the amount of centrotypes containing a higher h^. we observed a pattern of statistically reliable super-gaussian centrotypes . at c =  <dig> clusters suggested previously by the quality index, we obtained  <dig> such centrotypes. low significant centrotypes seemed to occur when we further increased the number of clusters, which means that c =  <dig> was also a good decision in terms of statistical reliability.fig.  <dig> bootstrap scores as a function of cluster number. when the cluster number is fixed, we could compute the h^ score  for each centrotype. then we monitored the distribution of scores as a function of cluster number



afterwards a comparison was made between the bootstrap score and kurtosis of these centrotypes. in previous studies, super-gaussian distributed components usually indicated interesting class separation structures while gaussian-like distribution  or sub-gaussian  contained less information  <cit> . in fig.  <dig>  it can be seen that low kurtosis centrotypes also have a low h^. however, the highest kurtosis does not ensure the highest bootstrap score .fig.  <dig> 
h estimates and kurtosis of centrotypes. the upper figures shows the distribution of the h estimate of each centrotype by box plot, sorted by their median, e.g. oc <dig> has the highest h^ so it is considered to be the most statistically-reliable. the lower figure shows the kurtosis of each corresponding centrotype



component order and interpretation
the components extracted by a single ica run have no order. however, we give an interpretation order for the centrotypes obtained based on their bootstrap score h^. we first interpret the centrotypes that have relatively higher h^  with smaller error bars . the following are biological interpretations for some of the top nine centrotypes . the script for visualization of scores and loadings is in tutorial.pdf .

ica detects outliers
ica seems to be sensitive to outliers. for instance, sample r1s <dig>  has an extreme negative score on oc <dig> compared to the other samples, including the two other replicates of s <dig> . the same situation was also observed on oc <dig> & oc <dig> . although the interpretation of these outliers is not so obvious, the reliability of the centrotypes encouraged us to investigate the potential technical errors.

ica detects phenotype separations
the three samples  of strain s <dig> have higher negative scores than all the other samples on oc <dig> . in general, if one component carries biological information, it is interesting to know which mass signals are highly involved. these signals have higher loadings in weights matrix a, which is the pseudo-inverse of the product of whitening matrix k and demixing matrix w:fig.  <dig> interpretation of a centrotype. a the score of each sample on oc <dig>  the three wines from the fermentation triplicates of strain s <dig>  all have higher negative scores. b loadings of metabolites on oc <dig>  metabolite having higher negative loadings contribute to the separation of s <dig> from other strains. c many of these metabolites are annotated in the biosynthesis of amino acids. here, red nodes are annotated compounds

 a=kwtkwkwt− <dig> 

mass signals with the top  <dig> highest negative loadings on oc <dig> were extracted. the concentration of these metabolites should be higher in wines fermented by s <dig> than other strains. under the assumption that exo-metabolome reflects cell activity, we mapped the extracted mass signals from the yeast metabolic network using the masstrix server   <cit> . among  <dig> annotated masses,  <dig> were metabolites in the yeast metabolic pathway biosynthesis of amino acids . this observation was in accordance with information from the yeast provider: strain s <dig> could synthesize more amino acids and thus stimulate secondary fermentation in wine.

similar results were observed on oc10: triplicates of s <dig>  had much higher positive scores than the other samples . corresponding metabolites annotated on masstrix revealed enrichment in several pathways in central carbon metabolism, such as fructose & mannose metabolism, the pentose phosphate pathway and the tca cycle. in fact, eca <dig> is a strain created by adaptive evolution to enhance sugar metabolism, notably the metabolic flux in the pentose phosphate pathway  <cit> .

comparison to other ica algorithms
the performance of metica was compared to other ica algorithms  using another non-targeted icr/ft-ms-based metabolomics dataset . the data matrix counted initially  <dig> signals measured in  <dig> urine samples from doped athletes, clean athletes and volunteers . for the purpose of filtering and formula annotation, such high data dimension was more efficiently handled by our in-house developed software netcalc compared to other standard approaches, such as chemospec  and metaboanalyst . the reduced data matrix doping.txt  with  <dig> mass signals remained were analyzed directly with metica, as well as two fastica algorithms in r . four other ica packages were tested on the pca score matrix xd : icapca in r  <cit> , icamix in r , kernel-ica toolbox version  <dig>  in matlab with a gaussian kernel  <cit>  and mean field ica toolbox in matlab for bayesian ica described previously  <cit> . if ‘out of memory’ problem occurred or the simulation failed to produce reasonable results, the corresponding package was applied only on first few columns of xd . for all  <dig> ica methods tested,  <dig> replicates were made with randomized algorithm inputs. we evaluated the shapes of extracted components table  <dig> ), the stability between simulation runs  and the reliability of components & model .table  <dig> comparison between different ica algorithms

seven ica algorithms were compared based on  <cit>  maximal percentage of variance the algorithm could handle ,  <cit>  optimal number of components that the algorithm suggests,  <cit>  kurtosis of the most super-gaussian component  <cit>  kurtosis of the most gaussian component,  <cit>  minimal kurtosis of components ,  <cit>  number of consistent components extracted in all  <dig> algorithms runs with an absolute spearman's correlation between them higher than  <dig>  and on whether the algorithm suggests  <cit>  model selection criteria  <cit>  importance order of components



the comparison revealed that metica extracted both super-gaussian and sub-gaussian components, while 'parallel' fastica, icapca and icamix only highlighted super-gaussian signals. components from kernel-ica & bayesian-ica were more gaussian-distributed. among seven algorithms, 'parallel' fastica and icamix gave consistent results between simulation runs. metica resulted in  <dig> out of  <dig> stable components if we fixed the number of clusters at  <dig>  our studies also showed that the amount of stable components would increase if the cluster number was tuned for each run through cluster visualization or bootstrapping. in the end, metica was among the few algorithms that suggested both model selection and component ranking. the icapca package suggests a reliable loo-cv-based component selection, but the simulation seemed computationally intensive for our dataset. as a result, the model from icapca only explained  <dig>  % of total variance.

CONCLUSIONS
in this paper, we developed the metica routine for the application and validation of ica on non-targeted metabolomics data. we adapted icasso, an algorithm previously used in medical signal processing, to our ms-based yeast exo-metabolome data. we studied the convergence of fastica in a way slightly different from that in the original icasso version  <cit> : spearman’s correlation was used instead of pearson’s correlation to simplify the relations between estimated sources; the cluster number was selected based on a simple geometric index on projected space, instead of quantitative indices in the original space. these two simplifications improved the efficiency for high-dimensional data, since we tried to keep the maximum variance after pca-denoising while having enough fastica runs. as a result, we usually generate a huge amount of estimated components , but using the original icasso is too time-consuming to handle this amount. an alternative fast approach for estimated sources clustering was to use the rounded kurtosis value  <cit> . however, metica seems to be much more sensitive to detect non-similarities for non-targeted metabolomics data.

furthermore, we investigated the statistical reliability of convergence points by comparing them to fastica estimates for bootstrapped data. reliable centrotypes revealed strong phenotype separations and pathway differences between phenotypes.

from the modeling viewpoint, bayesian ica optimized the model by bic - a trade-off between likelihood  and the risk of over-fitting. when processing high dimension data became difficult, our method provided an alternative mean of model optimization: increasing the number of reliable components instead of fitting the data. we suggested two ways of deciding the optimal number of model components, namely the number of clusters: either by using a cluster quality index , or through the bootstrap scores of all the centrotypes .

the whole metica routine was tested on simulated data and several ms-based non-targeted metabolomics data, including low resolution ms datasets . compared to other ica methods, metica could efficiently decide a reasonable number of clusters based on algorithmic reliability. the bootstrap scores further validated this decision. for both high and low mass resolution and for any biological matrices, metica was able to handle more than  <dig>  <dig> features and to sensitively select reliable models.

since our routine was based on a simple linear model, we could easily reconstruct the original dataset and calculate the fitting error. therefore, our procedure could also be further used for dimension reduction before applying supervised statistical methods, or data denoising to remove undesirable signals   <cit> . all in all, it opens a door for extracting non-gaussian information and non-linear independence from non-targeted metabolomics data.

additional files
additional file 1: source code and raw datasets used for metica evaluation. source code, raw datasets and user manual were also available at https://github.com/daniellyz/metica. 

additional file 2: figure s <dig>  generation of simulated data. the simulated data sx was generated by adding the background noise n  to a matrix reconstructed by two selected non-gaussian pcs . the blue intensity here represents signal intensity. figure s <dig>  hierarchical clusters in 2d space. distribution of estimated metica sources from simulated data when projected on a 2d cca space. sources belonging to the same hierarchical cluster have the same color. the splitting of the dark blue cluster into black, dark blue and cyan clusters was seen when we increased the cluster number nc from  <dig> to  <dig>  it splitted again when nc increased to  <dig>  the quality index is the ratio between the average within-cluster distance  and the average between-cluster distance . figure s <dig>  kurtosis distribution of all variables . three histograms represent kurtosis distributions for experimental data x_exp, simulated background noise n and simulated data sx , respectively. figure s <dig>  illustration for bootstrap scores. for a fixed algorithm input, fastica runs on b different bootstrapped data. the centrotype oca  is compared to all the estimated sources from each run. the spearman correlation coefficient  to the most correlated estimate  is the similarity score we are seeking. the final score hoca is the sum of scores from all the bootstrapped data. figure s <dig>  scores of samples on some centrotypes. a) on oc <dig>  sample r1s <dig>  has an extreme negative score, so it is considered as an outlier. b) c) for the same reason as r1s <dig> on oc <dig>  samples r3s <dig>  r2s <dig> and r3s <dig> are considered as outliers. d) the three wines from the fermentation triplicates of strain s <dig>  all have higher positive scores. 

additional file 3: evaluation of metica on lower resolution metabolomic data. additional text and figures are provided to illustrate the application of metica on lower resolution lc-ms data. 



abbreviations
afalcoholic fermentation

alaverage-link

bicbayesian information criterion

ccacurvilinear component analysis

cvcross-validation

didirect infusion

hcahierarchical clustering analysis

icaindependent component analysis

icr/ft-msion cyclotron resonance fourier transform mass spectrometer

loo-cvleave-one-out cross-validation

mapmaximum a posteriori

mdsmultidimensional scaling

msmass spectrometry

nmrnuclear magnetic resonance

pcaprincipal component analysis

somself-organizing map

competing interests

the authors declare that they have no competing interests.

authors’ contributions

the metica was designed by yl, ha, rdg and ks; ps, rdg and ha participated in the preliminary experimental design; yl performed the fermentation experiments and non-targeted analysis; yl wrote the scripts for metica; ml provided other experimental data for algorithm validation; yl, ks and ml designed the validation strategies for metica; ps suggested the masstrix server; ps supervised the research and manuscript preparation; the manuscript was drafted by yl. all the authors read and approved the final manuscript.

we thank lallemand inc. for providing the grape must and yeast strains. lallemand inc. and the région de bourgogne are thanked for their financial support.
