BACKGROUND
proteins and genes play fundamental roles in most organic functions. they cooperate to keep the entire organism's machinery working and prevent breakdowns. globally, their social relationships give rise to minutely organized networks, currently the target of meticulous studies  <cit> . genetic regulatory networks   <cit>  and protein-to-protein interaction networks  are the most representative classes of biological networks.

grns represent collections of dna segments which functionally interact with each other as well as with the chemicals that govern the transcription of genes into rna sequences. from another perspective, grns can be seen as input-output machineries which produce an output  by a combined application of basic functions to input stimuli. ppis are comprised of proteins that form chains in which each protein reacts to a stimulus of its predecessor to produce a signal directed to its successor. reactions in such chains are usually seen as directionally oriented because often they are not reversible. a reaction chain acts as a connector of a triggering event to a final physiological response and then sets up a complex  collaborative network.

both kinds of networks have been collected into several data banks spread throughout the www  <cit>  and modeled in a computerized fashion by means of some unambiguous and artificial formalisms. the most common one makes use of coupled ordinary differential equations   <cit> . alternatively, several other promising modeling techniques have been employed:  boolean networks  <cit> , petri nets  <cit> , bayesian networks  <cit> , graphical gaussian models  <cit> , process calculi  <cit>  and automata theory  <cit> . standard languages, xml-  or graphical-  based, have been further proposed to allow knowledge sharing. all of them are  connected to graph theory, since a common simple principle holds: interacting agents  are represented as the graph vertices whereas interactions  constitute the graph edges. moreover, strength of connections  is usually modeled by weighing the edges . protein-to-protein interaction networks, chemical structures, gene co-expression and contact graphs for protein structures are all examples of undirected graphs, whereas experimental protocols and taxonomies of species and traditional regulatory networks are examples of directed graphs. other examples include: dna, rna or protein sequences , sequence fragment overlap graphs  for shotgun sequence assembly, genetic maps and multiple sequence alignments . on top of them, a myriad of graphical- and textual- based tools has been developed with the aim to simultaneously make the process of networks design increasingly more intuitive and to speed up their functional description. in sec.  <dig>  we itemize the most representative tools and give some concise explanations of them.

once a network is defined by its constituents and interactions, it is common practice to simulate the network by means of deterministic/stochastic solvers. the deterministic solvers cope with interlocking sets of differential or difference equations and require the specification of some initial conditions. its response  is fixed, given the values of its input variables , and implies that var =  <dig>  this property distinguishes deterministic models from real-life experiments  <cit> . stochastic solvers use pseudorandom numbers  to produce different y values, starting from the same initial parameters. hence, the response y is a random variable with var = g, usually estimated through replication and fed with different random numbers. a user-defined accuracy level determines how many simulation runs are needed  <cit> . a stochastic trajectory or trace  is a sequence of temporal observations of the copy-number of the species in the state space.

traces are curves generally made by double precision  floating-point numbers sampled over time. they sketch the trend of some system variables that are changing because of a predefined set of mathematical rules. based on the continuous or discrete nature of such rules, the traces format changes accordingly. continuous traces exhibit smooth trends of variables regularly observed at continuous time-scales. the solution of an ode-based biological system gives the trend of the concentration of their variables within a desired time interval. therefore, variables change continuously according to a mathematical function which solves the ode's set. discrete traces only differ because of their non-homogeneous temporal sampling. they are evaluated at uneven time instants that are calculated at run-time. usually, biological discrete systems deal with chemicals populations  and, consequently, markovian trajectories are made of streams of integer numbers. sharing both kinds of simulation results is, to a great extent, hindered by the use of a variety of data formats by the existing simulation software packages. in fact, most of them output traces decorated by proprietary meta-information that contains private, not human-readable simulator settings. few standard formats exist, but although some of them seem to be very well-suited and complete, their general-purpose nature makes them little versatile for our needs. that is why snazer has been equipped with an ad-hoc and lightweight internal data format.

the rest of the paper is organized as follows: sec.  <dig> describes all the features of our tool compared to those of similar existing tools. particularly, in sec.  <dig>  the graph layout algorithms and the analysis features of snazer are presented, whereas in sec.  <dig>  we describe the implemented statistics routines. in sec.  <dig> , we present the internal data format together with the compression policy employed to enhance the storing of data. in sec.  <dig> we test snazer on a real case-study with the aim of highlighting features and strength points. then, we perform some benchmark tests on its compression capability and show the results. in sec.  <dig> we conclude the paper and present future works.

implementation
snazer is a software prototype whose aim is threefold:  manipulating biological networks,  providing advanced statistical analysis routines for simulated traces and  improving the traces sharing and storing processes. snazer can also be considered as a viewer package of beta workbench   <cit> . by parsing its output files, snazer imports two relevant pieces of information inherently bound to the modeled systems: the graphs of the simulated reactions and the simulated traces. it encloses them in a compact xml data structure, designed to work as an interchange data source/sink. through this, networks and simulation data look tightly coupled.

the overall architectural skeleton draws inspiration from the model-view-controller software architectural design pattern and is made up of two parts: the reaction graph  and the statistics  modules, that deal with visualization of the system interactions and with calculation of statistics outcomes of simulated traces, respectively . both modules share the optimized floating object compliant with the snazer xml-schema previously mentioned and discussed in sec.  <dig>  .

 <dig>  state of the art
the rationale behind snazer comes directly from the scientific community, which needs to manage together the biological models and their simulation results in a coherent manner. several tools exist that unilaterally tackle such need. pajek  <cit>  is a standalone application for analysis and visualization of large networks having ten to hundreds of thousands of vertices. it is a highly interactive program and its main strength is the variety of layout routines  that greatly facilitate exploration of patterns within large networks. it is equipped with several decomposition, connectivity, pattern searching algorithms. pajek can detect clusters in a network, extract vertices that belong to the same clusters and show them separately, shrink vertices in clusters or show relationships among clusters. it loads several proprietary input files and writes network layouts as eps, svg, bmp files. medusa  <cit>  is a java application . it provides 2d representations of medium-sized networks, up to a few hundred nodes and edges. it shows multi-edge connections and is optimized for protein-protein interaction data. medusa supports weighted graphs and represents the significance and importance of a connection by varying line thickness. graphs can be drawn by means of some embedded spring-like layout algorithms and exported to image or postscript files. medusa is designed for accessing protein interaction data from string <cit> . cytoscape  <cit>  is a standalone java application for the visualization of large-scale molecular interaction networks  and their integration with gene expression profiles and other data. it also allows the manipulation and comparison of multiple networks. the great quantity of available plug-ins enriches cytoscape with increasingly specialized analysis modules. the tool supports a variety of format files, including gene ontology   <cit> , sbml <cit>  and kegg <cit> . biolayout express3d <cit>  is another tool written in java for the visualization and analysis of networks derived from biological systems. it supports both unweighted and weighted graphs together with edge annotation of pairwise relationships. it mainly employs the fruchterman-rheingold  <cit>  layout algorithm for 2d and 3d graph positioning and display and makes use of a heavily optimized c-based markov clustering algorithm for graph clustering. its main goal is to offer different analytical approaches to microarray data analysis. it supports different file formats: cytoscape sif files, reactome files  <cit> , yed graphml files  <cit> .

osprey  <cit>  visualizes complex interaction networks. it represents not only interactions in a flexible and rapidly expandable graphical format, but also provides options for functional comparisons between data sets. osprey uses the general repository for interaction datasets as a database   <cit> , from which the user can rapidly build interaction networks. networks can be saved as tab-delimited text files for future manipulation or exported as jpeg, png, and svg. proviz  <cit>  is a standalone open source application developed for the visualization of protein-protein interaction networks. it provides facilities for navigating large graphs and exploring biologically relevant features. it has a plug-in library with dozens of layout algorithms. basically, they belong to three families: force-based, hierarchical and geometric layout . proviz adopts emerging standards such as go and psi-mi <cit> . biologicalnetworks  <cit>  models whole cell biochemical pathways and gene regulatory networks. it uses a specialized graph visualization engine to represent biological pathways, gene regulation networks and protein-protein interaction maps for intuitive exploration and prediction. the tool can handle a variety of tasks, including graphic drawing and layout optimization, data filtering and pathway expansion, and classification and prioritization of proteins. biologicalnetworks uses a proprietary file format  that stores information pertaining to the model and the corresponding simulation environment. it supports import and export of models from sbml, sif and gml file formats. finally, tulip  <cit>  is one of the forerunners of drawing packages for biological networks. it allows the visualization, drawing and editing of graphs up to a million elements. such a visualization system allows navigation through geometric operations as well as extraction of subgraphs and enhancement of the results obtained by filtering. its most interesting property is the underlying data structure used to inspect huge graph attributes. tulip implements the well-known "flyweight" and "chain of responsibility" patterns to access graphs through views. the real advantage is enabling a real sharing of the elements between graphs with a good memory management. all this software improve and obscure the first-generation tools from which they have drawn inspiration: otter  <cit> , a general-purpose network visualization tool; negopy  <cit> , a discrete, linkage-based program for the analysis of networks; krackplot  <cit> , a network visualization tool intended for social networks; multinet  <cit> , a windows-based computer program designed for exploratory data analysis of social and other networks. other tools exist that aim at providing advanced statistics routines for biological traces. traviando  <cit>  is a backend trace visualizer and analyzer. it interfaces the xml output file of möbius, a multi-paradigm multi-solution framework for the performance and dependability assessment of systems, to investigate the details of what happened in a simulation of a model. it verifies that certain events happen in a trace, that particular states are frequently reached or that certain conditions hold throughout a simulation. it further analyzes the cyclic behavior with graphics that show if states are repeatedly visited or how the length of the trace evolves if cycles are removed. traviando also supports various statistics on traces as well as the model checking of a trace with respect to ltl formulas. simwiz  <cit>  is an old but still interesting project. it is a collection of java tools that aims at visualizing data resulting from different kinds of biochemical simulation processes. it imports stode <cit>  and copasi <cit>  simulation output files as well as the relative reaction graphs as sbml files. its main feature is animating the network graph through the information coming from the simulated traces. vanted  <cit>  loads and edits graphs, which may represent biological pathways or functional hierarchies. it allows the mapping of experimental data sets onto the graph elements and visualizes time series data or data of different genotypes or environmental conditions in the context of the underlying biological processes. built-in statistic functions allow a fast evaluation of the data . poptools  <cit>  is a versatile add-in for microsoft excel that facilitates analysis of matrix population models and simulation of stochastic processes. together with routines for iterating and resampling, this allows the calculation of bootstrap and other statistics for stochastic processes. routines that facilitate calculation of some simple maximum likelihood and resampling statistics are supported as well.

snazer addresses many of these issues, but additionally offers a solid framework that couples networks and traces analysis frameworks. it provides the user with the possibility both of browsing, dissecting and analyzing the networks components and of performing statistics on the inspected components. snazer has been equipped with an efficient compression system to make huge amount of data manageable and treatable as well as to store and share them. compared to the existing tools, it covers most of the network visualization layout routines. in addition, it implements temporal-circuit , an original algorithm to layout networks according to the information coming from simulation. it is not as well equipped with decomposition and connectivity algorithms as pajek and biolayout express3d, because we mainly focused on original  rather than existing functionalities. snazer is cross-platform, being implemented in java . it structures input data in xml files  and makes use of an internal data structure to improve data access . at the best of our knowledge, snazer is the only tool that provides data compression . it supports some standard input data formats  to be fully compatible with all the orbiting software packages. moreover, snazer offers some new trace analysis routines that deeply differ from those implemented in the presented softwares. in fact, vanted fills network nodes with information coming from biological experiments  to statistically analyze them. but it does not provide any means to analyze simulated traces. simwiz, instead, makes use of traces to animate the related networks, but it does not provide any functionality in support of simulation. traviando analyzes individual traces by using classical model-checking methods, but lacks of any support for group-traces. poptools, offers some statistical facilities for wet-data  and stochastic processes, but disregards any group-traces issue as well. contrarily, snazer covers these lacks by offering new analysis algorithms centered on multi-traces analysis.

all these features are built-in in snazer and make it a unique tool.

 <dig>  the reaction graph perspective
the focus of network-level analysis in general is on properties of networks as a whole. these may reflect, e.g. typical or atypical traits relative to an application domain or similarities occurring in networks of entirely different origin. both from a global and a local view, quantitative analyses have been conducted on networks to investigate these traits. snazer provides the user with some clever layout algorithms to firstly give a global insight of a network, and with a centrality ranker index to highlight vertex importance.

 <dig> . <dig> layout analysis
graph layout methods are core techniques for applications based on graph-like network diagrams. since such a perspective is becoming more and more pervasive, and since snazer visualizes part of the data sets as reaction graphs, we equipped it with the fastest and most flexible layout algorithms. most of them belong to the force-based  class of algorithms. they are easy to use, since they do not require special knowledge about the graph theory, and often their results look very good. they mainly assign forces to networks as if the nodes were electrically charged particles and the edges were springs. thus, layouts are arranged according to real physical principles . as a result, forces repetitively applied to nodes push and pull them further apart in order to minimize the overall energy of the networks. when an equilibrium state is reached, the graph is drawn.

such algorithms enjoy several strength-points: uniform nodes distribution, symmetry, flexibility, simplicity, strong theoretical foundations, but suffer from some drawbacks: high running time ) and poor local minima. spring is the simplest force-based layout algorithm. basically, nodes try to get as far of each other as possible, but edges pull nodes near each other, merely according to their weights. the energy of the system is minimized by iteratively arranging the nodes . it enjoys all the strength-points as well as it suffers from all the drawbacks just mentioned.

in particular, the task of bringing under control the problem of poor local minima raised the interest of many scientists. such issue is based on the fact that the obtained minimum system energy could be considerably worse than a global minimum. since such issue becomes more and more important as the number of vertices of the graph increases and, hence, that the overall quality of the drawing could result lower and lower, some scientists focused on the initial layout sub-problem to solve it. they conjectured that any outcome of any force-based algorithms results to be strongly influenced by the initial layout, that in most cases is randomly generated.

the kamada-kawai  algorithm represents the first attempt to solve this problem. it was designed with the aim to quickly generate advantageous initial configurations. nodes are represented by steel rings and the edges are springs between them. the attractive force is analogous to the spring force and the repulsive one is analogous to the electrical force. the energy minimization in this algorithm is achieved by obtaining the derivative of the force equations. at the minimum energy, the derivatives of the force equations are zero. since these equations are not independent, they cannot be independently brought to zero, and therefore, only the node that has the maximum gradient value is moved. this process is repeated until the total energy is minimized .

a combined application of kklayout with the fruchterman-reingold  algorithm represents today one of the most successful layout solutions. frlayout contributes in meaningfully placing the neighbored nodes. it works with unweighted, undirected graphs, where attractive forces occur between adjacent nodes only, and repulsive forces occur between every pair of nodes. the movement of nodes is also function of the system temperature registered at each iteration. generally, temperature decreases through successive iterations while nodes occupy their place .

some alternative ways of approaching this problem lie in searching more directly for the energy minimum, either instead of or in conjunction with physical simulation. among such procedures, we consider a competitive learning method: the isom layout , which extends the kohonen's self-organizing map. it evenly fills the space with vertices and lets them take place because of an heuristic function rather than of attracting forces. the algorithm selects a random point in the graph area and picks the closest vertex to that point. this vertex is moved toward that point as well as all vertices connected to that initial vertex by up to a set number of edge steps. the amount by which the vertices are moved decreases the greater the number of edges in the shortest path between the current and initial vertices. the initial number of edge steps is decreased during the layout process so that the later steps form local clusters of connected vertices .

unfortunately, such methodologies do not properly work with huge networks, where problems like node-node or node-edge clashes are often encountered. in these cases, simple algorithms like the circle layout are employed with the aim to evenly space vertices on a geometric  trajectory, irrespective of the network size. circle attempts to minimize as many overlapping vertices as it can, by placing vertices next to each other that are adjacent in the graph. it is a fast algorithm, essentially because it is not optimal. that is, it does not resolve the problem of edge intersection, but propose a more familiar way to visualize the network. it does not undergo sequential arrangement since it is a static layout . all the described layouts are general purpose and have been widely employed in heterogeneous scientific areas. we designed and implemented temporal-circuit, an original and ad-hoc layout algorithm, aimed at drawing simulated biological networks. it locates network elements in an electric circuit. nodes and reactions take progressively place into tabular spots from left to right. in particular, the species initially present into the system and the reactions which involve them are drawn on the leftmost. afterward, if new species are created somewhere and somehow into the system , they are depicted on the immediate right, together with the reactions which implicate them. this process progressively continues until any new species is placed. therefore, the species that come last into the system occupy the rightmost side . since such a layout encodes and visualizes both the network and simulation information, it works only if both the graph of reactions and the simulated traces are provided by the user.

 <dig> . <dig> quantitative analyses
the determination of important elements, group of elements or evident treats of networks is collectively known as quantitative analysis  <cit> . since the 1950s, along with some global topological indices, many vertex centrality indices were introduced. these were to quantify an intuitive feeling that in most networks some vertices and edges are more central than other. in practice, they were to evaluate the 'reachability' of a vertex.

given any network, these measures rank the vertices according to the number of neighbors or to the cost it takes to reach all the other vertices from it. these centralities are directly based on the notion of distances within a graph, or on the notion of neighborhood, as in the case of the degree centrality. degree, eccentricity, closeness, centroid are only some among the most known and simple existing centrality indices. on their basis, several structural properties have been studied, e.g. the graph center, defined as the set of all the vertices of minimum eccentricity; the median graph, an undirected graph in which any three vertices a, b, and c have a unique median: a vertex m that belongs to shortest paths between any two of a, b, and c.

based on the set of shortest path in a graph, some other centrality indices are worth being mentioned: stress centrality, that is based on the enumeration of shortest paths; shortest-path betweenness centrality is a kind of stress centrality that accounts for the fraction of shortest paths between two nodes that contain a third node.

vitality measures have been further used to determine the importance of vertices or edges in a graph. given an arbitrary real-valued function on a graph, a vitality measure quantifies the difference between the value on the graph with or without the vertex or the edge. in particular, flow betweenness vitality is a measure for max-flow networks which is similar to the shortest-path betweenness, but that aims at measuring the degree that the maximum flow depends on a particular vertex; closeness vitality denotes instead how much the transport cost in an all-to-all communication will increase if a vertex is removed from the graph.

these and all the other existing local methods try to answer to questions like: which are the most central members of a network and which the most peripheral? which connections are crucial for the functioning of a subnetwork?

snazer contributes to answer to these questions by providing a little set of interactive tools. mainly, a degree ranker is implemented with the aim to highlight centrality of nodes within graphs. ranks are calculated according to the corresponding nodes degree and, then, nodes color tonalities are tuned accordingly: the more a node is central, the darker will be its color tonality. degree centrality is meant as the number of links incident upon a node . degree is often interpreted in terms of the immediate risk of node for catching whatever is flowing through the network . if the network is directed , then we usually define two separate measures of degree centrality, namely indegree and outdegree. indegree is a count of the number of ties directed to the node, and outdegree is the number of ties that the node directs to others. for positive relations such as friendship or advice, we normally interpret indegree as a form of popularity, and outdegree as gregariousness.

mathematically, for a graph g with n vertices, the degree centrality cd for a vertex v is defined as:   

each graph drawn by snazer is fully interactive. they can be graphically modified by stretching, translating and grouping nodes and can be saved as standard graphml files. snazer provides users with the possibility to discover any information embedded into nodes and edges. if encoded in a standard manner, nodes hold data source information in the form of miriam annotations  <cit> . by them, any detail  refers to a catalog of data types through uris and to their physical locations through urls. on the contrary, edges carry information about the nature of the reactions that they represent . an example is reported in fig.  <dig> 

another important feature is the support for color-blind users. snazer takes care of the most common color vision deficiencies  that affect the 8% of the male and the 2% of the female populations  <cit> . the set of the graph colors can be tuned on-demand by means of our own procedure, referred to as colorblind filter  <cit> . it provides users with a sufficient color contrast between nodes and the foreground/background, by changing the hue, saturation and lightness values of each color in a proportional way.

in any case, nodes can be visited and highlighted by the intuitive relationships of neighborhood, or by user selection. selected nodes can be sent to the analysis module to be further analyzed.

 <dig>  statistical perspective
many features have been ascribed to simulation traces over the years:  the trend component, namely the long term underlying direction  and rate of change;  the irregular component , namely the component that is left over when the other components of the series have been accounted for;  the autocorrelation, namely the relationship between members of a time series of observations and the same values at a fixed time interval later. some scientists make use of specialized algorithms to look for these and others properties in sets of traces, obtained by repetitive simulation of the same models. in their most basic form, multiple traces analysis algorithms treat all variables symmetrically without making reference to the issue of dependence versus independence and permit causality testing of all variables simultaneously. this is a major advantage of such algorithms compared to the multivariate time series algorithms. on the basis of these algorithms, we equipped snazer with  <dig> analyzers, dealing with isolated as well as grouped time series, each with its own specific parameters.

they all tackle the common problem of dealing with differently sampled traces. this problem is essentially due to the mathematics behind the generation of the time vector. indeed, whenever a gillespie-inspired algorithm for simulating a model is used, the time evolution of a well-stirred set {s <dig>  ..., sn} of biochemical species reacting through m ≥  <dig> reaction channels  {r <dig>  ..., rm} depends on:

• the probability aj()dt that, given , one reaction rj will occur in the next infinitesimal interval [t, t + dt). n.b. aj() is a function of the number of possible active instances of reaction rj. consider a reaction r : s <dig> → s <dig>  then a() = cx <dig>  where x <dig> is the number of active r in the current state , and c is a constant that depends on the physical characteristics of s1;

• the change vji of the number of molecules of the specie si produced or consumed by a reaction rj.

given aj(), the evolution of a biochemical network is described by the next reaction density function p, which represents the probability, given , that the next reaction in the system will occur in the infinitesimal time interval [t + τ, t + τ + dt) and will be on channel rj.   

where . by conditional probability, eq.  <dig> can be rewritten as  where      

where τ is a sample from an exponential random variable with rate a0(), and the selected reaction j is independently taken from a discrete random variable with values in  and probabilities . by the eq.  <dig> and eq.  <dig>  the next reaction and the instant of time when it will fire are then chosen. in particular, the repetitive calculation of the eq.  <dig> provides the time vector of a simulation, whose sampling is strongly dependent upon the initial seed of the generated stream of τ values. thus, in the context of a mrip simulation  <cit> , different initial seeds are carefully chosen and used to avoid the cross-correlation that would naturally exist between any pair of traces. however, although this does not represent a problem from a graphical point of view, traces with different sizes and different time scales would result difficultly comparable from an analytical perspective. e.g., in the case of two traces with different samplings , a slightly warping effect might be noticed at a certain point in their time scales. this means that it might happen that some corresponding points might be plotted on shifted temporal locations and, hence, that even the simplest analysis task might fail on them.

our analysis routines overcome this issue. they all are equipped with an embedded pre-processing functionality that prepares traces for analysis. it takes a set of traces and applies an alignment procedure called binning  <cit> , which has a twofold effect. it re-samples traces and meaningfully reduces their sizes. binning is one of the most used pre-processing technique in the area of signal analysis. it groups adjacent points into bins and elects a representative member for each group. in particular, it takes a subset of n points from a generic signal, represented by the couples , and substitutes them with an unique point , whose value v* is an aggregate function of the n original values , and the time t is usually chosen among the original times . such basic operations are conducted by scanning all the signal through a sort of sliding window  chosen by the user .

this technique has been successfully used in the area of mass spectrometry time course analysis, where the time warping problem equally exists.

 <dig> . <dig> analysis routines
• series allows the application of a plethora of statistical calculations to traces. it elaborates an output data point for each timestamp present in any of the input traces and produces a time series as output, one for each selected statistical routine and for each selected chemical. the available statistical calculations are: mean, root mean square, variance, standard deviation, standard error, geometric mean, harmonic mean, skew and kurtosis.

• the first hitting analyzer works like series. however, it samples traces only whenever a  boolean condition becomes true. indeed, for each run, it searches for the first timestamp where the condition is satisfied and performs the requested statistics on the selected species. finally, it outputs the result as points on 2d charts, where time runs on the x axis.

• pointwise is a uniform analyzer which differs from series only because it performs whatever chosen statistics at regular, user-defined, intervals.

• the steady state analyzer is applied to systems characterized by initial transient behaviors. under the assumption that the chance of entering whatever possible state of the system after a transient period is time-independent, this analyzer chooses a random timestamp  from each run, and performs there the requested statistics.

• the cumulative analyzer calculates the maximum, minimum and mean  values for each selected species within all the time intervals that verify a pre-defined boolean condition. in other words, it gives the possibility of performing statistics only within some limited slices of traces.

• the time analyzer measures the duration of the time intervals that verify a user-defined condition. we call "front" the timestamp when a condition becomes true . fronts can ascend or descend, whenever the related conditions flip from false to true . thus, this analyzer performs statistics between any two consecutive fronts. other than the usual statistics, it can further perform time-duration and time-sum calculations.

• the raw data analyzer displays data on the 2d chart, as they are.

snazer gives the user the possibility to alternatively display results in the chart panel at run-time or to export them in svg file format.

 <dig>  data boxing
as quickly introduced above, snazer makes use of an internal data representation. before choosing it as our favorite data structure, we examined the three major alternatives:

• hdf  is designed to assist users in the storage and manipulation of scientific data across diverse operating systems and machines. it is generally employed for managing very large  data sets with very fast access requirements. its main aim is to standardize the format and descriptions of many types of commonly used data sets . it allows self description of data and accommodation for symbolic, numerical and graphical information. it is platform independent.

• netcdf  is a data format made for managing array-oriented scientific data. as hdf, it is a standard, platform-independent format which allows self description of data and efficient information retrieving. it makes use of hdf to enhance managing of much larger files and multiple unlimited dimensions.

• sbrml  is a proposal for a new markup language whose aim is to give structure to the results of typical operations performed on sbml models. sbrml captures and describes operations, type of operations and the algorithms used to perform such operations. it is currently structured on three levels:  ontology terms,  software, algorithm and result information and  content of the result.

unfortunately, all three data formats did not properly suite our aims. the first two are very general-purpose and too complex for us. in fact, we only need to efficiently store/retrieve time course data and simultaneously keep them linked with the corresponding biological models. sbrml could work, but because it is still a proposal and it lacks of guidelines, it has never been adopted on concrete case-studies. consequently, we discarded it as well.

 <dig> . <dig> the internal data format
the internal data format is a trade-off between simplicity and functionality. it is ad-hoc made and it has not been meant to compete with the existing data formats. it makes the dialogue between both the visualization and the analysis modules possible. it is an intermediate and xml-encoded data structure . to be coherent with the content of both modules, it is logically structured in two tightly coupled sub-parts. the former takes inspiration from sbml in both inheriting and specializing some of its constructs. thus, since snazer agrees on the same policies of annotation and identification, the sbase and sid elements are inherited as-they-are. furthermore, whichever biological system is specified by a model entity and by a list of reactions, each containing a list of reactants, of products and of their chemical kinetics rates. differently from sbml, each reaction is categorized by a type tag. reactants and products reference three types of chemicals: entities, complexes and variables. the first two types allow representation of structured elementary or complex reactive chemicals. variables are destructured elements which express scalar measurements  evolving over time.

the remaining part of the schema captures the information concerning the simulated traces and time. in particular, a structured chemicals is here referenced and annotated by means of the tstart attribute, which specifies the time when it enters into the system, and of the structure attribute, which records its internal behavior . finally, streams of simulated timesteps are compressed and recorded together with the information about the first, last simulated timesteps and the overall number of timesteps. traces are equally sampled over time.

 <dig> . <dig> the compression policy
compression is one of snazer's strength points. traces of both integer  and decimal  numbers streams are compressed by applying a pipeline of semantic and structural compressors. the simple semantic compressor takes in input a stream of numbers and outputs a stream of tuples , where value is the placeholder for any number of the input stream, persistence counts the simulated instants of time where value stays constant and increment stands for a list of  changes from value, for each number of the input stream subsequent to value. a new tuple is produced whenever one of the numbers of the input stream stays constant. the overall stream of tuples flows into a structural compressor where it is further compressed by the gzip algorithm. as a result, an ultra-optimized binary stream is produced. with the final aim to write it to file, it is transformed into a printable  base <dig> string .

RESULTS
we successfully used snazer to analyze many real case-studies . here we make use of a couple of them to show some of its functionalities. they are the cell cycle  <cit>  and the circadian clock  <cit>  mammalian models. the former is a complex network of biochemical phenomena that controls the duplication of cells. such process is macroscopically subdivided into four phases which cyclically alternate because of some cyclin-dependent protein kinases . when bound to specific cyclin partners, cdks promote cellular progression along the cellular life-cycle. the latter model reproduces periodic critical triggering signals in charge to control the cellular size during the cell cycle. merged together, they give raise to a very interesting model  <cit> .

by applying the kamada-kawai layout algorithm , the modular feature of this network looks very evident. a fair chunk of it shows the cell cycle part . the other accounts for the circadian clock part. both are not explicitly linked, but they are dependent because of the influencing activity of the s_tf clock component on s_wee <dig>  the used layout algorithm manages to catch this aspect. once drawn the network, we estimate the importance of the nodes by running a degree ranker. as a result, it changes the nodes color tonalities as follows: the lower the nodes hue, the lower their importance within the network. finally, we list all the reactions which s_cp <dig>  is involved in. by just a click on it, its neighbor nodes and their connecting edges get highlighted  and selected. the selected nodes can be further analyzed or passed to the statistics panel.

the statistics panel turns on whenever at least one trace  is imported. we used three different software packages to perform multiple simulations of the model: bwb  <cit> , cyto-sim  <cit>  and copasi  <cit> .

initially, we import their traces and adopt the time analyzer. this is usually used whenever the user wants to count the number of oscillation periods  of a species, or to estimate the regularity of the cycles, or to measure the duration of the time intervals when a species is over/under a threshold. one can specify an  condition to be satisfied by one or more species. for each species, one can specify the front type  and the x-axis point where to print the results. since by means of the raw data analyzer we have verified in advance that s_tf oscillates around the value  <dig>  we are able to quantify the duration of its period by measuring the time intervals between two consecutive values  <dig> . in fig. 7a, the number of detected periods and their durations are shown in red. instead, the duration of any interval between two consecutive ascending and descending fronts is shown in blue and green. in this case, we measure the time interval during which s_wee <dig> and s_cp are below the specified threshold.

another example lies in calculating a particular statistics  exactly and only when a condition becomes verified . to this aim, we use the first hitting analyzer and we define a two-values condition that imposes that a molecule quantity  is less than another . hence, by applying the analyzer, one of the mentioned statistics is computed on the selected species whenever the condition becomes verified .

 <dig>  benchmark tests
we performed some benchmark tests about the snazer's ability to compress input data. we also tested the memory allocation due to the xml-encapsulation process. results are shown in table  <dig>  we simulated some biological models and saved their traces as csv files. subsequently, we inflated them using our compression pipeline and saved them in xml format.
data set names have been shortened in: cc   <cit> , uat   <cit> , mapkc , gpc-rs   <cit>  and bycc   <cit> . measurements are expressed in kbyte. we list the names of the models in the first column. the second one identifies the traces files size generated by bwb. the third one holds the sizes of the corresponding compressed xml files . the last one lists the quantity of memory allocated  for the compression and conversion routines from bwb to xml formats.

the compression routine results to be more effective the larger the model and the data set. in particular, it works fine on stochastic and integer traces, essentially because any sequential and stochastic framework schedules always only one reaction to occur at a time, while all the others stay unchanged. thus, since reactions occur because of their own propensity functions, some will fire more frequently than others and, hence, models with many reactions will generate some frequently changing traces and some rarely changing traces. moreover, the probability for a reaction to stay unchanged generally increases with the number of the reactions, irrespective of its propensity function.

the semantic compressor defined in sec.  <dig>  works fine when reactions do not change frequently, since all the consecutive stationary values are embodied by just one persistence value. moreover, the best case occurs when traces are made of integer numbers. in that case, the increment part of the tuple will be always made up of a few figures  and the resulting compressed string will be the shortest. contrarily, traces of decimal numbers would see the increment value always made of as many figures as the numbers precision.

therefore, the resulting compressed string will be longer than one of the same length, but made of integers. generally, we detected a slight increase in terms of memory usage due to the fixed overhead introduced by the xml structure. however, the larger the data set, the more negligible its overhead. this is shown in fig. 9b, where we depicted the ratio between the  memory allocated to perform the boxing process and the physical input file sizes. the curve goes significantly down with the increasing of the input file sizes.

CONCLUSIONS
we implemented snazer to help visualize biological networks and to handle and store their simulation traces. in particular, snazer has been equipped with two main categories of tools. the visualization tools are in charge of drawing graphs according to some well-known as well as new layout algorithms. the new circuit layout has been presented in this context. furthermore, some quantitative analyses routines have been implemented to allow the user to investigate some common traits of the biological graphs as  node centrality and node importance. on the other hand, snazer has been supplied with a set of statistics routines with the aim to analyze the time course files obtained by simulation. finally, snazer relies on a very efficient compression routine that allows for comfortable storing and sharing of the simulation results. we plan to extend snazer with further capabilities, such as remote database interfacing for data collection, more sophisticated graph centrality calculations and analysis routines . we finally aim at increasing compatibility with other standard languages  and i/o data formats.

availability and requirements
project name: snazer; project home page: https://www.cosbi.eu/index.php/research/prototypes/snazer; operating system: platform independent; programming language: java; other requirements: java  <dig>  or higher, libsbml  <dig> . <dig> or higher; upon acceptance of the cosbi-ssla license, snazer is freely available for non commercial purposes.

authors' contributions
tm conceived of the study, supervised its implementation and drafted the manuscript. gi implemented the most parts of snazer. cp participated in the coordination of the study. all authors read and approved the final manuscript.

supplementary material
additional file 1
snazer xml schema. the xsd file and two explicative jpg and html files. 

click here for file

 additional file 2
example models. some example models. 

click here for file

 acknowledgements
the authors thank alida palmisano and alessandro romanel for their precious suggestions and the cosbi team.
