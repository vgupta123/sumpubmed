BACKGROUND
as the volume of data in various forms continues to expand in the life sciences and elsewhere, it is increasingly important to find mechanisms to generate high quality metadata rapidly and inexpensively. this indexing information - the subjects linked to documents, the functions annotated for proteins, the characteristics identified in images, etc. - is what makes it possible to build the software required to provide researchers with the ability to find, integrate, and interact effectively with distributed scientific information.

current practices for generating metadata within the life sciences, though varying across initiatives and often augmented by automated techniques, generally follow a process closely resembling that long employed by practitioners in the library and information sciences  <cit> . first, semantic structures, such as thesauri and ontologies, are created by teams of life scientists working in cooperation with experts in knowledge representation or by individuals with expertise in both areas. next, annotation pipelines are created whereby professional annotators utilize the relevant semantic structures to describe the entities in their domain. those annotations are then stored in a database that is made available to the public via websites and sometimes web services. as time goes on, the semantic structures and the annotations are updated based on feedback from the community and from the annotators themselves.

this process yields useful results, but it is intensive in its utilization of an inherently limited supply of professional annotators. as the technology to produce new information and the capacity to derive new knowledge from that information increases, so too must the capacity for metadata provision. technologies that support this process by partially automating it, such as workflows for genome annotation  <cit>  and natural language indexing systems  <cit> , provide important help in this regard, but manual review of automated predictions remains critical in most domains  <cit> . there is clearly a need for an increase in the number of human annotators to go along with the increase in the amount of data.

serendipitously, social web applications such as connotea  <cit>  and citeulike  <cit>  are now enabling the emergence of an expanding pool of human annotators - albeit annotators acting to fulfil widely varying purposes and in possession of a broad range of expertise. connotea and citeulike are examples of what are known as 'social tagging systems'. such systems let their users organize personal resource collections with tags . the kinds of resources contained within them are essentially unlimited, with popular examples including web bookmarks  <cit> , images  <cit> , and even personal goals  <cit> . these resource collections are made available to the social network of their creators and often to the general public. the tags used to organize the collections are created by the owner of the collection  and can serve a variety of purposes  <cit> . the act of adding a resource to a social tagging collection is referred to as a 'tagging event' or simply as a 'post' . figure  <dig> illustrates the information captured in a record of a typical tagging event in which janetagger tags an image retrieved from wikipedia with the tags 'hippocampus', 'image', 'mri', and 'wikipedia'. academic social tagging systems, such as connotea, bibsonomy and citeulike, extend this basic functionality with the ability to identify and store bibliographic information associated with scientific articles  <cit> .

the tagline of connotea - "organize, share, discover" - illustrates the purposes that social tagging systems are intended to enable for their users. tags can be used to organize personal collections in a flexible, location independent manner. the online nature of these services allows users to easily share these collections with others - either within their circle of associates or with the general public. through the public sharing of these annotated references, it is possible for users to discover other users with similar interests and references they may not otherwise have come across.

this basic functionality has already attracted tens of thousands of users to these systems.

the expanding numbers of users and the concomitant increase in the volume of the metadata they produce suggests the possibility of new applications that build on the socially generated metadata to achieve purposes different from the personal ones listed above. for example,  <cit>  showed that the relevance of web search results achieved with both search engines and hand-curated directories could be improved by integrating results produced by social tagging services. in fact, the "social search engines" suggested by this result are already starting to appear .

as we consider the creation of new applications like these within the life sciences, it is important to begin with an understanding of the nature of the metadata that they will be built upon. this study is thus intended to provide a thorough characterization of the current products of social tagging services in biomedical, academic contexts. this is achieved through an empirical assessment of the tags used to describe citations in pubmed by users of connotea and citeulike. selecting pubmed citations as the resource-focus for this investigation makes it possible to compare socially generated metadata, produced initially to support disparate personal needs, directly with professionally generated metadata produced for the express purpose of enabling applications that serve the whole community. where commonalities are noted, similar kinds of community-level uses can be imagined for the socially generated metadata; where differences occur, opportunities are raised to envision new applications.

RESULTS
resource coverage
in the life sciences, the total number of items described by social tagging systems is currently tiny in comparison to the number of resources described by institutions. to illustrate, the medline bibliographic database contains over  <dig> million citations  <cit>  while, as of november  <dig>   <dig>  citeulike, the largest of the academic social tagging services, contained references to only about  <dig>  of these documents. figure  <dig> plots estimates of the numbers of new citations  added to both pubmed and citeulike per month over the past several years. the chart provides a visualization of both the current difference in scale and an indication of the rates of growth of both systems. it shows that both systems are indexing more items every month, that citeulike appears to be growing slightly faster then pubmed, and that citeulike is approaching  <dig>  unique pubmed citations added per month with medline approaching  <dig> .

this data suggests that, despite the very large numbers of registered users of academically-focused social tagging services - on november  <dig>   <dig>  connotea reported more than  <dig>   - the actual volume of metadata generated by these systems remains quite low. while the sheer numbers of users of these systems renders it possible that this volume could increase dramatically, that possibility remains to be shown.

density
density refers simply to the number of metadata terms associated with each resource described. though providing no direct evidence of the quality of the metadata, it helps to form a descriptive picture of the contents of metadata repositories that can serve as a starting point for exploratory comparative analyses. to gain insight into the relative density of tags used to describe citations in academic social tagging services, we conducted a comparison of the number of distinct tags per pubmed citation for a set of  <dig>  unique citations described by both connotea and citeulike. this set represents the complete intersection of  <dig>  pubmed citations identified in the citeulike data and  <dig>  pubmed citations found in connotea.

'n sampled' refers to the number of tagged citations considered. for example, the first row shows the statistics for the number of tags associated with distinct posts to the connotea service. in contrast, the 'connotea aggregate' row merges all the posts for each citation into one. in the aggregate cases, the numbers of tags reported refer to the number of distinct tags -- repeats are not counted.

in terms of tags per post, the users of citeulike and connotea were very similar. as table  <dig> indicates, the mean number of tags added per biomedical document by individual users was  <dig>  for connotea and  <dig>  for citeulike, with a median of  <dig> tags/document for both systems. these figures are consistent with tagging behaviour observed throughout both systems and with earlier findings on a smaller sample from citeulike which indicated that users typically employ 1- <dig> tags per resource  <cit> . on independent samples of  <dig>  posts  for both citeulike and for connotea, including posts on a wide variety of subjects, the medians for both systems were again  <dig> tags/document and the means were  <dig>  tags/document for citeulike and  <dig>  for connotea. the difference in means is driven, to some extent, by the fact that citeulike allows users to post bookmarks to their collections without adding any tags while connotea requires a minimum of one tag per post. other factors that could influence observed differences are that the user populations for the two systems are not identical nor are the interfaces used to author the tags. in fact, given the many potential differences, the observed similarity in tagging behaviour across the two systems is striking.

as more individuals tag any given document, more distinct tags are assigned to it. after aggregating all of the tags added to each of the citations in the sample by all of the different users to tag each citation, the mean number of distinct tags/citation for connotea was  <dig>  and the mean number for citeulike was  <dig> . this difference is a reflection of the larger number of posts describing the citations under consideration by the citeulike service. in total,  <dig>  citeulike tagging events produced tags for the citations under consideration while data from just  <dig>  connotea tagging events were considered.

overall, the subject descriptors from medline exhibited a much higher density, at a mean of  <dig>  and median of  <dig> descriptors per citation, than the social tagging systems as well as a lower coefficient of variation across citations. figures  <dig>   <dig> and  <dig> plot the distribution of tag densities for connotea, citeulike, and medline respectively. from these figures we can see that even after aggregating the tags produced by all of the users, most of the citations in the social tagging systems are described with only a few distinct tags. note that the first bar in the charts shows the fraction of citations with zero tags .

one of the reasons for the low numbers of tags/citation, even in the aggregate sets, is that most citations are tagged by just one person, though a few are tagged by very many. to illustrate, figures  <dig>   <dig>   <dig> and  <dig> plot the number of citations versus the number of users to post each citation in the connotea-citeulike-medline intersection. figures  <dig> and  <dig> show the data from connotea on both a linear  and logarithmic scale  and figures  <dig> and  <dig> show the equivalent data from citeulike. the plots clearly indicate exponential relationships between the number of resources and the number of times each resource is tagged that are consistent with previous studies of the structure of collaborative tagging systems  <cit> .

current levels of tag density are indicative, but the rates of change provide more important insights regarding the potential of these young systems. figures  <dig> and  <dig> plot the increase in distinct tags/citation as more connotea  and citeulike  users tag pubmed citations. these figures suggest that in order to reach the same density of distinct tags per resource as mesh descriptors per resource produced by medline , roughly  <dig> to  <dig> social taggers would need to tag each citation. since, at any given time it appears that the vast majority of citations will be described by just one person, as indicated in figures  <dig>   <dig>   <dig> and  <dig>  the data suggests that the density of distinct socially generated tags used to describe academic documents in the life sciences will remain substantially lower than the density of institutionally created subject descriptors. this prediction is, of course, dependent on current parameters used for the implementations of academic social tagging systems. as interfaces for adding tags change, the density of tags per post as well as the level of agreement between the different taggers regarding tag assignments may change.

inter-annotator agreement
measures of inter-annotator agreement quantify the level of consensus regarding annotations created by multiple annotators. where consensus is assumed to indicate correctness, it is used as measure of quality. the higher the agreement between multiple annotators, the higher the perceived confidence in the annotations.

in a social tagging scenario, agreement regarding the tags assigned to particular resources can serve as a rough estimate of the quality of those tags from the perspective of their likelihood to be useful to people other than their authors. when the same tag is used by multiple people to describe the same thing, it is more likely to directly pertain to the important characteristics of the item tagged  than to be of a personal or erroneous nature . rates of inter-annotator agreement can thus be used as an approximation of the quality of tag assignments from the community perspective. note that, as  <cit>  discusses, there may be interesting, community-level uses for other kinds of tags, such as those bearing emotional content. for example, tags like 'cool' or 'important' may be useful in the formation of recommendation systems as implicit positive ratings of content. however, the focus of the present study is on the detection and assessment of tags from the perspective of subject-based indexing. note also that the small numbers of tags per document in the systems under consideration here bring into question the relationship between consensus and quality.

to gauge levels of inter-annotator agreement, we calculate the average level of positive specific agreement  regarding tag assignments between different users  <cit> . psa is a measure of the degree of overlap between two sets - for example, the sets of tags used to describe the same document by two different people. it ranges from  <dig>  indicating no overlap, to  <dig>  indicating complete overlap.  for this study, we measured psa for tag assignments at five different levels of granularity: string, standardized string, umls concept, umls semantic type, and umls semantic group. at the first level, psa is a measurement of the average likelihood that two people will tag a document with exactly the same string of characters. at the next level, we measure the likelihood that two people will tag the same resource with strings of characters that, after syntactic standardization , are again exactly the same. moving up to the level of concepts, we assess the chances that pairs of people will use tags that a) can be mapped automatically to concept definitions in the umls and b) map to the same concepts.  at the level of semantic types, we are measuring the degree to which pairs of taggers are using the same basic kinds of concepts where these kinds are each one of the  <dig> semantic types that compose the nodes of the umls semantic network  <cit> . at the uppermost level, we again measure the agreement regarding the kinds of tags used, but here, these kinds are drawn from just  <dig> top-level semantic groups designed to provide a coarse-grained division of all of the concepts in the umls  <cit> . table  <dig> provides examples from each of these levels.

the reason for including multiple levels of granularity in the measures of agreement is to provide a thorough comparison of the meanings of the tags. since the tags are created dynamically by users entering simple strings of text, we expect large amounts of variation in the representations of the same concepts due to the presence of synonyms, spelling errors, differences in punctuation, differences in plural versus singular forms, etc. the mapping to umls concepts should help to reduce the possibility of such non-semantic variations masking real conceptual agreements. furthermore, by including analyses at the levels of semantic types and semantic groups, we can detect potential conceptual similarities that exact concept matching would not reveal. 

one interpretation of the low levels of agreement is that some users are providing incorrect descriptions of the citations. another interpretation is that there are many concepts that could be used to correctly describe each citation and that different users identified different, yet equally valid, concepts. given the complex nature of scientific documents and the low number of concepts identified per post, the second interpretation is tempting. perhaps the different social taggers provide different, but generally valid views on the concepts of importance for the description of these documents. if that is the case, then, for items tagged by many different people, the aggregation of the many different views would provide a conceptually multi-faceted, generally correct description of each tagged item. furthermore, in cases where conceptual overlap does occur, strength is added to the assertion of the correctness of the overlapping concepts.

to test both of these assumptions, some way of measuring 'correctness' regarding tag assignments is required. in the next several sections, we offer comparisons between socially generated tags and the mesh subject descriptors used to describe the same documents. where mesh annotation is considered to be correct, the provided levels of agreement can be taken as estimates of tag quality; however, as will be shown in the anecdote that concludes the results section and addressed further in the discussion section, mesh indexing is not and could not be exhaustive in identifying relevant concepts nor perfect in assigning descriptors within the limits of its controlled vocabulary. there are likely many tags that are relevant to the subject matter of the documents they are linked to yet do not appear in the mesh indexing; agreement with mesh indexing can not be taken as an absolute measure of quality - it is merely one of many potential indicators.

agreement with mesh indexing
as both another approach to quality assessment and a means to precisely gauge the relationship between socially generated and professionally generated metadata in this context, we compared the tags added to pubmed citations to the mesh descriptors added to the same documents. for these comparisons, we again used psa, but in addition, we report the precision and the recall of the tags generated by the social tagging services with respect to the mesh descriptors. 

for each of the pubmed citations in both citeulike and connotea, we assessed a) the psa, b) the precision, and c) the recall for tag assignments in comparison to mesh terms at the same five semantic levels used for measuring inter-annotator agreement. for each pubmed citation investigated, we compared the aggregate of all the distinct tags added by users of the social tagging service in question to describe that citation with its mesh descriptors. table  <dig> provides the results for both systems at each level. it shows how the degree of agreement with mesh indexing increases as the semantic granularity at which the comparisons are made widens. as should be expected based on the much lower numbers of umls concepts associated with the social tagging events, the recall is much lower than precision at each level.

focusing specifically on precision, we see that approximately 80% of the concepts that could be identified in both social tagging data sets fell into umls semantic groups represented by umls concepts linked to the mesh descriptors for the same resources. at the level of the semantic types, 59% and 56% of the kinds of concepts identified in the connotea and citeulike tags respectively, were found in the mesh annotations. finally, at the level of umls concepts, just 30% and 20% of the concepts identified in the connotea and citeulike tags matched concepts from the mesh annotations.

improving agreement with mesh through voting
the data in table  <dig> represents the conceptual relationships between mesh indexing and the complete, unfiltered collection of tagging events in citeulike and connotea. in certain applications, it may be beneficial to identify tag assignments likely to bear a greater similarity to a standard like this - for example, to filter out spam or to rank search result lists. one method for generating such information in situations where many different opinions are present is voting. assuming that there is a greater tendency for tag assignments to agree with the standard than to disagree - where multiple tag assignments for a particular document are present - then the more times a tag is used to describe a particular document the more likely that tag is to match the standard.

to test this assumption in this context, we investigated the effect of voting on the precision of the concepts linked to tags in the citeulike system with respect to mesh indexing.  figure  <dig> illustrates the improvements in precision gained with the requirement of a minimum of  <dig> through  <dig> 'votes' for each concept, semantic type, or semantic group assignment. as the minimum number of required votes increases from  <dig> to  <dig>  precision increases in each category. at a minimum of  <dig> votes, the precision of semantic types and semantic groups continues to increase, but the precision of individual concepts drops slightly from  <dig>  to  <dig> . we did not measure beyond five votes because, as the minimum number of required votes per tag increases, the number of documents with any tags drops precipitously. for documents with no tags, no measurements of agreement can be made. figure  <dig> illustrates the decrease in citation coverage associated with increasing minimum numbers of votes per tag assignment. requiring just two votes per tag eliminates nearly 80% of the citations in the citeulike collection. by  <dig> votes, only  <dig> % of the citations in the dataset can be considered. this reiterates the phenomenon illustrated in figures  <dig>   <dig>   <dig> and  <dig> - at present, most pubmed citations within academic social tagging systems are only tagged by one or a few people.

an anecdotal example where many tags are present
though the bulk of the socially generated metadata investigated above is sparse - with most items receiving just a few tags from a few people - it is illuminating to investigate the properties of this kind of metadata when larger amounts are available both because it makes it easier to visualize the complex nature of the data and because it suggests potential future applications. aside from enabling voting processes that may increase confidence in certain tag assignments, increasing numbers of tags also provide additional views on documents that may be used in many other ways. here, we show a demonstrative, though anecdotal example where several different users tagged a particular document and use it to show some important aspects of socially generated metadata - particularly in contrast to other forms of indexing.

in some cases, the tags added by the users of the social tagging systems are more precise than the terms used by the mesh indexers. for example, the main experimental method used in the article was two-photon microscopy - a tag used by two different social taggers . the mesh term used to describe the method in the manuscript is 'microscopy, confocal'.

within the mesh hierarchy, two-photon microscopy is most precisely described by the mesh heading 'microscopy, fluorescence, multiphoton' which is narrower than 'microscopy, fluorescence' and not directly linked to 'microscopy, confocal'; hence it appears that the social taggers exposed a minor error in the mesh annotation. in other cases, the social taggers chose more general categories - for example, 'hemodynamics' in place of the more specific 'blood volume'.

the tags in figure  <dig> show two important aspects of socially generated metadata: diversity and emergent consensus formation. as increasing numbers of tags are generated for a particular item, some tags are used repeatedly and these tend to be topically relevant; for this article, we see 'astrocytes' and 'vision' emerging as dominant descriptors. in addition to this emergent consensus formation  other tags representing diverse user backgrounds and objectives also arise such as 'hemodynamic'. 'neuroplasticity', 'two-photon', and 'wow'. in considering applications of such metadata, both phenomenon have important consequences. precision of search might be enhanced by focusing query algorithms on high-consensus tag assignments or by enabling boolean combinations of many different tags. recall may be increased by incorporating the tags with lower levels of consensus.

while we assert that this anecdote is demonstrative, a sample of one is obviously not authoritative. it is offered simply to expose common traits observed in the data where many tags have been posted for a particular resource.

discussion
the continuous increase in the volume of data present in the life sciences, illustrated clearly in figure  <dig> by the growth of pubmed, renders processes that produce value-enhancing metadata increasingly important. it has been suggested by a number of sources that social tagging services might generate useful metadata, functioning as an effective intermediate between typically inexpensive, but low precision automated methods and expensive professional indexing involving controlled vocabularies  <cit> . evidence in favour of this claim comes from reported improvements in the relevance of web search results gained by integrating information from social tagging data into the retrieval process  <cit> . where a substantial density of socially generated tags is present, we demonstrated that it is possible to achieve both deep resource descriptions  and improvements in annotation precision via aggregation . unfortunately, the results presented here also suggest that much of this potential is as yet unavailable in the context of the life sciences because the coverage of the domain is still very narrow and the number of tags used to describe most of the documents is generally very low.

if metadata from social tagging services is to be useful in support of applications that are similar in purpose and implementation to those currently in operation, more documents need to be tagged and more tags need to be assigned per document. these objectives can be approached by both expanding the number of users of these systems and adjusting the interfaces that they interact with. looking forward, the increasing volume of contributors to social tagging services should help to increase resource coverage and, to some extent, tag density, yet both the rich-get-richer nature of citation and the limited actual size of the various sub-communities of science will likely continue to result in skewed numbers of posts per resource. to make effective use of the annotations produced by social tagging applications, the metadata generated by individual users needs to be improved in terms of density and relevance because, in most cases, the number of people to tag any particular item will be extremely low. identifying design patterns that encourage collectively useful tagging behaviour is thus a critical area for future investigations. it has been shown that careful interface and interaction design can be used to guide individual users towards tagging behaviours that produce more useful metadata at the collective level  <cit> . future research will help to provide a better understanding of this process, illuminating methods for guiding user contributions in particular directions, e.g. towards the use of larger numbers of more topical tags, without reducing the individual benefits of using these systems that seem to provide the primary incentive for participation  <cit> . one such experiment in interaction design would be to inform the users of these systems that the annotations they create for themselves are to be used in the creation of applications that operate on a collective level and thus benefit the community as a whole. by making the desire to create such applications known and by explaining the attributes of the annotations required to make these applications effective, it is possible that some individuals might act intentionally to improve the collective product. such an experiment would help to shed light on the question of why there are such differences between the tagging behaviours of typical users and the annotations produced in professional contexts. perhaps an increased overlap in purpose would result in increased overlap in product.

aside from such overt requests, changes to the interfaces used to author annotations within social tagging systems might also have substantial effects. one key area of development in terms of tagging interface design is the incorporation of controlled vocabularies into the process. emerging systems in this domain let users tag with controlled terms  <cit>  and automatically extract relevant keywords from text associated with the documents to suggest as potential tag candidates  <cit> . by providing the well-known benefits of vocabulary control - including effective recognition and utilization of relationships such as synonymy and hyponymy - and by gently pressing users towards more convergent vocabulary choices and fewer simple spelling errors, such systems seem likely to produce metadata that would improve substantially on that analyzed here. in preliminary investigations of such 'semantic social tagging' applications - including faviki  <cit> , the entity describer  <cit> , and zigtag  <cit>  - the degrees of inter-tagger agreement do appear higher than for the free-text interfaces however the number of tags per document remains about the same . systems that aid the user in selecting tags - for example, by mining them from relevant text - may aid in the expansion of the number of tags added per document.

in addition to recruiting more users and producing interfaces that guide them towards more individually and collectively useful tagging behaviours, additional work is needed to better understand other aspects of the metadata from social tagging systems that are both important and completely distinct from previous forms of indexing. for example, one of the fundamental differences between socially generated and institutionally generated indexes is the availability of authorship information in the social data  <cit> . it is generally not possible to identify the person responsible for creating the mesh indexing for a particular pubmed citation, but it is usually possible to identify the creator of a public post in a social tagging system. this opens up whole new opportunities for finding information online whose consequences are little understood. for example, it is now possible for users to search based on other users e.g. searching for items in connotea that have been tagged by 'mwilkinson'  <cit>  or 'bgood'  <cit> . in addition to this simple yet novel pattern of information interaction, research is being conducted into ways to incorporate user-related data into keyword-based retrieval algorithms  <cit> .

CONCLUSIONS
academic social tagging systems provide scientists with fundamentally new contexts for collaboratively describing, finding, and integrating scientific information. in contrast to earlier forms of personal information management, the public nature and open apis characteristic of social tagging services make the records of these important scientific activities accessible to the community. these new public metadata repositories provide a novel resource for system developers who wish to improve the way scientists interact with information.

based on the results presented above, it is clear that the information accumulating in the metadata repositories generated through social tagging offers substantial differences from other kinds of metadata. in particular, both the number of documents described by these systems and the density of tags associated with each document remain generally very low and very unequally distributed across both the user and the document space. while expanding numbers of user-contributors and improving user interfaces will likely help to encourage the formation of greater numbers of tagged documents and more useful tags, the unbalanced distribution of scientific attention will almost certainly result in the continuation of the skewed numbers of taggers  per document displayed in figures  <dig>   <dig>   <dig> and  <dig> 

at a broad level, the key implication of these results from the standpoint of bioinformatics system design is that - despite surface similarities - these new metadata resources can not be used in the same manner as metadata assembled in other ways. rather, new processes that make use of the additional social context made accessible through these systems need to be explored. in the long run, it may turn out that the primary benefit of social tagging data might not be found in the relationships between tags and documents as explored here but instead in the information linking documents and tags to users and users to each other.

