BACKGROUND
cellular plasticity is the key property essential for multi-cellular development  <cit> , tissue maintenance  <cit>  and regeneration  <cit> . while the notion of state transitions from multipotent stem cells to mature functional cells is established, the breakthrough findings on transdifferentiation  <cit>  and reprogramming  <cit>  have sparked renewed interest into mechanisms driving cellular lineage choice with the prospect of therapeutic application  <cit> .

to understand differentiation kinetics and thus the origins of stem cell population heterogeneity, one has to observe the transition of cells between states of different lineage potential. however, observing cell state transitions is impossible in data obtained from a single time point, emerging from e.g. flow cytometry, transcriptome or immunofluorescence analyses. for example, a clonal colony of differentiated cells may have originated from a single differentiated cell following multiple divisions, or from the simultaneous differentiation of multiple cells after a few divisions. continuous time information and the tracking of individual cells is necessary to distinguish the two possibilities.

live cell imaging allows to observe state transitions e.g. via cell surface markers or cell morphology , but it cannot immediately provide a mechanistic explanation why the transition occurs. for example, the differentiation rate of a stem cell towards a more mature cell type may depend on time  <cit>  , cell density  <cit>  , the makeup of surrounding niche cells  <cit>  or on a combination of these features. while it is possible to quantify the emergence of cellular patterns in colonies  <cit> , it is impossible to tell from the mere observation if the simultaneous differentiation of multiple cells is a random event or if it is triggered by, e.g., the increased density in the colony. the inference of features predictive of this state transition rate requires robust statistical analysis, and thus a large number of time-lapse microscopy data, which is in particular for mammalian systems still a challenging and labor intensive task  <cit> .
fig.  <dig> state transitions observed via time-lapse microscopy can be explained by different mechanisms. a during a time-lapse microscopy experiment cells are imaged over multiple time points. from these images, spatial configuration, cell proliferation and changes in cell state, e.g. via surface markers  can be obtained. however, these observations do not inform about the underlying mechanisms that caused the transition in cell state. for example, the state transition could be entirely random , where cells spontaneously undergo state transitions , it could depend on time , such that the transition rate changes in the course of the experiment . alternatively, the transition could depend on local cell density , e.g. cells with higher local cell density preferentially transit from one state to the other



here, we present a model and analysis framework that can infer the spatiotemporal features predictive of state transitions and also allows to estimate the number of samples required for this analysis. to validate the performance of our framework, we first simulate cellular genealogies from a generative spatiotemporal model for different scenarios of transition rate dependencies. we then develop an inference method based on generalized linear models  and feature selection with l <dig> regularization. we show that our method is able to correctly identify the transition rate as a multi-feature function and determine the number of required genealogies and allowed tracking errors for different scenarios. finally, we use the correlations between cell siblings to validate the chosen approach and detect shortcomings – either due to non-considered features, or due to cell-internal effects that drive cell state transitions.

methods
a generative model for spatiotemporal cellular genealogies
throughout this paper, we use a simple model of cell state transition with two cellular states i and ii . a single cell in state i  can divide into two cells in state i, or transition into another state ii , where it can only divide. this unidirectional state transition could for example model cell differentiation, where a progenitor transforms into a more differentiated cell type, but the reverse transition does not occur naturally. the transition rate λ) of a cell i from state i to state ii depends on the features fi of the cell. notably, the features f, like time, cell cycle state, position or local cell density, can change over time. specific examples of the function λ) are introduced later on .
fig.  <dig> spatiotemporal simulation and analysis of cell state transitions. a in our model, a cell in state i  can divide or transition into state ii . the transition is governed by the transition rate λ, which can depend on features like time, position, cell cycle, or the local cell density. this unidirectional transition model is inspired by cellular differentiation where a undifferentiated progenitor cell irreversibly transitions into a more differentiated cell type. b visualization of a cellular genealogy in space and time with cells in state i  and state ii . c tree view of the genealogy depicted in b . d local cell density is modeled via a set of annular basis functions ϕ
k with inner radii k
Δ
r and constant thickness Δ
r . cells are indicated as crosses. e linear combinations of the ϕ
k can approximate any density dependence . f the tree structured data is transformed into a data matrix by discretizing time  and creating one sample  for each cell at each time interval, simulating a measurement process. for each cell i and each timepoint t, we record different features , e.g. cell coordinates x
i and y
i, the spatial features ϕ0i,ϕ1i,…  and state transition events y within the time interval



mathematically, in our model a single cell is defined by its 2d spatial coordinates x∈r <dig> , its state y∈  <cit> , where y= <dig>  if the cell is in state i  and its age τ∈r+, i.e. the time since the last division. the cell division rate γ is age dependent to account for non-exponential lifetime of cells . this system of dividing cells that undergo state transitions evolves probabilistically in time and has to be described by a master equation , whose derivation is sketched in additional file 1: section  <dig>  instead of solving the intractable master equation, we simulated realizations of the underlying stochastic process : since the system has continuous  and discrete  variables, a standard stochastic simulation algorithm cannot be applied and a hybrid simulation method must be used . cell position is treated as brownian motion  and is updated via an euler-maruyama scheme  <cit> .

to evolve the cell state in time for a single cell in state i, the simulation proceeds in small time steps Δt, during which a state transition event takes place with probability  
 pi=1−e−∫tt+Δtλ)dt′≈1−e−λ)·Δt 

for some arbitrary, state and time-dependent transition rate λ). the rate λ is evaluated at the beginning of each iteration, and the time step Δt is chosen sufficiently small . the cell divides after  <dig> hours on average, corresponding to the typical lifetime of mammalian stem and progenitor cells  <cit>  . the cell division replaces the dividing cell by two daughter cells, with positions close to that of the mother cell and with the same cell state: e.g. a mother cell in state i gives rise to two daughters in state i. these cells are then simulated in parallel. over the course of the simulation, a cellular genealogy with a distinct cell state pattern emerges . genealogies are simulated for  <dig> hours  corresponding to the typical observation periods of long term time-lapse microscopy .

local cell density
local cell density for a single cell is estimated using a kernel f that determines how much each cell contributes to the local density at a certain point x in space as a function of intercellular distance. we define the local cell density ρif of cell i at time t with respect to a kernel f:r→: 
  ρif=∑j≠ifd,xj), 

where xi is the spatial coordinate of cell i at time t and d denotes euclidean distance. we use either a tophat kernel  with 
  f=i, 

where i is the indicator function of  <cit> , or a gaussian kernel  with 
  f=12πσe−r22σ <dig>  

for the tophat kernel each cell within distance r contributes equally to the local density experienced by cell i, whereas cells with distance larger than r do not contribute at all. for the gaussian kernel the contribution to the local cell density decreases smoothly with distance.

local cell density as a linear combination of basis functions
in order to model and estimate any  density kernel f, we approximate f as a linear combination of basis functions ϕk, k= <dig> ,… 
  f≈∑kωk·ϕk, 

where the ϕk are defined as 
 ϕk=ikΔr<r≤Δr, 

and i denotes the indicator function. ϕk resembles a ring of inner radius kΔr and thickness Δr . for example, we can recover the tophat kernel with radius r  by choosing the coefficients ωk as 
 ωk= <dig> kΔr<r <dig> kΔr≥r. 

for our analysis, we choose Δr= <dig> μm, which allows to resolve short range interactions on the order of eukaryotic cell diameter  but also long range interactions due to diffusive signaling molecules .

cell state transition scenarios
we create four datasets corresponding to different scenarios of cell state transition:

 <dig>  we consider a scenario where the transition rate is constant , resembling spontaneous transitions independent of other effects: 
  λ)=c, 

with c= <dig>  h− <dig>  thus, a state transition in a cell with a typical  <dig> h lifetime will occur with probability p= <dig> .

 <dig>  for a time-dependent scenario , the transition rate is chosen as 
  λ)=a·t, 

i.e. linearly increasing with time . note that λ does not depend on any other feature f of the cell. a time-dependent transition rate might for example be encountered in an in vitro stem cell system, where primary stem cells are isolated, separated from the stem cell niche. over time the stem cells are depleted of crucial signaling molecules previously supplied by the niche cells and start transitioning into more mature cells.

 <dig>  for a density-dependent scenario , the local density of a cell i at time t is mediated by a tophat kernel  with r= <dig> μm, which is roughly the distance a cell can move in its lifetime . the transition rate λ is then defined by 
  λt,ρitophat=b·ρitophat, 

with b= <dig>  h− <dig>  density-dependent transition rates might be relevant for in vitro cultures of embryonic stem cells, which are known to differentiate when cell density becomes too large  <cit> . another example for density-dependent transitions are bacteria that use quorum sensing to estimate local cell density and base their fate decision on that, e.g. by becoming virulent  <cit> .

 <dig>  for a time and density-dependent scenario , the contributions of the previous two factors are summed, using a gaussian kernel  to define cell density: 
  λt,ρigauss=a·t+b·ρigauss. 

non-parametric estimation of the transition rate
given a dataset as described above, we now delineate two methods to estimate the transition rate from the data. first, the transition rate λ can be estimated non-parametrically by considering the definition of the rate as the probability of a transition in an infinitesimal time dt: 
  p)=λ)·dt, 

where p) is the probability for a transition in the interval  in the presence of the features f. we estimate the probability p of a state transition in  given features f as 
 p^=number of transition events|number of cells in state i|, 

which is the fraction of candidate cells  that transit into state ii in  having features f. after rearranging eq.  <dig>  we obtain 
  λ^=1Δt·number of transition events|number of cells in state i| 

to measure the uncertainty of the estimates, we calculate bayesian credibility intervals .

estimating the transition rate via generalized linear models
the transition rate can be inferred systematically using a machine-learning framework. we consider every timepoint of each cell as an observed sample ,y), where f is a set of features measured for this sample . we use superscripts to index the samples to clearly distinguish it from the per-cell indexing via subscripts used previously. y∈{ <dig> } denotes the class label of the sample being either “state i” =0) or “transition into ii” =1). a sample is considered as y= <dig> if a state transition occurred in the time interval of the sample. timepoints after the state transition  are discarded  since we are interested in what actually triggers the transition of cells, not the state of the cell itself. counter-intuitively, all samples ,y) are independent, even though, e.g. adjacent samples typically are strongly correlated with respect to their features .

we use generalized linear models  to learn the relation between features f and class labels y as 
 e|f,w)=μ=g−1), 

where μ is the expected value of an exponential family distribution, g− <dig> is called the mean function, and w is the weights vector that has to be learned from the data. choosing a bernoulli distribution and an exponential mean function would exactly correspond to our data generating process . however, this specific glm has unfavorable numerical properties leading to convergence issues  <cit> . therefore, we resort to a glm that has the desired exponential mean function but a poisson instead of a bernoulli distribution  and has better numerical properties. note that poisson regression is generally used to model count data ∈n0), but is a good approximation to binary data ∈{ <dig> }) in the case of rare events . thus, we obtain the following log-likelihood : 
  logp=∑iywtf−ewtf−log!). 

feature selection via l <dig> regularization
to determine the relevant features of the transition rate and to exclude features that only indirectly influence the state transition , we apply l <dig> regularization to the glm, also known as lasso   <cit> . here one minimizes the following function with respect to the weights w: 
  g=−logp+κ·∥w∥ <dig>  fig.  <dig> features regulating the transition rate λ can be estimated non-parametrically from cellular genealogies with annotated state transition events. a the transition rate estimated from  <dig> genealogies  agrees well with the true constant transition rate . gray areas indicate the  <dig> % credibility region of the estimate. b the transition rate estimated from  <dig> genealogies simulated with linear time-dependent rate agrees well with the true rate . c the transition rate as a function of local cell density ρ for  <dig> genealogies simulated with density-dependent rate. the estimated transition rate seems to depend on both local density ρ  and time . d the estimated transition rate λ^ as a function of both density and time reveals that the time-dependence observed in the inset in c is an indirect influence . instead, the transition rate depends only on local cell density ρ 



with ∥w∥1=∑i|wi|. this regularization is equivalent to placing a laplace prior with location parameter m= <dig> and scale parameter b=κ− <dig> on the weights  <cit> , resembling our knowledge that most of the weights should be zero and the resulting model should be sparse. depending on the chosen regularization strength κ, one obtains models of differing sparsity . we follow the standard approach to determine the optimal regularization parameter κ∗: for each κ, we perform ten-fold cross validation using the deviance of the model as the error criterion and choose κ∗ based on the 1se rule  <cit> : we select the largest κ  that in terms of its deviance is still within one standard error of the best κ. optimization and cross validation of lasso is performed using the function lassoglm() from the matlab statistics toolbox.
fig.  <dig> regularized generalized linear models  select the relevant features predictive of cell state transitions. a regularization path of the glms applied to the density dependent dataset. the means  and standard deviations  of the regression weights w are plotted against the regularization strength κ across  <dig> bootstrap samples . the mean of the optimal regularization strength κ
∗ determined by cross validation is shown as a vertical black line. solid  lines correspond to relevant  features in the respective scenario. b percentage of bootstrap samples that included the respective features. included features were determined as those with non zero weights at κ
∗. enforcing a  <dig> % threshold  on the inclusion probability for each feature, we select the relevant features of the model. the features ϕ
 <dig> ϕ
 <dig> are not included as their effect is too weak to be detected by the glm at the current sample size . c reconstructed kernel of local cell density  from the selected features in b. the true underlying tophat kernel shape is shown in black. as in b, the features ϕ
 <dig> ϕ
 <dig> are not included because their effect is to weak. d-f analogous to a-c, but for a dataset where the transition rate λ depends on time and local cell density with a gaussian kernel. both features are correctly identified and the density kernel is correctly estimated



additionally, we have to account for the fact that the classes in our dataset are severely imbalanced with more non-events than events . such class imbalance can lead to problems for machine learning algorithms  <cit> . therefore, we down-sample the majority class =0) to achieve a ratio of 1: <dig>  yielding a good tradeoff between class balance and number of overall samples. feature selection using lasso is applied to this down-sampled dataset via eq. . since down-sampling intentionally discards data and lasso feature selection is sensitive to data perturbation  <cit> , we repeat the procedure n= <dig> times, each time using a different sample of the majority class, combining it with the minority class and fitting the lasso to this dataset. this approach is adapted from rare event logistic regression with replication  <cit>  and is reminiscent of bootstrap lasso  <cit> . finally, for each feature, we record the probability of inclusion in the model, i.e. the percentage of the n iterations that included the feature into the model at the optimal regularization strength κ∗. we consider those features to be relevant that have an inclusion probability larger than  <dig> %  <cit> . this yields the final set of features for our model. we now fit this sparse model to the full data without the l <dig> penalty , since l <dig> regularization is biased towards too small weights. we thus obtain our final model, its associated weights ŵ and the corresponding transition rate λ^=−ŵtf·Δt.

the inclusion probability threshold  controls the probability α of a type i error, i.e. including a feature even tough it is irrelevant. in addition it is also important to assess the probability β of type ii errors, i.e. the chance that a relevant feature is not included into the model, or equivalently, the statistical power=1−β, which is the probability of discovering the feature if it is indeed relevant. the power is a function of sample size and effect size : the more samples are available and the larger the effect size, the higher to probability to discover a relevant feature. since no analytical expressions are available, we estimate the statistical power of our model with respect to a certain feature through repeated simulation: given a fixed sample size and effect size, we generate m independent datasets, apply the above glm with bootstrapping-based feature selection to each dataset, resulting in m different models, which might have selected different features. we then approximate the statistical power as the fraction of the m models that correctly selected the feature of interest. since computations become demanding , we choose m= <dig>  and m= <dig> .
fig.  <dig> the method’s performance is robust for different sample size and effect size. a the statistical power for each feature  plotted against sample size . relevant features have high probability of being included in the model  when  <dig> or more transition events  are used for the analysis. solid  lines correspond to relevant  features in the respective scenario. b the power for the density feature ϕ
 <dig> shown as a function of sample size and relative effect size. the red line indicates the section corresponding to a with a relative effect size of  <dig>  as expected, power increases with sample size and relative effect size. c,d the power as a function of sample size and relative effect size for all c relevant and irrelevant d features of the scenario. colorbar as in b. e-h analogous to a-d, but for a dataset used in fig. 4
d-f, where the transition rate λ depends on time and local cell density with a gaussian kernel

fig.  <dig> the method’s performance is robust for moderate amount of tracking error. a statistical power plotted against the amount of tracking error for the density dependent scenario from fig. 4
a-c . solid  lines correspond to relevant  features in the respective scenario. the correct features are identified reliably  up to a tracking error of  <dig> %. for larger tracking error, there is a high probability to include time  into the model even though it is only an indirect influence. note that tracking error seems to some extent facilitate the detection of ϕ
 <dig> ϕ
 <dig> . b analogous to a, but for the dataset where the transition rate λ depends on time and local cell density with a gaussian kernel 



expected frequencies of subtree patterns in cellular genealogies
having estimated the transition rate λ^ via the regularized glm, we calculate the number of subtree patterns expected under this transition rate. in the following we consider only subtrees of  <dig> generation, i.e. a mother and its two daughter cells, but the approach is easily extendable to larger subtree patterns. the expected frequencies of sister cell pairs where in either both cells, one cell, or none of the two cells state transition occurs, can be used to validate the inferred transition rate . we define the random variable ci to indicate whether cell i underwent a state transition within its lifetime  or stayed in state i . note that the ci describe the state of a cell over its entire lifetime, as opposed to the y used in the previous section, which denote the state of a cell at a small time interval Δt. using the estimated transition rate λ^, we calculate the probability of a state transition in a single cell i as 
  p=pi=1−exp−∫ζiηidsλ^) fig.  <dig> expected frequencies of sister pairs reveal if the model can account for the observed genealogical correlations. a comparison of the observed and expected frequencies of sister pairs  of the dataset used throughout fig. 4
a-c shows no significant difference . fitting the same data, but not accounting for the ϕ
 <dig> ϕ
 <dig> features causes significant deviations from the expected frequencies . b p-values of the χ
2-test  to compare the observed and expected frequencies of sister pairs against amount of tracking error for the density dependent scenario. for tracking errors < <dig> %, the method correctly concludes that the frequencies of observed sister pairs are in agreement with the model 



where λ^) is the estimate of the transition rate the cell experiences throughout its lifetime  based on its features fi . similarly, we derive the probability of a state transition in its sister cell i′ as p. considering the whole dataset containing m pairs of sister cells ,i=1…m, the expected number of pairs where both sister undergo a state transition is: 
 e2=∑i=1mp, 

where p is the joint probability of these events. however, assuming independence between sisters, this factorizes to 
 e2=∑i=1mp·p=∑i=1mpi·pi′. 

the expected number of pairs where a state transition occurs in only one sister  and in none of the sisters  are: 
 e0=∑i=1m·e1=∑i=1m·pi′+pi·. 

applying eq.  <dig>  we can evaluate  in terms of the estimated transition rate λ^.

in order to test whether our observed data matches these expected frequencies  we count the observed frequencies  in the data and perform a χ <dig> test with two degrees of freedom and 
 χ2=∑j=132ej. 

RESULTS
in the following, we use our generative model to simulate datasets from the simple cell state transition model  according to four different scenarios, where the transition rate λ depends on different features . subsequently, we apply our proposed inference methods to the data from different scenarios, assuming the data generating scenario is unknown. we show how the dependence of λ can be recovered from the data, e.g. allowing us to distinguish density- and time-dependent scenarios. furthermore, we analyze the impact of sample size and tracking error on our results in order to assess the required experimental design.

estimating the transition rate non-parametrically from simulated data
in the simplest scenario the rate λ is constant during the whole time of observation . this corresponds to state transitions occurring spontaneously independent of other influences. using our generative model for cellular genealogies , we generate a sample of  <dig> genealogies with constant rate λ. we then reconstruct the rate λ^ from the data via eq.  <dig> . the underlying true rate λ  is well contained within the bayesian  <dig> % credibility intervals of our estimate .

next, we simulate  <dig> genealogies with a linear time-dependent transition rate . with the same approach we estimate λ^  and again, we observe good agreement between the estimated  and the true transition rate .

we now account for cell-cell communication and consider a transition rate depending on local cell density : the more cells present in the vicinity of the cell of interest, the more likely it is that a state transition occurs. we estimate the density dependent rate from  <dig> simulated genealogies, assuming we already know the underlying density kernel . the estimated rate λ^  linearly increases with local cell density and the true rate is well contained within the credibility intervals , showing that one can identify the influence of local cell density on the transition rate. note that the estimates of the transition rates at high density  carry large statistical uncertainty  simply because very few cells are observed in those high local cell densities.

however, if we instead estimate the rate as a function of time from the same dataset, we would conclude that it is time-dependent, since the rate strongly increases over time . this is an indirect dependence: as time increases, local cell density grows exponentially and as a result, cells are more prone to undergo a state transition . we can resolve this by estimating the rate simultaneously as a function of time and local density, λ^ . for fixed local density ρ, the rate is almost constant across different times . however, the transition rate changes considerably if the local density changes. therefore, we can conclude that the true transition rate depends only on local cell density. notice however that this conclusion relies on having sufficiently many samples, yielding a good coverage of the  space, and knowledge of the range  and nature of the spatial interaction. if r is chosen too small, any dependence of λ on the local cell density is hidden by the dominating indirect time-dependence. moreover, analyzing λ^ visually becomes infeasible for higher feature dimensions.

estimating the transition rate with generalized linear models
to approach the aforementioned issues, we infer the transition rate more systematically using the machine-learning framework of generalized linear models . instead of considering only one feature at a time, we include all features at once and apply feature selection to determine the relevant ones. an additional advantage of this approach is that it is not necessary to assume any density kernel a priori . instead, we use a set of spatial features ϕk, whose linear combination can approximate any kernel . we then use the proposed glm equipped with l <dig> regularization to learn the relationship between features and class label and to obtain those features that directly influence the state transition rate.

we apply this approach to the density-dependent dataset . starting with strong regularization  only the most relevant features have non-zero weights and are included . by decreasing the regularization parameter, the weights of the features gradually increase, making the model more complex. the optimal regularization κ∗  is determined by cross validation . all features with non-zero weights at κ∗ are included in the model. the ground truth of features used to simulate the dataset is indicated by solid  and dashed  lines in fig. 4a.

we estimate the inclusion probability of a feature as the fraction of the  <dig> bootstraps that selected the feature . for example, the features ϕ <dig> …,ϕ <dig>  are present in all bootstraps, ϕ <dig> is present in  <dig> % of the bootstraps, and all other features have low inclusion probabilities. in particular, time is included in only  <dig> % of the bootstraps and spatial location  and time since last division  have zero inclusion probability. choosing a cutoff at  <dig> %  for a feature to be included in the final model, we recover all features  that were used to generate that dataset. we miss ϕ <dig> and ϕ <dig> since their contribution to the overall transition rate is effectively very low: the average number of cells within ϕ <dig> is approximately  <dig> , whereas the average number of cells within e.g. ϕ <dig> is approximately  <dig>  hence, leaving out ϕ <dig> will not change the overall result, and the algorithm chooses to neglect the feature in favor of sparsity.

after feature selection, we can reconstruct the density kernel as a weighted sum of the basis functions ϕk via eq.  <dig> . here, we observe that the reconstructed kernel closely resembles the true underlying tophat kernel that was used to simulate the data . to demonstrate that the method can faithfully report the range of the spatial interaction, we performed the same analysis on a dataset with a density dependence mediated via a short range tophat kernel , which indeed can be recovered from the data .

we extend the set of relevant features and now consider a scenario where the transition rate depends on time and on local cell density , this time modeled via a gaussian kernel  instead of a tophat kernel to illustrate the versatility of our method. since the gaussian kernel has infinite support, a priori there is no clear definition which ϕi are relevant. in the following, we define all ϕi inside the  <dig> % quantile of the gaussian distribution as relevant. this results in ϕ <dig> …,ϕ <dig> considered relevant while ϕ <dig> …,ϕ <dig> are deemed irrelevant.

the regularization path and the feature inclusion probabilities  show that the glm correctly selects both time and local cell density  with inclusion probabilities close to  <dig>  finally, using the weights associated with the selected density features we reconstruct the kernel of local cell density and find that it indeed matches a gaussian kernel . as before , the feature selection procedure misses ϕ <dig> due to its relatively small contribution to the overall transition rate. we conclude that our proposed method is capable of identifying the features that are most predictive of the transition rate and faithfully filters out indirect influences. furthermore, we can estimate the shape of the density kernel from the data.

sample size, effect size and statistical power
accurate single-cell identification and tracking in time-lapse movies is still a challenging task and requires, at least in mammalian systems manual data curation  <cit> . thus estimating the required sample size for any given effect size is necessary for efficient experimental design.

to assess the impact of sample size on the performance of the feature selection, we systematically reduce the number of observed state transition events  and calculate the statistical power of our method, i.e. the probability to detect a certain effect if present in the data . starting at the original sample size of  <dig> onsets , we find that the power is  <dig> for the features detected previously , suggesting these features can reliably be detected. similarly, the model’s power with respect to features ϕ <dig> and ϕ <dig> is  <dig>  hence those features are not detectable at the given sample size. decreasing the sample size, the power for certain features gradually drops : the data no more contains sufficient statistical information to identify the feature as relevant. at a sample size below  <dig> events, the power for all features is considerably smaller than one such that non of the features can reliably be identified. however, a sample size of  <dig> onsets  is sufficient  to faithfully detect the most important features influencing the transition rate and to distinguish a direct time-dependence  from an indirect one .

the statistical power does not only depend on the available sample size but also on the strength of the effect, i.e. small effects will be hard to detect for a fixed sample size than a strong one. we therefore vary the effect strength by changing the parameters a and b in eqs.  <dig>   <dig> within one order of magnitude and estimate the power for each feature not only as a function of sample size but also of effect strength . as expected the power increases with increasing sample size and effect strength. for example, in the density dependent scenario, for a large relative effect size of  <dig>   <dig> samples are sufficient to yield a power of  <dig>  for feature ϕ <dig>  while for small effect size  more than  <dig> samples are needed to achieve the same power . furthermore, features ϕ <dig> …,ϕ <dig> can reliably be identified  with more than  <dig> onsets almost independent of the effect strength considered . in contrast, ϕ <dig> cannot be detected  for any of the given effect strengths and sample sizes, and ϕ <dig> ϕ <dig> are only detectable for both large effect size and sample size .

looking at the irrelevant features , the probability of detecting them as relevant is mostly zero and they are correctly eliminated from the model. only for large effect size, ‘time’ has non-zero probability of being contained in the model: due to the large effect size, state transitions happen after the first cell division  and hence time- and density-dependence cannot be distinguished.

for the dataset used in fig. 4d-f, where the transition rate λ depends on time and local cell density with a gaussian kernel, similar patterns are observed . time is identified reliably  for a sample size large than  <dig> onsets, while for the other relevant features  more samples or a larger effect size are needed . interestingly, larger effect size seems to decrease the power for features ϕ <dig> ϕ <dig> . if the effect size is very large, most state transitions will happen even before cells spread out in space such that the outer density features get populated. therefore their effect cannot be inferred from the data.

influence of tracking error
to obtain genealogies from time-lapse microscopy data, manual  or automatic tracking  is required. neither automatic nor manual tracking can produce perfect genealogies, but will introduce errors especially when local cell density is high or cells move fast as compared to the time resolution of the imaging. to test the influence of tracking errors on the our method, we introduce artificial tracking errors into the simulated datasets by interchanging the identity of randomly selected cells of the same generation and hence swapping entire subtrees of the genealogies. the amount of tracking error is defined as the percentage of all cells in the dataset where an artificial tracking error was introduced. we simulate different amounts of tracking error with up to  <dig> % of all cells in the experiment containing a tracking error. note that tracking errors impact our analysis only by the creation of spurious state transitions . we now evaluate the previous results on these erroneous datasets.

we find that for both the density-dependent  and the time- and density-dependent scenarios  our method reliable identifies the underlying features  for up to  <dig> % of tracking error. for higher amounts of tracking error, we erroneously identify time as a relevant feature and fail to identify ϕ <dig> as relevant feature in the first scenario . note that the wrong inclusion of time is due to the fact that tracking errors and the spurious state transitions created by those errors are more likely at later timepoints where more cells are present. hence, those spurious onsets at late timepoint lead to the inclusion of time into the model.

for the second scenario , identification of the relevant features seems to be very robust with respect to tracking error, as all of them have power > <dig>  even for  <dig> % tracking error.

in both scenarios, tracking error seems to facilitate the detection of ϕ <dig>  that was not detectable previously or only for large effect size . as discussed before, ϕ <dig> is removed by the lasso in favor of sparsity as the other features are sufficient to explain the data. tracking error increases the noise level, i.e. the correlation between relevant features and class labels y becomes weaker. since the other features are no longer sufficient to explain the transition events, the lasso includes ϕ <dig>  which now significantly improves the model.however, at some point tracking error and hence the noise level will become so large that relevant features become decorrelated with the events and lasso removes them again in favor of sparsity.

model validation using sister correlations
apparently, our method is able to infer state transition mechanisms by identifying relevant features even in the presence of moderate tracking errors. however, what if we miss to include relevant features in the glm, e.g. unobserved influences like nutrient concentrations? in this section, we show how to use the tree structure – if available – to validate the chosen model. we investigate whether the transition rate λ estimated by the glm is capable of explaining the observed correlated transition events within the cellular genealogies. we focus here on correlations between sister cells, but the approach easily generalizes to higher order relationships within a genealogy, like cousin-quartets. suppose that we obtained a reasonable estimate λ^ of the transition rate. then, the state transition of one sister cell is independent of the other and just determined by the transition rate that might differ due to the spatial context in both cells. with this independence assumption, we can calculate the probability to observe sister subtree patterns  just as the product of the individual probabilities . note that these probabilities are calculated over the entire lifetime of each cell finally resulting in the expected number of sister subtree patterns for the entire dataset.

using these frequencies, we assess if the transition rate estimated by the glm  is capable of explaining the observed correlations in the genealogies and therefore is an adequate description of the data. for the dataset where the state transition depends only on local cell density , we calculate the expected frequencies of sister subtrees given the previously estimated transition rate  and compare these to the observed frequencies in the data . no significant differences are observed , and hence, there is no indication of correlations beyond what we expect from the density dependent transition rate, in agreement with the generative model.

next, we show how this idea can be used to determine if all relevant features have been included in the glm. to that end, we now deliberately neglect the spatial features ϕ <dig> ϕ <dig> when fitting the transition rate via the glm. since these two features influence the transition rate in the chosen scenario, fitting the impaired glm yields a different λ^ and hence also different expected frequencies of sister subtrees . the frequencies are significantly different , indicating the model is inappropriate, as there is more correlation in the trees than the model can account for . this difference is most pronounced for the pattern where both sister cells change their state.

furthermore we performed this analysis for a smaller sample size with  <dig> onsets  and recover a similar result : while observed and expected frequencies of sister subtrees are not significantly different, impairing the model leads to significant differences in the sister subtree frequencies.

our approach to validate the model using sister correlations  relies on entire correct trackings of both sister cells, as we integrate over the entire lifetime of these cells in eq.  <dig>  analogous to fig. 7a, we evaluate whether we observed frequencies of sister subtrees match the expectations of the model  via a χ2-test for different amounts of tracking error. for the density dependent scenario, we find that up to  <dig> % of tracking error, we do not observe significant differences between observed and expected frequencies , correctly indicating that the density dependent transition rate can explain the observed frequencies . however, more than  <dig> % of tracking error result in substantial changes of the sister correlations, which cannot be explained by the model of the transition rate .

discussion
in this paper, we have presented a method to investigate mechanisms driving cell state transition events observed in cellular genealogies. as two features explicitly regulating the transition rate, we have here considered time and local cell density. our method is complementary to the approach by snijder et al.  <cit>  who showed that the response of a cell to a certain stimulus  strongly depends on each cell’s “population context”, that is, its localization within the colony, its cell density and cell cycle stage. this approach, which has been applied to the analysis of high-content screens by knapp et al.  <cit> , is designed for static data and a single, controlled perturbation. the cells are subject to a treatment at a defined timepoint and their response is recorded by a single image. for our purpose a static approach, where the timepoint of the event is predetermined, is not applicable. instead, we assume that cells undergo state transitions spontaneously, and hence transition events can happen at any point in time but their probability chances over time due to the changing environment the cells experience.

our method currently assumes a linear relationship between features and the transition rate . hence, it is necessary to discuss whether the model can recover relevant features in the presence of nonlinearities or how it can be adapted. in general it is difficult to predict the outcome when fitting a glm that assumes a linear transition rate to data generated with a nonlinear transition rate. on the one hand, performance might suffer as the model cannot capture the nonlinearities and might potentially select the wrong features. on the other hand, nonlinearities might simplify the task of identifying relevant features. for example, if the transition rate is a steep, sigmoid function of cell density, this influence will be easier to detect than a linear one: in feature space, the samples with transition events =1) will be clearly separated from the samples without events =0) in the nonlinear case, while in the linear case there’s a continuum and no clear separation between those two classes. we simulated a scenario where the transition rate is a sigmoid function of cell density . here, our method can still deduce the relevant features despite the nonlinear relation. more generally, one can extend the presented method to handle nonlinearities: the set of features f can be augmented by nonlinear transformations, e.g. by including quadratic or interaction terms  into the data matrix and feature selection is performed on this extended set. alternatively, the glm can be replaced by nonlinear classifiers, e.g. random forests  <cit> . while those methods can handle nonlinear relationships in the data, they lack the build-in feature selection of lasso and will in general not be sparse. for random forests, one can instead use the calculated feature importance measures to perform feature selection.

in our model, we assume that cells can undergo just a single fate transition , while for example in stem cells, fate decisions are often binary, i.e. cells have to choose between two mutually exclusive follow-up states. for illustration, let us assume that black cells turn either into blue or red cells. the proposed method can easily be adapted to this setting. two different scenarios should be distinguished: 1) the two transitions are entirely independent, i.e. there exist two separate transition rates λ <dig> λ <dig> and whatever fate is chosen first determines the resulting cell state  <cit> . in this case, one can simply split the dataset into the cells undergoing the one transition and those cells undergoing the other transition and fit the model to both sets separately. 2) the transition time is determined by a single transition rate λ, and the outcome  is determined by a probability p, which might again be a function of features such as cell density. here, one would first build a model for the transition rate λ  and in a second step build a model of p . if it is unknown which setting applies a priori, both have to be analyzed and later compared to determine which one best explains the data.

our model assumes a homogeneous cell population, i.e. all cells in state i  are equivalent and share the same transition rate. in reality however, apparently homogeneous cell populations often contain subpopulations that behave differently but cannot be distinguished a priori by e.g. surface markers. in our context one could imagine two subpopulations of cells in state i: one subpopulation that undergoes state transitions in response to cell density , while the other subpopulation obeys a transition rate that is a function of time. in our model that is unaware of the subpopulations, two potential results can be imagined: the model might identify time and cell density as relevant features but it will miss the fact that cells respond to either one of those features. alternatively, the model might consider both density and time as irrelevant as neither of them is capable of explaining all the observed data, but just a fraction of it. here, one has to use more flexible models than a glm. a natural choice are “mixtures of generalized linear models”  <cit> , where instead of fitting a single glm to the data, multiple glms are fitted which are responsible for different parts of the data. ideally this would result in a mixture model with two glms, one containing only density features, the other containing only time as relevant variables.

in time-lapse microscopy, the cell’s state is usually read out via surface markers. we here assume that a change in such surface marker expression reports a cell state transitions immediately. however, the marker might not be perfect, i.e. if the cell undergoes the transition but the marker changes only several hours later causing a delay between the event and its observation. if this delay is short relative to the autocorrelation time of the relevant features , our proposed method is still capable of detecting the effect. however, delays becomes more difficult to handle in the same way that tracking error degrades the performance: the noise level increases and decorrelates predictors and response variables. a much longer delay  might be caused by cell intrinsic processes, e.g. a new gene expression program is initiated after the state transition and a change in phenotype  is observed only once this program has been completed. this causes correlations between related cells that cannot be explained by the observed features . here, one has to model the delay explicitly, exploiting the fact that the particular correlations between the cells inform about the delay length: for example, if one observes strong correlations between sister cells, but no correlations between cousins, this would indicate a delay of about one generation.

note that our approach shares certain aspects with cox proportional hazard models  <cit> . the standard cox proportional hazards models use fixed covariates measured once per individual to predict the time to an event for that individual. however, they can be extended to account for time varying covariates  using the counting process reformulation by andersen and gill  <cit> . analogous to our approach, one then considers small time intervals where the covariates are constant and builds a model that predicts whether an event happened in these small intervals. this reformulation is also crucial for our approach as it allows to handle the tree structure of the data by dissecting it into small intervals. the main difference of our model to proportional hazards is the form of the hazard rate , which in our case is linear in the covariates , while in the cox proportional hazard model it is multiplicative in the covariates. this choice is motivated by the form of the transition rate in an earlier study  <cit> . in general our assumption is equally strong as the proportional hazards assumption, however not relying on the proportional hazards machinery is beneficial as one then can easily exchange the glm framework by alternative machine learning techniques if required.

in the current formulation, we assume that the transition rate of a cell i at time t is a function only of the features fi at time t  and not a function of the history fi,s<t of the cell. on the one hand this is advantageous because no extensive tracking of cells over multiple generations, but only an accurate cell segmentation at time t is required to assess all observable features fi such as cell density. on the other hand, the method cannot identify a history-dependence of the transition rate, e.g. in a scenario where a cell integrates over the previously experienced cell densities via some internal mechanism. however, given reliable single cell tracking data and hence reliable timecourses of the features fi for single cells i instead of snapshots, the presented approach can be extended to also detect history dependent transition rates. to that end, one has to augment the feature vectors entering the glm by time-shifted versions of those features, i.e. including not only present cell density, but also densities at previous timepoints, and fits the model as proposed. this is analogous to applying the idea of granger causality  <cit>  to feature timecourses fi and binary cell state timecourses yi, i.e. inferring what features  contain information to predict the state-timecourses. accounting for potential history-dependence with time-shifted versions of the original features complicates the inference problem due to the increasing feature space, potentially requiring a larger sample size. however, the following reasons might still allow for a faithful inference:  the lasso regularization with bootstrapping is known to work well even for high dimensional problems  <cit> .  since the features have considerable autocorrelation , one only has to include the time-shifted features at intervals greater than the autocorrelation time. this leads to fewer features than considering every possible time-shift for each original feature. still, if the transition rate depends on a very long history  this approach might become infeasible due to the increasingly large features space. here one has to augment the problem with some additional regularization, e.g. enforcing the weights of neighboring time-lags to be similar  <cit> .

we showed how the kernel for spatial interactions can be learned from the data using a set of concentric basis functions with width Δr controlling the resolution of the kernel. from a biological perspective the most interesting quantity of the interaction kernel is its range, i.e. the distance on which cells communicate and influence each other. a kernel range on the order of a typical cell size indicates that state transitions are initiated or inhibited due to cell-cell contact . large kernel range suggest communication via signaling molecules, e.g. cytokines that are able to instruct cell fate choice in stem- and progenitor cells  <cit> . this range can be inferred using a relatively coarse kernel resolution . a fine resolution  has to be chosen if the precise shape of the kernel is of interest. from the exact shape of the kernel one could learn about the signaling mechanism, e.g. how the signal is integrated by the receiving cells. for example, a long range tophat kernel would indicate a threshold response in the signal-receiving cell, i.e. the cell’s surface receptor transfers the signal into the cell only if the signaling molecule exceeds a certain concentration. a long range gaussian kernel would instead indicate that the receiver responses gradually to the signal. however, a fine kernel resolution comes at the expense of a more challenging inference problem: not only does the number of features ϕi increase  but more importantly, the contributions of the individual ϕi becomes weaker such that they are more likely eliminated by the lasso regularization . this can either be compensated by increasing the sample size or putting constraints on the weights of the ϕi, e.g. enforcing the weights of neighboring ϕi to be similar, resulting in smooth kernels .

with respect to regulating features, our method can be extended to any other parameter that is experimentally accessible. in terms of tumor growth for example, the presence  of distinct cancer cell subtypes might influence transitions between states of different proliferative potential  <cit> . this could be analyzed by introducing cell type specific density features ϕic that take into account only a certain cell type c when calculating local cell density. for blood progenitor cells, including the expression levels of pu. <dig>  <cit> , a pivotal fate determining factor  <cit> , as a feature will allow to compare extrinsic and intrinsic  <cit>  effects on cellular plasticity.

CONCLUSIONS
our approach is designed for dynamic data provided by time-lapse microscopy, which allows to observe state transitions in their spatiotemporal and genealogical context. the requirements for an appropriate dataset are  single-cell genealogies obtained from automatic or manual cell tracking,  at least as many annotated state transitions as determined by our analysis, and  the identification of all cells surrounding a transition event in an sufficiently large radius. to the best of our knowledge, no such dataset exist up to now, but manual and automated tracking tools increase accuracy and efficiency . moreover, our method relies only on short trackings of one cell cycle to quantify sister correlations . since fluorescent fate markers exist for various systems, morphological quantification has been shown to be usable for fate recognition  <cit> , and robust cell segmentation algorithms work on full time-lapse movies  <cit> , we believe that adequate datasets from various cell systems will emerge in the near future. due to the method’s generality, many different types of cell state transitions can be investigated in their spatiotemporal context: for example, one can study the influence of cytokine signaling between differentiating blood stem- and progenitor cells  <cit> , i.e. if the presence of one celltype  promotes specific differentiation decisions. in mouse embryonic stem cells the impact of cell-cell interactions on transitions between nanog-high and nanog-low cells  <cit> , or on the cell fate decision between epiblast and primitive endoderm  <cit>  could be analyzed with our proposed method. similarly, transitions between cancer stem cells and non-tumorigenic cells  <cit> , or the epithelial-mesenchymal transition, which is thought to initiate tumor metastases  <cit>  can be analyzed in their the spatiotemporal context.

additional file
additional file  <dig> 
supplementary text. this document provides a detailed description of the full probabilistic model, bayesian credibility intervals, the proof of sample independence, and the relation of log-binomial and poisson regression. 



competing interests

the authors declare that they have no competing interests.

authors’ contributions

mks developed the method and conducted the simulation study. jf provided the simulation data. fjt critically commented on the study and the manuscript. mks wrote the manuscript with cm. cm and mks designed the study. cm supervised the study. all authors read and approved the final manuscript.

availability of data and materials

matlab code is provided at https://github.com/qscd.

