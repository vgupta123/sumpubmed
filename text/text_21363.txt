BACKGROUND
a central problem in molecular genetics is to understand the transcriptional regulation of gene expression. a transcription factor  is a protein that binds to a typical domain on the dna and influences transcription. the effect of this tf can be either a repression or an activation of transcription depending on the type of binding site, the distance to coding regions, or on the presence of other molecules. finding which gene is controlled by which tf is a reverse engineering problem, usually named network reconstruction. this question has been approached over the past years by various groups.

a first approach to achieve this task is to collect the information spread in the primary literature. following this idea, a large number of databases that take protein and regulatory interactions from the literature and curate them have been developed  <cit> . for the bacteria e. coli, regulondb is a dedicated database that contains experimentally verified regulatory interactions  <cit> . for the budding yeast , the yeast proteome database contains a large amount of regulatory information  <cit> . in this latter case, however, the amount of available information is not sufficient to build a reasonably accurate model of transcriptional regulation. databases with regulatory knowledge extracted from the literature are, nevertheless, an unavoidable starting point for network reconstruction.

the alternative to a literature-curated approach is a data-driven approach. this approach is supported by the availability of high-throughput experimental data including microarray expression analysis of deletion mutants , over expression of tf-encoding genes, protein-protein interactions, protein localisation, or chip-chip experiments coupled with promoter sequence analysis. we may cite several classes of methods that use these kinds of data, such as correlation, mutual information or causality studies, bayesian networks, path analysis, information-theoretic approaches, and ordinary differential equations  <cit> .

in short, most available approaches so far are based on a probabilistic framework which defines a probability distribution over the set of models. the reconstructed network is then defined as the most likely model given the data. such an optimization problem is usually non convex, and finding a global optimum cannot be guaranteed in practice. existing algorithms report a local optimum which should be interpreted with care: errors can appear and no consensual model may be produced.

as an illustration, special attention has been paid to the reconstruction of s. cerevisiae network from chip-chip data and protein-protein interaction networks  <cit> . a first regulatory network was obtained with promoter sequence analysis methods  <cit> , yet, some undetected transcriptional regulatory motifs were proposed using non-parametric causality tests  <cit> . moreover, bayesian analysis also identified new regulatory modules for this network  <cit> . thus, the results obtained with the different methods do not coincide and a fully data-driven search is in general subject to over-fitting and not fully reliable  <cit> .

in regulatory networks an important and non-trivial physiological information is the regulatory role of tfs as inducer or repressor, also called the sign of the interaction. this information is needed if one wants to know, for instance, the physiological effect of a change caused by external conditions or the effect of a perturbation on the tf. while this can be achieved for one gene at a time with  dedicated experiments, probabilistic methods such as bayesian models  <cit>  or path analysis  <cit>  are capable of proposing models from high-throughput experimental data. however, as for the network reconstruction task, these methods are based on optimization algorithms that compute an optimal solution with respect to an interaction model.

in this paper, we apply formal methods to compute the sign of interactions in networks that have an available topology. by doing so, we also validate the topology of the network. roughly, we use expression profiles to constrain the possible regulatory roles of tfs, and we report those regulations that are assigned the same role in all feasible models. thus, we over-approximate the set of feasible models, and then look for invariants in this set. a similar idea was applied in  <cit>  to check the consistency of gene expression assays. however, we use a deeper formalisation and stronger algorithmic methods to achieve the inference task.

different sources of large-scale data are exploited in this study: gene expression arrays, which provide information on the interaction signs; and chip-chip experiments, which provide the topology of the regulatory network when not available.

the main tasks we address are the following:

 <dig>  building a formal model of regulation for a set of genes that integrates information from chip-chip data, sequence analysis, and literature annotations.

 <dig>  checking its consistency with expression profiles on perturbation assays.

 <dig>  inferring the regulatory role of tfs as inducer or repressor if the model is consistent with expression profiles.

 <dig>  isolating ambiguous pieces of information if it is not.

the results section is organised as follows. we first introduce the mathematical framework which is used to define and to test the consistency between expression profiles and transcriptional networks. then, we apply our algorithms to address three main issues:

• analysis of the dependence between the number of available observations and the number of inferred regulations. in the case where all genes are observed, we prove that at most  <dig> % of e. coli network can be inferred and that  <dig> perturbation experiments are enough to infer 30% of the network on average. in the case of missing observations, we estimate how the proportion of unobserved genes affects the number of inferred regulations.

• illustration and validation of our method on the transcriptional network of e. coli, obtained from regulondb  <cit> , with a compendium of expression profiles  <cit> .

• execution of our inference algorithms over the s. cerevisiae transcriptional network. we inferred, for small scale subnetworks, more than 20% of the roles of regulations. for more complex networks, we detected and isolated inconsistencies  between expression profiles and a significant part of the model .

RESULTS
detecting the role of a regulation and validating a model
our goal is to determine the regulatory role of a tf on its target genes by using expression profiles. let us illustrate our purpose with a simple example.

we suppose that we are given the topology of a network . in this network, let us consider a node a with a single predecessor. in other words, the model tells us that the protein b acts on the expression of the gene coding for a and no other protein acts on a.

independently, we suppose that we have several gene expression arrays at our disposal. one of these arrays indicates that a and b simultaneously increase during a steady state shift experiment. then, common sense tells us that b must have been an activator of a during the experiment. more precisely, protein b cannot have inhibited gene a since they both have increased. consequently, we say that the model predicts the sign of the interaction from b to a as positive .

this naive rule is actually used in a large class of models; we will call it the naive inference rule. when several expression profiles are available, the predictions of the different profiles can be compared. if two expression profiles predict different signs for a given interaction, there is an ambiguity or inconsistency between data and model . then, the ambiguity of the regulatory role can be attributed to three factors:  a complex mechanism of regulation, the role of the interaction depends on the state of the system;  a missing interaction in the model;  an error in the experimental source. this simple strategy is implemented in the algorithm  <dig> 

let us consider now the case when a is activated by two proteins b and c. no more natural deduction can be done when a and b increase during an experiment since the influence of c must be taken into account. a model of interactions between a, b, and c has to be proposed. probabilistic methods estimate the most probable signs of regulations that fit with the theoretical model  <cit> .

our point of view is different; we introduce a basic rule that shall be checked by each interaction in the model. this rule tells us that any variation of a must be explained by the variation of at least one of its predecessors. in previous papers, we introduced a formal framework to justify this basic rule under some reasonable assumptions. we also tested the consistency between expression profiles and a graphical model of cellular interactions. this formalism will be introduced here in an informal way; its full justification and extensions can be found in the references  <cit> .

in our example, the basic rule means that if b and c activate a, and both  are known to decrease during a steady state experiment, a cannot be observed as increasing. then a is predicted to decrease . more generally, we apply the rule as a constraint for the model, we write constraints for all the nodes of the model, and we use several approaches in order to solve the system of constraints. from the study of the set of solutions, we deduce which signs are surely determined by these rules. then, we obtain necessary conditions on the signs instead of the most probable signs given by probabilistic methods.

a formal approach
consider a system of n chemical species { <dig> ...,n}. these species interact with each other and we model these interactions using an interaction graph g = . the set of nodes is denoted by v = { <dig> ...,n}. there is an edge j → i ∈ e if the level of species j influences the production rate of species i. edges are labelled by a sign {+, -} which indicates whether j activates or represses the production of i.

in a typical stress perturbation experiment a system leaves an initial steady state following a change in control parameters. after waiting long enough, the system may reach a new steady state. in genetic perturbation experiments, a gene of the cell is either knocked-out or over-expressed; perturbed cells are then compared to the reference. our approach relies on the signs of the variations in expression or activity of the species in the network. let us denote by sign ∈ {+, -, 0} the sign of the variation of species i during a given perturbation experiment, and by sign ∈ {+, -} the sign of the edge j → i in the interaction graph.

let us fix species i such that there is no positive self-regulating action on i. for every predecessor j of i, sign * sign provides the sign of the influence of j on the species i. then, we can write a constraint on the variation to interpret the rule that was previously stated: the variation of species i is explained by the variation of at least one of its predecessors in the graph.

  sign≈∑j→isignsign. 

when the experiment is a genetic perturbation, the same equation holds for every node that was not genetically perturbed during the experiment and such that all its predecessors were not genetically perturbed. if a predecessor xm of the node was knocked-out, the equation becomes

  sign≈−sign+∑j→ij≠msignsign. 

the same holds with +sign when the predecessor xm was over-expressed. there is no equation for the genetically perturbed node.

the sign algebra is the suitable framework for reading these equations  <cit> . it is defined as the set {+, -, ?, 0}, provided with a sign consistency relation ≈, and arithmetic operations + and ×. the following tables describe this algebra:

 ++−=?+++=++×−=−+×+=+−+−=−++0=+−×−=++×0=00+0=0−+0=−0×0=0−×0=0?+−=??++=??×−=??×+=??+?=??+0=??×?=??×0=0+≉−+≈0−≈0?≈+?≈−?≈ <dig> 

for a given interaction graph g, we will refer to the qualitative system associated with g as the set made up by applying constraint  for each node in g. we say that node variations xi ∈ {+, -, 0} are consistent with the graph g when they satisfy all the constraints associated with g using the sign consistency relation ≈.

with this material at hand, let us come back to our original problem, namely to infer the regulatory role of tfs from the combination of heterogeneous data. in the following we assume that:

• the interaction graph is either given by a model to be validated, or built from chip-chip data and tf binding site search in promoter sequences. thus, as soon as a tf j binds to the promoter sequence of gene i, j is assumed to regulate i. this is represented by an arrow j → i in the interaction graph.

• the regulatory role of a tf j on a gene i  is represented by the variable sji, which is constrained by eqs.  or .

• expression profiles provide the sign of variation of the gene expression for a set of r steady-state perturbation, mutant, or over-expression experiments. in the following, xik will stand for the sign of the observed variation of gene i in experiment k.

our inference problem can now be stated as finding values in {+, -} for sji, subject to the constraints:

  for all ,,s.t. i is notgenetically perturbed in the k-th experiment{xik≈∑j→isjixjkif all predecessors jare not genetically perturbedxik≈−smi+∑j→i,j≠msjixjkif m is knocked-out.xik≈smi+∑j→i,j≠msjixjkif m isover-expressed. 

most of the time, this inference problem has a huge number of solutions. however, some variables sji may be assigned the same value in all solutions of the system. then, the recurrent value assigned to sji is a logical consequence of the constraints , and a prediction of the model. we will refer to these inferred interaction signs as predictions of the qualitative system, that is, sign variables sji that have the same value in all solutions of a qualitative system . when the inference problem has no solution, we say that the model and the data are inconsistent or ambiguous.

let us illustrate this formulation with a very simple  example. suppose that we have a system of three genes a, b, c, where b and c influence a, as given in fig.  <dig>  let us say that for this interaction graph we obtained six experiments, and in each of them the variation of all products in the graph was observed. using some or all of the experiments provided will lead us to different qualitative systems, as shown in table  <dig>  hence to different inference results.


in this example the variables are only the roles of regulations  in the interaction graph. variations of the species in the graph are obtained from six experiments. using different sets of experiments we infer different roles of regulation. using experiments {e <dig>  e <dig>  e3}, for example, our qualitative system will have three constraints and not all valuations of variables sbaand scasatisfy this system according to the sign algebra rules. as we obtain unique values for these variables in the solution of the system, we consider them as inferred .

algorithmic procedure
when the signs on edges of the interaction graph are known , finding consistent node variations xi is a np-complete problem  <cit> . when the node variations are known , finding the signs of edges sji from xi can be proven np-complete in a very similar way. however, we have been able to design algorithms that perform efficiently on a wide class of regulatory networks. these algorithms predict signs of the edges when the network topology and the expression profiles are consistent. in case of inconsistency, though, they identify ambiguous motifs and propose predictions on parts of the network that are not concerned with ambiguities.

the general process flow is as follows :

step  <dig> sign inference

divide the graph into motifs . for each motif, find sign valuations  that are consistent with all expression profiles. if there are no solutions, call the motif multiple behaviours module  and remove it from the network.

solve again the remaining equations and determine the edge signs that are fixed to the same value in all the solutions. these fixed signs are called predicted edge and represent our predictions.

step  <dig> global test/correction of the inferred signs

solutions at the previous step are not guaranteed to be global. indeed, two node motifs at step  <dig> can be consistent separately, but not altogether . this step checks global consistency by solving the equations for each expression profile. new multiple behaviours modules can be found and removed from the system.

step  <dig> extending the original set of observations

once all conflicts have been removed, we get a set of solutions in which signs are assessed to both nodes and edges. predicted nodes, representing inferred gene variations can be found in the same way as we did for edges. we add the new variations to the set of observations and return to step  <dig>  the algorithm is iterated until no new signs are inferred.

step  <dig> filtering predictions

in the inconsistent case, the validity of the predictions depends on the accuracy of the model and on the correct identification of the mbms. the model can be incomplete , and mbms are not always identifiable in a unique way. thus, it is useful to sort predictions according to their reliability. our filtering parameter is a positive integer k representing the number of different experiments with which the predicted sign is consistent. for a filtering value k, all the predictions that are consistent with less than k profiles are rejected.

the inference process then generates three results:

 <dig>  a set of mbms, containing interactions whose role was unclear and generated inconsistencies. we have identified several types of mbms:

• modules of type i: are composed of several direct regulations towards the same gene. they are detected in the step  <dig> of the algorithm, and most of them are composed of only one edge like illustrated in fig.  <dig>  but bigger examples exist.

• modules of type ii, iii, iv: are detected in steps  <dig> or  <dig>  hence they contain either direct regulations coming from the same protein or indirect regulations and/or loops. each of these regulations represents a consensus of all the experiments, but when we attempt to assess them globally, they lead to contradictions. the indices ii-iv have no topological meaning, they label the most frequent situations and are illustrated in fig.  <dig> 

 <dig>  a set of inferred signs, meaning that the expression profiles fix the signs of certain interactions in a unique way.

 <dig>  a reliability ranking of inferred signs. the filtering parameter k used for ranking is the number of different expression profiles that validate a given sign.

on a computational level, the division between step  <dig>  and step  <dig>  is necessary to overcome the memory complexity of the search for solutions. to handle large scale systems we combine decision diagrams and constraint solvers .

since our basic rule is a weak constraint, we expect it to produce very robust predictions. on the other hand, there are theoretical limits to this approach. for certain interaction graphs, not a single sign may be inferred even with a high number of experiments. in the next paragraphs, we comment on the maximum number of signs that can be inferred from a given graph.

in perturbation experiments, gene responses are observed following changes of external conditions , gene inactivations, knock-outs, or over-expression. when one expression profile is available for all the genes in the network we say that we have a complete profile, otherwise the profile is partial .

in the following pragraphs we describe the results we obtained. first of all, in order to validate our formal approach, we evaluated the percentage of the e. coli network recovered from a reasonable number of artificial randomly generated perturbation experiments. secondly, we combined real perturbation experiments with the e. coli network and computed the percentage of the recovered network. finally, we performed the same previous analysis in a real setting of the s. cerevisiae network obtained from chip-chip data.

on a computational level, we checked that our algorithms were able to handle large scale data, as produced by high-throughput measurement techniques . this is demonstrated in the following by considering networks of thousands of genes.

stress perturbation experiments: how many do you need?
for any given network topology, even when considering all possible experimental profiles, there are signs that cannot be determined . sign inference has thus a theoretical limit, referred to here as theoretical percentage of recovered signs, that is unique for a given network topology. if only some perturbation experiments are available, and/or data is missing, the percentage of inferred signs will be lower. for a given number n of available expression profiles, the average percentage of recovered signs is defined over all sets of n different expression profiles consistent with the qualitative constraints eqs.  and .

in order to calculate the theoretical and the average percentages of recovered signs for the transcriptional network of e. coli, we modelled the network as an interaction graph using the public database regulondb  <cit> . for each transcriptional regulation a → b we added the corresponding arrow between genes a and b in the interaction graph. this graph will be referred to as the unsigned interaction graph.

from the unsigned interaction graph of e. coli, we build the signed interaction graph by annotating the edges with a sign. most of the time, the regulatory role of a tf is available in regulondb, however, when it is unknown or depends on the tf level, we arbitrarily choose the value + for this regulation. this provides a graph with  <dig> nodes and  <dig> edges, all signed edges. the signed interaction graph is used to generate complete expression profiles that simulate the effect of perturbations. more precisely, a perturbation experiment is represented by a set of gene expression variations {xi}i =  <dig> ...,n that are not entirely random, for they are constrained by eqs. and . then, we forget the signs of the network edges and compute the qualitative system with the signs of regulations as unknown.

the theoretical maximum percentage of inference is given by the number of signs that can be recovered assuming that complete expression profiles of all conceivable perturbation experiments are available. we computed this maximum percentage using constraint solvers . we found that at most  <dig> % of the signs in the network can be inferred, corresponding to mmax =  <dig> edges.

however, this maximum can be obtained only if all conceivable  perturbation experiments are done, which is in practice not possible. we performed computations to understand the influence of the number of experiments  on the inference. for each value of n , we generated  <dig> sets of n complete random expression profiles and performed our algorithm for each set. then, the percentage of inference was calculated as a function of n. the resulting statistics are shown in fig.  <dig> 

we can obtain a theoretical formula explaining the saturation aspect of the curve in fig.  <dig>  let us suppose that the network contains m <dig> single incoming regulations. these can be inferred with probability one from only one experiment, using the naive algorithm . let us suppose a second category of interactions, whose signs are inferred with probability p  on average, per experiment. this implies that the average number of inferred signs for one experiment is m = m <dig> + pm <dig>  where m <dig> is the number of interactions in the second category. supposing now that inference failures are independent for different experiments, we obtain the average number of inferred signs for n experiments: m = m <dig> + m2n). in general, we have m <dig> + m <dig> <e , meaning that there are edges whose signs cannot be inferred.

in our example, the value m <dig> =  <dig> corresponds to the average number of signs inferred by the naive algorithm. surprisingly, by using our method we can significantly improve the naive inference with little effort. for the whole e. coli network it appears that a few expression profiles are enough to infer a significant percentage of the network. more precisely,  <dig> different expression profiles may be enough to infer one third of the network . adding more expression profiles continuously increases the percentage of inferred signs. for n >  <dig> we are practically on the plateau close to  <dig> % .

according to our estimates the position of the plateau is m = m <dig> + m <dig> =  <dig>  which is smaller than the theoretical maximum m <mmax. the difference, although negligible in practice , suggests that the plateau has a very weak slope. this means that contributions of different experiments to sign inference are weakly dependent.

the values of m <dig>  m <dig>  p estimate the efficiency of our method: large p,m <dig> m <dig> mean small number of expression profiles needed for inference.

inferring the core of the network
obviously, not all interactions play the same role in the network. the core is a subnetwork that naturally appears for computational purposes and plays an important role in the system. it consists of all oriented loops and of all oriented chains leading to loops. all oriented chains leaving the core without returning are discarded when reducing the network to its core. acyclic graphs and in particular trees have no core. the main property of the core is that if a system of qualitative equations has no solution, neither has the reduced system built from its core. hence it corresponds to the most difficult part of the constraints to solve. it is obtained by reduction techniques that are very similar to those used in  <cit>  . as an example, the core of e. coli network  only has  <dig> nodes and  <dig> edges.

in the previous section, we applied the same inference process to this graph. not surprisingly, we noticed a rather different behaviour when inferring signs on a core graph than on a whole graph as demonstrated in fig.  <dig>  in the former case, we needed many more experiments for the inference since the sets of expression profiles contained from n =  <dig> to  <dig> random profiles.

two observations may be concluded. first, a greater number of experiments is required to reach a comparable percentage of inference; the value of p is smaller than for the whole network. this confirms that the core is more difficult to infer than the rest of the network. second, fig.  <dig> displays a much less continuous behaviour for the core. more precisely, when using the core, different perturbation experiments have a strongly variable impact on sign inference. for instance, the experimental maximum percentage of inference  can be obtained already from about  <dig> expression profiles, yet, most of the datasets with  <dig> profiles infer only  <dig> signs.

this suggests that not only the core of the network is more difficult to infer, but also that a brute force approach  may fail as well. this situation encourages us to apply experiment design and planning, that is, computational methods to minimise the number of perturbation experiments while inferring a maximal number of regulatory roles.

this also illustrates why our approach is complementary to dynamical modelling. in the case of large scale networks, when an interaction stands outside the core of the graph, an inference approach is suitable for inferring the sign of the interaction. however, when an interaction belongs to the core of the network, more complex behaviours occur  thus, a precise modelling of the dynamical behaviour of this part of the network should be performed  <cit> .

influence of missing data
in the previous paragraphs, we assumed that all products in the network were observed. that is, in each experiment each node is assigned a value in {+,  <dig>  -}. however, in real measurement devices, such as expression profiles, a part of the values is discarded due to technical reasons. a practical method for network inference should cope with missing data.

we studied the impact of missing values on the percentage of inference. for this, we have considered a fixed number of expression profiles . then, we have randomly discarded a growing percentage of observed products in the profiles, and computed the percentage of inferred regulations. the resulting statistics are shown in fig.  <dig> 

in both cases , the dependency between the average percentage of inference and the percentage of missing values is qualitatively linear. simple arguments allow us to find an analytic dependency. if not observing one node of the network implies losing information on d interaction signs, we are able to obtain the following linear dependency mi = mimax - d * f * mtotal; where mimax is the number of inferred interactions for complete expression profiles , f is the fraction of unobserved nodes, and mtotal is the total number of nodes. in order to keep mi non negative, d must decrease with f. our numerical results imply that the constancy of d and the linearity of the above dependency extend to rather large values of f. this indicates that our qualitative inference method is robust enough for practical use. for the whole network we estimated d =  <dig> , meaning that on average we lose one interaction sign for about  <dig>  missing values. however, for the same number of expression profiles, the core of the network is more sensitive to missing data . for the core, increasing the number of expression profiles increases d and hence the sensitivity to missing data.

application to e. coli network with a real compendium of expression profiles
we validated our method on the transcriptional e. coli network using the compendium of expression profiles publicly available in  <cit>  and  <cit> . this time the network was composed of  <dig> nodes and  <dig> edges. the difference with the previous model are the sigma-factors – gene interactions.

several profiles were available, including a reference condition. we grouped together the different profiles corresponding to the same experiment; for each gene we calculated its average variation in the group of profiles. when profiles were time series, we considered that each time series ends with steady state and we used the last state in the time series. then, we sorted the measured genes in four classes: 2-fold up-regulated, 2-fold down-regulated, non-observed, and zero variation; this last class corresponds to non significantly  expressed genes. only the first two classes were used in the algorithm. therefore, there will be missing data: for some edges, neither the input nor the output are observed. altogether, we have processed  <dig> sets of expression profiles corresponding to  <dig> different experiments . we verified, for all the experiments, that they correspond to the comparison between one perturbed condition against a control condition with identical levels in all chemical components except for the one altered in the perturbed condition.

we applied our inference algorithm twice: the first time we used the signed network in a pre-processing step, in order to clean the expression data. it appears that the signed network is consistent with only  <dig> of the  <dig> selected experiments. after discarding the inconsistent motifs from each experiment , we stayed with  <dig> experiments which only contained the data consistent with the signed network. in these  <dig> experiments, on average  <dig> % of the network nodes were observed. when summing up all the observations, we obtained that  <dig> %  of the edges  were observed in at least one expression profile; these represent the maximal set of signs that can be inferred at steps  <dig> and  <dig> of our inference algorithm. in order to test our algorithm we wiped out the information on edge signs and then tried to recover it. since the profiles and network were consistent, our algorithm found no ambiguity and predicted  <dig> signs, i.e. 20% of the edges observed at least once . the naive inference algorithm inferred  <dig> signs. hence, 18% of the total of our predictions could not be obtained by the naive algorithm.

afterwards, we tested our algorithm with the full set of observations, no data being discarded. conflicts appeared and we filtered our inference with different parameters on the full set of  <dig> experiments including inconsistencies. this time  <dig> % of the network products were observed on average. when summing all the observations,  <dig> %  of the edges  were observed in at least one expression profile. several values of the filtering parameter k were used from k =  <dig> to k =  <dig>  without filtering we predicted  <dig> signs of the network , among them,  <dig> % were not inferred by the naive algorithm. we compared the predictions to the known interaction signs:  <dig> % of the predictions were false predictions. sources of errors may lie on non-modelled interactions , or in using experiments on different e. coli strains. filtering improves our score allowing us to retain only reliable predictions. thus, for k =  <dig>  we inferred  <dig> signs, of them, only  <dig> was an incorrect prediction . we conclude that filtering is a good way to strengthen our predictions even when the model is not precise enough. we illustrated the effect of the filtering process in fig.  <dig> 

it should be noted that we obtained very similar results either by cleaning the data thanks to the signed network, either by using our filtering procedure. this is a particularly clear indication that this filtering procedure is an effective strategy to produce robust predictions.

our algorithm also detected ambiguous modules in the network. there are seven mbm of type i ; four of them are also stated as ambiguous by the naive algorithm. in addition, there are  <dig> mbm of type ii that are not detected by the naive inference algorithm. all the ambiguities are shown in fig.  <dig>  a list of experimental assays that yield ambiguities on each interaction is given in the supplementary web site. this analysis shows that there exist non-modelled interactions that balance the effects on the targets in the mbm detected.

a real case: inference of signs in s. cerevisiae transcriptional regulatory network
we applied our inference algorithm to the transcriptional regulatory network of the budding yeast s. cerevisiae. let us here briefly review the available sources that can be used to build the unsigned regulatory network. the experimental dataset proposed by lee et al.  <cit>  is widely used in the network reconstruction literature. it is a study conducted under nutrient rich conditions, and it consists of an extensive chip-chip screening of  <dig> tfs. estimations regarding the number of yeast tfs that are likely to regulate specific groups of genes by direct binding to the dna vary from  <dig> to  <dig>  depending on the selection criteria. in follow-up papers of this work, the chip-chip analysis was extended to  <dig> yeast tfs in rich media conditions and  <dig> of these regulators in at least one environmental perturbation  <cit> . analysis methods were refined in  <dig> by macisaac et al.  <cit> . other studies continued to work in this network using different approaches  <cit> . here we selected two of these sources. all networks are provided in the supplementary web site.

 the first network consists of the core of the transcriptional chip-chip regulatory network produced in  <cit> . starting from the full network with a p-value of  <dig> , we reduced it to the set of nodes that have at least one output edge. this network was already studied in  <cit> . it contains  <dig> nodes and  <dig> interactions.

 the second network contains all the transcriptional interactions between tfs shown by  <cit>  with a p-value below  <dig> . it contains  <dig> nodes and  <dig> interactions.

 the third network is the set of interactions among tfs as inferred in  <cit>  from sequence comparisons. we have considered the network corresponding to a p-value of  <dig>  and  <dig> bindings .

 the last network contains all the transcriptional interactions among genes and regulators shown by  <cit>  with a p-value below  <dig> . it contains  <dig> nodes and  <dig> interactions.

inference process with gene-deletion expression profiles
we first applied our inference algorithm to the large scale network  using a panel of expression profiles for  <dig> gene-deletion experiments  <cit> . the information given by this panel is quite small, since  <dig> % of all the products in the network is on average observed, and 12% of the edges  of the network are observed in at least one expression profile. using these data, we inferred  <dig> regulatory roles.

we validated our prediction with a literature-curated network on yeast  <cit> . we found that among the  <dig> sign-predictions,  <dig> were referenced with a known interaction in the database, and  <dig> with a good sign.

gene-deletion expression profiles were used in order to compare our results to path analysis methods  <cit>  since the latter can only be applied to knock-out data. other sign-regulation inference methods needed either other sources of gene-regulatory information , or time-series data to be performed  <cit> .

first, we tested the consistency between the inferred network obtained from path analysis methods with the  <dig> gene-deletion experiments. we obtained that the network was inconsistent with  <dig> of the  <dig> experiments. second, we compared the inference results for both methods, our approach and the path analysis method, obtaining in the latter that  <dig> roles of widely connected paths were inferred; whereas with our method  <dig> roles were inferred, mainly localised in the branches of the network. both results intersected on  <dig> interactions and no contradiction in the inferred role was reported. an illustration of these results is given in the supplementary web site.

this suggests that our approach is complementary to path analysis methods. our explanation is as follows: in  <cit> , network inference algorithms identify probable paths of physical interactions connecting a gene knock-out to genes that are differentially expressed as a result of that knock-out. this leads to a search for the smallest number of interactions that carry the largest information in the network. hence, inferred interactions are located near the core of the network, but not exactly in the core. on the contrary, as we already mentioned, the combinatorics of interactions in the core of the network are too intricate to be determined from a few hundreds of expression profiles with our algorithm, thus, we concentrate on interactions around the core.

inference with stress perturbation expression profiles
to overcome the problem exposed using the small amount of information contained in  <cit> , we have used stress perturbation experiments. these data correspond to curated information available in sgd   <cit> . when time series profiles were available, we selected the last time expression array. therefore, we collected and treated  <dig> experiments described in table  <dig>  for each expression array, we sorted the measured genes in four classes: 2-fold up-regulated, 2-fold down-regulated, non-observed, and zero variation. full datasets are available in the supplementary web site.

all experiments contain information on steady state shift and their curated data is available in sgd   <cit> .

as in the case of e. coli, it appeared that all the networks , , , and  were not consistent with the whole set of expression arrays. thus, when executing our algorithms we identified motifs that held ambiguities, and we marked them as mbm of type i-iv . we also generated a set of inferred signs and applied the filtered algorithm  to the large scale network .

we obtained our total inference rate by adding the number of inferred signs fixed in an unique way to the number of non-repeated interactions in the mbm detected, and dividing it by the total number of edges in the network. in table  <dig> we illustrate the inference rate obtained for each of the networks. depending on the network, the inference rate varies from 19% to 37%; thus, they are similar to the theoretical rates obtained for e. coli network even with a small number of perturbation experiments .

sign inference process applied on four transcriptional networks of s. cerevisiae.  <dig> significant  experiments were used for the inference. the in/out observed simultaneously rate refers to the possible sign inference rate if all observations of the in/out nodes of one edge lead to predictions. the inferred signs are the number of signs fixed in an unique way {+, -} by all the experiments; mbm type i, refers to the number of edges found in a module of this type; and analogusly for mbm type ii-iv. the sum of these  <dig> values divided by the total number of edges will represent the rate of the total inference. in addition, we calculated the inference rate obtained with the naive algorithm. for network  all the inference rates were obtained using the algorithm without filtering, except for the number of inferred signs.

we validated the inferred interactions comparing them to the literature-curated network published in  <cit> . we obtained  <dig> predictions when no filtering is applied. furthermore, among the  <dig> interactions predicted with a filter parameter k =  <dig>   <dig> were referenced with a known interaction in the database, and only  <dig> prediction had a wrong sign. as in the case of e. coli, we conclude that filtering is a good way to produce extremely robust predictions. additionally, we compared our predictions to the naive inference algorithm finding that the naive algorithm usually predicts half of the signs that we obtain. in fig.  <dig> we illustrate the inferred interactions for network .

as already mentioned, the algorithm identified a large number of ambiguities. the exhaustive list of mbm is given in the supplementary web site and the type i modules of size  <dig> found for the networks , , and  are detailed in table  <dig>  we noticed that the mbm of type i were detected in the four networks; whereas the mbm of type ii-iv were only detected, in an large number, for network ; type ii mbm being the most numerous . for each mbm, a precise biological study of the species should enable to understand the origin of the ambiguity: erroneous expression data, missing interactions in the model, or context-dependent regulations.

for each ambiguous module, we list two inconsistent experiments that infer a different role of regulation.

contribution of expression profiles to the inference
analysing only the sign inference process on the global network , we wish to estimate how the  <dig> experiments used influence the unique way {+, -} inferred signs. on that account we address the following question: assuming that all the inferred roles in step  <dig> of our inference algorithm are correct, which is the experiment that marks more inferred roles as inconsistent ?

therefore, we classified the  <dig> experiments according to the mbm of type ii-iv generated per experiment. mbm of type i are not included in this computation, for they are inferred in step  <dig> of the algorithm. the results of this classification are shown in fig.  <dig>  the fourth chart illustrates that the real contribution of each expression profile does not depend on the amount of observed genes it contains.

discussion
predicting from a "small" number of expression profiles
in principle, inferring the functional effect of regulations could be done using general reconstruction methods. the most outstanding approaches in this domain include bayesian networks  <cit> , linear ordinary differential equations   <cit>  and correlation/causal networks  <cit>  . these are quantitative methods which are carefully designed to cope with the high level of noise that is generally observed in expression data. they rely either on an explicit parametric modelling of noise distribution , either on robust statistical estimators for the network and its kinetic parameters. the main limitation of these approaches is the number of independent samples they require in order to be properly used. it is often stated  <cit>  that a minimum of  <dig> to  <dig> expression profiles are needed for the estimation procedure. while there exists a couple of datasets of such size, the usual number of available profiles for a given biological system is much smaller. our approach is meant to be used when the number of profiles ranges from  <dig> to a couple of hundreds, and should thus be seen as complementary to quantitative methods. indeed our simulations on e. coli network show that one can characterise about 30% of the regulations from  <dig> expression profiles. we additionally showed that this is close to the theoretical limit of our approach. this result was confirmed using expression data on the same network: we infer 20% of the regulations whose input and output are simultaneously observed in at least one experiment, using  <dig> expression profiles.

generating accurate predictions
the problem of inferring functional effect of transcription factors was specifically addressed by yeang and colleagues  <cit> , using a probabilistic discrete model. in this approach, one identifies probable paths of physical interactions connecting a gene knock-out to genes that are differentially expressed as a result of that knock-out. predictions correspond to the signs found in models of maximum likelihood. more generally, most reconstruction methods are based on computing an "optimal" model with respect to the data. this raises two main issues. first, the underlying optimization problems are often non convex, and finding a global optimum is a very difficult computational task. in practice, most algorithms only guarantee to find a local optimum, which should be cautiously examined before being reported as a prediction. second, even if a global optimum is found, it is important  to check that there is no slightly sub-optimal model that yields very different predictions. in other terms, it is necessary to evaluate the robustness of the predictions. in our approach, we describe the  set of models that are consistent with the data, then look for invariants in this set. this means that our predictions are compatible with all feasible models. in order to cope with experimental noise, we combine this strategy with a filtering procedure, which selects predictions that agree with a minimal number of expression profiles. this led us to very accurate predictions, as it was shown on data from e. coli and yeast. we compared our inference approach to the path analysis method by yeang and colleagues  <cit> . we found that both algorithms infer a similar number of regulations, and that the predictions coincide. we noticed that the predictions are located in different parts of the network, depending on the algorithm: path analysis tends to infer signs in highly connected regions, while our approach infer signs on regulations acting on small in-degree nodes. another difference is that path analysis requires expression profiles from gene-deletion experiments, whereas our method gives better results with stress perturbation experiments .

sign inference and network topology
using simulations, we evaluated the dependence between the number of available expression profiles and the number of signs that can be inferred from them. not surprisingly, we noticed that the topology of the regulatory network has a strong influence on the estimated relationship. this was illustrated by computing statistics on both a complete regulatory network and its core. the complete network is characterised by an over-representation of feedback-free regulatory cascades, which are controlled by a small number of tfs. in this setting, the number of inferred signs grows almost continuously with the number of observations. in contrast, the core network does not obey the simple law "the more you observe, the better", some expression profiles being clearly more informative than others. additionally, in these core networks an unfeasible number of experiments is necessary to infer a small number of signs with high probability. for these core networks, two different strategies may be adopted. first, to build a more accurate model for these restricted subnetworks using dynamic modelling techniques . second, to develop experiment planning in our qualitative framework: given some control parameters, how to find the most informative experiments while keeping their number as low as possible?

CONCLUSIONS
in this work we proposed a discrete approach for a particular case of reconstruction problem: given a set of regulations between genes, and a set of expression profiles, determine the functional effect of each regulation, as activation or inhibition. our approach is based on a qualitative modelling framework, that was initially introduced to check the consistency between a regulatory network and expression data  <cit> . this framework is based on a rule, which basically says that if the expression of a gene varies between two conditions, then this should be accounted for by the variation of at least one of its predecessors. here we applied this approach to predict the functional effect of transcription factors on their target genes.

while intuitive and simple, the qualitative rule we propose can be used to infer a significant number of regulatory effects from a reasonable number of expression profiles. as shown using data on e. coli and yeast, the predictions are particularly reliable, especially when they are validated with our filtering procedure. furthermore, our algorithms can handle datasets of realistic size. it should be noted that computing the predictions presented in this work requires to solve thousands of np-hard problems . each of these problem has several thousands of variables. nevertheless, our algorithms are exact and compute the predictions in no more than an hour using a standard desktop pc. this means that they are able to cope with system-wide data in a fairly reasonable amount of time. due to the structure of the algorithms, we are confident that they can handle even larger datasets in less time, by distributing the computations on several machines.

from our results on yeast, it appears that a significant proportion of the network – as given by chip-chip data – is not compatible with the available expression profiles. as explained in the results section, these data is discarded from the analysis, in order to compute safe predictions – but at the expense of a loss of information. the subject of our current work is to develop an improved notion of prediction, that copes better with inconsistent network and data. the goal is to include inconsistent data in the inference process, while preserving the reliability of the predictions.

