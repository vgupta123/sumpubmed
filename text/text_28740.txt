BACKGROUND
introduction
in recent years, the resource description framework  has become the most popular sematic web technology for modeling large collections of data over the web. as a w3c standard model for exchanging data among web data repositories, rdf has been used in a large number of applications such as dbpedia  <cit> , a knowledge-management community of structured information extracted from wikipedia, or freebase  <cit> , an online social database collected from thousands of sources. in the domain of life sciences as well as bioinformatics, rdf is the common data model for a lot of public online bioinformatics resources  <cit>  such as uniprot   <cit>  or bio2rdf  <cit> .

an rdf can be considered as a collection of facts in the form of triples  that represent the relationship, indicated by the triple pattern predicate, between the subject and the object. an rdf database can also be represented as a directed labeled graph, called rdf graph, in which the subjects and the objects are the nodes of the graph and the predicates are the edges connecting these nodes. in company with the popular availability of the rdf stores, sparql  <cit> , the w3c recommendation query language for rdf, has played an important role in searching and extracting the data from various web knowledge-bases. a sparql query is a sql-like rdf query which mainly consists of two clauses - the select clause and the where clause. the select clause specifies triple patterns that need to be returned as the answer for the query, and the where clause consists of series of triple patterns which can also form a query rdf graph pattern. an example of a sparql query for finding a chemical compound that has the name “tryptophan synthetase” is:

select ?x where {?x <hasname> “tryptophan synthetase”; ?x <hassubstrate> “chemical”}.

in order to express diverse requests on extracting information from rdf data, sparql needs to be able to efficiently support regular expression processing. we consider an example where a researcher wants to find all proteins related with the ‘mouse.’ while it is hard to express this query without the regular expression, it is desirable and easy to represent this request by using the following sparql query with a regular expression.

select ?protein where {

?protein a <protein>.

?protein <mnemonic> ?m.filter

}

moreover, since users usually do not know the exact matching values of an rdf triple pattern, this example presents a common kind of request over rdf data and thus shows the necessity of supporting regular expression processing in sparql. it therefore motivates us to study the regular expression processing in rdf systems which, to the best of our knowledge, has not been efficiently supported by any of the existing rdf systems.

the regular expression processing has been studied in many existing literatures  <cit> ; however, most of the existing techniques are developed for a given regular expression over a text corpus, not rdf data. recently, there have been several works dealing with regular expression processing  <cit>  in rdf data. however, they merely consider processing the regular expression along the paths between two nodes in an rdf graph, but do not consider processing regular expressions over each rdf triple pattern in a sparql query.

in this paper, we propose an efficient framework for efficiently processing the regular expression in the standard sparql over an rdf database. our framework contains two main steps: index building and query processing. in the index building step, we exploit the approach which uses grams  in a text corpus  <cit>  for building the index of an rdf data. specifically, we first extract all the “useful” grams appearing in all the triples of the rdf data. then, we index all the extracted grams with their occurrence information by using an inverted index structure. in the query processing step, we find all indexed grams in the input regular expression, and then, construct an execution plan using these grams. in order to construct an efficient plan, we propose a cost model for evaluating the plan. to demonstrate the efficiency and effectiveness of our proposed framework, we prototype the framework in c++. then, by conducting extensive experiments with the real dataset such as geneontology  <cit> , we show that our framework can have significant performance improvement over multiple kinds of sparql queries supporting regular expression processing.

related work
rdf systems
along with the growth of semantic-web research, there are increasing numbers of studies on rdf - one of the most popular frameworks for representing semantic-web ontologies and knowledge bases. as an rdf database can be considered as a collection of data items in the form of triples , most of the existing rdf systems store the knowledge bases by creating relational tables over these rdf triples  <cit> . they either store all the triples in a giant relational table having three attributes subject, predicate, and object  <cit> , or store each group of triples having the same predicate in a so-called property table  <cit> . early and popular open-source rdf systems such as sesame  <cit>  and  <cit>  use the latter method for storing rdf triples. however, these systems have been empirically shown to be unsuitable for large scale datasets  <cit> . the current best performance rdf systems in the approach using property tables, such as oracle  <cit>  and c-store-based rdf engine  <cit> , exploit the materialized join views and the indexes on them, and thus, incur the challenge of a physical design problem due to the diversity of predicates and the lack of a global schema. recently, neumann et al.  <cit>  has developed a novel rdf system, called rdf-3x, that stores all triples in a huge table indexed by using six compressed clustered b+-trees. by following a risc-style architecture  <cit>  for indexing as well as query processing, rdf-3x can avoid the problem of physical-design and achieve efficient performance on large join queries - the inherent performance challenge in large rdf dataset. as rdf-3x outperforms other existing systems by a large margin, it can be considered as the-state-of-the-art rdf system. however, as far as we know, there is no rdf system designed for efficiently supporting sparql with regular expression pattern.

regular expression processing
there is a lot of literature studying the regular expression processing problem  <cit>  which finds the matching strings in a text for a given regular expression . the most common approach uses the finite automata, in which the given regex is converted to an equivalent dfa, and then, all the strings accepted by the dfa are returned as the results for the regex  <cit> . for efficiently searching the matching strings of the regex in a large amount of documents, many techniques using pre-build indexes for the regex have been proposed. baeza-yates and gonnet  <cit>  construct a suffix trie for indexing all the suffixes of the text and directly simulate the minimal dfa corresponding to the given regex over all paths of the trie in order to find the matching strings. this solution can answer the queries in logarithmic average time; however, since the size of the constructed suffix trie is several times larger than the text corpus, this solution is not suitable for the large databases. in  <cit> , cho and rajagopalan speed up regular-expression matching on a large database by proposing an inverted index structure called multigram index. in this index, the most “useful” grams such as the most selective grams from the text corpus will be indexed, associating with the posting lists of document ids containing those grams. then, a physical access plan containing the indexed grams in the given regex will be generated for facilitating the query processing. however, in contrast to our framework, these regular expression processing techniques are developed for searching over the text corpus, not for querying the rdf data. in the domain of rdf databases, there have been several efforts considering the regular expression processing in rdf queries. however, the existing works concentrate on extending sparql queries to process the regular expression over the paths of rdf graphs , while our proposed framework supports the regular expression pattern in the standard sparql which is able to process a matching rdf triple pattern for a given regex. alkhateeb et al.  <cit>  adopt a new query language, called psparql, that extends from sparql by allowing regular expressions to be used for the labels of the arcs in the query graph. in this query language, the regular expression is used for capturing the relationship information along the paths between two nodes in the rdf graph. similar to this approach, kochut et al.  <cit>  propose another extension of sparql, called sparqler, by using the path variables in the query graph pattern. here, the path variables are simple paths between two nodes in an rdf graph that contain constraints based on regular expressions. for querying the knowledge graph such as rdf graph in a search engine, naga search engine  <cit>  introduces a graph-based query language which is akin to sparql that also allows regular expression matching over the paths in the rdf graph, in which the regular expression is placed at the edge label in the query graph.

to the best of our knowledge, there is no work fully supporting sparql with regular expression patterns in rdf databases.

methods
index building
to process regular expressions efficiently, we need to construct an efficient index structure. in this section, we will explain our index structure, and the algorithm for constructing the index.

first, we formally present some related concepts. each data item in rdf data is represented as a triple, with the form of . this triple is also called as an rdf triple. an rdf database is a database storing the rdf data. for a given rdf data, an rdf database d = {t <dig>  t <dig>  …, tn} is a set of all triples in the rdf data, where ti is an rdf triple whose id is i. each rdf triple ti in d contains subject, predicate, and object parts, represented as ti.s, ti.p, and ti.o, respectively. for example, an rdf database for the rdf data in figure  <dig> can be represented as

d = {t <dig> = , t <dig> = , …}.

because our index uses the gram as a basic indexing unit, we also need to know the concept of the gram. for a given string s, an n-gram is a substring of s whose length is n, and Ξn is a set of all n-grams of s. for example, for the string s = “mouse", the set of all 3-grams is represented as Ξ <dig> = {mou, ous, use}. in our index building algorithm, for given constants α and β, we index a subset of  to minimize the index size and maximize the pruning power.

algorithm  <dig> shows our index building method indexbuild. this algorithm returns three constructed indexes is, ip, io using three input parameters, d, α, and β. the first parameter d is an rdf database which stores all triples. the other parameters, α and β are the minimum and maximum size of the grams to be indexed. the algorithm first extracts three strings , presented in the tree part of the triple . then, for each string, it extracts all grams in the string and stores the selective grams among them by calling the extractandinsertgrams function . note that, we build three separate indexes for each subject, predicate, and object parts of triples. the extractandinsertgrams function first finds all n-grams, α ≤ n ≤ β, and assigns them to the set g . then, for each gram g in g, if g is a selective gram, g is inserted into the index i with its occurrence information . we will explain the concept of the selective gram later. by using this concept, we can reduce the number of the grams to be indexed. finally, indexbuild returns the constructed indexes is, ip, io .

selecting indexing grams
in this section, we explain how we select the grams to be indexed. we use the gram selecting technique described in  <cit> , where there are two goals for selecting grams: 1) maximizing the pruning power, and 2) minimizing the size of the index. to maximize the pruning power, the authors select infrequent grams that prune as many of the candidate strings as possible. to minimize the index size, they use the concept of the selective gram set. with this concept, they can further reduce the number of the grams to be indexed by removing redundant grams.

algorithm  <dig> indexbuild

require:d; a set of triples

require:α, β; minimum and maximum size of a gram

ensure: a set of indexes constructed {is, ip, io}

1: for each triple t in d

2: {ss, sp, so} ← extractstrings;

3: extractandinsertgrams;

4: extractandinsertgrams;

5: extractandinsertgrams;

6: return {is, ip, io};

function extractandinsertgrams

require:i; an index that extracted grams will be inserted

require:s; a string that grams will be extracted

require:α, β; minimum and maximum size of a gram

7: g ← findallgrams;

8: for each gram g in g

9: ifg is a selective gramthen

10: insert g into the index i with its occurrence information;

to maximize the pruning power,  <cit>  selects the grams which appear infrequently among all documents. the selectivity of the gram g, denoted as sel, is defined as c/n, where n is the total number of documents, and c is the number of documents that contain the gram g. if the selectivity sel is low, the gram g appears infrequently. thus,  <cit>  can prune many candidates by using the gram g that has low selectivities. for example, for given grams g <dig> and g <dig>  assume that the selectivities of these two grams are  <dig>  and  <dig> . they can prune 90% of documents using g <dig>  while they can only prune 10% of documents using g <dig>  to find the infrequent gram, for a given threshold c, they select grams where the selectivity of a selected gram is less than or equal to c, among all possible grams in all documents.

even if only infrequent grams are selected, the number of grams remains large. thus,  <cit>  reduces the number of grams to be indexed further by removing the redundant grams. assume that, for the string “gpr64_mouse”, the 3-gram “mou” is an infrequent gram. then, all the grams containing “mou” are also infrequent grams. for example, “4_mou”, “_mou”, “mous”, and “mouse” are infrequent grams, because the selectivities of these grams are either less than or equal to c. because these grams are redundant, it isn’t necessary to index all of them. thus, they remove these redundant grams — the grams which contain another infrequent grams — in all infrequent grams. in this paper, we call the remaining grams selective grams.

index structure
in this section, we explain how to store and manage the selective grams. we use an inverted index for which the grams are the keys. each key in the inverted index is associated with a posting list. the posting list contains occurrence information about the gram in the rdf database. formally, for a given gram g and its occurrence information p, we insert a pair  into the index as the key/value. the occurrence information p is a set of , where tid is the id of a triple containing the gram g, and offs is a set of offsets in the string where the gram g appears. note that, g can appear many times in one string. the offsets in offs are used for the query processing as we will explain later. we also build a dictionary of all triples to convert a triple into the three-part string ids. using this dictionary, a triple id in the posting list can be converted into a triple of string ids.

in our framework, we construct three indexes for subject, predicate, and object parts separately, because the regular expressions for these three parts should be processed independently. that is, we construct three indexes, is, ip , and io for each part of the rdf triples. figure  <dig> shows an example of the inverted index that we construct. as the figure illustrates, each index of is, ip, and io has two data structures, a b+-tree and a posting list. the b+-tree stores the gram and the reference to the posting list as a key/value pair. additionally, we also store the number of pages of the posting list in the value part. this information is used for the cost estimation of the query. the posting list contains the occurrence information. for example, the gram “mouse” appears in t <dig> and t <dig> at the sixth offset for both object part strings.

query processing
in this section, we explain the query processing algorithm. we first discuss the query processing without regular expressions for the background knowledge of the query processing in section. then, we explain how we process regular expression queries in section.

query processing without regular expressions
for a given sparql query q, the query processor converts query q into an optimal query execution plan , then executes the optimal qep to get the results. the qep is a rooted tree of operators. the leaf nodes of the qep are scan operators associated with triples in the query q. these scan operators find the triples in a database and return them to their parent nodes. the internal nodes of the qep get the inputs from their child nodes, and they do the appropriate operations according to their operator types. qep <dig> and qep <dig> in figure  <dig> are qeps for the example query in figure  <dig>  the query in figure  <dig> has two matching triples. the leaf operators of qep <dig> and qep <dig> are associated with these matching triples. these operators find the associated triples from the database, and return string ids for the variable ?x to the parent operator mgjn  or hsjn . mgjn or hsjn operator gets the string ids from child nodes, and returns the joining results of both inputs.

to find the optimal qep, the query processor uses a query optimizer. the query optimizer finds the optimal qep among all possible qeps based on their costs computed by using a cost model. that means, the query optimizer enumerates all possible qeps for a given query, and then selects the minimal cost qep as the optimal qep. for the query execution model, we use the getnext model  <cit>  which is very simple to use and has good scalability. specifically, all operators in a qep have getnext() functions. for each call of the getnext() function, the next results are returned one-by-one. to obtain all final results, the query executor calls the getnext() function of the root operator of the optimal qep until the function returns no more results. the getnext() function calls are propagated to the descendants. that means the getnext() function in the root operator calls the getnext() functions of its child operators in order to get the results from them.

query processing with regular expressions
to support regular expression queries, we develop a new operator, called regscan, and adapt it to the query processing engine. for a triple pattern matching with a regular expression in a sparql query, the regscan operator finds candidate triples which can be matched with that pattern in a database. in this section, we explain how this operator is created and implemented in a query execution.

plan generation
regscan has a regular expression sub-plan to evaluate the regular expression. to generate this sub-plan, we adapt the technique in  <cit> , and we summarize the technique in the next sub-plan generation section. we explain our technique using the example below. here, the results for the query triple should contain substrings, in the object part, “gpr” or “ccd”, and following these substrings, the substring “mouse” must appear.

?protein <mnemonic> ?m. filter.*mouse.*”)

for the example above, we generate qep with regscan as in figure  <dig>  the qep has two operators, regscan and filter. the regscan operator finds candidate triples for the matching triple pattern by using the containing-regular-expression sub-plan. because the regscan operator can find false positives, we must verify the results of the regscan operator. the filter operator verifies the results from regscan. we explain how the regular expression sub-plan in figure  <dig> is constructed in the sub-plan generation part.

plan execution
the execution model of the regular expression sub-plan follows the getnext model as well. every sub-operator in the sub-plan has the getnext() function, and it returns the results one-by-one for every getnext() function call. the getnext() function of the idxscan for the gram g returns all triple ids sequentially by scanning the posting list of g. idxand or idxor operators intersect or union the results from their children. the idxand operator uses offset information in the posting lists. when the offset of the triple from the left child is less than that of the right child and those triple ids are the same, it returns the triple corresponding to this triple id. when the idxand intersects two sets of the triple ids, it uses this constraint.

the getnext() function of regscan operator calls the getnext() function of the root operator of its sub-plan to get the candidate triple ids and then converts each triple id into three string ids using the dictionary built in the index building algorithm. for example, regscan operator in figure  <dig> gets the triple id by calling the getnext() function of the root operator  of the regular expression sub-plan. then, each triple id is converted into the string ids, and regscan returns these string ids to the filter operator to verify the result.

sub-plan generation
the technique in  <cit>  has three steps for a given index i and a regular expression. these three steps are summarized as follows:

 <dig>  normalize the input regular expression into the new regular expression that has only or or star.

 <dig>  construct the parse tree using the normalized regular expression.

 <dig>  convert the parse tree into the execution plan.

to adapt the technique in our framework, we first find the appropriate index among is, ip, and io by checking the position of regular expression appearing in the triple pattern. in the example query, the regular expression is bound with variable ?m, and ?m appears in the object part of the matching triple pattern. therefore, we can select io to evaluate the regular expression.

for the first step of the algorithm, the regular expression “. *mouse.*” in the query triple is converted into “**”, after the normalization. here, the periods are converted into the all characters connected with or operators. in this step, for example, an aggregated regular expression, such as , is converted into , and a+  is converted into aa*.

in the second step of the algorithm, we can construct a parse tree using this normalized regular expression. the example of the parse tree for the normalized regular expression is represented in figure  <dig>  the leaf nodes in the parse tree contain grams that are separated by or, and, or star operators. then, the sub-trees whose roots are the star nodes are converted into the all node. this is because star nodes can be represented as all combinations of their descendant nodes, and the number of possible grams for the star nodes is infinite. the shaded nodes in figure  <dig> are the converted ones. after the conversion, the nodes that have the all nodes as their children are merged into and or or nodes with their children. if the node is an or node and has an all node as its child, this node is merged into an all node. in this case, because the all node is generated again, the merging step is applied recursively, so that the final parse tree has only and or or nodes as its internal nodes.

in the third step, we build an execution plan using the parse tree constructed in the second step. in this step, we first convert the and and or nodes into idxand and idxor operators respectively, and leaf nodes are converted into idxscan operators. an idxscan operator for the gram g returns all ids of the triples which contain g. an idxand or an idxor are logical and or or operators between the two sets of the triple ids that come from its child operators. for example, figure  <dig> represents the converted parse tree in figure  <dig>  the and and or nodes are converted into the idxscan and idxor operators, and each leaf node g is converted into the idxscan operator.

because all grams in the parse tree are not indexed, there may be no results for the idxscan operators. therefore, we need further modifications in the execution plan. for the gram g, there would be three cases: 1) g is indexed, 2) g is not indexed, but some substrings of g are indexed, and 3) g is not indexed, and its substrings are not indexed either. for the first case, we do nothing. in the second case, we replace the idxscan operators with the idxand operators of the idxscan operators of g’s indexed substrings. in the third case, we replace the idxscan node with the all node and apply the merging technique in the parse tree building step. for example, in figure  <dig>  because the gram “mouse” is not index, and its substrings “mou” and “use” are indexed, idxscan in figure  <dig> is converted into the idxand between idxscan and idxscan.

cost model
in this section, we explain the cost model of the query processing. our cost model is based on the cost of i/os to access the index pages. the cpu cost for the verification of the candidate triples can also affect the execution time. however, we do not consider the cpu cost because it is negligible compared with the i/o cost. the cost for regscan operator, cost, is defined as follows:

cost = onepageaccesscost ×  + ∑g∊g numpages),

where onepageaccesscost is a unit cost for accessing a page, g is a set of all grams in the execution plan, and numpages is the number of pages of the posting list for the gram g. to evaluate the execution plan, we have to access all posting lists and pages in the b+-tree.

this cost model can be used in the cost-based query optimizers of the rdf database. we exploit a cost-based bottom-up query optimizer to find the optimal query execution plan for the given sparql. using the cost model, our algorithm can be adapted seamlessly.

RESULTS
setup
we implemented our framework using c++. the codes were compiled using the gnu gcc  <dig>  compiler. all the experiments were conducted on a pc with an intel xeon quad core cpu and 8gb ram running 64-bits linux. we compared the performance of our algorithm with sesame  <cit> . our framework is denoted as regscan. in our experiments, in order to avoid the problem of large database size as well as long index building time, we empirically select the values  <dig> and  <dig> for the parameters α and β, respectively. we use the threshold value  <dig> , which is commonly used for finding infrequent grams for the parameter c. we used geneontolgy rdf dataset  <cit> , a common real dataset which contains  <dig>  triples for the experiments. for query processing, we used  <dig> queries which are presented in appendix a. before execution of each query, we flushed the os caches using the /proc/sys/vm/drop_caches interface to minimize the caching effect.

results for the database building
results for the query processing
this is because regscan can avoid full-scanning of the database, and it only scans the posting lists which have very small sizes in comparison to the database size. we observe that regscan performs more effectively, when the selectivity of the regular expression is low, since the regular expression with low selectivity accesses small sizes of posting lists for the grams. since sesame does not support scalable regular expression processing and is implemented using java, it has much more overhead compared with our framework, and shows the worst performance results.

CONCLUSIONS
in this paper, we have presented a novel framework which can be considered as the first solution for efficiently supporting regular expression processing of sparql queries over rdf databases. for building this framework, we first extract the grams from rdf data and efficiently build an inverted index structure based on the set of selective grams. then, in the query processing, we find the indexed grams from regular expression patterns, and generate an efficient execution plan by using our proposed cost model for regular expression processing. to demonstrate the performance of our techniques, we have prototyped the proposed framework in c++ and then compared the efficiency and effectiveness of our systems with a popular rdf system . the experimental results over large knowledge resources which are commonly used in bioinformatics, such as geneontolgy, have shown that our framework is an efficient and effective solution for processing regular expressions over rdf data and is useful for extracting information from bioinformatics knowledge databases.

authors' contributions
jinsoo lee took part in the implementation of the prototype system. he wrote the manuscript describing the methods and took part in conducting the experiments. minh-duc pham wrote the remaining parts of the manuscript and conducted the experiments. jihwan lee implemented the prototype system with jinsoo lee. wook-shin han developed the idea, supervised the work, and helped write the manuscripts. hune cho, hwanjo yu, and jeong-hoon lee critically revised the manuscript. all authors read and approved the final manuscript.

competing interests
the authors declare that they have no competing interests.

appendix a: query set for geneontology
q <dig>  select * where {?gp rdfs:label ?name filter regex  .}

q <dig>  select * where {?gp rdfs:label ?name filter regex  .}

q <dig>  select * where {?gene ?pred ?isbn filter regex  .}

q <dig>  select * where {?gp rdf:type ?type .?gp rdfs:label ?name filter regex  .}

q <dig>  select * where {?gp rdf:type ?type filter regex  .?gp rdfs:label ?name .?gp rdfs:comment ?comment .}

q <dig>  select * where {?gp rdf:type ?type filter regex  .?gp rdfs:label ?name filter regex  .?gp rdfs:comment ?comment .}

q <dig>  select * where {?gp rdf:type ?type filter regex  .?gp rdfs:label ?name filter regex  .?gp rdfs:comment ?comment filter regex .}

q <dig>  select * where {?gp <dig> go:consider ?gp <dig> . ?gp <dig> ?p <dig> ?ns <dig> filter regex  .?gp <dig> ?p <dig> ?ns <dig> filter regex  .}

