BACKGROUND
correlations between genotype and phenotype variations have traditionally been studied by determining the genotype of known markers. for example, genome-wide association studies  have revealed associations of known common variants with several of the common diseases. but these associations typically explain less than 25% of the heritable risk estimated for each of those diseases  <cit> . this is a serious limitation for complex diseases, which are often genetically heterogeneous  <cit> . existing gwas data suggest that rare alleles also have a significant influence on common diseases  <cit> . therefore, targeted resequencing of suspected exonic, intronic, and intergenic loci mapped by gwas and linkage studies is the next logical consequence to identify the entire underlying genetic variation and its disease relevance.

target enrichment methods rely either on pcr or sequence-specific nucleic acid hybridization, and each method has unique advantages and disadvantages  <cit> . the combination of sample enrichment methods and next-generation sequencing  pipelines is an effective analysis approach, but it also raises some important questions: how accurate is the sequence variation discovery within the targeted ngs  data? what is the effect of the varying performance of the rapidly evolving alignment and analytical tools? can a sample multiplexing approach reduce the analysis and study costs? to address these questions, as a test model, we combined a hybridization-based sample enrichment method  with an ngs platform . with sample mix-up control and enrichment cost reduction in mind, we evaluated the enrichment of barcoded solid libraries , rather than preparing ngs libraries after enrichment  <cit> . individual samples are indexed by inserting unique nucleotide signatures, the barcodes, and are then pooled together before enrichment so that the pooled dna samples can be sequenced en-bloc. sample identity is then re-established during the bioinformatic processing of the reads after sequencing. sample barcoding has the advantage – in contrast to a simple pooling approach – that genotypes can be assigned at an individual level and that even rare variants can be identified with a high sensitivity  <cit> . we tested the applicability of the evaluated tngs method on e. coli, as a simple model to optimize the process, and human genomic samples in three experimental stages of increasing scope and complexity, culminating in a snp concordance evaluation for the brca <dig> and brca <dig> cancer genes.

to identify a suitable analytical approach in terms of computational time and accuracy of sequence variant detection in tngs data, we carefully analyzed the data using different software and tools. we found that snp detection depends strongly on the chosen analytical tools and settings, rather than on key enrichment measures such as high and uniform coverage, the percentage of reads mapped on target, or an adequate enrichment fold. unexpectedly, all evaluated tools failed to identify a large proportion of true sequence variations . whole-genome  mapping/snp-calling was time-consuming, but with the benefit of few false positive snps. target region  mapping/snp-calling was faster and yielded more true snps than wg mapping, but led to a high proportion  of likely false positive snps. target region mapping can force-map reads from other genomic loci into the target, leading to false positive snps and requiring a postprocessing cleanup. to benefit from both methods, we here report a novel ‘two-step’ mapping approach that starts with tr mapping/snp-calling, followed by backmapping only the snp-supporting reads to the wg. because of the recent wide interest in whole exome sequencing, we also applied our approach to public human exome enrichment data generated by illumina hiseq instruments. moreover, our report includes detailed practical instructions, such as validating snps by computing inter-sample snp-concordances between multiplexed technical replicates, filtering for novel snps, or establishing and evaluating a tngs method.

RESULTS
we developed a novel, fast two-stage backmapping method in the course of three experimental and analytical stages  and a fourth, purely analytical stage using human exome data. the study design, the established wet-lab workflow, and the bioinformatics workflows for our test tngs model are presented in figure  <dig>  in the first stage, we successfully enriched different solid libraries instead of gdna of e. coli for  <dig> genes and sequenced them on a solid ngs platform. in the second stage, we enriched three pools of barcoded libraries of e. coli for the same  <dig> genes and sequenced these on the same ngs platform. the results of this stage showed reproducible uniform coverages and enrichment folds for most barcodes and multiplexes tested. full details on the target enrichment experiments and results, especially the wet-lab procedure, can be found in the additional file  <dig>  in the following results section we concentrate on the respective data analysis and tool development.

pre-enrichment multiplexing of ngs libraries of brca1/ <dig> genes and data analysis
in the third stage, we focused on enriching pools of barcoded libraries of hapmap individuals for non-repeat-masked regions of two clinically relevant human cancer genes . we enriched these genomic regions at different multiplexing levels . we also included two non-barcoded controls. the sequencing results and enrichment measures are summarized in additional file 2: table s <dig> and additional file 3: figure s <dig>  for the data analysis we used several tools, partly because the original tools turned out to be insufficient and incompatible with newer tools, and partly to benchmark the performance. we used the solid spectral analysis enhancement tool  for correcting sequencing errors in the raw reads, and evaluated the coverage and snp-calls with and without the use of saet. we initially used solid corona lite for mapping and dibayes for snp-calling, but after achieving only insufficient results, we turned to solid bioscope for mapping and snp-calling. we validated the mapping and snp-calling using clc bio genomics workbench  <dig> . <dig> software  and the nextgene v <dig> software . we used samtools  <cit>  for data format conversion, pibase  <cit>  for automatic validations, and igv  <cit>  for viewing mapped reads. as a first result, we found that saet increased the coverage by about 15-20% , but that the run time was very high  and that the automatic snp discovery rate was slightly decreased . bioscope with saet mapped up to 113% more reads than corona lite without saet as shown in figure  <dig>  it also illustrates that the saet error-correction combined with the bioscope mapping led to the highest number of mapped reads and the highest coverage for the solid data. secondly, as expected, the tr mapping/snp-calling approach required far less time than the conventional wg approach. thirdly, we found that the resulting average coverages and enrichment factors were reproducible within each pool . the average depth of coverage  was 407×, 330×, 187× and 30× for the 4-, 8-, 16-, and 20-plex experiments, respectively. at 8× coverage – a minimum snp detection threshold commonly employed in ngs studies –  <dig> %,  <dig> %,  <dig> %,  <dig> % and  <dig> % of the targeted bases were covered in the control libraries, 4-, 8-, 16- and 20-plex, respectively . overall, the enrichment process was efficient, with regard to sensitivity and specificity, as indicated by the area under curve  values of  <dig>  and  <dig>  for the control libraries, and  <dig> ,  <dig> ,  <dig>  and  <dig>  for the 4-, 8-, 16- and 20-plex libraries, respectively . full details on the enrichment metrics can be found in the additional file  <dig>  

genotype concordance and overlap analysis in the brca1/ <dig> experiments
the identification of sequence variants in targeted region is a typical objective of resequencing. to test how well variants are discovered using our multiplexed tngs model, we benchmarked different analysis strategies and focused on snps, the most abundant form of variation. our snp discovery results for selected multiplexes and different analysis strategies are summarized in figure  <dig> and additional file 2: table s <dig>  the initial snp calling results using corona lite were not promising . we therefore reran the mapping and snp-calling stages using bioscope. we compared the snp overlap and genotype concordance for several data processing strategies. additional file 2: table s <dig> shows a genotype concordance rate of 100% for the non-indexed control samples. the 4-, 8-, 16- and 20-plex experiments yielded an average genotype concordance rate of up to 98%, 79%, 80% and 55%. it was surprising to us that bioscope consistently overlooked a few known ‘gold’ standard snps  despite high coverages at the respective snp positions in all yoruban samples . the bioscope consensus call output file gave more details than the bioscope snp file, reporting code ‘h15’ for rs <dig>  and codes ‘h <dig>  h <dig>  h9’ for rs <dig> . we therefore complemented our analysis with clc bio, nextgene, pibase and igv. we streamlined our manual genotype calling in igv by classifying a genotype as homozygous if more than 80% of the reads indicated the same base at that position. otherwise we classified it as heterozygous  <cit> . 

for the yoruban sample we observed a good snp overlap  and reached a concordance of 14/ <dig> through visual inspection in igv. bioscope called only 13/ <dig> snps with the chosen settings and also with many alternative settings . our manual inspection of the mapped reads at the false negative snp positions clearly showed a non-reference allele consensus, i.e. a snp . the clc bio analysis  called 15/ <dig> snps with a concordance of 13/ <dig>  . the nextgene analysis called 12/ <dig> snps . for the chinese individual , the bioscope genotype concordance rate was near 100% for the 4-plex samples and the control sample, but the bioscope snp overlap was significantly lower than for the yoruban samples. this, we assume, was partially due to the considerable false positive rate in the hapmap  <dig> data. finally, we analyzed the yoruban samples to distinguish potential novel snps from false positives or sequencing errors. we eliminated about 15-20% of snps as false positives using snp-backmapping . then we eliminated about 20-30% of snps in the 4-plexes as unconfirmed using inter-sample concordance checks between technical replicates . for one sample and one bioinformatics strategy , we manually inspected the remaining snps to assess more accurately the false positive snp calling rate after our filtering process: bioscope called  <dig> snps, of which we removed  <dig> snps  by backmapping the snp and its flanking  <dig> reference bases to the wg. we then eliminated  <dig> snps  after inter-sample validation within the same multiplex, considering concordant genotypes shared by several samples valid and rejecting the rest as potential false positives. we eliminated  <dig> snps  that were known in dbsnp <dig>  we eliminated one further snp  that was known in our ‘silver’ consensus . we inspected the remaining  <dig> snps  in the igv viewer , leaving  <dig> snps  as potential novel snps and estimating the upper bound for false positives called by bioscope at  <dig> snps , and the upper bound for false positives after manual inspection at  <dig> snps . we repeated this procedure for the non-saet reads that resulted in  <dig> potential novel snps . finally, we identified the most likely potential novel snps , i.e. those that were present in the saet-reads and the raw reads, and subjected these to sanger sequencing. as controls, we also selected some unlikely novel snps and a known snp for sequencing . we could not validate the potential novel snps.

fourth stage: read-backmapping approach for public exome data
in the fourth stage we applied our combined mapping approach  to two human exome illumina hiseq <dig> data sets: a female ceu hapmap individual and her father . the entire process  was two-fold faster, required only  <dig> hours  of computational time per exome on an eight-core linux compute node . at the initial snp-calling stage, samtools called  <dig>  variants for the daughter  of which  <dig>  were homozygous, and  <dig>  for the father, of which  <dig>  were homozygous. after read-backmapping to the wg and final snp-calling,  <dig>  variants remained for the daughter and  <dig>  for the father. this means that ~50% of the originally called variants were eliminated as likely false positives resulting from force-mapped reads . it should be noted that some of the eliminated variants could be true variants in homologous regions of the genome. after filtering away the variants outside the exome,  <dig>  remained for the daughter and  <dig>  for the father. we estimated the false negative rate of samtools snp-calling in the target region to be 9% , by computing the overlap with known snps in hapmap chip data . by comparison, we also performed mapping to the wg, snp-calling, and filtering of non-exome variants, which required  <dig> hours of computational time. this resulted in only  <dig>  snps for the daughter and  <dig>  for the father, and showed that our approach has the potential to ‘rescue’ 3%-5% of valid snps which may not be detected using wg mapping. the samtools false negative rate in wg was only  <dig> %. the snp overlap between our approach and the conventional approach was only 92% , reflecting the problem of samtools not detecting all true snps, which we already mentioned in the brca1/ <dig> experiment . it should be noted that we used all snps without filtering away low-quality snps, to reduce the number of false negatives. the concordance between tr/backmapping and wg is 99% when including these unfiltered snps, showing that the backmapping method is applicable. our results also suggest that samtools false negatives can only be reduced by using the tr/backmapping method as well as the wg method. 

discussion
we here present a novel and more specific ‘two-stage’ mapping/snp-calling approach, which can speed up the analysis of a human exome sample by  <dig> hours on an 8-core compute node, and which can be applied to any targeted enrichment sample/region. we evolved this approach when we analyzed targeted enrichment runs and found that conventional mapping and snp-calling takes up a wasteful amount of time and computing resources.

in our targeted enrichment experiments, we first evaluated a scalable multiplexed protocol for high coverage tngs to investigate cost saving and quality control potential. we subjected barcoded  solid libraries instead of gdna to one selected microarray-based sequence capture method. we tested this pre-enrichment sample multiplex approach by sequencing  <dig> e. coli genes as well as two human cancer genes  in three independent stages and different multiplexing folds on the solid system. we achieved good and reproducible coverage profiles for the trs across most of the different multiplexed samples, enriching human exonic as well as intronic regions with less than 10% strand bias . nevertheless, our enrichment design successfully captured only 54% of human brca1/ <dig> regions. this weakness of the tested enrichment design would also apply to other hybridization-based sequence capture bait designs that subject targets to repeat masking before probe design. for this reason, longer capture baits and iterative refinement of the bait design would be required for such genomic regions with low complexity. to evaluate the snp discovery performance we then analyzed our sequencing data employing different mapping and analytical tools, which have rapidly evolved within the last two years . we found that snp detection in enriched regions - even at high coverages - depends strongly on the tools and their settings. these snp-callers, including the widely used samtools  <cit> , do not seem to be well-trained to handle enrichment data, and thus produced a significant fraction of false positive and negative snp calls . to partially overcome this common mapping/snp calling problem we proposed to combine the advantages of wg mapping  with tr mapping   and developed our novel ‘two-step’ mapping approach. in this approach we first mapped raw reads to the tr to achieve faster mapping and snp-calling, due to the smaller reference sequence, and to detect more snps, due to mapping more reads into low-complexity regions and obtaining higher coverages. we followed the first step with a snp-cleanup mapping step to reduce false positives: in this step, only the snp-supporting reads and their paired mates are mapped back to the wg . our novel approach resulted in more valid snps being detected and a more than two-fold speed-up of the time-consuming exome analysis.

inadequate enrichment and/or coverage can prevent the detection of real nucleotide variants, leading to higher false negative rates, particularly for heterozygotes  <cit> . in general, 20-fold coverages are deemed necessary for reliable sequence variation calling in data from the  <dig> platform  <cit> , illumina genome analyzer  <cit> , and solid  <cit> . other studies recommend a sequence coverage higher than 30× to minimize the risk of failing to detect true snps  <cit> , and at least 33× to enable the correct genotyping of most of the heterozygous positions  <cit> . these high coverage thresholds are backed by a simulation of the snp detection performance at the nod <dig> gene, which is associated with crohn’s disease; it fell rapidly when the achieved coverage was below 40×  <cit> . but a high coverage is not the only prerequisite for accurate detection of sequence variation  <cit> . rather, snp detection seems to be significantly affected by the chosen alignment tools and snp callers, as revealed from the results of our analyses . this is also in agreement with the results of a comparative analysis  <cit>  of different alignment tools, which showed that there was a disturbingly low level of agreement between genome alignments produced by different tools. it concluded that it was not possible to make definitive qualitative statements concerning the alignment tools, as there are distinct trade-offs in their behaviours. indeed, the  <dig> genomes project consortium  <cit>  reported that their ngs genotyping accuracy at heterozygous sites was 95% and higher in some regions, dropping off to 70%-80% or lower in “harder to access regions of the genome”. to avoid false positives, the novel snps published by the consortium are the consensus of two or more independent groups, sequencing platforms, and pipelines.

our experimental results  show that the enrichment and sequencing led to highly covered bases and the greatest number of snp calls for the non-barcoded controls and the 4-plexes. they also illustrated that all snp-calling tools performed weakly, with the exception of pibase and ‘manual’ inspection of aligned reads in igv. depending on the snp-caller, wg mapping is generally characterized by a lower false positive and a slightly higher false negative snp detection rate. the reverse holds true for tr mapping. accordingly, combining both mapping approaches may help to rescue undetected true snps, and filter false positive snp-calls. regarding snps in non-repeat-masked regions, we feel confident that tr mapping with read-backmapping to the wg is an accurate and reliable method, because we discovered  <dig> genotyping errors in the hapmap data  within our trs. we supported this conclusion by sanger sequencing . when comparing our brca1/ <dig> snps with the hapmap <dig> snp chip data, we detected up to  <dig> × more known snps than have been published as hapmap chip data.

snp-callers were originally optimised for low or moderate variations in coverage. in other words, extremely high coverages or high coverage gradients are currently challenging from the bioinformatics point of view. even for non-enriched samples, some degree of automatic or manual postprocessing is required specifically to distinguish snps from misalignments  <cit> . a previous study  <cit>  confirms our findings and emphasizes that manual inspection is an essential part of the analysis. as large-scale manual inspection is unfeasible and prone to subjective errors, we use the pibase software  <cit> , manuscript under review) for interrogating bam files  <cit> , re-typing snps, and other analysis tasks. as shown in our brca1/ <dig> and exome analyses, the re-typing of snps in targeted enrichment experiments is cbuindispensible . we further recommend, if cost allows, to duplicate samples  and validate computed snps by inter-sample concordance checks between these technical replicates.

the bioinformatic processing of eukaryotic and metagenomic sample data sets can occupy a compute cluster for days to weeks. for multiplexed targeted enrichment of human samples , the genome-sized data traffic may overload the compute cluster when too many samples are processed in parallel. compared to bioscope, other pipelines such as bwa alleviate this problem, but a complete exome run using bwa, samtools and picard nevertheless takes about a full day. our aim was to cut the computational time significantly by mapping to the small tr  and cleaning up force-mapped reads. for exomes, we cut run time by more than half, enabling overnight runs on an 8-core node, or turnaround within a working day  on a 16-core node. this run time cut is necessary to match the speed of new high-throughput platforms such as the illumina hiseq <dig>  which takes  <dig> hours for producing  <dig> gb .

CONCLUSIONS
we successfully demonstrated our novel time-saving ‘two-step’ mapping approach using illumina hiseq <dig> human exome data from the  <dig> genomes project. this approach consists of tr mapping with subsequent snp-cleanup by read-backmapping to the wg. we developed this approach after designing targeted enrichment experiments and experiencing an odyssey of run-time and snp-detection problems when we used a wide range of mapping and snp-calling tools. we recommend our approach and the employed tools  for a reliable and efficient analysis of exome and targeted enrichment data. to attain confident results, we specifically recommend the snp-validation between duplicated samples and a final in silico validation using recently developed software and/or manual inspection.

