BACKGROUND
the knowledge discovery in databases  process is based on three main operations: data preparation, data mining, and interpretation of the extracted units. this process is guided and controlled by an expert of the concerned domain. the kdd process has been successfully applied in various domains such as marketing, finance, and biomedicine  <cit> .

however applications of kdd are limited by the fact that strong interactions between the system and domain experts are necessary. data manipulated in life sciences are complex and data mining algorithms generate large volume of rough results. as a consequence, the interpretation step of kdd in biology, aimed at extracting new and relevant knowledge units, is a hard task, i.e. time-consuming and tedious for the domain expert.

in computer science, ontologies provide a shared understanding of knowledge about a particular domain  <cit> . bio-ontologies are becoming more and more available and contribute to the understanding of the large amounts of data existing in life sciences  <cit> . the national center for biomedical ontology  has recently developed bioportal that offers a unified panorama on available bio-ontologies  <cit> .

one of the promising interests of bio-ontologies is their use for guiding the process of kdd as suggested by anand  <cit> , cespivova  <cit> , gottgtroy  <cit> , and napoli  <cit> . this idea seems to be much more realistic now that semantic web advances have given rise to common standards and technologies for expressing and sharing ontologies  <cit> .

in this way, the three main operations of kdd can take advantage of domain knowledge embedded in bio-ontologies.

 during the data preparation step, bio-ontologies can facilitate the integration of heterogeneous data and guide the selection of relevant data to be mined.

 during the mining step, domain knowledge allows the specification of constraints for guiding data mining algorithms by, e.g. narrowing the search space.

 during the interpretation step, domain knowledge helps experts to visualize and validate extracted units.

there exists a number of studies on the use of ontologies within the data mining step, e.g.  <cit> , and the interpretation step e.g.  <cit> . only a few studies  has focused on the first step, namely data preparation. this is the purpose of the present paper.

data preparation –or preprocessing– is aimed at improving the quality of the data, and consequently the efficiency of the kdd process. methods for data preparation involve operations of different types: data integration, data cleaning, data transformation and data reduction  <cit> . these operations are not exclusive since they may be combined. for example, data transformation can have an impact on data cleaning during normalisation of data. data integration can have an impact on data cleaning as well, when inconsistencies are detected and corrected, or when missing values are filled. still regarding data integration, the use of ontologies has been theoretically and practically studied in life sciences  <cit> . in this way, we have defined and used an ontology for integrating data on genetic variants  <cit> . perez-rey et al. have developed ontodataclean, an ontology-based tool aimed at solving inconsistencies, missing and wrong values in datasets  <cit> . data transformation operation produce formatted data, i.e. normalised and smoothed data, ready for being processed by data mining algorithms. euler and sholz propose a special ontology related to the transformation process  <cit> . this ontology provides facilities to manipulate data by using conceptualization of the transformation process.

the role of data reduction process is to reduce the description of data, e.g. lowering the number of dimensions within the data, without altering the integrity of the initial data set. strategies for data reduction include the followings.

– data cube aggregation produces data cubes for storing multidimensional aggregated data  for olap analysis  <cit> . for example, data on daily sales hold on millions of items and can be aggregated into monthly sales of some selected categories of items.

– dimension reduction leads to the encoding of data in a reduced format, with or without loss with respect to the initial data set. for example, principal component analysis can be used for dimensionality reduction that applies projections of initial data onto a space of a smaller dimension.

– data discretization techniques are used to reduce the number of values of an attribute and consequently facilitate interpretation of mining results. automatic discretization methods exist for continuous numerical attributes that recursively partition the attribute values according to a given scale. for example, the range of an attribute price can be divided by the means of histogram analysis into several intervals, which can in turn be iteratively aggregated into larger intervals. however, these methods do not apply for discrete or nominal attributes, when the attribute values of which are not ordered. the scale for an attribute has then to be manually defined by domain experts and possibly refined with the help of heuristic methods  <cit> .

– data selection aims at identifying appropriate subsets among the initial set of attributes. this operation can be performed with the help of heuristic methods based on tests of significance or entropy-based attribute evaluation measures such as the information gain  <cit> . data selection is one of the data reduction methods that is studied in this paper.

the use of domain knowledge in kdd process can be considered from two points of view. the first one uses knowledge about the kdd process itself, i.e. domain represented within ontologies are data transformation, data cleaning, or the whole kdd domain  <cit> . the second one uses knowledge related to the dataset domain  <cit> , e.g. pharmacogenomics. the work presented in this article follows the second view, and focuses on data preparation, and more precisely, on data selection. in addition it is made precise how available domain knowledge –contained in a knowledge base – can assist the domain expert in selecting relevant attributes or object subsets.

our case-study deals with genotype-phenotype relationships. finding relationships between genotype and phenotype is of primary interest in biological research. large scale clinical studies provide large mass of genomic and post-genomic data produced by high-throughput biotechnology devices . recent studies  <cit>  have shown that data mining methods can be used for extracting unexpected and hidden correlations between genotype and phenotype. however, these studies also illustrate the difficulty of achieving these analyses, mainly because of domain complexity and large volume of data to be analysed. keeping this in mind, we will illustrate here the benefits of using ontology for data selection within a kdd process, whose objective is to extract relationships between genomic variants and phenotype traits. the data sources explored in the experience described in this paper have two origins:  there are private datasets resulting from clinical investigations relative to familial hypercholesterolemia ,  there are public databases  partially integrated within snp-kb, a knowledge-base developed in our laboratory. an example of expected relationships that can be of interest, is in concern with modulator variants, i.e. any genomic variant  related to disease or disease symptom modulation. various levels of severity are for example observed in fh depending on allele versions of two genomic variants in the apoe gene   <cit> . modulator variants are of particular interest in pharmacogenomics since they are known to modulate the metabolism and effect of drugs  <cit> .

the next section on results presents an overview of the ontology-guided data selection method. three scenarios of data selection are described and illustrate the proposition and its advantages.

RESULTS
overview
an overview of the method is given in figure  <dig>  data relevant to the study are collected from various resources such as genomic variation databases, published pharmacogenomic studies and private datasets. various operations are applied to these data: cleaning, integration and transformation. these operations aimed first at participating in the instantiation of an existing kb, and second at producing the “initial dataset”. in this study, a dataset is defined as a relation between set of objects  and set of attributes . a mapping is then built between objects and attributes of this dataset, and instances of the kb. data selection results from the definition of a subset of instances in the kb, allowing the selection of corresponding objects and attributes with respect to the mapping. this process that takes as inputs the initial dataset and the kb, is controlled by the domain expert, and yields the “reduced dataset”. characteristics of the ontology such as subsumption relationships, properties and class descriptions, are used to guide the choice of meaningful instance subsets. these subsets are in turn used for data selection. data mining algorithms are then applied to the reduced dataset. in the three examples presented hereafter, two mining algorithms are used. the first algorithm is zart that extracts frequent itemsets  and frequent closed itemsets . the latter are special itemsets that cannot be extended in the dataset . the ratio fi / fci increases with the redundancy level of the itemsets. the second algorithm is cobweb, which carries on a clustering of data in an unsupervised way. actually, the results of the clustering are simply characterized by the number of obtained clusters.

articulation between data and knowledge
our method is based on a mapping between objects and attributes of the dataset, and instances of the kb. thus, formalized knowledge within the kb can be used for guiding data selection. figure  <dig> illustrates this mapping in the case of genomic variants assigned to concepts of the snp-kb such as conserved domain_variant, coding_variant, non_coding_variant, haplotype_member or tag_snp.

the efficiency of the interaction between data and knowledge is mainly based on the instantiation process in the kb with collected data. this process is dependent on data integration issues and has to be controlled by the domain expert, who has to choose the most accurate class corresponding to the considered data. in this way, the domain expert is in charge of instantiating the right classes in the knowledge base. in practice, information about the mapping is stored in the kb during the instantiation process by adding a property to the created instance. it can be noticed that depending on modelling choices, one object or one attribute can be mapped to more than one instance. three concrete scenarios for data selection are now described.

progressive selection of specific variants – guided by subsumption
the first scenario assumes that significant relationships between genotypes and phenotypes can be easily extracted from a reduced dataset, in which only coding variants or variants of conserved protein domains are considered. in our method, this kind of reduction results from the selection in the snp-kb of a subset of instances corresponding to most specific and adequate classes in the ontology, with respect to subsumption relationships. as illustrated in table  <dig>  a progressive selection of the most specific variant instances, successively belonging to variant class and coding_variant and conserved_domain_variant subclasses, leads to a decreasing number of attributes related to variants in the dataset: progressively  <dig>   <dig>  and  <dig> attributes. in practice, the guiding of instance selection is managed through a plug-in of protégé  <dig> adapted for this purpose .

the volume of data mining results progressively decreases as more reduced sets of variants are selected . this reduction can be read on the number of fi –from  <dig> to 304– and of clusters –from  <dig> to 56– making results easier to interpret.

being able to use subsumption relationships between ontology classes for guiding data selection is one main advantage resulting from the knowledge formalization effort, data integration and data cleaning preceding the snp-kb instantiation.

tag-snp based variant unification – guided by object properties
the examination of the data mining results obtained with the complete variant dataset reveals a high proportion of trivial and redundant association rules. this reflects the existence of variants belonging to the same haplotype. in simple words, ahaplotype designates a group of variants that segregate uniformly and can be replaced by a smaller group of variant, called “tag-snps”. replacing all members of a haplotype by corresponding tag-snp may lower the number of extracted redundant association rules.

rs_ <dig> := ishaplotypememberof

rs_ <dig> := ishaplotypememberof)

a knowledge base may include information about functional dependencies taking the form of object properties . since the snp-kb includes haplotype descriptions issued from the hapmap project  <cit>  and haploview software  <cit> , and includes ishaplotypememberof and istaggedby properties, then it is possible to distinguish between tag-snps and other haplotype members in the snp-kb. according to our method, reducing the dataset to tag-snps is based on the selection of a subset of variant instances of the tag_snp class. in the situation depicted in figure  <dig>  this implies in turn the removal of columns rs_ <dig>  rs_ <dig>  and rs_ <dig> in the dataset.

applied to the fh initial dataset, this strategy considerably reduces the number of attributes . the volume of extracted units to be interpreted is thus also considerably reduced, not only because of the lower number of attributes but also because of the reduced number of dependencies between selected attributes . one main advantage of guiding this selection process with domain ontology is to dynamically use the representation of functional dependencies between simple haplotype members and representative tag-snps in the snp-kb. the representation is dependent on the precision of haplotype construction and may evolve. automated updating of haplotype representation and instantiation in the snp-kb is under study.

patient selection – guided by class definition and classification
in contrast with the two previous scenarios dedicated to attribute selection, e.g. variant, this paragraph illustrates object selection, e.g. patient selection, leading to a reduction of the dataset as well. this third scenario illustrates the selection of instances based on the description of classes within so-pharm ontology. so-pharm encompasses and extends snp-ontology .

in the fh case study, groups of patients suspected to present specific genotype-phenotype profiles are defined. classes and properties of so-pharm allow to define four classes of patients: one already existing in so-pharm, and three others that are defined for the data selection.

patient 

patient_α ≡ patient ⊓ ∃ presentsgenotypeitem )

patient_β ≡ patient ⊓ ∃ presentsgenotypeitem )

⊓ ∃ presentsphenotypeitem )

patient_γ ≡ patient ⊓ ∃ presentsgenotypeitem )

⊓ ∃presentsphenotypeitem )

reasoning mechanisms as applied to instances classify patients according to their individual properties. this allows to detect and to select a set of objects sharing the same attributes, as a set of instances belonging to the same class. this selection may reduce the volume of data input for subsequent mining tasks, and allows the characterization and comparison of selected subgroups.

discussion
data selection is a crucial step in kdd process and any attention paid to selection makes more efficient the kdd process. indeed, the computational cost in space and time of data mining algorithms is exponential , and any reduction of the initial dataset has effect on the whole data mining process. in addition, the practical use of data mining algorithms is also often limited by size of datasets or machine capabilities. for example, the extraction of frequent itemsets from the fh dataset on a standard workstation with a pentium  <dig> ghz and 2mb of ram has to be limited to the calculation of the “most frequent” itemsets since the minimum support has to be set very high . data selection is an important operation participating to the preparation step of the kdd, allowing the data mining algorithm to handle large dataset. comparative tests show that data selection reduces quite always the volume of results and, in some cases, the redundancy within the extracted units. the efficiency of data selection is not so surprising and demonstrates, to a certain extent, some advantages of using ontology. more importantly, an actual positive feedback from the domain expert has been observed, who has enthusiastically piloted the data selection, being assisted by an ontology. the smaller size of the results has been a second cause of satisfaction for the domain expert, since results of the data mining tests have revealed non-standard results that may be of interest with respect to the domain knowledge.

ontology-guided data selection can be performed by taking advantage of subsumption relationships between ontology classes and by defining subsets of instances corresponding to the most specific classes. when association rules have been extracted from a reduced dataset, the subsumption relationships can be followed within the ontology, for generalizing the association rules. this bottom-up traversal of the ontology can be used, for example, to check whether an extracted association rule between a coding variant and a phenotypic trait can be extended to some non-coding variants. this kind of association may be observed when intron splice sites are affected as discussed in  <cit> .

CONCLUSIONS
this paper illustrates how domain knowledge captured in bio-ontologies facilitates the kdd process. an approach for data selection has been proposed that takes good advantage of time and effort spent for the kb construction.

three proposed scenarios of data selection can be combined in order to define optimized kdd strategies fulfilling biomedical objectives. for that purpose, additional scenarios can be planned such as object unification, i.e. grouping together patients from the same family and retaining a unique representative for the family, thus reducing the number of objects to be manipulated. the selection process depends on instance properties , and accordingly on data and instantiation quality. when an instance is missing or presents a fault, the selection will be erroneous or impossible. in this way, the available knowledge on haplotypes could also be used for completing missing values about observed alleles of each member of a haplotype.

challenging future work consists in automatically formalizing the results of the kdd process within a knowledge representation language, for enriching both the ontology and the kb. such a capability allows to iteratively run the kdd process, using more complete domain knowledge after each kdd iteration.

