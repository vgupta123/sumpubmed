BACKGROUND
an important challenge in serial analysis of gene expression   <cit>  analysis is the decision whether a gene is differentially expressed between two classes, for example tumoral vs. normal classes. in statistical terms, this essential step is to test the null hypothesis h0: "gene has no differential expression between the two probed classes". a much more usual approach is to assign an index  that measures the confidence/significance of the hypothesis and let the biologists themselves to establish a cutoff of what they call significant.

this necessity arises because counting sequenced sage tags is a process prone to random and systematic errors that affect gene expression abundance estimates. systematic errors may come from various sources such as gc content bias  <cit> , sequencing errors  <cit>  as well as the possibility of non unique tags. this kind of error can be detected/corrected using some bioinformatics procedures such as quality control of automatic sequencing pipe-line  <cit> , or statistical estimation procedures such as "denoising"  <cit> . random errors are due to the inherent stochastic characteristic of sage data acquisition: sampling from automatic sequencing. like colored balls in an urn, sampling and counting sage tags from a library is commonly modeled by a bernoulli process relying on an infinite population sampling approximation.

if an expressed sequence tag  library is non-normalized, its counting data, also known as "digital-northern", reflects the abundance of genes. likewise, the massively parallel signature sequencing   <cit>  technique counts tags to infer the transcriptome, but using a completely different strategy from traditional dna sequencing methods, that allows augmented high-throughput capability. therefore, all the results discussed here are readily applicable to "digital-northern" or mpss context since, from a mathematical viewpoint, all represent the same bioinformatics problem: counting transcripts .

nowadays, the variability in sage abundance data is modeled only as due to sampling from sequencing, since almost all statistical procedures are performed after aggregation of observations from various libraries of the same class, creating a "pseudo-library". see  <cit>  for good reviews on statistical techniques used in sage analysis. this extensively used trick tacitly ignores the within-class variability, i.e., the biological variability among individuals within a class , and could lead to overconfident conclusions.

RESULTS
here we propose a bayesian model of mixtures to account for within-class variability as a generalization of the beta-binomial model  <cit> . we also show that the usual "pseudo-library" construction is a particular case of our mixture model. finally, we propose the use of the bayes error rate to intuitively rank the differential expression hypothesis under a bayesian framework, avoiding several technicalities and difficulties such as: typei and typeii error analysis, bonferroni-like multiple testing correction, asymptotic results evocation, imposition of a test statistic and null probability density function , and so on.

statistical model
the counting process from automatic sequencing of one single i-th library is often modeled as a bernoulli process and a fixed unknown tag abundance πi is implicitly assumed. the pdf of the random variable of interest, "expression abundance" π ∈  among all n libraries is unknown, thus each library could be regarded as being created by a realization of π. these features lead naturally to mixture models  <cit> :



where: f is the unknown pdf of the abundance among same-class libraries parameterized by a vector θ, x =  is the vector of counts in all n libraries of same class, m =  is the vector of library sizes and l is the likelihood of each i-th observation.

the common procedure of merging all observations from libraries of the same class, constructing a "pseudo-library" before statistical inference, is recognized as a particular case of this mixture model: just assume that all libraries have strictly the same abundance, with no biological variability. mathematically, this is a function with infinite probability density over one single abundance value π = θ and zero over every other π ≠ θ, or a dirac's delta function. using f as a dirac's delta function constrained to , turn eq. <dig> into the familiar and commonly used binomial distribution .

we believe that dirac's delta is a naive description of real-life sage libraries. the beta distribution is an alternative with non-zero within-class variance to account for intuitively expected biological differences among them. using f as a beta in eq. <dig>  yields the so-called beta-binomial model .

given the parameter vector θ that describes the random variable π of some fixed gene g, we must decide if there is a difference between a and b classes . we propose to consider genes as being differentially expressed based on non-superposition of the predictive beta pdfs of both a and b classes. by "predictive" we mean that we use the a posteriori mode in the beta pdfs. the "non-superposition" intuitive feature is mathematically written as the bayes error rate e  <cit> :



where f is the beta pdf and "hat" over the parameters indicate the values that lead to an a posteriori pdf maximum. the a posteriori distribution is obtained as usual from bayesian statistical theory .

intuitively, if the pdfs are "far apart", the gene probably has reproducible differential expression between classes. in this case, rarely could one misclassify class a as b and vice-versa. figure  <dig> gives some insight about this fact. using our proposed approach, the "far apart" notion means a small bayes error rate e. for adepts of the frequentist statistics, this evidence measure could resemble a typei and typeii errors sum, however it is just an illustrative analogy.

as in any significance test method, the experimenter must define what is a high significance e value. this cutoff should be guided by external and independent confirmatory assays. to avoid crude decision boundaries, one could rank their significance results but there is no way to avoid some arbitrariness in any kind of statistical test.

in the classical frequentist statistics framework, it is common to call a result as significant if it presents a p-value ≤  <dig>  in a t-like-test, hoping that this could control the error at this level. however, due to technical difficulties such as lack of sensitivity of posterior confirmatory methods or high absolute expression  necessity, this apparent statistically sound results could be not useful in a pragmatic sense. that is why we prefer to rank the differential expression results and allow researchers to establish a cutoff compatible with their subsequent application for the selected genes, rather than split them based in assumption-derived error-rate cutoffs. people familiar with the frequentist statistics framework could miss multiple testing considerations, typei/typeii error studies, and so on. however, in the bayesian framework, several of these concerns are meaningless since we work with parameters space and not with sample space. the bayesians avoid statements about "data that could be observed but was not" and work only with available information , extracting all possible information from data effectively observed.

for those genes classified as differentially expressed, one should aggregate intuitive information adding "error-bars" to expression ratios. recently we have developed a method to add credibility intervals to gene expression ratio  <cit> , which could improve posterior analyses such as clustering  <cit>  or comparison with microarray data.

comparison with available methods using publically available data
to show the model is usefulness, we applied it to a tumor vs. normal two-classes comparison problem. we chose a subset of brain tumor sage data from the cancer genome anatomy project's sage genie public database web-site  <cit> . the sage genie performs several bioinformatics protocols to assure the quality of its data with systematic errors cleaning/correction  <cit> .

we used all  <dig> available libraries in sage genie until jan/ <dig> from astrocytoma grade iii tumors and almost all  normal brain libraries .

we want to stress  <dig> typical and important cases:  when our measure agreed with other evidence measures accepting null hypothesis h <dig>  i.e., there is no evidence of differential behavior between tumor and normal classes;  when our method agreed with others rejecting h <dig>  i.e., there is evidence of differential expression; and  when our method showed evidence in favor but other evidence measures showed evidence against the h <dig>  case  is the main motivation of our method since it reveals situations that researchers may call a gene differentially expressed and, in fact, it could be not so significant if biological replicates are taken into account. the other evidence measures used were: the audic-claverie bayesian evidence  <cit> , the classical fisher exact test p-value, and the classical χ <dig> p-value, all obtained using the ideg <dig> web-interface  <cit>  .

a case  prototype is the tttcaataga tag with xt =  and xn = . the audic-claverie, fisher and χ <dig> methods yield p-values of  <dig> ,  <dig> ,  <dig> , respectively, indicating low evidence against h <dig> for all mystical significance level cutoffs ≤  <dig> , ≤  <dig>  or ≤  <dig> . the bayes error rate evidence is e =  <dig> , an intuitively unacceptable superposition level between the normal and tumoral predictive beta pdfs, showing that there is no separable behavior between classes. figure 2a shows an obvious superposition between pdf and observations of this two classes.

a case  prototype is the aaaagaaact tag with xt =  and xn = . all p-values are  <dig>  , significant at any cutoff level. our evidence is e =  <dig> , showing safely that this gene behaves differentially between normal brain and astrocytoma grade iii patients. figure 2b shows that two betas are apart from each other and, even observing clear within-class variability, the expression is different.

a case  prototype is the ttggagatct tag with xt =  and xn = . all p-values are  <dig>  , indicating significant difference between classes. on the other hand, our evidence e =  <dig>  indicates high superposition between tumor and normal classes. figure 2c shows that within-class variability for tumor class is not negligible. it is obvious that individual libraries confound their results with normal brain libraries, and the betas have a relatively high intersection. using a common "pseudo-library" approach, one is lead to call this gene as a strong discriminator between classes. we believe that this is a suspect conclusion.

there are several other obvious case  examples, such as tag tacagtatgt in figure 2d, that received p-values <  <dig>  from all other methods, and they are the main concern of our method since they may lead to waste of resources in clinical validation efforts of genes that, by sage itself, could be left behind in favor of other promising genes. all tag results are available as additional file and graphics for all tags are at the supplemental web-site  <cit> .

one could think about a case  when considering within-class variability leads one to h <dig> rejection, but considering "pseudo-libraries" leads to h <dig> acceptance. this seems to be inconsistent since one expects that, once h <dig> is accepted in a simplified model, it should also be accepted in the complete model. in fact, we do not observe such a situation, except by tags with p-values or bayes error rate very close to arbitrarily defined cutoff values. we believe that these occurrences are just "edge effect" manifestations.

discussion
in order to assure that we are dealing with a fundamental question in sage analysis, we show more insights analyzing the method's robustness using the same data but excluding "small" libraries. also, we draw some parallels between our proposed method and the only available published solution for dealing with within-class variability, a t-test approximation  <cit> .

we used our method with all available libraries but some of them are smaller than  <dig>  tags . in the sage community, libraries smaller than this arbitrary limit are considered "small". several researchers claim that these are non-representative and should be excluded from analysis. we observed several case  tag examples which remain as case  if we use libraries with size >  <dig>  and >  <dig>  . figure  <dig> shows a tag example analyzed in these tree setups and it is clear that inclusion of "small" libraries gave pretty much the same result, indicating robustness of our method against small class size variations and against "small" sized libraries. moreover, these libraries are not always outliers from biological sampling but seem to be samples like any other. these results suggest that one can use the "small" libraries, jointly with non-"small" ones, because biological variability seems to be greater than binomial sampling variability.

obviously, we are not recommending to use only "small" libraries in sage analysis, but suggesting that our method is relatively robust. for low expression genes, the binomial sampling variability should become more relevant as the library size decreases. also, the results obtained using two/three libraries could be very different from using just one. these proprieties could be tag dependent since some tags could be much noisier than others for biological reasons. some "denoising" procedure could be used before application of our method  <cit> . therefore, our findings should be carefully interpreted.

to prove that the incoherence of using "pseudo-libraries" methods is not a prerogative of tags showing small fold-changes, we analyzed another three very illustrative examples: atggcaacag, ggatgtgaaa, and gtatgggccc; which are case  tags. these tags present high fold changes:  <dig> ,  <dig>  and  <dig>  fold-change respectively, augmented in pooled tumor libraries. using the well-known fisher exact test, χ <dig> classical test and the audic-claverie's method, we get  <dig>   for all p-values of the no differential expression null hypothesis. using the conceptually different bayesian p-value implemented at sage genie  <cit>  we obtain  <dig> ,  <dig>  and  <dig>  respectively for posterior probabilities of fold-changes greater than 4-fold. finally, using our own proposed measure, applied to the pool, we get e =  <dig>  meaning no superposition between the two classes pdfs. all these results indicate strong significance in differential expression of these tags.

however, if we consider within-class variability, the test proposed by baggerly et al.  <cit>  yields  <dig> ,  <dig>  and  <dig>  respectively for t-test p-values, and our method yields bayes error rates of  <dig> ,  <dig> ,  <dig>  respectively; indicating not so significant evidence in favor of the differentially expressed hypothesis. a closer look at the graphics of these tags induces one to believe that there is no reproducible differential expression because several observations of tumor and normal are superimposed .

since we show clearly that methods that use "pseudo-library" aggregation could be incoherent in some cases, a natural question is how our proposed method performs compared to the only published solution that accounts for within-class variability, the baggerly et al.  <cit> t-test approximation. without knowing the true state of all tags, it is impossible to carry out a serious benchmark. since the interpretation of evidence measures is very different, the performance could be subjected to an arbitrary cutoff selection for each method. figure  <dig> shows a scatter-plot of evidence measures obtained for each of the two methods.

it is clear from this graphic that there are many more tags considered as differentially expressed using our method than the t-test approximation, considering e ≤  <dig>  and p-value ≤  <dig> . there are also some tags selected by t-test and ignored by ours. it is impossible to know which method perform better without the true unknown status of those tags. looking at individual libraries results, constructed as depicted in figure  <dig> for example, could help in this analysis but this is a subjective procedure.

it is important to bear in mind that a difficulty is hidden in the beta modeling imposed in the very first beginning. if beta is not a good model for an unknown biological behavior, then some apparent inconsistency could appear in both baggerly et al.  <cit>  and our approaches. however, our general mixture model allows another propositions. other simplex constrained pdfs, different from beta, exist but the tractability is much more difficult  <cit> . we believe that to build a fully non-parametric approach to this problem is a very hard issue, but should be considered as a future challenge.

CONCLUSIONS
until now, almost all statistical methods for sage data analysis tacitly ignore the within-class variability. to our knowledge, the firsts to formally address this issue was baggerly et al.  <cit>  who introduced the beta-binomial model as the correct way to model the probability of counting tags instead of binomial models. they also proposed a t-like statistics, outlined a possible hypothesis test using the classical frequentist statistics framework and evoked some asymptotic results for t pdf justification.

in this work we presented the bayesian alternative for this problem and defined a theoretical model that views baggerly's beta-binomial approach or even the common binomial approach as particular cases of mixture models. other models are possible modifying the mixing distribution, such as beta-poisson  <cit> , or using other simplex constrained pdf  <cit>  to model expression abundance. at last, but not at least, we proposed a method for ranking differentially expressed genes between two classes using the bayes error rate as an intuitive measure of separation between the classes pdfs, avoiding statistical test formalism and its conceptual/practical difficulties.

we show that there are cases in which approaches that ignore within-class variability will lead to high significance in differences between tumor and normal classes, but looking carefully at individual observations jointly, one should not attribute such high significance to them since abundance probability density functions have considerable superposition.

in conclusion, we recommend that within-class variability must be taken into account in any statistical analysis of sage data if replicates are available. we suggest that biological replication should be considered in planning new sage experiments.

