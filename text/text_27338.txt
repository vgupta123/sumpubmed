BACKGROUND
tumor classification is performed on microarray data collected by dna microarray experiments from tissue and cell samples  <cit> . the wealth of this kind of data in different stages of cell cycles helps to explore gene interactions and to discover gene functions. moreover, obtaining genome-wide expression data from tumor tissues gives insight into the gene expression variation of various tumor types, thus providing clues for tumor classification of individual samples. the output of microarray experiment is summarized as an n × p data matrix, where n is the number of tissue or cell samples; p is the number of genes. here p is always much larger than n, which hurts generalization performance of most classification methods. to overcome this problem, dimension reduction methods are applied to reduce the dimensionality from p to q with q ≪ p.

dimension reduction usually consists of two types of methods, feature selection and feature extraction  <cit> . feature selection chooses a subset from original features according to classification performance, the optimal subset should contain relevant but non redundant features. feature selection can help to improve generalization performance and speed of classifiers. there have been a great deal of work in machine learning and related areas to address this issue  <cit> . but in most practical cases, relevant features are not known beforehand. finding out which features to be used is a hard work. at the same time, feature selection will lose the relevant information among features, while feature extraction is good at handling interactions among features.

feature extraction projects the whole data into a low dimensional space and constructs the new dimensions  by analyzing the statistical relationship hidden in the data set. principle components analysis  is one of the frequently used methods for feature extraction of microarray data. it is unsupervised, since it need not the label information of the data sets. partial least squares  is one of the widely used supervised feature extraction methods for analysis of gene expression microarray data  <cit> , it represents the data in a low dimensional space through linear transformation. although feature extraction methods produce independent features, but usually, a large number of features are extracted to represent the original data. as we known, the extracted features also contain noise or irrelevant information. choosing an appropriate set of features is critical. some researcher considered that the initial several components of pls contain more information than the others, but it is hard to decide how many tail components are trivial for discrimination. some authors proposed to fixed the number of components from three to five  <cit> ; some proposed to determine the size of the space by classification performance of cross-validation  <cit> . however each one has its own weakness. fixing at an arbitrary dimensional size is not applicable to all data sets, and the cross-validation method is often obstructed by its high computation. an efficient and effective model selection method for pls is demanded. furthermore, we consider not all the initial components are important for classification, subsets should be selected for classification.

here, we propose and demonstrate the importance of feature selection after feature extraction in the tumor classification problems. we have performed experiments by using pca  <cit>  and pls  <cit>  as feature extraction methods separately. in this paper, we will perform a systematic study on both pca and pls methods, which will be combined with the feature selection methods  to get more robust and efficient dimensional space, and then the constructed data from the original data is used with support vector machine  and k nearest neighbor  for classification. by applying the systematic study on the analysis of gene microarray data, we try to study whether feature selection selects proper components for pca and pls dimension reduction and whether only the top components are nontrivial for classification.

RESULTS
results by using svm
in order to demonstrate the importance of feature selection in dimension reduction, we have performed the following series experiments by using support vector machine  as the classifier:

 <dig>  svm is a baseline method, all the genes without any selection and extraction are input into svm for classification.

 <dig>  pcasvm uses pca as feature extraction methods, all the newly extracted components are input into svm.

 <dig>  plssvm uses pls as feature extraction methods, all the newly extracted components are input into svm.

 <dig>  ppsvm uses pca+pls as feature extraction methods, all the newly extracted components are input into svm.

 <dig>  gapcasvm uses pca as feature extraction methods to extract new components from original gene set and ga as feature selection methods to select feature subset from the newly extracted components, the selected subset is input into svm.

 <dig>  gaplssvm uses pls as feature extraction methods to extract new components from original gene set and ga as feature selection methods to select feature subset from the newly extracted components, the selected subset is input into svm.

 <dig>  gappsvm uses pca+pls as feature extraction methods to extract new components from original gene set and ga as feature selection methods to select feature subset from the newly extracted components, the selected subset is input into svm.

since there are parameters for svm, we try to reduce its effect to our comparison and use four pairs of different parameters for svm, they are c =  <dig>  σ =  <dig> , c =  <dig>  σ =  <dig>  c =  <dig>  σ =  <dig> , and c =  <dig>  σ =  <dig>  it is noted that different data sets including the extracted data sets and selected data sets need different optimal parameters for different methods, we do not choose the optimal parameters, because 1) this is unreachable, finding the optimal parameters is an np hard problem; 2) we do not exhibit the top performance of one special method on one single data set, but we want to show the effect of our proposed framework.

prediction performance
the average error rates and the corresponding standard deviation values are shown in table  <dig>  where the standard deviation values are produced from our  <dig> times repeated experiments. from table  <dig>  we can find that:

• results of all the classification methods with feature selection and extraction like plssvm, gaplssvm, pcasvm, gapcasvm, gappsvm are better than that of svm without any dimension reduction on average. only on the lung data set, when svm uses parameters of c =  <dig>  σ =  <dig> , results of ppsvm are worse than those of svm.

• results of classification methods with feature selection like gaplssvm, gapcasvm and gappsvm are better than those of the corresponding feature extraction methods without feature selection like plssvm, pcasvm and ppsvm on average. only on few cases, i.e. when c =  <dig>  σ =  <dig> is for svm, results of gapcasvm are slightly worse than those of pcasvm on the colon data set.

• results of gaplssvm are better than those of pcasvm and gapcasvm, even the corresponding results of ppsvm and gappsvm on average. only on the cns data set out of four data sets, gapcasvm obtains the best results than other methods do.

• results of ppsvm and gappsvm which combine pca and pls as feature extraction methods are not the best, just equal with those of pcasvm and gapcasvm.

number of selected features
we also show the number of features selected by each method with their corresponding standard deviation in table  <dig>  where the standard deviation values are produced from our  <dig> times repeated experiments. the values for pcasvm means the ratios of the number of top principle components to that of extracted components, those of plssvm and ppsvm have the same meaning. the values for gapcasvm means the ratios of the number of selected components used in svm to that of extracted components, and those of gaplssvm and gappsvm have the same meaning. from table  <dig>  we can see that if we use the top components, about 60–80% components are selected into learning machines, while if we use feature selection to select useful components, about 30% components are selected on average. only on the lung data set, the selected components by different methods are 70%–80% of extracted components.

distribution of selected features
fig.  <dig> shows the comparison of distributions of components selected by ga in two cases of gapcasvm and gaplssvm, and fig.  <dig> shows that of gappsvm. difference between fig.  <dig> and fig.  <dig> is that in fig.  <dig>  pca and pls are used as feature extraction individually, while in fig.  <dig>  pca is combined with pls as feature extraction methods.

from fig.  <dig> and fig.  <dig>  we can find that:

• when only pls is used for feature extraction, the top components are a little more than that of others in the selected components, but the others are also important.

• when only pca is used, the top components is less than others in the selected features, and the tail components are more important than others.

• when both pca and pls are used as feature extraction methods, they are nearly equal in the selected components, and the top components of pls is a little more than others.

results by using knn
in order to show the importance of feature selection, we have also performed the following series experiments on the knn learning machine to reduce the bias caused by learning machines.

 <dig>  knn is a baseline method, all the genes without any selection and extraction are input into knn for classification.

 <dig>  pcaknn uses pca as feature extraction methods, all the newly extracted components are input into knn.

 <dig>  plsknn uses pls as feature extraction methods, all the newly extracted components are input into knn.

 <dig>  ppknn uses pca+pls as feature extraction methods, all the newly extracted components are input into knn.

 <dig>  gapcaknn uses pca as feature extraction methods to extract new components from original gene set and ga as feature selection methods to select feature subset from the newly extracted components, the selected subset is input into knn.

 <dig>  gaplsknn uses pls as feature extraction methods to extract new components from original gene set and ga as feature selection methods to select feature subset from the newly extracted components, the selected subset is input into knn.

 <dig>  gappknn uses pca+pls as feature extraction methods to extract new components from original gene set and ga as feature selection methods to select feature subset from the newly extracted components, the selected subset is input into knn.

since there are parameters for knn, we try to reduce its effect to our comparison and use three parameters for knn, they are k =  <dig>  k =  <dig> and k =  <dig> 

it is noted that different data sets need different optimal parameters for different methods, we do not choose the optimal parameters, because we do not exhibit the top performance of one special method on one single data set, but we want to show the effect of our proposed framework.

prediction performance
the average error rates and the corresponding standard deviation values are shown in table  <dig>  from which we can find the similar observations:

• results of all the classification methods with feature selection and extraction like plsknn, gaplsknn, pcaknn, gapcaknn, gappknn are better than that of knn without any other dimension reduction on average and on each cases.

• results of classification methods with feature selection like gaplsknn, gapcaknn and gappknn are better than those of the corresponding feature extraction methods without feature selection like plsknn, pcaknn and ppknn on average and each cases.

• different from results by using svm, results of gappknn are better than those of pcaknn and gapcaknn, even the corresponding results of plsknn and gaplsknn on average. only on the lung data set out of four data sets, gaplsknn obtains the best results than other methods do.

• compared with results by using svm, on the cns, colon and leukemia data sets, results by using knn are better, while on the lung data set, results by knn are worse. but we can not compare learning machines by these results because we did not optimize the parameters.

number of selected features
we also show the number of features selected by each method in table  <dig>  where the values for pcaknn means the ratios of the number of top principle components to that of extracted components, those of plsknn and ppknn have the same meaning. the values for gapcaknn means the ratios of the number of selected components used in knn to that of extracted components, and those of gaplsknn and gappknn have the same meaning.

from table  <dig>  we can see that if we use the top components as in pcaknn, plsknn and ppknn, about 60–80% components are selected into learning machines, while if we use feature selection to select useful components as in gapcaknn, gaplsknn and gappknn, about 30% components are selected on average. only on the lung data set, the selected by different methods are 70–80% of extracted components.

distribution of selected features
fig.  <dig> shows the comparison of distributions of components selected by ga in two cases of gapcaknn and gaplsknn, and fig.  <dig> shows that of gappknn. difference between fig.  <dig> and fig.  <dig> is that in fig.  <dig>  pca and pls are used as feature extraction individually, while in fig.  <dig>  pca is combined with pls as feature extraction methods.

from fig.  <dig> and fig.  <dig>  we can find the similar observations as below:

• when only pls is used for feature extraction, the top components are more than that of others in the selected components, but the others are also selected, the top, the more.

• when only pca is used, the top components is less than others in the selected features, and the tail components are more important than others.

• when both pca and pls are used as feature extraction methods, they are nearly equal in the selected components, and the top components of pls are a little more than others.

discussion
the results are interesting, beyond our imagination, but they are reasonable.

from the experimental results, we know not the top components are important. the reason can be found in the subsection of feature extraction. for pca, components are extracted by maximizing the variance of a linear combination of the original genes, , but not maximizing the discriminative power for classifiers like support vector machine  and k nearest neighbor . therefore, the top component of pca is not the top one with high discriminative power of classifiers. for pls, components are extracted by maximizing the covariance between the response variable y and the original genes x, wq=arg⁡max⁡wtw=1). therefore, the top component of pls is more important than the others for classifiers. furthermore, the top components of pca are not the top feature subset with high discriminative power for classifiers, while the top ones of pls are the top feature subset with high discriminative power, but the tail ones have also discriminative power, they are selected too. so, we should not only choose the top components, but employ feature selection methods to select a feature subset from the extracted components for classifiers.

feature selection is performed by genetic algorithm , which shows great power to select feature subsets for classifiers, this can be seen from the experimental results. here genetic algorithm based feature selection is a so called wrapper model, which uses the classifier to measure the discriminative power of feature subsets from the extracted components. this method has been proved the best one feature selection method  <cit> . while this wrapper method is time consuming, nowadays, the scale of data sets is increasing rapidly, so efficient feature selection methods need be developed.

partial least squares is superior to principle component analysis as feature extraction methods. the reason is simple, pls extracts components by maximizing the covariance between the response variable y and the original genes x, which considers using the labels y and can be viewed as a supervised method. while pca extracts components by maximizing the variance of a linear combination of the original genes, which does not consider using the label y and can be viewed as an unsupervised method. here, we try to improve the classification accuracy of svm, this is a supervised task, so pls a supervised method is superior to pca, an unsupervised method.

features selected by different classifiers has minor difference, and results of prediction accuracy are also different. feature selection has done more effect on knn than that on svm. because knn is more sensitive on high dimensional data sets than svm. but, they all benefit from feature selection.

CONCLUSIONS
we have investigated a systematic feature reduction framework by combing feature extraction with feature selection. to evaluate the proposed framework, we used four typical data sets. in each case, we used principle component analysis  and partial least squares  for feature extraction, ga as feature selection, support vector machine  and k nearest neighbor  for classification. our experimental results illustrate that the proposed method improves the performance on the gene expression microarray data in accuracy. further study of our experiments indicates that not all the top components of pca and pls are useful for classification, the tail component also contain discriminative information. therefore, it is necessary to combine feature selection with feature extraction and replace the traditional feature extraction step as a new preprocessing step for analyzing high dimensional problems.

