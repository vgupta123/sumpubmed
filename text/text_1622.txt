BACKGROUND
chunking is a natural language processing technique that splits text into groups of words that constitute a grammatical unit, e.g., a noun phrase or a verb phrase. it is an important processing step in systems that try to automatically extract information from text. most chunkers are based on machine learning methods and require a text corpus annotated with chunks for training the system. the creation of a gold standard corpus  is tedious and expensive: annotation guidelines have to be established, domain experts must be trained, the annotation process is time-consuming, and annotation disagreements have to be resolved. as a consequence, gscs in the biomedical domain are generally small and focus on specific subdomains, which limit their usefulness.

in this study we investigate an alternative, automatic approach to create an annotated corpus. we have shown before that a system combining the outputs of various chunkers performs better than each of the individual chunkers. here we postulate that the annotations of such a combined system on a given corpus can be taken as a reference standard, establishing a "silver standard corpus" .

to test the practical value of this approach, we explore two use scenarios of such an ssc. in the first scenario, a chunker has to be trained for a biomedical subdomain for which a gsc is not available. rather than creating a new gsc, we generate an ssc for the new domain and train the chunker on the ssc. in the second scenario, a gsc from the domain of interest is available but its size is small and a chunker trained on it gives suboptimal performance. rather than expanding the gsc, we supplement the gsc with an ssc from the same domain and train the chunker on the combined gsc and ssc to improve chunker performance.

related work
during the past decade, much research has been devoted to systems that combine different classifiers, also called multiple classifier systems or ensemble-based systems  <cit> . the general idea is that the combined wisdom of multiple classifiers reduces the risk of errors, and indeed it has been shown many times that a combined system performs better than the best individual classifier. multiple classifier systems have been applied in many domains, including biomedical text mining and information extraction. for instance, smith et al.  <cit>  combined the results of  <dig> systems for gene mention recognition, and found that the combined system outperformed the best individual system by  <dig>  percentage points in terms of f-score. kim et al.  <cit>  combined eight systems for event extraction and showed that the performance of the combined system increased by  <dig> percentage points as compared to the best individual system. we previously combined six publicly available text chunkers using a simple voting approach  <cit> . the f-score of the combined system improved by  <dig>  percentage points for noun-phrase recognition and  <dig>  percentage point for verb-phrase recognition as compared to the best single chunker.

the notion that a combination of systems can be used to create a "silver standard" corpus has been explored in the calbc  project  <cit> . through calbc, the natural-language processing community has been invited to annotate a very large biomedical corpus with a variety of named-entity recognition systems. the combined annotations of multiple systems may provide a valuable resource for system development and evaluation, and the automatically generated creation of an ssc would allow corpora of unprecedented size. in a very recent study, chowdhury and lavelli compared a gene recognition system trained on an initial version of the calbc ssc against the system trained on the biocreative gsc  <cit> . the system trained on the ssc performed considerably worse than when trained on the gsc, but the authors propose several ways to automatically improve the quality of the ssc and are of the opinion that, in the absence of a gsc, a system trained on the ssc could be useful in the semi-automatic construction of a gsc.

methods
chunking systems
to generate a silver standard, we used five well-known and publicly available chunkers: gate chunker  <dig>   <cit> , lingpipe  <dig>   <cit> , metamap 2008v <dig>  <cit> , opennlp  <dig>   <cit> , and yamcha  <dig>   <cit> . three of these chunkers are trainable , the other two do not have a training option. sentence splitting, tokenization, and part-of-speech tagging were included in our chunking pipeline, either as integral part of the chunkers  or as separate components . we used the gold-standard sentence, token, and part-of-speech annotations for training, but did not use this information in creating the ssc or evaluating the trained models: the input of the annotation pipeline consisted of plain abstracts, the output were chunking annotations. all chunkers annotate noun phrases and verb phrases, except for gate which only generates noun phrases. more information on characteristics and performance of these chunkers can be found in our previous comparative study of chunkers  <cit> , which also included genia tagger. since genia tagger comes with a fixed pre-trained model based on the corpora that we use in this study, it could bias the results of our experiments and was not included. all chunkers were used with default parameter settings.

corpora
there are only a few publicly available corpora in the biomedical domain that incorporate chunk annotations. we used the genia treebank corpus  <cit>  and the pennbioie corpus  <cit> .

the genia corpus  <cit>  has been developed at the university of tokyo. the  <dig>  version of the corpus was released in  <dig> and consists of  <dig>  medline abstracts selected from a query using the mesh terms "human", "blood cells", and "transcription factors". the corpus has been annotated with various levels of linguistic and semantic information, such as sentence splitting, tokenization, part-of-speech tagging, chunking annotation, and term-event information. for chunker training, we selected a subset of  <dig> abstracts that constituted a previous version of the genia corpus  <cit> .

the pennbioie treebank corpus  <cit>  has been developed at the university of pennsylvania. the  <dig>  version of the corpus was released in  <dig> and includes the cyp and oncology corpora of the linguistic data consortium. the cyp corpus consists of  <dig> medline abstracts on the inhibition of cytochrome p <dig> enzymes. the oncology corpus consists of  <dig> medline abstracts on cancer and molecular genetics. the corpus has been tokenized and annotated with paragraph, sentence, part-of-speech tagging, chunking annotation, and biomedical named-entity types.

creation of the silver standard
we used a simple voting scheme to generate silver standard annotations from the annotations produced by the different chunkers. for each phrase identified by a chunker, the number of chunkers that gave exactly matching annotations was counted. if the count was larger than or equal to a preset voting threshold, the phrase was considered a silver standard annotation, otherwise it was not. in all our experiments, we used a voting threshold of three out of five chunkers for noun phrases, and a threshold of two out of four for verb phrases . these thresholds gave uniformly the best results in terms of f-score when the silver standard annotations of the training data were evaluated against the gold standard. the unstructured information management architecture  framework  <cit>  was used to integrate all chunking systems and combine their result.

silver standard as alternative for gold standard
to test whether an ssc could serve as a substitute for a gsc, we compared the performance of chunkers trained on silver standard annotations of the abstracts in the pennbioie corpus with the performance of the chunkers trained on the gold standard annotations of the same corpus. to create the ssc, the trainable chunkers  were trained on the gold standard annotations of  <dig> abstracts of the genia corpus. the chunkers then annotated the pennbioie corpus and the annotations of all chunkers were combined to yield the silver standard. subsequently, lingpipe, opennlp, and yamcha were trained on the pennbioie ssc and on the pennbioie gsc, using 10-fold cross-validation. in the cross-validation procedure for the ssc, the annotations of the abstracts in each test fold were taken from the gsc. thus, the performance of chunkers trained on either ssc or gsc was always tested on the gsc.

silver standard as supplement of gold standard
to test whether an ssc would have additional value as a supplement for a given gsc, we compared the performance of chunkers trained on a subset of the genia gsc with the performance of the chunkers trained on the same subset supplemented with an ssc. specifically, subsets of  <dig>   <dig>   <dig>   <dig>  and  <dig> abstracts were selected from the initial genia training set of  <dig> abstracts, each subset being contained in the next larger one. lingpipe, opennlp, and yamcha were trained on the gold standard annotations of each subset and the total set, and tested on the  <dig>  genia abstracts that were not used for training. for each subset, the chunkers trained on that subset were subsequently used to create an ssc of the abstracts in the set of  <dig> abstracts that were not part of the subset, i.e., for the gsc subset of  <dig> abstracts, the ssc consisted of the remaining  <dig> abstracts; for the subset of  <dig> abstracts, the ssc consisted of  <dig> abstracts; etc. the gsc and corresponding ssc  were then used to train the chunkers. their performance was tested again on the  <dig>  genia abstracts not used for training. the above experiment was repeated  <dig> times, each time starting with a different randomly selected subset of  <dig> abstracts. the reported results are the averaged f-scores of the  <dig> experiments.

performance evaluation
the chunker and silver standard annotations were compared with the gold standard annotations by exact matching, similar to the procedure followed in conll- <dig>  <cit> . an annotation was counted as true positive if it was identical to the gold standard annotation, i.e., both annotations had the same start and end location in the corpus. a phrase annotated by the gold standard was counted as false negative if the system did not render it exactly; a phrase annotated by a system was counted as false positive if it did not exactly match the gold standard. performance of the chunkers and silver standard was evaluated in terms of precision, recall, and f-score.

to reduce the effect of insignificant differences between chunks, words from the stopwords list in pubmed  <cit>  and punctuation remarks were removed before matching if they appeared at the start or the end of a phrase. for instance, "np is..." is considered the same annotation as "the np is...", and "the medicine vp..." is considered the same as "the medicine often vp...".

RESULTS
silver standard as alternative for gold standard
all systems are tested on the pennbioie corpus.

silver standard as supplement of gold standard
discussion
we have investigated the use of an ssc as a substitute or a supplement of a gsc for training chunkers in the biomedical domain. the ssc as a substitute for a gsc corresponds with a use scenario in which a chunker created for one subdomain has to be adapted to another, where a gsc for the new domain is not available. we have shown that a system trained on an ssc for the new domain performs considerably better than if that system is trained on the gsc of another subdomain, and only slightly worse  than if the system was trained on a gsc for the new domain. in the second use scenario, we supplemented a  gsc with an ssc for the same domain as the gsc. the addition of the ssc always improved the chunker performance, particularly if the size of the initial gsc was small.

our results on the practical value of an ssc are different from those that were recently reported by chowdhury and lavelli  <cit> . they found a considerable drop in performance of a gene recognition system trained on the calbc ssc as compared to the system trained on the biocreative gsc, and also noticed that the system trained on a combination of ssc and gsc performed worse than on the gsc only. there may be several reasons for these differences. one is that the ssc that we used for training the chunkers was evaluated against the gsc of the same subdomain, whereas in the other study the domains from which the calbc ssc and the biocreative gsc are taken, are more divergent. another possible reason is that the quality of the calbc ssc is simply not good enough, which may be related to the difficulty of the calbc task. named entity recognition is generally considered more difficult than chunking, having to deal with increased complexities in boundary recognition, disambiguation, and spelling variation of entities. clearly, the better a silver standard will approach a gold standard for the domain of interest, the better the performance of systems trained on an ssc. it should be noted that the performance of the silver standard compared with the gold standard in our study is far from perfect: the pennbioie ssc has an f-score of  <dig> % for noun phrases and  <dig> % for verb phrases. performance figures of the calbc ssc against gscs for named-entity recognition are not yet available, but we presume that they will be much lower. however, despite the differences between an ssc and gsc, chunking systems trained on these corpora showed remarkably similar performances. it is still an open question how an ssc of lower  quality affects the performance of a system trained on the ssc.

we used a simple voting approach to create an ssc. more sophisticated voting methods exist, such as weighted voting  <cit>  or borda count  <cit> , but these methods require information about the confidence or rank of the chunks, information that is not available for the chunkers in this study. we also tested a combined system based on the output of the three trainable chunkers instead of all five chunkers. when trained on genia gsc and tested on pennbioie gsc, the f-score of the combined system dropped to  <dig> % for noun phrases and  <dig> % for verb phrases. since this performance is considerably lower than that of the combined system based on all chunkers, we did not further pursue the use of an ssc based on the three trainable chunkers only.

we used exact matching in performance assessment of the chunkers and creation of the ssc. by removing stopwords before matching we tried to remove "uninformative" words that should not play a role in determining whether phrases are the same, similar to other studies . our main consideration to remove stopwords was that chunking is usually an intermediate step in the information extraction pipeline, and whether an unimportant word  is detected or not, is unlikely to affect subsequent processing steps . stopword removal can be seen as a relaxation of the strict matching requirement. when systems trained on genia gsc were tested on pennbioie gsc but without removing stopwords, performances dropped by  <dig> - <dig>  percentage points for noun phrases and  <dig> - <dig>  percentage points for verb phrases. this shows that chunkers may considerably differ with the gold standard with respect to the annotation of stopwords. we did not want to further relax the matching criterion, e.g., by allowing partially matching boundaries, first because this would produce matches between phrases that differ in other than uninformative words , and second because it is not obvious how partially matching phrases should be combined in a single phrase for inclusion in the ssc.

since the creation of an ssc is automatic, its size can be very large. for different text-processing applications, increasing amounts of data for training classifiers have been shown to improve classifier performance  <cit> . use of an ssc may be beneficial in mitigating the "paucity-of-data" problem  <cit> .

the combination of systems always performed better than any of the individual systems, but performance increase of the combined system was larger when the individual systems were trained on genia or pennbioie gscs than when they were trained on the pennbioie ssc . a possible explanation for this phenomenon is that the ssc incorporates results from the chunkers that are subsequently trained on it. as a consequence, the diversity of the chunkers trained on the ssc may be less than those trained on the gscs. indeed, when we pairwise determined the f-score between two chunkers trained on genia gsc and pennbioie gsc, the average score was  <dig> % and  <dig> %, respectively, in comparison to  <dig> % for pennbioie ssc . this indicates better agreement between the chunkers  for the ssc. since annotation diversity is generally considered a key factor for the improvement seen by ensemble systems , it may be expected that the combined chunker system shows a smaller increase of performance when based on the ssc than on the gscs.

we showed that chunkers can obtain almost similar performances whether trained on an ssc or a gsc, but this does not mean that we can dispose of gscs altogether. obviously, to create the ssc we need trained chunkers, and thus a gsc for their initially training. we explored the use of a gsc from another, but related, domain than the domain of interest. alternatively, we supplemented a gsc with an ssc in the same domain of interest. using this approach, good results can be achieved with remarkably small-sized gscs. our experiments indicated that a gsc consisting of only  <dig> or  <dig> abstracts but expanded with an ssc yields similar performances as a gsc of  <dig> or  <dig> abstracts. practically, these results suggest that the time and effort spent in creating a gsc of sufficient size may be much reduced.

we have tested two use scenarios of an ssc in the field of text chunking, but the proposed approach is general and could be used in any field in which gscs are needed to train classifiers. further investigations will have to reveal how the quality of an ssc affects classifier performance and whether the use of sscs in other application areas is equally advantageous as their use in text chunking.

CONCLUSIONS
we have shown that an automatically created ssc can be a viable alternative for or a supplement to a gsc when training chunkers in a biomedical domain. a combined system only shows improvement if the ssc is used to supplement a gsc. our results suggest that the time and effort spent in creating a gsc of sufficient size may be much reduced. whether the approach is applicable to other systems in a natural-language processing pipeline has to be further investigated.

authors' contributions
nk co-developed the methodology, built the software infrastructure, carried out the experiments, and drafted the manuscript. emm and jak conceived the study, co-developed the methodology, and helped to write the paper. all authors read and approved the manuscript.

