BACKGROUND
an increasing number of studies in phylogenetics have demonstrated, using a range of different inferential methods of varying complexity, that the assumption of site-independent evolution is overly restrictive and that evolutionary models that take into account context-dependencies may greatly improve model fit. specifically, context-dependent models are useful when studying mammalian genomes due to the extensive methylation of c in cg doublets, which could make such cs hotspots for mutation . indeed, more accurate mathematical models of molecular sequence evolution continue to be developed for good reasons as the additional complexity of such models can lead to the identification of important evolutionary processes that would be missed with simpler models. these models however come at a drastically elevated computational cost due to their increase in number of parameters and the need for data augmentation to make the likelihood calculations feasible  <cit> .

the power of markov chain monte carlo  techniques enables drawing inference under such complex high-dimensional models in molecular phylogenetics  <cit> . in this process, comparing alternative models according to objective criteria in a formal model selection procedure is essential in order to select models that best balance simplicity with flexibility and biological realism  <cit> . however, the computational demands associated with increasing model complexity and the amount of data available has considerably hampered careful model assessments. while mcmc approaches cleverly avoid calculating the normalization constant , it is in fact this constant that is of primary importance in model selection . in particular, it is used to calculate the  bayes factor between two models, which is a ratio of two marginal likelihoods , with y the observed data and m an evolutionary model under evaluation) obtained for the two models, m <dig> and m <dig>  under comparison  <cit> :

  b10=pp. 

kass and raftery  <cit>  introduce different gradations to assess the log bayes factor as evidence against m <dig>  a value between  <dig> and  <dig> is not worth more than a bare mention, whereas a value between  <dig> and  <dig> is considered as positive evidence against m <dig>  values larger than  <dig> and  <dig> are considered to respectively give strong and very strong evidence against m <dig>  the bayes factor offers advantages over likelihood-ratio-tests comparing nested models in which one garners evidence only in favor of rejecting less complex models. instead, the bayes factor evaluates the relative merits of both competing models and does not require nested models. further, when the individual  marginal likelihoods are estimated correctly, the  bayes factor takes into account differences in dimensions, so higher dimensional models are not automatically preferred.

several useful methods have been proposed to evaluate marginal likelihoods  in phylogenetics, but they are often limited to specific model selection situations, see  <cit>  for an overview. comparing a few of the methods of potentially general applicability, lartillot and philippe  <cit>  test a  monte carlo estimator of integrating the likelihood against the model prior and two variants of importance sampling : the posterior harmonic mean estimator  and the stabilized hme to a path sampling  approach  <cit> . of these approaches, the hme  <cit>  is by far the most popular method in the field of phylogenetics, only requiring samples from the posterior distribution . the hme is however often severely biased and overestimates the true marginal likelihood  <cit> . because the hme estimator’s variance may be infinite, a modified, stabilized version has been proposed  <cit>  with extensions to quantify its monte carlo error in phylogenetics  <cit> . ps is shown to outperform the other methods across all scenarios, remaining well-behaved in cases with high dimensions where all three is methods fail, even when these is methods use a huge number of posterior samples  <cit> .

recently, xie et al.  <cit>  introduced stepping-stone sampling , which employs ideas from both importance sampling and path sampling to estimate the marginal likelihood using a path that bridges the posterior and prior distribution of a model. using a gaussian model example, the authors demonstrate that ss yields a substantially less biased estimator than ps, while requiring significantly fewer path steps  to reliably estimate the marginal likelihood. further, ss outperforms the hme in terms of accuracy, consistency and repeatability  <cit> . while the performance of ps and ss was originally assessed using gaussian model examples and small phylogenetic examples, these approaches have recently been shown to considerably outperform the hme and a posterior simulation-based analogue of akaike’s information criterion  using extensive simulations and empirical analyses in the context of demographic and relaxed molecular clock model comparison  <cit> .

as a consequence of these developments, path sampling  and stepping-stone sampling  estimators are currently being integrated in popular software packages such as beast  <cit> . these methods represent very general estimators as they can be applied to any model for which mcmc samples can be obtained. despite the increased computational demands associated with these estimators, they are particularly suited to assess the performance of high-dimensional models, since the hme systematically favors parameter-rich models   <cit> .

in this paper, we focus on direct  bayes factor estimation between two competing models instead of estimating each individual model’s marginal likelihood. direct  bayes factor estimation, available in the path sampling paradigm as ‘model-switch path sampling’ and connecting the two models under comparison in the space of unnormalized densities, is more accurate and less prone to errors as we show here, which is especially important for cases where the difference between the logarithm of the marginal likelihoods of the two models is small compared to the separate marginal likelihoods themselves  <cit> . given the recent introduction of stepping-stone sampling to estimate marginal likelihoods more efficiently  <cit> , we here introduce ‘model-switch stepping-stone sampling’ to perform direct  bayes factor estimation and provide evidence that this represents the more reliable approach.

we test the mentioned approaches on three data sets  with distinctive properties to show the advantages of our presented approach. two mammalian data sets are analyzed, which are expected to yield increases in model fit due to the cpg-methylation-deamination process acting upon mammalian sequences and the ability of the proposed context-dependent model to incorporate this process. the largest of these data sets contains large amounts of sites for each of the parameters that need to be estimated and is hence expected to yield accurate context-dependent parameter estimates that will lead to a substantial increase in model fit over a traditional site-independent model. the smaller mammalian data set is likely to yield context-dependent parameter estimates with large variance, rendering the process of assessing differences in model fit challenging and hence presenting an interesting case for the different model selection approaches that are being compared. finally, we test a green plant data set of which the substitution processes are not expected to conform to those accounted for by the context-dependent model and which is hence expected to yield a decrease in model fit compared to a site-independent evolutionary model.

methods
data
a first  data set is a subset of the sequence data set analyzed in prasad et al.  <cit> , which is an expanded version of that reported by thomas et al. . all sequences are orthologous to a  <dig> -mb region of human chromosome  <dig>  that includes  <dig> known genes . we selected five sequences from the laurasiatheria, i.e. the sequences of domestic pig , indian muntjac , sheep , cow  and horse  assuming the following topology ,muntjak),pig),horse). the reported analyses in this manuscript were performed on the conserved non-coding part of the data set of prasad et al.  <cit> , comprising  <dig>  nucleotides per sequence. we refer to this data set as the “laurasiatheria” data set.

a second  data set consists of the ψη-globin pseudogene sequences of six primates: human , chimpanzee , gorilla , orang-utan , rhesus monkey  and spider monkey , containing  <dig>  nucleotides in each sequence. we have used the fixed consensus tree shown in the work of yang  <cit>  and refer to this dataset as the “pseudogenes” data set.

a third  data set consists of  <dig> small subunit  rrna genes  obtained from the alignment of karol et al.  <cit> . we have used the following sequences: cyanophora paradoxa, nephroselmis olivacea, chlamydomonas moewusii, volvox carteri, paulschulzia pseudovolvox, coleochaete orbicularis  <dig>  coleochaete solute 32d <dig>  coleochaete irregularis 3d <dig>  coleochaete sieminskiana 10d <dig>  zygnema peliosporum, mougeotia sp <dig>  gonatozygon monotaenium  <dig>  onychonema sp <dig>  cosmocladium perissum  <dig>  lychnothamnus barbatus  <dig>  nitellopsis obtusa f131b, chara connivens f <dig>  lamprothamnium macropogon x <dig>  arabidopsis thaliana and taxus mairei. we used the 50% majority-rule posterior consensus tree under the general time-reversible model. we refer to this dataset as the “nuclear ssu rrna” data set, which contains  <dig>  nucleotides per sequence.

context-dependent evolutionary model
we allow the substitution probabilities for a given site to depend on its “evolutionary context”, i.e. the combination of neighboring bases of that site. we assume that evolution occurs independently on each tree branch  but do not employ a branch-specific  context-dependent evolutionary model. we do not follow the approach of hwang and green  <cit> , who have approximated the continuous-time markov substitution process by partitioning each branch into two or more discrete time units so that the average substitution rate per time unit is ≤  <dig> , because our previous work demonstrated that this does not result in different parameter estimates  <cit> . we here present a first-order  context-dependent evolutionary model which mimics the model of hwang and green  <cit> .

for bases x≠z in the first-order context-dependent model, let ψi be the probability that, in one unit of time, the base x at position i mutates to z, given neighboring bases w  and y . should the branch not be partitioned, it is hence assumed that the neighboring bases of position i remain constant across the branch. partitioning the branch into two or more parts allows the substitution probabilities to depend on more recent ancestral sequences than at the start of the branch. the probability of no substitution is ψi=1−∑z≠xψi. if w, x, y or z is a gap, ψi= <dig>  each combination of two neighbors yields its own set of context-dependent frequencies fw−y, i.e. the substitution probabilities for a given set of immediate neighbors w and y yield a distribution of frequencies for those neighbors. these frequencies are used to scale the substitution probabilities ψi so that one unit of time is the time in which we expect to see one change per base, and this for each of the  <dig> evolutionary contexts, so that ∑x∑zfwxyψi= <dig>  this first-order context-dependent substitution model requires  <dig> parameters to be estimated when it is assumed that complementary events are unequal. here, we assume that such events are equal, i.e. that ψi=ψi, where xc denotes the complement of x, which reduces the number of parameters for this model to  <dig> 

for the laurasiatheria data set, the distribution of bases x that are at the ancestral root sequence is modeled as an inhomogeneous second-order markov chain with transition parameters Π, where v and w are the bases that immediately precede x . if x, v or w is a gap, Π= <dig>  for the pseudogenes data set, an inhomogeneous first-order markov chain with transition parameters Π is assumed , where v is the base that immediately precedes x. if x or v is a gap, Π= <dig>  for the nuclear ssu data set  <cit> , an zero-order markov chain Π is assumed . no symmetry conditions are imposed on the Π values.

prior distributions
given the importance of using proper prior distributions , we here provide the priors used in this manuscript. let t be the set of branch lengths with tb one arbitrary branch length and μ a hyperparameter in the prior for tb in t. the following prior distributions q were chosen for our analysis, with Γ the gamma function:

 tb∣μ∼exponential,q=1μe−tbfor eachtbint, 

 μ∼inv - gamma,q=Γμ−e− <dig> /μ,μ> <dig>  

branch lengths are assumed i.i.d. given μ. dirichlet priors  assign densities to groups of parameters that measure proportions . for each set of model frequencies of which the ancestral root sequence is composed, the following prior distribution is assumed:

 Πroot∼dirichlet,q=Γ. 

the above prior is also assumed for the set of base frequencies of the gtr model, which also specify its stationary distribution. for the model parameters of each context  independently, the following prior distribution is assumed :

 ψ∼dirichlet,q=Γ. 

when the model allows for the presence of multiple contexts of evolution, each context is assumed to have its own prior, independently of other contexts. the use of a hyperparameter for the branch-length priors is to reduce sensitivity of the posterior to the prior  <cit> .

harmonic mean estimators
among the available approaches to calculate the marginal likelihood of a model, the harmonic mean estimator  is by far the most used in the field of phylogenetics, only requiring samples from the posterior distribution. because hme variance may be infinite, a modified stabilized version  has been proposed  <cit> . the hme is however often severely biased and results in overestimating the true marginal likelihood  <cit> . lartillot and philippe  <cit>  suggest an intuitive reasoning for this by stating that, if the likelihood is unimodal, the marginal likelihood is more or less the product of two factors: the likelihood reached in the high-likelihood region  and the relative size of this region . estimators such as the hme have difficulties in assessing the mode width, which is estimated by measuring the relative frequency at which points of the sample fall inside and outside the mode, requiring that a sufficient number of points outside the mode be included in the sample. lartillot and philippe  <cit>  state that, in practice, the contrast between the low and the high likelihoods is in general so large that even a posterior sample of astronomical size will be virtually confined within the mode. the estimated frequency at which the low-likelihood region is visited is then  <dig>  which means that, in effect, the hme behaves as if the mode was occupying the entire parameter space, and therefore, completely underestimates the dimensional penalty. as a result, the hme overestimates the marginal likelihood, an overestimation that is more pronounced in the case of higher dimensional models, leading to the hme being biased in favor of such models.

path sampling
path sampling is considered a natural generalization of importance sampling and uses many and continuously connected “bridge” densities  to compute  normalizing constants  <cit> . path sampling is hence an extension of bridge sampling, which generalizes importance sampling through the use of a single “bridge” density. in a comparative study on a variety of methods for computing bayes factors, from laplace approximation to bridge sampling, diciccio et al.  <cit>  show that bridge sampling typically provides an order of magnitude of improvement. path sampling, which was not part of their study, has been demonstrated to yield even more dramatic improvement  <cit> .

using the notation put forward by  <cit> , suppose there are two unnormalized densities, q <dig> and q <dig>  defined on the same parameter space Θ, with corresponding true probability densities 

  pi=1ziqi,i= <dig> , 

where the normalizing constants are 

  zi=∫Θqidθ,i= <dig> . 

monte carlo simulation is widely used in statistics, mainly because of its general applicability, to approximate such analytically intractable normalizing constants. arguably, it is also the only general method available for dealing with complex, high-dimensional problems  <cit> . up until the introduction of bridge sampling and path sampling, estimation methods in statistics often relied on the scheme of importance sampling, either using draws from an approximate density or from one of pi. theoretical  <cit>  and empirical evidence  <cit>  provided in the context of bridge sampling, show that substantial reductions of monte carlo errors can be achieved with little or minor increase in computational effort, by using draws from more than one pi. the key idea is to use “bridge” densities to effectively shorten the distances among target densities, distances that are responsible for large monte carlo errors with the standard importance sampling methods. in fact, gelman and meng  <cit>  show that importance sampling, bridge sampling and path sampling represent a natural methodological evolution, from using no bridge densities to using a  infinite number of them.

lartillot and philippe  <cit>  recently introduced path sampling in the field of phylogenetics and propose a continuous method to directly estimate log bayes factors, which has the advantage of yielding greater accuracy compared to a previously introduced discrete method  <cit> . this approach  considers the unnormalized density function qβ to constitute a direct path between the two competing models  <cit> , which has normalizing constant cβ yielding the normalized density pβ. in other words, β interpolates between the two models’ posterior densities. the originally proposed continuous method consists in equilibrating a mcmc under β= <dig>  followed by smoothly increasing the value of β, by adding a constant increment δβ after each series of q cycles, until β= <dig> is reached. during this procedure, points θk are saved before each update of β. denote k= <dig> .k the series of points obtained this way. the constant-increment approach of lartillot and philippe  <cit>  assumes in particular β0= <dig>  βk= <dig>  and ∀k,0≤k<k,βk+1−βk=δβ, which is reflected in the expressions for the continuous estimator and its corresponding discretization and sampling error. specifically, the estimate of 

  lnz1−lnz0=∫01eβdβ, 

with zi the normalizing constant of model i, is given by: 

  μ^qs=1k12u+∑k=1k−1u+12u, 

with 

  u=lnf+lnΠ−lnf−lnΠ, 

where y represents the data , Θ is the vector of model parameters, mi,i= <dig>  are the two models under consideration, f is the likelihood function and Π,i= <dig>  the model priors. as the authors mention, one can also start at β= <dig>  equilibrate the mcmc, and then progressively decrease β, while sampling along the path down to β= <dig>  the mean of both estimates can then be used as a final estimate of the log bayes factor . we here adopt the terminology of lartillot and philippe  <cit>  and call the move from β= <dig> to β= <dig> the “annealing” integration whereas the move from β= <dig> to β= <dig> is called the “melting” integration. the constant-increment approach  <cit>  may be considered to yield an oversimplified expression for the continuous estimator. it is based on simpson’s triangulation formula to calculate the contribution to the overall log bayes factor of one step of the integration, for example from βk to βk+ <dig> as follows: 

  12u+12u. 

given that the increments are constant, it follows that ∀k0≤k<k,βk+1−βk=1/k and hence equation  is readily obtained. in the case of non-constant increments, equation  for the continuous estimator becomes: 

  μ^qs=∑k=0k−112u+12u. 

equation  <dig> is restricted to a sample from the last iteration of each β to calculate the marginal likelihood, as in the original paper on path sampling  <cit> . therefore, this approach only uses a limited amount of the available samples and more information can potentially be retrieved from the mcmc iterations in order to improve the estimation of the marginal likelihood. one possibility lies in using the mean of multiple values for each β, collected at fixed intervals, which requires replacing u and u in equation  <dig> by Û and Û respectively, i. e. the mean of a collection of samples at βk and βk+ <dig> respectively. we propose to collect a sample every  <dig> update cycles, where an update cycle involves an update for every model parameter, branch length and ancestral site. such a scenario is valid since, for large values of k, pβk− <dig> is only slightly more dispersed than pβk  and hence serves as an excellent importance distribution  <cit> . in other words, for large values of k, the mcmc chain will smoothly transition between different values of β, and hence the likelihood and parameter values in subsequent mcmc chains will not be all that different from one another, which allows for early sampling at each new value of β. we refer to appendix a for a derivation of the discretization and sampling error corresponding to the path sampling estimator discussed here.

stepping-stone sampling
recently, xie et al.  <cit>  presented a novel approach to estimate marginal likelihoods called ‘stepping-stone sampling’. the authors show that their approach yields an unbiased estimate of the marginal likelihood, as opposed to ps, and that their calculations can be performed more efficiently than ps. using a simulated gaussian example data set, which is instructive because of the fact that the true value of the marginal likelihood is available analytically, xie et al.  <cit>  show that ps and ss perform much better  than the hme at estimating the marginal likelihood. the authors go on to analyze a 10-taxon green plant data set using dna sequences of the chloroplast-encoded large subunit of the rubisco gene  and establish that ps requires a larger number of power posteriors to be explored compared to ss to overcome its additional bias. using the hme to estimate the marginal likelihood is reported to yield higher values than using both ps and ss.

we here present an extension of their approach to directly calculate  bayes factors, i.e. the stepping-stone version of model-switch path sampling, with the term “model-switch” indicating that a single path directly connects the two models in the space of unnormalized densities. whereas such a general approach to directly estimate  bayes factors is relatively new in the field of phylogenetics  <cit> , the idea stems from statistics and was first introduced in the work of meng and wong  <cit> , who propose a number of approaches to calculate the ratio of two normalizing constants. using the notation of  <cit> , consider again the unnormalized density function qβ, which constitutes a direct path between the two competing models m <dig> and m <dig> <cit>  and has normalizing constant cβ yielding the normalized density pβ: 

  qβ=fΠ1−β×fΠβ, 

  pβ=qβ/cβ, 

  cβ=∫Θqβdθ, 

where again y represents the data , Θ is the vector of model parameters, mi,i= <dig>  are the two models under consideration, f is the likelihood function and Π,i= <dig>  the model priors. the goal is to estimate the ratio c <dig> /c <dig> . similar to the original stepping-stone method, this ratio can be expressed as a product of k ratios: 

  r=c <dig> c <dig> =∏k=1kcβkcβk− <dig>  

where 0=β0<…<βk−1<βk<…<βk= <dig>  each ratio cβk/cβk− <dig> can be estimated by importance sampling, using pβk− <dig> as the importance sampling density. because pβk− <dig> is only slightly different from pβk, it serves as an excellent importance distribution. one of the k ratios, rk, can thus be expressed as follows:

  rk=cβkcβk−1=∫qβkdθ∫qβk−1dθ=∫qβkpβk−1pβk−1dθ∫qβk−1pβk−1pβk−1dθ=∫qβkqβk−1/cβk−1pβk−1dθ∫qβk−1qβk−1/cβk−1pβk−1dθ=∫qβkqβk−1pβk−1dθ=∫1−βkβk1−βk−1βk−1pβk−1dθ=∫βk−βk−1βk−βk−1pβk−1dθ=epβk−1fΠfΠβk−βk− <dig>  

an estimator r^k is constructed using samples θk− <dig> i from pβk−1:

  r^k=1n∑i=1nfΠfΠβk−βk− <dig>  

numerical stability can be improved by factoring out the largest sampled term ηk=max1≤i≤n{fΠ/fΠ}:

  r^k=1nβk−βk−1∑i=1n×fΠηkfΠβk−βk− <dig>  

combining all k ratios, the ss estimate of the bayes factor is

  r^=∏k=1kr^k. 

as for the marginal likelihood estimator based on stepping-stone sampling  <cit> , r^ is unbiased, being a product of independent unbiased estimators. on the log scale, and by performing calculations: 

  logr^k=logηk+log1n∑i=1nfΠηkfΠβk−βk− <dig>  

finally, summing logr^k over all k ratios yields the overall estimator:

  logr^=∑k=1klogr^k=∑k=1k+∑k=1klog1n∑i=1n×fΠηkfΠβk−βk− <dig>  

although r^ is unbiased, changing to the log scale introduces a bias  <cit> . note that direct bayes factor estimation  using stepping-stone sampling yields lower variance than calculating the ratio of two independently estimated marginal likelihoods . we refer to appendix b for the derivation of this variance and its comparison to the ratio of variances accompanying marginal likelihood estimation.

error assessment
instead of investigating the discretization and sampling error , we here focus on the differences that may occur between annealing and melting versions of the model-switch integrations that yield the log bayes factor. we use the split-calculation approach introduced in previous work  <cit> , which allows the integration shown in equation  <dig> to be rewritten as

  ∫01eβdβ=∫0α1eβdβ+…+∫αn1eβdβ, 

with α0=0<α1<…<αn<1=αn+ <dig> dividing the interval  <cit>  into n subintervals. each of these integrals can be calculated independently in parallel, allowing the calculation of the log bayes factor to be distributed across multiple computer nodes , yielding results much faster than when running on a single node. while the method discussed in  <cit>  was applied to path sampling, it can easily be adapted for stepping-stone sampling  and hence each integral shown in equation  <dig> can be calculated using both path sampling and stepping-stone sampling. we calculate the difference in contribution to the log bayes factor for each of the calculated  <dig> sub-integrals and consider the sum of the absolute values of these differences to be a bidirectional error . the higher this error, the larger the difference between both annealing and melting calculations of the same sub-integral, indicating that more stringent computational settings are needed to accurately estimate the contribution to the total log bayes factor.

even using very demanding computational settings to calculate the log bayes factor in both directions, small differences between the two estimates are to be expected for two reasons. one is the repeatability issue discussed in  <cit> , which results in small deviations of the marginal likelihood depending on the starting seeds of the analyses, a second is the so-called “thermic lag” of the mcmc chain  <cit> . indeed, as β changes continuously during sampling, the chain is never exactly at equilibrium, which will cause a “thermic lag” of the mcmc chain. when sampling a value of Θ at the current value of β, one is in effect sampling from pβ′, with β′ slightly smaller than β. because u is an increasing function of β, one expects this lag to result on average in an underestimation of the true marginal likelihood when β goes from  <dig> to  <dig>  and in an overestimation when β goes from  <dig> to  <dig>   <cit> . this thermic lag bias results from the fact that when the value of β is adjusted, the markov chain takes some time to adjust to the new value  <cit> , i.e. needs to be equilibrated before taking samples. we here check how consistent the estimators are in yielding an underestimation for the annealing calculations and an overestimation for the melting calculations.

RESULTS
laurasiatheria data set
as a means of comparison for our proposed approach, we first estimate the marginal likelihood of the presented context-dependent model and a site-independent reference model known as the general time-reversible  evolutionary model, which contains  <dig> free evolutionary parameters and  <dig> free base frequencies. the hme and shme estimates of the marginal likelihood for these models are listed in table  <dig>  showing that  the context-dependent model  offers a drastic improvement in model fit over the general time-reversible model, with a log bayes factor of  <dig>  log units. however, as mentioned in the methods section, the hme tends to be biased towards higher-dimensional models, meaning that the log bayes factor shown in table  <dig> is possibly an overestimation of the true log bayes factor.

harmonic mean estimates  and stabilized harmonic mean estimates  for both the site-independent general time-reversible model  and the context-dependent model of hwang and green   <cit> , combined with the one or more ancestral root distributions  <cit> . great care needs to be taken to make sure that under the gtr model, the same sites are taken into account when calculating the likelihood, this due to the presence of gaps. we emphasize this by stating which ancestral root distribution is used for the gtr model, on top of eliminating those sites for which the dependency pattern contains one or more gaps. the proposed context-dependent model  clearly outperforms the site-independent gtr model for all the data sets, according to the hme and shme. further, for the nuclear ssu rrna data set and according to the shme, the difference in model fit increases in favor of the hg <dig> model with increasing dependencies at the ancestral root.

as a baseline for the comparison of our analyses of different sigmoid shape parameters to determine which power posteriors to estimate, we first used a flexible-increment approach as introduced in previous work  <cit>  and determined the annealing and melting estimates for the log bayes factor and the accompanying bidirectional error . the flexible-increment approach is an extension of the original  path sampling method, where each integration interval shown in equation  <dig> employs a different but constant increment size for β. an equal number of iterations were run across all  <dig> integration intervals, summing up to a total of k= <dig>  path steps, with q= <dig> mcmc iterations being run per path step . our flexible-increment approach yields lower bidirectional errors for all three log bayes factor estimators compared to the original path sampling method  <cit>   and yields more similar results for the annealing and melting integrations. using these settings, it is immediately apparent that the estimated log bayes factor using ps or ss is drastically different from the one estimated earlier in the manuscript using the hme and shme, with the log bayes factor estimates differing by about  <dig> log units.

as discussed earlier, the manual determination and refinement of the number of path steps and mcmc iterations per path step is an iterative and time-consuming process. we used identical integration settings  and estimate the log bayes factor using a sigmoidal approach to determine the path steps to traverse. based on a visual comparison , a shape parameter of α= <dig>  is an appropriate starting value, which is increased until the accuracy of the results can no longer be improved. annealing and melting integrations, shown side by side in figure  <dig>  for the different sigmoid shape values indicate that the most adequate performance is obtained for sigmoid shape values between  <dig>  and  <dig> , where bracketing the true value of the log bayes factor becomes more reliable, although there are still quite a few units of difference between them. we increased the shape parameter up to α= <dig> , revealing that the lowest bidirectional errors are reached for α= <dig>  , and this for all three estimators. the computational settings  used so far focus on running many short chains, with each subsequent chain only slightly different than the previous one. we now test what the optimal values for both parameters are in order to achieve optimal performance.

given that xie et al.  <cit>  have shown that a value of k= <dig> path steps is sufficient for marginal likelihood estimation using stepping-stone sampling for a data set of  <dig> green plant rbcl sequences , we have first reversed the integration settings for two shape values . the results seem to indicate that both versions of the path sampling estimator converge really well, with reasonable bidirectional errors. this result is misleading however, as shown by our stepping-stone sampling estimator  which converges to a different value for the log bayes factor. in fact, only stepping-stone sampling offers an indication that something is amiss, given its higher bidirectional error, but mainly due to its melting estimate of the log bayes factor, which is much higher than the annealing estimate. hence, k= <dig> path steps are clearly insufficient to obtain reliable estimates of the log bayes factor and may even yield fairly accurate but unreliable results. increasing k to  <dig> solves the problem of the previous integration settings for a sigmoid shape value α= <dig>  for all three estimators, albeit that the associated bidirectional error estimates are very large. however, for a shape value α= <dig> , both path sampling estimators are still unable to bracket the true log bayes factor value, hence requiring that the number of path steps k be further increased. in other words, even a number of path steps of k= <dig>  which seems to be excessive based on the work of xie et al.  <cit> , is only sufficient when a suitable sigmoid shape value is chosen.

since doubling the number of iterations per path step to q= <dig>  when k= <dig> , only leads to limited improvements of the bidirectional error , we now gradually increase the number of path steps, starting with k= <dig>  or twice the amount of our first series of analyses, since increasing k alleviates the bias of stepping-stone sampling  <cit> . further increasing the number of path steps to k= <dig>  confirms this conclusion. because of the computational demands, we only apply a final doubling of the number of path steps  to the sigmoid shape values α= <dig>  and α= <dig>  . bidirectional errors continue to decrease, leading to even better bracketing of the true value of the log bayes factor . further, comparing the estimates for both sigmoid shapes shows that the different shapes for the first time lead to very similar results for both annealing and melting calculations, allowing us to accurately infer the true value. for α= <dig> , the mean bidirectional log bayes factor equals  <dig>  and for α= <dig>  this equals  <dig> .

our exploration of different α values allows us to create a visual representation for annealing and melting estimates of the log bayes factor and their bidirectional mean, which is used as the actual estimate of the true value . for a shape of α= <dig> , the bidirectional mean is the most stable across the different number of path steps  of the different shape values tested and has the appealing property that the difference between annealing and melting estimates decreases the most with increasing path steps .

pseudogenes data set
we first estimate the marginal likelihood of the hg <dig> context-dependent model and compared it to that of the gtr model using the hme and shme . these estimators again report a significant improvement in model fit of the hg <dig> model, equipped with a first-order ancestral root distribution, over the site-independent gtr model, by a log bayes factor of  <dig>  log units. the much lower number of nucleotides of the pseudogenes data set compared to the laurasiatheria data set allow for a much quicker evaluation of the  bayes factor using both path sampling approaches as well as the stepping-stone sampling approach. we have therefore chosen to do so using the most stringent settings used for the laurasiatheria data set, i.e. k= <dig>  and q= <dig>  the results are shown in table  <dig> 

pseudogenes data set. log bayes factor estimates for thendent model compared to the site-independent model using regular path sampling , path sampling using the mean of a collection of samples from each path step  and regular stepping-stone sampling . bidirectional checks, consisting of annealing  and melting  integrations, were performed for each log bayes factor calculation, yielding a bidirectional error  for each estimator. α values indicate the shape of the sigmoid function used to construct a path between the two models. k indicates the number of path steps  used, while q indicates the number of iterations to be performed per path step .

of the different sigmoid shape values tested, a shape value of α= <dig>  performs much worse than all other values tested, which perform quite similarly. the original path sampling method  <cit>  performed the worst  in terms of yielding an underestimation of the log bayes factor using the annealing integration and an overestimation using the melting integration, with our adaptation of path sampling and our proposed stepping-stone sampling approaching only reporting  <dig> comparison for which this is the case . a shape value of α= <dig>  yields the lowest bidirectional error for ss, yielding a mean bidirectional log bayes factor of  <dig> , confirming the conclusion of the hme and shme that the proposed context-dependent model hg <dig> provides a significantly better model fit than the gtr model. once again however, hme and shme seem to clearly be biased towards higher-dimensional model as the log bayes factor is again overestimated. this result for the pseudogenes data set confirms that a context-dependent model offers increased performance over site-independent models in mammalian data sets, even in the case of  short sequences and a small number of sequences.

nuclear ssu rrna data set
a land plant data set, such as the one analyzed here  <cit> , offers a real challenge for marginal likelihood and  bayes factor estimators using the models compared in this manuscript. the hg <dig> model is aimed at capturing specific context-dependent processes, such as the cpg-methylation-deamination process in mammalian sequences. since we are unaware of any such processes being present in land plant sequences, we expect that the context-dependent hg <dig> model will actually be outperformed by the site-independent gtr model in terms of model fit, on the premise that a model selection approach is used that is able to penalize the model for its excessive amount of parameters that are not accompanied by fitting site patterns.

given the probably low amount of context-dependent substitution processes that are able to significantly increase model fit, the ancestral root distribution will most likely also not contain any dependencies. we now test this assumption using both the hme, shme and ss approach, comparing a zero-order, first-order and second-order ancestral root distribution to the gtr model . note that table  <dig> shows  <dig> estimates of the hme and shme for the gtr model, depending on the actual ancestral root distribution used with the context-dependent model to ensure that the same set of likelihood contributions are used for the different models. according to the hme and shme , the most parameter-rich ancestral root distribution also explains the data the best, with a second-order distribution being preferred with a log bayes factor of  <dig>  log units over the gtr model . this result is questionable, since the pseudogene mammalian data set  benefits the most from a first-order ancestral root distribution  <cit> .

we now turn our attention to calculating the log bayes factor using the proposed ss approach. assuming k= <dig>  ratios with q= <dig> iterations each, a zero-order ancestral root distribution yielded a bidirectional mean log bayes factor of - <dig>  log units versus the gtr model, a first-order distribution a bidirectional mean log bayes factor of - <dig>  log units and a second-order distribution a bidirectional mean log bayes factor of - <dig>  log units. the outcome here is much more biologically plausible than what was obtained using the hme and shme, with no sign of dependencies being detected, not at the ancestral root nor throughout the remainder of the underlying tree. these results hence support the claim that the hme tends to be biased towards higher-dimensional models, both on the level of the context-dependent model and the ancestral root distribution used, and is hence unable to accurately perform model selection .

this data set is comparable with the pseudogenes data set in terms of computational complexity, meaning that we have again opted to use the most demanding integration settings for the different ps/ss approaches. the results are shown in table  <dig>  of the different sigmoid shape values tested, a shape value of α= <dig>  again performs worse than all other values tested. a shape value of α= <dig>  yields the lowest bidirectional error for ss, yielding a mean bidirectional log bayes factor of - <dig> , confirming the conclusion of the previous paragraph with more demanding integration settings. contrary to the two mammalian data sets in this manuscript, the overestimation of the marginal likelihood for high-dimensional models by the hme and shme leads to a different conclusion for these estimators compared to ps and ss, showing at ps and ss are able to take into account differences in dimension when estimating  bayes factors, unlike the hme and shme.

nuclear ssu rrna data set. log bayes factor estimates for the context-dependent model compared to the site-independent model using regular path sampling , path sampling using the mean of a collection of samples from each path step  and regular stepping-stone sampling . bidirectional checks, consisting of annealing  and melting  integrations, were performed for each log bayes factor calculation, yielding a bidirectional error  for each estimator. α values indicate the shape of the sigmoid function used to construct a path between the two models. k indicates the number of path steps  used, while q indicates the number of iterations to be performed per path step .

discussion and 
CONCLUSIONS
in this paper we have compared the performance of two versions of path sampling, an accurate  but computationally demanding model comparison approach, with that of stepping-stone sampling, for which we provide a so-called model-switch version to directly estimate  bayes factors. we have shown that our adaptation of stepping-stone sampling for direct  bayes factor calculation outperforms the original path sampling approach as well as an extension that exploits more samples. further, we have demonstrated that the  bayes factor estimator proposed in this manuscript generally has lower variance than the  bayes factor estimator obtained through the ratio of marginal likelihoods estimated using stepping-stone sampling.

the large number of combinations of number of path steps / ratios and chain lengths we investigated leads to the recommendation of stepping-stone sampling over path sampling. indeed, for a relatively small number of path steps / ratios, path sampling tends to converge towards an entirely different value for the  bayes factor, whereas only stepping-stone sampling is able to provide indications that more stringent analyses need to be performed to better approximate the  bayes factor. stepping-stone sampling is hence better-suited to provide rough initial estimates, using shorter and hence less time-consuming runs, of the magnitude of modeling assumptions. both path sampling and stepping-stone sampling methods to estimate  marginal likelihoods and  bayes factors are much more reliable approaches to perform model selection than the harmonic mean estimator, which is often employed because of its simplicity and computationally appealing properties.

given that at both ends of the integration interval when performing model-switch path sampling and stepping-stone sampling, one of the models requires sampling from its prior distribution, we have opted for a sigmoid function to determine the necessary power posterior distributions from which sampling is required. of the shape values compared for the different data sets, a value of between  <dig>  and  <dig>  is the most appropriate to accurately determine the difference in model fit for high-dimensional models as it is able to accurately bracket the  bayes factor and is accompanied by a low bidirectional error.

given that path sampling and stepping-stone sampling are far more reliable approaches when estimating the  marginal likelihood, we have checked whether this affects the outcome when performing model comparison. whereas for the laurasiatheria data set, there is a large difference in the log bayes factor estimated by path sampling and stepping-stone sampling on one hand, and the harmonic mean estimator on the other hand, these approaches still reach the same conclusion, i.e. that the context-dependent model presented here yields a much better model fit than a site-independent evolutionary model. while this is also the case for a smaller mammalian pseudogene data set, albeit with lower log bayes factors, we show that for a plant nuclear rrna ssu data set, the conclusions of the harmonic mean estimator and the path sampling and stepping-stone sampling estimators do not concur, with the former method being unable to accurately penalize the context-dependent model for its excess parameters.

high-dimensional models are typically used in, for example, studies of context-dependence in mammalian sequences. context-dependent models have been shown to yield much larger increases in model fit than the assumption of among-site rate variation  <cit>  or using mixture models  <cit> . the so-called first-order context-dependent evolutionary model , which we analyze in this paper, offers a fair balance between parameter complexity and performance, with further improvements in model fit appearing quite challenging  <cit> . in other words, the drastic increase in number of parameters, from  <dig> to  <dig>  is justified by the context-dependent evolutionary processes present in the data, even though this means that far less data per parameter is available. in non-mammalian sequences, these models may be prove to be less useful, as demonstrated in this manuscript, as the drastic increase in number of parameters is not accompanied by fitting context-dependent evolutionary patterns in the underlying data.

while stepping-stone sampling outperforms both versions of path sampling, as we have shown in this paper, it is not a silver bullet and still requires massive computation times, especially for the high-dimensional models tested in this manuscript. despite recent claims that data augmentation can speed up  bayes factor calculation  <cit> , the use of data augmentation is often a prerequisite to evaluate the likelihood for a context-dependent model and hence does not yield additional increases in speed here. for the laurasiatheria data set, the most demanding settings we tested  require about  <dig> days of calculation running across  <dig> cores simultaneously using 8-core intel xeon cpu x <dig> processors running at  <dig> ghz. this yields a total computational effort for this data set of about  <dig>  days or close to  <dig> computation years. the other two data sets analyzed in this manuscript require little more than  <dig> day of calculation running across  <dig> cores simultaneously on the reported system.

one way to circumvent the added complexity of model-switch path sampling and stepping-stone sampling is to shorten the path from posterior to prior whilst still calculating the marginal likelihood for each model separately. recently, fan et al.  <cit>  propose a more general version of stepping-stone sampling that introduces an arbitrary “working” prior distribution parameterized using mcmc samples from the posterior distribution. the authors show that if this reference distribution exactly equals the posterior distribution, the marginal likelihood can be estimated exactly. the authors show that generalized stepping-stone sampling is considerably more efficient and does not require sampling from distributions close to the true prior, currently still an issue with path sampling and stepping-stone sampling for many models. however, at the moment this method is restricted to evaluations on a fixed phylogenetic tree topology. integrating over plausible tree topologies complicates generalized stepping-stone sampling because of the need to define a reference distribution for topologies that provides a good approximation to the posterior. however, most context-dependent modeling approaches already make the assumption of a fixed underlying tree to make likelihood calculations feasible, making this last requirement less of an issue. the extension of generalized stepping-stone sampling towards constructing a direct path between two competing models is currently the subject of ongoing work.

appendix a: discretization and sampling error for path sampling
the corresponding discretization error for model-switch path sampling, originally provided in  <cit>  for the constant-increment approach, associated with using non-constant increments needs to be reformulated. since eβ is a monotonous function of β, the worst-case upper  error is given by the area between the piecewise continuous function joining the measured values of eβ and the upper  step function built from them  <cit> . both areas are equal to:

  σd=∣∑k=0k−112∣. 

calculating the sampling error in the case of non-constant increments is slightly more complicated:

  v=14∑k=0k−1v+u)+∑k=0k−1∑l= <dig> l≠kk−1cov+u),+u). 

assuming independence between the successive points of the chain, the first part of the sum in equation  can be written as: 

  ∑k=0k−12]+v). 

using that same assumption, the second part of the sum  in equation  only yields a non-zero contribution to the covariance if l=k+ <dig> or l=k− <dig>  this yields the following expression for the sampling variance: 

  v=14∑k=0k−12]+v)+2∑k=1k−1v. 

again, the presented formulas are valid only if the points are truly independent draws from their respective distributions. if this is not the case, then a factor τ=k/keff  needs to be taken into account in equation  <dig>  to account for the effective sample size  <cit> . given that β moves between  <dig> and  <dig>  the decorrelation time might change. while lartillot and philippe  <cit>  did not observe large variations in the decorrelation time for different values of β for the models they compared, this is not generally so, as shown in  <cit> .

appendix b: variance for stepping-stone sampling
as in the original work on  marginal likelihood estimation using stepping-stone sampling, we here provide an expression for the simulation variance of r^k in the context of estimating  bayes factor. the simulation variance of r^k is estimated by

  var^=1n2∑i=1nfΠfΠβk−βk−1−r^k <dig> 

based on the δ method  <cit> , the variance of logr^=∑k=1klog is approximated by:

  var^≈∑k=1k1r^k2var^=1n2∑k=1k1r^k2∑i=1n×fΠfΠβk−βk−1−r^k <dig>  

note that the two equations above are not used in the remainder of this appendix.

direct  bayes factor estimation is deemed to be less prone to errors than calculating the ratio of independently estimated marginal likelihoods  <cit> . using the δ method  <cit>  and assuming independence of r^k,k= <dig> …,k

  var^{logr^}=∑k=1kvar^≈∑k=1kvar^r^k <dig> 

and, also using the δ method  <cit> 

  var^{logr^}≈var^∏k=1kr^k∏k=1kr^k <dig>  

from equations  <dig> and  <dig>  it follows that

  var^∏k=1kr^k≈∏k=1kr^k2∑k=1kvar^r^kr^k <dig>  

we will use equation  <dig> together with the general approximation

  varrs≈e2e2vare2−2covee+vare <dig>  

in particular, assuming that the models m <dig> and m <dig> have the same priors, it follows from equation  <dig> that

  var^=r^2n∑k=1kvar^ffβk−βk−1cβkcβk−12=r^2∑k=1kvar^)βk−βk−1n2+var^)βk−βk−1n2−2covfβk−βk− <dig> fβk−βk−1n 

where θk is a random draw from pβk and

  cj,β=∫fβΠdθ. 

we compare this expression to the variance of the bayes factor obtained by independently calculating its marginal likelihoods using stepping-stone sampling  <cit> . in that case, and using expression  <dig> 

  var^=var^∏k=1kr^k,1r^k,0=r^2∑k=1kvar^r^k,1r^k,0r^k,1r^k, <dig> 

where

  r^k,j=1n∑i=1nfβk−βk− <dig>  

from expression  <dig>  it follows that for sufficiently large n, using the unbiasedness of r^k and assuming that the separate chains are independent

  var^=r^2∑k=1kvar^r^k,1r^k,0c <dig> βkc <dig> βk−1c <dig> βk−1c <dig> βk2=r^2∑k=1kvar^)βk−βk−1n2+var^)βk−βk−1n <dig>  

comparing expressions  <dig> and  <dig> shows that the variance of our direct bayes factor estimation differs from the variance of the ratio of marginal likelihoods only in the last term of expression  <dig> . this term is not zero because the densities are evaluated in the same parameter draws. furthermore, we can logically expect this covariance to be generally positive. this formalizes the idea that direct bayes factor estimation  yields lower variance than calculating the ratio of two independently estimated marginal likelihoods , if the two model priors are the same. from expression  <dig>  it follows that the variance of our proposed approach is finite as long as the denominators are not zero, which is to be expected as otherwise the marginal likelihoods themselves would have to be zero.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
gb initiated the study, designed the context-dependent evolutionary model and the model-switch stepping-stone sampling approach, implemented the path sampling and stepping-stone sampling approaches, performed all the analyses and wrote a first complete version of the manuscript. pl edited the manuscript. sv contributed statistical expertise to the analyses and edited the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
model comparison using path sampling  and stepping-stone sampling  for the laurasiatheria data set.

click here for file

 acknowledgements
the research leading to these results has received funding from the european union seventh framework programme  under grant agreement no.  <dig> and erc grant agreement no.  <dig>  stijn vansteelandt acknowledges support from ghent university  and iap research network grant no. p06/ <dig> from the belgian government . the computational resources  and services used in this work were provided by ghent university, the hercules foundation and the flemish government - department ewi. we acknowledge the support of the national evolutionary synthesis center  through a working group .
