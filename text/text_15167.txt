BACKGROUND
high throughput screening  is a method for scientific experimentation, which is widely used in drug discovery. it allows researchers to effectively conduct thousands or millions of biochemical or genetic tests simultaneously. microarray experimentation is a special case of hts. while one microarray chip can be used to test thousands of genes simultaneously, 96-well plates are typically used in hts, each well containing one compound. hundreds or thousands of 96-well plates are needed to test all the compounds once. the number of replicates in hts is often less than that in microarray experiments, rarely exceeding  <dig>  nevertheless, they all need to deal with the scenario where the number of statistical comparisons far exceeds the number of biological replicates. to connect with previous methods, we will use "hits" in hts and "differentially expressed genes" in microarray exchangeably.

many statistical methods have been developed to identify differentially expressed  genes in microarray experiments. there are variants of student's t test statistic that conduct a test on each individual biological entity and then correct for multiple comparisons. the problem is that, with a large number of tests and a small number of replicates, the statistics are very unstable. for example, a large difference in the measurements under different conditions might be driven by an outlier. also, a large t statistic might arise because of a small variance, even with a small difference in the sample means.

cui and churchill  <cit>  used the average of gene-specific variance and pooled variance to estimate variance component. there are several alternative statistics which also modify the estimator of variance. the sam t statistic was proposed in  <cit>  where a suitable constant is added to gene-specific variance estimates. a shrunken t statistic  <cit>  was developed with a variance estimator that borrows information across genes using the james-stein shrinkage idea. in james-stein estimation, the shrinkage estimate is a linear combination of the original unbiased estimator  and a target estimate to minimize a certain loss function . this procedure is computationally simple, yet produces efficient estimates. also in the framework of james-stein shrinkage, opgen-rhein and strimmer  <cit>  proposed a "shrinkage t" approach, which requires no distributional assumption. in general, these analytic shrinkage estimators show a powerful and robust performance in testing de genes.

from the bayesian perspective, the introduction of a prior distribution on gene-specific variance naturally implements the shrinkage idea. baldi and long  <cit>  proposed the regularized t statistic to replace gene-specific variance with a bayesian estimator based on a hierarchical model. fox and dimmic  <cit>  extended baldi and long's approach by explicitly calculating the marginal posterior distribution for the difference in mean expression levels. lonnstedt and speed  <cit>  proposed an empirical bayes approach for replicated two-color mi-croarray experiment. smyth  <cit>  extended the empirical bayes approach for general microarray experiments. sartor et al.  <cit>  further extended smyth's method by accounting for the dependence of variance on gene expression intensity. kendziorski et al.  <cit>  considered a hierarchical gamma-gamma model to test de genes.

lonnstedt and britton  <cit>  proposed full bayesian models and compared them to several highly-used frequentist methods and empirical bayes methods. they found that the full bayesian models seem to have less power selecting de genes. this is because the frequentist test statistics and the empirical bayes methods, which are similar in performance, put a stronger shrinkage on variance estimates. when the number of replicates is extremely small, the shrinkage becomes more useful in stabilizing the test statistics. in light of this study, we make a simple but important modification by adding a point mass component in the variance prior. it introduces adequate shrinkage in the estimation of variance components so that the full bayesian model could have equivalent or greater power compared to those highly-used differential expression methods.

the bayesian model can be combined with frequentist method to further enhance the performance. one of the most current developments in this area is the optimal discovery procedure  proposed by storey  <cit> . different from the conventional practise of calculating test statistic on each individual gene and then adjusting for multiple comparison, the odp statistic is calculated based on the information across genes. the method has shown significant gains in power relative to a number of leading methods. to estimate the proportion of the true nulls, storey used an ad hoc method which is based on ranking the tests by using a univariate statistic . he also used gene-specific sample mean and sample variance to estimate the parameters in the hypothesized null and alternative distributions. in this paper, we propose to use the posterior probability of a gene being de to estimate the set of true nulls. by doing this, we don't need to choose a cutoff to determine the null set. the uncertainty in the estimation is accounted for in a probabilistic fashion. furthermore, the sample mean and variance are replaced by the posterior mean and variance of gene expression level. the bayesian estimates can borrow strength across genes. they may be more reliable than sample mean and variance, which are computed separately for each gene. our study shows that the bayesian odp has considerable improvement over the original odp, especially when there are few replicates per gene.

methods
the bayesian model
in this section, we build a full bayesian hierarchical model, and then we construct the bayesian odp statistic to identify de genes. let xij be the expression measurement from the ith gene on the jth array under the control , and yik be the expression measurement from the ith gene on the kth array under the treatment . replicate number n0i and n1i can be different among genes and between conditions, which means that the bayesian method can deal with missing values and unbalanced experiment designs. through a logarithm transformation  on the original measurements, xij and yik are modeled by normal distributions. the first level of the bayesian model is

 xij|μi,σi2~n,i= <dig> ⋯,n,j= <dig> ⋯,n0i;yij|μi,Δi,σi2~n,i= <dig> ⋯,n,k= <dig> ⋯,n1i, 

where μi is the baseline expression level under the control, and Δi is the difference in expression levels between treatment and control. we assume that variance σi <dig> is the same under the two conditions for the ith gene.

in bayesian modeling, it is common to introduce a latent variable to indicate the expression status of the ith gene  <cit> . here we use ri = 1/ <dig> to denote differential/nondifferential expression for gene i. specifically, we have

 {Δi= <dig> if ri= <dig> Δi~n,if ri= <dig>  

thus Δi is modeled by a mixture of two components, one being a point mass at  <dig> for non-de genes, and another being a normal distribution for de genes. hyper-parameter sΔ <dig> is specified as a constant. we further assume that ri | pr ~ bernoulli, where pr is the mixing probability.

to introduce a shrinkage on variance component, we impose a mixture structure on σi2

 {σi2=σ <dig> if vi= <dig> σi2~ig,if vi= <dig>  

we assume that vi | pv ~ bernoulli, where pv serves as the mixing probability. thus vi =  <dig> indicates that gene i shares a common variance with some other genes, and vi =  <dig> indicates that it has a gene-specific variance arising from a continuous inverse gamma distribution. we specify hyper-parameters aσ and bσ as constants.

we complete the bayesian model with prior specifications for parameters ,

 μi~n,σ02~ig,pr~beta,pv~beta, 

where  are specified as constants.

let x and y be the collections of expression measurements from all the genes under control and treatment, respectively. our primary interest is zi = e, the marginal posterior probability that gene i is de. we use zi as the test statistic, i.e., a gene is flagged as de if zi > λ, where λ is a cutoff value.

computing zi involves integration over all the other parameters in the joint posterior distribution. this integration does not have a closed form. we implement a markov chain monte carlo  algorithm to make posterior inference. all the full conditional distributions are of standard forms such as normal, inverse gamma, beta, and bernoulli distributions, so it is efficient to run the mcmc simulation.

the bayesian odp
multiple testing methods are typically based on p-values obtained from each hypothesis test, which only uses information from individual tests. because there is often a strong biological structure among hts tests, the measurements from different tests can be related. storey  <cit>  proposed the optimal discovery procedure  to construct a test statistic using information across tests. denote the expected number of true positives as etp and the expected number of false positives as efp. the odp is optimal in that it maximizes the etp for each fixed efp level. the method has shown significant gains in power relative to a number of current leading methods.

here is the outline of the odp. suppose there are n tests, and test i has null density fi and alternative density gi, for i =  <dig> ..., n. the observed data are x <dig>  x <dig> ..., xn, where xi corresponds to test i. then the odp test statistic is

 sodp=sum of probability of data xunder each true alternative distributionsum of probability of data xunder each true null distribution. 

because the true parameters in the null and alternative distributions are unknown, storey et al.  <cit>  proposed the canonical plug-in estimate

  s^odp=∑i=1ng^i∑i=1nw^if^i, 

where f^i and g^i are the estimates of fi and gi, w^i =  <dig> if f^i is to be included in the denominator, and w^i =  <dig> otherwise. specifically, the authors  <cit>  assumed that the expression measurements follow a normal distribution, and they proposed to plug in the constrained maximum likelihood estimates under fi and the unconstrained maximum likelihood estimates under gi. the estimates are the sample mean and sample variance under the hypothesized normal distribution. to estimate the null set, storey et al. suggested an ad hoc approach to estimate wi. first, rank the tests using a univariate statistic . second, decide a cutoff, and the tests with the univariate statistic falling below the cutoff are classified into the null set . the cutoff is chosen where the proportion of statistics not exceeding the cutoff equals the estimated proportion of true nulls based on the method in  <cit> . finally, a null hypothesis is rejected if s^odp exceeds some cutoff chosen to attain a given efp level.

the above ad hoc approach can be improved because the distributional parameters are estimated only based on information from individual tests. the posterior estimates from the proposed bayesian model allow borrowing strength across all tests, which could provide more stable estimates. we propose to use the posterior means of μi, μi + Δi, and σi <dig> to estimate the parameters of fi and gi in the odp statistic.

one way to estimate wi is to decide a cutoff on the posterior probability  of a gene being de, i.e., w^i =  <dig> if zi is greater than the cutoff  and w^i =  <dig> otherwise. storey et al.  <cit>  suggested that wi can be thought of as weights estimating the true status of each hypothesis, and they could take on a continuum of values. then another option is to set w^i =  <dig> - zi, the probability of the ith gene being non-de, which can also be interpreted as the probability of the ith null hypothesis being true. the natural introduction of the posterior probability into the odp statistic overcomes the problem of choosing an arbitrary cutoff value. it also accommodates the uncertainty in estimating the true status of each test. in this paper, we implement this second option to construct the bayesian odp statistic.

RESULTS
we conducted simulation studies and data analysis based on two experimental datasets to assess the performance of the bayesian odp. it is compared to six methods in identifying de genes: the original odp, the posterior probability from the bayesian mixture model, the shrunken t  <cit> , fox and dimmic's bayesian t   <cit> , the moderated t  <cit> , and the intensity-based moderated t   <cit> .

simulation study
we simulated data based on the estimated parameters from the hts lung cancer data set described next. specifically, we used an inverse gamma distribution to model the gene variance components. figure  <dig> plots the empirical density curves of the observed sample variances and simulated sample variances based on the inverse gamma model. the two curves are similar, except that the curve based on the observed sample variances is relatively more spiked in the center. the difference can be accommodated by assuming that some genes have a common variance around the mean of the gene-specific variances. in the simulation, we used the inverse gamma model to generate gene-specific variances σi <dig> 

 {σi2=σ <dig> if vi= <dig> σi2~ig,if vi= <dig>  

where we set the common variance σ <dig> to be the mean of the gene-specific variances. without loss of generality, we assumed that the mean expression level under control equals  <dig> . the difference in expression levels between treatment and control is specified as

 {Δi= <dig> if ri= <dig> Δi~n,if ri= <dig>  

we conducted simulation studies with  <dig> to  <dig> replicates per gene. we considered two scenarios for a given number of replicates. in scenario  <dig>  all gene variances are gene-specific; in scenario  <dig>  80% of gene variances are gene-specific and 20% of genes have a common variance. one hundred datasets were simulated under each scenario, where each dataset contains  <dig> genes with  <dig> genes being de.

we used noninformative priors so that posterior inference is dominated by the information from data. specifically, we let sμ <dig> = s^odp =  <dig>  where  <dig>  is sufficiently large for the expression levels. to specify the hyper-parameters for the inverse gamma priors, first we set aσ = a <dig> =  <dig>  so that the inverse gamma priors have an infinite variance. then we let the prior means, bσaσ− <dig> and b0a0− <dig>  equal to the average of the sample variances to solve for bσ and b <dig>  finally, we choose ar = br = av = bv =  <dig>  which corresponds to the uniform priors for pr and pv. the computation is done by gibbs sampling with  <dig>  cycles. the burn-in is  <dig> . we monitor two parallel chains with different starting points to assess convergence.

figures  <dig>   <dig>   <dig>   <dig>  and  <dig> plot the false discovery rate  versus the number of rejected genes with  <dig> to  <dig> replicates per gene. the top panel is under scenario  <dig> and the bottom panel is under scenario  <dig>  in general, the two plots in each figure show a similar pattern, indicating that the true percentage of genes having a common variance does not affect the results much. the introduction of the mixture model on variance components is useful even when all the variance components are gene-specific. in all the cases considered, the bayesian odp significantly outperforms the others, including the original odp. the posterior probability shows similar performance as the shrunken t, the moderated t, fox, and ibmt. the extra shrinkage introduced by the mixture distribution on variance components makes the full bayesian model comparable to the shrinkage and empirical bayes statistics.

in  <cit> , the odp shows significant improvement over the shrunken t statistic. however, in our simulation study, the odp has the worst performance with  <dig> replicates per gene. it performs comparably to the shrunken t with  <dig> or  <dig> replicates per gene, and it outperforms the shrunken t with  <dig> or  <dig> replicates. the reason might be that, in  <cit>  each gene was tested on a relatively large number of arrays, i.e., with six, seven, and eight replicates under three conditions, respectively. the sample mean and sample variance, which are used in the odp statistic defined in , are much more stable compared to those based on few replicates. as shown in  <cit> , the fewer replicates there are, the more the shrinkage is introduced in the shrunken t statistic. in such cases, the odp, which uses sample mean and variance, might be outperformed by the shrinkage method. as the number of replicates increases, sample variance becomes more stable, the benefit of the shrinkage becomes less significant, and the advantage of the odp statistic can be revealed.

the bayesian odp is constructed based on the odp test statistic, which has been shown to have optimal performance in multiple significance tests  <cit> . it also takes advantage of the parameter estimates from the bayesian mixture model which are more reliable than those in the original odp. when the number of replicates is extremely small, the bayesian odp might have a better performance in identifying de genes.

experimental datasets
in this section, we applied the bayesian odp to two experimental datasets. the first dataset is from a real hts experiment. paclitaxel and related taxanes are routinely used in the treatment of non-small cell lung cancer and other epithelial malignancies. the goal of the experiment is to identify gene targets that specifically reduce cell viability in the presence of paclitaxel. whitehurst et al.  <cit>  designed an hts experiment which combined a high throughput cell-based one-well/one-gene screening platform with an arrayed genome-wide synthetic sirna library for systematic interrogation of the molecular underpinnings of cancer cell chemoresponsiveness. the information on the dataset can be accessed from the nature website . the dataset was generated under two conditions . over  <dig>  genes were measured, each with  <dig> replicates. the measurements are the cell viability scores based on adenosine triphosphate  concentration.

the raw data were normalized to internal reference control samples on each plate to allow for plate-to-plate comparisons. after we ranked the genes according to the bayesian odp statistic, we employed the bayesian fdr to control multiple test errors. the posterior probability of a gene being non-de can be interpreted as a local fdr  <cit> . a direct estimator of fdr  <cit>  can be computed based on the posterior probability zi. specifically, the posterior expected fdr is

 fdr¯=e=ed|data)=∑δid, 

where d is the number of total rejections, indicator δi =  <dig> if the ith gene is identified as a hit , and δi =  <dig> otherwise. plugging in the posterior probability zi, we obtained an estimated fdr. controlling the bayesian fdr at 5%, we produced a list of  <dig> genes identified as hits.

sixty eight genes from the list were retested using the same reagent  as in the original experiment, all of which turned out to be positive, showing a remarkably high level of reproducibility. through empirical testing, the gamma tubulin ring complex  is known to modulate paclitaxel sensitivity in a broad variety of non-small cell lung cancer cell lines. thus selected genes from the complex can be considered landmark hits. the bayesian odp selected all the seven major components of the γturc . considering the same number of selected genes , the original odp produced  <dig> major components of the γturc , and the other five methods produced at most  <dig> of the major components.

without knowing the list of truly de genes, we could not compare the bayesian odp and other competing methods accurately based on the hts lung cancer data. to overcome this problem, we used the golden spike data  <cit>  to compare the bayesian odp with the other six methods included in the simulation study.

the golden spike dataset includes two conditions, with  <dig> replicates per condition. each array has  <dig>  probesets among which  <dig>  probsets have spike-in rnas. among these  <dig>  spike-in probsets,  <dig>  probsets have equal concentrations of rnas under the two conditions and  <dig>  probsets are spiked in at different fold-change levels, ranging from  <dig>  to 4-fold. compared to other spike datasets, the golden spike dataset has a large number of probsets that are known to be de, which makes it very popular for comparing differential expression methods.

there have been criticisms of the golden spike data set  <cit> . one of the undesirable characteristics is that the non-de probesets have non-uniform p-value distributions. irizarry et al.  <cit>  identified a severe experimental artifact, which is that "the feature intensities for genes spiked-in to be at 1: <dig> ratios behave very differently from the features from non-spiked-in genes". pearson  <cit>  suggested that one can use the golden spike dataset as a valid benchmark with the  <dig>  equal fold-change probsets as the true negatives instead of including the non-spiked-in probsets. as such, there are  <dig>  true positives and  <dig>  true negatives. opgen-rhein and strimmer  <cit>  proposed to remove the  <dig>  equal fold-change probsets, leaving in total  <dig>  genes, and  <dig>  known de genes. in this paper, we conducted the analysis in both cases, with the former denoted as scenario  <dig> and the latter scenario  <dig>  we used the distribution free weighted method   <cit>  as the expression summary measure.

in addition to comparing the power of the seven methods given the same number of selected genes, we also compared their ability to correctly estimate the fdr. because the null distributions of some of the test statistics  are unknown, the benjamini-hochbergwe fdr procedure  <cit>  can not be applied. we estimated the fdr by permutation analysis  <cit> . the upper panels of figure  <dig> and figure  <dig> plot the true fdr versus the number of selected genes under the two scenarios. in general, the bayesian odp outperforms the other methods in both scenarios. in scenario  <dig>  the bayesian odp has a 1% fdr when the total number of rejections is less than  <dig>  while the original odp has a zero fdr. note that the difference is caused only by one gene that is a false negative. as the total number of rejections increases, the bayesian odp has a much smaller fdr than the original odp. fox and ibmt have the second best performance under scenario  <dig> and scenario  <dig>  respectively. we provided the list of the first  <dig> genes, along with their true expression status, identified by the competing methods under each scenario in additional file  <dig> and  <dig> 

the lower panel of figure  <dig> and figure  <dig> compare the estimated fdr with the true proportion of false positives  <cit> , from which we can assess the ability of the methods to correctly establish the statistical significance of de genes. we did not include the posterior probability because its permutation-based fdr assessment is computationally intractable . all of methods in the comparison underestimate the number of false positives, which is consistent with the results reported in  <cit> . correctly estimating fdr when the null distribution is unknown remains a challenge.

CONCLUSIONS
one important feature of high throughput screening is that the number of replicates is extremely small, rarely exceeding  <dig>  full bayesian hierarchical models were shown to be less competitive compared with some existing frequentist and empirical bayes methods  <cit> . this is because full bayesian models usually employ noninformative priors which do not provide suffcient shrinkage in the estimation. in this paper, we demonstrate that the full bayesian model can be made a competitive approach by simply adding a point mass component in the variance prior. this modification introduces adequate shrinkage which improves the performance of the full bayesian model considerably. the bayesian computation is efficient. it takes about  <dig> minutes to run the for-tran program on a hp laptop  <dig> cpu  <dig>  ghz,  <dig> gb ram) to analyze the lung cancer data.

the optimal discovery procedure  is one of the current developments in multiple testing. it has shown significant improvements over many leading methods. the full bayesian model can be further combined with the odp statistic. the bayesian odp can perform better than the original odp, especially when there are few replicates in hts. the bayesian odp employs the posterior probability of a gene being de which naturally accounts for the uncertainty in the estimation of the null set. the parameter estimates in the original odp, which are the sample mean and sample variance of individual test, are not reliable with few replicates. by replacing those with the estimates from the bayesian model, the odp is improved by a joint force of shrinkage estimation and borrowing strength across tests.

authors' contributions
jc, xx, and sz developed the methods. jc and xx implemented and applied the methods. jc and sz wrote the manuscript. aw and maw provided the lung cancer hts data and tested the analysis results.

supplementary material
additional file 1
bayesian odp r code. this file contains the r code to calculate the posterior probability from the bayesian model and the bayesian odp.

click here for file

 additional file 2
list of selected de genes under scenario  <dig>  this file contains the list of the de genes, along with their true expression status, identified by the different methods from the golden spike dataset under scenario  <dig> .

click here for file

 additional file 3
list of selected de genes under scenario  <dig>  this file contains the list of the de genes, along with their true expression status, identified by the different methods from the golden spike dataset under scenario  <dig> .

click here for file

 acknowledgements
the authors thank the associate editor and the reviewers for their constructive comments and suggestions, which led to substantial improvement of the manuscript. this work was partly supported by nih grant ul <dig> rr <dig> 
