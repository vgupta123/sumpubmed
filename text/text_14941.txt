BACKGROUND
micrornas  constitute currently a major research topic in molecular biology  <cit> . these molecules are responsible for post-transcriptional regulation of gene expression, and are involved in several biological processes such as embryonic differentiation  <cit> , skeletal muscle development  <cit> , several cardiovascular disorders  <cit> , expansion of skin stem cells  <cit> , hematopoiesis  <cit> , control of proliferation and death of cells  <cit> , insulin secretion  <cit> , adipogenesis and obesity  <cit> , diseases such as cancer  <cit> , in addition to play relevant physiological roles in animals and plants  <cit> .

in mirna biogenesis, the corresponding gene is transcribed into a primary mirna  that undergoes cleavage by the drosha complex, resulting in a hairpin-shaped mirna precursor  of approximately  <dig> nt. next, the precursor is exported to the cytoplasm by the protein exportin <dig> and cleaved into the mature mirna by the enzyme dicer  <cit> .

identifying mirna loci in vivo is an expensive and complex task. as a result, computational tools have been developed to perform in silico predictions. comparative approaches, i.e., methods that use conservation of functional genomic elements of phylogenetically-close species, are a natural means for carrying out predictions, because many mirnas are well conserved among eukaryotes  <cit> . additionally, methods based on next-generation sequencing  data are of increasing interest, as they deliver good sensitivity and provide the possibility of analyzing expression profiles  <cit> . another important computational solution that complements the approaches mentioned above, and which is a good alternative for identifying species-specific mirnas, is the so-called ab initio approach.

according to tempel and tahi  <cit> , ab initio methods can be classified into three categories: 1) methods whose input is a putative pre-mirna sequence and that classify the given candidate as true or false; 2) methods whose input is comprised of a genomic sequence as well as some other information that helps to predict pre-mirnas in the given sequence; 3) methods whose input is a genomic sequence and that use no external information to predict pre-mirnas in the given sequence. tempel and tahi call the third category completely ab inito methods.

ab initio methods are very useful when no closely-related species with identified mirnas are available. even when such an information is accessible, ab initio approaches are important for predicting pre-mirnas that are specific to the studied genome. furthermore, ngs still involves a considerable cost and laboratory infrastructure that are not the reality of a significant part of research groups worldwide. our work matches the third category of ab initio methods. some important approaches in this category have been previously proposed.

vmir was conceived to predict pre-mirnas in viruses  <cit> . initially, this method processes the input sequence through a sliding window to produce subsequences with length close to the expected length of a pre-mirna. then, in order to build a putative secondary structure for each subsequence, vmir uses the software rnafold  <cit> . finally, vmir calculates a score for each pre-mirna candidate based on selected secondary structure attributes. only those pre-mirna candidates with scores above an informed threshold value are given as output.

cid-mirna applies a stochastic context-free grammar built from secondary structures of known human pre-mirnas, along with a j <dig> decision tree to predict pre-mirnas in a given dna sequence  <cit> . to learn the characteristics of negative cases in its model, cid-mirna uses human ribosomal rna.

virgo predicts pre-mirnas using a variant of the learning algorithm support vector machines  called svm light  <cit> . for training the classifier, validated human mirna precursors are used as positive instances, while negative instances are artificially produced using coding regions of genes. virgo extracts several subsequences from the input sequence, and computationally folds them with the software rnafold. a filter is then applied to select the secondary structures that match known characteristics of pre-mirnas. finally, several attributes are extracted from the selected candidates so that the svm model can assign them a final classification.

the method mirpara also applies svm for performing classification and provides the prediction of mature mirnas in addition to their precursors  <cit> . initially, mirpara breaks the input sequence into 500-nt subsequences with a 200-nt overlap. next, the subsequences are given as input to the software unafold for extracting hairpins  <cit> . many of the obtained hairpins are then discarded by the application of a filter, and the remaining ones are further analyzed with unafold. at last, several attributes related to physical characteristics of mirnas/pre-mirnas are extracted from the final set of hairpins for predicting the most likely candidates. to construct the training sets, positive examples were extracted from validated mirnas/pre-mirnas, and negative examples were artificially produced from validated pri-mirnas with the start position of their mirnas randomly shifted within the sequences.

the method mirnafold is based on a statistical approach defined from a previous analysis the authors made of pre-mirnas contained in the mirbase database  <cit> . similarly to other methods, mirnafold analyzes fixed-length sequence windows, searching for putative pre-mirnas. for each window, mirnafold constructs the most likely secondary structure without the use of third-party computational tools, in order to speed up the prediction procedure. this is accomplished by the construction of a base-pairing matrix for each window, which is analyzed in three stages. for each stage, several characteristics of parts of putative hairpins are observed according to the statistical study made previously. the final hairpins that match a certain percentage of these characteristics are given as likely pre-mirna candidates.

the method of titov and vorozheykin  proposes a context-structural markov model and a hidden markov model for ab initio prediction of pre-mirnas and mirnas, respectively  <cit> . the authors used  <dig>  human pre-mirnas from the mirbase database as positive instances, and  <dig>  random sequences from the pseudo-precursors constructed by xue and colleagues as negative instances  <cit> . tvm uses the software garna to produce secondary structures from the subsequences extracted from the input sequence  <cit> . then, several patterns of the primary and secondary structures are evaluated with the proposed models.

even though the methods described above have provided important contributions, they deliver very low selectivity , i.e., a high number of false positives  are produced, which means more work at the laboratory bench. in particular, the statistical study made for the method mirnafold identified secondary structure characteristics of positive instances only, i.e., the method was based on pre-mirnas alone. the authors did not analyze negative instances in their work. as a consequence, even reaching excellent sensitivity, large amounts of fps are produced.

in this work, we propose an extension of mirnafold, which includes negative examples and applies machine learning  techniques to improve selectivity. we compared our approach with mirnafold and all other methods mentioned above. experiments with three datasets demonstrate that we preserved very high sensitivity, while substantially increasing selectivity. our method, termed mirnacle, provided an improvement of two-fold, 20-fold, and 6-fold in selectivity, for the respective datasets, compared with the best results of the other approaches.

methods
our aim is to predict new pre-mirnas from a dna sequence without using any additional information . for each subsequence extracted from the input sequence, mirnacle executes three stages to try to build the most likely hairpin structure. each stage classifies parts of the hairpin that is incrementally built. the steps are similar to the mirnafold approach. one major difference is that we consider negative examples in addition to positive instances . another important distinction is the use of ml techniques to perform a more sophisticated classification, where each stage has its own model, with the aim of minimizing the number of false positives.

ab initio prediction
similarly to many methods in the ab initio category, mirnacle applies a sliding window to the informed dna sequence to obtain subsequences that may represent pre-mirnas. for each extracted subsequence, the goal is to fold it into a typical pre-mirna hairpin structure. third-party programs are often used for this end. however, analogously to mirnafold, putative foldings are represented here by a triangular base pairing matrix, and the most likely hairpin is obtained from a three-stages analysis of this matrix. figure  <dig> illustrates the general structure of our method by showing an example of such a matrix.
fig.  <dig> general view of the mirnacle approach . given the input dna sequence a, a sliding window is used to extract subsequences of length close to the expected pre-mirna length. for each subsequence, a triangular base pairing matrix  is constructed and analyzed in three stages. in the first stage , long exact stems  are sought and classified. next, in the second stage , for each positively classified exact stem, its diagonal is searched to form a non-exact stem  that also passes through a classification procedure. finally, in the last stage , a complete hairpin is produced from each previously filtered non-exact stem, using the originally identified exact stem as the starting point for a further search in the matrix. in this search, other diagonals are tried so that secondary structures with asymmetrical internal loops are also considered. the resultant hairpins are then classified with a third ml model and only the ones predicted as positives are given as the final output 




to build the matrix for a given sequence s, a column i represents the character s, a row i represents the character s, and an entry i,j is a positive integer number if the corresponding bases are complementary, or zero, otherwise. the positive numbers indicate the extension of the paired region. algorithm  <dig> clearly describes how the base pairing matrix is constructed. lines 4- <dig> initialize the first column and the first row, while lines 16- <dig> fill the other entries.





the general idea is to build secondary structures incrementally. minor parts of possible hairpins are identified and then extended to form the complete structures, as can be seen in fig.  <dig>  initially, long regions of paired bases, termed exact stems, are sought . next, the long exact stems found in the previous step are extended to non-exact stems, i.e., parts of a hairpin composed of paired bases interposed between unpaired regions . these unpaired regions are symmetrical loops whose size is less than the length of the surrounding exact stems. the extension of an exact stem is achieved by taking into account only its diagonal in the base pairing matrix. each resultant non-exact stem is considered a good approximation of a hairpin and is thus used at the last stage as the basis for achieving a pre-mirna secondary structure . for each non-exact stem, the exact stem that gave rise to it is fixed and other diagonals are explored to make possible the occurrence of asymmetrical internal loops .
fig.  <dig> illustration of the incremental approach performed in the base pairing matrix analysis. a a long exact stem  is identified . b the exact stem is then extended to a non-exact stem  that, in turn, is the basis to build a complete hairpin 




our main contribution here is the application of ml techniques with the objective of minimizing false positives. it contrasts with the verification of a list of criteria performed in mirnafold. it is important to notice that each stage has its own ml model. for example, there is a specific model to apply on all exact stems of a minimum predefined length found in the first stage . only those instances considered as positives, i.e., for which the model assigns a probability value greater or equal to a predefined threshold, are given as input to the second stage. the non-exact stems produced from the positively classified exact stems, in turn, are classified with another ml model , and only the instances regarded as positives pass to the next phase. in the last stage, a third ml model is used to classify  the resultant hairpins to report the final predictions.

the three-stages procedure described above is repeated for each sliding window subsequence. at the end of each analysis, the sliding window is moved  <dig> nt downstream, as proposed by tempel and tahi  <cit> . this process continues until the end of the given dna sequence.

datasets
to validate the proposed method, three datasets used in the experiments of mirnafold were also used in our work to facilitate the comparisons. one of the datasets is an artificially constructed sequence obtained from  <dig> human pre-mirnas interposed between human mrnas, leading to a sequence of  <dig>  nt. the other two datasets are real genomic data containing clusters of pre-mirnas. the first one is a sequence extracted from the positive strand of the human chromosome  <dig>  starting at position  <dig> , <dig> and ending at position  <dig> , <dig>  containing  <dig> pre-mirnas. the second sequence was obtained from the positive strand of the mouse chromosome  <dig>  starting at position  <dig> , <dig> and ending at position  <dig> , <dig>  comprehending  <dig> pre-mirnas. these three datasets are referred throughout the text as had , hsd , and mmd , respectively.

the sequences from zebrafish and sea squirt chromosomes used in the mirnafold work were not included in our experiments because we could not match the stretches cited by the authors to the data available in genbank. furthermore, to our knowledge, there were no validated pre-mirnas in the mirbase database for these two species at the moment we performed our experiments. details about the datasets can be found in the work of tempel and tahi  <cit> .

to build the learning models, three independent datasets to produce training sets  were constructed: tsha, tshs, and tsmm, one for each experiment with the three test sets: had, hsd, and mmd, respectively. positive examples were obtained from experimentally validated pre-mirna sequences present in mirbase release  <dig>  <cit> . for a fair model evaluation, all pre-mirnas contained in the test sets were eliminated from tsha, tshs, and tsmm. therefore, the positive instances in tsha were a result of all validated human pre-mirnas in mirbase subtracted by the pre-mirnas present in had. similarly, tshs’s positive instances were obtained after subtracting the pre-mirnas in hsd from the set of validated human pre-mirnas. finally, the positive instances of tsmm were the result of the validated mouse pre-mirnas in mirbase minus the pre-mirnas in mmd. as a consequence, tsha, tshs, and tsmm contain  <dig>   <dig>  and  <dig> positive examples, respectively.

negative examples, in turn, were comprised of other types of non-coding rnas along with pseudo hairpins. gene sequences of snrna , snorna , trna, and miscrna  were extracted from genbank and the nrdr repository  <cit> , totaling  <dig>  sequences from the h. sapiens genome and  <dig>  sequences from the m. musculus genome. moreover,  <dig>  pseudo pre-mirnas from the work of xue et al. were considered to compose the h. sapiens tss  <cit> . therefore, tsha, tshs, and tsmm contain  <dig> ,  <dig> , and  <dig>  negative examples, respectively.

notice that each stage has its own ts to build its particular model. therefore, each of the datasets tsha, tshs, and tsmm led to three tss that are identified along the text with the indexes  <dig>   <dig>  and  <dig>  e.g., the dataset tsha gave rise to the tss: tsha <dig>  tsha <dig>  and tsha <dig>  one for each respective stage. figure  <dig> illustrates this procedure. in this work, the tss are in the arff format. this is the native format of the weka machine learning toolkit  <cit> , whose application programming interface  is used in the mirnacle implementation.
fig.  <dig> training set construction. each stage has its own training set for building its specific machine learning model. real pre-mirnas are used as positive examples, while negative examples are comprised of snrnas, snornas, trnas, miscrnas, and pseudo hairpins




features
another distinctive aspect of mirnacle regarding mirnafold was the inclusion of other features to improve the discriminatory power of the ml models. besides the features proposed by tempel and tahi, we also considered dinucleotides, mfe   <dig>  and mfe  <dig> from the work of batuwita and palade  <cit> , as well as triplets from the work of xue et al.  <cit> . furthermore, the feature “size of the internal symmetric loop” was replace to other three features: number of internal symmetric loops, average size of internal symmetric loops, and max size of internal symmetric loops. still, the feature “size of the biggest side of the biggest bulge” was replaced to other two features: size of the biggest bulge to the left of the terminal loop and size of the biggest bulge to the right of the terminal loop. considering that dinucleotides and triplets mean  <dig> and  <dig> additional features, respectively, our approach includes  <dig> extra features. notice that a triplet, according to xue and colleagues  <cit> , denotes whether each nucleotide in a consecutive sequence of three nucleotides is paired, represented by the symbol ‘, and random forest . our intention was to perform some experiments varying the combination of methods for imbalanced data with these learning algorithms to select the most appropriate approach. the results shown in the next section demonstrate that the best strategy among the possible combinations is smote + rf.

to deploy all these ml methods in our computational tool, we used the api of the weka machine learning toolkit  <cit> , version  <dig> . <dig>  that implements all algorithms and techniques mentioned above. in particular for the svm approach, we tested two implementations provided in the weka api: sequential minimal optimization  and libsvm.

complexity analysis
the analyses of time and space presented here do not consider the phase of ml model construction, because it is performed once and the resulting model is applied as much as needed.

concerning space, given an input sequence of size n and a sliding window of size m, the space required by the algorithm as a function of these variables has complexity θ. notice that it is necessary to store the whole sequence and the triangular base paring matrix. for large input sequences, such as an entire chromosome or even a whole genome, it is clear that n≫m. in this case, we can consider the space complexity as θ.

regarding time, for the same variables above, we can assume n window subsequences of size m to analyze, i.e., n executions of the three-stages pipeline, and an m×m matrix to explore in each case. considering an extreme scenario in every stage where each of the m
 <dig> entries of the matrix has to be processed, the matrix scanning has complexity o, because for each position in the matrix, it is needed m additional entry accesses to process the respective diagonals. therefore, the whole algorithm time complexity is o.

RESULTS
experiments were performed on a linux machine, intel core duo  <dig> t <dig>   <dig>  ghz, and  <dig> gb of ram. considering we used the same platform and processor reported in the mirnafold work, we could directly compare our running times with the times the mirnafold authors reported for their work and for other methods.

to evaluate the ml algorithms selected for classification as well as to compare our approach with other previously proposed methods, we report sensitivity  and selectivity  in our experiments, which are the same statistical measures used by tempel and tahi  <cit> , and are given by: 
 sn=100×tp,sl=100×tp, 


where tp is the number of true positives, fn is the number of false negatives, and fp is the number of false positives. furthermore, the geometric mean  of sn and sl is provided in each experiment, as suggested by gudyś et al. for the case of imbalanced data  <cit> .

deciding the ml methods to be part of mirnacle
in order to decide the best method among the ones selected for treating the imbalance problem, all combinations of these methods with the already mentioned ml approaches were tested using tsha <dig>  tsha <dig>  and tsha <dig>  we kept the algorithms’ parameters with their default values .
table  <dig> comparing different combinations of methods for imbalanced data and learning algorithms. a 10-fold cross validation was performed in each case using tsha <dig>  tsha <dig>  and tsha <dig> for the respective stages

 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 



further experiments with tshs <dig>  tshs <dig>  and tshs <dig>  as well as tsmm <dig>  tsmm <dig>  and tsmm <dig>  demonstrated the same. this time, we fixed smote as the technique to cope with the imbalance problem, and varied the ml algorithm. observing the best gm values  in table  <dig>  it can be noticed that the smote+rf approach is, in fact, a suitable choice to integrate the mirnacle method. rf achieved the highest values for gm in all stages for both organisms, i.e., it provided the best compromise between sn and sl. mlp also showed good results in the third stage. however, it presented worse gm values for the other phases compared with rf. moreover, the training time with mlp is much longer than with rf .
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 



comparing mirnacle with other methods for pre-mirna ab initio prediction
after defining the best ml approach to use, we could conclude our computational tool and compare it with other previously proposed methods for pre-mirna ab initio prediction. in our experiments, only ab initio methods of the third category, mentioned earlier, were considered.

the mirnacle parameters are: minimum exact-stem size, sliding window size, the probability threshold of each of the three stages, minimum pre-mirna size, and maximum pre-mirna size. in all experiments, the minimum exact-stem size, the sliding window size, the minimum pre-mirna size, and the maximum pre-mirna size were set to  <dig>   <dig>   <dig>  and  <dig>  respectively.

similarly to the experiments performed for mirnafold, appropriate probability thresholds for the three stages were established using the artificially created dataset had. notice that rf produces the probability of an example being positive, instead of a binary output. this is very useful because the discriminant probability can be used according to a particular goal. hence, if sensitivity is more important, then a low threshold should be used. on the other hand, if selectivity is the priority, e.g., to minimize laboratory validations, then a high threshold is more appropriate. it is necessary to define the thresholds of each model used for each stage. for this end, we tried several combinations of thresholds  and selected five of them  that led to different values for sn and sl: ; ; ; ; and . the triple  resulted in the best gm  and was used to the comparisons made with datasets hsd and mmd .
 <dig> 

 <dig> 
 <dig> 



the results shown in tables  <dig> and  <dig> for tvm were taken from its publication  <cit> , while the results for mirnafold, mirpara, cid-mirna, and vmir were obtained from the work of tempel and tahi  <cit> . using the same criterion as the authors of mirnafold, a predicted pre-mirna is considered true if the distance from its center to the center of the known hairpin is less or equal to 10% of the size of the latter.

it can be seen in table  <dig> that the best gm value of mirnacle was  <dig> , representing an increase of approximately 44% regarding the best result among the other methods, namely, gm =  <dig>  produced by tvm. concerning selectivity, which is the main bottleneck of the previously proposed approaches, mirnacle could deliver a selectivity of  <dig>  that means more than a two-fold increase compared with tvm. notice that a high sensitivity value was kept by mirnacle. in table  <dig>  the five results shown for different thresholds for the three stages can serve as a reference to threshold values to be used depending on a particular objective. if one is interested in high selectivity to reduce biological validations, the combination  is a good option, for instance.

concerning the running time, mirnacle’s performance was highly variable. this is because low thresholds in the first and the second stages mean a less strict filter of exact and non-exact stems, i.e., the third stage will contain more sequences to extend to a complete hairpin, which leads to a longer running time. high thresholds in the first and second stages, on the other hand, mean fewer non-exact stems to expand at the end, speeding up the process. notice that the matrix exploration in the third phase for inspecting different possibilities of a complete hairpin is the most computationally-expensive part. comparing the mirnacle execution that took  <dig> minutes and  <dig> seconds with the running time of other methods, mirnacle could only overcome cid-mirna. however, considering that we substantially improved selectivity, the time saved in laboratory experiments is likely more significant. it can be seen that the time taken by tvm was not reported. the reason for this is that the authors do not mention any experiment to measure time. furthermore, tvm is available only as a webserver, which makes infeasible to measure its running time in a fair way.

CONCLUSIONS
in this work, we propose an extension of the mirnafold method for pre-mirna ab initio prediction to address the low selectivity issue of mirnafold and other ab initio approaches. our experiments have shown that our method, termed mirnacle, substantially increased the selectivity compared with previously proposed procedures, whereas keeping high sensitivity.

to achieve these results, the main improvements implemented in mirnacle were: the analysis of negative training examples, in addition to positive examples; the use of machine learning techniques, namely, smote combined with random forest; and the inclusion of other important hairpin features. furthermore, mirnacle allows the user to provide positive and negative examples to generate new models, which results in great flexibility in the use of our computational tool, i.e., as long as appropriate training examples are available, mirnacle can be, in principle, used for other organisms of interest.

as a future work, we intend to improve mirnacle’s running time. first, the calculation of mfe and deltag will be part of mirnacle’s code, in order to eliminate calls to the vienna rna package. second, and most importantly, mirnacle will implement parallel approaches, such as gpu , as the analyses of the subsequences in a genome are completely independent. therefore, the parallelization of such analyses can certainly bring a huge gain in performance.

abbreviations
apiapplication programming interface

fpfalse positive

gmgeometric mean

gpugraphics processor unit

hadhuman artificial data

hsd
homo sapiens data

mirnamicrorna

mlmachine learning

mmd
mus musculus data

mfeminimum free energy

mlpmultilayer perceptron

miscrnamiscellaneous rna

ngsnext-generation sequencing

pre-mirnamirna precursor

pri-mirnaprimary mirna

rfrandom forest

svmsupport vector machines

snrnasmall nuclear rna

snornasmall nucleolar rna

smosequential minimal optimization

snsensitivity

slselectivity

tstraining set

