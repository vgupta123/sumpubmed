BACKGROUND
metagenomics is a relatively recent field of research, dealing primarily with the investigation of microbial consortia of uncultivable organisms. it has enabled the study of microbiota sampled directly from environmental niches, such as the ocean  <cit> , soil  <cit> , hot springs  <cit> , hydrothermal vents  <cit> , polar ice caps  <cit>  and hypersaline environments  <cit> . in depth investigation of these consortia have given insight into microbial ecology  <cit> , diversity  <cit> , as well as archeal lineages  <cit> . not only is such knowledge valuable to the understanding of our biosphere, it has also facillatated advancement of biotechnology  <cit> , human physiology  <cit>  and sequencing of contaminated samples of now extinct species  <cit> , to name a few. prior to the metagenomics approach, we see that attempts at characterising microbial communities using pure clonal samples of constiuent organisms resulted in a low discovery rate of novel microbes  <cit> .

metagenomics is able to tackle this problem by means of direct sequencing of an environmental sample without the need for a pre-cloning step. this enables approximately 99% of earth's undiscovered microbiota, which resist standard laboratory culturing techniques, to be sequenced and analysed. however, when an environmental sample is sequenced en masse, a fundamental computational challenge is introduced: the characterisation of sequenced reads with respect to their phylogenetic origin  <cit> . such in silico profiling of sequenced dna is referred to as binning.

binning is an important first step to further downstream analysis of a metagenome. of particular interest in this preliminary stage of analysis is the taxonomic composition of the sample, and further, the association between sequenced dna fragments and their parent organism. many reported attempts at this analysis are founded on one of three key concepts: marker gene based assignment, sequence similarity assignments or sequence composition based assignments.

taxonomic profiling using conserved marker genes
through various stages of an organism's evolution changes take place in the composition its genome, permitting adaptation to changes in the environment, for example. different locations in the genome experience distinct rates of change. hyper-variant regions are particularly found in non-coding, inter-genic regions  <cit> . this is because pronounced changes in genes that code for particular functions will degrade characteristic functionality of an organism. the exceptions to this are slowly evolving marker genes in the guise of non-coding ribsomal rnas. these conserved marker genes have been fundamental to the study of microbial phylogeny  <cit> . prior to the discovery of such marker genes, phylogenetic analysis of microbes revealed the existence of only two primeval lineages. however, a founding study  <cit>  highlighted the insuffciency of existing approaches to capture all extant lineages. it was this which lead to the establishment of three primary kingdoms or domains of archea, bacteria and eucarya.

more recently, studies that adopt this approach have greatly contributed to our knowledge of the actual diversity of microorganisms  <cit> . automatation of the phylotyping procedure for metagenomic dna using 16s rrna markers are also becoming more prevalent  <cit> . in  <cit>  two fundamental questions are posed regarding the number of bacterial phylotypes that can co-exist and the way in which they are organised, and an attempt at addressing both has been through 16s rrna sampling of a bacterioplankton assemblage. as  <cit>  argues, these two pieces of information are critical to the understanding of function, population biology and biogeography. further,  <cit>  uses 16s rrna libraries to compare the phylogenetic distance between various microbial communities.

on the one hand where such marker genes are providing valuable insight into the composition of microbial assemblages, they carry with them the inability to characterise the majority of sequenced reads. it has been reported that these marker genes appear infrequently in a typical set of sequenced reads, despite the high density of open reading frames found in microbial genomes  <cit> . consequently, only a fraction of reads can be assigned.

read length is also a factor to consider when attempting classification using marker genes. 16s rrna gene are generally  <dig>  base pairs in length and as such will be distributed over multiple reads of current state-of-the-art sequencing technologies. complications then arise in taxonomic profiling when marker genes are assumed to be partially located on sequenced reads. however, this is not so much a critical issue for future sequencing platforms, as whole-molecule sequencing is designed to deliver sequenced reads that are in the order of length of the marker genes  <cit> . marker genes will continue to be used for taxonomic characterisation of a metagenome, as it is arguably the most accurate  <cit> .

characterisation based on similarity to known sequences
a viable alternative to the use of conserved regions of genomic dna is the use of previously sequenced homologs as a basis for phylogenetic characterisation. databases of complete genomes and information on genes, protein families and so forth have experienced exponential growth, especially over the last few decades, attributed in part to advances made by marker gene based profiling of microbial organisms. tapping into this resource pool has been the focus of a variety of methods. for instance, early similarity methods used a simple blast  search against databases of previously sequenced complete genomes to assign short fragments to a particular taxonomic rank, using homologs  <cit>  or orthologs  <cit> . currently, the length of reads is an important challenge  <cit>  and methods that are able to assign short stretches of dna are required, particularly where next generation sequencing platforms are used to sequence metagenomes. in fact, claims have even been made to a solution for phlyotyping a metagenome irrespective of the sequencing technique used  <cit> .

the strength of these methods relies on the length of sequences able to be assigned. we see that recent attempts achieve accurate classification of reads down to  <dig> base pairs in length  <cit> . despite this, for real metagenomic samples, fewer than 10% of the reads could be identified using the pfam domain and protein families  <cit> , which is suggestive of poor database coverage over extant microorganisms. of this 10%, only a fraction could be assigned to a particular lineage.

in general, a bottleneck exists in conducting the initial search against various databases, often necessitating the use of a large number of cpu hours on high performance computing solutions. as these databases continue to grow at their current rate, this bottle-neck will increasingly impose significant delays in any metagenomic project, perhaps with minimal pay-off for the classification of novel organisms. however, by adopting these strategies we are fundamentally relying on the assumption that novel microbes sampled from the environment will be represented in existing databases. with an abundance of microbial diversity yet to be discovered, it is counter-intuitive to found decisions on previously discovered genomes and protein sequences  <cit> , particularly when the majority of these are derived from culture-dependant techniques, while an estimated 99% of novel microbes are yet to be discovered can not be cultured using current in vivo techniques  <cit> .

characterisation based on sequence composition
the composition of a dna sequence is defined by the non-random ordering of its base-pairs, in terms of the four atomic nucleotides. taking into account that there are specific causes and evolutionary factors for the variation in base composition of genomic dna, methods have been developed to extract common patterns between organisms at varying levels of taxonomic resolution, such that sequences of similar species are able to be grouped. the general trend with compositional approaches has been the modification of machine learning methods to work around existing compositional feature spaces. deterministic pattern spaces such as oligonucleotide frequency counts are among the more dominant of choices among compositional binning methods. these operate on the assumption that the relative abundance of certain words - also referred to as oligos, i.e. acgta is a 5-mer oligo - primarily dictate the association of one sequence to another. this is particularly useful for observing codon usage biases, for example. in fact, compositional biases, particularly in the case of tetranucleotide  frequencies, is hypothesised to have strong biological significance in terms of phylogeny  <cit> . it is further argued that the larger number of permutations possible in tetranucleotide frequencies allows greater authority to discriminate between genomic fragments from different genomes. this argument holds if the conditions of low intragenomic variation and large sequence length  hold. due to the large number of combinations of oligos possible, tetranucleotide frequency has been reported to have greater discriminatory power than the g-c content of a sequence, for instance  <cit> . clustering tetranucleotide frequencies using fixed-size self-organising maps  has been shown to be possible  <cit> . however, the imposition of a fixed size som has been attributed to feature maps that do not faithfully represent the input data, and as such the growing self-organising map has been used to alleviate this flaw  <cit> .

in the context of current next generation sequencing technologies, we find that reads are as short as 80- <dig> base pairs. in light of this, methods that operate on nucleotide frequency alone are at a disadvantage, as a signal strong enough to make inferences on phylogenetic origin of a sequence requires long stretches of dna. it has been shown that  <dig>  bp is an acceptable sequence length to make accurate predictions  <cit> , yet it has also been shown that sequences as short as  <dig>  bp can be classified  <cit> . as yet, the  <dig>  bp barrier - as it has been colloquially termed in the literature - is still an open challenge. to counteract this limitation, methods that adopt nucleotide frequencies as a means of sequence representation typically operate on assembled contigs. however, for complex communities the required amount of coverage for modest assembly translates to a substantial sequencing requirement. the feasibility, therefore, for current composition based methods looks to be limited to microbial consortia with minimal to moderate diversity. further, these methods compensate for weak phylogenetic signals by consolidating information from other sources, such as external databases. methods have also used training data from potential homologs from public databases to construct a representative signal for a particular clade  <cit> . in the same study it is also shown that it is possible to generate training data directly from the metagenome itself, but it is argued that at least  <dig> kbp of data is required to construct an accurate model for a particular organism at a particular taxonomic rank.

the literature suggests that significant advances in compositional approaches to the binning problem have primarily looked at the issue of representing the composition of a sequence, rather than refining machine learning methods that operate in a sub-par feature space. such is the case with the succession of g-c content by tetranucleotide frequencies  <cit> , for example. there is much that can be unveiled when patterns are extracted from dna sequences, and since compositional methods are generally database-independent they are not susceptible to cloning biases observed in similarity-based methods. it is, however, a matter of knowing what patterns to extract and how best to extract them. here we propose the use of the oligonucleotide frequency derived error gradient  as a feature space for the characterisation of dna sequences from isloate organisms. the proposed feature space relies on the concept of oligonucleotide frequency profiles and their demonstrated ability to characterise genomic dna.

RESULTS
evaluation on simulated metagenomic data sets
data set description
recently published metagenomic benchmark data sets  <cit>  have been selected to evaluate the binning performance using our proposed dna sequence representation. the benchmarks were formed using real dna sequences of  <dig> isolate microbial genomes, sequenced at doe joint genome institute. the dominant strain in the simlc set is given a coverage of approximately  <dig> × and just over  <dig> mbp in total sequence length. the dominant strain is anked by  <dig> lower abundance strains, with coverage less than  <dig> ×. simmc introduces three dominant strains which are represented with coverages ranging from  <dig> × to  <dig> ×. characteristic of agricultural soil, the simhc data set contains no dominant strains, and has poor coverage. the highest organism coverage in this data set is at an estimated  <dig> ×.

evaluation procedure
as previously described, the simulated metagenome data contains three data sets designed to represent three microbial communities of varying degrees of complexity. here we present the analysis of binning performance on the medium complexity simmc data set, as this serves as a basis for comparison against current compositional binning techniques. similar to  <cit> , we conduct two tests to evaluate the quality of binning using ofdeg. the first takes only contigs which are greater than  <dig>  bp in length - as these were deemed to have a minimal degree of chimerism. the second takes major contigs, which are those that are assembled using at least  <dig> reads. we use both the assembler output of arachne  <cit>  and phrap for evaluation, where phrap produces shorter contigs but is deemed more reliable  <cit> . contigs generated by jazz were excluded from the analysis  <cit> . similarly, for the purposes of comparison we restrict ourselves to the taxonomic rank of order, using ncbi's taxonomy definition. for all tests conducted, ofdeg values were computed on the basis of a 4-mer of profile for comparison against tetranucleotide frequency  recommended by  <cit> . the sampling depth was set to  <dig> and a step size of 10% of the sequence length was used. each ofdeg value for a sequence is an average over  <dig> - determined empirically - subsequences which were truncated to the length of the shortest contig for each test, given the criteria of selecting contigs as defined above. all the ofdeg values computed used 80% of the sequence length. in an attempt to increase the discriminatory power of ofdeg for sequence separation, we also consider the effect of ofdeg in conjunction with g-c content as it has been used previously to successfully characterise organisms and maintains the low dimensionality of the proposed feature space.

measuring the accuracy
accuracy is measured by how well a particular organism is characterised. with respect to each organism, the bin that is identified as containing the maximum number of fragments for that particular organism is considered as its reference bin. fragments assigned to the reference bin that are of the reference organism are deemed the true positives. similarly, fragments contained in the reference bin that are not of the reference organism are false positives. fragments of the reference organism that are located outside the reference bin are considered false negatives and lastly, fragments that are not of the reference organism and located outside the reference bin are considered true negatives. using these interpretations we use the standard definition of sensitivity and specificity to evaluate the quality of binning.

for the semi-supervised case we look at the label assigned to each fragment and compare this assignment to its true origin. for each class of organism, at a specific taxanomic rank, we look at how many of those fragments have been assigned correctly using the same definitions as for the unsupervised case. fragments that are not assigned a label are not included in the calculation.

unsupervised setting
we first evaluate the fidelity of ofdeg by clustering computed ofdeg values in an unsupervised manner. for compositional methods, the relative abundance of oligonucleotide frequencies and inherent biases therein have been linked to a sequence's phylogenetic origin. raw ofdeg values are clustered using partitioning around mediods  <cit> , and the average silhouette width is used to compute the most representative number of classes based on the clustering structure.

in general, it is clearly evident that clustered ofdeg values in conjunction with g-c content improves on the performance of tetranucleotide frequency alone . the lower sensitivity value for tetranucleotide frequency can be explained by the less defined clusters that result, where fragments that are associated with one genome type are distributed across multiple bins.

as the reference sequence length for ofdeg calculation must be the same for all sequences in order to produce meaningful comparisons, the minimum contig lengths in the  <dig> reads tests highlighted the benefits of using ofdeg. for the phrap assembled data, the minimum contig length is only  <dig> bp, yet we see that the binning fidelity is competitive with features that require the entire sequence length. with an increase in minimum contig length for the arachne assembled data to  <dig> bp, again for the  <dig> reads test, the binning performance increases. this is even more prevalent when considering the  <dig> kbp tests, which have a minimum contig length of  <dig> kbp, where a near perfect separation of the two out of three dominant species is observed.

semi-supervised setting
in this setting we require the use of a minimal amount of annotated data which we will refer to as labels, a basis for supervised learning. using labels, we are able to investigate the possibility of a performance increase using existing knowledge of sequenced microbial dna, an observation described in  <cit> . here we also use the s-gsom algorithm  <cit> . fixed-sized soms were not used, as the imposition of a fixed sized map may result in incorrect representation of the input space, and subsequently poor clustering. note, this was used only as a comparison of features, and not a contribution as a novel clustering technique. similar to  <cit>  we select as labels  <dig> kbp flanking regions of the conserved 16s rrna gene, subject to rules also defined therein. ofdeg values are computed for each flanking sequence of 16s rrna genes found in the genomes used in the simulated data sets. these values were augmented with the processed data sets as seeds. additionally, topology type, similarity measure, weight initialisation type, neighbourhood kernel, initial learning rates, and training epochs for s-gsom were selected to be the same. for comparison, we also evaluate classification performance at cluster percentages  of 55% and 75%, as these parameter settings are recommended in  <cit> .

particularly for the  <dig> kbp tests, we see an improvement in performance over a purely unsupervised attempt at characterising fragments, achieving a sensitivity and specificity of  <dig> for both assemblers . as is appreciated in purely clustered data, there will be ambiguity in assignment at the edges of clusters, i.e. does fragment x, which lies directly in between the centres of clusters a and b, belong to cluster a or b? in selecting only those sequences that will give confident predictions, avoiding the previously mentioned situation, the accuracy of binning is increased. in this case, the number of fragments assigned are reduced as a result. tests using ofdeg for semi-supervised classification at higher cp values  showed a decrease in accuracy, as spurious assignments were made. the increase in performance experienced by s-gsom using tetranucelotide frequency, on the other hand, can be explained by seed location within a fully trained map.

to be able to use semi-supervised learning, we require an indication of the diversity of the metagenome, perhaps through targeted sequencing of the 16s ribosomal rna. given this, the flanking sequences are used as reference classes for binning. however, reference genomes would unlikely be available, given the current knowledge of microbial diversity. the accuracy is then determined by the correct identification of the taxa that is actually present in the data set. this, however, is not a requirement for binning, but merely a means to get a sense of species composition. binning is still able to proceed without such a priori knowledge.

overall perfomance and discussion
in order to capture the overall relative performance of both ofdeg and tf feature spaces, we compute the discriminant ability of each using averaged sensitivity and specificity values over all tests conducted. with reference to  we see that both the unsupervised and semi-supervised methods which use ofdeg+gc as a feature space perform best overall with respect to the four different simmc tests. though the semi-supervised method outperforms the unsupervised method, the average number of assignments made by the unsupervised variant is far greater. if labels are available, we are able to classify fragments with approximately 96% accuracy. however, in the case where labels are not present, unsupervised methods can be applied using sequence ofdeg values with 95% accuracy, but at the same time with higher coverage:  <dig> % of sequences in the data set as opposed to  <dig> % for the semi-supervised case.

computed ofdeg values using 5-mer oligos for  <dig> microbial genomes selected arbitrarily from ncbi. the values listed here represent the sample mean plus or minus the sample standard deviation, averaged over  <dig> sampling sites over the entire length of the genome. the differences reflect the strength of the signal in relation to sequence length.

an important consideration is data dimensionality. for instance, projecting a sequence into tf space results in the generation of input vectors of dimension  <dig> for each sequence in the data set. with a large number of sequences, the computational cost of clustering such data will be taxing, even more so with higher order oligos. alternatively, ofdeg is a consistent, one-dimensional feature space - two at most, when used in conjunction with g-c content - irrespective of the underlying word size, which will be beneficial in the initial analysis of metagenome data. we appreciate that seeding posseses the capacity for a more accurate classification of environmental dna fragments. however, care should be taken when using this approach. as noted earlier, this setting relies on the premis that the 16s rrna flanking sequences are representative of the biota in the sample and are available for each species in the metagenome. using seeds which are incorrect will degrade the classification performance, even if the clustered data is structured correctly.

practical applications of using ofdeg should take into consideration the following. current next-generation sequencing technologies are producing output at higher rates and shorter read lengths. the method proposed operates under the assumption that some assembly has been carried out on the raw sequencer output. akin to compositional methods, sufficient sequence length is required to make inferences based only on the ordering of base-pairs. however, this compositional feature appears to make a stronger association between phylogeny and sequence composition given shorter strecthes of dna. of additional importance are repeated regions in genomic dna. these will be captured and reflected in the computed ofdeg values, which will be lower in comparison to other seqeuneces. for the detection and removal of, say, redundant repeats, a preprocessing tool could be used to remove these prior to ofdeg computation. this, however, is beyond the scope of this work. we emphasise that this work serves to describe the observation of a characteristic linear gradient and its potential application. although we are unable to fully explain its theoretical underpinnings or provide an in-depth biological interpretation, we are empirically able to show that it does have links to microbial phylogeny.

CONCLUSIONS
here we have presented a novel representation of short dna sequences, derived from oligonulceotide frequency profiles, which allows for the phylogenetic characterisation of relatively short sequences on the basis of sequence compostion alone. although we have found that microbial phylogeny is potentially captured in ofdeg, we aim to develop a theoretical framework as well as ellicit its biological meaning. unsupervised clustering revealed ofdeg related features performed better than standard tetranucleotide frequency in representing a relevant organism specific signal. the extension to a semi-supervised paradigm again demonstrated an improvement in binning performance when using ofdeg values. in light of the issues faced with semi-supervised classification of ofdeg values, an interesting prospect for future work is the analysis of seed selection and its influence on the accuracy of fragment classification, especially for data sets which contain short contigs.

expressing each sequence in an appropriate feature space is more beneficial than developing intricate machine learning methods that wrap around feature representations that do not capture phylogenetic signals in short sequences. there is a pressing need to break away from reliance on assemblers that were designed to handle single genomes, built without consideration for processing significantly different, heterogeneous metagenome sequence data. addressing the fundamental issue of a suitable representation for short dna sequences has shown potential as a first step toward unbiased, database-independent characterisation of metagenome data and novel microbiota.

