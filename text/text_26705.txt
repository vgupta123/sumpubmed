BACKGROUND
the first complete genomic sequence of a living organism, the bacterium h. influenzae, was obtained in  <dig>  ten years after, the number of fully sequenced genomes is steadily increasing: more than  <dig> bacterial and archeabacterial genomes and  <dig> eukaryotic genomes are presently available in public databases. however, the availability of the sequence is merely a starting point. the real challenge actually consists in interpreting and annotating the genomic text. when annotating a genome, biologists are especially looking for the genes, i.e. the regions of the chromosome containing the information to produce proteins or rna, as well as regulatory signals. finding all genes and regulatory signals on a complete raw genomic sequence is still an open problem, especially in the case of eukaryotic genomes where the coding regions are interspersed with non-coding regions called introns. moreover, finding genes and signals is just the first step of the process. once this has been done, the biologist should face the question to assign a putative function to the gene's product. this is done, for instance, by scanning databases of known proteins in order to pickup those that most resemble the protein to identify. finally, once all these information have been collected, new and more complex questions arise, such as positioning the protein within its metabolic or gene regulation networks. all these steps compose the annotation process and involve computer programs as well as a lot of human expertise.

genome annotation can be seen as an incremental, cooperative, data-driven, knowledge-based process  <cit> . the prediction of coding regions and regulatory signals requires the application of multiple methodsand the adequate combination of their results  <cit> . for instance, markov chain models can be used to compute the protein-coding probability of a genomic region  <cit>  and pattern matching or statistical methods will help locating intron-exon junctions and signals. moreover, results of proteic or nucleic database scanning will usually be superimposed to these predictions, together with any kind of additional information supporting the predicted gene structure. biologists will retain a gene prediction by confronting these various results and adding their own expertise.

the annotation is thus a long and tedious interpretation process. moreover, it might have to be executed more than once and subjected to revisions. first of all, sequencing errors may be reported or manual corrections may be supplied by experts and will ask for the reannotation of the corrected regions. moreover, as new prediction methods appear, they should be applied on the already annotated genomes to produce up-to-date annotations. there is therefore a strong need for computer systems which take in charge, not only the primary detection of genomic features, but the whole incremental annotation process. in this paper, we propose to adopt a blackboard architecture for designing such a system.

related works
to our knowledge, no annotation software has ever been designed as a blackboard system, but several existing automatic annotation platforms have adopted well-recognized architectures. our purpose is not to list hereafter all the existing platforms , but to pinpoint those that are emblematic of a particular problem-solving architecture.

biopipe  <cit>  has been partly inspired from ensembl  <cit> . it organizes the annotation methods into pipelines. in biopipe, a pipeline is the association of:

• a set of "analyses", which describe how a method can be accessed and what are the adequate parameter values;

• a set of rules, which specify when and how a method has to be executed; the set of rules thus defines the possible sequences of analysis methods;

• a manager, which is in charge of accessing the data.

taverna  <cit>  has adopted a workflow architecture, which adds control structures to the pipeline architecture. it is thus possible to specify iteration loops or conditional alternatives over the methods. taverna offers graphical interfaces to describe the graph which links the "processors", i.e., the data transformation methods. external methods can be remotely called as web services.

in the imagene  <cit>  object-oriented system, the sequence analysis methods are organized into tasks. a task is described by the objects it accepts as input and the objects it creates as output. tasks can be arranged into more complex tasks with the help of operators: sequence , conditional alternative, iteration. the user can follow the successive steps of execution of a complex task: its actual flow of execution is displayed together with its structure. executions are themselves objects and can be stored and executed again, possibly after modifying some parameter values.

in rule-based systems, the biological data are represented by facts and the methodological knowledge is represented by rules. rules express how facts in the current state of the system allow to infer new facts, which in turn allow the activation of rules, and so on. magpie  <cit>  is a good example of such a rule-based annotation system. two sets of prolog rules run concurrently. the first one is dedicated to the data collection task; the second to the analysis task. the rules dictate the sequence of execution of the elementary methods, which consume and produce data as prolog assertions.

in geneweaver  <cit> , the annotation system results from the interactions between autonomous, but interacting agents. five classes of agents are identified:

• the primary database agents maintain a shared sequence database up to date so that it can be read and used by the agents which need these information;

• the non-redundant database agents rely on the information provided by the primary database agents to maintain a curated non-redundant database.

• the genome agents manage the information related to a particular genome;

• the calculation agents are associated to sequence analysis methods;

• the broker agents register and manage the information on all the other agents so as to facilitate their working.

one of the limitations of geneweaver is that it has never actually been deployed as a truly operational annotation system. however it recently gave rise to agmial  <cit> , a system actually used in microbiology laboratories.

other systems using multi-agent concepts have been proposed: including biomas/decaf  <cit>  and edit_totrembl  <cit> . the biomas agents, for instance, are classified into three main categories:

• the information extraction agents provide access to databases as well as some calculation services 

• the task agents are mostly generic middle agents except for the annotation agent that orchestrates the collection of information for each sequence and, therefore, provides some reasoning capabilities about sequence features.

• the interface agents communicate with other agents and provide user interface to manual annotation and database querying.

these various architectures can be classified into two main categories. the first category, which includes pipeline, workflow and task-based systems, is characterized by a sequential method invocation scheme. the second class, which includes multi-agent, blackboard-based and rule-based systems, is characterized by an opportunistic, event-driven, method invocation scheme. the advantages and drawbacks of these various approaches will be discussed later on in the discussion section.

the blackboard architecture
blackboard systems exhibit several similarities with aforementioned rule-based systems: a shared working memory, a procedural representation of knowledge and an inference cycle. the blackboard architecture is well known to support cooperative data-driven interpretation processes  <cit> . indeed, the first experiment in blackboard development resulted in the hearsay system dedicated to human speech understanding  <cit> . the input to the system is the signal of a microphone; the output is expected to be a database query as it was expressed by the speaker.

a blackboard system has three main components  <cit> : the blackboard itself, a set of knowledge sources, and the controller .

the blackboard and the layers
the blackboard is a shared working space, hierarchically structured into layers. each layer receives domain entities. these entities are produced by knowledge sources acting on entities belonging to lower layers. the bottom layer is directly populated with input data. in hearsay, the bottom layer contains the raw signal coming from the microphone; the second layer receives the segments into which this continuous signal was decomposed; the third layer stores the hypothetical phonemes associated to each segment; on the fourth layer, these phonemes are grouped into syllables and so on, up to the last layer which contains the formal database query.

since a knowledge source may produce more than one entity for a given set of input data, the entities are given an hypothesis status. some of them will be confirmed later on and merged into new entities stored in upper layers. some others will be further discarded. the management of hypotheses is therefore an important feature of a blackboard system.

the knowledge sources
all layers of the blackboard are observed by knowledge sources . a ks takes as input the entities of one or more layers and will infer new entities to be stored on one or more higher layers. inference of a new entity may be the result of an algorithm, a set of expert rules, a formal neural network, or any executable code. from the system point of view, a ks is actually a black box which is only known by the pattern of entities it expects as input  and the type of entities its produces as output. in hearsay, the ks working on the lowest layer is a signal processing method; other kss deal with the succession of phonemes and attempt to merge them into syllables; other kss look up lexicons to predict words from syllable or check the syntax of a predicted sentence.

the controller and the inference cycle
the inference process of a blackboard system follows a cycle. first, as an event, such as the creation of an entity, occurs on the blackboard, the controller will inform all the kss that are concerned by this event. each selected ks then checks if its activation condition is satisfied or not. the resulting list of activable kss is further sorted by the controller in order to prioritize the kss that must be activated first. the further execution of these kss will then produce new entities on the blackboard, thus triggering new events and a new cycle begins. the process ends when no ks can be further activated so that the state of the blackboard remains unchanged. the role of the controller is essential in focusing the inference process. it maintains an agenda of the pending kss and may change the priorities of the agenda entries according to any specified criteria. the efficiency of the overall problem solving process may strongly depends upon the strategy used by the controller to order the kss.

adequacy of the blackboard architecture to problem solving
born as an ai architecture, the blackboard presents indeed several interesting features from the knowledge and software engineering points of view. most of these properties derive from the existence of a shared working space, which represents, at any time, the state of the system. first of all, the kss do not interact with each other. a ks is only concerned by the events occurring on one  layer. at this time, it checks whether some patterns of entities match its own input pattern and declares itself as applicable to the controller. it is eventually executed when requested by the controller and finally writes its results onto its associated output layers. a ks can therefore be added or removed from the overall system without affecting the other kss. moreover, the inner part of a ks can be modified without affecting the system, as long as its activation pattern is not modified. alternatively, the strategy of the controller can be independently modified and tuned up in order to make the inference process more efficient.

the inference process is said to be opportunistic: the sequence of method invocations is not explicitly expressed before run time, but is decided according to the state of the system. as a striking illustration of this opportunistic behavior, if data produced by external sources are laid onto the blackboard, they are taken into account as if internally produced by the kss and will affect the inference process accordingly.

blackboard systems have been built for a large spectrum of applications, for which the problems to be solved could not be linearly or hierarchically decomposed into sub- problems. this is the case of most interpretation problems, such as the seminal example of speech analysis and understanding. examples of application of blackboards in biology include protean  <cit> , which was designed to predict the 3d conformation of a sequence of amino acids through the application of known physical and chemical constraints, and crysalis  <cit> , which was designed to analyze the data resulting from the x-ray diffraction pattern of a protein in order to reconstruct its 3d structure.

adequacy of the blackboard architecture for genome annotation
apart from the striking similarities between genomic sequence annotation and speech analysis, the decision to adopt a blackboard architecture for an annotation system has been motivated by the very nature of the annotation process. as explained previously, the annotation process relies on multiple methods, the execution of which provides different clues on the presence of coding sequences and regulatory signals. these clues have to be confronted, possibly discarded, but hopefully merged at different levels to finally predict the location and the structure of genes. the annotation process can thus be seen as a cooperative , opportunistic , knowledge-based  and data-driven  problem solving process.

this latest point is probably the most important and the most characteristic of blackboard systems as compared to more traditional architectures. in blackboard systems each ks is autonomous and responsible of recognizing a particular state of the shared working space  and declaring itself as activable. therefore, the sequence of execution of methods is not programmed in a procedural manner but depends upon the current state of the blackboard. this aspect sometimes causes trouble to developers who want to have full control on the sequence of execution. with blackboards, one should better think in terms of event-driven programming.

implementation
we have implemented a blackboard framework  for developing automatic annotation systems. by framework, we mean that the system is not bound to any specific annotation strategy or to any particular kss. instead, the user can specify a blackboard structure  in a single configuration file and the system will actually instantiate and run this particular annotation strategy. this allows designing several strategies to target specific biological applications. however, all these strategies will share common mechanisms and properties that will be described by using the very simple prokaryotic annotation strategy depicted in figure  <dig>  the strategy consists in first identifying open reading frames , i.e. regions which are delimited by two in-frame stops, and then to search for the leftmost in-frame start triplet within each orf. the observation of a ribosome binding site pattern  upstream of the start triplet will ascertain the cds  <cit> . finally, the retrieval of similar sequences in annotated sequence databases may eventually lead the biologist to annotate the cds. this strategy is given mainly for the purpose of illustration; a more sophisticated and realistic example will be presented later in the results section.

the blackboard, the layers and the kss
the blackboard itself is structured into a hierarchy of layers, all collinear with the input sequence . the lowest layer contains the genomic sequence to be analyzed. the highest one contains the annotated genes as predicted by the application of the various knowledge sources. each layer contains genomic features  that have either been detected on the raw sequence or built through the aggregation of features from lower layers. a genomic feature, such as a stop triplet, a ribosome binding site  or a coding region , is described by its origin, its end and its type, i.e. as an oriented and typed interval over the sequence.

the ks activation patterns
some kss integrate feature detection methods relying on bioinformatics algorithms such as markov modeling or pattern searching, while others merely confront and merge features into more complex structures. an example of the latter is ks:hypocds in figure  <dig>  this ks computes an hypothetical cds starting from an orf and an in-frame start triplet located within this orf. more generally, it turns out that a large number of such annotation rules can be expressed by considering the relative position of the intervals representing the features. that is, by considering, for instance, that a given interval is located "before" or "after", or "overlaps" another interval. more formally, the activation pattern of a ks can be expressed by using a set of relations between intervals adapted from allen's work on temporal relations  <cit>  , completed with four relations specific to genomic sequences . figure  <dig> provides several examples of these extended allen relations in activation patterns. for instance, the activation pattern of ks:hypocds states that this source should be activated when it exists an in-frame start "during" an orf. in the same way, ks:annotcds will be activated when it exists a database hit "during" a predicted cds. this ks will further ensure that the characteristic of the overlap  is sufficient to produce an annotated cds in the upper layer. when the activation pattern of a ks matches a pattern of features, the ks is said to be applicable. it will be further triggered and its output  will be deposited on the corresponding layer.

the controller
a blackboard system repeats a detection/selection/execution cycle as long as some events take place on the blackboard. the occurrence of an event may be due to the creation or the modification of an entity on a layer. this event is collected by the controller, which selects the relevant kss. when its activation pattern can be fully satisfied, a ks is declared to be applicable. the newly applicable kss are added in the controller agenda and receive a priority. this priority depends upon the control strategy that has been selected by the user. we have currently implemented a simple depth-first strategy: the agenda is a stack so that the first kss to be executed are the last that have been selected. however, if needed, the data structures of the genepi agenda allow implementing more sophisticated strategies.

the propagation-activation network
in a naïve implementation of the controller, the activation patterns of all the kss concerned by an event must be scanned in order to decide on the applicability of the kss. in order to greatly improve the performance of the controller, all the activation patterns are compiled into a rete-like  <cit>  propagation-activation network. indeed, the problem of selecting the kss according to their input patterns is very similar to the problem of selecting the rules in a rule-based expert system according to their left-hand sides and the facts available on the working memory. a problem for which the rete algorithm was initially designed.

efficiency issues: packing features
implementing a blackboard such as the one described in figure  <dig> will probably not be very efficient in practice, especially on large sequences, because of the low granularity of the entities involved . in this approach, each start and stop entity will be considered and treated independently by one or more kss. as a consequence, there will be a lot of pending kss in the agenda and the whole process will slow down considerably. in practice, it is more efficient to group these features into  sets of features and to make the kss work with those sets instead of individual features. for instance, in figure  <dig>  the ks:hypocds that is responsible of computing the cdss will take as input the set of all starts and the set of all orfs and will produce a set of all cdss. this approach is called "feature packing". of course, it is less elegant than the low granularity approach since most of the logic should be encoded within the ks instead of being explicitly declared in the blackboard. for instance, the fact that a cds is defined as a maximal stretch of dna between a start and a stop in the same frame will not be declared in the ks activation pattern but will be hidden within the internal code of ks:hypocds.

in practice both approaches may be mixed, e.g. by packing features in the lowest layers  and by working with individual features at the higher levels . this approach will be further illustrated in the results section.

implementation issues
genepi has been implemented in java and can be used either as a standalone application or as a library embedded into larger applications. the system can be easily extended by adding new java classes complying with the api. some kss maybe associated to external executables  through the use of unix-shell calls. the current ks toolbox includes methods for finding prokaryotic orfs, trnas and rbss as well as blastp for scanning databases. it provides the basic elements to design blackboards targeted at prokaryotic genome analysis. as it will be illustrated in the next section, configuring a new blackboard to use already existing kss does not require any programming skill.

RESULTS
the prototype has not been developed with the ambition to overcome existing automatic annotation systems, but to demonstrate the appropriateness of the blackboard architecture for the development of genome annotation systems, both from the knowledge engineering and the software engineering points of view. although the system is robust enough to be used on real-size applications, it is of primary use to bioinformatics researchers who want to experiment with blackboard architectures. to this purpose we provide, in addition to the core system, a graphical user interface allowing to load a blackboard configuration, to run it  and to graphically visualize the creation of features on the chromosome during the execution.

instantiating a genome annotation blackboard
to instantiate a blackboard, the designer must declare, in a configuration file, the number of layers, the types of the different entities, the kss, i.e. their input and output patterns and their executable body. this process will be exemplified now on the real annotation strategy depicted in figure  <dig>  this strategy aims at finding genes on a bacterial chromosome and annotating those that putatively encode for enzymes. it works in the following way: first the chromosomal sequence is scanned  for long orfs . these orfs are used to build  a markov transition matrix . this matrix is further used to actually find  cdss. these cdss are then checked  against an enzyme database  whose entries are annotated with ec numbers . finally, when a cds gets sufficient matches with annotated enzymes and when the majority of these matches have the same ec number, then this ec number is transferred to the cds's product to eventually yield an enzymatic gene . it is important to note that, in this example, we stopped at the gene level but one could imagine to continue using these genes in higher layers of the blackboard, representing for instance bacterial operons  or metabolic pathways .

configuring the blackboard
to configure a blackboard implementing this strategy, one has to edit an xml configuration file such as the one described in figure  <dig>  this file contains two parts, one for the definition of the blackboard layers  and one for the kss . in figure 6- <dig>  we declare seven layers. the first four layers  are implemented as lists. they represent respectively, the raw sequence, the markov matrix, the database to scan  and the list of all orfs . the last three layers  are implemented as intervals  and represent individual features : cdss, blast hits associated to one cds and validated enzyme genes respectively. the second part of the configuration file deals with the declaration of the kss. each ks has two parts: the activation pattern  and the executable body . moreover, the type of entities produced by a ks is specified by the <source> tag attribute create. for instance, ks:findorfs in figure figure 6-2-a, has a very simple activation pattern that reads "select any entity from layer sequence". it produces  entity of type all_orfs through the call of the executable pkorf declared in the <actions> tag. all sub-tags of the <actions> tag are associated to a piece of java code. some of them perform internal operations  and others may call external executables . when adding a new ks to the system, one will have also to define the associated tag. a more sophisticated ks example is given on figure 6-2b. the ks:searchdb is responsible of computing blast hits associated to each cds. its activation pattern reads as "select all couples composed of one cds and the database. this cds will then be scanned  on the database and the resulting hits will be further packed . finally, an example of allen relation is given on figure 6-2c. ks:ecannotator activates on each couple composed of one cds and a set of associated hits  that is included in  the cds.

as mentioned earlier, the genepi standalone applications also provides a graphical interface allowing to follow step by step the execution of the blackboard. figure  <dig> is a snapshot of this interface during the execution of the previous example. in this case, only the domain entities corresponding to individual genomic features are graphically represented . as a real-sized test case, we ran this particular annotation strategy on the whole chromosome of b. subtilis . more than 97% of the actual genes  were correctly found .  <dig> genes were further annotated with ec numbers, most of them  beeing correct as compared to the published annotation. on the other hand,  <dig> genes with ec annotations remained unpredicted, indicating that this particular strategy  were probably too conservative. the total running time on the whole chromosome was  <dig> hours , most of the time  being actually spent in the blast steps.

discussion
as already mentioned in the background section, two main classes of architecture of automatic annotation systems can be identified. the first class, which includes pipeline, workflow and task-based systems, is characterized by a sequential method invocation scheme. the order in which the analysis methods have to be executed is static and predefined. some variations may be accepted if alternative sequences can be described. this is often the case in task-based systems, which allow the next task to be chosen according to the results of the previous one. the main drawback of such a sequential scheme is the lack of flexibility regarding the maintenance and the modification of the system, especially when new methods are to be added. on the other hand, the end user can easily follow the execution of the system. the second class, which includes multi-agent, blackboard-based and rule-based systems, is characterized by an opportunistic method invocation scheme. the order in which the methods are called is not preset but is determined at runtime by the state of the system. the major advantage is that a new knowledge chunk  corresponding to a new method can be easily added or removed without much disturbing the other parts of the knowledge base. if the conditions of a method invocation have been properly defined, the method will be appropriately called when the corresponding state will occur in the system. among these systems, blackboard architectures present determinant advantages: a shared working memory which is structured in layers in adequacy with the levels of hypothesis setting, a centralized control strategy which is easy to follow, an intuitive description of the methods and their activation patterns as independent knowledge sources.

blackboards and multi-agent systems
multi-agent and blackboard systems are both part of the so-called distributed ai systems in which the processing capacity is distributed among multiple entities: the kss of the blackboard systems and the agents of the multi-agent systems. the main difference lies in the way the entities communicate. in a multi-agent system, the entities, i.e. the agents, communicate directly one with the others by sending messages: the control is therefore also distributed. on the contrary, in blackboards, kss never communicate directly. they find their input on the blackboard and deposit the products of their execution on the same blackboard, which can thus be seen as a shared working memory. in both cases, the advantages of the architecture result from the modularity it induces from both the software and knowledge engineering points of view. however, we consider that the existence of a shared and structured working memory, together with a central controller, produce a reasoning system which is much easier to follow and understand, and therefore easier to maintain, to tune and to extend. moreover, the kss are truly independent modules that can be added or removed without affecting the others. on the other hand, the multi-agent architectures are better suited to distributed environments. despite their qualities, blackboard systems seem to be presently much less popular that multi-agent systems. an explanation for this situation has probably to be searched in the development of object-oriented techniques, which have provided the technology to efficiently implement the agents as interacting concurrent processes. in the same time, the complexity of blackboard systems increased  and thus lost some of their most appreciated properties.

advantages and drawbacks of blackboards for genome annotation
in the context of genome annotation, blackboards have several interesting features related to the way the methodological and biological knowledge can be updated. as research in bioinformatics produces new methods, they can be added to the system, wrapped as new kss. since ks never communicate directly but via the blackboard, these additions are simple because they do not interfere with the ones already integrated. from this point of view, the annotation of a genomic sequence can thus be updated after a new analysis method has been integrated. conversely, if a sequencing error has been detected and corrected on the raw sequence, all the ks executions for which the corrected region was involved can be forgotten, their output erased from the layers on the blackboard and the inference cycle reactivated. finally, we would like to mention another important case where the update facilities offered by a blackboard architecture could be put into play. usually, after a first pass of fully automatic annotation, the genomic features need to be manually reviewed by experts. for instance, the start position, the functional annotation or simply the presence of a gene may be modified. these manual modifications may have consequences on the overall annotation and need to be propagated, for instance if the modified cds is involved in higher structures like operons or pathways. in the blackboard view, this means that the human expert plays the role of a new ks . the propagation and update of the modification can then be handled by the architecture.

of course, besides these advantages, blackboard  systems also have some known drawbacks. the first one is the difficulty of making them do specific calculation in ordered tasks. as explained before, this is a natural consequence of their opportunistic behavior. developers should therefore think in terms of event-driven actions rather than strictly ordered tasks. however, if such a pipeline behavior is desired then a solution is to embed the ordered tasks within a single ks. indeed, the <action> part of a ks can be seen as a small pipeline. of course, this leads to a less declarative system where a part of the annotation strategy becomes hidden in the ks. depending upon the problem to be solved, there is therefore a tradeoff to find between "pure" blackboard and pipeline behavior. another known, more technical, difficulty is related to the debugging of the system. again, because of the event-driven method invocation scheme, it may be sometimes difficult to pinpoint the source of a potential problem.

CONCLUSIONS
the question of the reliability of bioinformatics software takes a slightly different form depending whether one considers a single piece of software or more complicated systems such as integrated platforms. in the first case, as long as the software correctly implements algorithms that are well known and understood, the software designers may consider that the results do not require to be further explained or justified. on the other hand, for genome annotation platforms, the execution of sequence analysis algorithms merely provide clues that have to be confronted, filtered and merged according to some methodological knowledge. this knowledge can be either directly provided by the user or formally expressed and integrated into the system. however, the possibility to formally express this knowledge, as rules, objects, tasks or any other modeling entities, does not mean that the resulting system will yield pertinent results. indeed, this highly depends upon the expertise of the designers and the results may be further discussed and possibly refuted by the end users. in this context, we believe that an annotation system should not only allow the formal expression and integration of the methodological knowledge; it must also provide facilities for the user to follow and understand the annotation process, and to tune, adapt or even refute the content of the methodological knowledge base. the blackboard architecture appears to offer most of these software and knowledge engineering properties.

availability and requirements
the genepi protoptype has been implemented in java and is freely available for download at the following url : . the distribution includes all the java sources as well as blackboard samples. the core system and graphical interface run on any platform supporting javavm. it has been tested on linux and macosx. some kss  need external executables. these executable are provided in the distribution for macosx and linux platforms.

authors' contributions
av and fr initiated the project. sdd, dz and fr designed the architecture and software requirements. sdd wrote most of the java code and av provided the external toolbox. all authors participated in testing the software and in editing and proofreading the manuscript. all authors read and approved the final manuscript.

