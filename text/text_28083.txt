BACKGROUND
when analysing microarray datasets for class comparison and class prediction  <cit>  purposes, the generalisation performance of machine learning algorithms such as linear discriminant analysis  and support vector machines  is typically estimated using sample-reuse techniques, as the sample sizes are often too small to use a separate withheld test set: such schemes include k-fold cross-validation , leave-one-out cv , bootstrap methods, or repeated holdout   <cit> . the estimates of generalisation performance are then used for model selection, parameter tuning, feature selection, or performance evaluation in empirical comparisons of machine learning methods. when comparing the results of different classifiers it is also necessary to test for statistical significance  <cit> . microarray data is atypical of the data commonly classified by machine learning methods as it often has a small sample size and low discriminability between the classes e.g. in cancer prognostic and therapeutic response studies. various biases can occur in such a setting when using the above mentioned validation schemes  <cit> . for example, ambroise and mclachlan  <cit>  and simon et al.  <cit>  demonstrated an optimistic selection bias that occurs when gene selection is done using the entire dataset rather than separately for each resampled training set. this bias arises through incorporation of information from the test sets into the training of the classifier. varma and simon  <cit>  demonstrated in a simulation study the optimistic hyperparameter selection bias  <cit>  which occurs when reporting the best error rates achieved on the validation set used to tune classifier parameters, rather than using a nested cv or separate test set to evaluate the classifier.

it is also known that in some limited situations, such as when measuring the error rate of a classifier on a perfectly balanced dataset, large pessimistic biases can occur when using cv  <cit>  due to negatively correlated class proportions between the training sets and their corresponding test sets . however, because of the limited circumstances in which this bias appears to be an issue  it has been largely ignored in the literature.

what has not been previously appreciated is that this systematic bias due to inadequate stratification is quite pervasive and can occur in a wide variety of contexts, including when using ranking performance measures such as the frequently used area under the receiver operating characteristic curve . this is not obvious as the auc is considered to be a measure that is insensitive to varying class proportions. importantly, with these measures we show that the bias can also occur in highly imbalanced or skewed datasets, and indeed the effect of the stratification bias can be larger for these performance measures than for the standard error rate.

many microarray studies have small datasets and weak signals , thus the bias is especially important and steps should be taken to minimise it. we demonstrate several techniques including careful implementation of the auc calculation and stratified variants of resampling schemes that can be used to remove or minimise the bias.

review of roc and auc calculation
for comparing and assessing the performance of gene selection and classification algorithms in the analysis of microarray and other biological datasets, the receiver operating characteristic curve  and the associated auc are popular measures of performance  <cit> . they have several advantages over error rate  <cit>  including insensitivity to the prior class probabilities and class-specific error costs. this is especially important in the case of microarray observational studies where the particular class proportions used may be unrelated to clinical prevalence, and in class comparison  studies where an inherent measure of the discriminability of the signal using a given classifier is required. the roc shows the trade-off between sensitivity and specificity for a two-class classifier or diagnostic system. it has long been used in medical diagnosis  <cit>  and has become widely used in evaluating machine learning algorithms  <cit> . the auc summarises an roc and provides a single measure of the performance of a classifier and the discriminability of a signal: a random signal has an auc of  <dig>  and a perfectly discriminable signal has auc of  <dig> .

the auc of a classifier with a scoring output, such as the probability of a sample being class  <dig>  can be computed without first constructing an explicit roc curve  <cit> :

 auc=s0−n+/2n+n−
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgbbqqcqwgvbqvcqwgdbwqcqgh9aqpdawcaaqaaiabdofatnaabaaaleaacqaiwaamaeqaaogaeyoei0iaemoba42aasbaasqaaiabgucaraqabagccqggoaakcqwgubgbdawgaawcbagaeyoei0cabeaakiabgucariabigdaxiabcmcapiabc+caviabikdayaqaaiabd6gaunaabaaaleaacqghrawkaeqaaogaemoba42aasbaasqaaiabgkhitaqabaaaaaaa@43ab@ 

where n+ and n- are the numbers of positive and negative samples, s0=∑iri
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgtbwudawgaawcbagaegimaadabeaakiabg2da9maaqahabagaemocai3aasbaasqaaiabdmgapbqabaaabagaemyaakgabaaaniabgghildaaaa@368b@, and ri is the rank of the ith positive sample in the sorted classifier output . similarly, the roc curve can be computed by an incremental algorithm which scans the sorted outputs  <cit> . the auc is equivalent to the wilcoxon-mann-whitney statistic, which measures the probability that two random samples from different classes will be ranked in the correct order  <cit> . an estimate of the standard error of the auc is given analytically by the hanley-mcneil estimate  <cit> . when combining the results of the folds or replicates of a validation procedure to compute an overall estimate of the roc curves or auc, there are two main approaches: the pooling and averaging strategies  <cit> . the pooling strategy involves collecting the classifier scoring outputs determined on each test set and calculating the auc on this set of combined outputs. in the averaging strategy, a separate auc is computed for each test set, and the mean of these aucs is computed. note that the pooling strategy is the only method of measuring auc  when using loocv. both strategies are used in practice and the current literature is equivocal about which approach is to be recommended . witten and frank  <cit>  note that the pooling strategy has the advantage that it is easier to implement. also, it is expected to have a lower variance. in part, this is because when taking an average over f averages, each calculated over n results of a random variable x, we get the following variance: varaverage∝1f−1⋅n−1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgwbgvcqwghbqycqwgybgcdawgaawcbagaemyyaemaemodaynaemyzaumaemocainaemyyaemaem4zacmaemyzaugabeaakiabcicaoiabdifaynaabaaaleaacqwglbqzcqwgzbwccqwg0badaeqaaogaeiykakiaeyyhiu7aasaaaeaacqaixaqmaeaadagcaaqaaiabdagamjabgkhitiabigdaxawcbeaakiabgwsixpaakaaabagaemoba4maeyoei0iaegymaedaleqaaaaaaaa@4d16@. in contrast if we calculate a direct average over all results  the variance will be lower: varpool∝1f⋅n−1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgwbgvcqwghbqycqwgybgcdawgaawcbagaemicaanaem4ba8maem4ba8maemibawgabeaakiabcicaoiabdifaynaabaaaleaacqwglbqzcqwgzbwccqwg0badaeqaaogaeiykakiaeyyhiu7aasaaaeaacqaixaqmaeaadagcaaqaaiabdagamjabgwsixlabd6gaujabgkhitiabigdaxawcbeaaaaaaaa@4737@. the difference in variance is only significant when n or f is small.

the pooling approach assumes that the results of each fold is a sample from the same population  <cit> . in the next section we show that this assumption is generally not valid for cv or bootstrap and can lead to large pessimistic biases.

methods
theoretical analysis
the fundamental issue is that most classifiers incorporate the prior probabilities of the classes, estimated from the training set, either as an explicit prior term, as is the case for generative classifiers such as lda, or implicitly . when using cv this estimate is negatively correlated with the class proportions of the corresponding test set. we now analyse the bias that arises from this negative correlation.

analysis of bias in error rate and auc estimation
cv uses sampling without replacement to partition the dataset into training and test sets, thus any deviation from the class proportions of the whole dataset in a training set leads to an opposite deviation in the corresponding test set. more specifically, the correlation of the class proportions of a training and test set pair is - <dig>  for small datasets, unstratified cv can lead to large variations in the training and test set class proportions, so a large negative covariance between training and test sets class proportions exists. stratified cv  <cit>  is a cv variant which ensures that the proportions of the classes in the test sets of the folds are as close as possible to the overall class proportions. stratified cv can reduce the above mentioned covariance but not remove it entirely due to irreducible quantisation errors which are significant for small test set sizes. for example, a 10-fold stratified cv on a dataset with  <dig> samples has three samples in each fold, and cannot be stratified correctly for two classes in equal proportions. somewhat surprisingly, as shown in the empirical section of this paper, this tiny error can still lead to substantial biases. the same reasoning applies when using repeated holdout.

by contrast, bootstrap methods use sampling with replacement. a training set of the same size as the whole dataset is created by randomly sampling the whole dataset with replacement, with the remaining unsampled instances forming the test set. the test and training sets, excluding replicates, have a similar covariance of the class proportions as a  <dig> :  <dig> repeated holdout, although the added variance of the replicate samples leads to a somewhat smaller correlation of approximately - <dig> .

using bayesian decision theoretic analysis, we show that this negative covariance between training and test set proportions can lead to large biases in error rate and auc estimation. let fk be the class-conditional density of feature vector x in class g = k, and let πk be the prior probability of class k, such that ∑l=1kπl=1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaaewaqaaggaciab=b8awnaabaaaleaacqwgsbabaeqaaaqaaiabdygasjabg2da9iabigdaxaqaaiabduealbqdcqghris5aogaeyypa0jaegymaedaaa@386a@, then bayes theorem gives the posterior probabilities

 pr=fkπk∑l=1kflπl
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiegacqwfqbaucqwfybgccqggoaakcqwghbwrcqgh9aqpcqwgrbwacqgg8bafcqwgybawcqgh9aqpcqwg4baecqggpaqkcqgh9aqpdawcaaqaaiabdagamnaabaaaleaacqwgrbwaaeqaaogaeiikagiaemieagnaeiykakcccigae4hwda3aasbaasqaaiabdugarbqabaaakeaadaaewaqaaiabdagamnaabaaaleaacqwgsbabaeqaaogaeiikagiaemieagnaeiykakiae4hwda3aasbaasqaaiabdygasbqabaaabagaemibawmaeyypa0jaegymaedabagaem4saseaniabgghildaaaaaa@53f6@ 

and, assuming the two class case with classes  <dig> and  <dig>  the log ratio is

 log⁡pr⁡pr⁡=log⁡f1f2+log⁡π1π2
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacyggsbabcqggvbwbcqggnbwzdawcaaqaaigbccfaqjabckhayjabcicaoiabdeeahjabg2da9iabigdaxiabcyha8jabdifayjabg2da9iabdiha4jabcmcapaqaaigbccfaqjabckhayjabcicaoiabdeeahjabg2da9iabikdayiabcyha8jabdifayjabg2da9iabdiha4jabcmcapaaacqgh9aqpcyggsbabcqggvbwbcqggnbwzdawcaaqaaiabdagamnaabaaaleaacqaixaqmaeqaaogaeiikagiaemieagnaeiykakcabagaemozay2aasbaasqaaiabikdayaqabagccqggoaakcqwg4baecqggpaqkaagaey4kasiagiibawmaei4ba8maei4zac2aasaaaeaaiigacqwfapacdawgaawcbagaegymaedabeaaaoqaaiab=b8awnaabaaaleaacqaiyagmaeqaaaaaaaa@6548@ 

note that as the signal becomes weaker, i.e. as the first term diminishes, the prior probabilities assume increased relative importance. the bayes decision rule for the two class case is to classify a sample as class  <dig> when the likelihood ratio f1/f <dig> exceeds a threshold t, where for the bayes  rate t is the inverse ratio of the prior proportions, π2/π <dig> 

first we consider the bias in error rate. during cv or bootstrap, the prior proportions used to determine the classification threshold are estimated from the training set, π1train/π2train
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfapacdaqhaawcbagaegymaedabagaeeidaqnaeeocainaeeyyaemaeeyaakmaeeoba4gaaogaei4la8iae8hwda3aa0baasqaaiabikdayaqaaiabbsha0jabbkhayjabbggahjabbmgapjabb6gaubaaaaa@4112@, but due to incomplete stratification this will typically differ from the prior proportions of the whole data set, π1true/π2true
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfapacdaqhaawcbagaegymaedabagaeeidaqnaeeocainaeeydaunaeeyzaugaaogaei4la8iae8hwda3aa0baasqaaiabikdayaqaaiabbsha0jabbkhayjabbwha1jabbwgalbaaaaa@3e8c@. the corresponding test set proportions are negatively correlated, and so if π1train/π2train>π1true/π2true
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfapacdaqhaawcbagaegymaedabagaeeidaqnaeeocainaeeyyaemaeeyaakmaeeoba4gaaogaei4la8iae8hwda3aa0baasqaaiabikdayaqaaiabbsha0jabbkhayjabbggahjabbmgapjabb6gaubaakiabg6da+iab=b8awnaadaaaleaacqaixaqmaeaacqqg0badcqqgybgccqqg1bqdcqqglbqzaagccqggvawlcqwfapacdaqhaawcbagaegomaidabagaeeidaqnaeeocainaeeydaunaeeyzaugaaaaa@53f8@, then π1test/π2test<π1true/π2true
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfapacdaqhaawcbagaegymaedabagaeeidaqnaeeyzaumaee4camnaeeidaqhaaogaei4la8iae8hwda3aa0baasqaaiabikdayaqaaiabbsha0jabbwgaljabbohazjabbsha0baakiab=xda8iab=b8awnaadaaaleaacqaixaqmaeaacqqg0badcqqgybgccqqg1bqdcqqglbqzaagccqggvawlcqwfapacdaqhaawcbagaegomaidabagaeeidaqnaeeocainaeeydaunaeeyzaugaaaaa@5167@. hence, as noted by kohavi  <cit> , for a no-signal balanced dataset , where the likelihood ratio and the ratio of prior proportions of the whole dataset, π1true/π2true
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfapacdaqhaawcbagaegymaedabagaeeidaqnaeeocainaeeydaunaeeyzaugaaogaei4la8iae8hwda3aa0baasqaaiabikdayaqaaiabbsha0jabbkhayjabbwha1jabbwgalbaaaaa@3e8c@, =  <dig>  the opposite decision to the correct classification for the test set elements will be reached, leading to a pessimistic bias and worse-than-random results. that is, for a no-signal dataset the optimal bayes classifier is a majority-voter, and in this case, the majority class on the training set is the opposite to that of the test set.

if there is a signal , then the classification accuracy will be pessimistically biased by this effect to a lesser extent, depending on the signal strength.

in the above analysis, the stratification bias for error rate only manifested when the dataset was balanced, however it can occur more widely in microarray studies. the aim in class comparison studies of microarrays is to determine the inherent discriminability of the genes in distinguishing the classes. a preprocessing technique in this case is to balance the classes by subsampling the majority class so that only the feature information is used in discriminating the classes and not the prior probabilities. another technique applicable to classifiers that minimise regularised risk, such as support vector machines , is to use class dependent regularisation inversely proportional to the class proportions. otherwise, for very imbalanced classes the classifier can effectively turn into a majority voter where no feature information is actually used to distinguish the classes. note that with such preprocessing, the bias in error rate will also occur for imbalanced datasets.

now we consider the bias in auc estimation using the pooling strategy. an equivalent formulation of the bayes decision rule described above for the two class case is that the posterior probability pr exceeds a threshold t, where for the bayes decision rule t =  <dig> . for a given trained classifier, varying the threshold in either of these formulations across its full range will generate equivalent roc curves, and hence auc. as the roc curve measures the inherent discriminability of the class conditional distributions irrespective of the prior class probabilities, calculation using the likelihood ratio formulation would be preferred  <cit> . in practice, however, most classifiers will return an estimate of the posterior probability, or a related uncalibrated measure such as the decision value, or distance to the hyperplane separating the classes, in svms, and so this second formulation is typically used.

the pooling strategy for auc calculation assumes that the classifier outputs across folds of cv are comparable and thus can be globally ordered. this is the case if the auc is calculated using the likelihood ratio for ranking; if the auc is calculated using posterior probabilities  for ranking as is usually done, the prior probabilities will vary in each training set because of incomplete stratification, and hence the classifier outputs will not be comparable across folds. this can lead to test samples in different folds being ranked in the wrong order:

assume a two-class classification problem with classes  <dig> and  <dig>  using the posterior probabilities  for ranking the samples. let π1train/π2train=δ⋅
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfapacdaqhaawcbagaegymaedabagaeeidaqnaeeocainaeeyyaemaeeyaakmaeeoba4gaaogaei4la8iae8hwda3aa0baasqaaiabikdayaqaaiabbsha0jabbkhayjabbggahjabbmgapjabb6gaubaakiabg2da9iab=r7akjabgwsixlabcicaoiab=b8awnaadaaaleaacqaixaqmaeaacqqg0badcqqgybgccqqg1bqdcqqglbqzaagccqggvawlcqwfapacdaqhaawcbagaegomaidabagaeeidaqnaeeocainaeeydaunaeeyzaugaaogaeiykakcaaa@599c@ where π1true/π2true
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfapacdaqhaawcbagaegymaedabagaeeidaqnaeeocainaeeydaunaeeyzaugaaogaei4la8iae8hwda3aa0baasqaaiabikdayaqaaiabbsha0jabbkhayjabbwha1jabbwgalbaaaaa@3e8c@ is the overall class proportions in the full dataset and δ may not equal  <dig> due to incomplete stratification. consider any fold. suppose δ >  <dig>  i.e. class  <dig> is over-represented in the training set. then the prior term of eq.  <dig> is log⁡π1train/π2train=log⁡π1true/π2true+log⁡δ
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacyggsbabcqggvbwbcqggnbwziigacqwfapacdaqhaawcbagaegymaedabagaeeidaqnaeeyyaemaeeyaakmaeeoba4gaaogaei4la8iae8hwda3aa0baasqaaiabikdayaqaaiabbsha0jabbkhayjabbggahjabbmgapjabb6gaubaakiabg2da9igbcygasjabc+gavjabceganjab=b8awnaadaaaleaacqaixaqmaeaacqqg0badcqqgybgccqqg1bqdcqqglbqzaagccqggvawlcqwfapacdaqhaawcbagaegomaidabagaeeidaqnaeeocainaeeydaunaeeyzaugaaogaey4kasiagiibawmaei4ba8maei4zacmae8htdqgaaa@6171@, and log δ >  <dig>  thus, the samples in the test set of this fold will overall receive a higher ranking than in a completely stratified fold.

however, due to the negative correlation between the training and test set proportions, π1test/π2test=γ⋅
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiigacqwfapacdaqhaawcbagaegymaedabagaeeidaqnaeeyzaumaee4camnaeeidaqhaaogaei4la8iae8hwda3aa0baasqaaiabikdayaqaaiabbsha0jabbwgaljabbohazjabbsha0baakiabg2da9iab=n7anjabgwsixlabcicaoiab=b8awnaadaaaleaacqaixaqmaeaacqqg0badcqqgybgccqqg1bqdcqqglbqzaagccqggvawlcqwfapacdaqhaawcbagaegomaidabagaeeidaqnaeeocainaeeydaunaeeyzaugaaogaeiykakcaaa@5718@, where γ <  <dig> i.e. class  <dig> is under-represented and class  <dig> is over-represented in the test set. this means that proportionally more samples of class  <dig> get this higher ranking than samples of class  <dig>  by symmetry, the reverse applies when δ <  <dig>  i.e. in the test sets where class  <dig> is over-represented, samples get lower rankings.

as the auc is an estimate of the probability of two independent samples being ranked in the correct order, it can be seen that this will lead to a pessimistic bias of the auc towards  <dig> if the samples are ranked together as in the pooling method. note that this bias becomes more substantial as the predictions become less certain; when the predictor is a constant uncertain predictor  =  <dig> , i ∈  <dig>  2) the bias is large and leads to an auc less than the expected  <dig> .

in contrast, the stratification bias discussed here does not affect auc calculated using the averaging strategy which averages the auc computed per test set over all folds. within each test set, the auc is insensitive to class proportions and so is accurate, and as a rank ordering between folds is not required, no bias will occur in the overall auc estimate.

application to practical induction algorithms
the classifier resulting from the bayesian decision theoretic analysis, the bayes classifier, is the optimal classifier. practical induction algorithms aim to approximate the bayesian decision theoretic analysis described above to varying extents, and suffer from stratification bias to varying degrees. a generative classifier such as lda approximates this analysis by effectively estimating an underlying model incorporating class conditional likelihoods and prior class proportions as described above  <cit> . for lda the class conditional likelihoods are modelled as multivariate gaussians. therefore, as analysed in the previous section, the degree of stratification bias depends on the signal strength: for low signal datasets, the relative importance of the prior term increases and so the relative stratification bias increases. in the limit of a no-signal dataset, only the prior term remains and the classifier becomes a simple majority voter with maximal stratification bias.

some variants of lda assume that the prior proportions are the same for all classes, and so the prior term is ignored. such classifiers include minimum distance  and nearest mahalanobis distance classifiers  <cit> , which are simple versions of lda that classify a sample to the nearest class mean. as they exclude the prior term, they are not affected by the form of stratification bias described here. other classifiers, including non-parametric classifiers such as k-nearest neighbour and discriminative classifiers such as svms, approximate the ideal bayes classifier, including the prior term, indirectly. such induction algorithms still implicitly incorporate prior probability information by inducing a classifier optimised for imbalanced datasets, and in the limit for random signals they will tend towards a majority voter. this means that the above analysis applies as well to these classifiers and they suffer from stratification bias accordingly, but without an explicit calculation of prior proportions they are more difficult to study analytically. svms are discriminative in that they find a separating hyperplane between the two classes and do not directly estimate the class-conditional likelihoods and prior proportions of the classes; they can output for ranking purposes the internal decision values used for classification . therefore svms are affected by the class prior probabilities only implicitly and incompletely due to shifts in the hyperplane with varying class proportions, and so are only partially affected by the form of stratification bias discussed here. svms that generate a posterior probability estimate  <cit>  approximate the bayesian analysis above and thus are expected to be substantially affected by this stratification bias. note that while we have limited the analysis to the two-class problem, the same arguments apply in the multiclass case, and to multiclass extensions of auc. in addition to the stratification bias discussed above, there is a similar additional bias that can affect the pooling strategy of auc estimation. any non-systematic noise across the folds or replicates of the validation scheme, due to slightly different classifiers learnt from the training sets, will perturb the ranking values or probabilities and attenuate the auc towards the random value of  <dig>  .

also, note that this analysis assumes homogeneous data in each of the labelled classes, as is the typical assumption in machine learning algorithms. if the classes in fact are heterogeneous with unidentified subclasses, then these internal subclasses may themselves not be fully stratified across folds , and so a similar analysis would apply to the subclasses in this case, although affecting the first, likelihood, term of eq.  <dig> rather than the second, prior probability, term as analysed here. further analysis of this form of stratification bias in such heterogeneous datasets is outside the scope of this paper and is a focus of future research.

ameliorating stratification bias
as described above, the bias is introduced by the negative covariance of the training and test set class proportions across the folds or replicates of the validation scheme. therefore, approaches to remove this bias are focused on removing this covariance.

as noted previously, for estimating auc, the averaging strategy does not suffer from this stratification bias. a more general approach, applicable also to other performance measures, is to use a sample-reuse method that does not introduce these correlated class proportions. stratified repeated holdout samples independently from each of the classes in the proportion of the original dataset  <cit> . stratified bootstrap sampling  <cit>  similarly ensures that the training set has classes in the same proportions as the original set by sampling with replacement separately from each class. as the covariance and correlation between a constant  and another random variable  equals  <dig>  these approaches completely remove this form of stratification bias.

for cross-validation, although stratified cv  <cit>  removes the bulk of the covariance, a significant amount can remain, especially for imbalanced datasets and small test set sizes. when the sample size for each class is not a multiple of the number of folds the stratification is incomplete, causing the proportion of samples in the training sets to differ. as shown in the empirical section, this residual covariance can lead to substantial biases when the test sets are small.

as noted above, if the class proportions in the training sets are constant across all folds, then they are uncorrelated with the test set class proportions and no stratification bias is present. this suggests a modification to the standard cv to eliminate the stratification bias: ensure that the training set class proportions are representative of the overall data class proportions but remain constant across all folds. we describe a new validation method, balanced, stratified cv , which does this. it sub-samples to exclude a small number of random samples from the training set in each fold. given a stratified k-fold cross validation  partition, let tcf be the count of class c in the training set of fold f, and define mc = minftcf to be the minimum count of class c in the training sets across all folds; then, in each training set tcf - mc samples are randomly deleted for each class c. as all classes will have count mc in each training set, there will be no correlation between training and test set class proportions, removing the bias. furthermore, as the number of elements of each class varies by only  <dig> among the training sets for stratified cv, the maximum number of removed elements from each training set is the number of classes: in the two class case at most two samples are removed. a potential drawback is the slightly smaller training sets, however, as at most one sample per class is removed, the degradation in performance due to learning curve effects is minimal and is dominated by the stratification bias. the pseudo-code of bscv is given in algorithm  <dig>  to implement bscv only minor modifications to stratified cv are needed. bscv applied to loocv is denoted as balanced loocv, as loocv cannot be otherwise stratified. in the case of balanced loocv, only a single sample needs to be removed from each training set.

input: training sets ti of the m folds provided by stratified cv, number of classes n

output: training sets ti of bscv

for i =  <dig> to n do

   for j =  <dig> to m do

      cij = |{x ∈ tj|x is of class i}|

   end for

end for

for i =  <dig> to n do

   mi = minj cij

end for

for i =  <dig> to n do

   for j =  <dig> to m do

      remove  random samples of class i from tj

   end for

end for

algorithm 1: bscv algorithm

empirical analysis
to evaluate empirically the effect of the stratification bias analysed in the previous sections, we performed experiments using monte carlo simulation and a real-world dataset. the sample-reuse validation schemes examined included stratified and unstratified cv, loocv, and bootstrap. all cvs were 10-fold with no repetitions where not indicated otherwise. the zero estimator ε <dig> bootstrap version was used in the bootstrap experiments as the aim was to investigate the relative performance differences due to stratification bias; the  <dig>  and  <dig> + estimators bootstrap versions  <cit>  correct for the learning curve effect of their smaller training set by incorporation of the resubstitution error, which could confound the bias we are investigating.

for svm, the auc can be computed by ranking on either the decision values or the estimated posterior probabilities. in the simulation experiments, estimated posterior probabilities were used, while for the real-world examples, decision values were used.

the grey lines in figures  <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> show the optimal values for auc and the optimal  error rate for class proportion  <dig> . if both optimal auc and error rate are shown, the line for auc is solid. the r code and data for the experiments are available from the authors upon request.

simulation
we performed monte carlo simulations using  <dig> and 10-dimensional normal distributions, the latter to approximately simulate ten significantly differentially expressed genes in a microarray study, as may be selected by some gene selection method . samples were drawn from two multivariate gaussian distributions . the discriminability  d′=tΣ−1
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwgkbazgaqbaiabg2da9maakaaabagaeiikagcccigae8hvd02aasbaasqaaiabicdawaqabagccqghsislcqwf8oqbdawgaawcbagaegymaedabeaakiabcmcapmaacaaaleqabagaemidaqhaaogaeu4odm1aawbaasqabeaacqghsislcqaixaqmaagccqggoaakcqwf8oqbdawgaawcbagaegimaadabeaakiabgkhitiab=x7atnaabaaaleaacqaixaqmaeqaaogaeiykakcaleqaaaaa@450c@ measures the degree of separation of the gaussians. in our simulations we used μ <dig> =  <dig>  Σ = i and varied μ <dig> to set the discriminability of the signal.

a d' of  <dig> is a random signal and corresponds to an error rate and auc of  <dig> ; a d' of  <dig>  is a weak signal; a d'of  <dig>  is a moderate to strong signal; and a d' of  <dig>  is a very strong signal. in the results section, we show the estimated error rate and optimal auc corresponding to each simulation d' as a grey line. two classes of induction algorithm were compared: diagonal lda  and svms. dlda is a version of lda that uses the assumption of independent features such that the covariance matrix is diagonal, which matches the simulation data model. note that dlda as defined here incorporates both the class conditional likelihood and prior proportion terms of eq.  <dig>  as defined in  <cit> , as opposed to the definition that excludes the prior term used in  <cit> .

the performance measures examined were classification accuracy , auc, and balanced error rate . ber is an estimate of classifier performance independent of class proportions, defined as the mean of the false positive and false negative rates. in the simulations, we computed the optimal bayes error rate and estimated the optimal auc using monte carlo estimation from the known distributions. the simulations were repeated  <dig> times which was sufficient to generate small standard errors on the plots to allow the demonstration of the bias effects with statistical significance. each bootstrap validation procedure used  <dig> replications.

real-world dataset
we used the van 't veer  <cit>  breast cancer dataset as an example of a relatively weak-signal prognostic microarray study. the dataset contains  <dig> samples, with  <dig> in the poor prognosis  class, and  <dig> in the good prognosis class. this dataset was classified using an svm  and all  <dig> genes. the classifier output was the decision values and not posterior probabilities. the auc was computed using both the pooling and averaging strategies. random subsamples were used to generate curves of varying sample sizes.

a useful check for remaining bias in classifier evaluation is to perform a label permutation test. permutation of the class labels yields a no-signal dataset with an expected random performance over multiple permutations. any significant deviations of the mean performance from the expected random performance level are likely caused by some uncorrected bias; such a check will also detect the other forms of bias discussed in the introduction. the permutation test was applied for measuring auc and error rate. in the case of error rate, the initial dataset was balanced by subsampling of the majority class to  <dig> samples, and stratified sampling of this balanced dataset was used, leading to an expected error rate of  <dig> . except where otherwise noted, at least  <dig> repetitions were performed for all experiments. each bootstrap run consisted of  <dig> replicates.

RESULTS
simulation
the results for unstratified cross-validation  shows that all classifiers are pessimistically biased compared with the bscv results, indeed, many classifiers show aucs below the random  <dig>  level for imbalanced datasets. as expected, the minimum distance classifier, not incorporating prior proportions, does not show any significant stratification bias. importantly, the relative ordering of the classifiers differs from cv to bscv, e.g. using cv, linear svm with c =  <dig> would appear to be worse than rbf svm for not too extreme proportions while it is clearly better using bscv. thus, this experiment also demonstrates that model selection could be impacted by stratification bias.

using bscv, which removes the stratification bias, dlda and minimum distance classifier have equal aucs, as expected, and they are both the best performing classifiers . the stratification bias of the svms is actually greater than that for dlda in this simulation. this is due to the svms being less effective classifers on this dataset and showing a lower auc. as discussed in the theoretical analysis section, with a weaker signal the prior proportions assume greater relative importance and thus leads to an increased stratification bias. for comparison, the hanley-mcneil estimate of se of the auc for a single sample of the simulation at proportion  <dig>   is  <dig> , and this is comparable to the bias shown in some svms at this class proportion. note that all of the induction algorithms have somewhat lower auc at the limits of the class proportions, even with bscv: this is due in part to such imbalanced datasets being intrinsically more difficult to classify, and not only due to stratification bias. the standard errors of all plots were lower than  <dig> . for all proportions and all classifiers, except the minimum distance classifier, the difference in auc between cv and bscv was found to be statistical significant with p values lower than  <dig> .

we also reran the experiments using 10-times repeated cv and bscv averaging results over  <dig> runs. the obtained graphs are almost identical to those in figure  <dig> and are therefore not depicted. the standard deviations for the two representative classifiers when using non-repeated and repeated cv and bscv are given in supplementary figure  <dig> . the standard deviations decrease only moderately when using the 10-times repeated versions instead of single runs of cv and bscv. the reason for this is that the noise between the different runs mainly stems from the difference in the data sets and not from the different separation of the data set into folds.

real-world dataset
the next series of experiments used a randomised version of the van 't veer dataset. figure  <dig> shows the results for a linear kernel svm using the pooling strategy for auc estimation. balanced, stratified cv and balanced loocv are approximately at the expected  <dig>  line. unstratified cv and loocv are pessimistically biased. stratified cv shows some small remaining stratification bias for small sample sizes – figure  <dig> showed that stratified cv has some remaining covariance between training and test sizes and so this bias is expected. note that stratified bootstrap also shows some remaining stratification bias for very small sample sizes. although in theory stratified bootstrap has no covariance between training and test set sizes, the training set sizes are made constant by sample replication, and presumably the effect of a duplicate sample on the training of the classifier would be weaker than a truly independent pattern, and so the "effective" training set size of a class is less than the sample size would suggest . also, results for svm with a radial basis function , aka gaussian kernel, with parameter values c =  <dig> and σ <dig> =  <dig>  are presented. only  <dig> random permutations were done with this svm type due to the computational cost, which however is sufficient to demonstrate the bias. it is known that for such kernels, which increase the effective dimensionality of the data, the svm will underfit and tend to a majority voter in large areas of the parameter space  <cit> , and so rely maximally upon the prior proportion information; therefore it would be expected to perform poorly and suffer maximally from stratification bias. indeed, figure  <dig> demonstrates very large pessimistic biases. the hanley-mcneil se estimate for sample size  <dig> is  <dig> ; note that the stratification bias for the rbf svm exceeds the se, indeed, some care would be needed to avoid confusing such a large negative auc with a genuine signal. note that stratified cv still shows substantial biases.

CONCLUSIONS
in this paper we have analysed a previously under-appreciated bias which can strongly affect evaluation of small sample size , low-signal datasets typical of microarray studies. we showed that common sample-reuse validation schemes such as cv and bootstrap can lead to large pessimistic biases due to correlated class proportions between training and test sets.

we have performed a systematic study of this bias using simulation and a real-world dataset, and have demonstrated and evaluated how this bias can affect not only accuracy, but also auc estimates. in low-signal microarray datasets, this bias can dominate the signal, and confound both model evaluation and model selection. in the case of model evaluation, while in practical applications a pessimistic bias can be, arguably, less detrimental than an optimistic bias, both are important to correct. a pessimistic bias could cause the rejection of a model which otherwise fulfills the required criteria. for example, when evaluating the effect of features in low-signal datasets in bioinformatics applications  a pessimistic bias can cause useful features to be misinterpreted or missed. in the case of model selection, the ranking of the performance of the models may change as different classifiers suffer from this stratification bias to differing extents, as noted in the experimental section .

from the analysis in this paper, we have identified several approaches to ameliorate or remove this form of stratification bias and allow more accurate estimates of error rate and auc for small, low-signal datasets: for auc and roc curve estimation, very substantial systematic stratification biases are introduced by the "pooling" estimation strategy; by contrast, we have shown that the "averaging" strategy of estimating the auc per-fold avoids this bias and is the recommended approach. note that  loocv, although commonly used for evaluating small datasets  <cit> , can only use the pooling strategy and so is contraindicated when evaluating auc. loocv can also exhibit a substantially biased error rate for weak signal, small, balanced, datasets and so should be avoided for such datasets.

as a general solution to remove this form of stratification bias, which can also be used with other performance measures including error rate and ber, we have demonstrated that the newly introduced balanced, stratified cross-validation and balanced loocv; stratified bootstrap; or stratified repeated holdout can avoid this stratification bias, and are recommended over their unstratified versions.

authors' contributions
bjp conceived the study. bjp, sg and jb drafted the manuscript and implemented the experiments. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplementary simulation results. supplementary simulation results: figure  <dig> shows standard deviations of single runs of two induction algorithms for standard bscv and cv and 10-times repeated bscv and cv; figure  <dig> shows simulated classification results using dlda with d' =  <dig>  and d' =  <dig> ; figure  <dig> shows simulated classification results for various induction algorithms for d' =  <dig> .

click here for file

 acknowledgements
nicta is funded by the australian government's department of communications, information technology and the arts and the australian research council through backing australia's ability and the ict centre of excellence program.
