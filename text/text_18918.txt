BACKGROUND
in the post-genomic era, the development of technologies for sequencing the genome and transcriptome has become a key issue in the global analysis of biological systems. even with the lowering cost of sequencing data, the majority of rna-seq experiments are still suffering from low replication numbers. the identification of differential expressed  genes and transcripts is still a key question of interest in many biological studies. to date, there are many methods that provide a test of whether a gene is de or not  <cit> , including cufflinks  <cit> , deseq  <cit>  and edger  <cit> . a feature in all of these methods is moderation of gene-wise variance estimates to improve de inference. moderation is important in small samples size comparisons, increasing both the power and accuracy of a de test  <cit> . the key differences between these methods are the extent of the moderation and their common variance estimate--the variance estimate that the procedure is moderating towards.

deseq and edger account for the heteroscedasticity observed in the read counts of genes by modelling the relationship between expected value of the count and its variability. we propose using additional information, such as gene length and variance estimates from external datasets, as explanatory variables to further model the heterogeneity seen in the observed gene variances. combining these improved models of gene variance with a moderation method  <cit>  creates a robust tool for estimating gene variances and hence calling differential gene expression. when evaluated on publicly available data this tool offers both improved gene ranking and power of detection when compared to deseq and edger. this method is implemented in the r package sydseq available on http://www.maths.usyd.edu.au/u/jeany/software.htm.

rna-seq
the development of high throughput sequencing technologies has made it possible to sequence the transcriptome at a much higher resolution and coverage than was previously available. sequencing of cdna samples  has a dynamic range larger than that of microarrays  <cit> . this, combined with its high level of reproducibility  <cit>  and falling cost, makes high throughput sequencing technologies an increasingly attractive alternative to microarrays for transcriptome analysis. in the following we will describe how variances are estimated for rna-seq data with small samples sizes.

a typical rna-seq data analysis work flow consists of many steps. these steps generally consist of mapping, summarisation, normalisation, differential expression analysis and systems biology  <cit> . the summarisation step counts how many reads were mapped to each gene or transcript. we will consider the case of mapping to genes rather than transcripts, so for us summarisation results in a matrix of counts, where the rows and columns correspond to genes and samples respectively.

let yij be the observed read count for the ith gene in the jth sample where sample j belongs to treatment t =  <dig>   <dig>  let σi <dig> and μi be the variance and mean read count for gene i. for ease of presentation we will assume that all effects that are generally normalised for or modelled, such as library sizes and gc content, remain constant across samples. the technical variability for a gene count in rna-seq can be modelled quite reliably as poisson  <cit> . this is attractive in situations of low replication as one parameter can be estimated to describe both the mean and variance of a gene. modelling the data as poisson will give a very reliable estimate of which genes have changed in expression between any two samples. however many experiments are not simply focused on the detection of gene expression differences between any two samples focusing instead on the differences between any two types of cells for example. this distinction is important as it requires us to not only model the technical variability of the experiment but to also model the biological variability of a particular cell type .

an over-dispersed poisson, a discrete distribution with dispersion greater than a poisson, can be modelled using a negative binomial. a negative binomial random variable, y , can be parametrised as

  p=r+y-1ypry. 

this standard formulation is generally referred to as nb <dig>  under this formulation, the biological variability of the expression of a gene is modelled as a quadratic function of its mean expression μ:

  σ2=μ+bμ <dig>  

where as b=1r gets small the negative binomial will approach a poisson. the parameter b has been referred to as the coefficient of biological variation. a negative binomial is generally parametrised as a function of r and p. however, by parametrising a negative binomial in terms of its mean μ and variance σ <dig> where

  r=μ2σ2-μ 

  p=μσ <dig> 

and σ <dig> >μ, a negative binomial can then be used to model counts that have untraditional mean-variance relationships. this relationship is generally expressed as

  σ2=μ+f 

where f explains the biological variability can be fitted by some form of local regression  <cit> . this formulation of σ <dig> highlights that σ <dig> should always be greater than or equal to μ.

in current rna-seq experiments it is still quite common to see experiments with very little biological replication. estimating variances from few observations is unstable  <cit> . to improve the stability and accuracy of these variance estimates there have been many methods proposed to shrink the variances to some common value for microarrays  <cit>  and rna-seq  <cit> . we will refer to this as moderation. by stabilising the variances and sharing information moderation also increase the power of a statistic as this increases the degrees of freedom of a variance estimate  <cit> .

heterogeneous gene variances
it is well accepted that some genes have a higher variance than other genes  <cit> . that is, some genes vary in expression more from cell to cell, person to person, or treatment to treatment than other genes. in rna-seq datasets, genes with larger average expression have on average larger observed variances. instead of shrinking the estimate of a genes variance towards some common value  to improve stability  <cit> , edger and deseq shrink the estimate towards some fitted curve describing the relationship between mean and variance. we refer to this fitted curve as the common variance. in doing this they are making the strong assumption  that genes with a similar average count should have a similar variance.

we incorporate external data from rna-seq and microarrays on mouse striatum and rna-seq data from different tissues to better estimate variances and hence identify de genes between c57bl/6j and dba/2j mouse striatum samples.

methods
tshrink+
we propose using local regression  <cit>  to fit a smoothed surface through any number of variables , γ ...) that may help to explain the observed pooled sample variances σ^2gene=s <dig>  we estimate the common variances σcommon <dig> as

  σ^common2=μ+fμ,γ <dig> γ <dig> ... 

when using variance estimates from other rna-seq experiments, these variances will also have a very strong mean-variance relationship. for use as an explanatory variable we normalise the external variance estimates in such a way that they have mean zero and variance one for all ranges of expression.

to illustrate how this improved common variance can aid in moderation we propose using a quasi-empirical bayes moderation method  <cit> , where the variance is moderated as

  σ^shrink2=λσ^common2+σ^gene <dig>  

and σ^shrink <dig>  σ^common <dig> and σ^gene <dig> are the moderated, common and sample variances. without making distributional assumptions, the shrinkage parameter λ can be estimated by the equation

  λ=min <dig> ∑k=1varσk2/σk2∑k=1σ^k2/σ^k2- <dig>  

λ is the ratio of the expected and average squared error of the common variance estimate. due to the large amount of smoothing that is used in estimating the common variance, we will make the assumption that the data, standardised using the common variance estimate, is approximately standard normal. under this assumption the variance of σk2/σk <dig> which has n -  <dig> degrees of freedom is 2/. our estimate of λ then becomes

  λ=min <dig> nn- <dig> ∑k=1σ^k2/σ^k2- <dig>  

a wald test for each gene is then performed using the statistic

  ȳt1-ȳt2σshrink21n1+1n <dig>  

where we utilise the welch-satterthwaite equation  <cit>  to estimate its degrees of freedom v ^. we have assumed earlier that that the degrees of freedom corresponding to common variance is νcommon = ∞ and can thus estimate vk as

  v ^k=λσ^k2+1-λσ^k221-λ2vgeneσ^k <dig> 

where νgene = n -  <dig>  for simplicity, rather than using a different v for each gene we instead use one degrees of freedom estimate, vshrink, for all genes, taken as the mean of the v ^k's.

data
bottomly dataset
the bottomly data  <cit>  was used as the main analysis dataset for evaluation and was chosen because of its relatively large number of biological replicates. the pre-processed rna-seq data comparing ten c57bl/6j  and eleven dba/2j  mouse striatum was downloaded from the recount project  <cit>  as a matrix of counts. for simplicity only the first ten dba/2j samples were used. all data used in the analysis are normalised counts as deseq and edger do not accept gene-wise normalisation factors. to model the disparate library sizes and biases of pcr amplification observed in the data, a cyclic robust linear model was used. using the first sample in the dataset as a reference, m values were calculated for each gene in the remaining samples and a straight line was fitted through the m-values using gc-content as an explanatory variable. the m-values were then normalised to this line such that the average m-value was zero over the range of gc-content. after this normalisation there were still batch and other sample specific effects evident in the data. these were normalised out using a cyclic loess  <cit>  strategy as described in additional file  <dig>  this normalisation appeared to be more suitable than ruv  <cit>  and sva  <cit>  improving concordance with microarray results as seen in additional figures  <dig> and  <dig> in additional file  <dig> .

external datasets
sample variances from three datasets were used as sources of additional information to aid in the estimation of the common variance. these are described in table  <dig>  all rna-seq data were mapped to the mm <dig> mouse genome using bowtie  <cit>  and normalised for gc content bias and library size differences as the bottomly dataset was. the microarray data were read and processed using the r packages affy  <cit>  and gcrma  <cit> .

variance estimates from these three datasets are be used to improve the estimation of the common variance function in the main analysis dataset.

evaluation study
in this study, we evaluate our proposed method of improving variance estimation for differential gene expression analysis, tshrink+. this evaluation consists of two components, assessing the capacity of a common variance estimate to explain the observed gene sample variances and evaluating how improving this common variance estimate can aid in the detection of differentially expressed genes. the performance of tshrink+ will also be compared with two commonly used packages, edger and deseq. this evaluation study is built upon one main dataset, the bottomly data, and three datasets which are used for additional information.

in order to assess the capacity of a common variance estimate to explain the observed gene sample variances we will use the shrinkage coefficient λ, described in the methods section, as a statistic. λ is the ratio of the expected and average squared error of the common variance estimate. we aim to assess the effectiveness of using information in addition to the average expression of a gene to estimate a common variance function. variance estimates from the external datasets described in table  <dig> and also gene length are used to aid in the estimation of the common variance functions of one hundred random comparisons of n samples of b <dig> mouse striatum tissue with n samples of d <dig> mouse striatum tissue. this is performed for one additional dataset at a time. the average λ value is calculated for each n comparison and information source using only the genes that are present in all data sources.

we then further demonstrate that improving the information content of an additional information source improves the estimation of the common variance. this will be achieved by using variance estimates from the d <dig> mice to aid in the estimation of a common variance function of the b <dig> mice. the variance estimates from a random n d <dig> mouse samples are used to estimate the common variance function of a random four b <dig> mouse, this is repeated one hundred times and average λ values are calculated.

we will assess the influence of using additional information and moderation on the detection of differentially expressed  genes. to do this we compare

 <dig>  a t-test ,

 <dig>  a moderated t-test  and

 <dig>  a moderated t-test using additional information .

these will also be compared to

 <dig>  deseq using only the common variance ,

 <dig>  deseq using the maximum of the common variance and sample variance  and

 <dig>  edger using a trended common variance and empirical bayes to shrink the gene sample variances towards the common variance .

to assess the effectiveness of the six de methods, a standard t-test was performed comparing ten b <dig> and ten d <dig> mouse striatum samples. in all of the following, the results of this t-test are taken to be the "truth". from this t-test a gene is conservatively called "truly" de if it has a bonferroni adjusted p-value of less than  <dig> . a gene is called "truly" not de if it has an unadjusted p-value greater than  <dig> . we will then evaluate the ability of the de methods to recover the information in the comparison of ten b <dig> samples with ten d <dig> samples by smaller comparisons of n b <dig> samples and n d <dig> samples, for n ranging from two to five. this is done by comparing a random set of n b <dig> and n d <dig> mouse striatum samples one hundred times and then

• generating receiver operator curves ,

• calculating partial areas under the roc for fpr less than  <dig>  and

• calculating true positives  and false positives  using a bonferroni adjusted p-value cut-off of  <dig> .

RESULTS
the estimation of the common variance
we begin by examining the effect of using information from different additional sources to help explain the variances observed in the bottomly data. that is, assessing the impact that each of the additional datasets in table  <dig> can have on estimating the pooled variances of n b <dig> vs n d <dig> mouse striatum comparisons. thus we only consider one additional dataset at a time and do not consider how they could interact. when used to help fit the common variance surface, using information from any of the additional data sources improve the estimate of the common variance as seen in figure  <dig>  this is observed through all of the average λ's being higher when using additional information when compared to using only the mean. λ is proportional to the reciprocal of the average squared error of the variance estimates, thus a larger λ corresponds to a better estimate of the common variance. a λ value of one implies that the common variance is representative of the population variance. a λ of zero suggests that the common variance estimate is failing to describe the observed gene sample variances.

the more relevant the information contained in the additional data source, the greater the improvement seen in the common variance estimate. as is perhaps expected either of the two striatum tissue datasets, rna-seq and microarray, when used to estimate the common variance produce the largest λ, with microarray striatum and rna-seq striatum only slightly out performing hippocampus. spleen and lung both also increase λ highlighting that information can still be gained from unrelated tissue types, however, liver and heart barely increase λ at all. this can mostly be explained by the use of liver and heart resulting in the variance of one gene, transthyretin, being severely under-estimated. if this gene is excluded the λ generated by using liver and heart are much similar to that of spleen and lung. including information on gene length also has the potential to improve variance information however this appears to relatively decrease as the sample size n increases.

improving the accuracy of the sample variance decreases λ and improving the accuracy of the common variance increases λ. as the sample size n increases, λ decreases. this is because as n increases the accuracy of the gene sample variance estimates increase. as the estimation of the gene sample variances improves, the inability of the common variance to describe the gene variances becomes more clear. the converse of this is seen in table  <dig>  as the information content of the additional data source improves, variance estimates from d <dig> mice calculated with increasing sample sizes, the ability of the common variance to describe the observed gene variances, calculated from four replicates of b <dig> mice, also improves. λ is doubled by using ten replicates of d <dig> mouse as opposed to nothing, that is, the average squared error of the common variance is halved.

the average λ values calculated using a random n d <dig> mouse striatum samples to estimate the variance of a random four b <dig> mouse striatum samples from one hundred simulations.

the impact of moderation on inferring differential expression
the aim of the remainder of the evaluation is to assess how the use of moderation affects inference on differential gene expression. this is done by assessing the impact of moderation on both gene ranking and sensitivity. moderation is used to both increase the sensitivity of a test, by increasing the degrees of freedom of the variance estimate, and to improve the ranking of a test, by improving the accuracy of the variance estimate.

we will start by simply comparing the t-test , moderated t-test  and a moderated t-test using additional information . for the additional data source used by tshrink+, the four striatum rna-seq samples  <cit>  in table  <dig> were chosen as they gave the second highest value but were not generated from the same lab as the analysis dataset .

by first considering only four vs four comparisons, the ability of moderation to improve gene ranking is illustrated in figure 2a where a partial average roc curve from one hundred four vs four comparisons of b <dig> and d <dig> mouse striatum is plotted for each method. this curve shows each methods tpr for a range of fpr, where a method is deemed to have ranked genes better than another at a given fpr if its tpr is higher. here we see that tshrink  performs better than t  for all fpr less than  <dig> . tshrink+  offers a similar improvement again on top of that of tshrink nearly doubling the improvement of tshrink to t.

moderation improves gene ranking and improving what a method moderates too can improve gene ranking further. this is again illustrated in figure 3a, where the partial area under the roc curve is plotted for a range of n vs n comparisons. a value of  <dig> corresponds to a perfect ranking and a value of zero corresponds to the most imperfect ranking. for all n considered tshrink+ appears to double the improvement of tshrink when compared to t. the relative improvements decrease as n increases as the information in the sample variance increases in comparison to the common variance.

moderation can improve the sensitivity of a test for differential expression as seen in figure 2b. figure 2b plots the average number of true positive genes called at varying p-value cut-offs for one hundred four vs four tests. at a bonferroni adjusted p-value cut-off of  <dig>   t calls  <dig> tp, tshrink  <dig> tp and tshrink+  <dig> tp. these improvements are seen at very little cost the the number of false positives called. the number of tp and fp called at a bonferroni adjusted p-value cut-off of  <dig>  for n ranging from two to five are plotted in figures 3b and 3c respectively. here we see the number of tp called for tshrink+ increases as n increases and the number fp decreasing as n increases. while the number of tp called also increase for t, it decreases for tshrink over this range of n. the number of tp called by tshrink will decrease until tshrink converges to t when it will continue to increase. tshrink may be over-zealous in its calling of tp calling a relatively large amount of fp as well for small n.

comparison with edger and deseq
tshrink+ performs favourably when compared to both deseq and edger when considering gene ranking. when assessing gene ranking using figure 3a, tshrink+ performs marginally better than deseqmax  which is better than edger  and deseqcommon . the relative performance of tshrink+ over deseqmax increases as n increases. for n equal to five edger performs worse than t. it could be argued that this is because t is becoming closer to the t-test that was used as "truth", however this behaviour is also observed when using the results from microarray array data  <cit>  as "truth" as seen in supplementary figure  <dig> in additional file  <dig>  this is performance could also be explained by edger over moderating to a common variance that is become decreasingly relevant as n increases.

tshrink+ compares comparably to edger and deseq when assessing sensitivity. t selects a similar number of tp at the cut-off when compared edger but selects less fp as seen in figures 3b and 3c. while deseqmax does not select as many tp for the given cut-off as deseqcommon it selects dramatically less fp.

CONCLUSIONS
using additional information improves the estimation of the common variance and the detection of differentially expressed genes. our differential expression test, tshrink+ which incorporates information from additional datasets, showed marked improvement in both gene ranking and sensitivity over a moderated t-test, tshrink, and a standard t-test, t. tshrink+ also performed favourably against edger and deseq when comparing gene ranking and comparably when assessing sensitivity.

whilst tshrink+ can offer improvements to a differential expression analysis it also provides insight into avenues for further research. the moderation used in tshrink+  <cit>  can be drastically affected by genes with unusual variances. a more sophisticated methodology which manages the influence of these genes on moderation could offer potentially large improvements. while using local regression to t the common variance when incorporating one additional dataset is easy to implement, it does not scale well to the use of multiple information sources. a parametric based approach may make the integration of multiple data sources feasible.

this methodology should be considered as a complement, not a replacement, for meta-analysis when similar studies to the rna-seq study of interest exist. tshrink+ leverages only the variance estimates from external datasets to improve the variance estimation in the study of interest. if information exists on the changes of expression between conditions as well, a researcher may be remiss to not utilise this information through the use of existing meta-analysis methodologies.

using external data to improve the estimation of the common variance for a particular problem highlights the significance of access to public data repositories like the gene expression omnibus   <cit> . these repositories have the ability to actualise improved inference lending both confidence and power to results. projects like recount  <cit>  aid in this process by providing access to pre-processed data that avoids the duplication of the computationally intensive procedure of both downloading and processing large datasets.

competing interests
the authors declare that they have no competing interests.

authors' contributions
ep developed the method, implemented the algorithm and drafted the manuscript. mb, dl, and yy participated in all aspects of the study and helped to draft the manuscript. all authors read and approve of the final manuscript.

declarations
the publication costs for this article were funded by the corresponding author's institution.

this article has been published as part of bmc genomics volume  <dig> supplement  <dig>  2013: selected articles from the eleventh asia pacific bioinformatics conference : genomics. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcgenomics/supplements/14/s <dig> 

supplementary material
additional file 1
additional file  <dig> includes a description of the normalisation used in the evaluation and additional figures.

click here for file

 acknowledgements
we would like to thank terry speed, john robinson, uri keich, samuel müller and john ormerod for their insightful comments. this work was supported in part by arc through grants ft <dig> , australian postgraduate award  and the alzheimer's association .
