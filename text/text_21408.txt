BACKGROUND
this paper presents an efficient exact parallel algorithm for the planted motif search  problem also known as the  motif problem  <cit> . a string of length l is caller an l-mer. the number of positions where two l-mers u and v differ is called their hamming distance and is denoted by hd. for any string t, t is the substring of t starting at position i and ending at position j. the pms problem is the following. given n sequences s <dig> s <dig> …,s
n
 of length m each, from an alphabet Σ and two integers l and d, identify all l-mers m,m∈Σ
l
, that occur in at least one location in each of the n sequences with a hamming distance of at most d. more formally, m is a motif if and only if ∀i,1≤i≤n,∃j
i
,1≤j
i
≤m-l+ <dig>  such that hd≤d.

the pms problem is essentially the same as the closest substring problem. these problems have applications in pcr primer design, genetic probe design, discovering potential drug targets, antisense drug design, finding unbiased consensus of a protein family, creating diagnostic probes and motif finding . therefore, efficient algorithms for solving the pms problem are very important in biology and bioinformatics.

a pms algorithm that finds all the motifs for a given input is called an exact algorithm. all known exact algorithms have an exponential worst case runtime because the pms problem is np-complete  <cit> . an exact algorithm can be built using two approaches. one is sample driven: for all 
n
 possible combinations of l-mers coming from different strings, generate the common neighborhood. the other is pattern-driven: for all Σ
l
 possible l-mers check which are motifs. many algorithms employ a combination of these two techniques. for example,  <cit>  and  <cit>  generate the common neighbors for every pair of l-mers coming from two of the input strings. every neighbor is then matched against the remaining n- <dig> input strings to confirm or reject it as a motif. other algorithms  consider groups of three l-mers instead of two.

pms algorithms are typically tested on instances generated as follows :  <dig> dna strings of length  <dig> are generated according to the i.i.d.  model. a random l-mer is chosen as a motif and planted at a random location in each input string. every planted instance is modified in at most d positions. for a given integer l, the instance  is defined to be challenging if d is the smallest integer for which the expected number of motifs of length l that occur in the input by random chance is ≥ <dig>  some of the challenging instances are ,,,,,,, , etc.

the largest challenging instance solved up to now has been . to the best of our knowledge the only algorithm to solve  has been qpms <dig>  <cit> . the algorithm in  <cit>  can solve instances with relatively large l  provided that d is at most l/ <dig>  however, most of the well known challenging instances have d>l/ <dig>  pairmotif  <cit>  can solve instances with larger l, such as  or , but these are significantly less challenging than . furthermore, for instances that current algorithms have been able to solve, the runtimes are often quite large.

in this paper we propose a new exact algorithm, pms <dig>  which can efficiently solve both instances with large l and instances with large d. the efficiency of pms <dig> comes mainly from reducing the search space by using the pruning conditions presented later in the paper, but also from a careful implementation which utilizes several speedup techniques and emphasizes cache locality.

one of the basic steps employed in many pms algorithms  is that of computing all the common neighbors of three l-mers. in qpms <dig>  this problem is solved using an integer linear programming  formulation. in particular, a large number of ilp instances are solved as a part of a preprocessing step and a table is populated. this table is then repeatedly looked up to identify common neighbors of three l-mers. this preprocessing step takes a considerable amount of time and the lookup table calls for a large amount of memory. in this paper we offer a novel algorithm for computing all the common neighbors of three l-mers. this algorithm eliminates the preprocessing step. in particular, we don’t solve any ilp instance. we also don’t employ any lookup tables and hence we reduce the memory usage. we feel that this algorithm will find independent applications. specifically, we state and prove necessary and sufficient conditions for  <dig> l-mers to have a common neighbor.

methods
for any l-mer u we define its d-neighborhood as the set of l-mers v for which hd≤d. for any set of l-mers t we define the common d-neighborhood of t as the intersection of the d-neighborhoods of all l-mers in t. to compute common neighborhoods, a natural approach is to traverse the tree of all possible l-mers and identify the common neighbors. a pseudocode is given in appendix  <dig>  a node at depth k, which represents a k-mer, is not explored deeper if certain pruning conditions are met. thus, the better the pruning conditions are, the faster will be the algorithm. we discuss pruning conditions in a later section.

pms <dig> consists of a sample driven part followed by a pattern driven part. in the sample driven part we generate tuples of l-mers originating from different strings. in the pattern driven part we generate the common d-neighborhood of such tuples. initially we build a matrix r of size n× where row i contains all the l-mers in s
i
. we pick an l-mer x from row  <dig> of r and push it on a stack. we filter out any l-mer in r at a distance greater than 2d from x. then we pick an l-mer from the second row of r and push it on the stack. we filter out any l-mer in r that does not have a common neighbor with the l-mers on the stack; then we repeat the process. if any row becomes empty, we discard the top of the stack, revert to the previous instance of r and try a different l-mer. if the stack size is above a certain threshold  we generate the common d-neighborhood of the l-mers on the stack. for each neighbor m we check whether there is at least one l-mer u in each row of r such that hd≤d. if this is true then m is a motif. the pseudocode of pms <dig> is given in appendix  <dig> 

a necessary and sufficient condition for  <dig> l-mers to have a common neighbor is discussed in the section on pruning conditions. for  <dig> or more l-mers we only have necessary conditions, so we may generate tuples that will not lead to solutions. however, due to the way the filtering works, the following observations apply. let the stack at any one time contain tl-mers: r <dig> r <dig> …,r
t
 where r
t
 is at the top of the stack. evidently, the l-mers on the stack pass the necessary pruning conditions for tl-mers. however, the following are also true. for any two l-mers r
i
 and r
j
 on the stack, there exists a common neighbor. this is true because whenever we filter an l-mer we make sure it has at least one neighbor in common with the l-mer at the stop of the stack. thus, we can prove by induction that any two l-mers in the stack have a common neighbor. second, any triplet of the form r <dig> r <dig> r
i
, 2<i≤t, has at least one common neighbor. this is true because when the stack had only the first two l-mers we filtered out any l-mer which doesn’t make a compatible triplet with the two. in general, for any number p<t the  tuples of the form r <dig> r <dig> …,r
p
,r
i
 where p<i≤t must pass the pruning conditions for p+1l-mers. therefore, even though the pruning for more than  <dig> l-mers is not perfect, the algorithm implicitly tests the pruning conditions on many subsets of the l-mers in the stack and thus decreases the number of false positive tuples generated.

speedup techniques
sort rows by size
an important speedup technique is to reorder the rows of r by size after every filtering step. this reduces the number of tuples that we consider at lower stack sizes. these tuples require the most expensive filtering because as the stack size increases, fewer l-mers remain to be filtered.

compress l-mers
we can speed up hamming distance operations by compressing all the l-mers of r in advance. for example, for dna we store  <dig> characters in a  <dig> bit integer, divided into  <dig> groups of  <dig> bits. for every  <dig> bit integer i we store in a table the number of non-zero groups of bits in i. to compute the hamming distance between two l-mers we first perform an exclusive or of their compressed representations. equal characters produce bits of  <dig>  different characters produce non-zero bits. therefore, one table lookup provides the hamming distance for  <dig> characters. one compressed l-mer requires l∗⌈log|Σ|⌉ bits of storage. however, we only need the first  <dig> bits of this representation because the next  <dig> bits are the same as the first  <dig> bits of the l-mer  <dig> positions to the right of the current one. therefore, the table of compressed l-mers only requires o) words of memory.

preprocess distances for pairs of l-mers
the filtering step tests many times if two l-mers have a distance of no more than 2d. thus, for every pair of l-mers we compute this bit of information in advance.

cache locality
we can update r in an efficient manner as follows. every row in the updated matrix r′ is a subset of the corresponding row in the current matrix r, because some elements will be filtered out. therefore, we can store r′ in the same memory locations as r. to do this, in each row, we move the elements belonging to r′ at the beginning of the row. in addition, we keep track of how many elements belong to r′. to revert from r′ back to r, we restore the row sizes to their previous values. the row elements will be the same even if they have been moved within the row. the same process can be repeated at every step of the recursion, therefore the whole “stack” of r matrices is stored in a single matrix. this reduces the memory requirement and improves cache locality. the cache locality is improved because at every step of the recursion, in each row, we access a subset of the elements we accessed in the previous step, and those elements are in contiguous locations of memory.

find motifs for a subset of the strings
we use the speedup technique described in  <cit> : compute the motifs for n′<n of the input strings, then test each of them against the remaining n-n′ strings. for the results in this paper n′ was heuristically computed using the formula provided in appendix  <dig> 

memory and runtime
since we store all matrices r in the space of a single matrix they only require o) words of memory. to this we add o words to store row sizes for the at most n matrices which share the same space. the bits of information for compatible l-mer pairs take o)2/w) words, where w is the number of bits in a machine word. the table of compressed l-mers takes o) words. therefore, the total memory used by the algorithm is o+)2/w).

the more time we spend in the sample driven part, the less time we have to spend in the pattern driven part and vice-versa. ideally we want to choose the threshold where we switch between the two parts such that their runtimes are almost equal. the optimal threshold can be determined empirically by running the algorithm on a small subset of the tuples. in practice, pms <dig> heuristically estimates the threshold t such that it increases with d and |Σ| to avoid generating very large neighborhoods and it decreases with m to avoid spending too much time on filtering. all the results reported in this paper have been obtained using the default threshold estimation provided in appendix  <dig> 

parallel implementation
we can think of m-l+ <dig> independent sub problems, one for each l-mer in the first string. the first string in each sub problem is an l-mer of the original first string and the rest of the strings are the same as in the original input. because of this, the problem seems to be “embarrassingly parallel.” a straightforward parallelization idea is to assign an equal number of subproblems to each processor. this method has the advantage that no inter-processor communication is necessary beyond broadcasting the input to all processors. this method would work well for algorithms where each subproblem is expected to have a similar runtime. however, for pms <dig>  the runtime of each subproblem is very sensitive to the size of the neighborhoods for various combinations of l-mers and therefore some processors may end up starving while others are still busy.

to alleviate the above shortcoming we employ the following strategy. the processor with rank  <dig> is a scheduler and the others are workers. the scheduler spawns a separate worker thread to avoid using one processor just for scheduling. the scheduler reads the input and broadcasts it to all workers. then each worker requests a sub problem from the scheduler, solves it and repeats. the scheduler loops until all jobs have been requested and all workers have been notified that no more jobs are available. at the end, all processors send their motifs to the scheduler. the scheduler loops through all the processors and collects the results. the scheduler then outputs the results.

pruning conditions
in this section we present pruning conditions applied for filtering l-mers in the sample driven part and for pruning enumeration trees in the pattern driven part.

two l-mers a and b have a common neighbor m such that hd≤d
a
 and hd≤d
b
 if and only if hd≤d
a
+d
b
. for  <dig> l-mers, no trivial necessary and sufficient conditions have been known up to now. in  <cit>  sufficient conditions for  <dig> l-mers are obtained from a preprocessed table. however, as l increases the memory requirement of the table becomes a bottleneck. we will give simple necessary and sufficient conditions for  <dig> l-mers to have a common neighbor. these conditions are also necessary for more than  <dig> l-mers.

let t be a set of l-mers and m be an l-mer. if ∑u∈thd>|t|d then, by the pigeonhole principle, one l-mer must have a distance from m greater than d. therefore, m cannot be a common neighbor of the l-mers in t. if we have a lower bound on ∑u∈thd for any m, then we can use it as a pruning condition. if the lower bound is greater than |t|d then there is no common neighbor for t. one such lower bound is the consensus total distance.

definition 1
let t be a set of l-mers, where k=|t|. for every i, the set t <dig> t <dig> ..,t
k
 is called the i-th column of t. let m
i
 be the maximum frequency of any character in column i. then cd= ∑i= <dig> .lk-m
i
 is called the consensus total distance of t.

the consensus total distance is a lower bound for the total distance between any l-mer m and the l-mers in t because, regardless of m, the distance contributed by column i to the total distance is at least k-m
i
. the consensus total distance for a set of two l-mers a and b will be denoted by cd. also notice that cd=hd. we can easily prove the following lemma.

lemma 1
let t be a set of l-mers and k=|t|. let d <dig> d <dig> …d
k
 be non-negative integers. there exists a l-mer m such that hd≤d
i
,∀i, only if cd≤Σi=1kdi.

theorem 1
let t be a set of  <dig> l-mers and d <dig> d <dig> d <dig> be non-negative integers. there exists a l-mer m such that hd≤d
i
,∀i,1≤i≤ <dig> if and only if the following conditions hold:

i)cd≤d
i
+d
j
,∀i,j,1≤i<j≤3

ii)cd≤d1+d2+d3

proof
the “only if” part follows from lemma  <dig> 

for the “if” part we show how to construct a common neighbor m provided that the conditions hold. we say that a column k where t1=t2=t <dig> is of type n <dig>  if t1≠t2=t <dig> then the column is of type n <dig>  if t1=t3≠t <dig> the column is of type n <dig> and if t1=t2≠t <dig> then the column is of type n <dig>  if all three characters in the column are distinct, the column is of type n <dig>  let n
i
,∀i,0≤i≤ <dig> be the number of columns of type n
i
. consider two cases:

case 1) there exists i,1≤i≤ <dig> for which n
i
≥d
i
. we construct m as illustrated in figure  <dig>  pick d
i
 columns of type n
i
. for each chosen column k set m=t
j
 where j≠i. for all other columns set m=t
i
. therefore cd=d
i
. for j≠i we know that cd≤d
i
+d
j
 from condition i) . we also know that cd+cd≤cd from the triangle inequality. it follows that cd≤d
j
. since cd=hd it means that m is indeed a common neighbor of the three l-mers.

case 2) for all i,1≤i≤ <dig> we have n
i
<d
i
. we construct m as shown in figure  <dig>  for columns k of type n <dig> n <dig> and n <dig> we set m=t <dig>  for columns of type n <dig> we set m=t <dig>  for any i,1≤i≤ <dig> the following applies. if n
i
+n4≤d
i
 then the hamming distance between m and t
i
 is less than d
i
 regardless of what characters we choose for m in the columns of type n <dig>  on the other hand, if n
i
+n4>d
i
 then m and t
i
 have to match in at least n
i
+n4-d
i
 columns of type n <dig>  thus, we pick max columns of type n <dig> and for each such column k we set m=t
i
. now we prove that we actually have enough columns to make the above choices, in other words Σi=13max≤n <dig>  this is equivalent to the following conditions being true: 

a)for any i,1≤i≤ <dig> we want n
i
+n4-d
i
≤n <dig>  this is true because n
i
<d
i
.

b)for any i,j,1≤i<j≤ <dig> we want +≤n <dig>  this can be rewritten as n
i
+n
j
+n4≤d
i
+d
j
. the left hand side is hd which we know is less or equal to d
i
+d
j
.

c)we want Σi=13ni+n4-di≤n <dig>  this can be rewritten as n1+n2+n3+2n4≤d1+d2+d <dig>  the left hand side is cd which we know is less than d1+d2+d <dig> 

one of our reviewers kindly pointed out that the above proof is similar to an algorithm in  <cit> .

RESULTS
pms <dig> is implemented in c++ and uses openmpi for communication between processors. pms <dig> was evaluated on the hornet cluster in the booth engineering center for advanced technology  at university of connecticut. the hornet cluster consists of  <dig> nodes, each equipped with  <dig> intel xeon x <dig> westmere cores and  <dig> gb of ram. the nodes use infiniband networking for mpi. in our experiments we employed at most  <dig> cores on at most  <dig> nodes.

we generated random  instances according to  <cit>  and as described in the introduction. for every  combination we report the average runtime over  <dig> random instances. for several challenging instances, in figure  <dig> we present the speedup obtained by the parallel version over the single core version. for p= <dig> cores the speedup is close to s= <dig> and thus the efficiency is e=s/p=94%.

in figure  <dig> we show the effect of the first speedup technique  on the runtime. note that all other speedups are enabled, only sorting rows by size is varied. the figure shows that sorting the rows by size improves the speed of pms <dig> by 25% to 50%.

the runtime of pms <dig> on instances with l up to  <dig> and d up to  <dig> is shown in figure  <dig>  instances which are expected to have more than  <dig> motifs simply by random chance  are excluded. the expected number of spurious motifs was computed as described in appendix  <dig>  instances where d is small relative to l are solved efficiently using a single cpu core. for more challenging instances we report the time taken using  <dig> cores.

a comparison between pms <dig> and qpms <dig>  <cit>  on challenging instances is shown in table  <dig>  both programs have been executed on the hornet cluster. qpms <dig> is a sequential algorithm. pms <dig> was evaluated using up to  <dig> cores. the speedup of pms <dig> single core over qpms <dig> is shown in figure  <dig>  the speedup is high for small instances because qpms <dig> has to load an ilp table. for larger instances the speedup of pms <dig> sharply increases. this is expected because qpms <dig> always generates neighborhoods for tuples of  <dig> l-mers, which become very large as l and d grow. on the other hand, pms <dig> increases the number of l-mers in the tuple with the instance size. with each l-mer added to the tuple, the size of the neighborhood reduces exponentially, whereas the number of neighborhoods generated increases by a linear factor. the ilp table precomputation requires solving many ilp formulations. the table then makes qpms <dig> less memory efficient than pms <dig>  the peak memory used by qpms <dig> for the challenging instances in table  <dig> was  <dig> mb whereas for pms <dig> it was  <dig> mb. furthermore, due to the size of the ilp table, qpms <dig> is not able to solve any instances where l> <dig>  pms <dig> is the first algorithm to solve the challenging instances  and .

1
16
32
48
comparison between qpms <dig> and pms <dig> on challenging instances. pms8
p
 means pms <dig> used p cpu cores. both programs have been executed on the same hardware and the same datasets. the times are average runtimes over  <dig> instances for each dataset.

some recent results in the literature have also focused on instances other than the challenging ones presented above. a summary of these results and a comparison with pms <dig> is presented in table  <dig>  starting with the most recent results. these results have been obtained on various types of hardware: single core, multi-core, gpu, grid. in the comparison, we try to match the number of processors whenever possible. the speed difference is of several orders of magnitude in some cases which indicates that the pruning conditions employed by pms <dig> exponentially reduce the search space compared to other algorithms.

side by side comparison between pms <dig> and recent results in the literature. time is reported in seconds , minutes  or hours . note that the hardware is different, though we tried to match the number of processors when possible. also, the instances are randomly generated using the same algorithm, however the actual instances used by the various papers are most likely different. for pms <dig>  the times are averages over  <dig> randomly generated instances.

we also compared pms <dig> with qpms <dig> on the real datasets discussed in  <cit> . we excluded datasets with less than  <dig> input sequences because these are not very challenging. for each dataset we chose two combinations of l and d. these combinations were chosen on a dataset basis because for large values of d the number of reported motifs is excessive and for small values of d the instance is not very challenging. to make qpms <dig> behave like pms <dig> we set the quorum percent to 100% . in table  <dig> we report the dataset name, the total number of sequences, the total number of bases in each dataset, the l and d combination and the runtimes of the two algorithms. note that both algorithms are exact algorithms and therefore the sensitivity and specificity are the same. similar to the comparison on synthetic data, the comparison on real data reveals that pms <dig> outperforms qpms <dig> 

for each dataset we tested two combinations of l and d. for qpms <dig> we set q =n. both algorithms were executed on a single cpu core. time is reported in seconds, rounded up to the next second.

CONCLUSIONS
we have presented pms <dig>  an efficient algorithm for the pms problem. pms <dig> is able to efficiently generate neighborhoods for tl-mers at a time, by using the pruning conditions presented in this paper. previous algorithms generate neighborhoods for only up to three l-mers at a time whereas in pms <dig> the value of t is increased as the instances become more challenging and therefore the exponential explosion is postponed. the second reason for the efficiency of pms <dig> comes from the careful implementation which employs several speedup techniques and emphasizes cache locality.

appendix
appendix  <dig> generating neighborhoods
algorithm  <dig>  generateneighborhood  

appendix  <dig> pms <dig> pseudocode
algorithm  <dig>  pms <dig>  

appendix  <dig> challenging instances
for a fixed l, as d increases, the instance becomes more challenging. however, as d increases, the number of false positives also increases, because many motifs will appear simply by random chance. the expected number of spurious motifs in a random instance can be estimated as follows . the number of l-mers in the neighborhood of a given l-mer m is n=Σi=0dd. the probability that m is a d-neighbor of a random l-mer is p=n/|Σ|
l
. the probability that m has at least one d-neighbor among the l-mers of a string of length m is thus q=1-)
m-l+ <dig>  the probability that m has at least one d-neighbor in each of n random strings of length m is q
n
. finally, the expected number of spurious motifs in an instance with n strings of length m each is: |Σ|
l
q
n
. in this paper we consider all combinations of l and d where l is at most  <dig> and the number of spurious motifs  does not exceed  <dig>  note that for a fixed d, if we can solve instance  we can also solve all instances  where l′>l, because they are less challenging than .

appendix  <dig> heuristics for t and n
in the methods section we mentioned that we heuristically estimate the threshold t at which we switch from the pattern driven to the sample driven part. the exact formula used by the algorithm to compute t was t=maxlogΣ-logm⌋). this follows the intuition that t should increase with Σ to avoid large neighborhoods and decrease with m to avoid spending too much time on filtering.

in the speedup techniques section we mentioned a speedup where we compute motifs for a subset of n′<n strings. by default, the algorithm heuristically computes n′ as n′=min. these simple heuristics worked well enough on all our test cases, however the user can easily override them.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
mn and sr designed and analyzed the algorithms. mn implemented the algorithms and carried out the empirical experiments. mn and sr analyzed the empirical results and drafted the manuscript. all authors read and approved the final manuscript.

