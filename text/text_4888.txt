BACKGROUND
with technological improvements and decreasing costs, microarrays are quickly becoming an affordable analytical tool for genetics analysis. additionally, the arrays being used are of increasing spot density, allowing for more genes to be tested at once. one impact of the resulting increase in data flow is that it will become more likely that a researcher using microarrays will have greater difficulty making sense of results from preliminary statistical analyses without further computational exploration. in other words, once the researcher has received a list of genes, by whatever statistical means, that are differentially expressed, the task of determining the biological implications of that gene list will need to be performed by statistical methods utilizing computers.

numerous research groups are developing software tools to perform an interpretation of the list of differentially expressed genes, generally by mapping against previously developed knowledgebases such as the gene ontology   <cit>  or genmapp  <cit>  as a reference data set . some tools, such as david  <cit>  and fatigo  <cit>  examine the percentage of the gene list that is directly associated with a node of the knowledgebase. this method is extremely fast due to its simplicity, but it does have disadvantages, which are also due to the simplicity of the analysis. for example, in some of these tools, information about how nodes  of the knowledgebase are related to each other is ignored. additionally, in hierarchical structures such as go, genes with a less precise functional definition will be associated with a node closer to the root than a gene with a more precise definition. in such a case, the information content about the two genes is split into different nodes, reducing the power of the analytical method.

other tools such as gominer  <cit>  and mappfinder  <cit>  analyze the gene list in a broader context of the knowledgebase, looking for patterns of a larger scale than a single node. mappfinder searches for whole pathways  over-represented by the gene list. gominer performs analyses using genes associated with a node in go or genes associated with any children of that node, sometimes called "inclusive analysis". in this way, gominer minimizes the power reduction of some simpler methods.

these tools provide a powerful way for the researcher to quickly get a summarization of the gene list within a biological context. one common problem for the inclusive analytical methods, especially those using knowledgebases with polyhierarchical structures  like go, is correcting for multiple statistical tests, usually thousands. in such a case, a bonferroni correction is overly conservative to the point of being counterproductive since few if any results of the interpretation remain significant  <cit> . as of june,  <dig>  godb had > <dig>  dag nodes which may be tested, meaning a correction factor of greater than four orders of magnitude would be needed in a bonferroni correction. other standard methods used include controlling the family-wise error rate  using a numerical correction of the p-value  or controlling the false positive rate . in both cases the methodology should again be overly conservative since, when using inclusive analysis, the p-values for each go term are not independent  <cit> .

here we present, in the context of the program goarray  <cit> , a preliminary analysis of the feasibility of using permutation-based simulations to provide an alternate method of handling the multiple-testing problem. goarray analyzes the gene list in the context of go. permutations of the differentially expressed gene list are generated from the total list of genes represented on the microarray to estimate the distribution of significant go terms expected by chance. we analyze the nature of the distribution of significant terms in reference to varying p-values and numbers of differentially expressed genes using publicly available data sets. we then compare the list of significant terms between data sets. finally, we discuss the implications of this distribution to provide one solution to the multiple-test problem when analyzing microarray data in the context of go.

RESULTS
four of the test data sets analyzed were extracted from the national center for biotechnology information's  gene expression omnibus   <cit> . the first is an array of drosophila markers used by arbeitman et al.  <cit>   for a time-series study of the drosophila life cycle. this array represents  <dig> microarray spots, from which there are  <dig> genes as represented by unique flybase  <cit>  accession numbers. the estimation of the distribution took ~ <dig>  hours. the mean numbers of significant terms for each combination of p-value cutoff and "gene of interest"  count are presented in figure  <dig> . a "trough" of less significant terms than the two surrounding goi counts for the same p-value for term significance can be observed in the topology diagonally from  <dig> goi and a p-value of  <dig>  down to  <dig> goi and a p-value of  <dig> . there are additional, similarly "wave-shaped" features, although of lesser degree. for example, there is one with a slower rate of change running diagonally from  <dig> goi in the vicinity of p-values  <dig>  and  <dig>  , and from  <dig> goi between p-values  <dig>  and  <dig> . <dig> . overall, however, there is an increase in the number of significant terms with both increasing goi and p-value cutoff. the increase is sharp from  <dig> to  <dig> goi, and then more gradual with increasing goi. the increase in significant terms with increasing p-value cutoff, however, is much more gradual.

the second data set is for an array of drosophila markers used by meiklejohn et al.  <cit>   for a study of interspecies variation. the array represents  <dig> cdna probes, from which there are  <dig> unique flybase accession numbers. the estimation of the distributions took ~ <dig>  hours. the mean number of significant terms for each combination of p-value and goi count were estimated by simulation . again, there is a general increasing trend in the number of significant terms with increasing numbers of goi and p-value cutoffs. there is another observed trough, however, starting from  <dig> goi and a p-value cutoff of  <dig>  diagonally to  <dig> goi and a p-value cutoff of  <dig> . as with the first data set, there are also "wave-like" structures in the topology such as from approximately  <dig> goi and a p-value of  <dig>  to  <dig> goi and a p-value of  <dig> . given the similarity in topology to the first data set, the possibility that these two sets of flybase accessions have large overlap was considered. indeed, ~90%  of the flybase accessions from the arbeitman data set are observed as ~50% of the meiklejohn data set.

that the two data sets would not be independent is to be expected, since one goal of both studies was to examine as many of the known drosophila genes as possible. this non-independence will probably be observed for most pairs of drosophila microarrays. because of this, we extracted the  <dig> flybase accession numbers from the meiklejohn data set that did not overlap with the arbeitman data set, and estimated the distribution of significant terms for just those genes as a comparison to the other two data sets. the simulation took ~ <dig>  hours and the results are presented in figure  <dig>  as with the previous two data sets, there is generally an increase in the number of significant terms with increasing numbers of goi and p-value cutoffs. there is also another trough extending from  <dig> and  <dig> goi with a p-value cutoff of  <dig>  to  <dig> goi with a p-value cutoff of  <dig> .

since the two real drosophila data sets and one simulated drosophila data set all had a trough in the distribution, it was possible that this is due to inherent structure in go specifically for drosophila. therefore, we extracted three other data sets for different species. the first of these non-drosophila sets of genes was for s. cerevisiae , a set of  <dig> genes. the overall topology is quite regular . unlike the drosophila data sets, within the range of goi and p-values considered there was no evidence of a trough  in the distribution. there was one data point  with a mean number of significant terms  less than that for the same p-value and the next lower number of goi . the difference between the two means is minute, and may not be meaningful. there are a few regions with leveling , but these were not large and overall pattern appears somewhat predictable.

the second and third non-drosophila data set were constructed by taking all wormbase  <cit>  and gramene  <cit>  accessions from godb. in the case of the wormbase data set , the distribution again appears somewhat regular , with just a few regions of leveling, but no major trough. there was, however, more "wave-like" structure with increasing numbers of goi and more stringent p-values. the same was noted for the gramene data set , although the leveling was considerably more apparent . this was especially true in the region from  <dig> goi and p =  <dig>  to  <dig> goi and p =  <dig>  × 10- <dig>  even with this region, however, the distribution appears somewhat smoother than that observed for the drosophila data set. the region in question is located near the edge of the explored space, however, and a pattern may emerge with higher number of goi.

finally, to see if the same terms were consistently appearing as significant in the drosophila data sets, we compared the actual number of significant occurrences for each term for the two data sets extracted from geo . genes with a five-fold or greater change in expression were chosen as goi. a p-value cutoff of  <dig>  was chosen. the list of terms that came up as significant, and the number of permutations out of  <dig> that were significant, was recorded and presented as a scatter plot . from the plot, it can clearly be seen that there is a lack of correlation between the number of times a term appears as a significant in one data set compared to the second data set, even accounting for a different maximum number of significant terms in the two data sets. a handful of terms were significant a similar number of times in each data set relative to the maximum count of significant terms for the respective data sets. in other words, a handful of terms mapped near the line extending from the origin to the point marked by the maximum value along each axis, which would mark roughly equivalent relative occurrences of the term as significant between the two data sets. however, these terms were near the origin and the vast majority of points were along the axes, showing a clear lack of correlation in how often terms were observed as significant between these two closely related lists of genes.

discussion
based on this set of simulations, predictability appears to be limited to specific data sets. one method of correcting our expectations after performing multiple tests would be to calculate a factor by which to modify α based on the dag of go terms. in other words, one could use an adjusted p-value to control the fwer or fdr. controlling for these two types of error by use of adjusted p-values, however, assumes independence of the tests  <cit> . since there is currently no practical method for directly untangling the interdependence of terms in the go hierarchy to generate a less conservative correction, adjusted p-values are limited to overly strict results.

another method would be to determine a formula that conservatively approximates the simulated distribution. unfortunately, the only commonality between the distributions is that, with the exception of the drosophila data sets, the number of significant terms increases with an increasing number of goi and an increasing p-value cutoff. the magnitude and detailed shape of the distribution varies between all tested data sets. even in the more regular non-drosophila data sets, there were some fluctuations in the distribution, and a smooth surface was not observed. since neither of the two methods of correcting expectations is currently feasible, it appears that, for now, we are forced to rely on simulation-based methods to estimate the expected distribution of significant terms for each set of genes being examined.

while it would be desirable to have a smooth topology that allows for a simple formulaic calculation of the number of significant terms one would expect by chance, it is unfortunately not observed for the drosophila data sets examined here. the trough that disrupts the drosophila data sets was not observed, however, in the data sets for other species. the cause of this trough is undetermined, but may be due to structure within the graph of go terms associated with flybase accessions. alternatively, there could be structure within the chosen genes that is more evident with smaller data sets, since the trough appears to be deepest for the two smaller data sets. one way to approach the question of cause would be to examine which, if any, terms are observed disproportionately in the permuted sets. based on the frequency of terms it may be possible to observe a pattern in either the genes tested or the set of associated go terms. we have been unable to observe such a pattern, but that does not mean it does not exist. if one could be found, it may give insights into how to dissect the structure, possibly leading to a more elegant solution to the multiple test problem than a simulation-based approach.

though we were unable to find hints of an easy formulaic way to correct our expectations, we may be able to find a practical  method of correction through simulations. there are several ways in which simulated estimates of the distribution could be implemented to provide a less conservative method, yet still statistically appropriate, than a bonferroni correction to handle the problem of correcting our expectations after performing multiple statistical tests. the simplest to implement, and likely the most accurate, would be to perform a permutation-based simulation for each analysis of a microarray data set in the context of go. the primary problem with this approach is that it is computationally intensive since the goi would need to be permutated and scored a thousand or more times for every analysis of a microarray. while tools such as parallel processing can reduce the absolute time necessary to perform the simulations, it is not the most elegant way to solve the problem.

another method would be to simply generate the estimated distribution, again using a permutation-based simulation, once for each set of accession numbers  for a range of goi counts and p-value cutoffs, similar to what we have done here but in finer detail, and storing the results. the most conservative simulation distribution neighboring the experimental combination of p-value and goi count could then be extracted from the stored table to provide an estimated distribution. one problem with this approach is determining how fine a table to design . with a simple  <dig> ×  <dig> matrix, the simulation took ~16– <dig> hours on a single  <dig>  ghz xeon processor. a finer matrix of parameter values will result in a better estimation of the topology, but consumes more time to compute in a non-linear fashion. however, if a large number of microarray experiments is to be conducted with a single geometry, this method would reduce the total time to estimate significance across all experiments since the simulation would only need to be performed once. additionally, it will be necessary to determine what range of values should be considered. for the smallest data set tested here , goi lists representing less than 20%  of the accession numbers were used. the amount of computation time that should be dedicated to simulating the distribution of significant terms expected by chance will likely be a balance determined by the computing resources available, estimates of how many experiments will use the array design, and minimal p-value cutoffs and maximal goi parameter values determined by the predicted user needs.

CONCLUSIONS
based on the large simulations performed here, it appears that the rate at which terms are observed as significant is not predictable between sets of genes for a given goi count and p-value cutoff. even within a particular species, there is no correlation in relative frequency at which particular terms are significant. therefore, permutation-based simulations appear to be the most reliable way to generate an estimate of the expected distribution of significant terms. as a result, we plan to extend the confidence tests in the next version of goarray  by implementing a "false positive frequency estimation" for individual terms based on simulation results. also, since which terms are observed as significant appears to be highly dependent on the structure of the gene list, and possibly the list of goi, we plan to examine the merits of bootstrap methods  rather than a strict permutation method .

in the best case, it appears feasible to pre-generate the estimated distribution of the number of significant terms through a permutation-based simulation, then use a lookup table during analyses of experimental data sets. in the worst case, one would need to generate the distribution for each experimental data set, possibly testing various p-value cutoffs to determine where power is maximal. even in the worst case, currently available processing power allows the test for a single set of genes and a single p-value cutoff to be performed in well under an hour. while near-instant results would be desirable by end users, the worst case scenario is still quite practical and will only improve over time alongside general computer performance. thus, relying on permutation-based methods may not be a serious inconvenience, and in fact a highly accurate method of assessing our confidence in the results of the analysis.

