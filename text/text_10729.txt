BACKGROUND
the need for smoothing model selection criteria
dna copy number alterations  can result from various types of genomic rearrangements, and are important in the study of many types of cancer  <cit> . in particular, clinical outcome of patients with neuroblastoma has been shown to be worse for tumors with segmental alterations or breakpoints in specific genomic regions  <cit> . thus, to construct an accurate predictive model of clinical outcome for these tumors, we must first accurately detect the precise location of each breakpoint.

in recent years, array comparative genomic hybridization  microarrays have been developed as genome-wide assays for cnas, using the fact that microarray fluoresence intensity is proportional to dna copy number  <cit> . in parallel, there have been many new mathematical models proposed to smooth the noisy signals from these microarray assays in order to recover the cnas  <cit> . each model has different assumptions about the data, and it is not obvious to decide which model is appropriate for a given data set.

furthermore, most models have parameters that control the degree of smoothness. varying these smoothing parameters will vary the number of detected breakpoints. most authors give default values that accurately detect breakpoints on some data, but do not necessarily generalize well to other data. there are some specific criteria for choosing the degree of smoothness in some models  <cit> , but it is impossible to verify whether or not the mathematical assumptions of these models are satisfied for real noisy microarray data.

to motivate the use of their cghflasso smoothing model, tibshirani and wang write “the results of a cgh experiment are often interpreted by a biologist, but this is time consuming and not necessarily very accurate”  <cit> .

in contrast, this paper takes the opposite view and assumes that the expert interpretation of the biologist is the gold standard which a model should attain. the first contribution of this paper is a smoothing model training protocol based on this assumption.

in practice, visualization tools such as vamp are used to plot the normalized microarray signals against genomic position for interpretation by an expert biologist looking for cnas  <cit> . then the biologist plots a model and varies its smoothness parameter, until the model seems to capture all the visible breakpoints the data. in this article, we make this model training protocol concrete by using an annotation database to encode the expert’s interpretation.

the particular type of annotations that we propose are counts of breakpoints in genomic regions. by visual inspection of the noisy signal, it is not obvious to locate the exact location of a breakpoint, but it is easy to determine whether or not a region contains a breakpoint. so rather than defining annotations in terms of precise breakpoint locations, we instead define them in terms of regions. for every region, we record the number of breakpoints that an expert expects in that region. these annotated regions can then be used to select an appropriate model, as shown in figures  <dig> and  <dig> 

we note that using databases of visual annotations is not a new idea, and has been used successfully for object recognition in photos and cell phenotype recognition in microscopy  <cit> . in array cgh analysis, some models can incorporate prior knowledge of locations of cnas  <cit> , but no models have been specifically designed to exploit visual breakpoint annotations.

our second contribution is a protocol to estimate the breakpoint detection ability of the trained smoothing models on real data. in the methods section, we propose to estimate the false positive and false negative rates of the trained models using cross-validation. this provides a quantitative criterion for deciding which smoothing algorithms are appropriate breakpoint detectors for which data.

the third contribution of this paper is a systematic, quantitative comparison of the accuracy of  <dig> common smoothing algorithms on a new database of  <dig> annotated neuroblastoma copy number profiles, which we give in the results and discussion section. there are several publications which attempt to assess the accuracy of smoothing algorithms, and these methods fall into  <dig> categories: simulations and low-throughput experiments. glad, dnacopy, and a hidden markov model were compared by examining false positive and false negative rates for detection of a breakpoint at a known location in simulated data  <cit> . however, there is no way to verify if the assumptions of the simulation hold in a real data set, so the value of the comparison is limited. in another article, the accuracy of the cnvfinder algorithm was assessed using quantitative pcr  <cit> . but quantitative pcr is low-throughput and costly, so is not routinely done as a quality control. so in fact there are no previous studies that quantitatively compare breakpoint detection of smoothing models on real data. in this paper we propose to use annotated regions for quantifying smoothing model accuracy, and we make available  <dig> new annotated neuroblastoma copy number profiles as a benchmark for the community to test new algorithms on real data.

several authors have recently proposed methods for so-called joint segmentation of multiple cgh profiles, under the hypothesis that each profile shares breakpoints in the exact same location  <cit> . these models are not useful in our setting, since we assume that breakpoints do not occur in the exact same locations across copy number profiles. instead, we focus on learning a model that will accurately detect an unknown, different number of breakpoints in each copy number profile.

to summarize, this article describes a quantitative method for dna copy number profile smoothing model selection. first, an expert examines scatterplots of the data, and encodes her interpretation of the breakpoint locations in a database of annotated regions. to repeat, the annotations represent an expert’s interpretation, not the biological truth in the tumor samples, which is unknown. we treat the annotated regions as a gold standard, and compare them to the breakpoints detected by  <dig> existing models. the best model for our expert is the one which maximizes agreement with the annotation database.

RESULTS
the  <dig> smoothing models described in the algorithms: copy number profile smoothing models section were applied to  <dig> neuroblastoma copy number profiles, described in the data: neuroblastoma copy number profiles section. after fitting the models, we used breakpoint annotations to quantify the accuracy of each model. we constructed  <dig> annotation databases based on  <dig> different experts’ interpretations of the same  <dig> profiles . the “original” annotations were created by typing  <dig> or  <dig> in a 6-column spreadsheet after systematic inspection of the same  <dig> regions on each profile. the “detailed” annotations were constructed by using guis which allow zooming and direct annotation on the plotted profiles. the  <dig> annotation data sets are mostly consistent, but the detailed annotations provide more precise breakpoint locations .

for the two annotation data sets , we show the annotation protocol, counts of annotated profiles and chromosomes, and counts of annotations.

for both annotation databases, we calculated the global and local error curves eglobal and eilocal, which quantify how many breakpoint annotations disagree with the model breakpoints. as shown in figure  <dig> for the original set of annotations, the smoothness parameter λ is chosen by minimizing the error curves.

among global models, cghseg.k and pelt.n exhibit the smallest training error
the global model is defined as the smoothness parameter λ^ that minimizes the global error eglobal, which is the total number of incorrect annotations over all profiles. training error curves for cghseg.k, pelt.n, flsa.norm, and dnacopy.sd are shown in figure  <dig>  an ideal global model would have zero annotation error eglobal= <dig> for some smoothness parameter λ^. however, none of the global models that we examined achieved zero training error in either of the two annotation databases. the best global models were the equivalent cghseg.k and pelt.n models, which achieved the minimum error of  <dig> % and  <dig> % in the original and detailed data sets.

the roc curves for the training error of the global models for each algorithm are traced in figure  <dig>  it is clear that the default parameters of each algorithm show relatively large false positive rates. the only exception is the pelt.default algorithm, which showed low false positive and true postive rates. the models chosen by maximizing agreement with the breakpoint annotation data also exhibit smaller false positive rates at the cost of smaller true positive rates. the roc curves suggest that the equivalent cghseg.k and pelt.n models are the most discriminative for breakpoint detection in the neuroblastoma data.

among local models, cghseg.k and pelt.n exhibit the smallest training error
since there is no global model that agrees with all of the annotations in either database, we fit local models with profile-specific smoothness parameters λ^i. for every profile i, the local model is defined as the smoothness parameter λ^i that minimizes the local error eilocal, the number of incorrect annotations on profile i. as shown in figure  <dig>  the local model fits the annotations at least as well as the global model: eilocal≤eilocal. however, the local model does not necessarily attain zero error. for example, figure  <dig> shows that dnacopy.sd does not detect a breakpoint in profile i= <dig> even at the smallest parameter value, corresponding to the model with the most breakpoints.

in figure  <dig>  we compare the fitting ability of the local models on the  <dig> annotation data sets. it clearly shows that some models are better than others for fitting the expert annotations of the neuroblastoma data. in particular, the equivalent cghseg.k and pelt.n local models show the best fit, with  <dig> % and  <dig> % error in the original and detailed data sets. note that these are lower error rates than the global models, as expected.

but even if the local models are better at fitting the given breakpoint annotations, they do not generalize well to un-annotated breakpoints, as we show in the next section.

global models detect un-annotated breakpoints better than default models
leave-one-out cross-validation was used to estimate the breakpoint detection of each model. figure  <dig> shows the error rates of each model, across both annotation data sets. it is clear that the training procedure makes no difference for models pelt.default, glad.default, dnacopy.default, cghseg.mbic, gada.default and cghflasso, which are default models with no smoothness parameters. each of these models is inferior to its respective global model in terms of breakpoint detection. the large error of these models suggest that the assumptions of their default parameter values do not hold in the neuroblastoma data set. more generally, these error rates suggest that smoothness parameter tuning is critically important to obtain an accurate smoothing of real copy number profiles.

to show an example of how the learned models outperform default models, figure  <dig> shows one representative profile with many breakpoints. note that the models were trained on other profiles, so the shown annotations can be used for model evaluation. for this profile, dnacopy.default shows  <dig> false positives and  <dig> false negatives, and dnacopy.sd shows no improvement with  <dig> false negatives. the pelt.default and cghseg.mbic show  <dig> and  <dig> annotation errors, respectively. the cghseg.k and pelt.n global models show only  <dig> annotation errors, demonstrating the usefulness of annotation-based model training.

in addition, additional file 1: figure s1-s <dig> compares these default and global models. again, the global models were learned on other profiles, so the shown annotations can be used for model evaluation.

global models detect un-annotated breakpoints better than local models
the leave-one-out cross-validation results in figure  <dig> also allow comparison of global and local models. for dnacopy.prune, glad.minbkpweight, glad.lambdabreak, dnacopy.sd and flsa, there is little difference between the local and global training procedures. for models flsa.norm, gada, pelt.n, and cghseg.k, there is a clear advantage for the global models which share information between profiles. the equivalent cghseg.k and pelt.n models show the minimal test error of only  <dig> % and  <dig> % in the original and detailed data sets.

only a few profiles need to be annotated for a good global model
to estimate the generalization error of a global model trained on a relatively small training set of t annotated profiles, we applied ⌊n/t⌋-fold cross-validation to the n= <dig> profiles.

for several training set sizes t, we plot the accuracy of the cghseg.k, pelt.n, gada, flsa.norm, dnacopy.sd and glad.lambdabreak models in figure  <dig>  it shows that adding more annotations to the training set increases the breakpoint detection accuracy in general, but at a diminishing rate. each model quickly attains its specific maximum, after only about t= <dig> training profiles.

in figure  <dig>  we used ⌊n/t⌋-fold cross-validation in the detailed annotations to estimate the error rates of all  <dig> models trained using only t= <dig> profiles.

the equivalent cghseg.k and pelt.n models show the best performance on these data, with an estimated breakpoint detection error of  <dig> %.

global models generalize across annotators
we assessed the extent to which the annotator affects the results by comparing models trained on one data set and tested on the other. figure  <dig> shows that test error changes very little between models trained on one data set or the other. this demonstrates that global models generalize very well across annotators.

timing pelt and cghseg
the pelt and cghseg models use different algorithms to calculate the same segmentation, which showed the best breakpoint detection performance in every comparison. but they are slightly different in terms of speed, as we show in figure  <dig> 

when comparing the global models, cghseg.k is somewhat faster than pelt.n. for cghseg.k, pruned dynamic programming is used to calculate the best segmentation μk for k∈{ <dig> …,20} segments, which is the slow step. then, we calculate the best segmentation for λ∈{λ <dig> …,λ100}, based on the stored μk values. in contrast, the pruned exact linear time algorithm must be run for each λ∈{λ <dig> …,λ100}, and there is no information shared between λ values.

timing the pelt and cghseg default models without tuning parameters shows the opposite trend. in particular, the default cghseg.mbic method is slower than the pelt.default method. this makes sense since cghseg must first calculate the best segmentation μk for several k, then use the mbic criterion to choose among them. in contrast, the pelt algorithm recovers just the μk which corresponds to the schwarz information criterion penalty constant β= logd. so if you want to use a particular penalty constant β instead of the annotation-guided approach we suggest in this article, the default pelt method offers a modest speedup over cghseg.

annotation-based modeling is feasible for high-density microarrays
although not the main focus of this paper, we have already started to apply annotation-based modeling to high-density microarrays. for example, figure  <dig> shows part of chromosome  <dig> from an affymetrix snp <dig> microarray. this microarray offers almost  <dig> million probes, and we show annotated breakpoints around  <dig> cnas from ≈1mb to ≈ <dig> kb. as long as there is gui software that supports zooming and annotation, it is feasible to apply annotation-based modeling.

CONCLUSIONS
we proposed to train breakpoint detection models using annotations determined by visual inspection of the copy number profiles. we have demonstrated that this approach allows quantitative comparison of smoothing models on a new data set of  <dig> neuroblastoma copy number profiles. these data provide the first set of annotations that can be used for benchmarking the breakpoint detection ability of future algorithms. our annotation-based approach is quite useful in practice on real data, since it provides a quantitative criterion for choosing the model and its smoothing parameter.

one possible criticism of annotation-based model selection is the time required to create the annotations. however, using the guis that we have developed, it takes only a few minutes to annotate the breakpoints in a profile. this is a relatively small investment compared to the time required to write the code for data analysis, which is typically on the order of days or weeks. in addition, in the neuroblastoma data, we observed that annotating only about  <dig> of  <dig> profiles was sufficient to learn a smoothness parameter that achieves the model-specific optimal breakpoint detection. more generally, our results suggest that after obtaining a moderately sized database of annotations, data analysis time is better spent designing and testing better models. additionally, the learned models generalized very well between annotators. so breakpoint annotations are a feasible approach for finding an accurate model and smoothing parameter for real copy number profiles.

we compared local models for single profiles with global models selected using annotations from several profiles. we observed that local models fit the given annotations better, but global models generalize better to un-annotated regions. in contrast with our results, it has been claimed that local models should be better in some sense: “it is clear that the advantages of selecting individual-specific λ values outweigh the benefit of selecting constant λ values that maximize overall performance”  <cit> . however, they did not demonstrate this claim explicitly, and one of the contributions of this work is to show that global models generalize better than local models, according to our leave-one-out estimates.

it will be interesting to apply annotation-based model training to other algorithms and data sets. in both annotation data sets we analyzed, cghseg.k and pelt.n showed the best breakpoint detection, but another model may be selected for other data.

our results indicate that even the best models have non-zero training and testing breakpoint detection error, which could be improved. to make a model that perfectly fits the training annotations, a dynamic programming algorithm called segannot was proposed to recover the most likely breakpoints that are consistent with the annotation data  <cit> . the test error of the cghseg model can be lowered by choosing chromosome-specific λ parameter values as a function of features such as variance and the number of points sampled  <cit> . developing a model that further lowers the test error remains an interesting direction of future research.

we have solved the problem of smoothness parameter selection using breakpoint annotations, but the question of detecting cnas remains. by constructing a database of annotated regions of cnas, we could use a similar approach to train models that detect cnas. annotations could be actual copy number  or some simplification . we will be interested in developing joint breakpoint detection and copy number calling models that directly use these annotation data as constraints or as part of the model likelihood.

