BACKGROUND
time is an important factor in developmental biology, especially in dynamic genetics. for example, when a number of genes are differentially expressed under two or more conditions, it is often of great interest to know which changes are causal and which are not. when different conditions are represented by different time-points, it helps us to understand not only how a gene gets turned on or off, but also which gene-gene relationships are based on the lags in the changes. novel genes have been identified by monitoring the transcription profiles during development  <cit>  or by looking at the differential responses of genes under different conditions  <cit> .

conventional time-series methods are not well suited to the analysis of microarray data. since the number of observed time-points in a microarray is usually very small, common methods such as auto-regression , moving-average  or fourier analysis modeling may not be applicable. furthermore, these classic autocorrelation approaches generate bias when applied to short time-course data  <cit> . in addition, observed time-points are sometimes distributed unevenly and the length of inter-time-points increases exponentially due to biological phenomena and resource limitation. for example, some commonly used time-points are  <dig>   <dig>   <dig>   <dig>  and  <dig> hours. with conventional approaches, it is not clear how to calculate the magnitude of the slope between adjacent time-points or how to determine the window size for smoothing, when needed. in order to address these problems, clustering analysis has been widely used. clustering algorithms, which explore the problem space whose size is the stirling's number ∑k=1ns
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadaaewaqaaiabdofatjabcicaoiabd6gaujabcycasiabdugarjabcmcapawcbagaem4aasmaeyypa0jaegymaedabagaemoba4ganiabgghildaaaa@39ed@ where

 s=1k!∑i=0kknand n is the number of genes
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgtbwucqggoaakcqwgubgbcqggsaalcqwgrbwacqggpaqkcqgh9aqpdawcaaqaaiabigdaxaqaaiabdugarjabcgcahaaadaaewaqaamaabmaabagaeyoei0iaegymaedacagloagaayzkaawaawbaasqabeaacqwgrbwaaaaabagaemyaakmaeyypa0jaegimaadabagaem4aasganiabgghildgcdaqadaqaauaabeqaceaaaeaacqwgrbwaaeaacqwgpbqaaaaacagloagaayzkaawaaewaaeaacqwgrbwacqghsislcqwgpbqaaiaawicacaglpaaadaahaawcbeqaaiabd6gaubaakiabbggahjabb6gaujabbsgakjabbccagiabd6gaujabbccagiabbmgapjabbohazjabbccagiabbsha0jabbigaojabbwgaljabbccagiabb6gaujabbwha1jabb2gatjabbkgaijabbwgaljabbkhayjabbccagiabb+gavjabbagamjabbccagiabbeganjabbwgaljabb6gaujabbwgaljabbohazbaa@70bf@ 

in order to group similar objects together, can identify potentially meaningful relationships between objects and often their results can be visualized  <cit> . phang et al.  <cit>  devised a non-parametric clustering algorithm using only the direction of change from one time-point to the next in order to group genes in a time-course study. ji et al.  <cit>  proposed a model-based clustering method based on a hidden markov model . these models assume that each gene expression profile has been generated by a markov chain with a certain probability. the original dataset of n time-points is standardized and then transformed into a three-digit-sequence  with the aid of a tolerance factor. luan et al.  <cit>  used cubic splines in building a mixed-effects model, where observed time-points are treated as samples taken from underlying smooth processes. ramoni et al.  <cit>  adopted a bayesian method for model-based clustering of gene expression dynamics. the method represents gene-expression dynamics as autoregressive equations and uses an agglomerative procedure to search for the most probable set of clusters given the available data. wu et al.  <cit>  considered a time-course gene expression dataset as a set of time series, generated by a number of stochastic processes. each stochastic process defines a cluster and is described by an autoregressive model. a relocation-iteration algorithm is proposed to identify the model parameters and each gene is assigned to an appropriate cluster based on posterior probabilities. ernst et al.  <cit>  assigned genes probabilistically to preselected sub-patterns which were generated independent of the data in a short time-course experiment.

as this wealth of approaches shows, a lot of effort has been put into developing clustering algorithms for gene clustering; however, they have some limitations. geneticists still need a more intuitive and statistically sound methodology. to address this issue, we propose a difference-based clustering algorithm  for a short time-course gene expression data. dib-c discretizes a gene into a symbolic pattern of the first- and second-order differences representing direction and rate of change, respectively. replicate and temporal order information from the input data are used in defining the clusters. dib-c outputs a cross-sectional view of cluster hierarchies with varied cutoffs, shown in a 2-dimensional map for biological interpretation. the clustering procedure used by dib-c is detailed in the methods section.

we now examine the limitations of standard clustering algorithms and explain how we addressed each of them in developing our algorithm. first, misleading or uninterpretable clusters can occur when one only considers the similarity of expression profiles, thereby disregarding discretization information. an example from real data  <cit>  is shown in figure  <dig>  the three yeast genes in figure  <dig> are well studied; we know that each gene plays a different role in yeast sporulation which is characterized by sequential transcription of sets of genes-'early', 'early-mid', 'middle', 'mid-late' and 'late'. every gene necessary for sporulation has been found to play a role in one of these five sets confirmed through genetic screens of visual assays. thi <dig> is known to have a specific temporal pattern in 'early-mid', pbp <dig> in 'mid' and cdc <dig> in 'mid-late'  <cit> . thus, all three genes have different profiles and roles. but these three genes would have put into the same cluster if a conventional clustering method was blindly performed considering profile similarity only.

second, the rate of change is ignored when delineating underlying patterns in traditional clustering algorithms. for example, cdc <dig> in figure  <dig> increases from  <dig> hr through  <dig> hr, but the rate of change decreases over time. this type of saturation is often observed in biological phenomena; examples include mrna accumulation, developmental acceleration, or gradual changes in the drug-response rate. although some discretization-based methods which use the direction of change have been presented  <cit> , none of these deal with differences in rates of change. we used the second-order difference – the difference between the first-order statistics – in dib-c in order to incorporate rate of change information into the clustering procedure.

third, replicates are not fully utilized in the existing algorithms. kerr et al.  <cit>  pointed out that replication in microarray experiments is a fundamental principle of good experimental design because it increases the precision of estimated quantities and provides information about the uncertainty of estimates. however, replicates were infrequently used in microarray experiments due to their high cost. but now, more replicates are being  used in order to achieve enough statistical power thanks to the dropping costs of microarrays with the advance of the microarray technology as in many other electronic products. even though appropriate methods are needed to analyze this emerging type of data, most of the current methods simply compute the average over replicates, disregarding variability. in contrast, dib-c makes use of moderated t-statistics  <cit> , which consider an empirical bayes variance estimate computed using the array replicates.

fourth, conventional clustering techniques such as hierarchical clustering  <cit> , tend to ignore temporal information by treating time-course data as an unordered collection of events under different conditions  <cit> . however, time-course experiments have a fixed order of conditions, i.e., the columns are not interchangeable  <cit> . the problem becomes more complicated in the presence of replicates, as any two members within-group are interchangeable unlike between-groups. dib-c incorporates this order-restriction in the algorithm.

fifth, template-based methods require prior knowledge to choose representative genes. in the yeast example, each gene was assigned to the nearest pre-chosen representative gene based on previous studies. peddada et al.  <cit>  also predefine a set of potential candidate profiles then assign each gene using the order-restricted inference method. however, these approaches are applicable only when there is enough information which is rare in practice.

finally, visualization of clustering results is not always informative. k-means  merely enumerates the list of genes, where each number signifies which cluster a gene belongs to. however, these are simply distinguishing symbols, adjacent numbers do not imply that two clusters are biologically related. self-organizing map  does a little better, as it displays clustering results in a 2-dimensional grid.

RESULTS
we evaluated the performance of our algorithm dib-c in comparison to k-means, som and short time-series expression miner  methods using both simulated and real data  <cit> . the simulation data had  <dig> clusters with  <dig> members each, at four time-points. there are eight replicates at each time point . real data on pancreas gene expression in mice  <cit>  was obtained from computational biology and informatics laboratory, university of pennsylvania  <cit> . we extracted  <dig>  gene expression measures from a unique set of probes and used six time-points with four or six replicates at each time point. the preprocessing methods used on the pancreas data are detailed in the methods section.

simulated data
for the simulated data, true clustering membership was used as knowledge external to the gene expression data. also, the agreement between the true and the resulting cluster memberships was measured. the adjusted rand index , an updated form of the rand index, is the number of agreements divided by the number of total objects  <cit>  defined as:

 ∑i,j−/12−/
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaadawcaaqaamaaqababawaaewaaeaafaqabegabaaabagaemoba42aasbaasqaaiabdmgapjabdqgaqbqabaaakeaacqaiyagmaaaacagloagaayzkaagaeyoei0yaasgbaeaadawadaqaamaaqababawaaewaaeaafaqabegabaaabagaemoba42aasbaasqaaiabdmgapjabc6cauaqabaaakeaacqaiyagmaaaacagloagaayzkaawaaewaaeaafaqabegabaaabagaemoba42aasbaasqaaiabc6cauiabdqgaqbqabaaakeaacqaiyagmaaaacagloagaayzkaaaaleaacqwgpbqaaeqaniabgghildaakiaawufacagldbaaaeaadaqadaqaauaabeqaceaaaeaacqwgubgbaeaacqaiyagmaaaacagloagaayzkaaaaaawcbagaemyaakmaeiilawiaemoaaogabeqdcqghris5aagcbawaasgbaeaadawcaaqaaiabigdaxaqaaiabikdayaaadawadaqaamaaqababawaaewaaeaafaqabegabaaabagaemoba42aasbaasqaaiabdmgapjabc6cauaqabaaakeaacqaiyagmaaaacagloagaayzkaagaey4kasyaaabeaeaadaqadaqaauaabeqaceaaaeaacqwgubgbdawgaawcbagaeiola4iaemoaaogabeaaaoqaaiabikdayaaaaiaawicacaglpaaaasqaaiabdqgaqbqab0gaeyyeiuoaasqaaiabdmgapbqab0gaeyyeiuoaaogaay5waiaaw2faaiabgkhitmaadmaabawaaabeaeaadaqadaqaauaabeqaceaaaeaacqwgubgbdawgaawcbagaemyaakmaeiola4cabeaaaoqaaiabikdayaaaaiaawicacaglpaaaasqaaiabdmgapbqab0gaeyyeiuoakmaaqababawaaewaaeaafaqabegabaaabagaemoba42aasbaasqaaiabc6cauiabdqgaqbqabaaakeaacqaiyagmaaaacagloagaayzkaaaaleaacqwgqbgaaeqaniabgghildaakiaawufacagldbaaaeaadaqadaqaauaabeqaceaaaeaacqwgubgbaeaacqaiyagmaaaacagloagaayzkaaaaaaaaaaa@825a@ 

where i and j index the clusters and classes, respectively. higher ari values indicate more accurate clusters. the ari is a more sensitive, generalized version of the original rand index and is used as our measure of comparison.

with ari measure, dib-c showed better accuracy across the cluster numbers than did the other three methods. under lower noise simulations of  <dig>   <dig>  and 5%, the maximum ari values were obtained by dib-c at the true number of clusters , indicating that dib-c has the highest accuracy of the three methods . under high noise , k-means achieved the maximal ari at  <dig> clusters, which was not the true cluster number. however, it is notable that dib-c peaks at the actual cluster number. dib-c outputs only in the neighborhood of the true cluster number unlike the other three methods because our algorithm refuses to separate insignificant changes.

as a data-driven evaluation measure without any external knowledge, the average proportion of the first eigenvalue  was used to delineate the best overall clustering results. apf is the normalized proportion of eigenvalues for each cluster defined by:

 ψ¯=1l∑l=1lψlwhere,ψl=γl1∑i=1pλlil=the number of clustersp=the number of time-pointsλli=the itheigenvalue in lthcluster
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaafaqaaegbcaaaaeaaiigacuwfipqegaqeaiabg2da9maalaaabagaegymaedabagaemitaweaamaaqadabagae8hydk3aasbaasqaaiabdygasbqabaaabagaemibawmaeyypa0jaegymaedabagaemitaweaniabgghildaakeaaaeaacqqg3bwdcqqgobaacqqglbqzcqqgybgccqqglbqzcqggsaalaeaaaeaaaeaacqwfipqedawgaawcbagaemibawgabeaakiabg2da9maalaaabagae83sdc2aasbaasqaaiabdygasjabigdaxaqabaaakeaadaaewaqaaiab=t7asnaabaaaleaacqwgsbabcqwgpbqaaeqaaaqaaiabdmgapjabg2da9iabigdaxaqaaiabdchawbqdcqghris5aaaaaoqaaaqaaiabbyeamjabg2da9iabbsha0jabbigaojabbwgaljabbccagiabb6gaujabbwha1jabb2gatjabbkgaijabbwgaljabbkhayjabbccagiabb+gavjabbagamjabbccagiabbogajjabbygasjabbwha1jabbohazjabbsha0jabbwgaljabbkhayjabbohazbqaaaqaaiabbcfaqjabg2da9iabbsha0jabbigaojabbwgaljabbccagiabb6gaujabbwha1jabb2gatjabbkgaijabbwgaljabbkhayjabbccagiabb+gavjabbagamjabbccagiabbsha0jabbmgapjabb2gatjabbwgaljabb2catiabbchawjabb+gavjabbmgapjabb6gaujabbsha0jabbohazbqaaaqaaiab=t7asnaabaaaleaacqwgsbabcqwgpbqaaeqaaogaeyypa0jaeeidaqnaeeiaagmaeeyzaumaeeiiaaiaemyaak2aawbaasqabeaacqwg0badcqwgobaaaagccqqglbqzcqqgpbqacqqgnbwzcqqglbqzcqqgubgbcqqg2bgdcqqghbqycqqgsbabcqqg1bqdcqqglbqzcqqggaaicqqgpbqacqqgubgbcqqggaaicqwgsbabdaahaawcbeqaaiabdsha0jabdigaobaakiabbogajjabbygasjabbwha1jabbohazjabbsha0jabbwgaljabbkhaybaaaaa@c719@ 

in this paper, eigenvalues are calculated from the within-cluster covariance matrix and assumed to be sorted in decreasing order so that the first eigenvalue corresponds to the largest eigenvalue, the second eigenvalue corresponds to the second largest eigenvalue and so on. from a dimension reduction perspective, the principal components of each resulting cluster lie in the directions of the axes of a constant density multi-dimensional ellipsoid  <cit> . if the relative magnitude of the first eigenvalue is large then the corresponding cluster is closer to linear in shape. recently, moller-levet et al. used the square root of the second eigenvalue as an overall clustering quality index for microarray data  <cit> . in the spirit of moller-levet, the ratio of the normalized eigenvalue to the total number of clusters is used as an evaluation measure.

with the apf measure, dib-c had the largest value in the neighborhood of the true cluster number  <dig> under  <dig>   <dig>  and 10% noise . the dots of dib-c appeared close to the true cluster number  <dig> and stayed only in the neighborhood of  <dig> with its apf values being the highest. based on this result, we argue that dib-c produces mostly linear-shaped clusters because it has the largest proportion of the first eigenvalue of each cluster covariance.

a two-dimensional pattern map  is shown to explain how the simulation data was generated . each gene is assigned to the true cluster where three first-order difference pattern on the columns are further partitioned into nine second-order patterns on the rows. in each gene, error bars are drawn around the mean for each time-points. ten member genes constitute a cluster with a total of  <dig> true clusters. then the clustering result of dib-c is shown in figure  <dig> to compare with the truth . the result is similar to the true answer since there was only one mis-clustered gene in the cluster  whose cluster size is  <dig>  actual membership of this gene is  whose cluster size is  <dig> 

the hierarchical layers of the simulated data are shown in figure 6; the four smallest thresholds are shown in this figure because the complete figure is so complex. every threshold level uniformly, and correctly, exhibited  <dig> clusters. after level  <dig>  there is no further repartitioning of a cluster and the three first differences are observed as three corresponding colors in each level.

real data
for real data, gene set enrichment using gene ontology  annotation was used instead of ari  since true cluster membership is not available. go is a structured, controlled vocabulary for describing the roles of genes and gene products  <cit> . following the work of gibbons et al., the molecular function aspect was used out of the three aspects of go. after mapping the third-level go id to our pancreas genes, a contingency table of  <dig>  genes by  <dig>  go ids was created. then the total mutual information mireal between the cluster result and all the go ids were computed. next, mirandom for a clustering result was obtained after random swapping of genes in the original clusters. this procedure was repeated  <dig>  times to get corresponding mirandoms. then, we subtracted the mean of mirandoms from mireal and divided it by the standard deviation of mirandoms. this is a z-score interpreted as a standardized distance between the mi value obtained from clustering after centering and scaling based on those mi values obtained by random assignments of genes to clusters. the higher the z-score, the better one's clustering result because it indicates the observed clustering result is further away from the distribution of the random clustering results  <cit> .

z-score for the pancreas data is shown in figure  <dig>  overall trend of the z-scores for the mutual information between clustering results and significant go annotation for the real data decreases with an increase in cluster size, as noted by gibbons et al.. when significant z-scores were considered , dib-c gave higher and more stable z-score values , achieving more significant and insightful clusters. dib-c had the largest z-score  at  <dig> clusters. this was obtained from the first- and the second-order thresholds pair of p-value cutoffs  for significant differences.

with the apf measure, both dib-c and stem outperformed som and k-means . dib-c gave the largest apf values across all cluster numbers larger than  <dig>  stem had the largest apf values when the cluster number was smaller than  <dig>  but the differences in apf values between stem and dib-c were small. while dib-c showed stable apf values, stem's values decreased as the cluster number increased. overall, dib-c had the largest  average magnitude of the first eigenvalue in each cluster.

for the pancreas dataset, three representative threshold levels, including the 'optimal' result with  <dig> clusters , are applied to construct the corresponding three hierarchical layers . levels  <dig> and  <dig> are included as ancestor layers of the optimal layer. level  <dig> had a z-score of  <dig>  at cluster number  <dig> with threshold pair ; level  <dig> had a z-score of  <dig>  at cluster number  <dig> with threshold pair . an interactive version of this hierarchical layer can be found at the supplementary webpage  <cit> .

the optimal clustering result from the last hierarchical layer is reconstructed as a two-dimensional pattern map for the pancreas data . dib-c partitioned  <dig>  probes of pancreas data into  <dig> clusters. the pattern map had six first-differences and  <dig> second-differences. as expected, a huge cluster  of the null pattern , ) was found.

discussion and 
CONCLUSIONS
dib-c is a novel clustering algorithm based on the first- and second-order differences of a gene expression matrix. our algorithm has several advantages over previous clustering algorithms for short time-course data with replicates. first, dib-c generates interpretable clusters through discretization. instead of producing many unlabeled partitions, dib-c offers self-explanatory clusters. the resulting pattern map visualizes using both horizontal  and vertical  structures. each cluster has a label composed of symbols indicating increases or decreases, which have intuitive biological interpretations. second, our algorithm deals with the rate of change: convex and concave categories are incorporated into the definition of the symbolic pattern. hence, we can discriminate genes into further subgroups. third, the identification power is increased by using both the mean and variance of replicates. conventional algorithms blindly use averaged summary data from replicates. in this way, two average values with different variances are treated equally, thereby decreasing the sensitivity to non-random patterns. fourth, temporal order is incorporated into the algorithm. column-wise shuffling  of input data would give a different output, which is not the case for k -means or som because they do not consider the order of input data points. fifth, dib-c requires no prior knowledge of representative genes. even after the appropriate clustering algorithm is chosen, deciding the optimal number of clusters is very important. dib-c overcomes this problem by exhaustive space searching in an efficient way. also, dib-c offers informative visualization. clusters are arranged so that closely related patterns are gathered together. such a meta-structure approach is often needed in developmental and cancer biology.

when a cluster has few members, the apf value tends to get large since most eigenvalues of its within-cluster covariance matrix could be zeros. for this potential bias of apf measure, we have assigned equally  <dig> members to each  <dig> cluster in the simulated data. apf values had a tendency to increase as the cluster number decreased  in the clustering algorithms other than dib-c. but our algorithm produced the highest apf value at the true cluster number  <dig> and only around this number. this result tells us that there might be a small bias in favor of the smaller number of the cluster, but not big enough to mask the performance of dib-c over the other algorithms. for real data, dib-c produced a huge null cluster having the majority of genes and a small number of clusters having a few members. however, note that in practice, a filtering step often precedes clustering methods based on the assumption that most genes are not expressed significantly under the conditions of microarray experiments. hence the final clustering results are affected by the choice of filtering criteria, making the optimal partitioning  problem more complex and potentially biased in other direction. in contrast, dib-c performs filtering and clustering simultaneously because the all-null-pattern , ) is just another symbolic pattern in our algorithm. by excluding this null cluster, simultaneous filtering and clustering can be performed, unlike in other clustering algorithms. tseng et al.  <cit>  also criticized that current clustering algorithms are forced to assign every gene to a cluster. many genes are irrelevant to biological pathways or conditions under screening and so the main interest of investigators lies in identifying the most informative, clusters of small sizes. with this in mind, we consider that the best clustering algorithm should produce a huge cluster of "irrelevant genes"  and a small number of clusters having a few members and this is exactly what dib-c does.

our algorithm is based on conceptual discretizations, such as increasing, decreasing, remaining flat, and convexity or concavity, that are used as basic building blocks to define a pattern or cluster that shares a pattern with itself. this makes each cluster meaningful and interpretable. although discretiztion may cause some loss of information, what investigators expect in gene expression data with time course may be such simple statements such as: which genes express more rapidly with an increase of time? which genes express early and late but remain flat in the middle? simply finding patterns and clusters may not be sufficient for the biologists, but it is our belief that we need more effort to relate, even at the cost of loosing some information, computational analysis results with biological phenomena.

for the exhaustive search, dib-c iterates |t| <dig> ×  times where |t| is the number of threshold values in the methods section and p is the number of time-points. in practice, the investigator would not need to use as many threshold values as in this study since there are multiple testing issues. common choice of t would be t = { <dig> × 10- <dig>   <dig> × 10- <dig>  ...,  <dig> × 10- <dig>  ...,  <dig> × 10- <dig>  ...,  <dig> × 10-4} which has a length  <dig>  the runtime of our algorithm increases exponentially with the number of time-points. however, most publicly available gene expression datasets have only a small number of time-points. according to the survey by ernst et. al.  <cit> , most datasets involved only 2~ <dig> time-points. if the number of time-points is very high, conventional time-series techniques can instead be used; dib-c is designed for situations where this is not the case.

dib-c can be generalized for use on other types of ordinal data, including stress-response or drug-treatment data, although the example datasets presented here focus on time-course data. in the future, we plan to extend dib-c to two-factor designs whose orders are in two directions. for example, the analysis of drug-induced gene expression has both time-order and treatment dose-order. we could apply dib-c after redefining the first-and second-order differences in order to take this account.

