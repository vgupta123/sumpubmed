BACKGROUND
gene expression data from tumors reflects many important clinical characteristics. for example, methodologies have been developed that can differentiate subtypes  <cit> , predict disease outcome  <cit> , and predict response to therapy  <cit> . most of these aspects will also have a genetic basis, which is often unknown, and is typically not unveiled by purely data-driven techniques. knowing the underlying molecular mechanisms is important if targeted therapies still need to be developed, and to determine whether a particular therapy is likely to be effective  <cit> .

based on the idea that tumors must be the result of an underlying mixture of oncogenic events  <cit> , several attempts have been undertaken to decompose the gene expression profiles of tumors into components representing these oncogenic events. the components identified in these decompositions might then provide further leads towards understanding tumorigenesis. for example, teschendorff et al.  <cit>  have used independent component analysis , and principal component analysis , to decompose gene expression data from breast cancer samples. these methods are purely data-driven, and thus have the disadvantage that they do not employ any prior knowledge. for this type of decomposition, the relation between the components is pre-defined, e.g. they are required to be orthogonal/independent. in order to apply these methods a collection of tumor expression profiles is required. the choice of the number of components is typically based on the cumulative amount of variance explained by a set of components, which is largely arbitrary.

on a similar note, brunet et al.  <cit>  have used non-negative matrix factorization  to decompose different leukemia subtypes. similar to ica/pca, this method is data-driven, which makes the interpretation afterwards complicated. the main difference is that it places constraints on the decomposition: both the components vectors and weights are required to be non-negative.

interpretation of the components/weights that are obtained using data-driven decomposition strategies is still very difficult. for example, the results can be compared to existing functional databases in order to attach an interpretation to the obtained components  <cit> .

we would like to use the knowledge about relevant components directly in the decomposition. more specifically, we would like to use this knowledge by pre-defining the components used in the decomposition, rather than performing a post-hoc analysis of a fully data-driven result. using this type of framework, i.e. employing components with a clear biological meaning, might result in a more meaningful decomposition and ease the interpretation afterwards.

bild et al.  <cit> , acharya et al.  <cit> , and anders et al.  <cit>  have used information about genetic perturbations to construct classifiers for perturbed vs wild type status. for every perturbation that was tested, they created a separate classifier, thus, they did not model possible interactions between these perturbations. here, we use the expression profiles of a set of perturbed cell lines, and assume a linear model for their interaction, i.e. we model combinations of perturbations as a linear combination of the expression profiles. thereby, we can include this type of knowledge directly into the decomposition. as opposed to post-hoc analyses of fully data-driven results , several approaches have been developed to include information about pathways  as prior information into the analysis. for instance, segal et al.  <cit>  derived activity scores for gene sets by employing the hypergeometric distribution. such an approach requires discretization of the expression data, which we preferably avoid since this might lead to a loss of information. similarly, chuang et al.  <cit>  derived activity scores for subnetworks of a protein-protein interaction network by summing the z-scores of the genes in such a subnetwork. a related approach is gene set enrichment analysis , which was developed by mootha et al.  <cit>  and subramanian et al.  <cit> . later on, this approach was adjusted to allow the computation of the activity of a gene set in a single sample . a common denominator among these approaches is that they treat each gene set separately, in the sense that the activity of a gene set in a particular sample is solved independently of the other gene sets. in contrast, we represent the expression of a given sample as the linear combination of the gene memberships of a predefined collection of gene sets.

we present a mathematical model  that allows us to include knowledge driven components in the decomposition. more specifically, we model the expression profile of a single tumor as a weighted linear combination of a set of components. for these components, we use two sources of knowledge. first, we use the expression data from cell lines in which cancer associated genes have been perturbed, and second, we use gene sets that are representative of the six hallmarks defined by hanahan et al.  <cit> . in order to keep the weights produced by the decomposition in an interpretable range, the process needs to be regularized. we do this by adding constraints on the weights, and introduce a lasso regularization parameter  <cit> .

we use the proposed model and the expression profiles of cell lines in which cancer associated genes were activated to decompose the gene expression profiles of genetically manipulated mice. since the mutation status of these mice is known, and the mutated genes correspond to the genes perturbed in the cell line experiments, a direct performance comparison can be made. next, we decompose the expression profiles of a set of breast tumors taken from six independent datasets, for which no mutation status is known, but where a set of clinical parameters, including disease and survival endpoints are known. moreover, when changing the regularization parameter, we show that consensus patterns emerge in the order in which the weights become non-negative. the results show that tumors can be stratified into several subgroups, each characterized by a unique perturbation profile, which are associated with distinct outcomes. this is a powerful approach since it allows the characterization of subtypes based on specific molecular aberrations, and allows a more directed search for targeted therapies.

methods
mathematical framework
in our decomposition, we assume that a linear combination of a set of pre-defined components  describes the gene expression observed for, for instance, a human tumor sample y. this implies that the gene expression of a sample can be written as a weighted summation over a set of components.

mathematically the model can be defined as:

  y = cw 

where y is a column vector representing the gene expression that needs to be decomposed, c represents a matrix of individual component vectors , and w is a column vector of weights. in the linear model the weights in w reflect the extent to which the sample y, resembles the expression of the components that are in c. we assume that the weights in w are the same for all genes .

for a given sample  and matrix of component vectors , we can obtain an estimate of w by minimizing the mean square error . the mse  is defined as:

  ϵmse=arg⁡min⁡w|||| <dig>  

without any constraints, a solution to equation  <dig> can be found using the moore-penrose generalized pseudoinverse , defined as:

  w = - <dig> ct y 

  w = c† y 

where t indicates a matrix transposition, - <dig> indicates a matrix inversion, and † indicates the pseudoinverse of a matrix. in the remainder of this paper, we refer to this variant as 'u' .

without any constraints the weights in w are unbounded. as a result, weights in w might not have any biological relevance. for instance, it is hard to interpret negative weights in w, implying that the expression profile of a given component has a negative contribution to the reconstructed sample. thus, a logical step is to include a variant, that ensures non-negativity of the weights in w, similar to an nmf approach  <cit> . this changes equation  <dig> to

  ϵmse=arg⁡min⁡w||||2subject tow≥ <dig>  

in the remainder of this paper, we refer to this variant as 'p' . apart from constraints on w we also consider a regularization term. for instance the l1-norm is an often applied form of regularization . this regularization shrinks weights such that they become exactly zero, allowing the conclusion that the associated component vectors in c do not contribute to the reconstruction at all. this results in the remaining components with non-zero weights being 'selected'. in the spirit of the l1-norm, we introduce a constraint based variant that restricts the l1-norm to  <dig>  that is, we include a variant for which the weights in w sum to  <dig>  this changes equation  <dig> to

  ϵmse=arg⁡min⁡w||||2subject to∑w=1w≥ <dig>  

we will refer to this variant as 's' .

these different variants lead to different regions of possible solutions in the gene expression space, as indicated in figure  <dig> 

in addition, we also considered the option where we include a lasso term into the variant with positive weights. this way, equation  <dig> changes to:

  ϵmse=arg⁡min⁡w||||2+λ∑|w|subject tow≥ <dig> 

we will refer to this method as 'l' . given the non-negativity constraint, the solution to equation  <dig> is fairly simple. we appended a row to the matrix c with all elements set to λ, and append the target vector y with a zero.

the setting of the λ parameter will influence the weights that are obtained in w. setting λ to infinite will result in an all zero w vector. progressively lowering λ will result in an ordering in which the individual weights become non-zero  <cit> . eventually, when λ is set to  <dig>  the solution will be equivalent to the 'p' solution, with up to all components having a non-zero weight.

we hypothesized that there is a relation between the order in which the weights become non-zero, and the biology of the sample. that is, the first weight that becomes non-zero will be the most important, and each additional weight that becomes non-zero is less and less important. we hypothesize that the order of importance might be different for different clinical subgroups of tumors. we visualized the order in which the weights become non-zero by means of an adjusted karnaugh map  <cit> , see figure  <dig> for a detailed example.

for each of the variants a constrained least squares optimization problem needs to be solved. to this end, we employed the mosek optimization toolbox for matlab  <cit> . this toolbox allows any number of equality and inequality constraints to be set, and employs an interior point algorithm.

datasets
hmec dataset
we used a previously published dataset  which contains gene expression measurements of  <dig> human mammary epithelial cell cultures  samples. these hmec samples have been perturbed by an adenovirus, resulting in five different perturbations in genes , namely in myc , ras , e2f <dig> , src , and bcatenin . these samples were analyzed on an affymetrix human genome u <dig> plus  <dig>  array, containing  <dig> probes.

mouse dataset
we used a previously published dataset  which contains  <dig> mouse samples. these mouse samples belong to five classes with different perturbations, namely in myc , ras , rbnull , her <dig> , and a wild type  class which serves as reference. these samples were measured on an affymetrix array, containing  <dig> probes.

mcf <dig> dataset
creighton et al.  <cit>  created a gene expression dataset from mcf <dig>  cell line samples. these cell lines were transfected with constitutively active raf, mek, erbb <dig>  and egfr . each transfection was measured in triplicate, resulting in  <dig> arrays. measurements were performed using the affymetrix human genome u133a array, containing  <dig> probes.

human dataset
we used a collection of six publicly available breast cancer datasets . these six datasets were all measured on the affymetrix human genome u133a array, containing  <dig> probes. in total this combined dataset contains  <dig> samples for which distant metastasis free survival data , er status, and hu et al.  <cit>  subtype information is available.

matching probes across the datasets
given our four datasets, we want to decompose the mouse samples using the hmec data as components, and, similarly, the human data using the mcf <dig> data as components. the samples from these four datasets originate from different organisms, and were measured on different platforms. in order to facilitate the decomposition, we have to match the probes from the mouse and hmec data. to do this, we used the chip comparer utility  <cit> , from bild et al  <cit> . this way, we mapped these two datasets to a common set of  <dig> genes. in case multiple probes mapped to one of the common genes, we selected the probe with the largest variance. since the data were measured on different platforms, it is required to normalize each dataset separately. we applied mean-variance normalization per gene per dataset.

both the human data and the mcf <dig> data was measured using the affymetrix human genome u133a array, eliminating the need to apply any probe matching. to normalize them, both datasets were median centered per gene prior to the analysis.

gene sets
a collection of gene sets were gathered from the respective repository websites of go  <cit> , kegg  <cit> , and reactome  <cit> . in total we gathered  <dig> gene sets . based on their description, we assigned gene sets to the hanahan hallmarks. for four hallmarks  we found associated gene sets. in our analysis, we used  <dig> gene sets that were associated with apoptosis,  <dig> for growth,  <dig> for angiogenesis, and  <dig> for dna replication. in additional file  <dig>  we provide a list of the gene sets and their hanahan hallmark.

RESULTS
decomposing mouse data into hmec components
first, we decomposed the  <dig> mouse tumors into the available five classes of hmec samples. to do this, we construct a c matrix, where each column consists of the class-means of the five perturbation classes represented in the hmec samples . it is unlikely that a perturbation will have an effect on all genes, causing many genes to be irrelevant with respect to a specific perturbation, consequently only contributing noise to the modeling problem. therefore, we also applied a feature selection step on the hmec data. we were most interested in genes that distinguish one of the hmec classes from the other four. therefore we ranked, for each of the classes, the genes based on their ability to discriminate between that class and the remaining classes. we employed the absolute signal to noise ratio  as ranking criterion. next, we selected the top n genes for each of the five ranked lists, and then took the union of these five lists. of course, alternative feature selection procedures can be employed, but they are beyond the scope of the current analysis. using the set of genes selected in the feature selection step, and each of the mouse samples as target in y, we applied each of the four decomposition variants, u, p, s, and l . after applying the models, we assign each mouse sample to the hmec class which has the highest absolute wi. since we know the mutation status of the mouse samples, we can compare the class assignment from the different variants to the known mutation status. to evaluate the predictive accuracy we only employed the three classes in the mouse dataset for which an equivalent class is present in the hmec data. therefore, we used the classes myc, ras and e2f <dig> , from the mouse data.

the w vectors found using the u variant have both positive and negative weights. several mouse samples are incorrectly classified, most notably the ras samples, which are all incorrect . next, in the p variant, where all weights are constrained to be positive, all w vectors are almost identical for all mouse samples. this is a clear disadvantage, since no clear distinction between the mouse samples is made. for this variant, classification of the e2f <dig> samples turns out to be most difficult. constraining all weights to sum to one , results in a distinctly different set of w vectors, but in terms of performance it is equal to variant p.

the lasso-based variant  provides the most desirable output, positive weights for the correct classes, and zeros everywhere else. of course the result depends on the setting of λ, which was chosen such that a single non-zero weight is left for each of the mouse samples. this single remaining non-zero weight is what we hypothesize to be the most important weight.

of course, the set of genes used in the decomposition influences the results. to investigate this, we inspected the performance of the four decomposition variants relative to the number of genes n that is selected in each one-versus-the-rest rankings. we define performance as the fraction of mouse correctly classified mouse samples, i.e. assigned to the hmec class with the correct corresponding perturbation. we varied the number n from  <dig> to  <dig> genes. figure  <dig> shows the resulting performance curves for each method. it is clear that the lasso-based method outperforms the other methods over the entire range of n, and reaches the best performance around  <dig> to  <dig> genes.

decomposing human data into mcf <dig> components
for the collection of  <dig> human breast cancer samples, we applied a decomposition into four mcf <dig> classes. thus, we used the human samples as y vectors, and created a c matrix where the mean vectors of the four mcf <dig> formed the columns. we used the l variant to decompose the samples, since that showed the best performing decomposition on the mouse-hmec data. we applied a feature selection step similar to that employed for the mouse-hmec data to select the genes that are most discriminating between the four mcf <dig> classes. we employed the top  <dig> genes and set λ to 15% of the total number of genes, since these settings resulted in the best performance in the mouse-hmec decomposition.

unfortunately, for the human breast cancer data there is no information with regard to presence/absence of mutations. nevertheless, a multitude of other clinical parameters is available for most of these samples. for all samples the estrogen receptor  status, distant metastasis free survival time, and hu et al.  <cit>  subtype information is available. any link between these clinical parameters and the mcf <dig> components is interesting.

based on the grouping obtained in figure  <dig>  some relations with the clinical parameters are already visible. we employed the chi-squared test to formally test the associations between each clinical parameter, and mcf <dig> class, see figure  <dig>  this allows us to test whether an association exists between the non-zero/zero weights and a given clinical parameter such as er status. figure  <dig> shows the most significant associations that were detected.

as shown in figure  <dig>  the majority of er negative samples have a zero erbb <dig> weight. at the same time, the er positive samples are equally distributed between the erbb <dig> present  and erbb <dig> absent  groups. this association is highly significant . similarly, the majority of the samples from the basal group, have a zero erbb <dig> weight . this confirms a previous observation that the basal samples are predominantly triple negative, i.e. erbb <dig>  er, and pr negative, kreike et al.  <cit> . in addition, we made a kaplan-meier plot for the two groups obtained by splitting on erbb <dig> weight status in . the difference in survival is clearly significant .

the subtypes provided by hu et al.  <cit>  include a her <dig> group. this group is of particular interest since this is equivalent to the erbb <dig> class in the mcf <dig> dataset. it turns out that there is little to no correlation between these two assignments, only  <dig> out of the  <dig> her <dig> samples have a non-negative erbb <dig> weight . strikingly, the majority of samples with a non-negative erbb <dig> weight are the luminal a and normal-like samples. another method has been published that allows the determination of the her <dig> status solely based on  <dig> probe that shows a bi-modal expression distribution . we also determined the her <dig> status using this method . it turns out that there is limited to no correlation among the her <dig> assignments, as derived by hu et al.  <cit> , gong et al.  <cit> , and our method. a potential explanation for this might be that the hu et al. and gong et al. her <dig> subtype is defined largely by the her <dig> expression itself, and much less by its downstream effects. only a known ground truth can give an indication which of the three assignments best reflects the actual perturbation status of her <dig>  however, such data is currently not yet available.

order of lasso shrinkage
next, we inspected the effect of the regularization parameter on the order in which the weights in w become non-negative. thus, we obtain tables with trajectories for each of the  <dig> samples, similar to the example shown in figure  <dig>  in order to create an aggregate plot of all trajectories across  <dig> samples, we created a slightly adapted representation, see figure  <dig>  more specifically, the linewidth of the red/blue lines in figure  <dig> is proportional to the fraction of tables  that have that link, relative to the total number of samples. for example, let's assume there are  <dig> out of  <dig> samples that traverse  <dig> to  <dig> based on a positive weight for raf , then that line will be plotted with  <dig>  times the maximum linewidth.

decomposing human data into gene sets
as an alternative source for knowledge driven components, we used gene sets. more specifically, we used gene sets that were linked to the hallmarks of cancer described by hanahan et al.  <cit>  . this was done by creating a c matrix with these  <dig> gene sets as components. thus, the c matrix has  <dig> columns, one for each gene set. for each gene set  the entries in c are set to  <dig> for gene that is part of that gene set, and set to zero otherwise. we used this c matrix with gene sets to decompose the  <dig> breast cancer samples . once again we applied feature selection, by performing the decomposition using only those genes that are assigned to at least one of the gene sets used.

on the other hand, in the normal-like group, there is no clear consensus, and samples first get a non-zero weight for either one of apoptosis, growth, angiogenesis. only very few samples obtain a non-zero weight for replication. consequently, almost none of the normal-like samples get to the stage with four non-zero weights. this signifies a discriminating characteristic of the normal-like samples with respect to the other four categories. this is slightly contradictory to the original hanahan et al.  <cit>  hypothesis, which states that a tumor must have obtained all six of the hallmarks. however, the normal-like group of breast cancer samples, is also the one with the best survival characteristics  <cit> . perhaps this better survival is, in part, explained by the fact that the replication hallmark is not as active as in some of the other breast cancer subtypes.

CONCLUSIONS
we described a linear model which links a set of knowledge-derived expression vectors to the expression profile of samples, potentially with unknown mutation status. when benchmarked on data from hmecs and mouse, the lasso-based method outperforms the best. moreover, the lasso-based method is relatively insensitive to the setting of the regularization parameter λ, and performs well for the entire range of genes  that is selected. thus, the proposed lasso-based constrained least squares decomposition provides a parameter-insensitive and accurate assignment of mutation status to samples.

on the collection of  <dig> human breast cancer samples, we found several associations between the molecular component class the samples were assigned to and the clinical parameters. this includes both new associations , and known associations . thus, the proposed decomposition framework has a clear capability to unveil relevant relations between the molecular components and the human samples, for which no mutation status is known. using gene sets as components has unveiled different consensus trajectories of appearance for the components representing the hu et al. subtypes when changing the regularization parameter λ. we hypothesize that these trajectories provide insight into the key events that gave rise to the tumor and might shed light on the future behavior of the tumor, including how it will react to therapy.

a main advantage of our method is that it allows the incorporation of knowledge derived components, which is not possible for most data-driven methods. moreover, it is possible to do the decomposition for even just one sample . this is not possible for, for example, pca, where a group of y vectors is required. a limitation of our method is that it requires a set of components derived from knowledge. of course, for interpretation of the data-driven components, this knowledge has to be available as well. thus, the knowledge based decomposition presented here is a viable alternative.

competing interests
the authors declare that they have no competing interests.

authors' contributions
mhv, lfaw, and mjtr contributed to the conceptual design of the study. mhv performed the analysis. mhv, lfaw, and mjtr wrote the manuscript.

supplementary material
additional file 1
names of the gene sets used. in this table, we indicate which gene sets were used for which of the hanahan hallmarks.

click here for file

 acknowledgements
this work was part of the biorange programme of the netherlands bioinformatics centre , which is supported by a bsik grant through the netherlands genomics initiative .

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2009: proceedings of the seventh asia pacific bioinformatics conference   <dig>  the full contents of the supplement are available online at
