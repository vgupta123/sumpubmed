BACKGROUND
bioinformatics data analysis is often based on the use of a linear mixture model  of a sample  <cit> , whereas mixture is composed of components generated by unknown number of interfering sources. as an example, components can be generated during disease progression that causes cancerous cells to produce proteins and/or other molecules that can serve as early indicators  representing disease correlated chemical entities. their correct identification may be very beneficial for an early detection and diagnosis of disease  <cit> . however, an identification of individual components within a sample is complicated by the fact that they can be "buried" within multiple substances. in addition to that, dynamic range of their concentrations can vary even several orders of magnitude  <cit> , i.e., single components could no longer be recognizable  <cit> . nevertheless, there are the algorithms able to extract either individual components or a group of components with similar concentrations within a sample. these algorithms are known under the name blind source separation   <cit> , and they commonly include independent component analysis   <cit> , and nonnegative matrix factorization   <cit> . however, bss methods perform unsupervised decomposition of the mixture samples. thus, it is not clear which of the extracted components are to be retained for further prediction/classification analysis. to this end, several contributions toward solution of this problem have been published in  <cit> . in  <cit> , a matrix factorization approach to the decomposition of infrared spectra of a sample is proposed taking into account class labels i.e., the classification phase and the components inference tasks are unified. thus, the concept proposed in  <cit>  is a classifier specific. it is formulated as the multiclass assignment problem where the number of components equals the number of classes and must be less than the number of samples available. as opposed to  <cit> , the method proposed here selects automatically the case and control specific components on a sample-by-sample basis. afterwards, these components can be used to train arbitrary classifier. in  <cit>  gene expression profile is modelled as a linear superposition of three components comprised of up-regulated, down-regulated and differentially not expressed genes, whereas existence of two fixed thresholds is assumed to enable a decision to which of the three components the particular gene belongs. the thresholds are defined heuristically and in each specific case the optimal value must be obtained by cross-validation. moreover, the upper threshold cu and the lower one cl are mutually related through cu = 1/cl. as opposed to that, the method proposed here decomposes each sample  into components comprised of up-regulated, down-regulated and not differentially expressed features using data adaptive thresholds. they are based on mixing angles of an innovative linear mixture model of a sample. the method proposed in  <cit>  uses available sample labels  to select component, extracted by independent component analysis  or nonnegative matrix factorization , for further analysis. ica or nmf are used to factorize the whole dataset simultaneously and one selected component  is used for further analysis related to gene marker extraction. this component cannot be used for classification. alternatively, basis matrix with labelled column vectors  or row vectors  can be used for classification in which case the test sample needs to be projected to space spanned by the column/row vectors, respectively. however, in this case no feature extraction can be performed. as opposed to ica/nmf method proposed in  <cit> , the method proposed here extracts disease and control specific component from each sample separately. since no label information is used in the selection process, extracted components can be used for classification and that is the goal in this paper. the disease specific component can, however, be also retained for further biomarker related analysis as in  <cit> . the important difference is that by the method proposed here such component can be obtained from each sample separately while the method in  <cit> , as well as in  <cit> , needs the whole dataset. the method  <cit>  uses again ica  to factorize the microarray dataset. extracted components  were analyzed to discriminate between those with biological significance and those representing noise. however, biologically significant components can be used for further gene marker related analysis but not for classification. the reason is that, as in  <cit> , the whole dataset composed of case and control samples is reduced to several biologically interesting components only. in the extreme case it can only be one such component. in  <cit>  the jade ica algorithm is used to decompose whole dataset into components . as in  <cit>  these components cannot be used for classification. they are used for further decomposition into sub-modes to identify a regulating network in the problem considered there. we want to emphasize that the component selected as disease specific by the method proposed here can also be interpreted as a sub-mode and used for the similar type of analysis. however, since it is extracted from an individual and labelled sample it can be used for the classification as well. that is the main goal in this paper. the method in  <cit>  again uses ica  to extract components . similarly, as in  <cit>  these components are not used for a classification. instead, they are further analyzed by data clustering to determine biological relevance and extract gene markers. similar types of comments as those discussed in relation to  <cit>  can also be raised to other methods that use either ica or nmf to extract components from the whole dataset,  <cit> . hence, although related to the component selection methods  <cit>  the method proposed here is dissimilar to all of them by the fact that it extracts most interesting components on a sample -by-sample basis. to achieve this, the linear mixture model  used for components extraction is composed of a test sample and a reference sample representing control and/or case group. hence, a test sample is, in principle, associated with two lmms. each lmm describes a sample as an additive mixture of two or more components. two of them are selected automatically  as case  and control specific, while the rest are considered neutral i.e. not differentially expressed. decomposition of each lmm is enabled by enforcing sparseness constraint on the components to be extracted. this implies that each feature  belongs to the two components at most . the model formally presumes that disease specific features are present in the prevailing concentration in disease samples as well as that control specific features are present in prevailing concentration in control samples. however, the features do not have to be expressed equally strong across the whole dataset in order to be selected as a part of disease or case specific components. it is this way due to the fact that decomposition is performed locally . this should prevent losing some important features for classification. accordingly, the level of expression of indifferent features can also vary between the samples. thus, postulating one or more components with indifferent features enables their removal that is sample adaptive. as opposed to that, existing methods try to optimize a single threshold for a whole dataset. geometric interpretation of the lmm based on a reference sample enables automatic selection of disease and control specific components , without using label information. hence, the selected components can be further used for disease prediction. by postulating existence of one or more components with differentially not expressed features the complexity of the selected components can be controlled , whereas the overall number of components is selected by cross-validation. although the feature selection is the main goal of the proposed method, component extracted from the sample as disease specific can also be interpreted as a sub-mode as in  <cit> . it can be used for further biomarker identification related analysis. we see the linearity of the model used to describe a sample as a potential limitation of a proposed method. although linear models dominate in bioinformatics, it has been discussed in  <cit>  that nonlinear models might be more accurate description of biological processes. assumption of an availability of a reference sample might also be seen as a potential weakness. yet, we have demonstrated that in the absence of expert information the reference sample can be obtained by a simple average of all the samples within the same class. the proposed method is demonstrated in sections  <dig>  to  <dig>  on disease prediction problems using a computational model as well as on the experimental datasets related to a prediction of ovarian, prostate and colon cancers from protein and gene expression profiles.

methods
this section derives sparse component analysis  approach to unsupervised decomposition of protein  and gene expression profiles using a novel mixture model of a sample. the model enables automatic selection of the two of the extracted components as case and control specific. they are retained for classification. in what follows, the problem motivation and definition are presented first. then, lmm of a sample is introduced and its interpretation is described. afterwards, a two-stage implementation of the sca algorithm is described and discussed in details.

 <dig>  problem formulation
as mentioned previously, bioinformatics problems often deal with data containing components that are imprinted in a sample by several interfering sources. as an example, brief description of endocrine signalling system, secreting hormones into a blood stream, is given in  <cit> . likewise, reference  <cit>  describes how different organs imprint their substances  into a urine sample. as pointed out in  <cit>  and  <cit>  disease samples are combinations of several co-regulated components  originating from different sources  and disease specific component is actually "buried" within a sample. hence we are dealing with the two problems simultaneously: a sample decomposition  problem and a classification  problem that is based on sample decomposition. thus, automatic selection of one or more extracted components is of practical importance. it is also important that component selection is done without a use of label information in which case it can be used for classification.

matrix factorization is conveniently used in signal processing to solve decomposition problems  <cit> . it is assumed that data matrix x ∈ ℝn × k is comprised of n row vectors representing mixture samples, whereas each sample is further comprised of k features . it is also assumed that n samples are labelled: xn∈ℝk,yn∈{ <dig> -1}n=1n, where  <dig> denotes positive  sample and - <dig> stands for a negative  sample. data matrix x is modelled as a product of two factor matrices:

  x = as 

where a ∈ ℝn × m and s ∈ ℝm × k , and m represents an unknown number of components present in a sample. each component sm∈ℝkm=1m is represented by a row vector of matrix s. nonnegative relative concentration profiles am∈ℝ+nm=1m are represented by column vectors of matrix a and are associated with the particular components. here, it will be presented how innovative version of the lmm  of a sample xn∈ℝkm=1m enables automatic selection of the case  and control specific components out of smm=1m components extracted by unsupervised factorization method: a two stage sca. the method will then be demonstrated on a computational model as well as on a cancer prediction problem using well known proteomic and genomic datasets.

 <dig>  novel additive linear mixture model of a sample
the lmm  is widely used in various bioinformatics problems  <cit> . unless constraints are imposed on a and/or s, the matrix factorization implied by  is not unique. typical constraints involve non-gaussianity and statistical independence between components by ica algorithms  <cit> , and non-negativity and sparseness constraints by nmf algorithms,  <cit> . in addition to that, many ica algorithms, as well as many nmf algorithms, also require the unknown number of components m to be less than or equal to the number of mixture samples n.

depending on the context, this constraint can be considered as restrictive. there are, however, ica methods developed for the solution of underdetermined problems that are known as overcomplete ica, see chapter  <dig> in  <cit> , as well as  <cit> . however, as discussed in details in  <cit> , overcomplete ica methods also assume that unknown components are sparse. the two exemplary overcomplete ica methods based on sparseness assumption are described in  <cit>  and  <cit> . in  <cit>  it is assumed that components are sparse and approximately uncorrelated . this basically means that each feature belongs to one component only. that is even a fairly stronger assumption than what is used by the method proposed here. likewise, in maximum likelihood  approach to the overcomplete problem in  <cit>  it is assumed that marginal distributions of the components are laplacian. in this case the component estimation problem  is reduced to linear program with equality constraint. in other words, a probabilistic ml problem is converted into a deterministic linear programming task. hence, the overcomplete ica effectively becomes sca. this further justifies our choice of the state-of-the-art sca method , to be used in a component extraction task. here, we propose a novel type of the lmm model which is composed of two samples only:

  xcontrolx=acontrolscontrol 

  xdiseasex=adiseasesdisease 

the first sample is a reference sample representing control group, xcontrol ∈ ℝk, in  and case  group, xdisease ∈ ℝk, in . the second sample is actual test sample: x∈xn∈ℝkn=1n. coefficients of matrices acontrol∈ℝ+2×m and adisease∈ℝ+2×m in  and  refer to the amount of relative concentration at which related components are present in the mixture samples x and xcontrol in  or x and xdisease in . source matrices scontrol ∈ ℝm × k and sdisease ∈ ℝm × k contain , disease- and control specific components and, possibly, differentially not expressed components. number of components m is assumed to be greater than or equal to  <dig>  evidently, for m =  <dig> existence of differentially not expressed components is not postulated. importance of postulating components with indifferent features is to obtain less complex disease and control specific components used for classification . these components absorb features that do not vary substantially across the sample population. these features are removed automatically from each sample. the concentration is relative due to the fact that bss methods enable estimation of the mixing and source matrices up to the scaling constant only. therefore, it is customary to constrain the column vectors of the mixing matrix to unit ℓ <dig> or ℓ <dig> norm. the lmm proposed here is built upon an implicit assumption that disease specific features  are present in prevailing concentration in disease specific samples and in minor concentration in control specific samples. as opposed to that, control specific features are present in prevailing concentration in control specific samples and in minor concentration in disease specific samples. features that are not differentially expressed are present in similar concentrations in both control and disease specific samples. these groups of features constitute components, whereas similarity of their concentration profiles enables automatic selection of the components extracted by unsupervised factorization. the assumption on the prevailing concentrations of up- and down-regulated features needs to be understood in the relative sense. it is justified on the basis of locality of proposed method since the components are extracted on a sample-by-sample basis. thus, to be allocated in the same component  feature does not need to be expressed in each sample equally strong. since the lmms / considered here are comprised of two samples only the non-negative mixing vectors are confined in the first quadrant of the plane spanned by control reference sample and test sample, see figure 1a, or by disease reference sample and test sample, see figure 1b. thus, upon decomposition of the lmm  into m components, the one associated with the mixing vector that confines the maximal angle with respect to the axis defined by control reference sample is selected as a disease specific component, figure 1a. as opposed to that, the one associated with the mixing vector that confines the minimal angle with respect to the axis defined by control reference sample is selected as a control specific component. when decomposition is performed with respect to a disease reference sample, lmm , the logic for an angle-based automatic selection of disease and control specific components is the opposite, see figure 1b. the components not selected as disease or control specific are considered neutral i.e. not differentially expressed. thus, lmms / enable automatic selection of the components extracted by unsupervised factorization of mixture samples. unlike selection method presented in  <cit>  that is based on fixed thresholds which need to be determined by cross-validation, the thresholds  used in the method presented here are sample adaptive. an assumption that each feature is contained in disease specific and one of the neutral components, or control specific and one of the neutral components, represents a sparseness constraint. it enables solution of the related bss problems through, in principle, two-stage sca method described in section  <dig> . however, sparseness constraint is not justified by mathematical reasons only but also, as emphasized in  <cit> , by the biological reasons. as noted in  <cit>  this is necessary if underlying component  is going to be indicative of ongoing biological processes in a sample . the same conjecture has actually also been used in a three components based gene discovery method in  <cit> . in this respect, the sparseness constrained nmf methods for microarray data analysis proposed in  <cit>  also assume the same working hypothesis. as discussed in  <cit> , it is the sparseness constraint that enabled biological relevance of obtained results. in microarray data analysis enforcement of sparseness constraint is biologically justified due to the fact that more sparse s gives rise to metagenes , or to the expression modes , that comprise few dominantly co-expressed genes which may indicate good local features for specific disease  <cit> . a subtle interpretation of the reference-based mixture model / reveals its several profound characteristics. since placement of the features to each of the two or more postulated components is based on sample adaptive thresholds , one gene  may be highly up-regulated in a case of one sample and significantly less expressed in a case of an another sample. yet, if it is contained in prevailing concentration in both samples it will be contained in both cases in the component automatically selected as disease or control specific. moreover, sample adaptive component  selection enables that features selected as up- -regulated in one sample be less  expressed than differentially not expressed features in another sample. thus, extracted components selected as disease or control specific are composed of multiple features with different expression levels and joint discriminative power rather than of several  features only.

for disease prediction, disease and control specific components can be used to train a classifier. the reason is that in each lmm / they are extracted with respect to different reference samples and, thus, carry on different but specific information. hence, proposed method yields four components to be retained for classifier training. in accordance with figure  <dig> they are denoted as scontrol ref.;ndisease, scontrol ref.;ncontrol, sdisease ref.;ncontrol, and sdisease ref.;ndisease, where n denotes index of a test sample xn used in current decomposition. components extracted from n mixture samples, form four sets of labelled feature vectors as follows: scontrol ref.;ndisease,ynn=1n, scontrol ref.;ncontrol,ynn=1n, sdisease ref.;ncontrol,ynn=1n and sdisease ref.;ndisease,ynn=1n. one or more classifiers can be trained on them and the one with the highest accuracy achieved through cross-validation is selected for a disease diagnosis.

selection of the unknown number of components m is generally non-trivial problem in a matrix factorization and is the part of a model validation procedure. m is selected through cross-validation and postulated to be  <dig>   <dig>   <dig> or  <dig> because it directly determines the number of features used for classification. this follows from previously described interpretation of the lmm  and . since disease prediction is based on four components selected as disease and control specific it is important that they are composed of features with the high discriminative power. it means that they should contain features which are truly disease or control specific. the component considered here as disease or control specific  can actually be composed of features  belonging to multiple substances  that share similar relative concentrations. this is practically important since it makes decomposition much less sensitive to an underestimation of the true total number of substances present in a sample. by setting the number of substances to predefined value m, proposed method is enforcing substances with similar concentrations to be linearly combined into one more complex components composed of disease, neutral or control specific features. provided that concentration variability of these features across the samples is small, it would suffice to select overall number of components as m =  <dig> or even m =  <dig>  . however, since we are dealing with biological samples it is more realistic to expect that relative concentrations could vary across the sample population. this is illustrated in figures 1a and 1b by ellipsoids around vectors that represent average concentration profiles of each group of features . as seen from figure  <dig>  some features considered neutral can be present in the prevailing concentration in a certain number of samples than the features considered in a majority of the samples as disease  specific. to partially remove such features from disease and/or control specific components, an unknown number of components m should be increased to m =  <dig> or perhaps even to m =  <dig>  thus, existence of two or three neutral components should be postulated. this is expected to yield less complex disease and control specific components and that is in agreement with the principle of parsimony . model validation presented in section  <dig>  suggests that this, indeed, is the case when concentration variability across the samples is significant. when it comes to the real world datasets, the information about number of components will not be known in advance. the strategy to comply with this uncertainty is to use the cross-validation and to verify whether increased number of components m indeed contributed to increased accuracy in disease prediction.

 <dig>  sparse component analysis algorithm
proposed feature extraction/component selection method is based on a decomposition of lmms / comprised of two samples  into m ≥  <dig> components. from the bss point of view this yields determined bss problem when m =  <dig> and underdetermined bss problem, when m ≥  <dig> . the enabling constraint for solving underdetermined bss problems is a sparseness of the components and the methods are known under the common name as sparse component analysis  . as commented at the beginning of section  <dig>  the overcomplete ica, , is basically reduced to sca and also demands sparse sources. sca has already been applied to microarray data analysis in  <cit> . it has also been used in  <cit>  to extract more than two components from the two mixture samples of nuclear magnetic resonance and mass spectra. a sparseness constraint implies that each particular feature point k =  <dig>  ...,k  belongs to the several components only. to this end, for the two-samples based lmms / used here, it is assumed that each feature point belongs to at most two components: either disease specific and neutral or control specific and neutral. from the viewpoint of biology, a plausibility of this assumption has been elaborated before.

algorithmic approaches used to solve underdetermined bss problem associated with / belong to the two main categories:  estimating concentration/mixing matrix and component matrix simultaneously by minimizing data fidelity terms x-acontrolscontrolf <dig> or x-adiseasesdiseasef <dig>  where x follows from the left side of  or . a minimization is usually done through the alternating least square  methodology with sparseness constraint imposed on source matrices scontrol and sdisease,  <cit> ;  estimating concentration/mixing matrices first by clustering and source/component matrices afterwards by solving underdetermined system of linear equations through minimization of the ℓp norm,  <dig> <p ≤  <dig>  of the column vectors sk ∈ ℝm of scontrol and sdisease,  <cit> . as discussed in  <cit> , a sparseness constrained minimization of the data fidelity term is sensitive to the choice of a sparseness constraint. on the other side, it has been recognized in  <cit>  that accurate estimation of the concentration matrix enables accurate solution of even determined bss problems. to this end, selection of feature points where only single component is present is of a special importance. at these points, feature vector and appropriate mixing vector are collinear. for example, if feature k belongs to component m then: xk ≈ am smk. thus, clustering of a set of single component points  ought to yield an accurate estimate of the mixing matrix. its columns are represented by cluster centroids. it has been demonstrated in  <cit>  that such estimation of the mixing matrix, where hierarchical clustering was used, yields more accurate solution of determined bss problem: s = pinvx, than the one obtained by ica algorithms. thus, selection of scps is of an essential importance for accurate estimation of the mixing matrix. such feature points are identified from the overall number of k points using geometric criterion based on the notion that at them real and imaginary parts of the mixture samples point either in the same or in the opposite direction  <cit> . since protein  and gene expression levels are real sequences an analytic continuation  <cit>  of mixture samples:

xn↦x˜n=xn+-1h is used to obtain complex representation, where h denotes hilbert transform of xn. the feature point k will be selected to the set of j scps provided that the following criterion is satisfied:

 rtiri≥ cosk∈{ <dig> ...,k} 

where r and i denote real and imaginary part of x˜k respectively, 't' denotes transpose operation, r and i denote ℓ2-norms of r and i while Δθ stands for the angular displacement from direction of either  <dig> or π radians. evidently, Δθ determines quality of the selected scps and, thus, accuracy of the estimation of the mixing matrices acontrol and adisease. setting Δθ to a small value  enforces, with an overwhelming probability, the selection of feature points that contain one component only. if, however, all the components are not present in at least one feature point alone it may occur that corresponding columns of the mixing matrices will be estimated inaccurately. this problem can be alleviated by increasing the value of Δθ in which case the selected feature points may not contain one component only, but may rather be composed of one dominant component and one or more components present in a small amount.

thus, in practice, Δθ needs to be selected through a cross-validation. in the experiments described in sections  <dig>  to  <dig> , Δθ has been selected from the set of radians equivalent to { <dig>   <dig>  50} together with a postulated number of components m and with a regularization parameter related to sparseness constraint imposed on scontrol and sdisease  below). hierarchical clustering implemented by matlab clusterdata command  has been used to cluster the set of selected j feature points with a single component belonging. number of clusters has been set in advance to equal the postulated number of components m. cluster centres represent estimated concentrations vectors am∈ℝ+2m=1m. it is also possible to use other clustering methods, such as k-means, as an alternative to hierarchical clustering. the problem with k-means, however, is that it is non-convex and its performance strongly depends on the initial value selected for cluster centroids. on the other side, hierarchical clustering produces repeatable result i.e. for a given set of scps it yields the same result for the mixing matrix in each run. since the number of selected scps is modest, the computational complexity of hierarchical clustering approach is not too high. that is why hierarchical clustering is used to estimate the mixing matrices in  and . after mixing matrices are estimated, estimation of the component matrices proceeds by minimizing sparseness constrained cost functions:

  s^control=mins12a^controls-xcontrolxf2+λs <dig> 

  s^disease=mins12a^diseases-xdiseasexf2+λs <dig> 

where the hat sign denotes estimates of the model variables acontrol/adisease and scontrol/sdisease. problems  relate to the sparseness constrained solution of the underdetermined systems of linear equations. for a decomposition of gene expression profiles, a non-negativity constraint is additionally imposed on s: s ≥  <dig>  problem  can be solved by the lasso algorithm  <cit>  or, by some other solver for underdetermined system of linear equations  <cit> . here, for problem  we have used the iterative shrinkage thresholding  type of method  <cit> , with a matlab code available at  <cit> . this approach has been shown to be fast and it can be easily implemented in batch mode such as / i.e. as a solving of all k systems of equations simultaneously. in relation to standard ist methods, the method  <cit>  has guaranteed better global rate of convergence. in addition to that, through the effect of iterations, it shrinks to zero small nonzero elements of s that are influenced by noise. this prevents them to determine level of sparseness of s. as discussed in  <cit>  this shrinking operation is important in preventing selection of less sparse s over the sparse version of s. with non-negativity constraint s ≥  <dig> problem  becomes a quadratic program. thus, we have used a gradient descent with projection onto non-negative orthant: max. a sparsity of the solution is controlled by the parameter λ. there is a maximal value of λ  above which the solution of the problems  is maximally sparse, i.e. it is equal to zero. thus, in the experiments reported in sections  <dig>  to  <dig>  the value λ has been selected by cross-validation  with respect to λmax as: λ∈{10-2·λmax, 10-4·λmax, 10-6·λmax}. we conclude this section by an observation that the situation suggested in  <cit> : x = as = apseuspseu, where  represents alternative factorization of x such that spseu would be less sparse than s, during minimization of  cannot occur. that is due to ist algorithm  <cit>  as well as due to accurate estimation of the mixing matrices that is enabled by clustering set of the scps. first, this is a consequence of the fact that a shrinking operation used by ist algorithm  <cit>  imposes sparseness constraint of the type given by eq. in  <cit> :

 0≤στ=number of elements of sk≤τ⋅skmaxnumber of elements of sk≤ <dig> τ∈ <cit> , 

i.e. small nonzero elements of sk are set to zero. this prevents selection of less sparse spseu over sparser s. second, sca method used here is a two-stage method where a is estimated accurately by clustering on a set of scps. this, in addition to a sparseness measure discussed above, prevents estimate of s to deviate from the true value significantly. it is this way because when s is being estimated by means of ist algorithm the very estimate of a is fixed. as opposed to the case when a and s are estimated simultaneously, as in  <cit> , an estimate of a can't now be adjusted by the algorithm to some value apseu that will counteract changes in s. hence, selecting spseu would increase a data fidelity term in the cost function. thus, situation as suggested in  <cit> : x = as = apseuspseu can't occur. a proposed two-stage sca approach to feature extraction/component selection is in a concise form presented in table  <dig>  a matlab code is posted in the additional material files section accompanied with the paper as additional file  <dig> 

   <dig>  for lmms / select a set of single component points for a given
Δθ.
RESULTS
this section presents model validation procedure. it is demonstrated how increased number of postulated components retains, or slightly improves, prediction accuracy when concentration variability of the features across the sample population is significant. moreover, an increased number of postulated components yields the disease and control specific components used for classification with a smaller number of features. this is in an agreement with the principle of parsimony which states that less complex solution ought to be preferred over the more complex one. proposed method for feature extraction/component selection is also applied to a prediction of ovarian, prostate and colon cancers from the three well-studied datasets. prediction accuracy  is estimated by  <dig> independent two-fold cross-validations. proposed sca component selection method is compared  against state-of-the-art predictors tested on the same datasets including our implementation of methods proposed in  <cit> . regarding our implementation of a predictive matrix factorization method  <cit> , we have used the matlab fminsearch function to minimize the negative value of the target function suggested in  <cit>  while selecting the threshold vector. we have set the tolfun to 10- <dig>  the tolx to 10- <dig> and the maxfunevals to  <dig> . an initial value of the two-dimensional threshold vector has been set to  <cit> t. regarding a gene discovery method proposed in  <cit>  we have cross-validated three values of the threshold cu ∈{ <dig>   <dig> ,  <dig> } . the best result is presented in section  <dig> . regarding a comparison of a proposed component selection method against many methods in sections  <dig>  to  <dig> , our intention has been to provide a brief description of the methods and to provide fair comparison given the fact that code for compared methods has not been available to us. that actually was the main reason for choosing a well known datasets such as in  <dig>  to  <dig> , since a rich list of published results exists for them. we are aware of the fact that results by many other methods were obtained by different cross-validation settings. therefore, our reasoning is that fair comparison is possible as long as the results to be compared were obtained on the same datasets under conditions that favor less the method proposed here. that is the reason why we have chosen to perform two-fold cross-validation, since it is known to yield the least optimistic result. thus, if such results are compared favorably against those obtained under milder  cross-validation settings, conclusion can be made that proposed feature extraction/component selection method represents contribution to the field. as opposed to the two-fold cross-validation applied here, cross-validation details for many cited results were not specified. sometimes ten-fold, or three-fold, cross-validations have been performed. hence, it is believed that performance assessment of proposed component selection method is more realistic than performance of the majority of methods cited in comparative analysis. for each of the three types of cancers three classifiers were trained on four sets of extracted components: scontrol ref.;ndisease,ynn=1n, scontrol ref.;ncontrol,ynn=1n, sdisease ref.;ncontrol,ynn=1n and sdisease ref.;ndisease,ynn=1n. the three classifiers used were linear svm and nonlinear svm with radial basis function  and polynomial kernels  <cit> , with c =  <dig>  parameters of the nonlinear svm classifiers were selected by cross-validation. prior to the classification, the sets of extracted components were standardized to zero mean and unit variance. although the standardization across the features is used more often, a standardization across the components  has been performed here. it yielded much better accuracy and such a fact has also been observed in chapter  <dig> in  <cit> , where in microarray data analysis standardization across the samples has also been preferred over standardization across the features. in comparative performance analysis presented in tables  <dig>   <dig> and  <dig> the best result  on all four sets of selected components has been used to represent component selection method proposed here. since many components extracted by other combinations of the parameters yielded also good prediction accuracy we have posted complete results in the additional material files section  accompanied with the paper. reference samples used to represent disease and control groups were obtained by averaging all the samples in disease group, xdisease=1n <dig> ∑i=1n1xi where xi∈xn:yn=1n=1n <dig>  and control group, xcontrol=1n <dig> ∑i=1n2xi where xi∈xn:yn=1n=1n <dig> and n <dig> + n <dig> = n. we thought this is the most fair approach in the absence of any prior information that could suggest which labelled sample could serve as a gold standard. we conclude this section by providing assessment of the computational complexity of proposed method. it has been implemented in matlab  <dig>  environment on a desktop computer based on  <dig> ghz dual core processor and  <dig> gb of ram. processing of proteomic and genomic datasets used in sections  <dig>  to  <dig>  took  <dig>   <dig> and  <dig> minutes respectively.

proposed method m =  <dig>  Δθ = 50
λ = 10-4λmax
proposed method m =  <dig>  Δθ = 30
λ = 10-6λmax
proposed method m =  <dig>  Δθ = 10
proposed method m =  <dig>  Δθ = 10
proposed method m =  <dig>  Δθ = 10
proposed method m =  <dig>  Δθ =  <dig> λ = 10-2λmax
 <dig>  model validation
this section presents model validation results obtained on simulated data using lmm /. to this end, each mixture sample has been composed of ten orthogonal components comprised of k =  <dig> features. the orthogonality implies that each feature belongs to one component only. by a convention, the first component has been selected to contain disease specific features, the tenth component to contain control specific features and the components two to nine contain features that are not differentially expressed and share similar concentrations in control and disease labelled samples. a concentration variability across the sample population is simulated using the following model for disease group of samples:

 xn= ∑m-1msin2sm 

and for control group of samples:

  xn= ∑m-1mcos2sm 

thus, by controlling the mixing angles {θnm}n= <dig> m=1n,m the amount of a concentration of each component in disease and control samples is controlled. also amount of concentration variability is controlled by selecting {θnm}n= <dig> m=1n,m to be confined within  overlapping angular sectors. note that  implies that component sm is contained in a related disease and control samples in overall concentration of 100%. to simulate biological variability between the samples, the relative concentration has been varied across the sample population, where disease and control groups contained  <dig> samples each. the concentration vectors were overlapping in the mixing angle domain i.e. a concentration vector for disease specific features was confined in the sector of , for the neutral features it was in the sector of  <cit>  and for control specific features it was confined in the sector of . thus, amount of overlap between concentration profiles was significant, implying that in many cases neutral features were contained in greater concentrations in disease labelled samples than disease specific features, as well as that neutral features were contained in greater concentrations in control labelled samples than control specific features. figures 2a and 2b show disease prediction results using four extracted disease and control specific components with the postulated overall number of components equal to m =  <dig> , m =  <dig> , m =  <dig>  and m =  <dig> . reference samples used in lmm / were obtained by averaging all the samples in control i.e. disease group. results reported in terms of sensitivity  and specificity  were obtained by the linear support vector machine  classifier using  <dig> independent two-fold cross-validations. scps selection parameter has been set to Δθ =  <dig> and sparseness regularization parameter in / to λ = 10-6·λmax. these parameters were not selected through cross-validation since the purpose of the computational experiment has been to evaluate influence of the assumed number of components m to the prediction accuracy when concentration varies across the sample population. the presented results demonstrate that greater number of postulated components does not decrease prediction accuracy . however, increased number of postulated components m reduces the number of features contained in disease and control specific components selected for classification. as discussed previously, a greater m yields less complex disease and control specific components. following the principle of parsimony such solution should be preferred over the more complex ones that are obtained for smaller m. thus, selected disease and control specific components are expected to be more discriminative and less sensitive to over-fitting when the number of postulated components is increased. in practical implementation of the proposed approach to component selection the optimal number of overall components needs to be evaluated by a cross-validation. in the three real world experiments reported below the number of components has been selected by cross-validation from m ∈ { <dig>   <dig>   <dig>  5}. if a prediction accuracy achieved for the two values of m is approximately equal, it is better to prefer components extracted from the samples with a greater value of m.

 <dig>  ovarian cancer prediction from a protein mass spectra
low resolution surface-enhanced laser desorption ionization time-of-flight  mass spectra of  <dig> controls and  <dig> cases have been used for ovarian cancer prediction study  <cit> . see also the website of the clinical proteomics program of the national cancer institute ,  <cit> , where the used dataset is labelled as "ovarian 4-3-02". all spectra were baseline corrected. thus, some intensities have negative values. table  <dig> presents the best result obtained by the proposed sca-based component selection method together with results obtained for the same dataset by competing methods reported in cited references as well as by predictive factorization method proposed in  <cit> . described sca method has been used to extract four sets of components with the overall number of components m assumed to be  <dig>   <dig>   <dig> and  <dig>  figure  <dig> shows sensitivities and specificities estimated by  <dig> independent two-fold cross-validations using linear svm classifier which yielded the best results compared against nonlinear svm classifiers based on polynomial and rbf kernels. performance improvement is visible when assumed number of components is increased from  <dig> to  <dig>   <dig> or  <dig>  the error bars are dictated by the sample size and would decrease with a larger sample. thus, the mean values should be looked at to observe the trend in performance as a function of m. the best result  has been obtained with the linear svm classifier for m =  <dig> with sensitivity of  <dig> % and specificity of  <dig> %, but results with the very similar quality have been obtained for several combinations of the parameters m, Δθ and λ, see figure  <dig>  most notably m =  <dig> . as seen in table  <dig>  only  <cit>  reported better result for a two-fold cross-validation with the same number of partitions. there, a combination of genetic algorithm and k-nearest neighbours method, originally developed for mining of high-dimensional microarray gene expression data, has been used for analysis of proteomics data. however, the method  <cit>  is tested on proteomic ovarian cancer dataset only, while the method proposed here exhibited excellent performance in prediction of prostate cancer from proteomic data , as well as on colon cancer from genomic data . the method shown in  <cit>  used  <dig> samples from the control group and  <dig> samples from the ovarian cancer group to discover a pattern that discriminated cancer from non-cancer group. this pattern has then been used to classify an independent set of  <dig> samples with ovarian cancer and  <dig> samples unaffected by ovarian cancer. in  <cit> , a fuzzy rule based classifier fusion is proposed for feature selection and classification  of protein mass spectra based ovarian cancer. demonstrated accuracy of 98-99% has been estimated through  <dig> ten-fold cross-validations . moreover, as demonstrated in sections  <dig>  and  <dig> , the method proposed here exhibited good performance on diagnosis of prostate and colon cancers from proteomic and gene expression levels, respectively. in  <cit> , a clustering based method for feature selection from mass spectrometry data is derived by combining k-means clustering and genetic algorithm. the method exhibited an accuracy of  <dig> % , but this has been assessed through three-fold cross-validations .

 <dig>  prostate cancer prediction from a protein mass spectra
low resolution seldi-tof mass spectra of  <dig> controls: no evidence of cancer with prostate-specific antigen < <dig>  and  <dig> cases :  <dig> with 4<psa< <dig> and  <dig> with psa> <dig>  have been used for prostate cancer prediction study  <cit> . there are additional  <dig> control samples with benign cancer  available as well , in dataset labelled as "jnci_data_7-3-02". however, in the two-class comparative performance analysis problem reported here these samples were not used. proposed sca-based method has been used to extract four sets of components with the overall number of components m assumed to be  <dig>   <dig>   <dig> and  <dig>  the best result has been achieved for m =  <dig> with sensitivity of  <dig> % and specificity of 99%, but results with the very similar quality have been obtained for several combinations of the parameters m, Δθ and λ, . table  <dig> presents two best results achieved by the proposed sca-based approach to component selection together with the results obtained by competing methods reported in cited references. linear svm classifier yielded the best results when compared against nonlinear svm classifiers based on polynomial and rbf kernels. according to table  <dig>  comparable result  is in the reference  <cit>  only. the method  <cit>  is proposed for analysis of mass spectra for screening of prostate cancer. the system is composed of three stages: a feature selection using statistical significance test, a classification by radial basis function and probabilistic neural networks and an optimization of the results through the receiver-operating-characteristic analysis. the method achieved sensitivity  <dig> % and specificity  <dig> % but the cross-validation setting has not been described in details. in  <cit> , the training group has been used to discover a pattern that discriminated cancer from non-cancer group. this pattern has then been used to classify an independent set of  <dig> patients with the prostate cancer and  <dig> patients with the benign conditions. the obtained specificity is low. the predictive matrix factorization method  <cit>  yielded significantly worse result than the method proposed here. in  <cit>  a clustering based method for feature selection from mass spectrometry data is derived combining k-means clustering and genetic algorithm. despite a three-fold cross-validation, the reported error was  <dig> %. figure  <dig> shows sensitivities and specificities estimated by  <dig> independent two-fold cross-validations using linear svm classifier on components selected by the method proposed here. for each m the optimal values of the parameters λ and Δθ  have been used to obtain results shown in figure  <dig>  increasing a postulated number of components from  <dig> to  <dig> increased accuracy from  <dig> % to  <dig> %. thus, better accuracy is achieved with the smaller number of features  contained in selected components.

 <dig>  colon cancer prediction from gene expression profiles
gene expression profiles of  <dig> colon cancer and  <dig> normal colon tissue samples obtained by an affymetrix oligonucleotide array  <cit> , have been also used for validation and comparative performance analysis of proposed feature extraction method. gene expression profiles have been downloaded from  <cit> . original data produced by oligonucleotide array contained more than  <dig> genes but only  <dig> high-intensity genes have been used for cluster analysis in  <cit>  and are provided for download on the cited website. the proposed sca-based approach to feature extraction/component selection has been used to extract four sets of components with up- and down-regulated genes and with the overall number of components m assumed to be  <dig>   <dig>   <dig> and  <dig>  the linear svm classifier has been applied to groups of the four sets of selected components extracted from gene expression levels for specific combinations of parameters Δθ, λ and m. the best result in terms of sensitivity and specificity for each m has been selected and shown in figure  <dig>  the complete list of results obtained by linear svm classifier is presented in the additional file  <dig>  an increased number of postulated components m did not decrease accuracy but it yielded components selected for classification with reduced number of genes. this is verified in figure  <dig> which shows component with up-regulated genes scontroldisease extracted from a cancer labelled sample w.r.t. the control reference for assumed number of components m =  <dig> and m =  <dig>  thus, it is confirmed again that an increased m yields less complex components that , should be preferred over the more complex ones obtained by smaller m. in order to  increase the prediction accuracy, we have applied nonlinear, polynomial and rbf svm classifiers to the two groups of the four sets of components that yielded the best results with the linear svm classifier: m =  <dig>  and m =  <dig> . the polynomial svm classifier has been cross-validated for degree of the polynomial equal to d =  <dig>   <dig> and  <dig>  the rbf svm classifier κ= exp-x-y22/2σ <dig> has been cross-validated for the variance σ <dig> in the range  <dig> ×  <dig> to  <dig>  ×  <dig> in steps of  <dig>  the best result has been obtained with σ <dig> =  <dig>  ×  <dig> for m =  <dig> and with σ <dig> =  <dig>  ×  <dig> for m =  <dig>  an achieved accuracy is comparable with the accuracy obtained by other state-of-the-art results reported. that is shown in table  <dig> as well as in the additional file  <dig>  a predictive matrix factorization method  <cit>  yielded slightly better results here, but it has shown significantly worse result in the cases of ovarian  and prostate  cancers. gene discovery method  <cit>  has been applied for three values of the threshold cu ∈ { <dig>   <dig> , 3} used to select up-regulated genes. maximum a posteriori probability has been used for an assignment of genes to each of the three components containing up-, down regulated and differentially not expressed genes. thus for each threshold value the two components were obtained for training a classifier. the logarithm with the base  <dig> has been applied to gene folding values prior gene discovery/selection took place. the best result reported in table  <dig> has been obtained for a component containing up-regulated genes with cu =  <dig>  and an rbf svm classifier, whereas σ <dig> has been cross-validated in the range  <dig> to  <dig> in steps of  <dig>  the best result has been obtained for σ <dig> =  <dig> ×  <dig>  the gene discovery method  <cit>  outperformed slightly the method proposed here. however as opposed to the proposed method, the gene discovery method  <cit>  is not applicable to the analysis of mass spectra. the gene selection method in  <cit>  is a model driven trying to take into account the genes' group behaviours and interactions by developing an ensemble dependence model . the microarray dataset is clustered first. the edm is based on modelling dependencies that represent inter-cluster relationships. inter-cluster dependence matrix is the basis for discrimination between cancerous and non-cancerous samples. classification accuracy of 85% reported in  <cit>  is very close to the one obtained by the sca-based method proposed here. however, while sca-based performance has been assessed through two-fold cross-validation, no cross-validation details were reported in  <cit> . similarly, sensitivity had to be estimated indirectly from figure  <dig> in  <cit> . the method in  <cit>  combines a recursive feature extraction and the linear svm to yield accuracy of  <dig> %. this is also less accurate than what has been achieved by the method proposed. moreover, the very accuracy reported in  <cit>  has been assessed by a ten-fold cross-validation only and that is known to yield a too optimistic performance assessment. in this regard accuracy reported in  <cit>  can be taken closer to the realistic one since it has been assessed by two-fold cross-validation. this method, as  <cit> , again combines recursive feature elimination with the svm, but it is taking additionally into account the parameter c. a reported accuracy of  <dig> % is slightly better than the one obtained by the method proposed here. however, the proposed method is a classifier independent one and, as demonstrated in sections  <dig>  and  <dig> , it yields good results on cancer diagnosis from proteomic datasets as well.

CONCLUSIONS
this work presents a feature extraction/component selection method based on innovative additive linear mixture model of a sample  and sparseness constrained factorization that operates on a sample-by-sample basis. that is different in respect to the existing methods which factorize complete dataset simultaneously. the sample model is comprised of a test sample and a reference sample representing disease and/or control group. each sample is decomposed into several components selected automatically , without using label information, as disease-, control specific and differentially not expressed. an automatic selection is based on mixing angles which are estimated from each sample directly. hence, due to the locality of decomposition, the strength of the expression of each feature can vary from sample to sample. however, the feature can still be allocated to the same  component in different samples. as opposed to that, feature allocation/selection algorithms that operate on a whole dataset simultaneously try to optimize a single threshold for the whole dataset. selected components can be used for classification due to the fact that labelled information is not used in the selection. moreover, disease specific component can also be used for further biomarker related analysis. as opposed to the existing matrix factorization methods, such disease specific component can be obtained from one sample  only. by postulating one or more components with differentially not expressed features the method yields less complex disease and control specific components that are composed of smaller number of features with higher discriminative power. this has been demonstrated to improve prediction accuracy. moreover, decomposing sample with one or more components with indifferent features performs  sample adaptive preprocessing related to removal of features that do not significantly vary across the sample population. the proposed feature extraction/component selection method is demonstrated on the real world proteomic datasets used for prediction of the ovarian and prostate cancers as well as on the genomic dataset used for the colon cancer prediction. results obtained by  <dig> two-fold cross-validations are compared favourably against most of the state-of-the-art methods cited in the literature and used for cancer prediction on the same datasets.

authors' contributions
ik has proposed novel linear mixture model of the samples and methodology for automatic selection of disease and control specific components extracted from the samples by means of sparse component analysis. he also has been performed model validation and implemented the clustering phase of the sparse component analysis method. mf implemented iterative thresholding based shrinkage algorithm for extraction of the components and performed cross-validation based component extraction and classification. all authors read and approved the final manuscript.

supplementary material
additional file 1
code with implementation of proposed feature extraction/component selection method.

click here for file

 additional file 2
classification results obtained by the linear svm applied to disease and control specific components extracted from the ovarian cancer dataset for various combination of parameters m, λ and Δθ.

click here for file

 additional file 3
classification results obtained by the linear svm applied to disease and control specific components extracted from the prostate cancer dataset for various combination of parameters m, λ and Δθ.

click here for file

 additional file 4
classification results obtained by the linear svm applied to disease and control specific components extracted from the colon cancer dataset for various combination of parameters m, λ and Δθ.

click here for file

 additional file 5
best classification results obtained by the rbf svm applied to disease and control specific components extracted from the colon cancer dataset for m =  <dig>  λ = 10-2λmax and Δθ =  <dig> and m =  <dig> and Δθ =  <dig> 

click here for file

 acknowledgements
this work has been supported by ministry of science, education and sports, republic of croatia through grant 098-0982903- <dig>  professor vojislav kecman's and dr. ivanka jerić's help in proofreading the manuscript is gratefully acknowledged.
