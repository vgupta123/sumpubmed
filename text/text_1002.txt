BACKGROUND
the prevalence of next-generation sequencing  has increased throughput for generating genomic data and our ability to perform genomic analysis. genomic analyses continue to increase our ability to understand genetic diseases and disorders.

ngs technologies take short fragments of dna obtained from the genome of an organism of interest. the fragments are then sequenced. sequencing reads is the process of identifying a fragment's constituent nucleotides through chemical processes. a fragment with all bases identified, whether correctly or incorrectly, forms a read  <cit> .

these reads are used in several different types of analyses. one example of this is in genome assembly. the generated reads are assembled into the source genome through a process of identifying reads that originated from the same regions and assembling them by merging them into longer, contiguous sequences called contigs. assembly is complex and difficult due to the short length of sequenced reads and requires volumes of high fidelity data to be accomplished accurately  <cit> .

ngs technologies are, however, imperfect and misidentify some of the nucleotides contained in a dna fragment as they are being sequenced  <cit> . thus, errors are introduced into the reads. these errors are problematic because they introduce false genetic information into a dataset and complicate genome assemblies. error correction packages have been created that use different techniques in order to locate misidentified bases during sequencing and to correct them to their true sequence  <cit> . this allows for recovery of data that could otherwise confound a genome assembler. while error correction software is intended to correct errors, it can also introduce errors into a readset through mis-correction.

genome assembly becomes even more complicated in the presence of heterozygosity. many organisms have a ploidy greater than one, including humans. diploid and polyploid organisms inherit genetic information from multiple sources . one set of genetic variation inherited from a parent is called a haplotype. when sequencing and assembling the genome of an organism, haplotype conservation is important for preserving and understanding the biological reality of an organism's genome. despite lower heterozygosity rates in humans compared to some organisms  <cit> , proper identification of polymorphism remains fundamental to genotype-phenotype analyses.

many current genome assemblers do not maintain segregated haplotypes  <cit> . conservation of separate haplotypes during genome assembly allows for better understanding of complex diseases and phenotypes that have not been associated with single variations. haplotype-aware analyses allow for more powerful understanding of haplotypes and phenotypes. they also enable better protein structure prediction  <cit> . studies of the effects of heterozygosity on error correction performance are lacking.

in this study, two different error correction techniques are comparatively analyzed by examining their effects on next-generation sequencing data from a heterozygous genome. these two packages are quake and echo.

quake
quake is a k-spectrum based approach to base-call error correction. quake analyzes the k-mer coverage distribution. then, a cut-off is determined from the distribution that identify trusted and untrusted k-mers. trusted k-mers are used to identify errors in the readset. a set of possible corrections is made and searched to find the correction with the maximum likelihood of making all k-mers trusted. in addition to correcting miscalled bases, quake also trims reads. trimming is the process of removing the ends of reads to remove low quality bases. quake requires that the value of k be specified at runtime. a formula is given by the creators of quake to calculate k based on genome size  <cit> .

echo
echo utilizes a multiple sequence alignment  approach in order to perform base-call error correction. based on a probabilistic model, echo first clusters reads from the same region of the source genome then corrects the reads. echo does not require input of genome size or any other input parameters to be run. the authors state that " explicitly models heterozygosity in diploid genomes and provides a reference-free method for detecting bases that originated from heterozygous sites"  <cit> .

RESULTS
synthetic datasets
quake
both haploid and diploid genome sizes of the genome were used when calculating the appropriate k for quake. the results from using the haploid and diploid genome sizes had nearly identical results. quake's general performance does not appear to change much as heterozygosity increases. quake's performance on both heterozygous and homozygous errors considered together does not appear to change much as heterozygosity increases .

when correcting the datasets with all reads for a particular error and heterozygosity rate combined  and using the diploid and the haploid genome sizes as parameters, the error-corrected reads showed several of the same general trends for the first three error rates for errors at heterozygous positions. the rate of errors at heterozygous locations that were corrected correctly increased. the rate of errors at heterozygous locations that were not corrected decreased. the rate of errors at heterozygous locations corrected to the wrong haplotype was low  and varied little. the rate of errors at heterozygous locations corrected to neither of the haplotypes was low  and varied little. finally, the rate of heterozygous locations that had no error corrected to the other haplotype or to neither haplotype was low and varied little .

a comparison of the first three error rates showed two variations. first, the rate of errors at heterozygous locations corrected to the wrong haplotype increased slightly as the error rate increased. second, the rate of non-error heterozygous locations corrected to the wrong haplotype was slightly increased with increased error rates.

when correcting the datasets split by haplotype , several general trends hold for the first three error rates at all levels of heterozygosity. the rate of errors at heterozygous locations corrected correctly was high. the rate of errors at heterozygous locations not corrected was low. the rate of errors at heterozygous locations corrected to the wrong haplotype was low. lastly, the rate of errors at heterozygous locations corrected to neither of the haplotypes was low.

echo
echo differs from quake because it does not require the user to specify parameters at runtime. therefore, echo was run once on the heterozygous datasets and again on the homozygous datasets with no variation of parameters. the accuracy of echo was similar to quake as little variation in performance when observing all errors  was found. the rate of errors at heterozygous locations that were corrected correctly increased. the rate of errors at heterozygous locations corrected to the wrong haplotype decreased. the rate of errors at heterozygous locations that were not corrected decreased. observed variations between the first three error rates show that the rate of non-error heterozygous locations corrected to the other haplotype decreases as the error rate increases .

echo's correction of the homozygous data for the first three error rates produced trends similar to quake's. the rate of errors at heterozygous locations corrected correctly was high. the rate of errors at heterozygous locations was low. the rate of errors at heterozygous locations corrected to the wrong haplotype or to neither haplotype was low.

real sequencing data
quake
assemblies using quake-corrected reads were superior to the uncorrected assembly . the corrected assembly had a decreased number of contigs, larger n <dig>  and an increased largest contig length. the assembly using quake-corrected reads where the raw reads were segregated by strain during correction performed better than the assembly when all reads were corrected together. n <dig> nearly doubled when comparing the assembly of reads segregated by strain during correction to the assembly with reads together during correction. the length of the largest contig also increased significantly.

there were five different assemblies. the assembly of raw reads involved no correction. for both quake and echo, the reads were corrected separately by strain and corrected together with reads from both strains present then assembled. the number of contigs, the n <dig> size and the length of the largest contig of each assembly is shown.

echo
assemblies using echo-corrected reads made improvements to the uncorrected assembly . the improvements were not as drastic as those made by quake. the n <dig> size remained at  <dig> base pairs for both assemblies after using echo. the largest contig created by both assemblies was better than the uncorrected assembly. it was also slightly improved in the assembly where strains were corrected together compared to when strains were corrected separately. the number of contigs also drops when compared to the uncorrected assembly and is additionally improved when the reads were corrected with strains segregated.

discussion
general trends
both error correction software packages increased their rate of correctly correcting errors at heterozygous locations as the heterozygosity rate increased. this may be the case because as heterozygosity increases, the haplotypes become more unique. as the sequences become more unique, the error correction algorithms are able to treat the homologs as unique sequences. as can be seen, reads from an organism with high heterozygosity were error corrected better than reads from an organism with low heterozygosity. however, many interesting organisms, such as humans, have low heterozygosity  <cit> .

quake
quake differed from echo by its conservative nature when choosing whether or not to error correct. this trend manifested itself in the high rate of errors at heterozygous positions that quake did not correct when heterozygosity was low. at lower heterozygosity rates, the variations between homologs were sparse. there were, therefore, fewer surrounding heterozygous markers to indicate a proper correction for an error. quake was less certain of how to correct a putative error at these low heterozygosity rates and preferred to leave them uncorrected. a benefit of this can also be seen by the lower rate at which errors were introduced into the readset by quake . fewer errors being corrected is undesirable but also results in fewer introduced errors.

quake has a low rate of correcting heterozygous errors to the wrong haplotype. additionally, it did not introduce errors at non-error heterozygous positions by correcting them to the wrong or neither haplotype . additionally, few chimeric reads were present after correction . the low levels of introduced errors and chimeric reads can be attributed to conservative correcting and trimming of reads.

trimming reads may have given quake an advantage in introducing few errors and creating few chimeric reads. a downside to this, however, can be seen by looking at the proportion of reads that contain >  <dig> heterozygous marker. as heterozygosity increases, quake has much fewer reads with >  <dig> heterozygous marker compared to echo . reads with >  <dig> heterozygous marker are essential to haplotype specific genome assembly. variants contained within the same read can be used to associate them with each other. this association of variants can be used to form a haplotype  <cit> . upon further analysis, approximately 60% of all errors in reads that were corrected were removed through trimming . 40% of these errors were at heterozygous sites when an error rate of approximately  <dig> % was used.

echo
one of the strengths of echo is that no user input parameters are required in order to run. there is also no trimming. this leaves entire reads intact except for when corrected. the lack of trimming may or may not be a weakness depending on the needs of the user.

echo showed itself to be aggressive in terms of error correction. this differentiates it from quake's conservative tendencies. aggressive correction proved to be a good feature when presented with homozygous data. however, it was detrimental when heterozygous data was present. at low heterozygosity rates, echo corrected errors and non-errors at heterozygous positions to the wrong or neither haplotype approximately 10% of the time in the worst cases at the lowest error rate. echo corrected many errors that quake did not correct at low heterozygosity rates. however, it also introduced more errors into the readset. the effects of introducing errors at heterozygous positions can be seen in the number of chimeric reads after correction.

echo produced more chimeric reads than quake . as discussed earlier, this feature can be detrimental for downstream use of these corrected reads. the most likely reason for this increase in chimeric reads is the lack of read trimming in echo when errors are encountered near the end of a read. this leaves all heterozygous positions present in a read. having more heterozygous positions present is a key feature for haplotype-aware studies. compared to quake, the number of reads that contain >  <dig> heterozygous base is higher, especially as heterozygosity increases . though more reads may be chimeric, the higher number of reads with multiple heterozygous markers may be beneficial.

real sequencing data
using either quake or echo improved the real sequence data genomic assemblies. the overall number of contigs returned after assembly were decreased on all error-corrected assemblies. this is important because with fewer returned contigs, the scaffolding graph is much less complicated. more improvements can be found when using quake compared to echo. it is unclear how much of the improvement that was found after using quake is attributed to read trimming or due to the correction of errors. the improvements seen in all error-corrected assemblies shows how error correcting can improve downstream analysis. the assembly created from reads corrected by quake when the reads were separated by strain was significantly better than the assembly produced with mixed reads. a haplotype-aware error correction algorithm should be able to achieve this level of performance.

CONCLUSIONS
neither quake nor echo performed adequately as an error correction algorithm on heterozygous data. an acceptable error correction algorithm for heterozygous reads would provide results similar to figure  <dig> when correcting a heterozygous dataset.

echo is a very aggressive corrector. quake, on the other hand, is much more conservative. aggressive correction provides the benefit of correcting many of the errors. it also results in the introduction of errors. many of these errors occur at heterozygous positions. conservative correction results in more of the original errors persisting through the correction phase but fewer new errors being introduced.

an important distinction between the original errors and introduced errors should be made. original errors are much less detrimental to downstream analysis than introduced errors. introduced errors are placed in non-random locations and decrease the ability to compensate for these errors through high coverage.

in addition to correction accuracy, the number of heterozygous markers present after correction should also be considered. because quake trims reads, a large portion of heterozygous bases were removed from the datasets. this created fewer errors but left fewer heterozygous markers in the corrected dataset. echo, although it had lower accuracy as a result of more introduced errors and mis-corrections of read errors, left all heterozygous positions in the corrected dataset.

looking at the effects on real sequencing data and their subsequent assemblies was also enlightening and showed effects of error correction on real data. it is shown that both algorithms improve the assembly. the effects of haplotype-aware error correction can particularly be seen in the case of quake when the reads from each strain were segregated.

there is much improvement that can be made to error correction algorithms to better handle heterozygous datasets. haplotype-aware correction algorithms should be able to enhance genome analysis. other approaches also include read classification and segregation by haplotype before error correcting. segregating the data creates datasets that resemble homozygous data. thus, many current bioinformatic tools could be used on heterozygous datasets that are segregated by haplotype.

