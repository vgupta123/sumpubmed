BACKGROUND
the internet is fast becoming a recognized source of information in many fields, including health. in this domain, as in others, users are now experiencing huge difficulties in finding precisely what they are looking for among the numerous documents available online, and this in spite of existing tools. in medicine and health-related information accessible on the internet, general search engines, such as google, or general catalogues, such as yahoo, cannot solve this problem efficiently  <cit> . this is because they usually offer a selection of documents that turn out to be either too large or ill-suited to the query. free text word-based search engines typically return innumerable completely irrelevant hits, which require much manual weeding by the user, and also miss important information resources.

in this context, several health gateways  <cit>  have been developed to support systematic resource discovery and help users find the health information they are looking for. these information seekers may be patients but also health professionals, such as physicians searching for clinical trials. health gateways rely on thesauri and controlled vocabularies. some of them are evaluated in  <cit> . thesauri are a proven key technology for effective access to information since they provide a controlled vocabulary for indexing information. they therefore help to overcome some of the problems of free-text search by relating and grouping relevant terms in a specific domain. nonetheless, medical vocabularies are difficult to handle by non-professionals.

many tools have been developed to improve information retrieval from such gateways. they exploit techniques such as natural language processing, statistics, lexical and background knowledge ... etc. however, a simple spelling corrector, such as google's "did you mean:" or yahoo's "also try:" feature may be a valuable tool for non-professional users who may approach the medical domain in a more general way  <cit> . such features can improve the performance of these tools and provide the user with the necessary help. in fact, the problem of spelling errors represents a major challenge for an information retrieval system. if the queries  generated by information seekers remain undetected, this can result in a lack of outcome in terms of search and retrieval. a spelling corrector may be classified in two categories. the first relies on a dictionary of well-spelled terms and selects the top candidate based on a string edit distance calculus. an approximate string matching algorithm, or a function, is required to detect errors in users' queries. it then recommends a list of terms from the dictionary that are similar to each query word. the second category of spelling correctors uses lexical disambiguation tools in order to refine the ranking of the candidate terms that might be a correction of the misspelled query. several studies have been published on this subject. we cite the work of grannis  <cit>  which describes a method for calculating similarity in order to improve medical record linkage. this method uses different algorithms such as jaro-winkler, levenshtein  <cit>  and the longest common subsequence . in  <cit>  the authors suggest improving the algorithm for computing levenshtein similarity by using the frequency and length of strings. in  <cit>  a phonetic transcription corrects users' queries when they are misspelled but have similar pronunciation . in  <cit>  the authors propose a simple and flexible spell checker using efficient associative matching in a neural system and also compare their method with other commonly used spell checkers.

in fact, the problem of automatic spell checking is not new. indeed, research in this area started in the 1960's  <cit>  and many different techniques for spell checking have been proposed since then. some of these techniques exploit general spelling error tendencies and others exploit phonetic transcription of the misspelled term to find the correct term. the process of spell checking can generally be divided into three steps  error detection: the validity of a term in a language is verified and invalid terms are identified as spelling errors  error correction: valid candidate terms from the dictionary are selected as corrections for the misspelled term and  ranking: the selected corrections are sorted in decreasing order of their likelihood of being the intended term. many studies have been performed to analyze the types and the tendencies of spelling errors for the english language. according to  <cit>  spelling errors are generally divided into two types,  typographic errors and  cognitive errors. typographic errors occur when the correct spelling is known but the word is mistyped by mistake. these errors are mostly related to keyboard errors and therefore do not follow any linguistic criteria . cognitive errors, or orthographic errors, occur when the correct spelling of a term is not known. the pronunciation of the misspelled term is similar to the pronunciation of the intended correct term. in english, the role of the sound similarity of characters is a factor that often affects error tendencies  <cit> . however, phonetic errors are harder to correct because they deform the word more than a single insertion, deletion or substitution. indeed, over 80% of errors fall into one of the following four single edit operation categories:  single letter insertion;  single letter deletion;  single letter substitution and  transposition of two adjacent letters  <cit> .

the third step in spell-checking is the ranking of the selected corrections. main spell-checking techniques do not provide any explicit mechanism. however, statistical techniques provide ranking of the corrections based on probability scores with good results  <cit> .

honselect  <cit>  is a multilingual and intelligent search tool integrating heterogeneous web resources in health. in the medical domain, spell-checking is performed on the basis of a medical thesaurus by offering information seekers several medical terms, ranging from one to four differences related to the original query. exploiting the frequency of a given term in the medical domain can also significantly improve spelling correction  <cit>  : edit distance technique is used for correction along with term frequencies for ranking. in  <cit>  the authors use normalization techniques, aggressive reformatting and abbreviation expansion for unrecognized words as well as spelling correction to find the closest drug names within rxnorm for drug name variants that can be found in local drug formularies. it returns only drug name suggestions. to match queries with the mesh thesaurus, wilbur et al.  <cit>  propose a technique on the noisy channel model and statistics from the pubmed logs.

research has focused on several different areas, from pattern matching algorithms and dictionary searching techniques to optical character recognition of spelling corrections in different domains. however, relatively few groups have studied spelling corrections regarding medical queries in french. in this paper, a simple method is proposed : it combines two approximate string comparators, the well-known levenshtein  <cit>  edit distance and the stoilos function similarity defined in  <cit>  for ontologies. we apply and evaluate these two distances, alone and combined, on a set of sample queries in french submitted to the health gateway cismef  <cit> . the queries may be submitted both by health professionals in their clinical practice as well as patients. the system we have designed aims to correct errors resulting in non-existent terms, and thus reducing the silence of the associated search tool.

methods
similarity functions
similarity functions between two text strings s <dig> and s <dig> give a similarity or dissimilarity score between s <dig> and s <dig> for approximate matching or comparison. for example, the strings "asthma" and "asthmatic" can be considered similar to a certain degree. modern spell-checking tools are based on the simple levenshtein edit distance  <cit>  which is the most widely known. this function operates between two input strings and returns a score equivalent to the number of substitutions and deletions needed in order to transform one input string into another. it is defined as the minimum number of elementary operations that is required to pass from a string s <dig> to a string s <dig>  there are three possible transactions: replacing a character with another, deleting a character and adding a character. this measure takes its values in the interval . the normalized levenshtein  <cit>   in the range  <cit>  is obtained by dividing the distance of levenshtein lev by the size of the longest string and it is defined by the following equation :

  levnorm=levmax 

levnorm  ∈  <cit>  as lev <max.

for example, levnorm =  <dig> , as lev =  <dig> ; |eutanasia| =  <dig> and |euthanasia| =  <dig> 

we complete the calculation of the levenshtein distance by the similarity function stoilos proposed in  <cit> . it has been specifically developed for strings that are labels of concepts in ontologies. it is based on the idea that the similarity between two entities is related to their commonalities as well as their differences. thus, the similarity should be a function of both these features. it is defined by the equation  where comm stands for the commonality between the strings s <dig> and s <dig>  diff for the difference between s <dig> and s <dig>  and winkler for the improvement of the result using the method introduced by winkler in  <cit> :

  sim=comm-diff+winkler 

the function of commonality is determined by the substring function. the biggest common substring between two strings  is computed. this process is further extended by removing the common substring and by searching again for the next biggest substring until none can be identified. the function of commonality is given by the equation :

  comm=2×∑i|maxcomsubstringi||s1|+|s2| 

for example for the strings s <dig> = trigonocepahlie and s <dig> = trigonocephalie we have: |maxcomsubstring1|=|trigonocep| = 10; |maxcomsubstring2|=|lie| =  <dig> comm =  <dig> .

the difference function diff is based on the length of the unmatched strings resulting from the initial matching step. the function of difference is defined in equation  where p ∈ , |us1| and |us2| represent the length of the unmatched substring from the strings s <dig> and s <dig> scaled respectively by their length:

  diff=|us1|×|us2|p+1-p×|us1|+|us2|-|usl|×|us2| 

for example for s <dig> = trigonocepahlie and s <dig> = trigonocephalie and p =  <dig>  we have: |us1| = 2/15; |us2| = 2/15; diff =  <dig> .

the winkler parameter winkler is a factor that improves the results  <cit> . it is defined by the equation  where l is the length of common prefix between the strings s <dig> and s <dig> at the start of the string up to a maximum of  <dig> characters and p is a constant scaling factor for how much the score is adjusted upwards for having common prefixes. the standard value for this constant in winkler's work is p =  <dig>  :

  winkler=l×p×) 

for example, sim, between the strings s <dig> = hyperaldoterisme and s <dig> = hyperaldosteronisme. we have |s1| =  <dig>  |s2| = 19; the common substrings between s <dig> and s <dig> are hyperaldo, ter, and isme. comm =  <dig> ; diff = 0; winkler =  <dig>  and sim =  <dig> .

processing users' queries
as detailed in  <cit> , spelling errors can be classified as typographic and phonetic. cognitive errors are caused by a writer's lack of knowledge and phonetic ones are due to similar pronunciation of a misspelled and corrected word. the queries are pre-processed by a phonetic transcription before applying the levenshtein edit distance along with the similarity function stoilos.

cismef is a quality-controlled health gateway developed at rouen university hospital in france  <cit> . doc'cismef is the search tool associated with cismef. many ways of navigation and information retrieval are possible through the catalogue. the most used is the simple search, with a free text interface. the information retrieval algorithm is based on the subsumption relationships  between medical terms, using their hierarchical information, going from the top of the hierarchy to the bottom. if the user query can be matched to an existing term from the terminology, the result is thus the union of the resources indexed by the term, and the resources that are indexed by the terms it subsumes, either directly or indirectly, in all the hierarchies it belongs to. for example, a query on the term hepatitis gives a set of documents indexed by the descriptor hepatitis but also by the descriptors hepatitis a, hepatitis b and so on. however, the vocabularies of medical terminologies are difficult to apprehend for a user who is not familiar with the domain.

the different materials that we have used to apply the method of spell-checking are related mainly to the search tool doc'cismef: a set of queries and a dictionary of entry terms.

first set of test queries
we first selected a set of queries sent to doc'cismef by different users. a set of  <dig>  queries were extracted from the query log server . only the most frequent queries were selected. in fact some queries are more frequent than others. for example, the query "swine flu" is more present in the query log than "chlorophyll". we eliminated the doubles . from these  <dig>  queries, we selected  <dig>  queries to extract those with no answers . from these, we selected queries with misspellings from the most frequent queries in the original set and constituted a first sample test of  <dig> queries. to avoid phonetic errors of misspelling we first performed a phonetic transcription of this sample with the "phonemisation" function the method of which is detailed below.

phonetic transcription of queries and dictionary
soundex  was the first phonetic string-matching algorithm developed in  <dig>  <cit>  for name matching. the idea was to assign common codes to similar sounding names. intuitively, names referring to the same person have identical or similar soundex codes. the length of the code is four and it is of the form letter, digit, digit, digit. the first letter of the code is the same as the first letter of the word. for each subsequent consonant of the word, a digit is concatenated at the end of the code. all vowels and duplicate letters are ignored. the letters h, w and y are also ignored. if the code exceeds the maximum length, extra characters are ignored. if the length of the code is less than  <dig>  zeroes are concatenated at the end. the digits assigned to the different letters for english in the original soundex algorithm are shown in table 1: soundex = r163; soundex = r <dig> ; soundex = s <dig> and soundex = s <dig> 

many variations of the basic soundex algorithm, such as changing the code length, assigning a code to the letter of the string or making n-gram substitutions before code assignment have been tested.

for the french language, phonex  <cit>  was developed for french names. we present here some variations of the original phonex algorithm adapted to french medical language, the pronunciation of which is more complex than that of names and bringing together letters according to their type of pronunciation may cause confusion. for example phonex = phonex =  <dig>  whereas pronunciation is very different . the codes of the phonemisation algorithm are in table  <dig> 

the phonemisation function of medical terms that have been developed, allows us to find a word even if it is written with the wrong spelling but with good sound. for example, for the query "kollesterraulle"  phonemisation = phonemisation="kolesterol". we have also constituted manually a list of words that are pronounced "e" in french but ending in "er" or "ed". to encode the terms, changes are made according to the letters that follow or precede groups of letters that have a particular sound. for example, for the word "insomnia" the letters 'in' are replaced by the code '1' giving phonemisation = "1somnia". however, in the word "inosine" we also find the same combination of letters 'in' but, as the next letter "o" is a vowel, no changes in the word are made.

we have also considered that in many cases some letters or even combinations of letters are not pronounced at the end of a word. some combinations are reported in table  <dig> modifications in table  <dig> and some examples in table  <dig>  the algorithm of the phonemisation function  takes as input a single word and as output another string.

in order to compare the sound of two strings, one query and one entry term, all the terms of the dictionary were segmented, lowercased and coded using the function phonemisation. this segmentation is also necessary in cases where for example a user formulates the query "cretzvelt" instead of the descriptor "creutzfeldt-jakob". the function phonemisation was performed on the set of  <dig> queries as a preliminary stage before spell-checking by combining the levenshtein edit distance and the stoilos similarity function. the reference dictionary  was created between  <dig> and  <dig> exclusively on the french version of the mesh thesaurus  <cit>  maintained by the us national library of medicine, completed by numerous synonyms in french collected by the cismef team.

second sample of test queries: multi-word queries
the second set of test queries was constituted to evaluate spell-checking on a larger scale. a set of  <dig>  frequent queries was constituted from the original set of  <dig> . in this set, the queries were composed from  <dig> to  <dig> and more words . to process multi-word queries, we used basic natural language processing steps and the well-known bag-of-words  algorithm:

query segmentation
the query was segmented in words thanks to a list of segmentation characters and string tokenizers. this list is composed of all the non-alphanumerical characters .

character normalizations
we applied two types of character normalization at this stage. mesh terms are in the form of non-accented uppercase characters. nevertheless, the terms used in the cismef terminology are in mixed-case and accented.  lowercase conversion: all the uppercased characters were replaced by their lowercase version; "a" was replaced by "a". this step was necessary because the controlled vocabulary is in lowercase.  deaccenting: all accented characters  were replaced by non-accented  ones. words in the french mesh were not accented, and words in queries were either accented or not, or wrongly accented .

stop words
we eliminated all stop words  in the query. our stop word list was composed  <dig>  elements in french .

exact expression
we use regular expressions to match the exact expression of each word of the query with the terminology. this step allowed us to take into account the complex terms  of the vocabulary and also to avoid some inherent noise generated by the truncations. the query 'accident' is matched with the term 'circulation accident' but not with the terms 'accident' and 'chute accidentelle'. the query 'sida' is matched with the terms 'lymphome lié sida' and 'sida atteinte neurologique' but not with the terms 'glucosidases', 'agrasidae' and 'bêta galactosidase'.

phonemisation
the function is as described in the previous section. it converts a word into its french phonemic transcription: e.g. the query alzaymer is replaced by the reserved term alzheimer.

bag of words
the algorithm searched the greatest set of words in the query corresponding to a reserved term. the query was segmented. the stop words were eliminated. the other words were transformed with the phonemisation function and sorted alphabetically. the different reserved term bags were formed iteratively until there were no possible combinations. the query 'therapy of the breast cancer' gave two reserved words: 'therapeutics' and breast cancer' .

evaluations
to evaluate our method of correcting misspellings, we used the standard measures of evaluation of information retrieval systems, by calculating precision, recall and the f-measure. we performed a manual evaluation to determine these measures. precision  measured the proportion of queries that were properly corrected among those corrected.

  precision=|{queriescorrectlycorrected}||{queriescorrected}| 

recall  measured the proportion of queries that were properly corrected those requiring correction.

  recall=|{queriescorrectlycorrected}||{queriestobecorrected}| 

the f-measure combined the precision and recall by the following equation:

  f-measure=2×precision×recall 

we also calculated confidence intervals at ρ = 5% to avoid evaluating the whole set of queries, but some sets that are manually manageable. for a proportion x and a set of size nx the confidence interval is:

  cix=x- <dig> ×x×nx;x+ <dig> ×x×nx 

RESULTS
choice of thresholds for the first set of queries
the levenshtein and stoilos functions require a choice of thresholds to obtain a manageable number of correction suggestions for the user. we thus tested different thresholds, as shown in tables  <dig>   <dig> and  <dig> and figure  <dig>  for the normalized levenshtein distance, the similarity function of stoilos and for the combination of both. for example, the query "accuponture"  is corrected with levenshtein <  <dig> . at a threshold of  <dig> ,  <dig> suggestions are proposed. the same query is corrected with stoilos >  <dig>  and at a threshold of  <dig> ,  <dig> suggestions are proposed. when combining lev <  <dig>  and stoilos >  <dig>  only one  suggestion is proposed. the query "suette" ) is corrected properly with levenshtein <  <dig>  , stoilos >  <dig>   and with levenshtein <  <dig>  combined with stoilos >  <dig>  . the query "rickttsiose"  is corrected properly with levenshtein <  <dig>  , stoilos >  <dig>   and with levenshtein <  <dig>  combined with stoilos >  <dig>  .

as shown in tables  <dig>   <dig> and  <dig> and figure  <dig>  the number of suggestions provided to the user in order to correct is variable and the task of correcting queries may become overwhelming if the user has to select the correct word from hundreds, even millions . manageable results  are obtained for the following thresholds for  levenshtein <  <dig> ;  stoilos >  <dig>  and  the combination of lenshtein <  <dig>  and stoilos >  <dig> .

evaluation on the first sample of queries
we first tested the method with standard levenshtein with thresholds from  <dig>  to  <dig> . manual evaluation gave from  <dig> queries corrected without any error, to  <dig> queries corrected,  <dig> with false suggestions. precision decreased from  <dig> to  <dig> % and recall increased from  <dig> % to  <dig> %. the best f-measure is obtained for levenshtein <  <dig>  . however, for this threshold, the total number of suggestions is  <dig>  . we tested the method with stoilos function with thresholds from  <dig>  to  <dig> .

manual evaluation gave from  <dig> queries corrected,  <dig> with false suggestions, to  <dig> queries corrected,  <dig> with false suggestions. precision increased from  <dig> % to  <dig> % and recall decreased from  <dig> % to  <dig> . the best f-measure is obtained for stoilos >  <dig> . however, for this threshold the total number of suggestions is  <dig>  . the resulting curves of precision and recall of stoilos and levenshtein according to different thresholds are in figure  <dig> 

we also tested the combination of stoilos along levenshtein. manual evaluations were not performed on all the possible combinations . figure  <dig> and  <dig> contain resulting curves of precision and recall respectively.

p = 100
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p = 100
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p = 100
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
p =  <dig> 
r =  <dig> 
note that the function phonemisation gave a 38% recall, 42% precision and  <dig> % f-measure, which are lower than the methods based on string edit distance or similarity function.

according to all those results  we retained a threshold of  <dig>  for levenshtein edit distance and  <dig>  for stoilos function, when combinated for spelling-correction.

we also measured the time necessary to propose spelling-corrections to information seekers according to the size of the queries, using levenshtein <  <dig>  along with stoilos >  <dig>  and we obtained at min:  <dig>  ms and at max :  <dig>  ms .

evaluation of the second sample of queries
the second set of queries was larger  and composed of queries of  <dig> to  <dig> and more words. in this evaluation, we chose to retain the following thresholds: levenshtein >  <dig>  and stoilos >  <dig> . to determine the impact of the size of the query we measured the number of suggestions of corrected queries . for a user, the maximum number of manageable suggestions for one query was  <dig> 

manual evaluations were performed on sets of ~1/ <dig> of each type of queries. table  <dig> contains all the precison, recall and f-measure values. evaluations of the quality of queries suggestions  were performed manually on several sets, according to the size of the query, but also according to the following methods : bag-of-words, levenshtein distance alongside the stoilos similarity function, but also the bag-of-words processed before and after the combination of the levenshtein distance along with the stoilos similarity function. levenshtein and stoilos remained constant at <  <dig>  and >  <dig>  respectively.

by combining the bag-of-words algorithm along with the levenshtein distance and the similarity function of stoilos, a total of  <dig>   queries matched medical terms or combinations of medical terms. the remaining queries with no suggestions  not belong to the dictionary. for 1-word queries, it remained  <dig> , for 2-words queries it remained  <dig> queries ; for 3-words queries it remained  <dig>  and for  <dig> words queries it remained  <dig>  queries  . for example, the query "nutrithérapie"  contains no error but cannot be matched with any medical term in the mesh thesaurus.

evaluations shown that best results were obtained by performing the bag-of-words algorithm before the combination of levenshtein alongside stoilos. the resulting curves of precision, recall anf f-measure are in figures  <dig>   <dig> and  <dig> respectively.

discussion
several studies have explored the problem of spelling corrections, but the literature is quite sparse in the medical domain, which is a distinct problem, because of the complexity of medical vocabularies. nonetheless, the work of  <cit>  uses word frequency based sorting to improve the ranking of suggestions generated by programs such as gnu gspell and gnu aspell. this method does not detect any misspellings nor generate suggestions but reports that aspell gives better results than gspell. in  <cit>  ruch studied contextual spelling correction to improve the effectiveness of a health information retrieval system. in  <cit>  the authors created a prototype spell checker using umls and wordnet in english sources of knowledge for cleaning reports on adverse events following immunization. we also cite the work of  <cit>  which proposes a program for automatic spelling correction in mammography reports. it is based on edit distances and bi-gram probabilities but is applied to a very specific sub-domain of medicine, and not to queries but to plain text. in  <cit>  the authors use normalization techniques, aggressive reformatting and abbreviation expansion for unrecognized words as well as spelling correction to find the closest drug names within rxnorm for drug name variants found in local drug formularies. the spelling algorithm is that of the rxnorm api which returns only drug name suggestions. the unknown word must have a minimum length of five characters for spelling correction to be tried. however, the effective usage of the spelling correction component was only  <dig> % in the approximate matching of drug names. in addition many spelling corrections were applied to unknown tokens which were not intended to be drugs. the different experiments we performed show that with 38% recall and 42% precision, phonemisation cannot correct all errors : it can only be applied when the query and entry term of the vocabulary have similar pronunciation. however, when there is reversal of characters in the query, it is an error of another type : the sound is not the same and similarity distances such as levenshtein and stoilos can be exploited here. similarly, when using certain characters instead of others , string similarity functions are not efficient. the best results  are obtained with multi-word queries by performing the bag-of-words algorithm first and then the spelling-correction based on similarity measures. due to the relatively small number of correction suggestions , which are manually manageable by a health information seeker, we have chosen to return an alphabetically sorted list rather than ranking them.

CONCLUSIONS
the general idea of spelling correction is based on comparing the query with either dictionaries or controlled vocabularies. if a query does not match the vocabulary, one or more suggestions are proposed to the user. recent research has focused on the development of algorithms in recognizing a misspelled word, even when the word is in the dictionary, and based on the calculation of similarity distances. damerau  <cit>  indicated that 80% of all spelling errors are the result of  transposition of two adjacent letters   insertion of one letter   deletion of one letter  and  replacement of one letter by another . each of these wrong operations costs  <dig> i.e. the distance between the misspelled and the correct word.

in this paper, we present a method to automatically correct misspelled queries submitted to a health search tool that may be used both by patients but also by health professionals such as physicians during their clinical practice. we have described how to adapt the levenshtein and stoilos to calculate similarity in spell-checking medical terms when there is character reversal. we have also presented the combined approach of two similarity functions and defined the best thresholds. our results show that using these distances improves phonetic transcription results. this latter step is not only necessary but is less expensive than calculating distance. the best results  are obtained by performing the bag-of-words algorithm  before the combination of levenshtein and stoilos similarity functions.

the use of keyword configuration, by studying the distances between keys, is another possible direction to suggest spelling corrections. for example, when the user types a "q" instead of an "a" which is located just above on the keyboard, similarly to the work detailed in  <cit>  for correcting german brand names of drugs. these errors are more frequent when queries are submitted by a tablet pc or a smart phone, the keyboard being smaller in size.

this method may also be used to extract medical information from clinical free texts of electronic health records or discharge summaries. indeed, the efforts to recognize medical terms in text have focused on finding disease names in electronic medical records, discharge summaries, clinical guideline descriptions and clinical trial summaries. the survey of meystre et al.  <cit>  describes several studies on detecting information elements in clinical texts using natural language processing and show their impact on clinical practice. these information elements may be diseases  <cit> , treatments  <cit>  in english, or other medical information in french  <cit> . however, as in any free text, clinical notes may contain misspellings. using our method may be a preliminary step to cleaning these notes before coding. the algorithms we have presented in this paper will be integrated into the first work package of the following two research projects, both of which are funded by the french national research agency: the ravel project for information retrieval through patient medical records and the sifado project for helping health professionals to code discharge summaries, which free-text components require manual processing by human encoders.

competing interests
the authors declare that they have no competing interests.

authors' contributions
lfs, epg, tl and sjd formulated the idea of this study. lfs, epg, tl and sjd designed it and participated in writing the draft. zm designed the first part of the method  and zm and sjd evaluated it. lfs designed the second part of the method and evaluated with sjd. all authors read and approved the final manuscript.

