BACKGROUND
identifying protein-protein interactions  is necessary for constructing protein interaction networks and increases the understanding of both the functional role of individual proteins and the underlying biological processes. although numerous ppis have been manually curated by biomedical curators into databases, such as dip, bind, mint, intact, and hprd, many valuable ppis remain available in research articles. since the manual curation of relevant ppis from millions of research articles is too time-consuming, methods of automatic ppi extraction from articles are necessary.

many automatic ppi extraction methods from articles have been developed. the first, simplest kind of method, co-occurrence, which classifies two proteins as interacting if they exist in the same sentence or in the same abstract, yields high recall but low precision. conversely, the second kind of method, pattern- or rule-based, which often utilizes handcrafted patterns or rules, achieves high precision but low recall. the third kind of method, machine learning-based, can be divided into two types: feature-based and kernel-based.

feature-based methods represent an instance, which is a protein pair that consists of two protein names in a sentence, by many features, e.g., lexical, word context, and syntactic features derived from the sentence or its syntactic structure. besides lexical and syntactic features, liu et al.  <cit>  exploited various features from dependency information, including predicate features involved in dependency graphs. landeghem et al.  <cit>  proposed rich feature vectors containing semantic information from dependency graphs and lexical information from sentences. they also first applied automatic feature selection techniques and showed that these techniques can enhance the generalization performance in ppi extraction and make the models faster and more cost-effective.

recently, various kernel-based methods have been proposed that provide kernel functions that measure the similarity between any pair of instances represented by structural representations, such as constituent parse trees or dependency graphs. these kernel functions differ from each other in their type of input representation and how they compute similarity functions. airola et al.  <cit>  proposed a graph kernel-based method in which a sentence is represented by a combination of a dependency graph and another graph showing the linear order of words and considered all the possible paths connecting two entities in the dependency graph. miwa et al.  <cit>  proposed a method that combines kernels based on several syntactic parsers to retrieve the widest possible range of important information from a given sentence. their method, which combines a bag-of-words kernel, a subset tree kernel, and a graph kernel, assigns the same weight to each individual kernel. qian et al.  <cit>  proposed a tree kernel-based method by exploiting both constituent parse trees and dependency graphs and further refined the tree representation from a constituent parse tree by utilizing the shortest dependency path between two proteins in a dependency graph.

feature-based methods are considered more appropriate to practical applications than kernel-based methods due to the computation complexity of sophisticated kernels  <cit> . moreover, feature-based methods can be improved by applying feature selection to enhance the generalization performance and attain faster and more cost-effective models  <cit> . the latest study of tikk et al.  <cit>  analyzed and compared the performances of diverse kinds of kernels. they found that different kernels using the same input representation perform similarly on a large number of protein pairs, which are identified as misclassified by most state-of-the-art kernel-based methods. based on their convincing experimental results, to improve the ppi extraction performance, they argued that we should concentrate on finding and choosing informative features suitably rather than devising novel similarity functions encoded in kernels.

in this paper we propose a novel feature-based method to extract ppis from articles. we exploit various features, including lexical features and word context features obtained directly from sentences, syntactic features obtained from parse trees, and features using existing patterns. we arrange the related features into four groups. for instance, we assemble two related features, which represent the positions of two protein names that constitute an instance in the sentence containing them, into one group. we also define the contribution level  of each group, which consists of related features. our method differs from existing methods in two ways. first, based on the structure of the sentence and the presence of significant keywords , we divide the training set into subsets and apply the sentence patterns provided beforehand to each one. second, after computing the cl values of the four groups that consist of related features, we automatically implement feature selection based on their cls and the k-nearest neighbor algorithm  through three approaches:  focusing on the group with the best contribution level ;  unoptimized combination of three groups with the best contribution levels ;  optimized combination of two groups with the best contribution levels . to the best of our knowledge, this is the first method that automatically selects the most appropriate features for each group consisting of related features by combining their cls with k-nn.

lexical features extracted from sentences

outline of features of protein pairs in sentences used in extraction of protein-protein interaction information
the automatic extraction of ppi information from articles is regarded as a binary classification in which instances  that include ppi and instances that do not include ppi are classified as positive and negative instances. in machine learning approaches, based on labeled training data, extracted features, which are the characteristics of sentences containing protein pairs, are utilized to discriminate positive from negative instances. then a model is learned from the training data, and each instance is classified as positive or negative.
fig.  <dig> example of a constituent parse tree. constituent parse tree for sentence, “oxytocin stimulates ip <dig> production in dose-dependent fashion as well,” from sentence iepa.d <dig> s <dig> of iepa corpus 



our features are categorized into four types: lexical features obtained directly from the sentence, word context features obtained directly from the sentence, syntactic features obtained from the parse tree, and features that use the existing patterns. in the following tables, p <dig>  p <dig>  and k denote the protein name that appears first, the protein name that appears later, and the keyword in a sentence, respectively. 
lexical features obtained directly from sentences: these features are outlined in table  <dig> 

word context features obtained directly from sentences: these features are outlined in table  <dig> 
1
x
2’: x
 <dig> is ‘t’ if a comma exists between a and b, and x
 <dig> is ‘t’ if a comma exists between b and c, otherwise x
 <dig> or x
word context features extracted from sentences. p <dig>  p <dig>  and k denote the protein name appearing first, the protein name appearing later, and the keyword in a sentence, respectively. ‘t’ and ‘f’ are abbreviations of ‘true’ and ‘false’



syntactic features obtained from parse trees: all sentences are transformed into representations called constituent parse trees, which can capture the syntactic structures of sentences. the features obtained from the constituent parse tree are outlined in table  <dig> 
all sentences were transformed into representations called constituent parse trees, output from the stanford parser  <cit> . syntactic features were extracted from constituent parse trees. p <dig>  p <dig>  and k denote the protein name appearing first, the protein name appearing later, and the keyword in a sentence, respectively



an example of a constituent parse tree output from the stanford parser  <cit>  is shown in fig.  <dig> 

features using existing patterns: we prepared thirteen syntax patterns related to the presence or absence of ppi in table  <dig> based on the syntax patterns that were proposed by plake et al.  <cit> . inoun and iverb, which represent the sets of nouns and verbs related to interaction, are improved from the original ones used by plake et al.  <cit> . the wildcard ‘*’ represents any word or words in a pattern. the number of words substituted by a wildcard in a pattern is limited to five. if an instance ) matches  one of these patterns, the feature value is set as ‘true’ .
p1
∗
iverb
∗
p2
p1
∗
iverb
∗ by ∗
p2
iverb of ∗
p1
∗ by ∗
p2
iverb of ∗
p1
∗ to ∗
p2
inoun of ∗
p1
∗  ∗
p2
inoun of ∗
p1
∗  ∗
p2
inoun between ∗
p1
∗ and ∗
p2
p1
∗ and ∗
p2
p1
∗ and ∗
p2
p1
∗ form ∗ complex with ∗
iverb
∗
p2
p1
∗
p2
∗
inoun
p <dig> depend of p2
we prepared syntax patterns related to ppi based on the syntax patterns proposed by plake et al.  <cit> . p <dig> and p <dig> denote the protein names appearing first and later in a sentence, respectively. inoun and iverb denote sets of nouns and verbs related to interaction. the number of words substituted by a wildcard ‘ ∗’ in a pattern is limited to five. after the training set was divided into subsets based on the existence of significant keywords and the structure of the sentence, these syntax patterns were applied to each subset





methodfig.  <dig> framework of our ppi extraction system. our system consists of two phases. first, training set is divided into subsets based on presence of significant keywords and the feature position of keyword. second, after cross-validation is performed on the training data to assess the contribution levels of four groups, which consist of related features, feature selection is performed automatically through our three approaches . finally, the k-nn classifier is used to classify candidate ppi pairs of test data



division of the training set into subsets
significant keyword 
since the feature keyword  performs a noticeable role in identifying whether the sentence contains a ppi, it is utilized in the majority of research related to ppi extraction  <cit> . nevertheless, emphasizing only this feature can cause an adverse effect in which other features may not receive appropriate attention or might even be ignored. consequently, we distinguish between cases when the feature keyword contributes significantly to ppi classification and when it does not. we call the keyword in the former case sk.

through the observation of the imbalance of the classes of instances when classifying them based on the presence or absence of a certain keywordk, we determine whether this feature keywordk is sk by defining imbalance degree id as follows: 
  <dig> id=np/nn, 

where np and nn denote the number of positive and negative instances containing k in the training set, respectively. id= <dig> means that k is completely balanced. on the contrary, id= <dig> =∞) or id=∞ =0) means that k is completely imbalanced. thus, when the value of id or 1id is less than predefined threshold t, we regard k as a sk.

position of keyword
in addition to the importance whether a keyword is sk, since the basic structure of a sentence can be grasped by determining the feature position of keyword that signifies the word order of the keyword and the pair of protein names in that sentence, this feature should also be stressed. we recognize that if the basic structures of the sentences differ, the features that should be emphasized in ppi classification will also differ. if the feature position of keyword’s value is ‘infix’ , the sentence structure has the typical subject-verb-object form. for example, in the sentence, “gere binds to a site on one of these promoters, cotx, that overlaps its - <dig> region,” , since the relation of a protein name and the keyword is that of the subject  and the verb, some features  perform substantial roles. conversely, if the feature position of keyword’s value is ‘prefix’ or ‘postfix’, the sentence structure is regarded as atypical, such as the parallel expression of protein names, inverted structure, and so forth. for example, in the sentence, “association between cdc25a and cdc <dig> was detected in the hela cells,” , since the relation of a protein name and the keyword, which is not that of the subject  and the verb, is that of inverted structure, some features  are not always emphasized. therefore, the importance of a feature is likely to be varied depending on the structure of the sentence.

division of training set
because the features that should be emphasized can vary depending on whether the feature keyword is a sk and the feature position of keyword between the two protein names as stated above, we divide the training set into three subsets, a, b, and c , and generate three classifiers from each subset. similarly, we divide the unlabeled instances into one of three subsets, a’, b’, and c’, and utilize the corresponding classifier to identify whether ppis exist in these instances. the outline of this process is illustrated in fig.  <dig> 
fig.  <dig> outline of ppi prediction based on division of training set. training set was divided into subsets, a, b, and c, based on existence of significant keyword and feature position of keyword. three classifiers were generated from every subset. similarly, unlabeled instances were divided into one of three subsets, a’, b’, and c’, and corresponding classifier was used to identify whether ppis exist in these instances

the training set was divided into subsets, a, b, and c, based on presence of the significant keyword and the feature position of keyword




because the original training set is divided into three subsets, not all the patterns  always match each subset. consequently, we discard the irrelevant and useless patterns beforehand based on the structure of the sentence . for subset a, since the feature position of keyword’s value is ‘infix’ , we observed that patterns  <dig>   <dig>   <dig>  and  <dig>  do not match this subject-verb-object form. patterns  <dig>   <dig>  and  <dig> stand for phrases k- p1- p <dig> ). similarly, pattern  <dig> is also unsuitable for subset a. for subset b, because the feature position of keyword’s value is ‘prefix’ or ‘postfix’ , patterns  <dig>   <dig>   <dig>  and  <dig> do not match these sentence structures. as a result, we remove them beforehand for subsets a and b. for subset c, because the position of keyword’s value is ‘infix’, ‘prefix’, or ‘postfix’, we do not eliminate any pattern shown in table  <dig> 
after the training set was divided into subsets a, b, and c, the syntax patterns  we prepared were checked to determine whether they matched each subset. unsuitable patterns were removed beforehand for subsets a and b. no pattern was excluded for subset c



prediction based on evaluating contribution levels of groups consisting of related features
groups consisting of related features
the syntactic features derived from the parsers, features showing the distances between two protein names and the keyword, and features showing the positions of two protein names are considered important in ppi classification  <cit> . however, among some related features , we cannot affirm intuitively which feature is definitely more important than the others without understanding the data characteristics. therefore, we arranged the related features into the following four groups to automatically evaluate each one separately in the ppi extraction performance by the cl defined hereinafter: 
group g <dig> consists of three related features: distance_kp <dig>  distance_kp <dig>  and distance_p1p <dig> .

group g <dig> consists of two related features: position_p <dig> and position_p <dig> .

group g <dig> consists of three related features: height_p <dig>  height_p <dig>  and height_k .

group g <dig> consists of three related features: pos_p <dig>  pos_p <dig>  and pos_k .



when some features in a group, which consists of related features, are useless and redundant, using all the features in that group can decrease the accuracy of the learning algorithm. in this case, removing some irrelevant features from that group  can improve the ppi extraction performance. conversely, when all the features in a group are useful, it is unnecessary to remove any feature from that group. by utilizing the original training data, we can determine automatically whether it is necessary to eliminate any feature in any group individually to enhance the ppi extraction performance.

after individually selecting the optimal feature set for each group, we assess the cls of these groups in the process of training the classifier with the original training data. when some groups greatly contribute to the classifier training, we had better put more focus on these groups. in the opposite case, it is important to give ample consideration to other groups.

cross-validation
cross-validation , which is a widespread strategy to estimate the model prediction performance, can also be utilized in feature selection to determine which subsets of features are useful in building good predictive models. in this paper we perform s-fold cv  on original training data trainall to identify the redundant features in groups g <dig>  g <dig>  g <dig>  and g <dig>  which consist of related features, compute the cls of these groups, and perform feature selection. in sfcv, original training data trainall is divided into s equal-sized partitions pi. in each roundi of sfcv , a combination of s− <dig> partitions is used as training set traini, defined as trainall−pi , to train the predictive model that is then validated on the remaining partition, called validation set validationi. we discuss it in detail below.
fig.  <dig> 
s-fold cross-validation  performed on original training data. original training data t
r
a
i
n
all was divided into s equal-sized partitions p
i to perform sfcv on it to estimate contribution levels of four groups, g
 <dig>  g
 <dig>  g
 <dig>  and g
 <dig>  and perform feature selection



contribution level  of a group consisting of related features
to describe our method more simply from now, we also utilize the abbreviation sfcv, the notations of original training data trainall, partitions pi, training set traini, and validation set validationi, which were defined in the cross-validation section.

because group g <dig> contains three related features , there are eight ways  to select features from these features to create eight combinations of features for g <dig> 

we performed sfcv to train the k-nn classifier on original training data trainall. for each training set traini, we trained the k-nn classifier with every combination of the features in g <dig> . then for each traini, we search for the optimal combination of features in g <dig> that yields the maximum f-score on validation set validationi , denoted as fcon1i . similarly, for each traini, we also look for the optimal combinations of features in groups g <dig>  g <dig>  and g <dig> separately that yield the maximum f-score, denoted as fcon2i, fcon3i, and fcon4i, on validationi, respectively.

in this paper we define the cl for each group consisting of related features to improve the ppi extraction accuracy. cl is an indicator for determining the efficiency in the selection of the optimal combination of the features for each group. the pseudo-code for the calculation of the cls of these four groups is shown in listing ??. cj denotes the cl of each group gj . function max returns the maximum value among its arguments. for all values of i from  <dig> to s− <dig>  if the maximum among the values of fconji  is one of group gt, i.e., fconti , we increase cl ct of gt.

from the cl values of the four groups output from function evalcon <dig>  if groups gx, gy, and gz still exist , which have the same cl values, we only recompute the cls for gx, gy, and gz by function evalcon <dig>  without recomputing the cl for the remaining group. after computing the cls clx, cly, and clz of gx, gy, and gz , which resembles the calculation of the cls of function evalcon <dig>  we resolve the exception  in which the cls of gx, gy, and gz become equal again. in this exception, we compute ax, ay, and az . aj  of group gj denotes the maximum among fconj <dig>  fconj <dig> …, and fconj. we regard the cls of gx, gy, and gz as identical as ax, ay, and az, respectively. if all three groups, gx, gy, and gz, still have the same cls , we regard the cls of gx, gy, and gz as identical as ax′, ay′, and az′, respectively. in lines 18– <dig>  function second_max returns the second maximum among its arguments.

from the cls of the four groups output from function evalcon <dig>  if only two groups remain with identical cls, we only recompute the cls for these groups by function evalcon <dig> without recomputing the cls for the remaining groups. because function evalcon <dig> resembles function evalcon <dig>  we do not present its code here.





feature selection through three approaches 
the ultimate goal of automatic ppi extraction from articles is extracting ppis from any new unseen text with high predictive accuracy. in addition to ppi extraction accuracy, reducing the computational time for training and testing a ppi extraction system is also crucial. tikk et al.  <cit>  compared the performances of diverse kinds of kernels on a large-scale database called medline, which contains nearly 120-m sentences. they reported that when the top three kernels  that show the best performance were applied to medline on a single processor, their runtimes were about  <dig>   <dig>  and  <dig> days, respectively. including the time to parse sentences, their runtimes changed to  <dig>   <dig>  and  <dig> days, respectively. in other words, to extract ppi on medline, we need about half a year. consequently, lowering the computational time becomes critical for ppi extraction tasks. landeghem et al.  <cit>  also argued that we have to consider a trade-off between ppi extraction accuracy and computational time to decrease the amount of computational resources utilized by machine learning.

based on the cl values of four groups, g <dig>  g <dig>  g <dig>  and g <dig>  we employ the following three approaches: focusing on the group with the best contribution level , unoptimized combination of three groups with the best contribution levels , and optimized combination of two groups with the best contribution levels  to automatically perform feature selection to enhance ppi extraction accuracy. for the above reason, we also take into account reducing the computational time by feature selection. we briefly describe the advantages of our three approaches below.

best1g: we only performed feature selection automatically on the group with the best cl among the four groups. although we also aim at improving ppi extraction accuracy, our concern with this approach is mainly about limiting the computational time by feature selection as much as possible. therefore, compared with u3g and o2g, the advantage of best1g is the least computational time due to feature selection. however, since best1g is not guaranteed to always attain better extraction accuracy than u3g and o2g, it is suitable for a large-sized dataset in which effectively decreasing the computational time is always the most required factor.

u3g: we merged all the features in the three groups with the best cls among the four groups. then we automatically performed feature selection on these merged features from these three groups by a greedy algorithm. although this greedy strategy generally does not produce an optimal subset of these features, it may yield a locally optimal subset of them. we want to exploit the information from the features in these three groups to improve the extraction accuracy without wasting time performing feature selection by exhaustively searching for all their combinations. hence, compared with best1g and o2g, the advantage of u3g is maintaining a trade-off between extraction accuracy and computational time due to feature selection. however, u3g is also not guaranteed to always achieve better extraction accuracy than best1g and o2g. as a result, u3g is suitable for a medium-sized dataset in which balancing extraction accuracy and computational time is necessary.

o2g: we merged all the features in the two groups with the best cls among the four groups. then we automatically performed feature selection on these merged features from these two groups by exhaustively searching for all the combinations of these features. consequently, we assume that o2g performs better with respect to extraction accuracy than best1g and u3g. the disadvantage of o2g is higher computational times due to feature selection than best1g and u3g when applying o2g, best1g, and u3g to a medium-sized dataset or a large-sized dataset. as a result, o2g is suitable for a small-sized dataset in which effectively increasing the extraction accuracy is always the most required factor.

we describe the implementation details of best1g, u3g, and o2g below.

best1g
first, we selected group gj having the best cl among the four groups. for all the values of i from  <dig> to s− <dig>  we then searched for the maximum of fconji and its corresponding combination of features in gj when applying sfcv and k-nn to training set traini.

u3g
first, we selected three groups having the best cls among the four groups and merged all the features in them. assume that the order of cls ca, cb, and cc of the three corresponding selected groups ga, gb, and gc is ca>cb>cc. for each training set traini, we performed k-nn by gradually removing the features in these three groups in the order of their cls , and if the f-score, denoted as fi, on validation set validationi improved even slightly, we conclude immediately that this improved value of fi, denoted as fui, is the unoptimized value for traini.

next, for all the values of i from  <dig> to s− <dig>  we searched for the maximum of fui and its corresponding combination of features in these three groups.

o2g
first, we selected two groups having the best cls among the four groups and merged all the features in them. for each training set traini, we performed k-nn with every combination of the features in these two merged groups. then, for each training set traini, we searched for the optimal combination of the features in these two merged groups that yields the maximum f-score, denoted as foi, on validation set validationi.

next, for all the values of i from  <dig> to s− <dig>  we searched for the maximum of foi and its corresponding combination of features in these two merged groups.

RESULTS
datasets
we utilized all the datasets from four typical ppi-annotated corpora: lll  <cit> , hprd <dig>  <cit> , iepa  <cit> , and aimed  <cit>  . table  <dig> shows the number of positive and negative instances in them. in addition to the  <dig> pubmed abstracts in aimed that were manually annotated for interactions between human genes and proteins,  <dig> other abstracts without ppis were added to aimed as negative instances. hprd <dig> is comprised of  <dig> abstracts, in which the human gene and protein names were automatically identified by prominer software. iepa was created from  <dig> pubmed abstracts, each of which contains a specific pair of co-occurring chemicals. the lll corpus contains  <dig> sentences and was the shared dataset for the learning language in logic  <dig> challenge . the lll domain is the gene interactions of bacillus subtilis. these corpora carry information about named entities from biological domains and annotated ppis. nevertheless, they differ from one another. for example, with respect to the scope of the annotated entities, most of them typically contain proteins and genes, some contain rnas, but iepa contains only chemicals. the policies of entity annotation and interaction annotation among these copora are also slightly different  <cit> . pyysalo et al.  <cit>  reported that although the entity annotation of the types relevant to the corpus is exhaustive only in aimed and bioinfer, entity annotation is merely based on lists of entity names or the named entity recognizer output in other corpora. they also indicated that the differences in interaction annotation are even greater than those in entity annotation, e.g., only bioinfer and iepa contain information identifying the words that state an interaction, and all but hprd <dig> specify the direction of the interactions. they converted these corpora into a unified xml format, which we utilized in our study, with a very simple structure to make the corpora easily accessible to users. in the unified xml format, each corpus is comprised of documents that are abstracts of articles, each document is comprised of sentences, each sentence might contain some proteins, and the names and relations of proteins are annotated by some attributes of this xml format.
four corpora, lll, hprd <dig>  iepa, and aimed, were converted into a unified xml format with a very simple structure by pyysalo et al.  <cit>  to make the corpora easily accessible to users. number of positive instances  and negative instances  in each corpus is shown



we regard the ppi extraction task as a binary classification in which interacting protein pairs are considered positive instances and vice versa. if a sentence contains n proteins, n <dig> instances  are generated. in this paper, the two protein names of a candidate ppi instance and the other proteins in the same sentence are renamed as p <dig>  p <dig>  and p <dig> to blind the learner to allow direct comparison to earlier studies. by utilizing the information contained in the attributes of the unified xml format of these corpora, we parsed all the sentences in them to extract the features.

bioinfer has an especially extensive annotation policy that combines three types of annotations: termed entity, entity relationship, and dependency. some sentences of this corpus were annotated with protein names that do not contain contiguous tokens. for instance, in the phrase, “when liver- or islet-type glucokinase was transiently expressed in cos- <dig> cells” from sentence bioinfer.d <dig> s <dig>  two protein names are annotated, “liver-type glucokinase” and “islet-type glucokinase,” and “liver-type glucokinase” is annotated as a protein reference despite its discontinuous name. in this paper, the two protein names of a candidate ppi instance and the other proteins in the same sentence are assumed to contain only contiguous tokens. although we currently cannot tackle these non-contiguous protein names of the bioinfer corpus, which is why we do not analyze the dataset of this corpus, we intend to deal with this problem in the future.

evaluation methods
to allow for direct comparison to earlier studies, we evaluated the performance of our method by 10-fold document-level cross-validation . in each round of 10fdlcv, one of the ten partitions containing  <dig> % of the documents is used as a test set, and the combination of the remaining nine partitions containing  <dig> % of the documents is used as a training set. sfcv, which is described in the method section, is performed as 9fdlcv . we also adopted the one answer per occurrence strategy in which a correct interaction has to be extracted for each occurrence of the instance. the threshold t’s value  is set to  <dig> .

for classifying ppi, we use the k-nn algorithm mentioned above by normalizing the value of each feature before computing the euclidean distance between two feature vectors. the values of the features are normalized so that they all lie between  <dig> and  <dig> and the features on different scales have the same impact on the distance function. if two categorical features are identical , we consider the difference between them as  <dig> .

for small-sized corpora, lll, hprd <dig>  and iepa, based on the rule of thumb in machine learning that chooses parameter k of k-nn to be near the square root of the size of the training set, we chose k to be odd and equal to  <dig>   <dig>  and  <dig> . for a larger aimed corpus containing the most highly imbalanced ppi data, if the rule of thumb is applied, k is too big, about 71– <dig>  and the computation cost  also increases greatly with this value of k. therefore, for the aimed corpus, we performed cross-validation on the training data with a range of value of k, and chose k that equals  <dig> based on the lowest root mean square error.

precision , recall , and harmonic value f-score  are used as evaluation measures, which are defined as follows:

  <dig> p=tp/ 

  <dig> r=tp/ 

  <dig> f=2∗p∗r/, 

tp, fp, and fn denote the number of true positives, false positives, and false negatives, respectively. precision is the percentage of correct predictions from all the instances predicted as positive. recall is the percentage of correctly predicted positive instances from all positive instances.

experiment resultstable  <dig> experiment results of our three approaches: best1g, u3g, o2g


 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
precision , recall , f-score  results of our three approaches  evaluated by 10-fold document-level cross-validation on four corpora, lll, hprd <dig>  iepa, and aimed, shown in the second, third, and fourth row. as a baseline, in the first row, we add results when only k-nn is applied, and feature selection using contribution levels of groups consisting of related features was not performed. precision , recall , and f-score  values are shown by percentage . bold typeface shows best results per corpus in terms of precision, recall, and f-score



compared with best1g, although u3g attains a better result on iepa or an equivalent result on hprd <dig>  best1g outperforms u3g on lll and aimed in terms of f-score. best1g learned all the possible combinations of the features in the group with the best cl, whereas there are cases where u3g can immediately halt the feature selection completely when only a feature in the group with the best cl is removed and the f-score on the validation set is only slightly improved. therefore, the best1g results are generally better than those of u3g.

conversely, since o2g exhaustively searches for all the combinations of the features in the two merged groups having the best cls, o2g exceeds best1g on hprd <dig> and iepa and greatly surpasses u3g on lll and hprd <dig>  generally, despite the highest computational time due to feature selection when applied to the medium-sized iepa corpus or the large-sized aimed corpus, o2g performs the best among our three approaches, as we assumed in the method section.

however, depending on the characteristics of the corpus, the best performance belongs to best1g, u3g, and o2g on aimed, iepa, and hprd <dig>  respectively, in terms of f-score. both best1g and o2g performed best on lll. o2g failed to attain the best f-scores on iepa and aimed, as we assumed in the method section, because the heterogeneity among corpora can lead to heterogeneous evaluation results. for example, unlike other corpora,  <dig> abstracts without ppis were intentionally added to aimed as negative instances. as a result, the percentage of sentences that have no entities is  <dig> % in aimed, but it is  <dig> % in the other corpora. the percentage of sentences that have no interactions is  <dig> % in aimed, but it is less than or equal to  <dig> % in the other corpora. the ratio between the number of positive instances and all the instances in the highly imbalanced aimed is too low, about only  <dig>  % compared with the other corpora. additionally, unlike other corpora, the scope of the annotated entities in iepa is chemicals. therefore, due to the enormous differences among the corpora, in a few cases, o2g may not be the best approach, even though overall it is superior to best1g and u3g with respect to extraction accuracy.

moreover, the f-score results of o2g on iepa and aimed still surpass those when only k-nn is applied. as described in the method section, with a large-sized corpus like aimed in which effectively decreasing the computational time is always the most required factor, we should apply best1g because it has the lowest computational time. with a medium-sized corpus like iepa in which we need a trade-off between extraction accuracy and computational time, we should utilize u3g. with small-sized copora like lll and hprd <dig> in which effectively increasing extraction accuracy is always the most necessary factor, o2g remains the best choice.

comparison with other related research
the performance comparison of our three approaches  with other related research is shown in table  <dig>  the results of the co-occurrence and rule-based methods are also listed in table  <dig> as a baseline. fundel et al., who proposed the relex system  <cit> , applied a small number of simple rules to dependency parse trees to extract ppi. kabiljo et al., who proposed the akaneppi system  <cit> , utilized support vector machines with tree kernels to extract rules for ppi extraction by a combination of deep syntactic parser enju and a shallow dependency parser. our approaches  are mostly far better than these co-occurrence and rule-based methods on all the corpora in terms of f-score.
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
 <dig> 
performance comparison of our three approaches  with other related research on four corpora: lll, hprd <dig>  iepa, and aimed. co-occurrence and rule-based methods results are also listed as a baseline. precision , recall , and f-score  values are shown by percentage . bold typeface shows best results of feature-based and kernel-based methods per corpus in terms of precision, recall, and f-score



compared with the feature-based methods, best1g, u3g, and o2g outperformed their performances on lll, hprd <dig>  and iepa .

compared with the kernel-based methods, best1g, u3g, and o2g achieved f-score results on par with two of these methods  on lll and with three of these methods  on iepa. in terms of precision, our three approaches outperformed two of the kernel-based methods  on lll and two of the kernel-based methods  on iepa. each of the methods has its own disadvantages and advantages. therefore, no method is almighty and powerful for all the corpora. for each kernel-method, there exists a corpus for which that particular method is the best compared with other methods , and there exist corpora for which that particular method is not suitable . in the method by tikk et al., there is no corpus showing remarkably high result of f-score compared with our approaches and other methods.

especially in case of the hprd <dig> corpus, our approaches greatly surpassed the feature-based methods as well as the kernel-based ones. in terms of f-scores on hprd <dig>  o2g exceeds the best results of the feature-based methods and the kernel-based ones  by  <dig>  % and  <dig>  %, respectively. in that respect, our approaches have a distinct unique value compared with other related research.

conversely, in case of the aimed corpus, although our approaches performed fairly better than the tikk et al. and yakushiji et al.  <cit>  methods or on par with the method by landeghem et al.  <cit> , our approaches are not better than other feature-based and kernel-based methods in terms of f-scores. however, a direct comparison among different systems on aimed is not straightforward due to the difference in data preprocessing, remarkably different interpretations related to the number of interacting or non-interacting pairs in the aimed corpus  <cit> , learning methods, parameter tuning, and different evaluation methods  <cit> . for instance, liu et al.  <cit>  identified two more interacting and  <dig> fewer non-interacting protein pairs than our study in aimed. similarly,  <dig> fewer non-interacting protein pairs were reported in the work of landeghem et al. moreover, since the self-interactions  of the aimed corpus are not regarded as ppi candidates and were removed from the corpus prior to evaluation in the methods proposed by liu et al., airola et al., miwa et al., and qian et al., the f-score results of these systems would be higher than our method in which we did not discard self-interactions in the preprocessing step. these differences could boost the performance of these studies. further, we did not utilize dependency information, like the methods proposed by liu et al., landeghem et al., airola et al., miwa et al., qian et al., and tikk et al., or the deep syntactic information , like the method proposed by miwa et al., which are derived from a dependency parser or a deep parser, respectively, and can increase the accuracy of predictive models  <cit> .

CONCLUSIONS
in this paper we propose a novel method for automatic ppi extraction from research articles. our method automatically implements feature selection based on evaluating the cls of the groups that consist of related features to enhance the extraction performance. our three approaches  attain comparable performance in all the corpora and achieve better results in the hprd <dig> corpus than the other previous research. in addition, our three approaches always gain better f-scores in all the corpora than when only using k-nn without utilizing the cls of the groups that consist of related features.

in realistic ppi datasets, there are far fewer interacting protein pairs than non-interacting ones. the imbalance of the ppi data can compromise the process of learning. dealing with the imbalance of ppi data is highly challenging  <cit> . for future work, we plan to design a more efficient method to resolve this problem. moreover, we intend to design an ensemble that is comprised of dissimilar kernels, devise more effective representations of instances, and explore novel useful features to improve the ppi extraction performance. we also plan to create a predictive model for ppi extraction based on deep learning techniques by combining with kernels to extract ppis from such large text collections as medline.

