BACKGROUND
the nucleotide substitution rate matrix, conventionally denoted q, holds the rate of change from each type of nucleotide to each other nucleotide in the markov model of molecular evolution  <cit>  . q is often treated as a nuisance parameter in phylogenetic reconstruction, and is often assumed to be constant over a phylogenetic tree. however, if q were really constant, all genomes would have identical composition, assuming that sufficient time had passed for the process to reach equilibrium. neutrally-evolving sites reach equilibrium rapidly . however, genomes range in gc content  from about 25% to 75%  <cit> , and members of all three domains of life have a wide range of gc content  <cit> . consequently, discovering how q varies is key to building better phylogenies and to understanding fundamental processes of molecular evolution. however, there are many methods for providing an estimate of q, i.e. q^, that may differ substantially in accuracy and performance, and lack of information about these performance characteristics has hindered discovery of patterns of q on a large scale within and between genomes. several factors are known to influence the proportions of the four nucleotides in the genome , and hence presumably q, under specific conditions. for example, in the spirochete borrelia burgdorferi, the strand-specific compositional biases induced by deamination are so great that the strand on which each gene is coded can be determined simply by examining its composition  <cit> . other examples include the influence of cpg doublets on mammalian gene composition  <cit> , deamination biases in mitochondria  <cit> , and the origin of mammalian isochores through variation in mutation patterns  <cit> . within vertebrate genomes, the balance between oxidation and deamination mutations can also differ radically  <cit> . however, despite the potential for relating changes in q to specific mutational processes, little attention has been paid to obtaining empirical q^ in specific lineages  <cit> .

that q does vary, and that variations in q can tell us something interesting about the underlying processes in molecular evolution, has thus drawn substantial attention only in the last 10– <dig> years. most interest in obtaining q^ has come from studies of phylogenetic reconstruction: for example, likelihood methods for tree-building typically involve optimizing a single q^ matrix for a phylogenetic tree  <cit> , and testing for the symmetries in q^ that best explain the sequence data  <cit>  is a standard part of modern phylogenetic inference workflows  <cit> . although some work has been done on non-stationary models  <cit> , and recent evidence suggests that non-stationarity may have a large influence on the accuracy of phylogenetic reconstruction  <cit> , no comparison of the different methods for obtaining q^ has yet been performed. available methods for obtaining q^ are based on different underlying assumptions, and are hence expected to differ both in accuracy and in speed of calculation. most available methods obtain p^, where the entries of p^ are the estimated probabilities of changing from each base to each other base after a defined time interval of duration t. q^ is related to p^ by the equation p^=eq^t. therefore, q^t can be obtained from p^ in two ways: either by simply taking the matrix logarithm of p^, or by using constrained optimization to find a valid q^t without negative off-diagonal elements that, when exponentiated, produces p^ with minimal error. the time factor can be removed by scaling q^t so that the trace is , leaving q^ on a standard scale . for equilibrium processes, the weighted trace can be scaled such that −∑ipiiq^ii= <dig>  taking into account the equilibrium base frequencies  <cit>  to standardize to a fixed number of substitutions per site, but for nonstationary processes this procedure is not justified. the simplest method for obtaining p^ is to count the frequencies that a base i ∈ {a, c, g, t} in one sequence is aligned with base j ∈ {a, c, g, t} in the other sequence, and denote this by nij. to isolate the effects of mutation, regions of the sequence that are thought to be under no selection or very weak selection, e.g. introns or 4-fold degenerate synonymous sites, are used for this purpose. if the substitution process is time-reversible, nij = nji in expectation. it suffices to consider the special case where the observed counts n is symmetric. if n is not symmetric, we replace n by the average of n and the transpose n' <cit> . then the entries of p^ are given by p^ij=nij∑knik, which can then be used to obtain q^ as described above  <cit> . alternatively, since this method of obtaining p^ assumes the divergence between sequences is small, p^~i+q^t. then an estimate for the rate of change of i to j is given by ij=nij∑knik for i ≠ j, and the diagonal elements are set such that each row of q^t sums to  <dig>  <cit> . we refer to this method as the 'goldman' method  <cit> . again the time factor can be removed by scaling q^t. however, both of these methods are based on the assumption that the evolutionary process is time-reversible. a method that can better recapture non-time-reversible substitution processes uses rooted triples of sequences where x and y are the sister taxa, and z is the outgroup  <cit> . if x has the same base at that position as z, the assumption is that the change most likely occurred between y and its common ancestor with x. this method thus leads to a directional and not necessarily time-reversible p^, which can then be used to obtain q^ . we refer to this three-sequence method as the 'gojobori' method of otaining p^ <cit> , and use the implementation in the pycogent toolkit  <cit> .

more elaborate methods include maximum likelihood methods and markov triple analysis  <cit> . each of these methods assume nucleotide sites are independent and identically distributed, and do not require the evolutionary process to be stationary or reversible . to explain the methods, we introduce the following notation. consider a rooted triple of sequences from taxa x, y, z. define r to be the root and let v^r denote the estimated vector of nucleotide probabilities for the sequence at r. let p^u denote the estimated nucleotide probability transition matrix from r to taxa u, for u ∈ {x, y, z}, i.e.,

 p^u=pr,fori,j∈{a,c,g,t},u∈{x,y,z}.   

the transition probabilities in each row of p^u sum to  <dig>  therefore each matrix p^u consists of  <dig> unknown parameters and similarly the vector v^r consists of  <dig> unknown parameters. to determine the unknown parameters, the methods each make use of the observed joint probability distribution between the three taxa. let j^x,y,z denote this distribution. that is,

 j^x,y,z=pr,fori,j,k∈{a,c,g,t}. 

each state of j^x,y,z can be written in terms of p^x, p^y, p^z and v^r. for example,

 j^x,y,z=∑i∈{a,c,g,t}prprprpr=∑i∈{a,c,g,t}v^rp^xp^yp^z. 

maximum likelihood methods obtain p^x, p^y and p^z by finding the values for the parameters that maximize the likelihood of the observed joint frequencies  <cit> . that is, align the x, y, and z sequences and let nxyz denote the number of times x = i, y = j, z = k occurs in the alignment. the likelihood function is proportional to the conditional probability, given by

  pr=∏i,j,k∈{a,c,g,t}j^x,y,zn. 

one method for finding the  <dig> unknown parameters  is to use a global optimization algorithm to search parameter space and find p^x, p^y, p^z and v^r that maximize the likelihood. we refer to this method as the 'mw' method below. since the chosen ordering of the nucleotides is arbitrary, there are 4! permutations of the elements of v^r , and 4! corresponding permutations of the rows of the p^ matrices, that will result in the same j^x,y,z and therefore the same likelihood. thus, once the global optimization algorithm converges to one of the maximum likelihood solutions, a unique solution can be determined by choosing the permutation that maximizes the sum of the traces of the p^x, p^y and p^z matrices  <cit> , which provides the solution that implies least change in the overall sequence . if the underlying process is not one that maximizes the diagonal entries, the process is unidentifiable using this method . one solution might be to restrict the class of processes to those with diagonal-dominant p  <cit> , although we did not perform such restrictions in this study.

an alternate approach for finding the maximum likelihood parameters that does not involve a global optimization routine was introduced by barry and hartigan and is referred to as the 'bh' method  <cit>  below . bh rewrites the likelihood function in terms of joint probability matrices along each edge of the tree. solving for the maximum value of the likelihood leads to a system of iterative equations for the joint probability matrices along each edge. thus, given initial starting guesses for the joint distributions along each edge, the system can be iterated until convergence is reached  <cit> . the joint probability matrices along each edge can then be used to obtain their corresponding p^ matrices.

bh is valid on unrooted trees of any size, but for simplicity we briefly outline it here using the unrooted version of the rooted triple defined above. let ui denote the ith element in the sequence from taxon u, with u ∈ x, y, z. similarly, let ri denote the ith element in the sequence at node r. then

 j^x,y,z=∑r1∈{a,c,g,t}j^x,rp^yp^z. 

let n denote the length of the sequences. then the conditional probability can be written as

  pr=∏i=1n∑r∈{a,c,g,t}j^x,rp^yp^z. 

the maximum likelihood solutions are therefore the values of j^x,r, p^y and p^z that maximize the likelihood subject to the constraint that the entries in j^x,r sum to  <dig>  as it is a joint probability matrix. in other words, bh maximizes

 log⁡)+λ). 

differentiating with respect to j^x,r and λ and setting equal to zero leads to the following iterative equation for j^x,r,

 j^x,r=1n∑i=1nij^x,rp^yp^z∑r∈{a,c,g,t}j^x,rp^yp^z, 

where i is an indicator function that takes the value  <dig> if x = xi and  <dig> otherwise  <cit> . this iterative equation can be written entirely in terms of joint distributions along each edge by using the relation

  p^u=j^u,r∑ui∈{a,c,g,t}j^u,r,foru∈{x,y,z}. 

these steps are repeated to derive iterative equations for j^y,r and j^z,r, leading to an iterative system for j^x,r, j^y,r and j^z,r. suitable initial values are chosen and the system is iterated until it converges  <cit> .

the j^ matrices can then be converted to p^ matrices along each edge using the relationship in . finally, the p^ matrices can then be used to obtain q^ matrices using the relationship p^=eq^t.

both the global optimization and bh methods maximize the same likelihood function, but they differ in the computational methods used to find the maximum and in whether they write the unknown parameters in terms of joint probability matrices or probability transition matrices. maximizing the likelihood does not result in a unique solution without adding further restrictions. mw chooses the p^ matrices with the largest sum of the traces, while in the bh method, the initial starting values determine which maximum is chosen. the bh method's default initial j^ matrices, are diagonally dominant. thus we expect the iteration to converge to a diagonally dominant solution . thus, in theory the bh and mw methods will result in the same q^ matrices. in practice, their answers can differ if either converge instead to a local maximum, or if the initial starting points used in either algorithm lead to too slow of a rate of convergence, or if the original p matrices used to generate the sequences are not diagonally dominant.

another method, which we refer to as the 'lake' method, uses markov triple analysis  as a different approach to finding the p^u matrices for rooted triples of sequences  <cit> . this method uses the fact that the conditional joint probabilities j^x,y|z can be written in terms of p^x, p^y, and p^z*, where p^z* is the probability transition matrix from taxon z to r.  for example,

 j^x,y,|z=∑r∈{a,c,g,t}p^z*p^x,p^y. 

mta uses the observed conditional joint probabilities as estimates of the true conditional joint probabilities. this results in a system of  <dig> nonlinear equations in terms of the  <dig> unknowns in p^x, p^y and p^z*. mta solves this system by first using the properties of determinants to set up a simpler system in terms of p^z*. specifically, determinants of the conditional joint probabilities are used to derive a system of  <dig> quartic polynomials. p^z* is determined by first finding the roots of the polynomials and then searching through  <dig> possible orderings of the roots to find the ordering that results in a consistent system. in the case that the original system of  <dig> equations is inconsistent, the lake method chooses the ordering that minimizes an inconsistency function defined by the author  <cit> . these ordered roots are coefficients in a system of equations that is linear in the unknown parameters of p^z*. thus once the ordered roots are found, p^z* can be determined by solving the linear system. again, any of the 4! permutations of the columns of p^z* will also be a valid solution. thus the lake method chooses the ordering with a positive determinant that maximizes the trace. once a unique p^z* has been determined,  <dig> systems of equations that are linear in the parameters of p^x, p^y, p^z can be solved to determine these matrices. again, the p^ matrices estimated using the lake method can then be used to obtain q^ matrices.

if the observed conditional frequencies do in fact accurately estimate the true conditional frequencies and result in a consistent system of  <dig> equations, the lake method provides a computationally feasible way to find the solutions of the system. however, since this method estimates the joint probability distribution from observed frequencies, the accuracy of this estimation will be dependent on sequence length and sequence alignment. therefore, the system of  <dig> equations can often be inconsistent . in this case, it is possible that the lake method will be unable to find valid estimations of the probability transition matrices, as the linear systems in the method may also be inconsistent or may result in p^x, p^y or p^z having negative elements.

in this paper, we present a comparison of the speed and accuracy of the different methods described above, using sequences of different lengths and divergences. we also compare two methods for obtaining q^ matrices from the resulting p^ matrices: using the matrix log  <cit>  and using constrained optimization.

RESULTS
in principle, each of the methods described has advantages and disadvantages. the goldman method uses the observed number of changes between two sequences to obtain q^. it is thus very simple and fast computationally. however it assumes that the evolutionary process is time-reversible and that the divergence between sequences is small, which may not model real evolutionary processes well. the gojobori method obtains p^ from groups of three taxa in which the outgroup is known, by counting the directed changes in two sister taxa relative to the outgroup, then normalizing the rows of this count matrix. it does not require the evolutionary process to be time-reversible and thus is more general than the goldman method. since both the goldman and gojobori methods rely on the count matrix, we would expect them both to do better on longer sequences, which provide more observed data for more accurately sampling p^ and q^. the maximum likelihood methods  do not require the evolutionary process to be stationary or reversible, and in principle should be more robust to sampling error in the observed count matrix. however, maximizing the likelihood alone does not lead to a unique solution for the p^ matrices without imposing further restrictions. by default, these methods resolve this issue by searching for the diagonally dominant solution. thus, we expect these methods to do the best when the original q matrices are such that their corresponding p matrices are diagonally dominant for the level of sequence divergence under consideration.

finally, the lake method uses observed conditional joint probabilities as estimates for the true conditional probabilities. if the observed probabilities accurately estimate the true probabilities and result in a consistent system of equations, the lake method should provide an accurate estimate of q. however, because this method depends on solving several linear systems that may be inconsistent on the given data, it is the only method of those we consider that, in certain cases, fails to provide any valid output for p^. we would expect it to do worse on shorter sequences, when the observed conditional joint probabilities estimate of the true joint probabilities poorly due to sampling error. overall, although some of the methods are restricted in their solution space, our primary interest is in determining empirically how accurately each method obtains q^ from a randomly generated q.

different techniques differ by orders of magnitude in performance, and the fastest techniques are among the most accurate
the different techniques for obtaining p^, and hence q^, vary widely in both accuracy and performance. we measured accuracy by testing for the ability to recover q from three-taxon trees with specified topology and branch length using the standard markov model of evolution and arbitrary, randomly generated substitution matrices with no constraints on symmetry or the relative sizes of the elements. error was measured using the euclidean distance between the parametric q that the sequences were evolved under and the inferred p^ for the same sequences.

the speed and accuracy of each method were tested under the following conditions. the length of the sequence ranged from  <dig> to  <dig> in steps of  <dig>  then  <dig> to  <dig> in steps of  <dig> . the length of the inner branch ranged from  <dig> to 10% divergence in steps of 1%, and from  <dig> to 50% divergence in steps of 5%. the branch ratio, i.e. the ratio of the inner branch to the outer branch, was  <dig>  to  <dig> in steps of  <dig> , and  <dig> to  <dig> in steps of  <dig>  the substitution matrix was either kept constant for the two inner branches, or varied between these two branches. we sampled  <dig> random matrices under every possible combination of these conditions. the overall workflow, including a description of these parameters, is shown in figure  <dig> . because we are studying nonstationary processes, we scaled the trace to  , but this procedure is valid for the purposes of comparing the methods and was used consistently for all methods.

the apparent benefits of the lake method are due in large part to its failure to run on difficult datasets
from the data in fig.  <dig>  it appears that the lake method is more accurate than most other methods under most conditions. because the lake method is one of the slower methods, and also fails to produce results on a substantial fraction of matrices, we decided to test whether this apparent increase in accuracy was due to the method rejecting datasets for which the other methods produced especially inaccurate predictions. figure  <dig> demonstrates that this is the case: when we consider only the subset of datasets on which the lake method was able to complete successfully, we find that the results of the lake method are worse than the results of other methods under many conditions, and are only better in a minority of cases where the branch ratio  is very long, the sequence is short, or the inner branch length is short. for these reasons, and because the method fails to complete on as many as 80% of the datasets under some conditions, we believe that the apparent advantages of this method do not outweigh its disadvantages relative to other methods.

the two methods for converting probability matrices to rate matrices performed indistinguishably
we compared two different methods for converting the probability matrices to rate matrices: simply taking the matrix logarithm, and constrained optimization of the rate matrix to ensure that the resulting rate matrices contained only non-negative off-diagonal elements . the accuracy results  were visually and statistically indistinguishable, and the values from  <dig> randomly sampled points did not differ significantly in mean error . we performed an additional internal control by testing whether the estimates of the two inner branches' rate matrices were equivalent when the two inner branches were simulated under the same q with similar results . matrices with negative off-diagonal elements were not corrected in any way: however, the distances between these matrices with negative off-diagonal elements and the corresponding matrices corrected by constrained optimization were apparently equivalent. this result is not necessarily unexpected because most negative off-diagonal elements produced by taking the logarithm of the p matrix are small compared to the negative elements on the diagonal.

CONCLUSIONS
in general, fast methods provided little or no degradation of accuracy relative to slower methods under the conditions tested: either the gojobori method or the goldman method, which were orders of magnitude faster than the other methods, gave equivalent or improved accuracy under most conditions. overall, we recommend the gojobori method when the sequences are long , the goldman method for shorter sequences, and the bh method for very long sequences  if computation time is not an issue. we did not observe improved accuracy  using constrained optimization rather than the matrix logarithm for obtaining p^ matrices from q^ matrices, but constrained optimization does have the desirable property that the resulting matrices have only non-negative off-diagonal elements . one cautionary note about the work presented here is that we examined random rate matrices: as data are collected about which kinds of rate matrices are biologically reasonable, it may be useful to revisit this study using biologically-inspired rate matrices, which may reveal larger differences among methods.

the findings presented here open the door to very large-scale studies of the variation in nucleotide substitution matrices in different genes and taxa, which would be completely impractical with the slower methods. we can thus begin to explore the evolution of this fundamental parameter in all models of molecular evolution, and perhaps ultimately discover the molecular basis for differences in nucleotide substitution patterns in different organisms.

