BACKGROUND
combining evidence from several heterogeneous data sources is a central operation in computational systems biology. we assume several vector-valued data sources, such that each source consists of measurements from the same object or entity, but on different variables.

in modeling in general, when it is possible to make sufficiently detailed modeling assumptions, data integration is in principle straightforward. given a statistical model of how transcriptional regulation works, for instance, the bayesian framework tells how to integrate gene expression data, prior knowledge, and transcription factor finding data. lots of practical problems of course remain to be solved. alternatively, in a classification task of proteins to ribosomal or membrane proteins, for instance, integration is likewise straightforward: do the integration such that the classification accuracy is maximized. this has been done effectively in semidefinite programming for kernel methods  <cit>  and using gaussian process prior within the bayesian framework  <cit> .

in exploratory analysis, that is, when "looking at the data" to start data analysis while the hypotheses are still vague, it is not as straightforward to decide how data sources should be integrated. the task of exploring data is particularly important for the current high-throughput data sources, to be able to spot measurement errors and obvious deviations from what was expected of the data, and to construct hypotheses about the nature of the data. nowadays in bioinformatics applications this stage is typically done using dimensionality reduction and information visualization methods, and clusterings. a good exploratory analysis method is  fast to apply interactively,  easily interpretable by the analyst, and  widely applicable. linear projection methods, as such or as preprocessing for clusterings and other methods, fulfill all these criteria.

fusing the sources is not trivial since we need to choose from three very different options. if all sources are equally important and there is not special reason to do otherwise, it makes sense to simply concatenate the variables from all sources together, and then continue with the resulting single source. the classical linear preprocessing method for this case is principal component analysis . the second option is suitable when one of the sources, such as the class indicator in functional classification tasks, is known to be of the most interest. then it is best to include only those variables or features within each source that are informative of the class variable. a classical linear method applicable in this case is linear discriminant analysis. this second option is supervised, and only applicable when the class information is available.

the third option is to include only those aspects of each source that are mutually informative of each other. those are the shared aspects, and this task can be motivated through two interrelated lines of thought. the first is noise reduction. if the sources are measurements of the same entity corrupted by independent noise, then discarding the source-specific aspects will discard the noise. the second line of motivation is more abstract, to analyze what is interesting in the data. here the different measurement sources can convey very different kinds of information of the entities being studied. one example is copy number aberrations and expression measurements of the same genes in cancer studies  <cit> , and another is the activation profiles of the same yeast gene in several stressful treatments in the task of defining yeast stress response  <cit> . in these examples it is what is in common in the sources that we are really interested in. note that the "noise" may be very structured; its effective definition is that it is source-specific.

commonalities in data sources have been studied by methods that search for statistical dependencies between them. the earliest method was classical linear canonical correlation analysis   <cit> , which has later been extended to nonlinear variants and more general methods that maximize mutual information instead of correlation. yet, being fast, simple and easily understandable, the linear cca still has a special place in the data analysis toolbox, analogously to the linear principal component analysis which is still being used heavily instead of all modern dimensionality reduction and factor analysis techniques.

cca addresses the right problem, searching for commonalities in the data sources. moreover, being based on eigenvalue analysis it is fast and its results are interpretable as linear correlated components. it is not directly usable as a data fusion tool, however, since it produces separate components and hence separate preprocessing for each source. if the separate outputs could be combined in a way that is both intuitively interpretable and rigorous, the resulting method could become a widely applicable dimensionality reduction tool, analogously to pca for a single source. performing dimensionality reduction helps in avoiding overfitting, focusing on the most important effects, and reduces computational cost of subsequent analysis tasks.

in this paper we turn cca into a data fusion tool by showing that the justified way of combining the sources is simply to sum together the corresponding cca components from each source. an alternative view to this procedure is that it is equivalent to whitening each data source separately, and then running standard pca on their combination. this is one of the standard ways of computing cca, but for cca the eigenvectors are finally split into parts corresponding to the sources. so the connection to cca is almost trivial and it is amazing that, as far as we know, it has not been utilized earlier in this way.

our contribution in this paper is to point out that cca can be used to build a general-purpose preprocessing or feature extraction method, which is fast, and easily interpretable. there are two alternative interpretations. the first is the connection to cca discussed above. the second is that it extends the standard practice of standardizing the mean and variance of each variable separately before dimensionality reduction. now each data source is standardized instead of each variable.

we have developed a practical software tool for r that incorporates the subtle but crucial choices that need to be made to choose the dimensionality of the solution. the method is demonstrated on three collections of gene expression measurements.

a kernelized version of cca  has been used in specific data fusion tasks  and it could be easily extended to be used in the same way as the linear cca here. we will focus on the linear mappings for two practical reasons: computation of the linear version is fast and the components are more easily interpretable. in particular, the kernelized version does not reveal which of the original features cause the dependencies between sources.

RESULTS
algorithm
in this section we first explain a simple two-step procedure, based on whitening and pca, for finding the aspects shared by the sources, and then show how the same fusion solution can equivalently be derived from the result of applying a generalized cca to the collection. the two-step procedure provides the intuition for the approach: first remove the within-data variation, and then capture all the variation that is still remaining. the connection to cca then demonstrates how the procedure provides a solution to the issue of combining the separate components cca gives.

denote a collection of p data sets by {x <dig> ...,xp}, where each xi is a m × ni matrix such that m ≫ n, and n = ∑ni. the rows of the matrices correspond to the same object in each set, while the columns correspond to features that need not be the same in the data sets. for example, in traditional expression analyses the rows would be genes and the columns would be conditions, treatments, time points, etc. for notational simplicity, we assume zero mean data.

in the first step, each data set is whitened to remove all within-data correlations, and the data are scaled so that all dimensions have equal variance. the whitened version x¯i of a data matrix xi is given by x¯i=xiwxi, where wxi is the whitening matrix. the whitening matrix is simply wxi=cxi−1/ <dig>  where cxi is the covariance matrix of xi.

after each data set has been whitened, the next step is to find the shared variation in them. this is done by principal component analysis  on the columnwise concatenated whitened data sets. since all the within-data structure pca could extract has been removed, it can only find variation shared by at least two of the data sets, and the maximum variance directions it searches for correspond to the highest between-data correlations.

formally, applying pca to the columnwise concatenation of the whitened data sets z= yields the factorization

  cz = v Λ vt, 

where the orthonormal matrix v contains the eigenvectors, Λ is a diagonal matrix of projection variances, and cz is the covariance matrix of z.

projecting z onto the first d eigenvectors vd corresponding to the d largest eigenvalues gives the d principal components, which are the optimal d-dimensional representation in terms of the shared variance. the whole data collection becomes integrated into

  pd = zvd, 

where pd is of size m × d and contains a d-dimensional feature vector for each of the analyzed objects. the idea is then simply to use this new representation for any further analysis, which can be made using any method that operates on vectorial data. the whole procedure can be seen as fusing the collection of data sets into a single set, while at the same time reducing the total dimensionality of the data to find the most reliable shared effects.

as mentioned in the background section, the above two-step procedure is equivalent to running cca on the collection and summing the separate components from each source. the connection is derived here for two data sets. the proof extends easily for several data sets, for one of the many alternative generalizations of cca.

cca is a method for finding linear projections of two sets of variables so that the correlation between the projections is maximal. cca is often formulated as a generalized eigenvalue problem

  =λ, 

where cij denotes the covariance of xi and xj. the eigenvalues λ of the solution appear as pairs  <dig> + ρ <dig>   <dig> -ρ <dig> ..., <dig> + ρp,  <dig> - ρp,  <dig> ..., <dig>  where p = min, and  are the canonical correlations. the canonical weights corresponding to the canonical correlations are ui=t,, i =  <dig> ...,p.

in conventional use of cca we are usually interested in the correlations, the canonical weights ui, and the canonical scores, defined as projections of x <dig> and x <dig> on the corresponding canonical weights. next we show how the combined data set  can be obtained from the canonical scores, thus providing a way of using cca to find a single representation that captures the dependencies.

for a single component,  can be equivalently written as

 v=αv, 

where α is the variance, v is the corresponding principal component, and c¯ij denotes the covariance of x¯i and x¯j. due to the whitening, the c¯ <dig> and c¯ <dig> are identity matrices. we can alternatively write c¯12=w1tc12w <dig> and c¯21=w2tc21w <dig>  leading to

 v=v, 

where iv has been subtracted from both sides. equivalently,

  v=−1v. 

let us denote  by diag , and multiply the right hand side of  by the identity matrix i = diag-1diag . on the right side of the equation we then have the term diag- <dig> diag- <dig> = diag  based on the definition of the whitening matrix, and thus  can be written as

  v^=v^, 

where v^=diagv. adding iv^ to both sides gives equation structurally identical to . both methods thus lead to the same eigenvalues, i.e. λ = α, and the eigenvectors are related by a linear transformation,

 diagv=t. 

the combined representation  of d dimensions can be written in terms of canonical scores as pd = zvd = diagvd =  t = x1u <dig> d + x2u <dig> d, where u <dig> d and u <dig> d are the first d canonical directions of the two data sets.

cca can be generalized to more than two data sets in several ways  <cit> , and the two-step procedure described here is equivalent to the one formulated as solving a generalized eigenvector problem cu = λdu, where c is the covariance matrix of the column-wise concatenation of the xi and d is a block-diagonal matrix having the dataset-specific covariance matrices cii on its diagonal. here u is a row-wise concatenation of the canonical weights corresponding to the different data sets. the proof follows along the same lines as for two data sets, and again the combined data set for any d<∑i=1pni dimensions can be written in terms of the generalized cca results as

 pd=∑i=1pxiui,d, 

where each ui,d contains the d eigenvectors corresponding to the d largest eigenvalues.

in summary, the simple linear preprocessing method of whitening followed by pca equals computing the generalized cca on a collection of data sets and summing the canonical scores of the data sets. in practice it does not matter in which way the result is obtained, but the two-step procedure illustrates more clearly why this kind of approach is useful for data integration. furthermore, it is not limited to linear projections, and the same motivation could be extended to different kind of models. in practice implementing the first step might, however, be difficult in more complex models.

choice of dimensionality
the dimensionality of the projection can be chosen to be fixed, such as two or three for visualization, or alternatively an "optimal" dimensionality can be sought. in this section we introduce our suggestion for optimizing the dimensionality. intuitively, the dimensionality should be high enough to preserve most of the shared variation and yet low enough to avoid overfitting. the first few components contain most of the reliable shared variation among the data sets, while the last components may actually represent just noise, and thus dropping some of the dimensions makes the method more robust.

the maximum dimensionality is the sum of the dimensionalities of the data sets, but in practice already a considerably smaller dimensionality is often sufficient, and in fact leads to a better representation due to suppression of noise. note also that in the case of two data sets the number of unique projections is only the minimum of the data dimensionalities.

in a nutshell, we increase the dimensionality one at a time, testing with a randomization test that the new dimension captures shared variation. to protect from overfitting, all estimates of captured variation will be computed using a validation set, i.e., for data that has not been used when computing the components . the randomization test essentially compares the shared variance along the new dimension to the shared variance we would get under the null-hypothesis of mutual independency. when the shared variance does not differ significantly from the null-hypothesis, the final dimensionality has been reached.

to compute the shared variance of the original data, we divide the data into training, xit and validation data, xiv. the two step procedure described in the algorithm subsection is applied to the training data to compute the eigenvectors vt and the whitening matrix wt, where wt is a block diagonal matrix containing the whitening matrices for each matrix in training data. the fused representation for the validation data is computed as pdv=xvwtvdt, where xv is the columnwise concatenation of the validation data matrices. variance in the fused representation is now our estimate of shared variance. we average the estimate over  <dig> different splits into training and validation sets.

to compute the shared variance under the null hypothesis, random data sets are created from the multivariate normal distribution with a diagonal covariance matrix where the values in diagonal equal the columnwise variances of xit. the shared variance for the random data is computed in the same way as described above. we repeat the process for  <dig> randomly created data sets.

the shared variance in the original data is then compared to the distribution of shared variances under the null hypothesis, starting from the first dimension. when the dimensions no longer differ significantly , we have arrived at the "optimal" dimensionality and the rest of the dimensions are discarded.

note that assuming normally distributed data in the null hypothesis is consistent with the assumptions implicitly made by cca. the underlying task is to capture all statistical dependencies in the new representation, and finding correlations  is equivalent to that only for data from the normal distribution. for considerably non-normal data the choice of dimensionality may not be optimal, but neither is the method itself. therefore transforming the data so that it would roughly follow normal distribution  would be advisable.

implementation
we have implemented the method, including the choice of dimensionality and the validation measures presented in the section validation measures, as an open-source package for r .

experiments
validation on gene expression data
we first validate the method on three gene expression data sets , by checking how well it preserves the shared variation in data sets and discards the data-specific variation.

in case of two data sets an estimate of mutual information can be computed directly from the canonical correlations as

 i=−12∑i=1dlog⁡, 

based on the assumption of normally distributed data. consequently we started by confining to pairs of data sources. figure  <dig> shows the results for one of the pairs in each collection; the rest are analogous. it is evident that the method retains the shared variation between data sets and the shared variation increases with increasing number of dimensions in the combined data.

for more than two variables, the measures explained in the methods section are used. we compare the results with pca of the concatenated data matrices. pca is equally fast, linear, and unsupervised. note that the proposed cca-based method is also unsupervised as no class information is used. furthermore, since both methods have a global optimum, differences in performance cannot be due to optimization issues. the only difference then is related to the main topic of this paper: whether to model all information in the whole data collection, as pca does, or only the mutual dependencies.

shared variance  and data-specific  variance captured by the fused data were computed for each of the three data collections. the presented results are averages over five-fold cross-validation, and the variances have always been computed for the left-out data. in addition to the pca comparison, we provide baseline results obtained with random orthonormal projections that have uniform distribution on the unit sphere.

the results are presented for each of the data sets in figures  <dig>   <dig>  and  <dig>  in all cases it is easily seen that the proposed method retains clearly less data-specific variation than pca , regardless of the dimension. the cca-based method still keeps more variation than random projections, indicating that it is not purposefully looking for projection directions that would lose more variation than necessary.

at the same time the proposed method retains more between-data variation  for wide range of dimensionalities in all cases. the difference is particularly clear for the leukemia data  where the cca-based approach is considerably better than the pca. in stress data  the difference is also clear, but pca is also very good in comparison to the random baseline. for cell-cycle data  the differences are smaller, but for dimensionalities between  <dig> and  <dig> the cca-based method is still clearly better.

it is striking that in all three cases the pca, which simply aims to keep maximal variation, is the best also in terms of the shared variation for dimensionality of one. a one-dimensional projection, however, loses a lot of the variation and is not too interesting as a summary of several data sets. hence, this finding does not have a lot of practical significance.

one notable observation is that especially for the leukemia data  the between-data variance of the cca-method is, for a wide range of dimensionalities, higher than the corresponding value for the original collection. this does not, however, seem to have clear operational meaning but is merely a side-effect of the heuristic measure.

the curves of extracted variance can be contrasted to the suggested dimensionalities , marked with ticks in the plots. for two of the three data sets the suggested dimensionality is very close to the maximum point of the between-data variance curve, and when increasing the dimensionality the result remains relatively constant, or even decreases for the leukemia data. while the amount of data-specific variation still keeps increasing, there is no longer a significant amount of shared variation available, and the chosen dimensionality is thus good in terms of these two measures. for the third data collection, the cell-cycle, the suggested dimensionality is somewhat lower than what is needed for maximally capturing the between-data variation. however, as seen in the next section, the chosen dimensionality is still very good for a practical application, providing the best result in the actual case study.

prototypical applications
in this section we will discuss a few prototypical ways in which the method could be applied. the method is a general-purpose tool for integrating a collection of data sets in such a way that the effects common to several sets are enhanced. after the integration step any analysis method operating on vectorial data can be used. here some simple methods are used for demonstrational purposes. the applications are demonstrated on the same data sets that were used in the technical validation.

shared effects in leukemia subtypes
pediatric acute lymphoblastic leukemia  is a heterogeneous disease with subtypes that differ markedly in their cellular and molecular characteristics as well as their response to therapy and subsequent risk of relapse  <cit> . combining the expression measurements of the five different all subtypes gives a representation where the genes that have similar  expression profile in several subtypes are similar. here we are interested in the genes that are highly  expressed, and thus study the equivalent of differential expression in the combined data set.

the fusion method was applied to combine the five all data sets, resulting in a 11-dimensional representation. after this we can proceed as if we only had one data source. it has a 11-dimensional feature vector for each gene, and we separate the 1% of genes that have the highest distance from the origo, implying highest total contribution to the shared variation. this set of genes is compared to the corresponding set obtained from a 11-dimensional pca projection of the whole collection. in addition, a baseline result computed from the full concatenation of the original data sets is included.

a functional annotation tool, david   <cit>  was used to annotate the gene lists to find the gene ontology  enrichments in the biological processes category. the most enriched go-terms were the same for both cca and pca, which is understandable as we are using two linear projection methods on the same collection. we picked the go-terms which have p-values  lower than  <dig> , and present the counts of the genes from these categories in table  <dig>  the notable observation is that the cca-based method has higher count in all but one category, in which the counts are tied. both methods thus reveal the same kinds of processes, all related to immune response, but cca is more accurate and is able to include more genes related to these biological processes in the top 1% genes.

the enriched gene ontology terms from the biological processes category with p-values  lower than  <dig> . both cca and pca result in the same  <dig> terms, and here they are sorted according to the p-value of the gene list obtained with pca preprocessing. each cell lists the count of genes in that term, together with the p-value . in  <dig> out of  <dig> the count is higher for cca, showing that it is able to capture relevant genes with better accuracy, avoiding outliers. the baseline method shares  <dig> common go terms with cca/pca, and the two different go enrichments are antigen presentation, endogeneous antigen  and antigen processing, endogeneous antigen via mhc class i .

classification of cell cycle regulated genes in yeast
the second prototype application is about cell-cycle regulation using the gene expression of saccharomyces cerevisiae  <cit> . expression measurements from  <dig> different experiments are combined with the proposed method. the combined data is used for the classification of cell-cycle regulated genes.

as the new representation is simply a real-valued vector for each gene, several alternative classifiers are applicable; here k-nearest neighbor  classifier is selected for demonstrational purposes. we use the cell-cycle regulated genes reported by  <cit>  as the class labels, giving a two-class classification problem: either a gene is or is not cell-cycle regulated.

the leave-one-out classification accuracy of cca and pca projections is shown in figure  <dig>  together with a baseline obtained by using the full concatenation of the original data sets. it is evident that the cca-based method provides a considerably better representation for separating the cell-cycle regulated genes from the rest. already the one-dimensional cca-projection gives a higher accuracy than what is obtained with an eight-dimensional pca-projection, and the maximal accuracy is clearly higher for cca and obtained already with a three-dimensional representation. this is exactly the dimensionality suggested by the procedure explained in section choice of dimensionality.

defining the environmental stress in yeast
we also study yeast gene expression data from  <cit> , consisting of time series of gene expression measurements of saccharomyces cerevisiae in various stressful treatments. the data is combined to study the genes related to general environmental stress response .

in  <cit>  the environmental stress response of yeast was studied based on a broad collection of different treatments, out of which  <dig> are used in our experiment. the original analysis relied primarily on a hierarchical clustering of the whole collection, and was thus based on the overall similarity of the expression patterns. while it is able to cluster the genes into sensible categories, it is ideologically comparable to the pca approach for preprocessing: it does not take into account that not all variation is equally important.

we suggest that it might be a better idea to focus on the variation shared by the different data sources, instead of trying to characterize the similarity based on all variation. treatment-specific effects would be specific stress responses and if the task is to find a general response, its fingerprint is in the shared variation. thus the analysis of environmental stress response should start with a preprocessing step like the one suggested here. we demonstrate how the results of such approach differ from those obtained by  <cit> .

we applied a knn classifier to the combined data space to classify the genes to belong to the three categories labeled in  <cit>  . the accuracies of cca and pca approaches in this task are presented in figure  <dig>  again a baseline obtained by using the full concatenation of the original data sets in included. though the accuracies are similar for some initial dimensionalities, we notice that the accuracy after preprocessing by pca is higher, by a margin of roughly  <dig> % to 1%, for a wide range of dimensionalities including the suggested dimensionality of combined representation,  <dig>  obtained with the method of section choice of dimensionality. also, for the higher dimensionalities the baseline method which simply uses the original data is better. as argued above, this does not tell that cca was the worse preprocessing method, but instead suggests that the original classes have indeed been constructed based on all variation in the data, including treatment-specific responses. this is not desirable since the definition of an esr gene is that it would be responsive to stress in general. as the data set has slightly less than  <dig> genes this corresponds to a difference of roughly  <dig> to  <dig> misclassifications. this characterizes the scale of the disagreement between the two fundamentally different approaches to the preprocessing phase.

this result hints that the definitions created after cca-based preprocessing would be mostly the same as the ones given in  <cit> , but for some roughly  <dig> – 10% of genes the classification should be changed.

CONCLUSIONS
we studied the problem of data fusion for exploratory data analysis in a setting where the sensible fusion criterion is to look for statistical dependencies between data sets of co-occurring measurements. we showed how a simple summation of the results of a classical method of canonical correlation analysis gives a representation that captures the dependencies, leading to an efficient and robust linear method for the fusion task. it does not solve the data integration task in general, but it shows that the criterion in the data fusion task should not necessarily be to keep all the possible information present in the data collection. instead, we may want to focus on the aspects shared by different views. we showed how that can be achieved with simple and easily applicable methods.

we demonstrated the validity of the method on three different real gene expression data sets using technical criteria. we further presented three examples on how the method could be used as the preprocessing step in different kinds of analysis tasks.

