BACKGROUND
recent advances in sequencing technologies have led to a greatly reduced cost and increased throughput  <cit> . the dramatic reductions in both time and financial costs have shaped the experiments scientists are able to perform and have opened up the possibility of whole human genome resequencing becoming commonplace. currently over a dozen human genomes have been completed, most using one of the short read, high-throughput technologies that are responsible for this growth in sequencing  <cit> . the datatypes produced by these projects are varied, but most report single nucleotide variants , small insertions/deletions , structural variants , and may include additional information such as haplotype phasing and novel sequence assemblies. paired tumor/normal samples can additionally be used to identify somatic mutation events by filtering for those variants present in the tumor but not the normal.

full genome sequencing, while increasingly common, is just one of many experimental designs that are currently used with this generation of sequencing platforms. targeted resequencing, whole-exome sequencing, rna sequencing , chromatin immunoprecipitation sequencing , and bisulfite sequencing for methylation detection are examples of other important analysis types that require large scale databasing capabilities. efforts such as the  <dig> genomes project , the cancer genome atlas , and the international cancer genome consortium  are each generating a wide variety of such data across hundreds to thousands of samples. the diversity and number of sequencing datasets already produced, in production, or being planned present huge infrastructure challenges for the research community.

primary data, if available, are typically huge, difficult to transfer over public networks, and cumbersome to analyze without significant local computational infrastructure. these include large compute clusters, extensive data storage facilities, dedicated system administrators, and bioinformaticians adept at low-level programming. highly annotated datasets, such as finished variant calls, are more commonly available, particularly for human datasets. these present a more compact representation of the most salient information, but are typically only available as flat text files in a variety of quasi-standard file formats that require reformatting and processing. this effort is substantial, particularly as the number of datasets grow, and, as a result, is typically undertaken by a small number of researchers that have a personal stake in the data rather than being more widely and easily accessible. in many cases, essential source information has been eliminated for the sake of data reduction, making recalculation impossible. these challenges, in terms of file sizes, diverse formats, limited data retention, and computational requirements, can make writing generic analysis tools complex and difficult. efforts such as the variant call format  from the  <dig> genomes project provide a standard to exchange variant data. but to facilitate the integration of multiple experimental types and increase tool reuse, a common mechanism to both store and query variant calls and other key information from sequencing experiments is highly desirable. properly databasing this information enables both a common underlying data structure and a search interface to support powerful data mining of sequence-derived information.

to date most biological database projects have focused on the storage of heavily annotated model organism reference sequences. for example, efforts such as the ucsc genome databases  <cit> , the generic model organism database’s chado schema  <cit> , and the ensembl database  <cit>  all solve the problem of storing reference genome annotations in a complete and comprehensive way. the focus for these databases is the proper representation of biological data types and genome annotations, but not storing many thousands of genomes worth of variants relative to a given reference. while many biological database schemas currently in wide use could support tens or even hundreds of genomes worth of variant calls, ultimately these systems are limited by the resources of a single database instance. since they focus on relatively modest amounts of annotation storage, loading hundreds of genomes worth of multi-terabyte sequencing coverage information, for example, would likely overwhelm these traditional database approaches. yet the appeal of databasing next generation sequence data is clear since it would simplify tool development and allow for useful queries across samples and projects.

in this work we introduce the seqware query engine, a scalable database system intended to represent the full range of data types common to whole genome and other experimental designs for next generation sequence data. hbase was chosen as the underlying backend because of its robust querying abilities using the hadoop mapreduce environment and its auto-sharding of data across a commodity cluster based on the hadoop hdfs distributed filesystem . we also present a web service that wraps the use of mapreduce to allow for sophisticated queries of the database through a simple web interface. the web service can be used interactively or programmatically and makes it possible to easily integrate with genome browsers, such as the ucsc browser  <cit> , gbrowse  <cit> , or igv , and with data analysis tools, such as the ucsc table browser  <cit> , galaxy  <cit> , and others. the backend and web service can be used together to create databases containing varying levels of annotations, from raw variant calls and coverage to highly annotated and filtered snv predictions. this flexibility allows the seqware query engine to scale from raw data analysis and algorithm tuning through highly annotated data dissemination and hosting. the design decision to move away from traditional relational databases in favor of the nosql-style of limited, but highly scalable, databases allowed us to support tens of genomes now and thousands of genomes in the future, limited only by the underlying cloud resources.

methods
design approach
the hbase backend for seqware query engine is based on the increasingly popular design paradigm that focuses on scalability at the expense of full acid compliance, relational database schemas, and the structured query language . the result is that, while scalable to thousands of compute nodes, the overall operations permitted on the database are limited. each records consists of a key and the value, which consists of one or more “column families” that are fixed at table creation time. each column family can have many “labels” which can be added at any time, and each of these labels can have one or more “timestamps” . for the query engine database, the genomic start position of each feature was used as the key while four column families served to represent the core data types: variants , coverage, features , and coding consequences which link back to the variants entries they report on. the coverage object stores individual base coverages in a hash and covers a user-defined range of bases to minimize storage requirements for this data type. new column families can be added to the database to support new data types beyond those described here. additional column family labels are created as new genomes are loaded into the database, and timestamps are used to distinguish variants in the same genome at identical locations. figure  <dig> shows an example row with two genomes’ data loaded. this design was chosen because it meant identical variants in different genomes are stored within the same row, making comparisons between genomes extremely fast using mapreduce or similar simple, uniform operators . the diagram also shows how secondary indexes are handled in the hbase backend . tags are a convenient mechanism to associate arbitrary key-value pairs with any variant object and support lookup for the object using the key . when variants or other data types are written to the database, the persistence code identifies tags and adds them to a second table where the key is the tag plus variant id and the value is the reference genomic table and location. this enables variants with certain tags to be identified without having to walk the entire contents of the main table.

datasets
fourteen human genome datasets were chosen for loading into a common seqware query engine backend, see table  <dig> for the complete list. most datasets included just snv, small indel, and a limited number of sv predictions. the u87mg human cancer cell line genome was used to test the load of large-scale, raw variant analysis data types. for this genome, snvs, small indels, large deletions, translocation events, and base-by-base coverage were all loaded. for the snv and small indels, any variant observed once or more in the underlying short read data were loaded, which resulted in large numbers of spurious variants  being loaded in the database. this was done purposefully for two reasons: for this study, to facilitate stress testing the hbase backend, and for the u87mg sequencing project, to facilitate analysis algorithm development by giving practical access to the greatest potential universe of candidate variants. in particular, the fast querying abilities of the seqware query engine enabled rapid heuristic tuning of the variant calling pipeline parameters through many cycles of filtering and subsequent assessment.

fourteen whole genome datasets were loaded into the database, including the u87mg genome, with the march  <dig> assembly of the human genome used as reference . variant types  loaded and publication references are noted for each respective dataset. this table was adapted from snyder et al.  <dig> 

a secondary dataset generated in our lab, the “ <dig> gbm” tumor/normal whole genome sequence pair, was used to compare the performance between the berkeleydb and hbase query engine backend types. this dataset, like the u87mg genome, included loading raw variant calls seen once or more in both backends in order to profile the load and query mechanisms.

programmatic access
the seqware query engine provides a common database store interface that supports both the berkleydb and hbase backend types. this store object provides generic methods to read and write the full range of data types into and out of the underlying database. it handles the persistence and retrieval of keys and objects to and from the database using a flexible object mapping layer. simple to write bindings are created when new data types are added to the database. the underlying schema-less nature of key/value stores like berkeleydb and hbase make this process very straightforward. the store also supports queries that lookup all variants and filter by specific fields, such as coverage or variant call p-value, and it can also query based on secondary indexes, typically a tag lookup . the underlying implementation for each store type  is quite different but the generic store interface masks the difference from the various import and export tools available in the project. the store interface was used whenever possible in order to maximize the portability of the query engine and to make it possible to switch backends in the future.

two lower-level apis are available for querying the hbase database directly. the first is the hbase api, which the store object uses for most of its operations including filtering variants by tags. this api is very similar to other database interfaces and lets the calling client iterate and filter result sets. hbase also support the use of a mapreduce source and sink, which allow for database traversal and load respectively. this was used to iterate over all variants in the database as quickly as possible and to perform basic analysis tasks such as variant type counting. the speed and flexibility of the mapreduce interface to hbase make it an attractive mechanism to implement future functionality.

web service access
the web service is built on top of the programmatic, generic store object and uses the restlet java api . this provides a restful  <cit>  web service interface for the query methods available through the store. when loaded from a web browser, the web service uses xslt and css to display a navigatable web interface with user-friendly web forms for searching the database. queries on three data types are supported: variants, coding consequence reports, and per-base coverage. variants can be searched by tag and also a wide variety of fields such as their depth of coverage. the tag field is used to store a variety of annotations and, in the u87mg database, this includes dbsnp status, mutational consequence predictions, and names of overlapping genes, among others. for the variant reports, the standard bed file type  is supported and users can alternatively select to load the query result directly in the ucsc browser, the igv browser, or generate a list of non-redundant tags associated with the variant query results. coverage information can be queried only by location, and the result can be generated in wig format , wig with coverage averaged by each block, or loadable links for the ucsc and igv browsers.

when queried programmatically, the web service returns xml result documents. these contain enough metadata to construct urls accessible from a wide variety of programming languages which can then be used to return query results in standardized formats . since every query is just a url, they can be created from within a web browser using the user-friendly form fields and cut-and-pasted into another tool or script. these urls can then be shared over email, linked to in a publication, and bookmarked for later use, thereby providing a convenient, stateless, and universally interpretable reference to the results.

data load tools
most of the genome datasets used in this project where limited to snv, indel, and sv predictions. for those genomes, the variant information files that were available were loaded into the query engine using either standard file type loaders  or using custom annotation file parsers. the pileup file format  was used to load both the u87mg and “ <dig> gbm” tumor/normal genome variants and coverage information. the variant loading tool supports a plugin interface so new annotation file types can be easily supported. this import tool is available in the query engine package and uses the generic store interface for loading information in database backend . the hbase store currently uses an api very similar to other database connection apis, and is therefore not inherently parallel, although multiple loads can occur simultaneously.

analysis tools
two prototype analysis tools were created for use with the u87mg and “ <dig> gbm” tumor/normal databases. first, a mapreduce variant query tool was written to directly compare the performance of the retrieval of variants from hbase using the api versus using mapreduce. this simple tool used the hbase tableinputformat object and a mapreduce job to traverse all database rows. the second analysis tool created was a simple somatic mutation detector for use with the “ <dig> gbm” tumor/normal genome database. again, the hbase tableinputformat object was used to iterate over the variants from the tumor genome, evaluating each variant by user-specified quality criteria in the map step and identifying those that were present in the cancer but not in the normal. in the reduce phase the coverage at each putative somatic mutation location was checked and only those where the coverage was good and no normal sample variant was called where reported as putative somatic mutations.

performance measurement
backend performance was measured for both the berkeleydb and hbase stores using both data import and export times as metrics. the “ <dig> gbm” tumor/normal genome was used in this testing, which included enough variant calls  to stress test both backends. a single threaded, api-based approach was taken for the load test with both the berkeleydb and hbase backends. each chromosome was loaded in turn and the time taken to import these variants into the database was recorded. a similar approach was taken for the retrieval test except in this case the berkeleydb backend continued to use an api approach whereas the hbase backend used both an api and mapreduced approach. the export test was interleaved with the import test, wherein after a chromosome was loaded the variants where exported and both processes were timed. in that way we monitored both the import and export time as a function of overall database size.

the hbase tests were conducted on a  <dig> node hbase cluster where each node contained  <dig>  <dig> ghz xeon cpus, 24gb of ram, and 6tb of hard drive space. each node contained  <dig> map task slots and  <dig> reducer task slots. the berkeleydb tests were conducted on a single node, since it does not support server clustering, but with hardware identical to that used in the hbase tests.

RESULTS
u87mg genome database
for our original u87mg human genome cell line sequencing project, we created the seqware query engine database, first built on berkeleydb  and then later ported to hbase . for the work presented here, the u87mg database was enhanced with the addition of the  <dig> other human genome datasets that were publicly available when this effort commenced. to further enhance the query engine, we also added new query strategies and utilities, such as a mapreduce-based variant search tool. unlike the u87mg genome, which included all variant calls regardless of quality, the other genomes included only post quality filtered variants. still, they offer a proof of concept that the hbase backend can represent multiple genomes worth of sequence variants and associated annotation data. this sample query engine is hosted at http://genome.ucla.edu/u <dig> and can be used for both programmatic and interactive queries through the web service interface. a database snapshot is not available for download  but all the source datasets are publicly available and the database can be reconstituted in another location using either the berkeleydb or hbase backends along with the provided query engine load tools.

performance
software availability
the seqware query engine is a sub-project of the seqware toolset for next generation sequence analysis. seqware includes a lims, pipeline based on pegasus  <cit> , and metadata schema in addition to the query engine. like the rest of the seqware project, seqware query engine is fully open source and is distributed under the terms of the gnu general public license v <dig> . the software can be downloaded from version control on the project’s sourceforge.net site . visitors to the site can also post questions on the mailing list, view documentation on wiki pages, and download a pre-configured seqware pipeline and query engine as a virtual machine, suitable for running in a wide array of environments. the present authors are very interested in working with other developers on this project and welcome any contributions.

discussion
recent innovations from search-oriented companies such as google, yahoo, facebook, and linkedin provide compelling technologies that could potentially enable computation on petascale sequence data. projects such as hadoop and hbase, open source implementations of google’s mapreduce framework  <cit>  and bigtable database  <cit>  respectively, can be converted to powerful frameworks for analyzing next generation sequencing data. for example, mapreduce is based on a functional programming style where the basic methods available to perform analysis are a map phase, where data is transformed from one form to another, and a reduce phase, where data is sorted and condensed. this fits well to fundamental sequence analysis computations such as alignment and variant calling. already there are versions of analysis algorithms such as crossbow  and gatk  that make use of the mapreduce paradigm. the approach is not fundamentally different from other functional programming languages that have come before, but in this case the approach is combined in hadoop with both a distributed filesystem  and tightly coupled execution engine. these tools, in turn, form the basis of the hbase database system. unlike traditional grid technologies—for example a sun grid engine computation cluster—the hadoop environment automatically partitions large data files across the underlying cluster, and computations can then be distributed across the individual data pieces without requiring the analysis program to know where the data resides, or manage such information. the hbase database system—one of the most mature of the nosql database projects—takes advantage of this sharding feature and allows a database to be broken into distinct pieces across the underlying computer cluster. this enables the creation of much larger databases than can be supported in traditional relational implementations which are constrained to run on a single database server, up to the scale of billions of rows  and millions of columns . this is considerably larger than most database systems typically support, but is the correct scale needed to represent heavily annotated whole genome sequence data for future large scale biomedical research studies and clinical deployment to the broadest patient populations.

CONCLUSIONS
here we have introduced the open source seqware query engine that uses an hbase backend and the mapreduce/hadoop infrastructure to provide robust databasing of sequence data types for the seqware project. the results show the scaling benefits that result from these highly distributable technologies even when creating a database of genomic variants for just  <dig> human genome datasets. the basic database functions, such as importing and exporting, are one to two orders of magnitude faster with hbase compared to berkelydb. moreover, the highly nonlinear improvement in scaling is readily demonstrated at the critical point where the standard database server becomes saturated, whereas the hbase server maintains a proper distributed load as the data burden is increased. this fully cloud-oriented database framework is ideal for the creation of whole genome sequence/variant databases. it is capable of supporting large scale genome sequencing research projects involving hundreds to thousands of genomes as well as future large scale clinical deployments utilizing advanced sequencer technology that will soon involve tens to hundreds of thousands of genomes.

list of abbreviations
acid: atomicity, consistency, isolation, and durability; api: application programming interface; bed: browser extensible data ; chip-seq: chromatin immunoprecipitation sequencing; css: cascading style sheets; gatk: genome analysis toolkit; gbm: glioblastoma multiforme; gff: general feature format ; hdfs: hadoop distributed file system; igv: integrative genomics viewer; indel: small  insertion or deletion; rest: representational state transfer; rna-seq: rna sequencing; snp: single nucleotide polymorphism; snv: single nucleotide variant; sql: structured query language; sv: structural variants; tcga: the cancer genome atlas; url: uniform resource locator; vcf: variant call format; xml: extensible markup language; xslt: xsl transformations

competing interests
the authors have declared no competing interests.

authors’ contributions
bdo designed and implemented the seqware query engine. bm provided guidance on project goals, applications, selection of annotation databases, and choice of analysis algorithms used. sfn is the principal investigator for the u87mg genome sequencing project which supported the development of the seqware project including the query engine.

acknowledgments
this work was supported by grants from the ninds , the dani saleh brain tumor fund, and the henry singleton brain tumor fund. the authors would like to acknowledge jordan mendler for his contributions to the seqware pipeline project and hane lee for her feedback on the seqware query engine tools.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2010: proceedings of the 11th annual bioinformatics open source conference   <dig>   the full contents of the supplement are available online at http://www.biomedcentral.com/1471-2105/11?issue=s <dig> 
