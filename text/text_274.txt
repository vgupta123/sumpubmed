BACKGROUND
whole genome doubling  triggers the wholesale shedding of duplicate genes through processes such as epigenetic silencing, pseudogenization, and deletion of chromosomal segments containing one or more genes  <cit> . the extent to which this paralog reduction is a gene-by-gene inactivation process  <cit>  targeting redundant copies at random points throughout the genome, or a consequence of largely random excision, elimination of excess dna  <cit> , is controversial and likely varies from one phylogenetic domain to another. the distinction between these two processes is not sharp: the inactivation effect may be produced not only by pseudogenization and various suppression and silencing mechanisms but also by the actual excision of a small but critical region of a gene or promoter. conversely, the apparent excision of two or more adjacent genes may rather be due to any of a variety of genetic, epigenetic or functional interactions, rather than the deletion of a dna fragment. nevertheless, the determination of whether paralog reduction is a gene-by-gene process or the deletion of longer stretches dna is key to understanding the dynamics of genome evolution, not only following wgd, but as part of the continual innovative expansion and simplifying shrinkage of genomes over time.

the other face of paralog reduction is the process of fractionation. when a duplicate gene is lost, it may be lost from one copy  of a chromosome or the other. when compared to the pre-wgd genome, or to a closely related but unduplicated genome, this creates an interleaving pattern, such that it is only by consolidating <cit>  the two homeologous single-copy regions that the full original gene complement becomes apparent. that the consolidated region is directly comparable to homologous regions in related genomes is due to the fact that single-copy genes are rarely deleted - of the two duplicates created by wgd, it is unlikely that both are deleted, for obvious functional reasons.

fractionation is an important evolutionary process whenever wgd occurs, and is of particular interest for comparative genomics, since it results in a genome that is highly scrambled with respect to its pre-wgd ancestor. the study of fractionation also brings up the question of bias: are paralogs always or generally lost from the same “side”, or are they lost randomly from one homeologous chromosome or the other  <cit> ?

in this paper, we analyze paralog reduction and fractionation in terms of two models, one easier to analyze but the other more realistic. first we model paralog reduction on only one of the two homeologous chromosomes as a series of excisions of geometrically distributed lengths and show how to use the observed run lengths of single-copy genes on the other chromosome to estimate the parameter of the deletion length distribution.

in the second model we allow excisions on both homeologous chromosomes, and model deletion lengths in terms of truncated geometric distributions to account for the above-mentioned prohibition against deleting single-copy genes.

this work is essentially the creation of a simple, one-parameter “null” model of paralog reduction, where deletion is by random events involving geometrically distributed  numbers of genes on one homeologous chromosome or randomly on both of them. this sets up the possibility of statistical tests of real wgd descendants, to see if the geometric hypothesis is acceptable and to see if fractionation is unbiased or not. we will not explicitly investigate the alternative hypotheses of gene-by-gene excision or biased fractionation; our task here, aside from estimating the parameters of our model, is simply to set up the null statistical model with a view to eventually developing useful statistical tests of hypothesis for this problem.

in a previous study of post-wgd evolution  <cit> , we took chromosomal rearrangement events into account. in the present paper, we do not incorporate rearrangement into our model, but we do reanalyze some of the data, to explore the effects of genome rearrangement processes in confounding the evidence of fractionation and to suggest a way of redressing the loss of information.

the lengths of runs of undeleted genes may be considered independent samples from a geometric distribution, and the lengths of runs of deleted genes are also independent, but we show that the deletion events making up a run of deleted genes are not independent. as a consequence, the distribution of deleted run lengths seems beyond the scope of straightforward mathematical derivation. the major analytical and computational result of this paper is the construction, implementation and evaluation of a deterministic recurrence to calculate the distribution of the number of deletion events per run as a function of µ and the proportion θ of unreduced paralog pairs.

the models
the structure of the data
the data on paralog reduction are of the form , where g and h are binary sequences indexed by ℤ, satisfying the condition that g + h >  <dig>  this condition models the prohibition against deleting both copies of a duplicated gene. we may also assume that whatever process generated the 0s and 1s is homogeneous on ℤ.

the sequence g + h consists of alternating runs of 1s and 2s. we denote by p, l ≥  <dig> the probability distribution of length of runs of 1s. for any finite interval of ℤ we denote by f, l ≥  <dig> the empirical frequency distribution of length of runs of 1s.

the use of ℤ instead of a finite interval is consistent with our goal of getting to the mathematical essence of the process, without any complicating parameters such as interval length. in practice, we will use long intervals of  <dig>  or  <dig>  so that any edge effects will be negligible. see  <cit>  and the section below on  <dig> wgd-descendant genomes for ad hoc ways of handling biological scale intervals.

one-sided deletion
in this case h =  <dig>  for –∞ < i < ∞. we assume a continuous time process, parameter λ >  <dig>  only to ensure no two events occur at the same time. we start  with g =  <dig> for all i. at any t >  <dig>  consider any i where g =  <dig>  with probability λdt, the following deletion event occurs, anchored at position i: we choose a positive number a according to a geometric variable y with parameter 1/µ, i.e.:   

and we convert g =  <dig>  g =  <dig>  … , g =  <dig>  unless one or more of these is already  <dig>  in which case we skip over it and continue to convert the next available 1s into 0s, until a total of a 1s have been converted. this is a natural way to model the excision process, since deletion of duplicates and the subsequent rejoining of the dna directly before and directly after the excised fragment means that this fragment is no longer “visible” to the deletion process. observationally, however, we know deletion has occurred because we have access to the sequence h, which retains copies of the deleted terms.

when the deletion event has to skip over previous 0s, this hides the anchor i and length a of previous deletion events. denote by r the random variable indicating the total number of deletion events responsible for a run. then, given r = r, the run length z is distributed as the sum of r geometric variables, which would result in the negative binomial distribution:   

if these geometric variables were independent. as we shall see later, however, the hypothesis of independence does not hold.

if we observe g at some point in time, as in the last row of table  <dig>  all we can observe are the run lengths of 0s and 1s. we cannot observe the a, i or r, while t and λ are unknown and, as we shall see, only mathematical conveniences that do not enter into our calculations. the parameter about which we wish to make statistical inferences is the deletion length distribution parameter µ, since it is this quantity that is at the heart of the biological controversy about paralog reduction. this inference therefore can only be based on the run lengths and the proportion of remaining 1s. if the probability distribution of r is π, the distribution of run length x is approximately:   

four deletion events leading to two runs of 0s. illustrates the creation of a long run with r =  <dig> subsuming two previous shorter runs. note that r is not observable.

the one-sided model is an extreme version of biased fractionation and is not meant to model any real situation. it is, however, relatively tractable and hence provides a mathematical framework for understanding more realistic cases.

two-sided deletion
in a more realistic model, deletions can occur both in sequence g and sequence h as in table  <dig>  thus before choosing a position i, we chose either g or h with probability ø and  <dig> – ø, respectively. the default we shall study here, , represents unbiased fractionation. then position i, where g + h =  <dig> and geometric variable a are chosen as before.

suppose g is the chosen sequence. then g is set to  <dig>  g is set to  <dig>  and so on until g, unless we first reach a position j where g is already  <dig>  in which case we skip as before, or until we reach a position k where h =  <dig>  in this case, we cannot continue to delete, because g is a single-copy gene, and we are prohibited from letting g + h =  <dig>  for any k. in this case, we must truncate the geometric variable a, having already deleted only k – i < a terms.

four deletion events affecting two homeologous chromosomes, leading to two runs of single-copy genes. the fourth step illustrates how further deletion  and the “skip” process  are blocked when a single-copy gene is encountered  on the homeologous chromosome, truncating the geometric variable a.

in this model, the deletion length is no longer geometric but a mixture of geometric and truncated geometric variables, and run length is no longer negative binomially distributed, even approximately.

RESULTS
simulations to determine π
we carried out a simulation of the one-sided model on an interval of ℤ of length  <dig> . the top row of fig.  <dig> compares π when θ =  <dig>  and θ =  <dig> , for µ =  <dig>   <dig>   <dig>  and  <dig>  we can see that the number of deletion events contributing to a run is somewhat dependent on µ when half of the the sequence has been deleted, but is strongly dependent when  <dig> % has been deleted. in the bottom row, the graph on the left shows that run length l is distributed very differently for µ =  <dig> and µ =  <dig>  when the proportion of the sequence deleted is exactly the same. this strongly suggests that observing the run length distribution and the overall proportion of deletions should allow us to infer µ.

finally, the remaining graph in fig.  <dig> shows that any edge effects in our simulation are negligible. whether we work with g and h on an interval of ℤ of length  <dig>  or, as in another simulation, length  <dig> , gives virtually the same results.

non-independence of deletion events in a run
a long deletion event within a run if undeleted genes has a greater chance of including all the following genes in that run, and possibly successive runs as well, than a short event deleting, say, only one or two genes. this implies that longer deletion events will tend to be grouped together in an event while short events are more likely to be in short runs. this the events making up a run are not chosen independently. this is reflected in the simulations in fig.  <dig> for the case θ =  <dig> .

application to  <dig> descendants of wgd events
to explore the relevance of our models for real genomes, we emphasize that we observe only the proportion θ of unreduced duplicates and the distribution of run lengths of single-copy genes on both homeologous chromosomes.  we cannot observe t or λ. we cannot sample from the geometric distribution of deletion sizes, only their accumulation into runs, so that we cannot directly estimate its mean µ, the parameter of biological interest.

in  <cit> , we studied  <dig> descendants of  <dig> ancient wgd events. in real genome sequences such as these, many or most runs of deleted paralogs will be impossible to identify; one or both of the homeologous regions will have been disrupted by inversions, translocations or other rearrangement events that juxtapose the surviving genes in the run with genes originally remote on the chromosome or from elsewhere in the genome.

we could, however, identify some two-sided undisrupted runs of single-copy genes, fractionated between two chromosomal regions. we searched for such analytical units , two-sided runs flanked at either end by a pair of undeleted duplicate genes, with the two flanking genes on a chromosome having the same orientation, and including no intervening gene having a paralog somewhere outside the run, as in fig.  <dig>  it is statistically unlikely that such an au configuration be produced by a series of compensating rearrangements, so that any rearrangements must have occurred entirely within the run, or have included the entire run intact plus the flanking duplicate pairs.

among all the runs of single-copy genes in a wgd descendant genome, it is only the au that can be used as evidence for the paralog reduction process, because it is only from these that we can reconstruct common conserved homeologous regions on two chromosomes .

key characteristics of the genomes, their global properties and the properties of the two-sided runs are given in table  <dig>  d = d/ is the rearrangements per gene since wgd, where d is calculated only on the duplicates by the algorithm in  <cit> , n is the total number of genes in the given genome and m is the number of single-copy genes.. the way d is calculated, there are between one and two breakpoints per rearrangement. we do not know how many rearrangements have affected the whole genome, duplicates and single-copy, but as a first approximation we assume that the probability that any adjacency will be disrupted by a rearrangement since the wgd is proportional to d, or αd. the proportionality constant α ≤  <dig> is unknown, but experience suggests  is a reasonable value.

evolutionary inference about  <dig> descendants of  <dig> wgd events. t: time in my since event, n: total genes, m: single-copy genes, , d: halving distance  <cit> ,  average run length.

in  <cit> , the au lengths in each of the  <dig> genomes were distributed as in fig.  <dig>  since we model run length in terms of an unknown mixture of distributions involving r geometric or truncated geometric variables, where π is unknown, we cannot infer µ directly. nevertheless, we remark in the figure that the frequency distribution f of the run lengths u ≥  <dig> is closely approximated by a geometric distribution with mean  in all of the cases, except where there are few data. the mean  varies widely from genome to genome. in this section we will continue to make use of this approximation to help understand the data.

consider an au of length u. there are u+ <dig> possible breakpoints in an au of length u, including the two at either end of the run of single-copy genes involving the flanking duplicate genes, that could destroy the au, according to definition.

each adjacency in an au will survive  an evolutionary period equal to the time from wgd with probability approximately . an au of size u will survive with probability  then f/ is an estimate of the frequency of aus of size u if there had been no rearrangements. the predicted relative frequency of run length becomes a geometric distribution with mean ν, where:   

where , and   

fig.  <dig> shows the two-sided curve of run length versus proportion deleted as in fig.  <dig>  but with the mean run length , averaged over the descendants of each of the six distinct wgd events superimposed. each point is connected in the figure to the corrected mean ν calculated from eq.  <dig>  we used α =  <dig> . this somewhat arbitrary choice is bounded above by the fact that z must be greater than  in eq. .

this correction procedure is relatively unstable, since it is very sensitive to the arbitrary parameter α. all the more so with very low values of θ, as on the right of fig.  <dig>  where the model begins to percolate, i.e., where the runs merge together at a rapidly increasing rate. nevertheless we see no evidence in the figure that µ is much greater than  <dig>  leaving a gene-by-gene model very much a viable candidate alongside the geometric excision model.

a model for π in the one-sided model
we are interested in inferring µ from the observed distribution of run lengths and the proportion of undeleted terms θ. at the outset θ =  <dig>  as t → ∞, θ →  <dig>  we are not, however, interested in t, since it is not observable and any time-based inference we can make about µ will depend only on run lengths and θ in any case. on the other hand, r, the number of deletion events per run is an interesting variable since we can assume run length is rµ on average, and we can model the evolution of r directly in the one-sided model. we consider the distribution π as a function of θ.

as π changes, probability weight is redistributed among several types of run:

 <dig>  new runs  falling completely within an existing run of undeleted terms, not touching the preceding or following run of deleted terms

 <dig>  runs that touch, overlap or entirely engulf exactly one previous run of deleted terms with r ≥  <dig>  thus lengthening that run to r +  <dig> events,

 <dig>  runs that touch, overlap or engulf, by the skipping process, two previous runs of r <dig> and r <dig> events respectively, creating a new run of r <dig> + r <dig> +  <dig> events, and diminishing the total number of runs by  <dig>  and

 <dig>  runs that touch, overlap or engulf, by the skipping process, k >  <dig> previous runs of of r <dig>  … , rk events respectively, creating a new run of r <dig> + … + rk +  <dig> events, and diminishing the total number of runs by k –  <dig>  case  <dig> above may be considered a special case of this for k =  <dig> and case  <dig> for k =  <dig> 

the first process, involving a deletion event of length a requires a run of undeleted terms of at least a +  <dig>  what can we say about runs of undeleted terms? we know that runs of deleted terms alternate with runs of undeleted terms, so that there is one run of the former for each of the latter. the mean length  of the deleted runs should be /θ times the mean length  of the undeleted runs:   

the distribution ρ of lengths of the undeleted runs is geometric, since each deletion event creates a randomly placed demarcation between two runs in the sequence consisting of all the remaining terms. the number of terms between two successive such demarcations corresponds to the difference between successive order statistics, and is hence geometrically distributed.

the proportion of terms in runs of length l is lρ/eρ, where eρ = el>0lρ. as depicted in fig.  <dig>  the probability pa that a deletion event falls within a run of length l without deleting the terms at either end is:   

where j indexes the starting position of the deletion within the run, and a is the number of terms deleted in the event.

the probability pb that a deletion event touches only the run of deletions on the left of the run of undeleted terms is:   

the probability pc that a deletion event touches or overlaps the run of deletions on the right but does not extend over the entire run of undeleted terms beyond that is:   

the probability pd that a deletion event completely overlaps the run of deletions on the right and touches or overlaps the run of deletions beyond that but does not extend over a further run of undeleted terms:   

the probability pe that a deletion event touches the run of deletions on the left of the run of undeleted terms and touches or overlaps the run of deletions on the right but does not extend over the entire run of undeleted terms beyond that is:   

the event a adds one new run with r =  <dig>  the events b and c lengthen an existing run from r events to r +  <dig>  the events d and e join two existing runs of of r and s events to create a single run of length r + s +  <dig>  in our initial model, we neglect the merger of three or more runs of deletions. there is no conceptual difficulty in including three or more mergers, but the proliferation of embedded summations leads to computational problems. thus we should expect the model to be adequate until θ gets very small, when mergers of several runs at a time become common.

the last lines of each of , and  include the collection of terms, significantly cutting down on computing time when these formulae are implemented.

we define the change δ in the number of runs of deleted terms with r =  <dig>   <dig> ... as:      

for r >  <dig>    

in an implementation on a finite interval of ℤ, the number of runs of deleted terms will change from some value r to r′, where:   

the distribution of run lengths will also change from π to π′, where:   

where the mean increases accordingly from  to , so that the mean  of the new distribution ρ′ of run lengths of undeleted terms satisfies:   

the new proportion θ′ of undeleted terms is .

we implement equations  to  as a recurrence with a step size parameter Λ to control the number of events using the same pa, pb, pc, pd and pe and δ between successive normalizations, using Λδ instead of δ in -. the choice of Λ determines the trade-off between computing speed and accuracy.

fig.  <dig> shows the results of our current implementation of our deterministic recurrence for the case µ =  <dig>  the results fit simulations of the stochastic model quite well. there are at least two reasons for the observed discrepancies. at the outset, since we used a large step size Λ for the computationally costly recurrence, its trajectory lags behind the simulation, especially with respect to the slower decrease in pa and slower increase in pb + pc. later discrepancies are partially due to not accounting for the merger of three or more runs. these can be estimated and are summarized as “other ” in the diagram, but the quantities involved are not fed back to the recurrence through .

other possible sources of error might be due to the cutoffs in x used for calculations involving γ and ρ. however, extensive testing of various cutoff values has indicated such errors to be negligible in our implementation.

CONCLUSIONS
we have developed a model for the fractionation process based on deletion events excising a geometrically-distributed number of contiguous paralogs from one of a pair of homeologous chromosomes. this is extended to the mathematically less tractable case where both homeologs are susceptible to deletion events. the existence of data prompting this model is due to a functional biological constraint against deleting both copies of a duplicate pair of genes.

the mathematical framework we propose should eventually serve for testing the geometric excision hypothesis against alternatives such as gene-by-gene inactivations or imbalanced fractionation, although we have not developed these here.

simulations of these models indicate the feasibility of estimating the mean µ of the deletion event process from observations of the length of runs of single-copy genes and the overall proportion of single-copy genes. application to real data from an earlier survey of  <dig> genomes descended from  <dig> wgd events, however, is hampered by the accumulation of rearrangement events that have obscured most of the runs of single-copy genes. we have proposed a way of correcting for the missing runs, but this remains a rather approximate procedure.

the main outstanding question remains the exact derivation of π, the distribution of the number of deletion events contributing to a run of single-copy genes. the simulations are convenient in practice, since they depend on only one parameter µ as they evolve over time, but they give little mathematical insight. our most important advance is a deterministic recurrence for the π as the proportion θ of undeleted genes decreases, albeit for the one-sided model only. this takes into account the appearance of new runs over time, the lengthening of existing runs, as well as the merger of two existing runs with the new deletions to form a single, longer one. this calculation fits the process as simulated rather well and seems promising for further development.

competing interests
the authors declare that they have no competing interests.

