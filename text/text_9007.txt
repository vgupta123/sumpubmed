BACKGROUND
this article describes the preparation of training and test materials for a critical assessment of text mining methods in molecular biology  task 1b, which evaluated the ability of an automated system to generate the list of unique gene identifiers from pubmed abstracts for three model organism datasets  <cit> . we chose this task because we had available the sets of gene lists from the three model organism databases to serve as training and test materials. however, it was necessary to adjust these lists to our task. we had to make these adjustments because our guidelines for what to curate differed from all three databases and we were able to provide only abstracts, but two of the three datasets  came from full text annotation. this meant that we had to modify the gene lists to reflect genes mentioned in the abstract – including, in some cases, genes that were not curated because they were only mentioned in passing. however, we felt that the abstract provided too little context to make these distinctions. our data preparation therefore required removing genes from the database gene list that were not mentioned in the abstract , and adding in those that were mentioned "in passing" in the abstract. we used an automated procedure to quickly prepare large quantities of "noisy" training data.

because the gene lists were used in an evaluation to score text-mining systems, we felt that it was particularly important to do a careful assessment of interannotator agreement on our test data, and, based on these findings, to improve the quality of annotation on the test set. this paper describes these experiments and an analysis of our process of generating the gene lists.

RESULTS
generation of gene lists
we were able to generate gene lists for three different model organisms, using data from their respective databases. this was done in several steps. first, we had to assemble a synonym list for each organism, using tables from the model organism database. this took about a week per organism. then we had to manually adjust the gene lists by curating each abstract according to guidelines, which we developed . we estimate that an annotator was able to curate  <dig> abstracts in one week. an additional week was needed to assemble and verify the data. we also found it necessary to check the accuracy of our annotators by running an interannotator experiment. three-way interannotator agreement on fly and yeast was reasonable  but the mouse gene list had many conflicts , which resulted in errors .

accuracy of gene lists
the interannotator experiment indicated that our gene lists contained errors. the cost of having an additional annotator look at each abstract would have been prohibitive. therefore, we developed a secondary method to check for errors in our test data gene list. we used the participants' own results to check for errors. by pooling participant answers, we were able to quickly find additional errors. this only required one week for an annotator to check over the three sets of test abstracts  for errors. this method was fast, but it did not find all the errors; for example, it missed some of the errors detected by the interannotator experiment. using this method, the fly and yeast annotations changed less than 1%; however, mouse changed 8%. we conclude that our "gold standard" gene lists are of reasonable quality and well suited to evaluate information extraction systems, but probably still contain a small number of errors, particularly for the mouse answer key.

methods
guidelines
we created a set of guidelines for the gene list task . during the annotation of the development test data, the guidelines underwent several rounds of modifications. these changes were driven by questions raised by the annotators. this feedback helped to make the guidelines easier to follow and provide clear examples of problematic gene name questions. additional questions arose during the annotation of the test data that led to changes in the guidelines. the annotators found this process very useful, especially the explicit examples that were developed.

the guidelines call for annotating explicit mentions of genes as well as gene mentions implicit in mentions of gene mutants, alleles, and products. genes are required to come from the appropriate organism for the specific database  and must be identified by their unique identifier given in the model organism database synonym list. any gene mentioned "in passing" should be included, even though these are often not included by the databases.

the mention of a gene has to be specific enough to identify the gene. mentions of aggregates are allowed where this reference can be decomposed into specific genes. for example in the sentence, our data reveal that a prolonged oht treatment, by increasing p44/ <dig> mapk activity, affects a key step in growth control of mcf- <dig> cells, the gene identifiers include both the identifiers for p <dig> mapk and p <dig> mapk. mentions of families of genes are included if they can be expanded to  explicit gene mentions, as defined by the following rules:

• the mention of a gene that is part of a genome of many organisms and not attributed to a particular species is a sufficient mention for any of those species. for example, histone h2a, if mentioned in a mouse abstract, would count.

• from a single chromosomal location, each gene must be enumerated, and genes, which are explicitly enumerated, should be counted. for the latter, if the phrase was "the three six genes ..." then six <dig>  six <dig>  and six <dig> should be added to the gene list. however, if the phrase was "the three six genes ...", then none of them would be added. this is because it is not clear, which three six genes are intended; for example, there might now be six six genes, while at the time the paper was written there might have only been three known six genes.

when orthologs are mentioned in an abstract for a given organism, but not directly associated with a specific organism, they are counted. for example, in the sentence galanin is a 29- or 30-amino acid peptide with wide-ranging effects on hormone release, feeding behavior, smooth muscle contractility, and somatosensory neuronal function, we add the identifier for galanin to the gene list, even though mouse is not mentioned in the abstract, because only one gene is mentioned and not a gene family. this gets tricky once paralogs are introduced, because they are gene duplications that might not be found in the specific organism being annotated  <cit> . for more detailed examples, see additional data file  <dig> for the task 1b guidelines.

lexical resources
for each organism, we provided participants with a synonym list that included:

• a mapping between gene symbols, gene names and the unique gene identifiers;

• a set of possible synonyms for each gene.

we derived the organism-specific lexical resources from the synonym and gene lists provided by each model organism database. we can see from table  <dig> that the lexical resources for the organisms vary quite a bit, reflecting, in part, the variable number of genes and entries in each database . fly is predicted to contain only  <dig>  genes, but there are an average of  <dig>  synonyms per gene, whereas mouse is predicted to have  <dig>  genes , with an average of  <dig>  synonyms per gene. yeast has the fewest predicted genes with ~ <dig>  <cit> , and  <dig>  synonyms per gene. fly has the most ambiguity:  <dig> genes have over  <dig> synonyms, whereas neither mouse nor yeast has any genes with over  <dig> synonyms. the percent of entries with a single definition was 30% for fly, 74% for mouse, and 37% for yeast . note also that the fly resources contained genes for fly species other than just d. melanogaster.

generation of training and test data
to generate both training and test data for this task, we began with the resources available in three model organism databases . we did this in three steps:

 <dig>  for each database, we created the synonym list , which included, for each gene, its unique identifier, the gene symbol and any synonyms listed in the model organism database.

 <dig>  next, we extracted a list of pubmed ids  and the associated gene lists for the full text articles in the model organism database.

 <dig>  we then "edited" these gene lists to make them correspond to the abstract.

for step  <dig>  we did the editing in four stages, making use of the synonym lists generated in step  <dig> above

• stage 1: we created a program that used the synonym list to search for mentions of each gene in the gene list for a given abstract. if it found a synonym in the abstract, it marked the gene present ; if it did not find any synonym, the program marked it absent . this enabled us to create large volumes of "noisy"  training data. we provided  <dig> abstracts with gene lists for each of yeast, mouse, and fly.

• stage 2: for the development test set and the final test set, we had biologists with curation experience review and edit the lists generated in stage  <dig>  this is illustrated in figure  <dig>  which shows the gene list with an extra fourth column, reflecting the hand-checked entries, and a new row, representing a gene that was not on the database gene list but was added by the annotator. initially, only one curator saw an abstract because of time constraints.

• stage 3: we then evaluated the quality of the test data sets. we ran an interannotator agreement experiment between the initial curators, to determine  how well the annotators agreed with each other in preparing the test key. we corrected the data set when we found that the initial curator made a mistake.

• stage 4: as a further check, we also utilized the participants' own data using answer pooling. this led to the creation of a revised "gold standard" and a rescoring of all results.

for the hand-checking , the curators did two things to edit the gene list to correspond to the abstract: they removed genes that were mentioned in the full text, but not in the abstract; and they added back genes not listed either because of a matching error during the automated pruning, or because they were outside of the scope required by the specific organism database, but were within the scope of our guidelines. for example, the mouse database does not curate information about adult gene expression. so, if a paper only contains information about adult gene expression for a gene, the information about that gene will not be curated, and the gene will not be listed. however, for the purposes of biocreative, we decided that the abstracts contained so little text that it would be impossible to distinguish genes "mentioned in passing" from genes with sufficient data to merit curation. thus, these genes, which might have not been curated by the databases, were added to the list.

evaluation of the data sets
each database uses different criteria for what genes are curated and we only annotated the abstracts for biocreative. therefore, we expected many differences in comparing our final "answer key" with the gene lists from the model organism databases. table  <dig> shows the results of this comparison. for mouse, we added an additional 41% to the gene list, which were not on the original list . the yeast results were much closer, with only 12% added . the fly list had the fewest additions with 7% added .

it was surprising to find so many genes mentioned in the mouse abstracts that were not on the curated gene list for the corresponding full text article. this is presumably because the mouse database  is interested in genes with embryo expression, genes involved in mouse tumor biology, gene alleles, and evidence for gene ontology  terms. mgi does not curate adult gene expression or genes mentioned in passing. so, we had to enter the additional "missing" genes in the hand annotation phase . on the other hand, there were  <dig> genes listed for the full papers in mgi, and we only found  <dig> genes in the abstracts. some of the genes from mouse that we did not include in the gene list came from genomics papers. for example, two abstracts had  <dig> and  <dig> genes listed for them , but we found only a handful present in the abstract. in other cases, it was not clear whether the gene listed was a single gene or a gene family and as such, it was not added to the gene list. the remaining "missing" genes were presumed to be in the full text but not in the abstract. overall, fly and mouse showed 25% and 36% respectively of genes listed in the database as being present in the abstract. this can be contrasted with yeast , which curates largely from abstracts unless there is something of particular interest, in which case curation is done from the full paper.

interannotator analysis
we performed an internal check on how well we annotated the abstracts. each organism had a total of three annotators covering about  <dig> abstracts each, for a total of  <dig> abstracts from the test data set. these were the same annotators who worked on the generating the initial gene lists. two annotators have a ph.d. in biological sciences, one of which has experience as an annotator; the third annotator is a graduate student in biology. the results from this initial experiment of 3-way interannotator agreement are shown in table  <dig>  we analyzed these results to understand the sources of disagreement. a gene was marked as a disagreement if one of the three annotators disagreed. the most common mistake appeared to be just missing a term that was there. the second most common mistake was assigning a term that referenced a different species but was found in the target species. this was more of a problem for mouse, since a human gene may be discussed in the same article as a mouse gene. if the genes share the same name, then it is necessary to be sure that the gene under discussion was really the mouse gene, not the human gene.

mouse proved to be the hardest organism to annotate, with  <dig> conflicts out of  <dig> genes returned. fly was easier to annotate, with  <dig> conflicts out of  <dig> genes returned, and yeast the easiest, with only  <dig> conflicts for  <dig> returned genes. one interesting problem that arose was linking genomic loci or sequences to genes. for example, we needed to relate m2-related dna sequences are present on mouse chromosomes  <dig>   <dig>   <dig>  and  <dig> to identifiers like rrm <dig>  rrm2- <dig>  rrm2-ps <dig>  -ps <dig>  and -ps <dig>  these results demonstrated to us that it is important to conduct interannotator experiments. we were only able to check a small sample of the total abstracts, but based on this small study, we determined that our gold standard contained significant numbers of errors, especially for the mouse data set.

improving the manual annotation: answer pooling
from the interannotator work, we already knew that the organism that posed the greatest problem was mouse. so we decided to use an answer pooling and selection method to check our gold standard. this was based on looking at genes that were marked as false positives for more than 75% of the participants . we also looked at genes that all the participants failed to return . we again found that mouse had the most changes based on what the participants returned.

using the 75% criteria, we found  <dig>   <dig>  and  <dig> questionable gene references for fly, yeast, and mouse, respectively . for mouse, the annotations for  <dig> of the  <dig> genes were correct . we also found two bad abstracts in that they were not annotated but given out to the participants. this mistake accounted for  <dig> more genes. of the rest,  <dig> were not given on the model organism database list, i.e. not listed in the abstracts for the curator to see. only  <dig> had been on the original database list, but by mistake, we marked them as not present. both of these were in abstracts with a very big list of genes  in which most of the genes were missing from the abstract. finally, we added an additional  <dig> genes when we were reviewing the abstracts in question. these additional genes were found by fewer than 75% of the participants.

the high number of conflicts in yeast  was not expected, given the high level of interannotator agreement. however,  <dig> of the genes were correctly annotated by us . we correctly identified two genes, but because of formatting errors, these genes were not present in the gold standard. four genes were not on the original database list and we missed them: the participants were correct in identifying them. thus, as we found for mouse, most of the ones we missed for yeast were not on the original list.

for fly, we only missed two genes. the other three genes, which all of the participants found were errors on their part . one false positive gene was scored as not present because it was from the wrong organism, staphylococcus aureus pi-plc. another false positive was an interesting error, in that the systems returned the shorter of two gene names; they picked "histone h2a" over "histone h2a variant". the term "histone h2a" was present in the abstract, but in reference to other organisms and as a family. the last false positive is more perplexing, in that the participants' systems picked a gene name that had a partial match over one that matched exactly: they picked derk over der for the reference of the receptor tyrosine kinase der.

we next looked at the genes that all of the participants missed, which were  <dig> for yeast,  <dig> for fly, and  <dig> for mouse . for mouse,  <dig> of the candidate genes were genuine misses by the participants; i.e. the gold standard was correct. nine were mistakes by us. a major cause of these misses seemed to be in converting between different forms of a name. for example, gene- <dig> and gene- <dig> vs. gene-alpha and gene-beta or adding a number or symbol to a gene name which lacks one because at the time the article was written there was only one gene. another general problem was the problem of conjoined names, for example, mt-i and -ii or designated otf-3a through otf-3h. there was one case of a prefix problem. in this case, the prefix "pro" was added to signify that it is modified to make the final protein.

for fly,  <dig> of the candidate genes were correct and  <dig> were errors by us. in two cases, an allele designation probably hindered detection by the participants. the correct way to mark an allele for flies is to superscript it. however, in these two cases this was not done probably because of the use of ascii text. so, flr <dig> in the abstract should have been flr <dig> and there is no flr <dig> gene. finally for yeast,  <dig> of the candidate genes were correct and  <dig> were errors by us. it seems once again that getting conjoined names was a problem. an additional problem was handling different spellings and word order for complex proteins, such as nad dependent isocitrate dehydrogenase  vs. nad-specific isocitrate dehydrogenase .

overall, our most common annotation mistakes were just simple errors, such as missing a mention, formatting, copying and pasting errors, and relying too heavily on the model organism list for each article. in the last case, we would say that a gene was present because the database said it was there and we found one that matched. however, in some cases, the database only put down one family member and there were several more. since our guidelines said that we would only count explicitly enumerated gene families, we were wrong to have annotated a gene family name, as opposed to the explicit gene. this round of careful checking greatly improved our gold standard, especially for mouse.

overall changes in the gold standard
using the above methods, we found that some of our gold standard annotated data changed significantly. the most drastic was mouse, where we found an f-measure of  <dig>  measuring the original "gold standard" against the revised gold standard . fly had  <dig>  and yeast had  <dig>  for f-measures. our precision for mouse was fairly good , but our recall  was the lowest for all three organisms.

we also looked at how these changes affected relative rankings of systems . looking at just mouse f-measure, the overall standings did not change much. out of  <dig> submissions,  <dig> submissions changed relative rankings based on f-measure. this did result in a change in third place. however, there was no change when looking at precision. there were four submissions that changed in recall; two of these were also ones that changed positions in f-measure.

CONCLUSIONS
we have found that generating a list of genes found in abstracts is a complex problem. we concluded that it is important to have well-defined guidelines, but even with guidelines, it is critical to check the annotators against each other for consistency.

we found that some organisms were harder to annotate than others depending on how the original gene lists were developed. there are intrinsic differences between the model organism data sets, which reflect the differences in how each database curates , differences in what each database is interested in curating , differences in lexical resources, and differences in the scientific communities writing the papers. compared to the three organism databases, our task 1b annotation guidelines were very liberal, which accounts for most of additions to our gene lists. we made the least number of mistakes for fly. this could be because we had to add the least number of genes to the fly gold standard. also, it was surprising that we made more mistakes with yeast, considering that its lexicon was by far the simplest and smallest. it is unknown why we had to add more genes to yeast than to fly.

we also concluded that abstracts alone might be a poor resource for identifying genes in a paper : for fly, only 25% of genes on the full text gene list were mentioned in the abstract. for mouse, the figure was 37%. on the other hand, for yeast, which was curated largely from abstracts, the figure was 73%.

this experiment shows that we were able to prepare data sets that consisted of lists of gene identifiers mentioned in abstracts. we were able to use the model organism gene lists from full text articles as a starting point and adapt these to our task. the automated adaptation produced a relatively high precision training data set , with a number of "missed" genes that had to be added manually. our interannotator agreement experiments showed that these missed genes were the main source of error. our error varied considerably, with mouse abstracts having the greatest number of missed genes.

the cost of validating our methods was significant: we estimate that it took about a person-week to generate each  <dig> abstract test set. overall, it took about half that much time to do run the various answer pooling experiments. this raises the issue of whether it would be preferable  to use full text data and the original model organism gene lists for future experiments. this would pose a more difficult, but more realistic, task for the automated systems, since they would have to process full text articles and take into account the criteria for gene curation, which differ among model organism databases.

authors' contributions
mec annotated mouse and yeast, collected the interannotator and participants' results, and analyzed data. am annotated yeast, created the guidelines, obtained and created the lexical resources for fly, and obtained and organized the corpus of abstracts. ay obtained and created the lexical resources for yeast and mouse. jc annotated fly. lh is the principle investigator for the mitre biocreative effort.

supplementary material
additional file 1
guidelines for biocreative gene list evaluation guidelines for what genes should be listed in this task 1b gene list task.

click here for file

 acknowledgements
this paper reports on work done in part at the mitre corporation under the support of the mitre sponsored research program and the national science foundation . copyright ©  <dig> the mitre corporation. all rights reserved.

figures and tables
percent of the genes on the database  list describes what percentage of the genes we found in the abstracts that were on the lists given from the databases. percent total manual genes are the percents of the genes we added out of the total genes found.

