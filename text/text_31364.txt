BACKGROUND
the ever increasing volume of web resources as well as generated data from automated applications is challenging current approaches for biomedical information processing and analysis. one current trend is to build semantic spaces where biomedical data and knowledge resources can be mapped in order to ease their exploration and integration. semantic spaces are usually defined in terms of widely accepted knowledge resources , and they are populated by applying automatic semantic annotation processes. this is the result of a decade of integration initiatives aimed at inter-linking and merging publicly available biomedical databases . most of these initiatives have followed a warehousing approach, where existing data are loaded into a central store under a common schema . recently, with the emergence of the web of data  <cit> , this integration effort is being performed in the context of the semantic web under standard formats like rdf  <cit>  and owl  <cit> .

additionally to these integration projects, literature based discovery   <cit>  has aimed at inferring implicit knowledge by mining scientific papers. lbd approaches also take profit from knowledge resources in order to identify biomedical entities in the texts as well as their associations .

visualization tools play a very relevant role in integration and lbd projects. this is because summarized visual information is required for analyzing the huge amount of data involved in these projects. in this context, visual inference has shown useful in many biomedical research projects  <cit> . the main visual representation adopted in these projects is the conceptual map, where entities  and their asserted or inferred associations are visualized as a graph. cytoscape  <cit>  and ondex  <cit>  are the main representatives for integration projects, and telemakus  <cit>  and litlinker  <cit>  are examples of visualization tools for lbd.

the main limitation of current visualization tools is that they have been developed as stand-alone applications, requiring all the data to be locally loaded and processed. this makes it unfeasible to deal with very large data collections as well as to visualize the information in portable small devices such as mobile phones and tablets. clearly, a web-based architecture is more appropriate for performing visual analysis tasks over huge amounts of integrated data. however, as far as we know, there are no web-based interfaces providing rich and dynamic visualizations for analyzing biomedical data. instead, web services are designed to provide discovered knowledge and biomedical data in plain formats . our approach proposes the use of on-line analytical processing  techniques  <cit>  to integrate and visualize large collections of biomedical data from conventional web browsers. olap technology has shown very successful for analyzing summarized data from different perspectives  and detail levels . part of this success is due to its simplicity in data structures and its efficiency performing data summaries. in a typical olap architecture, data are integrated and pre-processed in the back-end , so that the amount of data users receive in the client side  is dramatically reduced. moreover, olap tools provide a series of operators that allow users to interact with the summarized information as well as to get more detailed information of those parts she wishes to explore. all these features overcome the previously mentioned limitations of current biomedical visualization tools. in this paper, we propose a novel method for building multidimensional semantic spaces from semantically annotated biomedical databases. the main aims of these semantic spaces are: to provide a summarized view of the data sources, to find interesting associations between concepts present in the collection, and to visualize the collection contents for exploration purposes. as in most of the reviewed visualization tools, conceptual maps have been also adopted in our approach to visualize the integrated data. however, our conceptual maps have three main distinctive features:  concepts are arranged into a set of pre-defined biomedical research perspectives,  the visualization is oriented to perform olap-based operations, and  the visualization is rendered in a 3d scenario. the first feature enables a more structured visualization, where associations  must involve entities of different levels . the second feature is related to the interactivity of the user with the visualized data. finally, the latter feature allows a better use of the space to allocate as much data as possible. it must be pointed out that conceptual maps are dynamically built from the web browser according to the users requirements, by selecting the appropriate levels to be visualized. the current implementation of this method is publicly available in  <cit>  for testing purposes.

the paper is organized as follows. first, the methods section is devoted to introduce the methodological aspects of our approach. first, we describe the normalization formalism to represent both the knowledge resources and the target data collections, and the olap-like operators defined over the normalized representation . afterwards, in the results section, we describe some use examples to show the functionality of the implemented prototype, and the experiments carried out to evaluate the quality of the visualized data. finally, we give some conclusions and future work.

methods
olap   <cit>  tools were introduced to ease information analysis and navigation from large amounts of transactional data. olap systems rely on multidimensional data models, which are based on the fact/dimension dichotomy. data are represented as facts , while dimensions contain a hierarchy of levels, which provide different granularities to aggregate the data. one fact and several dimensions to analyze it give rise to what is known as the data cube. common operations include: slice, which performs a selection on one dimension of the given cube resulting in a sub-cube, dice, which performs a selection on two or more dimensions, drill-down, which navigates among levels of data ranging from the most summarized  to the most detailed , roll-up, which is the inverse of drill-down, pivot, which rotates the data to provide an alternative presentation, and drill-through, which accesses the detailed data that is involved in the summary.

since multidimensional models provide a friendly, easy-to-understand and intuitive visualization of data for non-expert end-users, we have borrowed the previous concepts and operations to apply them to the proposed conceptual maps.

this section is devoted to present the necessary methods to generate and manage multidimensional semantic spaces.

overview of the architecture
unlike other visual integration approaches like ondex  <cit> , in our approach knowledge resources  are distinguished from data resources . krs are well-structured databases consisting of concepts and their relationships , whereas drs are any kind of biomedical database to be integrated under some reference kr. drs are usually semi-structured and text-rich . for the sake of simplicity, we assume that a dr consists of a collection of uniquely identified items, whose contents can present arbitrary structures 

dr normalization is performed in two steps:  semantic annotation of the dr collection items with concepts from the reference kr, and  normalization of each collection item to the multidimensional schema derived from the normalized kr. the subsequent sections are devoted to describe in detail these normalization processes as well as the generation of semantic bridges.

semantic annotation
during the last years, we have witnessed a great interest in massively annotating biomedical scientific literature. most of the current annotators rely on well-known lexical/ontological resources such as mesh, uniprot, umls and so on. these knowledge resources usually provide both the lexical variants for each inventoried concept and the concept taxonomies.

in our work, the knowledge resource used for generating semantic annotations is called reference ontology, denoted with o. the lexical variants associated to each ontology concept c are denoted with lex, which is a list of strings. the taxonomic relations between two concepts a and b are represented as a ≼ b. a semantic annotation of a text chunk t is the task of identifying the most specific concepts in o such that they are more likely to represent the meaning of t.

most semantic annotation systems are dictionary look-up approaches, that is, they rely on the lexicon provided by the ontology in order to map text spans to concept lexical variants. some popular annotation systems in the biomedical domain are whatizit  <cit>  and metamap  <cit> , which rely on go and umls resources respectively.

it must be pointed out that metamap has been widely used in literature-based discovery to identify biomedical entities in the mined texts. however, this kind of tool does not scale well for very large collections. to overcome this limitation, annotations are restricted to a few entity types or to the mesh controlled vocabulary. another limitation of this tool is that it is not extensible with new concepts and lexical variants coming from other krs.

in our work we have adapted an annotation system called concept retrieval   <cit> , which scales well over large collections as well as large krs. moreover, this system can easily include any kind of kr and deal with merged kr lexicons. this annotation system was tested in the two calbc competitions  <cit>  over a collection of  <dig> thousand pubmed abstracts about immunology  <cit> , which is annotated in less than  <dig> hours.

the idea behind the cr system consists in ranking the lexical strings of the lexicon with respect to each target text chunk t by applying the following formula:

 rank=idf-idfidf⋅si∩tambiguity 

concept strings si and text chunks t are represented as bags of words. the function idf represents the amount of information contained in the concept string s, which is estimated as follows:

 idf=-∑w∈sp 

in the current implementation we use the whole wikipedia as the background collection for estimating word probabilities. finally, the function ambiguity returns the number of concepts that have s as lexical variant. to sum up, the formula above promotes those strings with high information amount, long matches, and low ambiguity degree.

the final annotation is generated by taking the top ranked concepts that cover as many words as possible from the text chunk t. as an example, figure  <dig> shows the semantic annotations generated by the cr annotator.

knowledge resource normalization
in order to build semantic spaces for analyzing document collections, the reference ontology o associated with the knowledge resource has to be normalized into a well-structured multidimensional schema. the main issue to be addressed in this process is to manage the highly irregular structures of the kr taxonomies. with this issue in mind, the kr normalization is performed as follows:

• first a set of dimensions are defined, , which represent a partition of the concepts in the domain ontology. each dimension di represents a different semantic space , and cannot share any common sub-concept with the other dimensions.

• each dimension di can define a set of categories or levels lji, which forms in turn a partition over di but with the following constraints:  there cannot be two concepts c and d in lji such that either c ≼ d or d ≼ c, and  all the concepts in lji have a common super-concept that belongs to di. by imposing these constraints we ensure summarizability and good olap properties for the generated dimensions hierarchies.

• in order to efficiently build the dimensions hierarchies from the reference ontology o with such constraints, we index the taxonomic relationships using intervals as presented in  <cit> . this way, every concept of o has associated two sets of intervals corresponding to its ancestors  and descendants  in the ontology. by using an interval's algebra over this representation, we are able to query about the taxonomic relationships between concepts as well as to compute common ancestors and descendants. for example, let c =  and d =  be two indexed concepts. we infer d ≼ c because  <cit> - ⊆  <cit> -. similarly, we can obtain common ancestors of c and d by intersecting the intervals of the ancestors space,  ∩ .

in this way, we can automatically build each dimension di with the ontology fragment obtained with the signature formed by all the concepts identified in the collection  and that belong to some semantic group representing the dimension. to obtain the categories of a dimension di, we take into consideration the taxonomic relationships in the fragment and the previous restrictions over dimensions and their categories.

data resource normalization
after semantic annotation, each item of the target collection col has associated a list of concepts from the reference ontology o. however, these annotation sets are not suited for multidimensional analysis, and therefore a normalization process similar to that applied to the ontology must be performed. the main goal of this normalization is to represent the semantic annotations within the normalized multidimensional space described in the previous section. thus, each item d∈col is represented as the multidimensional fact:

 fact= 

where ci  is either a concept from the dimension di or the null value.

as a semantic annotator can tag more than one concept of the same dimension, the normalization process basically consists in selecting the most relevant concepts for each dimension. one issue to take into account in this process is the presence of ambiguous annotations, that is, when more than one concept is assigned to the same text chunk. we say that two concepts are in conflict when they are included in some ambiguous annotation. for example, the string "follow-up" is annotated with two concepts c <dig>  and c <dig> , and therefore they are in conflict.

the selection of relevant and right concepts for each document d is performed through a reduction process based on a concept affinity matrix md of size nc × nc, where nc is the number of distinct concepts present in the annotations of d. the idea behind this matrix is to capture the affinity of the concepts associated to each item, so that the more similar a concept is with its neighbors the more relevant it is. the affinity matrix is calculated as the linear combination of the following matrices:

 md=misa+mancs+mr+msentsd 

these matrices are defined as follows:

• misa =  <dig> iff ci ≼ cj in the reference ontology o, that is, two concepts are similar if one is a sub-concept of the other,

• manc = |common_ancestors|/γ, being γ a parameter that depends on the taxonomy depth, that is, the more ancestors two concepts share the more similar they are,

• mr =  <dig> iff ∃r ∈ o, that is, two concepts related to each other through some relation r are deemed similar,

• msentsd= <dig>  if ci and cj co-occur in a same sentence of the document d and they are not in conflict.

the affinity matrix can be used in many ways to rank the annotation concepts of an item. for example, we can use any centrality-based algorithm to obtain the concept ranking. however, our aim is not only to get the concepts ranking but also to solve the ambiguities produced by the annotation system. for this reason, we require a classification framework able to perform both tasks. the chosen framework is that presented in  <cit> , which is called regularization framework, and which models the classification as an optimization problem over graphs expressed in matrix notation as follows:

  rd=⋅-1⋅yt)t 

here, r is the calculated vector representing the rank of concepts present in the annotations of the collection item d, denoted cd. this ranking is obtained by finding an optimal smoothed function that best fits a given vector y, which is achieved by applying the laplacian operator over the affinity matrix md as follows:

 sd=d-1∕2⋅md⋅d-1∕ <dig> 

the parameter α is directly related to the smoothness of the approximation function . for disambiguation purposes, each ambiguous annotation a ⊆ cd is associated to a vector y where yi =  <dig> if ci ∈ a and  <dig> otherwise. after computing rd with this vector, we can reject all the ambiguous concepts in a whose score is lower than the maximum in rd. rejected concepts imply a reduction in the matrix md, and we can apply again the disambiguation process until no more concepts are rejected. for ranking purposes, the vector y consists of the frequencies of each concept within the item d. once the rank rd is obtained, the normalization process selects the top-scored concepts of each dimension to represent the d's fact. as an example, figure  <dig> shows the resulting fact for the example tagged text. since collection items are mapped to a set of disjoint dimension concepts in the resulting conceptual map, the relevance of each concept can be measured in terms of the items that support it. the relevance of a concept c ∈ o can be calculated by aggregating the relevance of its sub-concepts w.r.t each specific collection. formally,

 relcol=Γ∀c′∈descendantsscorecol 

where Γ is an aggregation function  and score is the function that is evaluated against the collection. the simplest scoring function is the number of hits, namely:

 scorecol=hitscol=count=c′}) 

alternatively, the scoring function can take into account the relevance of each concept in the items it appears. thus, we can aggregate the relevance scores estimated to select concept facts as follows:

 scorecol= ∑d∈col,∃i,fact=crd 

semantic bridges
a semantic bridge is any interesting association between concepts of two different dimensions. interesting associations can be derived from the facts extracted from the target data sources. figure  <dig> illustrates the notion of semantic bridge by means of an example. next, semantic bridges are formally introduced. given two dimension levels lni and lmj, belonging to dimensions di and dj  respectively, the following cube stores the aggregated contingency tables necessary for correlation analysis:

 cubecol={∣ci∈lni∧cj∈lmj} 

here ni,j measures the number of objects in the collection where ci and cj co-occur, ni is the number of objects where ci occurs, and nj is the number of objects where cj occurs. notice that ni and nj are calculated in a similar way as concept relevance. the contingency table for each pair  is calculated as shown in table  <dig> 

c


i

c¯i
c
j
i,j
i 
-n
i,j
col 
-n
i 
-n
j
the contingency table accounts for the number of observations of present  and/or absent  concepts in facts.

the measures ni,j, ni and nj are calculated as follows:

 ni,j={d∣d∈col∧fact≼ci∧fact≼cj}ni={d∣d∈col∧fact≼ci}nj={d∣d∈col∧fact≼cj} 

semantic bridges can be now calculated from contingency tables by defining a scoring function ϕ. in this way, bridges will be those concept associations whose scores are greater than a specified threshold δ:

 bridgescolϕ={)∣ϕ>δ} 

in association analysis  <cit> , scoring functions are used to measure the kind of correlation one can find between several items. traditionally, the confidence of the rule ci → cj has been used, which is defined as:

 ϕ=conf=ni,jni 

however, this measure presents some limitations. for example, it is not able to distinguish between positive and negative correlations. thus, other measures like the interest factor can be used instead:

 ϕ=if=ni,j⋅nni⋅nj 

as in a collection we can find many kinds of correlations, we use a comprehensive set of well-known interestingness measures to find all the interesting bridges between two levels. examples of these measures are log likelihood ratio, mutual information and f1-measure. more information about this kind of measures can be found in  <cit> .

one special kind of bridges are those that maximize some interestingness measure for each pair of concepts of the two compared levels. we call these bridges δ-maximum interesting pairs. these bridges will serve us for evaluating the quality of the generated bridges for different collections.

from the implementation point of view, bridges can be either pre-calculated and stored in the back-end or generated on the fly. in the former case, the pre-calculation of all the bridges for all the level combinations can result in very large data sets. in the latter case, although it makes the browser slightly slower, the calculation is only performed when drilling-down a concept, which usually involves a few new concepts, and therefore it is efficient to calculate their bridges w.r.t. the visualized concepts.

operations over the conceptual map
our main aim is to build a browsable representation of the semantic spaces defined previously. for this purpose, we define the conceptual map as a sequence of different layers that correspond to different dimensions expressed at some detail level . in this map, concepts are visualized as balls, which are placed within their corresponding layer with a size proportional to their relevance w.r.t. the target collection. concept bridges  are visualized as links between concepts of adjacent layers. conceptual maps are built from the normalized conceptual representation described previously. table  <dig> summarizes the main operations over conceptual maps.

this table describes the main operations over a conceptual map and the actions they involve in both the back-end  and the client-side .

RESULTS
the work presented in this paper has been mainly developed in the context of the european health-e-child  integrated project  <cit> . hec aimed to develop an integrated health care platform to allow clinicians to access, analyze, evaluate, enhance and exchange integrated biomedical information focused on three pediatric domains: heart disorders, inflammatory disorders and brain tumors. the biomedical information sources covered six distinct levels of granularity, also called vertical levels, classified as molecular , cellular , tissue , organ , individual , and population . to represent these levels and annotate data resources, we have selected the unified medical language system metathesaurus   <cit>  as the reference knowledge resource, which constitutes the main multipurpose reference thesaurus for biomedical research.

in this project, we developed a prototype, called 3d-browser tool, which provides an interactive way to browse biomedical concepts as well as to access external information  and hec patient data related to these concepts. the developed prototype included the uniprot database  <cit> , pubmed abstracts related to the diseases studied in the project, and the hec patient database  <cit> . recently, the external web service scaiview  <cit>  was also integrated to provide alternative protein-disease associations mined from the literature  <cit> .

the tool requirements were guided and evaluated by the clinicians participating in the hec project. moreover, the 3d browser tool was fully integrated within the workflow of other hec related tools such as the hec toolbar  <cit> , allowing selected data from the 3d-browser to be linked with real patient data. apart from the usability tests performed within the hec project, we are also concerned with measuring the quality of the visualized data. as our method mainly relies on an automatic annotation system, which can produce errors and ambiguities, we must evaluate how it affects the results shown to end-users. next sections are devoted to show use cases within the hec project, as well as the experiments carried out to measure the quality of the generated data.

prototype implementation and testing
the current prototype of our method has been developed using ajax  technologies. figure  <dig> shows an overall view of the 3d-browser tool  <cit> . it consists of three main parts, namely: 1) the configuration of the conceptual map, which contains the selected vertical levels, and an optional free text query to locate concepts of interest in the conceptual map, 2) the conceptual map itself, which contains the biomedical concepts stratified in vertical levels according to the previous configuration, and 3) a series of tabs that present ranked lists of data items associated to the selected concept from the conceptual map. in the latter, each tab represents a different data collection . there is a special tab entitled "tree" which contains all the possible levels that can be selected to configure and build the conceptual map. the levels are based on the umls semantic types  <cit>  which are grouped within the corresponding hec vertical levels as in  <cit> . the layers of the conceptual map can be defined by selecting levels of the "tree" tab or through a free text query. in the second case, only the most specific concepts satisfying the query are visualized in the conceptual map.

the visual paradigm of the conceptual maps relies on the vertical integration vision proposed in hec. that is, all the involved knowledge, data and information are organized into different disjoint vertical levels, each one representing a different perspective of the biomedical research. figure  <dig> shows the stratified view of the conceptual map based on these vertical levels, in this case individual.disease and organ. within each level, biomedical concepts deemed relevant to both the clinician domain  and the clinician information request are shown as balls in the conceptual map. the size of each ball is directly related to the concept relevance and its color indicates the operation that was performed over it, namely: green if it satisfies the free text query, red if it was expanded as a sub-concept, and blue if no action was taken on it.

semantic bridges are represented as 3d lines in the conceptual map. semantic bridges can represent either discovered co-occurrences of concepts in some target data collection or well-known relationships between concepts stated in some knowledge resource . semantic bridges can help clinicians to select the context in which the required information must hold. for example, from the conceptual map in figure  <dig> we can retrieve documents or patient unique identifiers about arthritis related to limb bones by clicking an existing bridge between the concepts arthritis and limb_bone. finally, semantic bridges have also associated a relevance index, which depends on the correlation measure we have chosen for their definition  the relevance of each semantic bridge is indicated by both its color  and its thickness. thus, the semantic bridge between arthritis and limb_bone can be considered as a strong connection. another interesting feature of the conceptual maps is the ability of browsing through the taxonomical hierarchies of the biomedical concepts . in the example of figure  <dig>  the user can expand the concepts operation and implantation ). the resulting concepts are red-colored ) and represent more specific concepts like catheterisation, surgical repair, intubation, or cardiovascular operations.

in order to manage the elements of the conceptual map a series of operations are provided in the conceptual map tools panel . these operations are split into two categories: operations to manage the whole conceptual map  and concept-related operations. the operations to manage the concept visualization involve  the retrieval of the objects associated to the clicked concept,  the expansion of the clicked concept,  the removal of the concepts of a level with the exception of the clicked concept, and  the removal of the clicked concept.

in the following paragraphs we show the functionalities of the prototype through several use examples based on some hec clinician information requests.

example 1: surgical procedures and their results in the tetralogy of fallot domain
clinicians are interested in knowing the relation between the different surgical techniques reported in the literature and the findings and results that are usually correlated to them. for this purpose, a conceptual map for the semantic levels individual.health_procedures. and individual.finding is built as shown in figure  <dig>  we can restrict the view to only repair techniques. this can be done by specifying the keyword repair in the query input field. the resulting conceptual map is shown in the figure  <dig>  the map can be further refined in order to focus on some concrete concept, for example repair fallot tetralogy, just showing the concepts and bridges affected by it ). in this case, there is just one bridge that relates the surgical technique with the outcome death. figure  <dig> shows the documents that are retrieved by clicking this bridge. notice that these abstracts likely report death causes related to tof repair.

example 2: finding potential proteins for brain tumour-related diseases
in this use case, the clinician is interested in comparing the proteins related to a disease and its subtypes. taking the brain tumour domain, the clinician specifies the concept query epilepsy without selecting any vertical level. as a result, she obtains the conceptual map of figure  <dig> which contains the concepts attack epileptic, epilepsy intractable, epilepsy lobe temporal, epilepsy extratemporal and epilepsy focal. to retrieve the proteins related to these diseases, the tab @swissprot is selected. for example in figure  <dig> the related proteins to attack epileptic are shown. the user can then get much more information about these proteins by clicking the buttons ncbi and kegg, which jump to the corresponding pages in entrez gene and kegg sites respectively. note that, the relevance of each protein entry is calculated with the frequency of the concept and its sub-concepts in the uniprot description of the protein.

example 3: immunologic factors in juvenile idiopathic arthritis
juvenile idiopathic arthritis  is an autoimmune disease, that is, the immune system attacks its own cells and tissues. the cell-surface antigen hla-b <dig> is well known to be associated with different kinds of jia and it plays an important role in its classification. moreover, male children with the hla b <dig> antigen are at significantly higher risk of developing jia. in this case, the clinician is interested in analyzing the relationships between the hla-b <dig> and the different jia subtypes, for this purpose the disease or syndrome and immunological factor semantic levels are explored. as shown in the conceptual map of figure  <dig>  hla-b <dig> plays a central role with most of the bridges associated to jia-related diseases.

example 4: location of brain tumours
this example is based on the work presented in  <cit> , which consists in retrieving patient data according to the location of the brain tumours. figure  <dig> shows the conceptual map that relates the vertical levels organ and disease. green nodes represent the relevant concepts which involve cerebellum. by using the node removal facility of the 3d-browser, we can easily focus on the cerebellum related nodes ).

evaluation of the quality of conceptual maps
apart from the usability tests performed within the hec project, we are also concerned with measuring the quality of the visualized data. as our method mainly relies on an automatic annotation system, which can produce errors and ambiguities, we must evaluate how it affects to the results shown to end-users. data quality refers to the correctness of the system-generated multidimensional semantic spaces  as well as the reduction achieved by the method. wrong and ambiguous annotations can degrade the precision of the visualization by introducing misleading or noisy concepts in the conceptual maps, whereas a poor reduction of the annotation sets will introduce a lot of noise in them. additionally, we must ensure that the reduction method captures the relevant concepts, disregarding the spurious ones.

the experiments we carried out to measure data quality have been performed over three pubmed abstract collections, one per target disease of the hec project, namely: juvenile idiopathic arthritis , tetralogy of fallot , and pediatric astrocytomas . we use as gold-standard the mesh indexes provided by pubmed for each abstract. we can consider that mesh-indexes constitute a multidimensional summary of each abstract, and that we can apply the usual assessment measures for comparing our method w.r.t. the gold-standard, namely: precision , recall  and f-score. however, before applying these measures, we need to harmonize the annotations provided by our system, which refer to umls, and those of the gold-standard, which refer to mesh. as mesh is fully included in umls, the harmonization just consists of aligning umls and mesh concepts. we consider that a umls concept cumls is aligned to a mesh concept cmesh if cmesh ≼ cmesh. notice that many concepts in umls will be not aligned to the gold-standard, for they are not related with the mesh taxonomy.

l

mesh

l

mdss

method evaluation. mesh and mdss are the number of concepts in the gold-standard and the system-generated multidimensional spaces for the three domains: jia, tof and ac. p, r and f represent the precision, recall and f-score respectively. lmesh and lmdss are the average number of concepts associated to each item in the gold-standard and the mdss respectively.

in order to see the main differences between the gold-standard and mdss representations, table  <dig> reports the distribution of annotations across dimensions. from this table, we can also notice a notable divergence between both representations, specially for chemical, drug and finding dimensions. this suggests that manual annotation has a great bias towards a few semantic types, which seem to be of special interest for pubmed users. in contrast, the concepts belonging to finding are much more frequent in the abstracts than accounted by mesh indexes.

this table shows the distribution of the semantic annotations generated by our method  and the gold-standard  for the pubmed abstract collections in the three domains: jia, tof and ac.

quality of semantic bridges refers to the interestingness of the bridges generated from the multidimensional semantic spaces. again, we use the mesh indexes as gold-standard, and we compare the bridges generated with the gold-standard and those generated with our method. for this purpose, we have selected a few combinations of dimensions for each disease collection, which are related to the query examples of the previous section. thus, for the jia collection we have selected the levels disease and proteingene. tables  <dig> and  <dig> show the best scored bridges for mdss and the gold-standard. notice that except for three bridges, both sets are completely different. the main reason for these differences stems mainly from the different nature of the underlying annotation processes. for example, the immunologic factor igg appears in  <dig> documents in the mesh representation, whereas it only appears  <dig> times in mdss. this is because the automatic semantic annotator finds more specific concepts involving igg, like "igg antigen", "serum igg", and many others. instead, mesh-based annotation unifies all these concepts under "igg". additionally, as previously mentioned, some mesh descriptors are not explicitly mentioned in the abstracts and consequently they are not regarded in the mdss representation.

best δ-maximum scored bridges between the disease and proteingene levels for the the jia domain, using our reduction method . interestingness measures used are: cohesion , confidence  and f-measure . the column s indicates the number of documents supporting each bridge.

best δ-maximum scored bridges between the disease and proteingene levels for the the jia domain, using the gold-standard . interestingness measures used are: cohesion , confidence  and f-measure . the column s indicates the number of documents supporting each bridge.

for the tof collection, we have selected the levels disease and healthprocedure, restricting them to the semantic types congenitalabonormality and therapy respectively. tables  <dig> and  <dig> show the best scored bridges for the mdss and mesh-based representations respectively. notice that in this case, bridges indicate relations between abnormalities and surgical methods applied to them. for the mesh representation, bridges always refer to "surgical procedures heart", but not to any specific technique. this is again due to the mesh-based manual annotation of abstracts, which systematically selects this concept when an abstract talks about heart surgical procedures.

best δ-maximum scored bridges between the congenitalabnormality and therapy levels for the the tof domain, using our reduction method . interestingness measures used are: cohesion , confidence  and f-measure . the column s indicates the number of documents supporting each bridge.

best δ-maximum scored bridges between the congenitalabnormality and therapy levels for the the tof domain, using the gold-standard . interestingness measures used are: cohesion , confidence  and f-measure . the column s indicates the number of documents supporting each bridge.

finally, for the ac collection we have selected the dimensions anatomy, restricted to cells, and disease restricted to neoplastic processes. tables  <dig> and  <dig> show the generated bridges. in this case, the mdss method obtains a much richer set of bridges than those generated from the gold-standard.

best δ-maximum scored bridges between the cell and neoplasticprocess levels for the the ac domain, using our reduction method . interestingness measures used are: cohesion , confidence  and f-measure . the column s indicates the number of documents supporting each bridge.

best δ-maximum scored bridges between the cell and neoplasticprocess levels for the the ac domain, using the gold-standard . interestingness measures used are: cohesion , confidence  and f-measure . the column s indicates the number of documents supporting each bridge.

concluding, our method generates interesting bridges comparable in quality to those generated from the gold-standard. it is worth mentioning that we have found very few errors due to the semantic annotation system. an example of error is shown in table  <dig>  where acls is not a disease. finally, due to the significant divergence present in the mdss and the gold-standard representations, bridges derived from them can vary greatly. future work must pay attention to the impact of the used annotation method in both the resulting multidimensional space and its generated bridges.

CONCLUSIONS
current knowledge resources and semantic-aware technology make possible the integration of biomedical resources. such an integration is achieved through semantic annotation of the intended biomedical resources. this paper shows how these annotations can be exploited for integration, exploration, and analysis tasks.

the presented approach relies on multidimensional semantic spaces and olap-style operators, which has been shown suitable for browsing biomedical information. we also show that the same knowledge resources that support the semantic annotations  provide the necessary elements to build the taxonomical dimensions that facilitate the exploration of the semantic spaces. the viability of the approach is finally demonstrated with the developed prototype , which has been tested over a real scenario.

as for the quality of the generated semantic spaces, we show that the conceptual representations of our approach are partially complementary to the representation given by mesh descriptors. the normalization process defined to accommodate the semantic annotations into the given dimensions does not suffer from quality loss. the quality of discovered bridges is usually similar or, in some cases, better than those derived from the mesh descriptors.

as future work, it would be interesting to investigate probabilistic translation methods  <cit>  for different conceptual representations, so that the quality of the semantic annotations can be further improved. for example, with these methods, some hidden concepts in the abstract that are captured by mesh descriptions could be discovered by other annotation systems. other future work will be focused on the discovery of interesting bridges using association rules algorithms. recently, we have investigated in  <cit>  the generation of rules from semantic annotations derived from patient record databases. these rules could be included in the proposed conceptual maps for exploring them as well as for comparing them to existing bridges. finally, we will investigate how to include in our approach those semantic relationships that are being extracted from the literature, as those obtained with the dido tool  <cit> .

list of abbreviations
ac: astrocytoma; ajax: asynchronous javascript and xml; calbc: collaborative annotation of a large biomedical corpus; cr: concept retrieval; dr: data resources; go: gene ontology; hec: health-e-child; jia: juvenile idiopathic arthritis; kegg: kyoto encyclopedia of genes and genomes; kr: knowledge resources; lbd: literature based discovery; mdss: multidimensional semantic spaces; mesh: medical subject headings; ncbi: national center for biotechnology information; olap: online analytical processing; omim: on-line mendelian inheritance in man; owl: ontology web language; rdf: resource description framework; tof: tetralogy of fallot; umls: unified medical language system; xml: extended mark-up language.

competing interests
the authors declare that they have no competing interests.

authors' contributions
rb designed the 3d-browser and carried out its implementation  as well as the normalization methods. ejr developed the hec use cases and designed the experiments related to them. vn implemented and adapted the ontology indexing scheme over which most of the browser operations are performed in the back-end. all authors drafted, read and approved the final manuscript.

