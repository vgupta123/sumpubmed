BACKGROUND
information from genomic, proteomic and metabolic measurements has already benefited identification of disease subgroups and the prediction of treatment responses of individual subjects, which is known as molecular profiling based patient stratification  <cit> . biomedical research is moving towards using high-throughput molecular profiling data to improve clinical decision making. one approach for building classifiers is to stratify subjects based on their molecular profiles. unsupervised clustering algorithms can be used for stratification purposes. this paper introduces significant optimisations to unsupervised clustering using four kinds of correlation methods with high-dimensional molecular profiling data , by taking full advantage of a programming model specifically designed for parallel processing of big datasets.

our motivation for optimisation of unsupervised clustering is based on our experiences in using transmart, a biomedical data integration platform that includes support for clinical and molecular data  <cit> . transmart was originally developed by johnson & johnson for in-house clinical trial and knowledge management needs in translational studies. it has been open-sourced recently. our aim is to optimise transmart so that clinicians can make use of it for faster and more confident clinical decision making. however, we have found that the performance of the r workflow currently used in transmart for preparing correlation matrices when analysing high-dimensional molecular data is sub-standard.

for example, we performed unsupervised hierarchical clustering on the publicly available multiple myeloma   <cit>  dataset taken from ncbi’s gene expression omnibus   <cit> . the dataset contains  <dig> subjects’ gene expression data produced by an affymetrix genechip human genome u <dig> plus  <dig>  array. in order to build the classifiers, the subjects are clustered using a hierarchical clustering algorithm hclust() implemented in vanilla r  <cit> . in our preliminary tests, we found that running this algorithm on a virtual machine configured with  <dig> cores and 32 gb of memory took over 6 minutes calculating a euclidean distance matrix using the function rdist() in package fields, and more than a week if performing a kendall rank correlation using the function cor(). based on these observations we concluded that the bottleneck in the hierarchical clustering algorithm was in generating the correlation matrix. clearly optimising the performance of these analyses would expedite clinical decision making. with high-throughput sequencing technologies  <cit>  promising to produce even larger datasets per subject, we expect the performance of the state-of-the-art statistical algorithms to be further impacted unless efforts towards optimisation are carried out.

in this paper our optimisation method applies on four kinds of correlation methods used to generate correlation matrices that are used by the hierarchical clustering algorithm in transmart – the pearson product–moment correlation  <cit> , spearman’s rank-order correlation  <cit> , kendall’s rank correlation  <cit> , and euclidean distance correlation. we describe a series of processing optimisations, based around the mapreduce programming model  <cit> , on the calculation of correlation matrices used for hierarchical clustering. we go on to present how our correlation matrix calculations implemented on r mapreduce package rhipe  <cit> , which works in combination with hadoop  <cit> , a robust and well-supported distributed data storage and computation framework that supports mapreduce, significantly outperforms their comparable implementations configured for distributed execution in r using snowfall  <cit>  package with rmpi  <cit> , a parallel computing package for r scripts.

methods
in data mining, hierarchical clustering is a method of cluster analysis that seeks to build a hierarchy of clusters using correlation matrices. currently there are three main types of correlation coefficient calculation algorithms: product–moment correlation, rank correlation and other dependence measurements. pearson’s product–moment correlation is a measure of the linear correlation between two variables x and y, giving a value between + <dig> and − <dig> inclusive, where  <dig> is total positive correlation,  <dig> is no correlation, and − <dig> is total negative correlation. spearman and kendall correlation methods are two examples of using a rank correlation coefficient. spearman’s rank correlation coefficient is a non-parametric measure of statistical dependence between two variables by using a monotonic function. if there are no repeated data values, a perfect spearman correlation of + <dig> or − <dig> occurs when each of the variables is a perfect monotone function of the other. kendall tau rank correlation coefficient is a statistic used to measure the association between two measured quantities using a tau test, a non-parametric hypothesis test for statistical dependence based on the tau coefficient. euclidean distance is a popular dependence measurement that differs from correlations by calculating the distance between two points according to pythagorean theorem within a metric space.

r is a widely used analysis tool for clustering and correlation calculation. many emerging parallel r packages, such as rhipe, sparkr  <cit> , rabid  <cit> , snowfall, rmpi and pbdmpi  <cit> , can be used to parallelize r processes. rhipe is a hadoop mapreduce based r package that transforms r functions into mapreduce jobs. sparkr and rabid are mapreduce packages, which works in combination with apache spark  <cit> . though hadoop performs slower than spark in several cases, such as iterative computations, hadoop is much more mature to provide a more stable performance. snowfall with rmpi is a combination of parallel packages that works in a master–slave interactive mode, where all code and data are distributed to each process within a cluster, then the code works respectively in each process, and finally the master collects the final result. while pbdmpi uses single program multiple data parallel programming model, which is not as popular as snowfall.

mapreduce is a simple processing model based around three functions that execute at two distinct stages: the mapper function, map(), transforms raw input data into a set of intermediate key-value pairs; then the combiner function, combine(), sorts and packs the key-value pairs in the map stage; finally the reducer function, reduce(), takes the related data  and merges the data together. the mapreduce task division is based around how the input data is distributed and processed by the mapper and reducer at each stage. initially the input data is split and distributed to mappers that run on different compute nodes. after emitting some key-value pairs, the mapreduce system sorts the pairs into related groups by key. these groups are then each provided as input to reducers that also run on different compute nodes. finally, the output of the reducers in collected and combined into a single output dataset.

our approach to applying mapreduce to calculating correlation matrices on gene expression data is inspired by work presented in li q, et. al.  <cit>  who used a mapreduce-like model for distributing large datasets on gpus to calculate euclidean distance. in the case of our mapreduce implementation, our input corresponds to a set of vectors, each containing the probesets intensity values for a single subject. this input set of vectors is divided into data blocks that are dispatched to different mappers, dependent on the total number of cpu cores available. in the mapper stage, each data block is copied to each reducer by emitting as a key the index of the corresponding reducer and as the value the data block. each reducer then calculates correlation values by loading the data block corresponding to the key and calculating the correlation coefficients against each data block output from the mappers.

in the example shown in figure  <dig>  each data block can contain several subjects, where in this example we show two subjects per data block. each block is loaded by a mapper, and then copied to each reducer. based on the reducer id, a corresponding data block is loaded and coefficients calculated against all other data blocks. for example, reducer <dig> loads data block  <dig> and performs a pairwise comparison with outputs from each mapper , producing coefficient values d <dig>  d <dig> and d <dig> figure  <dig> 
basic correlation matrix calculation using mapreduce.




to optimise this mapreduce model, we made further modifications to the one presented by  <cit> . we note that there is a certain amount of redundancy in the calculation of a full correlation matrix. each coefficient appears twice in the correlation matrix. looking back at the example in figure  <dig>  we can see that each of d <dig> and d <dig>  d <dig> and d <dig>  and d <dig> and d <dig> correspond to the same pair-wise calculations. to avoid this, we can compare the id of each mapper with that of the target reducer before distributing data. if the data block id is greater than the reducer id, then that particular data block distribution and correlation coefficient calculation can then be skipped. for example, in figure  <dig> we can see that mapper  <dig> does not send data block  <dig> to reducer  <dig>  this results in only d <dig> being calculated, instead of both d <dig> and d <dig>  this optimisation results in all correlation coefficients only being calculated once.figure  <dig> 
correlation matrix calculation with redundant coefficient calculations skipped.




this optimisation the coefficient calculations shown in figure  <dig> on results, however, now produces an imbalanced workload on the reducers. if we look at figure  <dig> more closely, we can see that reducer  <dig> receives a workload corresponding to one pair-wise calculation , while reducer  <dig> pairs calculations , and so forth. with this pattern of workload, the longest running reducer determines the overall performance of the mapreduce workflow. if we can balance the load on the reducers, the workload will execute fully in parallel, thus reducing the overall execution time.

to do this, we have used a matrix transformation algorithm  to balance all reducers by moving the bottom left triangular section of the correlation matrix to the top right, as shown in figure  <dig> figure  <dig> 
matrix transformation. the elements in the bottom left triangle are mapped to the top right triangle. each row represents a reducer’s load, which contains either c elements or f elements.



denote:k is the number of all the data blocks.

i is the id of a data block.

a is the average number of distance calculation per reducer, a=∑i=1ki/k.

c is the ceiling of a, c = ⌈a⌉.

f is the floor of a, f = ⌊a⌋.



algorithm pseudo:for the data block i in each mapper

if 

mapper send this block to reduceri, reduceri+ <dig> …, reduceri+f-1;

else if 

mapper send this block to reduceri%k, reducer%k,…,reducer%k.



by using this matrix transformation to balance the reducers, each reducer will process either c pairs or f pairs of data blocks, where in theory all reducers load are fully balanced and each reducer only calculate about half of the pairs in an original reducer. in the example shown in figure  <dig>  six pairs of data blocks are calculated with an imbalanced workload. after balancing using the above matrix transformation, we can see in figure  <dig> each reducer now only calculates two pairs of data block.figure  <dig> 
balanced reducers.




finally we have designed our workflow to take into account uneven numbers of available reducers to mappers. the hash function in mapreduce cannot always map mappers’ output to the expected number of reducers even if the mapper output keys are well designed. for example, if six mapper output keys are sent to six respective reducers and only three reducers are available to receive the data, this results in one or more reducer receiving a greater workload to process sequentially. to ensure all reducers are fully utilized, calculations in reducers are shifted to combiners, which read pairwise data blocks directly from the file system, that calculate the result at the mapper stage before the hash mapping, as shown in figure  <dig> figure  <dig> 
calculation in combiner. this is the mapreduce framework where all calculations in reducers are moved to combiner according to the algorithm in figure  <dig> 



RESULTS
our optimisations were tested against large datasets to validate being able to handle large studies that we would expect to see in transmart. we used three publicly available datasets: oncology   <cit>  taken from ncbi geo consisting on  <dig> subjects and  <dig>  probesets , leukemia   <cit>  consisting on  <dig> subjects and  <dig>  probesets  multmyel consisting on  <dig> subjects and  <dig>  probesets , and a breast invasive carcinoma dataset taken from tcga  <cit>  consisting of  <dig> subjects and  <dig>  probesets . we used ic cloud  <cit> , a computational testbed based at imperial college london to set up comparable virtual machine configurations for the r implementation and our mapreduce implementation.

in order to verify the universality of our new method we tested all types of correlation functions. currently, there are three main types of correlation coefficient calculation algorithms, product–moment coefficient, rank correlation coefficients and other dependence measurements. we took pearson correlation for product–moment type, spearman and kendall for rank type and euclidean distance for other dependence measures, which are implemented in r packages, r-base and fields. we compared a vanilla r instance and parallel r  against mapreduce via rhipe.

two benchmarks are used for the performance evaluation. the micro-benchmark used datasets multmyel . the vanilla r configuration used  <dig> cpu cores and 32gb of memory in a single vm. snowfall used  <dig> vms each with  <dig> cpu cores, 8gb of memory and  <dig> workers. rhipe used  <dig> vms each with  <dig> cpu cores, 8gb of memory and  <dig> mappers. the macro-benchmark used datasets oncology, leukemia and tcga. three main tests are performed using three datasets, including oncology , a cross-study consisting of oncology and leukemia , and an artificial dataset consisting of dual oncology and dual leukemia . due to the extremely long execution time of kendall correlation, only the smallest tcga data was used to calculate kendall correlation. the vanilla r configuration used  <dig> cpu cores and 64gb of memory in a single vm. the master node of snowfall used  <dig> cpu cores, 8gb of memory and  <dig> workers, with  <dig> slave vms each with  <dig> cpu cores, 4 gb of memory and  <dig> workers. the master node of rhipe used  <dig> cpu cores and 8gb of memory using  <dig> mappers, with  <dig> slave vms each with  <dig> cpu cores and 4 gb of memory using  <dig> mappers. each experiment consists of two stages: data preparation and calculation. there are five methods for comparison: vanilla r, default snowfall using socket connections, optimised snowfall using the rmpi  <cit>  package, and rhipe using both the basic mapreduce algorithm and the optimised one. the vanilla r data preparation is loading comma-separated value  data matrix from an ext <dig>  local file system to an r matrix object. with default snowfall, the data preparation overhead consists of loading the csv data matrix from an ext <dig> local file system, initializing the worker nodes, and exporting the data matrix and code to the workers. the data matrix is split by row  rather than data block, where the corresponding computation calculates the correlation between rows. with snowfall using rmpi, the data preparation overhead includes splitting the data matrix using the linux split command and copying each of the data block files to every vm. the number of data blocks depends on the number of workers. during the calculations, rmpi workers perform the same tasks as in the mappers in mapreduce. each worker loads each corresponding data block sequentially from the local ext <dig> filesystem. after each data block is loaded, the worker performs the corresponding calculations. using rhipe, the data preparation overhead consists of splitting the data matrix and uploading each file to hdfs  <cit> . the calculation then follows the algorithms described in the method section.

we carried out a performance evaluation between vanilla r, parallel r with snowfall, and mapreduce implementation. we calculated a subject correlation on the all subjects, calculating the coefficient matrices of the two benchmarks using euclidean distance, pearson and spearman correlation functions.

in the micro-benchmark, as shown in figures  <dig>  vanilla r performs fastest and default snowfall performs the slowest. vanilla r has a larger data preparation overhead than rhipe, but the calculation itself greatly outperforms all the other methods. all parallel r methods do not perform any better due to the data communication overhead. there is an extreme example in default snowfall spearman where the parallel calculation is  <dig> times slower than vanilla r. the optimised rhipe demonstrates a  <dig> - <dig>  fold increase compared to the default snowfall. the optimised rhipe conducts  <dig> - <dig>  times faster than the basic rhipe, which almost achieves the expected two times acceleration, considering all the overheads, such as data preparation and hadoop job initialization.figure  <dig> 
performance on the micro-benchmark. this is the performance evaluation using vanilla r, default snowfall package with socket connection, optimised snowfall with rmpi package and rhipe package with the basic mapreduce algorithm and our optimised one. the bottom three bars in each method shows the data preparation time. the vanilla r data preparation indicates loading data from a local file into memory; while in all parallel r methods, data copy almost occupy the whole data preparation time. the upper three bars respectively indicate the euclidean , pearson  and spearman  calculation time.



though the optimised rhipe is outperformed by optimised snowfall, it has a lower data preparation overhead. this is advantageous as rhipe is likely to be able to perform better overall with much larger datasets. thus, we utilized the macro-benchmark to further test the optimised snowfall and the optimised rhipe with vanilla r as a baseline.

in the tests using the macro-benchmark, as shown in figure  <dig>  the optimised rhipe outperforms all other methods. though the optimised rhipe calculation time is still a little longer than optimised snowfall, the optimised rhipe outperforms the optimised snowfall due to faster data transfer via hdfs and thus shorter data preparation times. in figure 7a , benefiting from the  <dig>  times faster data preparation, the optimised rhipe performs  <dig>  -  <dig>  times faster than the optimised snowfall. in figure 7b , benefiting from the  <dig>  times faster data preparation, the optimised rhipe performs  <dig>  -  <dig>  times faster than the optimised snowfall. in figure 7c , benefiting from the  <dig>  times faster data preparation, the optimised rhipe performs  <dig>  -  <dig>  times faster than the optimised snowfall and  <dig> - <dig>  times faster than the vanilla r. we propose that rhipe holds great promise for large data analysis with the data size increasing.figure  <dig> 
performance on the macro-benchmark. this is the performance evaluation using vanilla r, optimised snowfall and optimised rhipe package. the upper part of each figure indicates the total execution time. in this part, the bottom three bars in each method shows the data preparation time; while the upper three bars respectively indicate the euclidean , pearson  and spearman  calculation time. the lower part of each figure details the data preparation of each method. in this part, data split shows the time used for splitting the large data matrix into smaller pieces, data transfer for the snowfall shows data copy time for the pieces to corresponding mpi workers, data transfer for the rhipe shows the data uploading time for the same pieces to hdfs, system boot respectively shows the boot time of the mpi cluster and the hadoop cluster, and the direct load shows the data loading time for vanilla r. a: performance on oncology dataset. . b: performance on the cross-study consisting of oncology and leukemia . c: performance on the large artificial dataset .



as part of our baselines for comparison, we performed a full kendall correlation calculation in our vanilla r configuration, where we found that the total execution time was indeterminately long. we used the tcga and multmyel datasets to estimate the full time because this scaling property of this particular dataset allows us to extrapolate the total calculation time more quickly. each vector is a subject with about  <dig>  probeset values. we started from  <dig> subjects to  <dig> subjects to simulate the trend and formula, as shown in figure  <dig>  we calculated, based on the observed trends, that for the processing all of the tcga subjects the estimated execution time would be  <dig>  seconds  and for multmyel the estimated time would be  <dig> ,734 seconds .figure  <dig> 
estimation of kendall in vanilla r environment.




we successfully performed the parallel kendall correlation with all subjects of tcga using rhipe and snowfall. the total execution time of the optimised rhipe  is very similar to the optimised snowfall execution time . both of the data preparation times, less than 20 second, can be ignored comparing to the extremely long execution times. both of these parallel methods perform approximate  <dig> times faster than vanilla r. the same parallel algorithms could be applied to the multmyel and oncology datasets. this test indicates the optimised rhipe gradually downgrades to the optimised snowfall in the tests with smaller input dataset but longer calculation time.

CONCLUSIONS
in this paper, our work is aimed at creating an efficient data distribution and parallel calculation algorithm based on mapreduce to optimise the correlation calculation. we evaluate the performance of our algorithm using two gene expression benchmarks. in the micro-benchmark, our implementation using mapreduce, based on the r package rhipe, demonstrates a  <dig> - <dig>  fold increase compared to the default snowfall and  <dig> - <dig>  fold increase compared to the basic rhipe mapreduce in the euclidean, pearson and spearman correlations. in the macro-benchmark, with  <dig> - <dig>  times faster data preparation operations, the optimised rhipe performs  <dig> - <dig>  times faster than the optimised snowfall and  <dig> - <dig>  times faster than the vanilla r. both the optimised rhipe and the optimised snowfall finish the long parallel kendall correlation with all subjects of tcga within 7 hours. both of them conduct about  <dig> times faster than the estimated vanilla r. we propose that mapreduce framework holds great promise for large molecular data analysis, in particular for high-dimensional genomic data.

competing interests

the authors declare that they have no competing interests.

authors’ contributions

sw designed the mapreduce algorithm, ran the experiments, analysed the results and drafted the manuscript. ip helped to define the research theme, designed and explained the test cases, and drafted the manuscript. dj participated in mapreduce algorithm design and drafted the manuscript. ie participated in the snowfall experiments design. fg participated in the snowfall experiment implementation. ao participated in the mapreduce experiment implementation. yg defined the research theme and participated in all the algorithm design. all authors read and approved the final manuscript.

