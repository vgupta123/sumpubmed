BACKGROUND
chip-seq experiments use chromatin immunoprecipitation and then high-throughput sequencing, primarily to locate transcription factor binding sites across entire genomes, and to better our understanding of biological control systems  <cit> . as a brief overview of the relevant experimental protocols, they begin by irreversibly cross-linking a transcription factor  molecule to its binding site in genomic dna. they then shear the dna into millions of short sequence fragments. usually, the ends of a fragment are near the corresponding cross-link, but the exact distance between the end of the fragment and the cross-link is random. moreover, the fragments on the two dna strands show different systematic biases in the positions of their ends relative to the cross-link. antibodies to the tf then precipitate each tf molecule along with its attached fragment. fragments are dissociated from the tf molecules, amplified by polymerase chain reaction . the fragments are then sequenced into short subsequences, called “tags”. computational analysis then enters by mapping the tag sequences to a reference genome. if a tag sequence is long enough, the tag matches only one genomic coordinate. sometimes, however, its sequence is short and maps to more than one coordinate, making the mapping ambiguous. the possibilities of ambiguous mapping, false positive tag-reads, and other experimental errors have motivated the development of programs to analyze chip-seq experiments.

peak-calling programs locate potential binding sites as “peaks” where mapped tags concentrate. peak-calling programs use widely differing approaches, none of which has yet emerged as dominant in reviews  <cit> , because relative accuracy of programs varies with the dataset examined  <cit> . improvement is probably possible, however, because the models underlying existing programs do not consider mapping ambiguities directly, despite the existence of packages for enumerating the ambiguities, e.g., the peakseq suite  <cit> . moreover, some programs ignore strand-specific information  <cit> .

most programs use  kernel smoothing to compensate for mapping ambiguities, the most popular kernel being the uniform density, which is equivalent to counting tags in sliding windows of fixed-length  <cit> . programs also manipulate information about a tag’s strand in various ways: as mentioned, some ignore it  <cit> ; some use it explicitly ; and some use it indirectly by transposing tag locations over to the other strand . programs that combine strand information and windowing are essentially using two separate uniform densities as kernels for the forward and backward strands. for example, quest  <cit>   estimates tag densities with a kernel smoother than the uniform density, to mimic the observed shape of tag peaks. quest did not dominate in comparisons, however, perhaps because it transposes tag locations, rather than estimating two separate tag densities, one for each strand.

the significance of peaks can be reported either as the number of tags in a window, a p-value, a q-value, or a posterior probability. although p-values guide a naïve user better than tag numbers, they introduce problematic assumptions. to derive a p-value, some programs  <cit>  assume a globally uniform background intensity of tags, an assumption known to fail in chip experiments. to assign the significance of peaks, different programs use various model assumptions such as poisson  <cit> , local poisson  <cit> , binomial  <cit> , and hidden markov  <cit>  models. some programs  <cit>  use control data to account for local variations in background tag intensity or to compensate for experimental artifacts like pcr over-amplification, which can cause a spurious concentration of tags in a few specific locations. the reproducibility of control data is suspect, however, because it varies across cell types and chip protocol  <cit> . although control data mitigates some experimental artifacts, its unreliability ultimately undermines any inference based on the corresponding p-value.

mapping ambiguities can also be problematic for a naïve p-value calculation. consider, e.g., a large genomic region where exactly a locations are ambiguously mapped . in the same region, consider a window containing a total of l locations, including the a ambiguous locations. the window length is an arbitrary parameter , and as it increases, l increases. the a ambiguous locations are essentially censored data, so the simplest maximum likelihood estimate of the tag count in the window is l/ times the observed tag count. thus, if observed tag count is fixed, the estimated tag count decreases as the window length increases. if a p-value decreases with the estimated tag count, it then depends on the window length. false discovery rates  depend on p-values, so the use of fdrs does not remove the dependency. under the circumstances described, therefore, the arbitrary choice of window length influences the number of peaks reported. a recent study on the number of binding sites in a genome  <cit>  indicates that many real binding sites from chip-seq data go unreported, suggesting that the assumptions and approximations underlying current p-value estimates leave room for improvement.

intuitively, the spatial resolution of a peak should also improve as more tags contribute to it. in principle, therefore, a program should also assign errors to its location estimates, but in fact, existing programs do not infer the accuracy of their estimated peak locations.

to examine the performance of the proposed model  against current standards, we selected several programs from a recent comparison  <cit> : hpeak  <cit> , spp package   <cit> , cisgenome  <cit> , macs  <cit> , quest  <cit> , and sissrs  <cit> . a summary of selected programs is given in table  <dig>  the details of next-peak appears in the methods.

RESULTS
fitting the next-peak model to chip-seq data
using chip-seq datasets for three tfs: stat <dig>  <cit> , nrsf, and znf <dig> , results with and without mappability information were examined; for each dataset, only the better of the two are presented here. mappability information improved results for stat <dig>  but degraded results for nrsf and znf <dig> 

searches with position-specific scoring matrices from jaspar  <cit>  yielded candidates for actual stat <dig>  nrsf, or znf <dig> sites within each region with a binding event. the searches used the p-value cut-off 5×10- <dig> for all datasets. see methods for details on the p-value computation for finding motif sites. figure 1a shows a density of the normal-exponential two-peak  model . figure 1b-d displays the tag number, normalized to a probability density, for each location around the position of the candidate sites. the observed tag density is superimposed on the estimated density . maximum likelihood estimation on the next-peak model produced parameter estimates underlying λjr and λjl. table  <dig> reports estimated parameter values for each dataset.

for stat <dig>  the observed tag counts follow the density curve of the next-peak model with a small difference in terms of average trend, except for two unexplained dips . the dips  display symmetry around the binding site. the tag counts for nrsf  also follow the next-peak density with a small trend difference. the tag counts for znf <dig>  show a larger trend difference from the next-peak density, perhaps because the chip experiment was noisier.

examples of regions with unmappable locations
figure  <dig> shows three regions with large number of unmappable locations from stat <dig> data. in figure  <dig>  unmappable locations are marked by grey blocks. in figure 2a, 49% of locations are unmappable; in figure 2b, 41%; and in figure 2c, 38%. the circles indicate motif sites; the triangles, estimated sites from the next-peak model. the estimated sites approximate the motif sites reasonably well. the estimated tag counts due to the binding event are  <dig> ,  <dig> , and  <dig> ; the total observed tag counts in the region are  <dig>   <dig>  and  <dig>  although tags at unmappable locations are not observable, the next-peak model increases the corresponding estimated tag counts to compensate. the compensation permits next-peak to sharpen estimates of binding strength.

comparison of the programs: top  <dig>  peaks
to compare peak-calling programs, we run next-peak along with other popular programs like hpeak  <cit> , spp package   <cit> , cisgenome  <cit> , macs  <cit> , quest  <cit> , and sissrs  <cit>  . on running these programs including next-peak, following standard practice, we used the default parameters to ensure reproducibility. as the first stage of comparison, we consider the  <dig>  top peaks from each. the next-peak program uses the estimated tags per binding event  to rank its peaks. we considered every peak called within  <dig> bp of a candidate site  to be a true positive . our primary performance measure was the number of tps within the  <dig>  highest peaks.  our secondary performance measures considered placement of tp peaks:  the mean distance between a tp peak center and the nearest motif site, and  the mean bias between a tp peak center and the nearest motif site. a tp peak upstream of the nearest motif site contributes to a negative bias; downstream, a positive bias. thus, small distances and biases are desirable.

table  <dig> contains summary statistics for various peak-calling programs. for stat <dig>  next-peak found  <dig> tps, a full  <dig> tps more than any other program. for nrsf, next-peak found  <dig>  tps, more than any other program; mtc was the second at  <dig>  tps. for znf <dig>  next-peak found  <dig> tps, less than only macs . for all three datasets, next-peak had the smallest mean distances among all programs; it had one of the smallest biases as well. in addition, next-peak is the only program that produced small biases for all three datasets. all other programs show a noticeable bias in at least one dataset. specifically, hpeak and quest had a noticeable bias in stat1; wtd, mtc, macs and sissrs had a noticeable bias in nrsf and znf143; cisgenome had a noticeable bias in stat <dig> and znf <dig> 

comparison of the programs: top peaks in general
the previous section gives performance measures based on the top  <dig>  peaks called by each program. researchers might wish to compare the measures based on lists of top peaks with different truncations, e.g., lists truncated at rank  <dig> ,  <dig> , or  <dig> . figure  <dig> shows the precision  for top peaks truncated at ranks up to  <dig> . for each rank r in the x-axis, the precision is computed for cumulative peaks between rank  <dig> and rank r. as expected, precision generally decreased with the length of the list. for stat <dig>  next-peak had the largest precision  over the full range of lengths, up to  <dig>  peaks. for nrsf, next-peak had nearly the best precision up to  <dig>  peaks ; next-peak had the best precision between  <dig>  and  <dig>  peaks. for znf <dig>  next-peak had near the best precision up and  <dig>  peaks. for znf <dig>  macs performed similarly to next-peak between  <dig>  and  <dig>  peaks, but macs had a significantly poorer performance between  <dig> and  <dig>  peaks compared to other programs.

figure  <dig> shows mean distances for tp peaks ranked up to  <dig> . in all three datasets, for the most of the range up to rank  <dig> , next-peak had the smallest mean distances. note that other programs did not show the same consistency among three datasets in terms of mean distances. for example, mtc was the second best in stat <dig> but performed poorly for nrsf; quest was the second best in znf <dig> but performed poorly for stat <dig> 

figure  <dig> shows mean biases up to  <dig>  peaks. as noted in the previous section, next-peak is the only program showing small biases for all three datasets. any other program shows a noticeable bias in at least one dataset. that is, for znf <dig>  only next-peak, quest, and hpeak had small biases, but quest and hpeak had noticeable biases in stat <dig>  making their performances highly dependent on the dataset at hand.

correlation of estimated standard deviation and distance to motif site
unlike other programs, next-peak indicates the accuracy of a peak’s estimated location by estimating the corresponding standard deviation. figure  <dig> displays smooth scatterplots for the estimated standard deviation versus distance to the nearest candidate site.  dark regions represent high densities of data points and small dots represent isolated points. ideally, all points should fall near the line y= x. most data points, however, are concentrated at the bottom-left corner for all three plots. that is, most standard deviation estimates are small and actual distances to the motif site tend to be small as well. the pearson correlation coefficients corresponding to the smooth scatterplots are  <dig>  ,  <dig>  , and  <dig>  , indicating that although the relationship is rather weak, the estimated error is positively correlated with the actual distance between the estimated peak location and the motif site.

discussion
alone among existing peak-calling programs, next-peak analyzes data with a parametric statistical model. of the existing programs, therefore, it alone provides a principled foundation for elaborating the statistical analyses of chip-seq data. one obvious elaboration is to model multiple binding events in a region. this work is currently underway and the results will be reported elsewhere.

next-peak can estimate the average fragment length, even if the experiment does not measure the average fragment length. let d denote the tag length, e.g., d= <dig> for stat <dig>  in the next-peak model, the average distance from a fragment end to a cross-link is β. the average distance between the fragment ends is therefore 2β + d −  <dig> . for the stat <dig> dataset, β^ = <dig>  and d = <dig>  so the next-peak model estimate of the average fragment length is  <dig> , consistent with a previous estimate of  <dig>  <cit> . for nrsf data, β^ = <dig>  and d= <dig>  the estimate of the fragment length is  <dig> , and for gabp data, β^ = <dig>  and d= <dig>  the estimate is  <dig> .

existing programs simply discard ambiguously mapped reads. in contrast, next-peak explicitly models the locations where reads do not map uniquely into the reference genome. next-peak can therefore adjust for ambiguous mapping while estimating the total number of tags in a region, thereby sharpening its estimates of tf binding strength. sharper estimates of binding strength can promote better physical interpretation of chip-seq results.

existing peak-calling programs require tedious visual screening of up to tens of thousands of binding regions, to eliminate experimental artifacts like spikes in tag numbers caused by pcr anomalies. the goodness-of-fit tests in next-peak can reduce the burden of visual screening. moreover, the same tests can detect the presence of multiple tf binding sites, which are usually found in regions longer than those containing pcr anomalies. the long regions with small p-values can therefore be set aside for further, more intensive analyses, such as searching for multiple binding events or sequence motifs.

for stat <dig>  the observed tag distribution follows the next-peak density closely, indicating that the next-peak model captured the essence of the physical processes in the chip-seq experiment. consequently, next-peak outperformed its competitors, possibly because the next-peak model successfully mimicked the true experimental kernel. on the other hand, for znf <dig>  the observed tag distribution is somewhat deviated from the next-peak density, possibly degrading next-peak’s performance slightly. the observed tag density might reflect a mixture of multiple binding events, however, resulting from tf binding fluctuating between different protein complexes. mass and structural differences between the protein complexes could cause binding locations or mean fragment lengths to fluctuate. conventional motif analysis or a more elaborate model including multiple binding sites might expose the protein-protein interactions, however.

by adding mappability information, stat <dig> increased true-positive binding sites by  <dig> % on average. unlike stat <dig>  mappability information for nrsf and znf <dig> actually degraded the performance of next-peak: on average, it decreased true-positives by  <dig> % and  <dig> %, a surprising result given that both nrsf and znf <dig> had large numbers of mapped tags . the truncated read length for nrsf and znf <dig> was  <dig>  however, much larger than read length of  <dig> for stat <dig>  thus, fewer genomic locations were mapped ambiguously for nrsf or znf <dig>  than for stat <dig> , diminishing next-peak’s ability to enhance its performance by adding mappability information.

this article examined three chip-seq datasets with a single dominant binding motif, permitting motif sites to serve as surrogates for the true binding sites. in general, however, even with an antibody specific to a protein, protein-protein interactions between tf molecules might cause multiple tfs  to cross-link to an antibody. the two global parameters σ  and β  then require delicate estimation. one could select a few hundred of the most tag-rich regions. one could screen the regions visually, choosing the ones with a good fit to the dual normal-exponential density and then estimate σ and β. alternatively, one could perform a motif search on the tag-rich regions. the observed tag density for each motif then can be fit to the next-peak model. thus, next-peak can analyze any chip-seq experiment, even without specific information on the protein interactions.

CONCLUSIONS
we proposed a new statistical model for identifying binding sites from chip-seq data. the model successfully mimics the underlying data-generating process in chip-seq experiments by using the dual density of a normal-exponential two-peak model. the next-peak program produced better prediction with more true positives and a better spatial resolution than any other program tested. the next-peak program tests the validity of its underlying next-peak model without depending  on an unrealistic assumption of a global uniform background tag distribution. the next-peak program stands alone in quantifying errors by reporting a standard error for its estimates of binding intensity. moreover, smooth scatterplots showed that its standard errors are informative about errors in motif location, as estimated from external standards. the next-peak program also provides a goodness-of-fit test, automating screening of the spurious binding, and work is in progress to extend its model to locate multiple binding events in a region.

