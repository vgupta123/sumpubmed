BACKGROUND
dimensionality reduction  is the process of rendering high dimensional complex data in a low dimensional space. provided the process is calculated accurately, this low dimensional representation is preferred for use in inference and summarization for multiple reasons, among which are ease of visualization in a reduced variable space and clarity  of clustering or classification. other benefits include the insights into the data structure that can be obtained from the projected axes and the obvious denoising effect attained in some types of dr. reduction strategies often rely on linear approaches defined by a method that represents x <dig>  ..., xn ∈ ℝq as x^ <dig> …,x^n
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwg4baegaqcamaabaaaleaacqaixaqmaeqaaogaeiilawiaesojgskaeiilawiafmieagnbakaadawgaawcbagaemoba4gabeaaaaa@3557@ in such a way that each x^i
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacuwg4baegaqcamaabaaaleaacqwgpbqaaeqaaaaa@2fbc@ is obtained by projecting xi into a common linear subspace of ℝq. commonly used methods on data types relevant to bioinformatics include principal components analysis   <cit>  and classical multidimensional scaling   <cit> , which calculate linear projections of the data; clearly such projections are unsuitable for nonlinear or curved surfaces.

these methods generally are based on minimization of a global cost function, wherein large distances can drive the embedding, minimizing the effect of local distance structures  <cit> . where local data structures are not best summarized linearly , nonlinear methods that are kernel-based   <cit>  and graph theoretic like spectral embedding  <cit>  can be more appropriate. these methods attempt to model the underlying manifold by fitting a kernel parameter to optimize performance   <cit> . unfortunately it is usually necessary to re-fit one or more tuning parameter to each different data type or experiment set, making it difficult to propose a more generalized method across multiple data types. it is also difficult to avoid over-fitting the model to the data in this scenario. in addition, when attempting to determine class structure in the low dimensional space calculated from these nonlinear approaches, different classifiers may require separate spatial representations in order to appropriately partition the classes  compared to linear discriminant analysis ). such parameter modifications are optimized with a specific range of values that can be different for each classifier.

two examples of high dimensional data types that fall into this nonlinear domain include dna microarrays and image data. microarrays contain the simultaneous measurement for thousands of mrna transcripts  <cit> , which can be viewed as n arrays with q dimensions . many of the biological processes  represented by measurements generated with microarrays are nonlinear, providing a great challenge in expressing associations between biological entities in a linear domain. nilsson et al. demonstrated the importance of this concept in their comparison of mds with a nonlinear algorithm, isometric feature mapping   <cit> . isomap uses nonlinear distances as estimated in the ambient space along with a linear mds to a euclidean target projection space  <cit> . this nonlinear method was shown to render more robust partitioning of disease class structure on the low dimensional manifold, when class membership predictions were evaluated against those obtained from linear projections from mds.

images are another data type that can be represented in q dimensions as well, where each image n is a vector. this data type can exhibit comparable complexities to the microarray example, particularly when imaging tissues and organs. the euclidean distance between two similar images is seldom the optimal comparison criterion. simple variations on the main image features, such as those caused by registration issues , can alter the pixel alignment, thereby modifying the definition of distance between the original image and the rotated one, and distorting the apparent relationship. the ideal method for dr should be capable of extracting meaningful patterns in multiple data types , should not be confined to a linear domain, and should exhibit tuning parameter-fitting independence to minimize parameter optimization specific to each example and classification method.

given this goal, we examined the performance of a spectral method presented by lafon  <cit>  and have shown that it is successful in extracting meaningful structure in these two disparate data types, both having high dimensionality paired with low replication, with a method for calculating the tuning parameter that does not have to be varied across classifiers to achieve correct results. previous work by lafon has demonstrated how ordered structure from both helix and trefoils curves in ℝ <dig> can be accurately preserved in the embedded space  with a spectral method  <cit> . we extend this work to address biological examples of higher dimensionality, where accuracy in embedded results is evaluated using a known ordering and classification structure. in a more global sense, we demonstrate that the spectral method is able to preserve the implicit ordering within biological image data and can accurately classify different taxonomic species within microarray data. these results are compared to two linear approaches , one nonlinear counterpart to classical mds , and two similar nonlinear approaches , for the latter two of which numerous variations are often promoted in the mathematical/statistical literature for their successful application to a number of nonlinear data types  <cit> . we demonstrate that for our datasets, the spectral approach presented here is not dependent upon tuning parameter optimization to allow success across any of three separate classifiers chosen. this is a considerable advantage to an investigator who needs to make cross-experiment or multi-data type comparisons that benefit from a tuning parameter-independent nonlinear dr approach.

RESULTS
image dataset
the image dataset was used to test the ability of each of the projection methods to predict the correct image ordering, based on the size increment of the brain. since the largest source of variability separating each image in the series is the increase in feature surface area, as a result of the head size, only the first eigenfunction for each method was used in the comparison. this calculation reduces the dimensionality from ℝ <dig>  to ℝ <dig>  to assess the accuracy of each method, a non-parametric measure of association  was used, by which the scores from the primary eigenfunction were ranked and correlated against the correct ordering. a straight-line fit is indicative of perfect image ordering.

both the kernel pca and the spectral method from ng et al. require fitting for the smoothing epsilon term to optimize performance for the dataset. the results for this parameter optimization are provided in figure  <dig>  the maximum rho coefficient possible indicates the appropriate value for this epsilon term for each method. based upon the observation that neither line reaches a maximum value of  <dig>  it is apparent that neither method is capable of determining the correct ordering of all of the images.

the remaining methods: pca-correlation, pca-covariance, nonmetric mds, and the spectral method from lafon  <cit>  do not require parameter fitting that is dependent upon performance , so the images can be directly rendered into a low dimensional representation.

two linear methods and four nonlinear methods are considered. for both kernel pca and the spectral method from ng et al., the methods were optimized at the epsilon  values provided.

microarray dataset
the microarray dataset was used to evaluate the ability of the projection methods to accurately classify samples derived from three taxonomically separate species into their respective groups, without conducting any preliminary feature selection . in each case, the results of a method were calculated such that dimensionality was reduced from ℝ <dig>  to ℝ <dig>  thereafter three classifiers using leave-one-out cross-validation  were calculated on this projected space, both to assess the classification accuracy for each method and to compare the differences in value of the optimal parameters for kernel pca and the spectral method from ng et al. a nonlinear classifier, k-nearest neighbors , was calculated, setting k =  <dig> and k = 3; two settings of k were used since each dr method renders the groupings differently, thereby favouring two nearest neighbors for some methods and three nearest neighbors for others. the average error rates were computed across  <dig>  trials, to account for the variation arising from ties broken at random in the assignment of the nearest winning class which results in slightly different classification results for subsequent trials. in the event that the mean error rate is calculated to be greater than zero, even when one or more of the trials provide a smaller error rate, a range is denoted in the results  to indicate that the occurrence of a lower error rate is possible. another commonly utilized classifier, qda, was calculated, as well as the linear classifier counterpart, lda, to allow comparison of the dependence of the results of different classification methods on the tuning parameter fitting in the first dr step.

as was done with the image data, optimal parameters were determined for kernel pca and the spectral method from ng et al. these values were evaluated with each classification algorithm separately. in determining the most appropriate epsilon value for optimal classification accuracy, the optimal value for the spectral method from ng et al. varies according to the classifier used . there is a small window at a value of ε ~ <dig>  in which both qda  and lda  reach respective minimum total classification error rates of  <dig> % and  <dig> %. however, the knn classifiers for k =  <dig>  and k =  <dig>  require slightly higher epsilon values  to reach their respective minimum total classification error rates . it can be argued that increasing the trials of the knn classifier might better adjust this minimum point in the two knn curves, where it coincides with the qda and lda window for minimum error. however, without calculating error rates with three separate classifiers, but rather independently determining the value for a given classifier, this window would be unknown, in which case different optimal parameter values for each classifier would be suggested. for example, using lda as a classifier to determine the minimum error rate, a value of ε =  <dig>  can be chosen as the optimal parameter for the spectral method from ng et al.; however, for a knn  classifier in the same example, this parameter value would not fall within the range of the minimum error rate. instead, a value of ε =  <dig>  would provide a local minimum error of  <dig> %, as compared to the global minimum error of  <dig> %.

the parameter optimization for kernel pca shows similar trends to the optimal values from the spectral method from ng et al. however, the differences in optimal parameter solutions between the knn classifier and both qda and lda is much more pronounced with this dr method. for the qda classifier, the epsilon value is optimized to provide a minimal error rate  in the window of ε =  <dig>  –  <dig> , while for the lda classifier, the epsilon value is optimized to provide a minimal error rate  in the window of ε =  <dig>  –  <dig> . in addition, the epsilon values that provide the minimal knn error rates for k =  <dig>  and k =  <dig>  are at ε =  <dig>   and ε =  <dig>  , respectively. this result from kernel pca is consistent with those obtained from the spectral methods of ng et al., and demonstrates that each of these two nonlinear approaches have a dependence between the outcome of the classification algorithm and an appropriately optimized parameter. in addition, when comparing training set classification results to loo-cv, the optimal parameter values are drastically different for both kernel pca and the spectral method of ng et al. more importantly, the parameter selection is completely dataset-dependent. note that here the scale is vastly different from that seen in the image example, thus the optimal parameter is as well. for these examples we examined two very disparate data types, but the same conclusion of dataset-dependence would almost certainly occur if two microarray datasets were compared, since there would still be differences in distance distributions . in other work we have examined additional microarray datasets and examined functional sub categories as well as disease state, cited here for those who are interested  <cit> .

all individual classification results are summarized in table  <dig>  and the two-dimensional projections for each method are shown . for the kernel pca and spectral method from ng et al., the projection plots were generated with the epsilon terms optimized for qda . from the total error rate results reported in table  <dig>  it is apparent that the nonlinear dr methods of kernel pca and the spectral method from lafon perform more accurately than do the two linear methods  across all three of the classification algorithms . of these three nonlinear dr methods tested , the spectral method from lafon  <cit>  outperforms both kernel pca and the spectral method from ng et al. not only does the lafon spectral method project the different species into well-partitioned groups  for a 0% error rate across all classification algorithms , but the knn classifier does not exhibit any deviation in classification results across  <dig>  iterations, unlike the results obtained with any of the other methods. these results, in addition to the property that tuning parameter optimization is only dependent on the distribution of squared euclidean distances , blue 'o' symbols denote samples from gorilla , and black '*' symbols denote samples from human .  two-dimensional projection calculated with pca-correlation.  two-dimensional projection calculated with pca-covariance.  two-dimensional projection calculated with nonmetric mds.  two-dimensional projection calculated with kernel pca. epsilon parameter was selected at optimal classification using qda .  two-dimensional projection calculated with the spectral method from ng et al. epsilon parameter was selected at optimal classification using qda .  two-dimensional projection calculated with the spectral method from lafon.

*the k-nearest neighbors  classifier was calculated over  <dig>  independent trials for the mean error rates to be determined. range of error values indicate that one or more trials provided a smaller error rate, though the average was computed.

† knn k = 2: ε =  <dig> ; knn k = 3: ε =  <dig> ; qda: ε =  <dig> ; lda: ε =  <dig> 

‡ knn k = 2: ε =  <dig> ; knn k = 3: ε =  <dig> ; qda: ε =  <dig> ; lda: ε =  <dig> 

**the  <dig> % error rate corresponds to a single pan paniscus sample that is misclassified by loo-cv using lda as a classifier.

CONCLUSIONS
within these examples, the spectral method from lafon is demonstrated to extract more meaningful structure, relative to two linear and three nonlinear methods, for calculating low dimensional representations of high dimensional data types, such as image and microarray data, for purposes of determining ordered patterns or classification. as a nonlinear method it is shown to be a reasonable choice for biological and image data types, where it is important to preserve nonlinear relationships and local geometries in a low dimensional embedding. though the nonlinear methods of kernel pca and ng's spectral method also may be well suited for these data types, they suffer the primary drawback of requiring dataset- and classifier-specific tuning parameter optimization, making the validity of cross-experiment comparisons problematic. other nonlinear manifold methods, such as isomap  <cit>  and local linear embedding   <cit> , have similar optimization requirements as drawbacks, although tuning parameter optimization and classification accuracy for these two methods was not assessed here. this data fitting step can be not only time consuming, but also, as we have shown, varies according to which classification algorithm is used as well as which dataset is examined. in this work, the spectral method from lafon is shown to outperform competing methods and exhibit independence to tuning parameter fitting across three separate classifiers and two unrelated high dimensional data types. much like any method of dr, this method is not proposed to always elucidate the most meaningful structure across all high dimensional data types. methods such as boosting and bagging  <cit>  and the relative distance plane   <cit>  may be better suited for certain high dimensional datasets. rather, the results presented here demonstrate success in two disparate datasets of high dimensionality and the authors' hope is that this presentation will encourage others to extend applications of this method in research within the computational biology community.

