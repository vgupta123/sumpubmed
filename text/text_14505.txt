BACKGROUND
sequence comparison for homology detection remains one of the core tools in bioinformatics. for example, blast  <cit>  and psi-blast  <cit>  are widely used for this task, from wet biologists to bioinformaticians. thanks to those tools, more than half of the newly identified protein sequences are nowadays recognized as having homologs  <cit> . the identification of remote homologs, however, remains a challenging task because sequence divergence can prevent sequence comparison algorithms from recognizing those homologies. in order to improve the performance of sequence comparison algorithms, a possible strategy is to use data from large databases like scop  <cit> , pfam  <cit>  and cog  <cit>  in order to optimize the parameters of the algorithm to detect homology. following this strategy, we previously developed a score to compare protein sequences, called the local alignment kernel   <cit> , which in combination with a support vector machine could detect remote homology better than several state-of-the-art methods, including the smith-waterman  algorithm  <cit> , in a benchmark experiment based on the scop database. although the la kernel was used as a kernel function in combination with a support vector machine in  <cit> , it can also be independently thought of as a measure of similarity between biological sequences, based on the scoring of local alignments between the sequences. in fact it bears similarities to the aas algorithm  <cit> , hybrid alignment algorithm  <cit>  and balsa algorithm  <cit>  for sequence comparison, in the sense that all of these algorithms compute a summation of the scores over all possible local alignments , instead of computing the score of only the best alignment , as the sw algorithm does.

both the sw algorithm and the la kernel depend critically on gap parameters and on a substitution matrix  that quantifies the contribution in the score of an alignment between any two given amino acids. different substitution matrices lead to different alignment scores, and potentially to, different performance in terms of homology detection. although homology detection is the actual goal of sequence comparison, most substitution matrices used in bioinformatics have been optimized for different purposes . for example, the pam  matrices  <cit>  are based on the probability of single point mutations and the theory of markov chains. among the pam series the pam <dig> matrix, which corresponds to the  <dig> pam evolution time, is most frequently used in bioinformatics. subsequently, gonnet et al.  <cit>  and jones et al.  <cit>  applied the same method to different and larger databases, resulting in different amino acid substitution matrices . the blosum matrices  <cit>  are constructed from the blocks database of aligned protein sequences. the popular blosum <dig> matrix is constructed from the blocks of sequence segments with identity larger than 62%.

a different methodology to construct a substitution matrix has been followed by hourai et al.  <cit>  and kann et al.  <cit> . following the idea that the final goal of sequence comparison is to detect homologies, these authors investigated the possibility to automatically optimize a substitution matrix to improve the performance of the final score in terms of homology detection. this optimization, based on a training dataset of pairs of proteins extracted from the cluster of orthologous group  database  <cit> , uses an objective function that quantifies how well the final score separates the true homologs from non-homologs. homology detection is known to be particularly difficult for pairs of proteins with less than 25% sequence identity, and the main motivations for these studies are to go further in this so-called "twilight zone" by assessing the performance of homology detection as the main objective function for the optimization of the substitution matrix. the methods of hourai et al. and kann et al. differ in the objective function that is optimized. hourai et.al. try to separate the distribution of homologs from the distribution of non-homologs by minimizing the bayes error rate. kann et al. prepared a dataset of homologous pairs from the cog database and maximized the average c-value of the pairs, where the c value is designed to be large when the expected number of non-homologous sequences scoring higher than the candidate pair is small. in spite of these differences, the methods by hourai et al. and kann et al. both suffer from the difficulty to optimize the sw score with respect to the substitution matrix. indeed, the fact that the sw score only takes into account the maximum scoring alignment makes it non-differentiable with respect to the substitution matrix. as a result, the final objective function which is based on sw scores is itself not differentiable with respect to the substitution matrix, and therefore difficult to optimize. the trick used by both algorithms is to observe that the sw score of a pair of sequences is piecewise differentiable, as long as the maximum scoring alignment remains the same. hence the authors suggest to alternate both local optimization of the substitution matrix by gradient descent and computation of the best scoring alignment that depends on the current substitution matrix. a drawback of this approach is that the local moves of the substitution matrices, based on a given set of alignments, might be very different from those required to globally optimize the objective function.

this paper is devoted to the extension of these approaches to the la kernel, instead of the sw local alignment score. the motivations for this work are twofold. first, the la kernel was previously shown to be a more sensitive measure of similarity for remote homologs, suggesting that it could also remain competitive with an optimized substitution matrix. second, contrary to the sw score, the la kernel is differentiable with respect to the elements of the substitution matrix and the gap parameters, and we show below that these derivatives can be computed efficiently by dynamic programming. this means that any objective function that is itself differentiable with respect to the la kernel is differentiable with respect to the substitution matrix and can be optimized by simple gradient descent methods, without the need to alternate between the gradient descent steps and alignment steps used in the optimization of the sw score. applying this procedure to the objective function used in  <cit> , we optimized the substitution matrix as well as the gap parameters to separate true homologs from non-homologs in a dataset of protein sequences extracted from the cog database, and evaluated the performance of the resulting methods for homology detection on several independent test sets. we compared these results with those obtained after optimizing the substitution matrix with the smith-waterman algorithm  <cit> , and compared how each scoring algorithm performs with each optimized matrix.

RESULTS
pairs of homologous sequences with identity smaller than 20% were collected from the cog database and used for the training and testing of the method. for each pair, an e-value measuring the significance of the alignment score was computed, from which the corresponding confidence value c = 1/ was derived. the objective of the optimization procedure is to maximize the mean confidence value ⟨c⟩ over the training set, and its performance is evaluated by the average confidence value on the test set. in order to avoid the risk of falling into local optima, we used several amino acid substitution matrices  with default gap parameters  as starting points of the optimization. among them, blosum <dig> led to the best local optimum, and we present the performance of this optimization below.

improvement of confidence values for the sw algorithm and the la kernel
the mean confidence values ⟨c⟩ over the  <dig> training,  <dig> validation and  <dig> test pairs during the optimization procedure for both the la kernel and the sw score are plotted in figure  <dig>  the optimization was carried out on the training set until the criterion reached a maximum on the validation set, to prevent over-fitting of the parameters to the training set. the performance of this procedure was then evaluated on the independent test set. as expected, we observe that the confidence value on the training set smoothly increases during the optimization. in the case of the la kernel, a maximum is reached around  <dig> iterations in the validation set. the mean c value on the test set also seems to have reached its maximum around  <dig> iterations. the learning curve for the sw score also increases on the training, validation and test sets, and reaches a maximum after the first iteration. however, we observed that the convergence is not always as fast when starting from different substitution matrices . this figure also demonstrates that the optimization with the la kernel goes further than with the sw algorithm in terms of mean c value. figure  <dig> is the comparison of the optimized c values for the la kernel and the sw algorithm. in the graph, each point corresponds to a training pair of sequeces. this graphs illustrate the fact that the optimization process progresses further for the la kernel than for the sw algorithm, and that many pairs  reach a remarkable confidence value.

the amino acid substitution matrices as well as gap parameters optimized for the sw score  and the la kernel  on the training set are shown in tables  <dig> and  <dig>  respectively. it should be noted that the score matrix optimized for the sw score is based on kann et al.'s method, and the score matrix optimized for the la kernel is based on our method. in spite of a global conservation of most values, slight variations can be observed. to see the difference of those slight changes, we performed a principal component analysis  for each obtained matrix and plotted along the 1st and the 2nd principal components .

moreover, we calculated the average l1-distance between two matrices m <dig> and m <dig> as

d=1|a||b|∑a,b|m1−m2|,
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwgebardaqadaqaaiabd2eannaabaaaleaacqaixaqmaeqaaogaeiilawiaemyta00aasbaasqaaiabikdayaqabaaakiaawicacaglpaaacqgh9aqpdawcaaqaaiabigdaxaqaamaaemaabagaemyyaegacaglhwuaayjcsdwaaqwaaeaacqwgibgyaiaawea7cagliwoaaawaaabuaeaadaabdaqaaiabd2eannaabaaaleaacqaixaqmaeqaaowaaewaaeaacqwghbqycqggsaalcqwgibgyaiaawicacaglpaaacqghsislcqwgnbqtdawgaawcbagaegomaidabeaakmaabmaabagaemyyaemaeiilawiaemoyaigacagloagaayzkaaaacaglhwuaayjcsdaaleaacqwghbqycqggsaalcqwgibgyaeqaniabgghildgccqggsaalaaa@58c6@

where mdenotes the substitution score between amino acids a and b, |a| and |b| are the number of amino acids . the averaged l1-distances between blosum <dig> and blosum62swopt, blosum <dig> and blosum62laopt, and finally blosum62swopt and blosum62laopt, are  <dig> ,  <dig>  and  <dig> , respectively. these differences show that the optimization with the la kernel diverged further from the original matrix  than with the sw score , and that both optimizations did not necessarily go in the same direction. the matrix shown in table  <dig> is the final matrix optimized for the la kernel using both the cog training and test data. the l1-distance between this matrix and the original blosum <dig> matrix is  <dig> , and the result of pca on this matrix is shown in figure  <dig>  the overall placement of each amino acid residue was similar through the matrices, reflecting physicochemical properties of the different amino-acids. for example, charged  or polar amino acids  are placed on the left side of the figure, while non-polar amino acids  are placed on the right side. one exception is glycine , which is known to be a non-polar amino acid but is placed within the cluster of polar amino acids. this may be because of its conformational flexibility with only a proton constituting its side chain. for pc <dig>  we can observe that amino acids with rings  are placed distinctly from other amino acids.

the reason why the optimized matrix at first sight look very similar to the blosum <dig> matrix is certainly that the latter is already a very good substitution matrix extensively used by the research community for homology detection. the slight differences in the substitution matrices, however, lead to significant improvements in the mean ⟨c⟩ value.

results on independent test sets
performances of algorithms over the cog test set were evaluated for both the la kernel and the sw score in combination with both blosum62laopt and blosum62swopt. figure  <dig> shows the errors per query plot proposed by brenner et al.  <cit> , where coverage was defined as the fraction of true homologs that have scores above the threshold, and errors per query was defined as the number of non-homologous pairs above the threshold divided by the number of queries. this approach is closely related to roc  analysis: coverage can be considered as a fraction of true positives and errors per query is a rate of false positives. more "coverage" with less "errors per query" means better performance. therefore the methods with the left-most distributions in this plot can be thought of as being capable of going deeper into the twilight zone, in terms of of detecting remote homologs. the normalized area under the roc curve is also used as a common measure to compare the performance of ranking, and takes values between  <dig> and  <dig>  an roc score of  <dig> means a perfect ranking, and an roc score close to  <dig>  means almost as good as random. the roc score for each method is shown in table  <dig>  in the table, the result of psi-blast  <cit>  with the blosum <dig> matrix was shown together as a baseline method. psi-blast was trained using the training dataset only with the option "-j 2". in order to evaluate the performance of the score matrix, no large external database was used to make a profile for psi-blast, since incorporating a huge database into a profile eliminates the effect of a initial score matrix. the roc score for psi-blast with blosum <dig> was  <dig>  in the cog distant test set. for the la kernel, they were  <dig>  and  <dig>  with blosum <dig> and blosum62opt, respectively. the roc score for psi-blast with blosum <dig> was  <dig>  in the pfam distant test set. for la kernel, they were  <dig>  and  <dig>  with blosum <dig> and blosum62opt, respectively. this shows that the gain in performance resulting from the use of the la kernel instead of the psi-blast method is in the range  <dig> % –  <dig> %, while the gain resulting from the optimization of the substitution matrix with the la kernel is in the range  <dig> % –  <dig> %. overall the performance improvement resulting from the use of the la kernel with blosum62laopt over the standard psi-blast method with blosum <dig> is around 10% for the distant test sets, and almost zero for the close test sets in terms of roc score .

with these criteria, the la kernel based on the blosum62laopt substitution matrix is the most effective. interestingly, the blosum62laopt matrix is as good as the blosum62swopt matrix when used with the sw algorithm as well, although the blosum62laopt matrix was optimized with the la kernel. this highlights the fact that optimization based on the sw score encounters more difficulty in finding a good maximum than the optimization based on the la kernel. finally we observe that the la kernel outperforms the sw algorithm also in the independent pfam distant test set  with both optimized matrices, confirming its superiority over a large choice of parameters.

interestingly, although the blosum62laopt matrix and blosum62swopt matrix are optimized for the detection of remote homologs, they also performed competitively in the cog and pfam close homologs dataset . the average c value that is calculated for the whole test dataset with various optimized matrices in combination with the sw algorithm and the la kernel is shown in table  <dig>  results of an roc analysis for each method in each database are also shown in table  <dig>  we can observe that in the dataset of close homologs, the differences of performance are much smaller than those in the dataset of distant homologs. interestingly, the superiority of this method is particularly important in the case of the pfam distant test set , that is, in the detection of remote homologs. this can certainly be attributed to the fact that the training set itself  was made of distant homologs, and suggests that different substitution matrices might be optimized for different levels of sequence similarities.

discussion
the main contribution of this paper is to propose an optimization framework for substitution matrices based on an exact gradient descent method. this approach is made possible by the fact that the alignment score we consider, the la kernel, is differentiable with respect to the substitution matrix, contrary to the sw alignment score. the fact that the matrix optimized with this approach outperforms the matrix optimized with the sw score even for the sw algorithm itself suggests that there is an important benefit for the la kernel approach compared to the more heuristic nature of the optimization in the case of the sw score. it should be pointed out, however, that there is a continuum between the la kernel and the sw score  <cit> . indeed the la kernel depends on a parameter β set to  <dig>  in this study, but increasing β high enough finally leads to the sw score. in other words, the sw score, which is piecewise linear with respect to the elements of the substitution matrix , can be seen as the limit of infinitely differentiable functions. intuitively, the optimization of the la kernel can be thought of as the optimization of a smooth approximation of the sw score, which can more easily find good local optima. this suggests, in the spirit of simulated annealing, that further improvements for the sw algorithm might be obtained by optimizing the la kernel and increasing β simultaneously; however, we leave this avenue of research for future work.

it should be pointed out that fixing the scaling parameter β to  <dig>  is not a restriction in itself. indeed, although the derivative of the la kernel with respect to β can also be computed efficiently by dynamic programming, leading to the possibility of optimizing β as well as the substitution matrix and gap parameters, this would have no effect on the optimal score function that only depends on the products of β with the substitution and gap parameters. in other words, allowing β to vary would lead to an over-parameterized model. although fixing β has therefore no effect on the global optimum of the objective function, it might nevertheless have an important effect on our optimization procedure because it defines where the optimization starts. taking β =  <dig>  with default gap and substitution parameter, which were shown in previous studies to perform well on remote homology detection, is certainly a safe choice as starting point of the optimization.

a second point to be highlighted is the good performance of the la kernel as a similarity score compared to the sw score. while it was shown in  <cit>  that the la kernel outperforms the sw score as a kernel function for support vector machines, the present studies validate the relevance of the la kernel as a measure of similarity. it should be pointed out that the advantage of the la kernel over the sw score is expected to increase for remote homologs, because when the sequence identity is small the best local alignment computed by the sw score is likely not to be the correct one, and in this case multiple hits of relatively short suboptimal alignments between two sequences would be of importance, leading to the idea that averaging scores over a large number of candidate alignments might provide better evidence for homology  <cit> .

finally, let us mention that the la kernel is in fact infinitely differentiable, and its second derivative  with respect to the substitution matrix could be computed, also by dynamic programming. it would therefore be possible, in principle, to use faster gradient descent algorithms such as newton's method for the optimization. we did not follow this avenue in our experiments because this would require the computation of a  <dig> ×  <dig> hessian matrix at each iteration, which would need more than  <dig> times the amount of computation than without the computation of the hessian. of course, the hessian is of no help for the sw algorithm, because it is constantly equal to zero on the points where the sw score is differentiable.

CONCLUSIONS
we proposed a method to optimize amino acid substitution matrices for the la kernel, based on the properties of differentiability of the la kernel with respect to the substitution matrix. this is the first time amino acid substitution matrices for pairwise sequence comparison are optimized for use with the forward algorithm  <cit> . the optimized matrices exhibit good performance on distant datasets both with the sw algorithm and the la kernel, and they are competitive on close datasets. the derived matrices may be useful when standard methods fail to detect homologs.

