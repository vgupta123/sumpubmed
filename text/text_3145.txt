BACKGROUND
high-throughput next-generation sequencing  technologies from illumina , life technology , and roche/ <dig>  have evolved rapidly and are reshaping the scope of genomics research  <cit> . technical improvements have greatly decreased sequencing costs and, as a result, the size and number of datasets have increased dramatically. the lower costs mean that sequence data are being produced more often by small to midsize research groups. this trend is likely to continue as newer generation sequencing technologies keep driving costs down  <cit> . the increasing volume of data has enabled the rapid adoption of whole-genome sequencing  to enhance drug research and development, which has led to a significant increase in the need for computational methods and bioinformatics tools  <cit> . for example, deep sequencing  of the entire human genome on an illumina’s hiseq  <dig> platform typically generates billions of 100-bp short reads, and the corresponding fastq files can be as large as  <dig> gigabytes . for a wgs project consisting of  <dig> subjects,  <dig> terabytes of disk space are required to store the raw reads alone. the data storage and cpu resources needed pose a huge practical challenge for data analyses in a local environment.

fortunately, in recent years, cloud computing has emerged as a viable option to quickly and easily acquire the computational resources for large-scale data analyses  <cit> . cloud computing offers network access to computational resources where cpu, memory, and storage are accessible in the form of virtual machines . using these vms eliminates the need to build or administer local infrastructure while addressing the challenges involved in the rapid deployment of computing environments for bioinformatics. in addition, cloud computing offers machines with different hardware and software specifications, including large-memory machines, fast-cpu machines, and abundant disk space. in addition, users can select and configure vms to meet their different computational needs. more importantly, cloud computing can provide storage and computation at a far lower cost  than resources that are often dedicated to specific projects. with the massive economies of scale, cloud-computing providers are continually driving costs down. this has led to considerable enthusiasm within the bioinformatics community for the use of cloud computing for ngs sequence analyses  <cit> . several cloud-based bioinformatics applications and resources have been developed specifically to address the challenges of working with the very large volumes of data generated by ngs technology. cloud-based bioinformatics software include cloudburst  <cit> , crossbow  <cit> , myrna  <cit> , and clovr  <cit> .

our focus has been on identifying genetic variations from wgs data, mainly single nucleotide polymorphisms . to do this, all the short reads are aligned to a human reference genome and then snp calls are made. a few open-source applications are available for mapping large numbers of short reads to reference sequences, including bowtie  <cit> , soap  <cit> , and bwa  <cit> . these tools were designed initially for implementation on a single compute node or on a local workstation, and generally require a long running time even with multiple threads, making them impractical for processing a large number of samples. however, when a software program is executed in a compute cluster in which many processors work in parallel, the calculations can be completed in significantly less time. we used crossbow  <cit> , which is a hadoop-based  <cit>  parallel pipeline for genomic analysis, to search for snps using cloud computing. crossbow uses bowtie  <cit>  and soapsnp  <cit>  for alignment and snp calling, respectively. crossbow harnesses cloud computing to efficiently and accurately align billions of reads and call snps in a few hours. data from an over 35× coverage of a human genome can be analyzed by crossbow in three hours for  <dig> us dollars using a 40-node, 320-core cluster hosted in the amazon cloud. when crossbow was applied to large wgs projects in which multiple subjects were sequenced, various limitations were observed. the focus of this paper is to describe the development of rainbow to address these limitations, and to demonstrate its practical usage by analyzing the genomic sequencing data from a large number of subjects in the amazon cloud.

implementation
gaps and challenges for large-scale wgs analysis using cloud computing
a major challenge with wgs analysis in the cloud is the process of transferring large data files. the raw sequence data generated by large-scale wgs studies are generally multiple terabytes  in size. it is impractical to transfer this data to the amazon cloud via a typical network connection. amazon import is an efficient service for uploading large volumes of data to the amazon s <dig>  platform  <cit> . users can ship multiple hard drives containing their data to amazon via fedex. amazon then copies the data directly to s <dig>  this process usually takes two to three days. after the data are uploaded to s <dig>  the large files still need to be transferred between the s <dig> platform and the amazon’s ec <dig>  instance  <cit> ; this remains a practical challenge that is yet to be resolved. currently, amazon does not offer a built-in command line tool to facilitate the high-throughput transfer of large files between s <dig> and an ec <dig> instance.

crossbow uses the s3cmd command line tool to download data from s <dig>  <cit> , but s3cmd cannot handle data files larger than  <dig> gb. a typical fastq file for wgs is a few hundred gigabytes, much larger than the limit for s3cmd.  therefore, fastq files have to be split into smaller files for crossbow runs. when crossbow is executed in a cluster, it loads sequence data in multiple files in parallel to multiple nodes. thus, file splitting reduces the data transfer time when crossbow is run in a cluster. we developed a data pre-processing python script that automatically splits large fastq files into smaller files and generates the corresponding manifest files as inputs to crossbow runs.

multiple ec <dig> instances can be launched in amazon to split raw sequence data files in parallel. an ec <dig> instance might fail, crash, hang, or run away, which is the second challenge for rainbow, namely, managing and monitoring multiple ec <dig> instances in the amazon cloud. for example, when  <dig> ec <dig> instances are launched in the cloud, it is not practical to manually monitor this number of remote instances by logging into them one by one. to be useful, rainbow should be able to monitor and detect some common hardware and network failures, and respond accordingly. to manage large-scale wgs data analysis in the cloud it is necessary to keep track of the progress and status of the application’s execution, and collect and record runtime metrics such as processing times, transfer times, and file sizes. to address this issue, we developed a data pre-processing python script to log the necessary information for monitoring and to store the logs in s <dig>  the script also performs automated management to ensure no common, foreseeable errors occur; if they do, the script can handle them appropriately. take data transfer as an example. data transfer from s <dig> to ec <dig> can sometimes fail because of network congestion in the cloud. when this occurs, instead of immediately terminating the ec <dig> instance, the script waits for several minutes before re-fetching the data. other similar examples of automated management to handle common problems encountered during development and testing have been built into the script.

the third challenge for rainbow is the aggregation of snps from multiple samples in a wgs project. the outputs from soapsnp for the sequences from a single subject are chromosome-based plain text files in which each snp is annotated in detail. unfortunately, the soapsnp output is not in a standard format, making it difficult for other genome-wide association studies  tools such as plink  <cit>  to use the data. identification of snps is one of the first steps in a wgs project. other tools are used in downstream analyses to understand the significance of the identified snps. to address this problem, we developed a perl script that can aggregate all the snps from multiple samples and merge them into chromosome-based genotype files, thereby allowing the files to be used as inputs to other gwas tools.

the fourth and final challenge for rainbow is the delivery of data from sequence providers. no standard has been set for providers, so data can be delivered in a variety of different formats. raw sequence data are usually delivered as fastq files by shipping multiple hard drives to the customers. however, sequence providers might run their analysis pipelines automatically after sequencing and deliver bam files to the customers.  file; sam is a tab-delimited text file that contains sequence alignment data.) this reduces the number of multiple hard drives that are required for the data from large sequencing projects because a bam file is roughly one-third the size of the corresponding fastq file. in a bam file, all the raw reads have been aligned, but customers may want to redo the mapping using a different alignment program or a different reference genome . when raw data are stored in the bam format, the raw sequence reads first have to be extracted using picard  <cit> , and then re-aligned with bowtie  <cit>  or another alignment tool. rainbow can use both bam and fastq files as input and extracts sequence reads from bam files on the fly.

cloud as the execution environment for rainbow
many cpu hours are required to align raw reads in large datasets to a reference genome. for a subject with 60× sequencing coverage, it takes about two weeks to map  <dig> billion reads using bowtie  <cit>  on a local linux machine without parallel processing. on a single machine, it would take about three years to align all the reads in a wgs project consisting of  <dig> subjects. in principle, it is possible to set up a local high-performance computing cluster to meet the computational and storage challenges of large-scale wgs data analysis; however, this option is not always available. furthermore, this option cannot be scaled up or down quickly to meet the needs of different sequencing projects. cloud service providers give customers on-demand access to a wide range of cloud infrastructure services irrespective of the size of the data, and charge only for the resources that are actually used. cloud service providers offer virtually unlimited storage and cpu resources, and provide a computational environment that is ideally suited to large-scale wgs data analyses.

we chose to build rainbow using the amazon web services as the cloud provider for the following reasons:  s <dig> centralizes data storage, including inputs, intermediate results, and outputs from every computational step in rainbow, and stores the data permanently. a variety of tools are available to access the data stored in s <dig>  such as s3fox, a web browser plug-in  <cit> , and s3cmd, a command line tool  <cit> . s <dig> provides virtually unlimited storage space for cloud computing, and data in s <dig> are stored as multiple copies. multiple data copies and redundancy guarantee the safety of data in amazon’s cloud. the objects in s <dig> seem to belong logically to the same folder, but they are not necessarily on the same physical device . all objects in s <dig> have unique identifiers and can be fetched in parallel without input/output  congestion. this parallelism is critical when multiple instances or clusters are uploaded to or downloaded simultaneously to files in s <dig>  and ec <dig> instances or clusters requested by the user can be released after the computational tasks are complete, and the user no longer needs to pay for those resources.

manifest file and name convention makes the use of rainbow easy
to support the centralization of all data files and to manage different types of files in s <dig> in an automated fashion, we used a master manifest file and naming convention. for a large wgs project, the manifest file is the only file that a user needs to prepare to run rainbow. a master manifest file is a plain text file used to describe all subjects in a wgs project. each subject has a corresponding entry in the manifest file. the format for each record is:

test <dig>    s3://test.bucket/test <dig> bam s3://test.results/

test <dig>    s3://test.bucket/test2_ <dig> fastq; s3://test.bucket/test2_ <dig> fastq s3://test.results/

each entry consists of three fields separated by spaces or tabs:  a unique identifier;  locations of the raw reads in s3; and  an output folder in s <dig>  the raw reads can be in either bam or fastq files. the naming convention that we have used, together with the unique identifier for each genome, control how all the intermediate and result files are named, where they are stored, and how they are logically organized in s <dig>  each individual step in the rainbow workflow uses this single manifest file as input, thus guaranteeing that all files are named and stored consistently. this process ensures that naming conflicts or overwriting of any file associated with another subject is avoided.

merging snp outputs for multiple subjects
individual snp records generated using soapsnp are in soapsnp output format, namely, one snp per line with several tab-separated fields per snp. the fields include snp coordinate information, subject genotype, quality score of subject genotype, the best base and the number of uniquely aligned reads corroborating the best base, and the second best base and the count of all aligned reads corroborating the second best base. for each subject, the snps are organized into one gzipped result file per chromosome, and the snp records are sorted based on their coordinates on the chromosome. we developed a merge-sort-based algorithm to merge the snps chromosome by chromosome. when merging snps from multiple subjects, we wanted to ensure that only high-quality snps were retained for downstream analysis. the criteria that we used to define a quality snp were:  the “quality score of subject genotype” attribute is greater than  <dig>  giving at least 95% confidence that the genotype is correct. the number  <dig> was calculated using the formula − 10 * log; and  at least two uniquely aligned reads corroborate both the best and the second best bases. during the snp merging process, there is no need to load all the snps into memory at once. as a result, our algorithm has a very small memory footprint, and can merge snps from a very large number of subjects. the merged genotypes are stored as plain text files with one row corresponding to one snp marker, and with each column corresponding to one sample.

RESULTS
description of rainbow
the workflow of rainbow is shown in figure  <dig>  a user first ships multiple hard drives to amazon via fedex, and amazon copies the data to s <dig> directly. this process typically takes two to three days. after the bam or fastq files have been uploaded to s <dig>  they can be processed in parallel by launching multiple ec <dig> instances or clusters in the cloud. when the analysis is complete, the results can be downloaded directly or exported back via amazon export, which takes an additional two to three days.

there are four major steps in wgs data analysis . step  <dig>  the data pre-processing step, automates  the extraction of raw reads from bam files;  the splitting of large fastq files into smaller files; and  the generation of manifest files as inputs to crossbow. in step  <dig>  some user data are passed to the ec <dig> instance through the cloud-init mechanism  <cit> . this user data is an executable shell script that is responsible for downloading the pre-processing python code from s <dig>  the python script is responsible for software installation, system configuration, fetching data from s <dig>  extracting raw reads, splitting files, and uploading all the results to s <dig>  steps  <dig> and  <dig> are performed by crossbow and are responsible for mapping reads to the reference sequence and for snp calling. a perl script is used to parse a master manifest file, prepare the crossbow command line, and launch the crossbow run in the cloud for each subject. step  <dig> uses a perl script that was developed to consolidate the snps for all samples.

a practical test run
we applied rainbow to analyze the  <dig> subjects listed in table  <dig>  all  <dig> subjects were pair-end sequenced on illumina hiseq  <dig> platforms. the estimated insert size was approximately  <dig> bp. each subject generated  <dig> – <dig> billion 100-bp short reads. the largest bam file was  <dig> gb, and the corresponding fastq files were  <dig> gb . sequencing coverage ranged from  <dig> to 60×. all the raw data were in bam format and delivered to us on four  <dig> tb hard drives. we shipped the hard drives to amazon’s import service and loaded the data files to s <dig>  then, we launched  <dig> m <dig> large instances in the amazon cloud to pre-process the raw data in parallel and to upload all split files to s <dig>  after data pre-processing,  <dig> clusters were launched in parallel in the cloud to align the reads and make snp calls. finally, the snps from all  <dig> subjects were merged and a master chromosome-based genotype file was generated for further analysis.

ahomo_snps are snps where both alleles are the same but different from the reference.

bhetero_snps are snps where one allele is the same as the reference and the other allele is different.

chet2_snps are snps where both alleles are different from the reference, and different from each other.

the total cost of shipping the four  <dig> tb hard drives was  <dig> us dollars per  <dig> tb hard drive including   <dig> us dollars for fedex shipping,  a flat  <dig> us dollars charge per device, and   <dig> us dollars for the data loading fee. the additional charge for data loading was  <dig>  us dollars per data-loading-hour, and the actual cost is dependent upon the i/o speed of the storage device and the data size. it took one business day to ship the hard drives to amazon by fedex, and amazon started to upload the data within  <dig> h of receiving the hard drives.

the running environments were as follows. for step # <dig>  we chose the amazon m <dig> large instance, which has two cpus,  <dig>  gb memory, and two  <dig> gb instance drives. for each instance, an extra  <dig> gb ebs   <cit>  volume was requested to store the raw bam file. when running picard  <cit> ,  <dig> gb memory was allocated to the java virtual machine. picard read the bam file from ebs, and output pair-ended sequences into two fastq files  stored in the two instance drives. when splitting files, fastq_ <dig> and fastq_ <dig> were processed in parallel because an m <dig> large instance has two cpus, and fastq_ <dig> and fastq_ <dig> were logically in different instance drives. all requested resources  of the m <dig> large were used fully, which made it the optimal choice. for steps  <dig> and  <dig>  each compute cluster had  <dig> c <dig> xlarge nodes as recommended by the crossbow developers. each c <dig> xlarge node has eight cpus,  <dig> gb memory, and  <dig> gb instance storage.

the performance of rainbow is summarized in figures  <dig>   <dig>   <dig>   <dig>  the relationship between the download time and bam file size is shown in figure  <dig>  outliers of the trend line reflect fluctuations of network traffic. downloading was performed using boto  <cit> , an open source python tool for easy connection to the amazon web service. on average, a data transfer speed of  <dig>  gb per min was achieved. picard  <cit>  comprises java-based command-line utilities that manipulate bam and sam files. one of the utilities was used to extract raw sequence reads from bam/sam files. the more reads in a bam file, the longer it takes to extract them, and the linear relationship between picard processing time and the number of reads represents this . it usually takes one to two days for picard to complete a run on an m <dig> large node; it would not have been faster if another more powerful ec <dig> instance had been chosen. in addition, there is no way to run picard in parallel to process a bam file. the output of a picard run is two large fastq files. it takes another eight to  <dig> h for fastq file splitting, compression, and uploading to s <dig>  step  <dig> will of course take much less time if the raw reads are in fastq files rather than in a bam file.

steps  <dig> and  <dig> are much more time consuming than step  <dig>  but they can be completed in a much shorter time in a cluster with multiple nodes . in a 320-cpu cluster, the alignment of billions of reads takes between  <dig>  and  <dig>  h, whereas, on a local resource, it could take up to  <dig> days. the linear relationship shown in figure  <dig> is accurate because the sequence data blocks in the hdfs   <cit>  were physically local to the nodes that processed them, which reduces virtual i/o delays. crossbow runs rarely failed because the hadoop-based cluster was built to run on commodity hardware, and hadoop has built-in mechanisms for failover and disaster recovery. a hadoop-based cluster not only reduces the running time by processing the data in parallel, but also significantly improves the robustness of applications. the soapsnp running time  ranged from  <dig> to  <dig>  h, which overall, was a little longer than step  <dig>  the numbers of snps identified from the  <dig> subjects are listed in table  <dig>  on average, about  <dig> million snps were identified from a single subject. after combining the snps from all  <dig> subjects, roughly  <dig> million unique snps were obtained. it was very rare for both alleles to be different from the reference sequence, indeed, only a few thousand snps per subject fell into this category .

on average, it cost less than  <dig> us dollars to analyze each subject, and the total cost for analyzing  <dig> subjects was around  <dig>  us dollars, including data import. all ec <dig> instances and clusters are terminated immediately after the jobs on them finish, and the large amount of data in s <dig> can be deleted after data export to reduce continual charges. no upfront investment in infrastructure is required and there are virtually no additional administrative costs involved in using the amazon web service. more important than the cost is the ability to scale rainbow up or down so that the analyses are accomplished in a short time. currently, we are working on a large whole-genome sequencing project in which  <dig> subjects are sequenced. rainbow will be able to process the data from these  <dig> subjects in less than two weeks, including the physical data import and export with amazon. compared with the  <dig> years or so it would take to process this number of subjects sequentially on a local machine, the time savings of parallel processing enabled through rainbow are obvious.

discussion
rainbow was built on crossbow, but the complexity of the crossbow command line is hidden, which facilitates its use for large-scale wgs analysis in the cloud. rainbow has been well tested, and has proven to be robust and scalable. the implementation is open-source based, and is available for third-party deployment and use. illumina hiseq  <dig> and  <dig> are currently the dominant sequencing platforms, and accordingly, the default parameters in rainbow have been optimized and finely tuned for data generated by these platforms. user can tailor the parameters for other platforms, but this is rarely needed in practice. for human wgs data sequenced on an illumina hiseq  <dig> or hiseq  <dig> platform, rainbow can be used straight out of the box.

the low cost of whole genome sequencing has led to the rapid adoption of wgs for drug research and development. to understand the relationship between snps and disease better, and to obtain insights into the relation between snps and drug response, large-scale sequencing projects are continuously being initiated in academic institutes and drug companies  <cit> . these sequencing projects need high-performance computing capabilities for wgs data analyses. as we have shown, cloud computing drives down infrastructure costs both up-front and on an on-going basis, and offers operational advantages, such as setting up infrastructure in minutes rather than months, completing massive computational projects with a large number of resources quickly, and scaling the architecture up and down to provide the required computational resources.

analyzing large datasets in the cloud is different from performing the same analysis in a local environment. by implementing and running rainbow in the cloud, we have learned many valuable lessons. here we summarize what we have learned while developing and testing rainbow.

•boot time should be taken into account when new resources are starting up. it is good practice to give cloud providers 10– <dig> min before attempting to use a newly requested resource.

•it is not trivial to move large datasets around in the cloud. users should be prepared to handle network congestion or failures. when data transfers fail, it is advisable to wait for 5– <dig> min before retrying.

•cloud providers typically offer a variety of compute instances to choose from. it is necessary to understand the bottleneck  for the algorithm that is to be run, and choose the best option accordingly.

•when large amounts of data are moved between cloud resources, it is essential to ensure that they are in the same region or data center.

•it is difficult to debug workflows in the cloud without heavy logging.

CONCLUSIONS
we have described the motivation and implementation of rainbow for large-scale wgs data analyses in the cloud. rainbow has the capacity to process more than  <dig> subjects in two weeks using the amazon web service, including physical data import and export with amazon. the average cost to process a single sample in the cloud was less than  <dig> us dollars. in essence, rainbow is a wrapper of crossbow, which can handle additional challenges in large-scale wgs data analyses. compared with crossbow, the main improvements of rainbow include the ability  to handle bam as well as fastq files as inputs,  to split large sequence files for better load balance downstream,  to log the running metrics in data processing and monitoring multiple ec <dig> instances, and  to merge soapsnp outputs for multiple subjects into a single file to facilitate downstream gwas studies. rainbow is scalable and easy to use.

availability and requirements
rainbow is available for third-party use. because it was built for use with amazon web services, users need to first set up an amazon account before launching rainbow from a linux machine. the source code for rainbow is freely available for download; however, users need to pay amazon to run analyses in the amazon cloud. detailed instructions on using rainbow are available on the rainbow website   <cit> .

abbreviations
vm: virtual machine; ec2: elastic compute cloud; s3: simple storage service; ebs: elastic block storage; hdfs: hadoop distributed filesystem; wgs: whole-genome sequencing; ngs: next generation sequencing; sam: sequence alignment map; bam: binary version of sam; snp: single nucleotide polymorphism.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
sz designed, implemented, tested rainbow, and wrote the manuscript. kp implemented pre-processing, performed tests, and collected results. ls helped with amazon data import/export and performed tests. tm contributed to discussion in implementation. ej offered computational resources for tests. kp, hf, and ss participated in writing the manuscript. all authors read and approved the final manuscript.

