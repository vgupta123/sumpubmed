BACKGROUND
with so many biology papers being published each month, researchers have a difficult time keeping track with the latest developments or finding details that were not important when the paper was published. automated information extraction and retrieval are thus important tools for biologists . the textpresso text-mining engine has made progress in this direction with an ontology that marks up the biological concepts within the full-texts of caenorhabditis elegans papers  <cit> . using a simple ontology, we found that search efficiency was improved 3-fold when looking for two uniquely named genes and a term that means an interaction. here we explore the prospect of further utilizing the ontology to aid performance when using an algorithm to classify papers.

within the field of information retrieval, text classification provides the means to drastically improve the efficiency of researchers. hierarchical paper taxonomies allow users to focus on the topics and quickly locate papers of interest. in addition, taxonomies allow users to find papers that are similar. a well known taxonomy, yahoo's directory, analogously allows users to quickly find internet sites of interest, by first descending down a topic tree.

many recent developments in the information retrieval field have focused on clustering ephemeral search results – live clustering of search results from general web searches  <cit> . here we investigate the specific problem of clustering biology papers. a sizable number of biology papers are published every month , so automated procedures are necessary for a successful categorization of papers. on the other hand, dickman noted that machine learning algorithms did better when more human-crafted rules were involved  <cit> . thus, one of the main focuses of this clustering engine is to allow more human guidance than current state-of-the-art systems to obtain higher quality results.

support vector machines , with their strong theoretical foundations on structural risk minimization, have become popular tools in classification. the svm algorithm works by learning a separating hyperplane that divides two groups of vectors. this separation requires that the text documents be represented as vectors, but this problem has been tackled in numerous classification and clustering algorithms. generally, each word in the vocabulary of the corpus becomes a dimension, and a vector represents the number of occurrences of the respective words in the document. word-stemming is utilized so that words such as "cell" and "cells" are counted together. joachims showed that support vector machines could classify papers more accurately than previous algorithms  <cit> . support vector machines work well for text classification since there are many words in the vocabulary, yielding a high-dimensional vector space. at the same time, each paper might only use a small subset of the thousands of words in the vocabulary of the corpus. support vector machines are thus well suited for such document vectors that are sparse but contain dense concepts .

while support vector machines are powerful and allow the user much control of the classification, creating many subcategories and finding associated training papers would be prohibitively expensive in terms of human effort. thus, another method must also be employed to more autonomously create categories and assign the documents to them. conventional clustering algorithms, such as k-means, separate documents by vector representations, similar to those used in svm, and then attempt to label the clusters. in contrast to such algorithms, recent research on clustering has focused on creating meaningful labels for the clusters, often by extracting phrases to represent underlying concepts  <cit> . such algorithms are an active field of research, but many of the most successful algorithms are closed source. for example, vivisimo has one of the best commercial clustering engines, capable of extracting a hierarchy of concepts and classifying a variety of documents, from search engine results to abstracts of scientific reports  <cit> . while many of the recent systems focus on clustering on-the-fly search results, our system performs the classification as a background task. this allows our system to utilize system resources more extensively without runtime as a constraint. for example, we can utilize the full text of the papers instead of just the abstracts. the human-based guidance that our setup involves also provides more control of the produced taxonomy, to help ensure that the results match what a user would expect.

beil et al. described an algorithm to hierarchically classify documents based on frequent term sets  <cit> . their system provided an intuitive way for users to understand the contents of the clusters. for example, the top clusters for a corpus of documents on the beach may be: "sun", "fun", "beach", and "surf." the second layer of clusters would be: "sun, fun", "surf, beach", etc. such annotations marked a step forward from conventional clustering algorithms that would first cluster the papers and then attempt to label the clusters, usually producing labels that are difficult for humans to understand.

RESULTS
we developed a document classification engine that can classify papers into a topic-based hierarchy. the engine is currently optimized for caenorhabditis elegans papers by using human-created rules. the rules are in the form of a list of terms associated with each topic. the engine classifies all articles with the full-text available from textpresso, which currently has over  <dig> such papers. by combining two different methods, the classification results are more useful than conventional algorithms while minimizing the amount of manual curation needed. support vector machine-based classifiers assign the documents into nine primary categories, and a phrase-based clustering process creates a hierarchy of up to  <dig> subcategories for each primary category and labels the papers with these finer descriptions. the classification engine outputs the taxonomy as a large number of html files, which allow users to intuitively parse through the hierarchy to find the papers belonging to a biological concept. figure  <dig> presents an example of the interface that users browse through to find topics of interest.

classification by support vector machine
the initial classification is done by support vector machine, which assigns each paper into at least one of the nine main categories. the nine categories were taken from the chapters of wormbook: genetics/genomics, molecular biology, cellular biology, sex determination, developmental control, signal transduction, neurobiology and behavior, ecology and evolution, and wormmethods  <cit> . the germline chapter from wormbook was excluded, since the category would contain too few papers . these nine categories divide the nematode literature in a manner with which biologists are familiar, guaranteeing that the top layer of classification matches the expectations of users.

support vector machines learn a separating hyperplane that separates two groups of vectors. following conventional practice when representing text documents as vectors, each dimension of the vector space represents a word in the corpus. when creating the vector representations, words that are not useful are skipped. most of the rules, such as skipping stopwords and words that occur in too many or too few of the documents, apply to all text domains. a few domain-specific rules are utilized: when counting the number of occurrences of words in a documents, words that are in human-created lists of important words are multiplied by a boosting factor to increase the classification performance. the domain-specific rules for skipping words and the boosting improvement are described in methods.

since a paper can belong to more than one of the nine categories, the multi-class classification is done with nine runs of one-against-all classification for each paper. one-against-all refers to the fact that each svm classification decides whether a document belongs to a category or does not belong. in addition, the svm step forces each paper to belong in at least one category, since the assumption is that all papers in the c. elegans textpresso corpus discuss the biology of nematodes.

the training set currently consists of  <dig> examples, but a training document may be a positive example for more than one category. using 10-fold cross-validation, we achieved micro-averaged precision of  <dig> % and micro-averaged recall of  <dig> % . 10-fold cross-validation refers to dividing the training set into ten groups of  <dig> documents and then treating a different group as the test set during each of the ten runs. while the cross-validation performance looks low, the decision to allow a paper to belong in multiple categories would give even a human difficulty in classification, since there is no strict threshold for the extent to which a paper must discuss a category before being assigned to it.

the distribution of the papers, which is shown in table  <dig>  appears reasonable to the expected number of articles that should discuss each topic. for example, many papers discuss cellular processes or structure, so the cell biology category is expected to be large. the wormmethods category is somewhat larger than expected because it also functions as a catch-all for worm papers that did not belong elsewhere. after svm, the mean number of categories per paper is  <dig>  with standard deviation of  <dig> .

the output of the svm step serves directly as data to feed into the next step. since there can be more than a thousand papers assigned to a category after svm and such a number could easily be overwhelming, users are not given an interface to browse only the results of the svm process. instead, the clusters produced by phrases found within each category serve to further divide the papers within each of the nine main topics.

clustering by frequent phrases
the key step in clustering is to find phrases that can serve as descriptive labels for the clusters. from informal analysis, phrases of two and three words were found to be descriptive and represented many biology topics. thus, for each of the nine categories, the most frequent two and three word phrases are automatically mined and considered as the possible clusters for the respective categories.

a distinguishing aspect of clustering biology literature is which types of phrases are preferred. in general searches, proper nouns, such as names of people and places, are highly descriptive and useful to quickly locate specific information. for biology researchers, in contrast, there already exist tools to locate papers by specific authors and biological topics are preferred over names. thus, the mark-up provided by textpresso is used to automatically decide which phrases are likely to represent concepts. furthermore, each of the nine categories has a list of manually curated words that should be emphasized in the clusters. for example, the genetics category emphasizes the words "transposon", "homolog", "repeats", etc. these lists of words are used as human guidance to help the algorithm choose which phrases to use as potential clusters. the lists vary from  <dig> words for the smaller categories up to  <dig> words for the wormmethods category, so the combined human effort to construct all the lists is minimal. both the lists of words and the rules that utilize the textpresso markup are not necessary to the function of the system. they should be modified for enhanced clustering quality if using this system on a new corpus that has been marked up by textpresso.

after the program automatically checks that the phrases are not mentioned in too many papers in the category, parent-child relationships are found via a subsumption algorithm similar to what sanderson and croft describe   <cit> . each cluster can have at most one parent to ensure an acylic graph.

at the risk of creating extraneous clusters, the preference is that there exist enough clusters so that the leaves of the hierarchy are sufficiently descriptive to be useful. all papers are assigned to at most four clusters in each category, and papers that do not contain any of the phrases for a category are excluded from the classification for that category.

despite the customizations that tailor the engine to cluster biology literature, the overall process can create a topic hierarchy from general search results. as a proof of principle, a program to cluster the top  <dig> results for a given query from yahoo was created. the rules specific to caenorhabditis elegans papers are ignored , and a few rules specific to snippets from search engines are inserted: words such as "free" and "welcome" are added to the list of words to skip. an online interface for the clustering engine is available  <cit> .

testing
we used the reuters  <dig> test-set as a benchmark to compare the quality of the clustering engine against previously published results. this set consists of  <dig>  news articles from  <dig> that were later indexed by humans and is freely available for download  <cit> .

beil et al. used a subset of  <dig> articles from the reuters  <dig> set  <cit> . this subset consists of those articles that were manually tagged to belong to exactly one topic.

a common way to measure clustering quality among a hierarchy is to use the f-measure. first, the recall and precision for a given topic tk and cluster cj must be defined. the recall refers to the ability to retrieve all expected results while precision represents the portion of returned results that are correct. for each cluster and topic, the documents tagged with that topic represent the set of expected results and the cluster acts as the results returned.

let nj,k denote the number of documents in cluster cj that also belong to topic tk; that is, the number of correct results.

recall=nj,k#tk     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiegacqwfsbgucqwflbqzcqwgjbwycqwghbqycqwgsbabcqwgsbabcqggoaakcqwgubavdawgaawcbagaem4aasgabeaakiabcycasiabdoeadnaabaaaleaacqwgqbgaaeqaaogaeiykakiaeyypa0zaasaaaeaacqwgubgbdawgaawcbagaemoaaomaeiilawiaem4aasgabeaaaoqaaiabcocajiabdsfaunaabaaaleaacqwgrbwaaeqaaaaakiaaxmaacawljawaaewaceaaieqacqgfxaqmaiaawicacaglpaaaaaa@4a25@

precision=nj,k#cj     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaaiegacqwfqbaucqwfybgccqwglbqzcqwgjbwycqwgpbqacqwgzbwccqwgpbqacqwgvbwbcqwgubgbcqggoaakcqwgubavdawgaawcbagaem4aasgabeaakiabcycasiabdoeadnaabaaaleaacqwgqbgaaeqaaogaeiykakiaeyypa0zaasaaaeaacqwgubgbdawgaawcbagaemoaaomaeiilawiaem4aasgabeaaaoqaaiabcocajiabdoeadnaabaaaleaacqwgqbgaaeqaaaaakiaaxmaacawljawaaewaceaaieqacqgfyagmaiaawicacaglpaaaaaa@4e50@

where #tk denotes the total number of documents with this topic and #cj denotes the number of documents that belong in this cluster.

the f-measure for the given pair of topic tk and cluster cj, is then

f−measure=2*recall*precisionrecall+precision     
 mathtype@mtef@5@5@+=feaafiart1ev1aaatcvaufkttlearuwrp9mdh5mbpbiqv92aaexatlxbi9gbaebbnrfifhhdyfgasaach8aky=wiffydh8gipec8eeeu0xxdbba9frfj0=oqffea0dxdd9vqai=hguq8kuc9pgc9s8qqaq=dirpe0xb9q8qilsfr0=vr0=vr0dc8meaabaqaciaacagaaeqabaqabegadaaakeaacqwggbgrcqghsislcqwgtbqbcqwglbqzcqwghbqycqwgzbwccqwg1bqdcqwgybgccqwglbqzcqggoaakcqwgubavdawgaawcbagaem4aasgabeaakiabcycasiabdoeadnaabaaaleaacqwgqbgaaeqaaogaeiykakiaeyypa0zaasaaaeaacqaiyagmcqggqagkcqqgsbgucqqglbqzcqqgjbwycqqghbqycqqgsbabcqqgsbabcqggqagkcqqgqbaucqqgybgccqqglbqzcqqgjbwycqqgpbqacqqgzbwccqqgpbqacqqgvbwbcqqgubgbaeaacqqgsbgucqqglbqzcqqgjbwycqqghbqycqqgsbabcqqgsbabcqghrawkcqqgqbaucqqgybgccqqglbqzcqqgjbwycqqgpbqacqqgzbwccqqgpbqacqqgvbwbcqqgubgbaagaaczcaiaaxmaadaqadiqaagqabiab=ndazagaayjkaiaawmcaaaaa@707f@

the range of the f-measure falls between  <dig> to  <dig>  with  <dig> indicating the best quality for a cluster. the overall f-measure for the entire hierarchy is the weighted average among the topics, choosing the cluster for each topic that gives the highest f-measure, i.e., it is the summation over all the topics, adding the f-measure from the best cluster for that topic multiplied by the number of documents that are known to belong in that topic divided by the total number of documents.

beil et al. obtained an overall f-value of  <dig>   <cit> . the clustering engine from this project obtained an improved f-value of  <dig> , but the key advantage conferred by our system is that the produced clusters, labeled with phrases, are easier to understand than clusters annotated by term sets. for example, one of the top-level clusters our system produces on the reuters set is "last year", with child clusters such as "trade surplus" and "trade deficit". these types of relationships between phrases would not be possible under beil's system because their system finds term sets, and the next layers must include the previous layers: as illustrated in the background section, the top clusters for a corpus of documents on the beach may be: "sun", "fun", "beach", and "surf." the second layer of clusters are then constrained to be: "sun, fun", "surf, beach", etc.

as another comparison, larson et al. obtained f-values that varied around  <dig>  using a clustering engine that is designed to run quickly and scale linearly with the number of documents  <cit> . while they obtained fairly good results from the numerical measurement, their cluster annotations consist of the features that were considered important during clustering and may not necessarily be understandable to a human. these tests do not utilize the textpresso ontology with the reuters text.

as a test of the svm performance, classification on the top  <dig> reuters categories was performed. this set consists of the  <dig> training articles, and  <dig> testing samples. the svm classification obtained a micro-averaged f <dig> score of  <dig>  and macro-averaged f <dig> score of  <dig> . these scores slightly exceed the results that debole and sebastiani obtained on this set with svm classification  <cit> . as debole and sebastiani describe, this reuters subset is the easiest because it contains the most training examples. such a test, however, is a good measure for the top-level classification that svm provides in our clustering engine, since each category should have a substantial number of training examples. the competitive svm classification performance indicates that this initial classification stage performs as well as state-of-the-art methods.

discussion
accomplishments
this clustering system provides an effective way to classify documents into a taxonomy, with category descriptions that are easily understandable by humans. the benchmarking experiments indicate that the clustering results are competitive with state-of-the-art algorithms. such a taxonomy with useful cluster labels allows novices in the field to quickly discover the key concepts among the papers while enabling experts to browse through the hierarchy and quickly locate papers discussing a specific topic. the classifications also allow researchers to locate similar papers, by finding papers that contain the same concept. both, the provision of key concepts of a research field as well as the ability to find similar papers, are not easily achieved by simply entering a set of keywords into a search engine. the software, along with source code, is available for download  <cit> .

the human-provided guidance helps ensure higher quality clustering results. for example, the sex determination category provides the following for the top layer of choices: "hermaphrodite male", "sex determination", "development gene", "development cells", "cells fate", "vulval cells", "anchor cells", and "cells male." as a comparison, the search for "sex determination caenorhabditis elegans" on clustermed, vivisimo's engine to cluster up to  <dig> titles and abstracts from pubmed, yielded the top clusters as "fem", "dosage compensation", "behavior", "gld-1", "mab-3", "fog, germ cells", "caenoharbdities elegans sex-determining gene tra-2", "cdna sequence", "translational control", and "fish medaka". compared to vivisimo's results, our labels emphasize concepts compared to gene names, which should be more useful for researchers trying to explore the field.

the immediate goal of the project was to provide a way to classify caenorhabditis elegans literature, but the process to classify text in another domain is fairly straightforward. the curator must first choose the primary categories, create lists of important words in each of these categories, and then identify training papers for these categories. domain-specific rules may be needed in the code to specify which words should be skipped in the vector representations. for increased svm performance, the svm parameters should also be tuned for these categories, but this step can be automated by the software system once the training papers are found. if an ontology has been used to markup the text in the new domain, the rules used in the clustering step should be modified although an ontology is not required.

the unique combination of support vector machines with phrase-based clustering allows the creation of a topic taxonomy that is more guided, and thus, of higher quality than current state-of-the-art methods. at the same time, the amount of human guidance required is kept limited to finding the initial training set for the svm step. other steps of human intervention such as finding lists of boosted words or rules for using the textpresso markup are minimal compared to this step.

besides the utility of the entire system, the individual components of the project might be useful. as demonstrated with the clustering engine on yahoo search results, documents or snippets from a general source may be used to construct a topic hierarchy. the phrase-based clustering algorithm, while currently optimized for a specific group of papers, can be adapted to a variety of different types of documents. an ontology is required for neither svm classification nor phrase-based clustering. on-the-fly clustering of search results can also be performed on caenorhabditis elegans literature, but the clustering engine is currently not optimized for such usage.

areas for improvement
the svm process provides acceptable performance, but further slight modifications may allow better performance. for example, recent reports have indicated superior text classification performance when using transductive svm, which creates the separating hyperplane while maximizing the margin to both the training and testing vectors, compared to the inductive svm that the system currently uses  <cit> . another concern is the creation of the training set, finding the optimal precision and recall that can be obtained, and which documents to train on to minimize the work needed in making the training set. schohn and cohn developed an approach called active learning, an iterative process of identifying papers to add to the training set  <cit> .

the clustering engine, with few published algorithms to compare to, is highly experimental. more human guidance could be implemented when choosing possible phrases or creating the hierarchy. concepts may be found to be synonyms by looking in a knowledge source, such as the metathesaurus included in the umls  <cit> . the umls also includes data sources that organize topics into a hierarchy. for example, the medical subject headings included in umls can relate apoptosis as a child concept of cell death, which is a child concept of cell physiology, and so forth. thus, by integrating these kinds of relations that have already been created by expert curators into the probabilistic method currently used, a better hierarchical tree could be produced.

CONCLUSIONS
we have presented a simple but effective two-step method to categorize a corpus of biological papers. it consists of a support vector machine component as well as a novel phrase-based clustering algorithm. the method automatically generates clusters with labels that are intuitive and understandable to humans. it is amenable to human intervention and modification such as hand-crafted rules, but can also be used in an unsupervised environment. this method performs competitively when compared to similar algorithms.

