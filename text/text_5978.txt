BACKGROUND
overview of biomedical natural language processing
research in the field of nlp is concerned with the development of systems that take textual data  as input and/or output. examples of these systems encompass "core" tasks that often provide components of larger systems, such as syntactic analysis or semantic disambiguation, as well as practical applications for tasks such as summarisation, information extraction and translation. over the past decade, the new field of biomedical text processing has seen dramatic progress in the deployment of nlp technology to meet the information retrieval and extraction needs of biologists and biomedical professionals. increasingly sophisticated systems for both core tasks and applications are being introduced through academic venues such as the annual bionlp workshops  <cit>  and also in the commercial marketplace.

this meeting of fields has proven mutually beneficial: biologists more than ever rely on automated tools to help them cope with the exponentially expanding body of publications in their field, while nlp researchers have been spurred to address important new problems in theirs. among the fundamental advances from the nlp perspective has been the realisation that tools which perform well on textual data from one source may fail to do so on another unless they are tailored to the new source in some way. this has led to significant interest in the idea of contrasting domains and the concomitant problem of domain adaptation, as well as the production of manually annotated domain-specific corpora.

in this paper we study the phenomenon of subdomain variation, i.e., the ways in which language use differs in different subareas of a broad domain such as "biomedicine". using a large corpus of biomedical articles, we demonstrate that observable linguistic variation does occur across biomedical subdisciplines.

furthermore, the dimensions of variation that we identify cover a wide range of features that directly affect nlp applications; these correspond to variation on the levels of lexicon, semantics, syntax and discourse. in the remainder of this section - before moving on to describe our methods and results - we motivate our work by summarising related prior research on corpus-based analysis of domain and subdomain variation and on the recognised problem of domain adaptation in natural language processing.

analysis of subdomain corpora
the notion of domain relates to the concepts of topic, register and genre that have long been studied in corpus linguistics  <cit> . in the field of biomedical nlp, researchers are most often concerned with the genre of "biomedical research articles". there is also a long history of nlp research on clinical documentation, frequently with a focus on extracting structured information from free-text notes written by medical practitioners  <cit> .

a number of researchers have explored the differences between non-technical and scientific language. biber and gray  <cit>  describe two distinctive syntactic characteristics of academic writing which set it apart from general english. firstly, in academic writing additional information is most commonly integrated by modification of phrases rather than by the addition of extra clauses. for example, academic text may use the formulations the participant perspective and facilities for waste treatment where general-audience writing would be more likely to use the perspective that considers the participant's point of view and facilities that have been developed to treat waste. secondly, academic writing places greater demands on the reader by omitting non-essential information, through the frequent use of passivisation, nominalisation and noun compounding. biber and gray also show that these tendencies towards "less elaborate and less explicit" language have become more pronounced in recent history.

we now turn to corpus studies that focus on biomedical writing. verspoor et al.  <cit>  use measurements of lexical and structural variation to demonstrate that open access and subscription-based journal articles in a specific domain  are sufficiently similar that research on the former can be taken as representative of the latter. while their primary goal is different from ours and they do not consider variation across multiple different domains, they do compare their mouse genomics corpus with small reference corpora drawn from newswire and general biomedical sources. this analysis unsurprisingly finds differences between the domain and newswire corpora across many linguistic dimensions; more interestingly for our purposes, the comparison of domain text to the broader biomedical superdomain shows a more complex picture with similarities in some aspects  and dissimilarities in others . friedman et al.  <cit>  document the "sublanguages" associated with two biomedical domains: clinical reports and molecular biology articles. they set out restricted ontologies and frequent co-occurrence templates for the two domains and discuss the similarities and differences between them, but they do not perform any quantitative analysis. hirschman and sager  <cit>  document aspects of clinical writing that affect language processing systems, such as a pronounced tendency towards ellipsis and the use of phrases in the place of full sentences; similar observations are made in the corpus study of allvin et al.  <cit> .

other researchers have focused on specific phenomena, rather than cataloguing a broad scope of variation. cohen et al.  <cit>  carry out a detailed analysis of argument realisation with respect to verbs and nominalisations, using the genia and pennbioie corpora. nguyen and kim  <cit>  compare the behaviour of anaphoric pronouns in newswire and biomedical corpora; among their findings are that no gendered pronouns  are used in genia while demonstrative pronouns  are used far more frequently than in newswire language. nguyen and kim improve the performance of a pronoun resolver by incorporating their observations, thus demonstrating the importance of capturing domain-specific phenomena.

domain effects in natural language processing
recent years have seen an increased research interest in the effect of domain variation on the effectiveness of natural language processing  technology. the most common paradigm for implementing nlp systems  is statistical or machine learning, whereby a system learns to make predictions by generalising over a collection of training data  that has been annotated with the correct output. a fundamental assumption of statistical methods is that the data used to train a system has the same distribution as the data that will be used when applying or evaluating the system. when this assumption is violated there is no guarantee that performance will generalise well from that observed on the training data and in practice a decrease in performance is usually observed. the dimensions of variation that directly affect a given statistical tool will depend on the application and methodology involved. for example, a document classifier using a bag-of-words representation will be sensitive to lexical variation but not to syntactic variation, while a lexicalised parser will be sensitive to both.

in many nlp tasks, a standard set of human-annotated data is used to evaluate and compare systems. these data sets are often drawn from a single register or topical domain, news text being the most common due to its availability in large quantities. for example, syntactic parsers are usually trained and evaluated on the wall street journal portion of the penn treebank, though it is known that this gives an overoptimistic view of parser accuracy  <cit> . for example, gildea  <cit>  demonstrates that a parser trained on the wall street journal section of the penn treebank suffers a significant drop in accuracy when tested on the brown corpus section of the treebank, which is composed of general american english text. losses in performance caused by mismatch between training and test domains have been observed for a wide range of problems, from sentiment classification of reviews about different classes of products  <cit>  to named entity tagging for printed and broadcast news text  <cit> .

when considering the transfer of nlp tools and techniques to biomedical text processing applications, the distance between source and target domains is far greater than that between the brown and wsj corpora or between film and electronics reviews on an on-line retailer's website. as described in the previous section, the language of biomedical text differs from general language in many diverse ways, making an awareness of variation effects crucial. two strategies are available to developers of tools for statistical biomedical text processing: creating a new annotated corpus of domain-specific data, and "adapting" a model trained on an existing out-of-domain data set to the domain of interest. the strategies are complementary: domain adaptation methods usually require some amount of annotated target-domain data, while the construction of specialised domain corpora for complex tasks is extremely labour-intensive and it is infeasible to produce large standalone corpora for multiple tasks and domains. describing the range of methods that have been introduced by nlp and machine learning researchers for domain adaptation is beyond the scope of this paper; for a representative sample see  <cit>  and the proceedings of the acl  <dig> workshop on domain adaptation for nlp  <cit> .

there are many examples of corpora constructed to facilitate the implementation and evaluation of tools for specific problems in biomedical language processing, for example the bioscope corpus  <cit>  for speculative language detection and the biocreative i and ii gene normalisation corpora  <cit> . there are also text collections that have been annotated for multiple tasks, most notably genia  <cit> , pennbioie  <cit>  and bioinfer  <cit> . one common feature of these corpora is that they have been compiled from just one or two specific subject areas, typically molecular biology. genia consists of  <dig>  abstracts dealing with transcription factors in human blood cells. pennbioie is also a corpus of abstracts, in this case covering topics in cancer genomics and the behaviour of enzymes affecting a particular family of proteins. bioinfer contains  <dig>  sentences that relate to protein-protein interactions. while these are without a doubt extremely valuable resources for application building, their limited coverage casts doubt on the assumption that a system that performs well on one will also perform well on biomedical text in general. one of the central questions addressed in the present paper is how representative a corpus restricted to a single subdomain of biomedical text can be of the overall biomedical domain.

methods
we now describe the implementation details of our study: first, we present the openpmc corpus of biomedical text and its division into the subdomains that constituted our basic units of enquiry. second, we enumerate the linguistic features we considered, and explain how we extracted them from the corpus. third, we describe our choice of metric for measuring divergence between subdomains, and our approach to gauging its statistical significance. fourth, we describe the clustering method we used on the raw feature distributions. finally, we explain how the results of these methods are presented graphically.

data set and preprocessing
the open access subset of pubmed  is the largest publicly available corpus of full-text articles in the biomedical domain  <cit> . openpmc is comprised of  <dig>  articles drawn from  <dig>  medical journals indexed by the medline citation database, totalling approximately  <dig> million words. articles are formatted according to a standard xml tag set  <cit> . the national institute of health  maintains a one-to-many mapping from journals to  <dig> subdomains of biomedicine  <cit> . the mapping covers about a third of the openpmc journals, but these account for over 70% of the total data by word count. journals are assigned up to five subdomains, with the majority assigned one  or two  . our data set is composed of journals that are assigned a single subdomain. to ensure sufficient data for comparing a variety of linguistic features, we discarded the subdomains with less than one million words of data. this makes for a total of  <dig> journals in  <dig> biomedical subdomains. we also added a reference subdomain, "newswire", composed of a  <dig> million word random sample from the gigaword corpus. these subdomains were our initial objects of comparison.

feature choice motivation
we considered subdomain variation across a range of lexical, syntactic, semantic, sentential and discourse features. here, we motivate our choices and point to nlp applications that make use of specific features, and hence are potentially affected by their variation.

lexical features
differences in vocabulary are what first come to mind when defining subdomains, and to measure this we considered lemma frequencies. a lemma is a basic word-form that abstracts beyond inflection: for example, the tokens "runs", "run" and "running" would all be considered instances of the verb lemma "run". we considered noun, verb, adjective and adverb lemma frequencies separately. lexical features are fundamental to methods for text classification  <cit> , language modelling  <cit>  and most modern parsing approaches  <cit> . these systems may therefore be affected by variations in lexical distributions, either as a result of misestimating frequencies or out-of-vocabulary effects.

part-of-speech  tags capture lexical properties not preserved by lemmatisation, such as singular vs. plural and passive vs. active, as well as various function words. at the same time, pos tags abstract over potentially large classes of words such as the class of all common nouns. for example, "runs" may be tagged "vbz", indicating that it is 3rd person singular, while "running" may be tagged "vbg", indicating it is a present participle. pos tags reflect several known features of scientific language, such as pronominal usage, verb tense and punctuation. pos tagging is a first step in many nlp tasks, such as morphological analysis and production  <cit>  and constructing lexical databases  <cit> .

syntactic features
lexical categories that describe a word's combinatorial properties are essential to the success of some classes of lexicalised statistical parser. in the framework of combinatory categorial grammar  lexical categories are the essential bridge between the lexical and syntactic levels, encoding information on how a lexical item combines with its neighbours to form syntactic structures  <cit> . ccg categories are assigned by a "super-tagger" sequence labeller, akin to the process for pos tags. for example, the most frequent ccg category for the verb "run" is "/np", which indicates it combines with a noun phrase to the right, then to the left, to form a sentence. ccg categories have been proposed as a good level for hand-annotation when re-training lexicalised parsers for new domains  <cit> , as they provide syntactic information while remaining relatively easy for non-experts to label, compared e.g. to full sentence parse-trees. changes in their distribution would affect parsing accuracy, but could be a tractable starting-point for domain adaptation if the problem is anticipated.

grammatical relations , also called syntactic dependencies, specify relationships between words, and by extension, between higher-level syntactic structures. for example, the two grs "ncsubj" and "dobj" indicates that "dog" is the subject, and "home" the object, of the verb "runs", as in the sentence "the dog runs home". more complex sentences may include multiple clauses, and so further distinctions are made between clausal and non-clausal arguments . gr distributions will reflect characteristic syntactic preferences in a domain, such as the preference for modification observed by biber and gray  <cit>  in scientific text. variation in gr distributions across subdomains may be expected to degrade parsing performance and necessitate model adaptation.

semantic features
semantic features capture what a text is "about" at a more general and interpretable level than individual lexical features. one approach we adopted, known as "topic modelling", models each document of interest as a mixture of distributions over words or "topics" that have been induced automatically from the corpus. these topics provided a bottom-up vocabulary for investigating semantics in the corpus that is complementary to the top-down vocabulary provided by the nih subject headings.

we also investigated a more specific kind of semantic behaviour relating to verb-argument predication. this was motivated by the observation that relations between verbs and their arguments are central to important semantic tasks such as semantic role labelling  <cit> . variation in the pattern of verb-argument relations across subdomains is likely to indicate difficulty in porting tools from one subdomain to another.

sentential and discourse features
sentence length is known to roughly correlate with parsing difficulty and syntactic complexity  <cit> . noun phrase  length increases as more information is "packed" via pre-/post-modification. scientific language is known to aim for high information density  <cit> . pronominal usage, which is touched on by pos tags, can be a stylistic indicator of scientific writing at a finer level, e.g. the avoidance of personal pronouns in laboratory sciences, and the restriction of gendered pronouns mainly to clinical sciences  <cit> . co-reference resolution is crucial to many information extraction applications where valuable information may be linked to a referent in this fashion. nguyen and kim  <cit>  compare the use of pronouns in newswire and biomedical text, using the genia corpus as representative of the latter, and found significant differences. moreover, they improved the performance of a pronoun resolution system by tailoring it based on their findings, which demonstrates the practical value in considering these features.

feature extraction
lexical and syntactic features
we first converted each openpmc article from xml to plain text, ignoring "non-content" elements such as tables and formulae, and split the result into sentences, aggregating the results by subdomain. the sentences were fed to the c&c parsing pipeline  <cit> , using pos tagging and supertagging models augmented with training on the genia corpus of annotated biomedical texts  <cit> . c&c uses the morpha morphological analyser  <cit> , maximum entropy labellers for tagging and supertagging and a log-linear parse model. rasp-parser-style  <cit>  grammatical relations were extracted from c&c output using deterministic rules. tables  <dig> and  <dig> show the system's output for the sentence "multiple twinning in cubic crystals is represented geometrically".

lexical features extracted from the sentence "multiple twinning in cubic crystals is represented geometrically" using the c&c parser

grammatical relations extracted from the sentence "multiple twinning in cubic crystals is represented geometrically" using the c&c parser

from this output we simply counted occurrences of noun, verb, adjective and adverb lemmas, pos tags, grs and ccg categories. the lemma distributions tended to be zipfian in nature, while the others did not. we experimented with filtering low-frequency items at various thresholds, to reduce noise and improve processing speed, and settled on filtering items that occur less than  <dig> times in the entire corpus.

sentential and discourse features
we measured average sentence, noun phrase and base nominal lengths  for each subdomain, using the parsed output from c&c. in order to filter out lines that are not true sentences, we ignored lines containing less than 50% lowercase letters. sentence length is defined as the number of non-punctuation tokens in a sentence. noun phrase length is defined in terms of a sentence's dependency structure as the number of words from the leftmost word dominated by a head noun to the rightmost dominated word. base noun phrase length is simply the number of tokens contained in the head noun and all premodifying tokens. as our corpus is not annotated for coreference we restricted our attention to types that are reliably coreferential: masculine/feminine personal pronouns , neuter personal pronouns  and definite noun phrases with demonstrative determiners such as this and that. to filter out pleonastic pronouns we used a combination of the c&c parser's pleonasm tag and heuristics based on lappin and leass  <cit> . to filter out the most common class of non-anaphoric demonstrative noun phrases we simply discarded any matching the pattern this... paper|study|article.

semantic features
to facilitate a robust analysis of semantic differences, we induced a "topic model" using latent dirichlet analysis   <cit> . lda models each document in a corpus as a mixture of distributions over words, or "topics". for example, a topic relating to genetics will assign high probability to words such as "gene" and "dna", a topic relating to experimental observations will prefer "rate", "time" and "effect", while a topic relating to molecular biology will highlight "transcription" and "binding". as preprocessing we divided the corpus into its constituent articles, removing stopwords and words shorter than  <dig> characters. we then used the mallet toolkit  <cit>  to induce  <dig> topics over the entire corpus and make a single topic assignment for each word in the corpus. we collated the predicted distribution over topics for each article in a subdomain, weighted by article word count, to produce a topic distribution for the subdomain.

an alternative perspective on semantic behaviour is provided by mapping the distribution of syntactically-informed classes of verbs across subdomains. these classes are learned by generalising over the nouns taken by verbs as subject arguments and as direct object arguments in the corpus. the learning method is a topic model similar to the lda selectional preference model of Ó séaghdha  <cit> , though instead of associating each verb with a distribution over noun classes, here each noun is associated with a distribution over verb classes. the decision to study verb classes was motivated by the fact that classifications of verbs have been shown to capture a variety of important syntactic and semantic behaviour  <cit> . by learning classes directly from the corpus, we induced a classification that reflects the characteristics of biomedical text and its subdomains. for each grammatical relation considered ,  <dig> verb classes were induced and every instance of the relation in the corpus was associated with a single class.

measuring divergence
our goal is to illustrate the presence or absence of significant differences among the subdomains for each feature set. the feature sets  are represented as probability distributions. we therefore calculate the jensen-shannon divergence   <cit>  for each feature set between each subdomain. jsd is a finite and symmetric measurement of divergence between probability distributions, defined as  

where h is the shannon entropy of a distribution  

jsd values range between  <dig>  and  <dig> .

random sampling for intra-subdomain divergence
comparability of jsd values is dependent on the dimensionality of the distributions being compared: approximations of significance break down with large dimensionality  <cit> . our feature sets vary widely in this respect, from  <dig>  to over  <dig>  . we therefore compute significance scores based on random sampling of the subdomains. for each subdomain, we divide its texts into units of  <dig> contiguous sentences, and build  <dig> million-word samples by drawing randomly from these units. we then calculate the pairwise jsd values between the random samples. this gives us  <dig>  jsd values calculated between random articles drawn from this subdomain . the significance of an inter-subdomain jsd value x between subdomains a and b is the proportion of intra-subdomain jsd values from a and b that are less than x. basically, this uses the null hypothesis that the variation between the two subdomains is indistinguishable from random variation within the subdomains. additionally, the intra-subdomain jsd values can be used by themselves to indicate how homogeneous a subdomain is with respect to the given feature set. the choice of sample size is based on general guidelines for significant corpus sizes  <cit> , where million-word samples are considered sufficient for specialized language studies.

clustering
to find natural groupings of the subdomains, we perform k-means clustering directly on the distributions, using the gap statistic  <cit>  to choose the value for k. the gap statistic uses within-cluster error and random sampling to find optimal parameters tailored to the data set. a typical measurement of within-cluster error, the sum of squared differences between objects and cluster centres, is compared with the performance on a data set randomly generated with statistical properties similar to the actual data set. as k increases, performance on both data sets improves, but should improve more dramatically on the actual data set as k approaches a natural choice for cluster count. k is selected as the value where the improvement in performance at k +  <dig> is not significantly more than the improvement in performance on the random data.

presentation
the non-distributional sentential and discourse features are directly reported as tables. the jsd values for the lexical and syntactic feature sets are presented in four figures per feature set: a heat map, a dendrogram, a distributional line plot, and a scatter plot.

heat maps
heat maps present pairwise calculations of a metric between a set of objects: cell < x, y >is shaded according to the value of metric. our heat maps show three types of values: the top half shows jsd values between pairs of subdomains. the bottom half shows the significance of the jsd values , calculated as described in the section "random sampling for intra-subdomain divergence" above. the diagonal shows the average intra-subdomain jsd value, again as described previously. in all cases, the actual values are inscribed in each square. the significance scores are shaded from white  to black . the jsd values are shaded from white  to black . in other words, white indicates more absolute variation  and higher significance .

dendrograms
dendrograms present the results of hierarchical clustering performed directly on the jsd values . the algorithm begins with each instance  as a singleton cluster, and repeatedly joins the two most similar clusters until all the data is clustered together. the order of these merges is recorded as a tree structure that can be visualised as a dendrogram in which the length of a branch represents the distance between its child nodes. similarity between clusters is calculated using average cosine distance between all members, known as "average linking". the tree leaves represent data instances  and the paths between them are proportional to the pairwise distance. this allows visualization of multiple potential clusterings, as well as a more intuitive sense of how distinct clusters truly are. rather than choosing a set number of flat clusters, the trees mirror the nested structure of the data.

distributional line plots
the line plots present the distribution of intra-subdomain jsd values, with each line representing a subdomain. higher values for one subdomain versus another shows that its texts have more variety with respect to that feature.

scatter plots
the scatter plots project the optimal k-means clustering onto the first two principal components of the data. the components are normalised, and points coloured according to cluster membership, with the subdomain written immediately above. the "newswire" subdomain is not included in the plots: as an outlier, it compresses the subdomains into unreadability. in clustering, it was typically grouped with "ethics" and "education", or its own singleton cluster.

RESULTS
general observations
the most striking general trend is the strong similarity between biochemistry, genetics and molecular biology. these subdomains form the most consistent cluster across feature sets, and are often one of the most closely-related triplets in the dendrograms . as mentioned previously, these subdomains are the basis for most annotated resources for bionlp. our results suggest that not only are these resources tuned for a handful of subdomains, but these subdomains exhibit a narrow range of linguistic behaviour, less representative of other biomedical subdomains. this is true for both lexical features  and syntactic features .

the heatmaps show that, for all feature sets, the variation is significant between nearly all pairs of subdomains . the intra-subdomain variation is much greater and more diverse for the vocabulary features than for the pos and gr features, with the ccg features in between. the science subdomain's generalist scope  gives it unusually high intra-subdomain scores, and we don't consider it further. newswire is the least similar outlier for every feature set, and is not included in the pca plots to improve readability.

some clusters of subdomains recur across features, and we present a useful breakdown in table 3: these subdomain clusters are present in the optimal clustering for at least 8/ <dig> of the feature sets. the first cluster includes subdomains dealing primarily with microscopic processes and can be further subdivided into groupings of biochemical  and cellular  study. the second cluster includes subdomains focused on specific anatomical systems . the third cluster includes subdomains focused on clinical medicine  or specific patient-types . the fourth and final cluster includes subdomains focused on social and ethical aspects of medicine . this is almost always the most distant cluster from the rest of pmc and usually the closest to newswire.

subdomains that are clustered together by at least 8/ <dig> feature sets

properties of the feature sets
we now consider each feature set in terms of the significance of variation and clustering of subdomains.

over- and under-use of lexical items
before considering the lexical feature sets, we discuss a phenomenon noticed when examining the lemmas that most characterize each cluster. we compiled lemmas with extreme log-likelihood values, indicating unusual behavior relative to the corpus average  <cit> . we noted that they tend to define their clusters by over- or under-use of lemmas relative to the corpus average, with some favouring one extreme or the other. this may reflect differences in how lexical items vary: for nouns, over-use tends to be characteristic because the basic objects of enquiry are often disjoint between subdomains. conversely, common verbs that are used with subdomain-specific meanings show over- and under-use . these two types of variation, the introduction of completely new nouns and the modified behaviour of common verbs, call for different adaptation techniques. for example, self-training can be used to re-estimate distributional properties of common verbs but may be less successful at handling the out-of-vocabulary effects caused by unseen nouns.

lexical features
noun distributions  show the highest inter-subdomain divergence. nouns also show the most intra-subdomain variation, particularly in catch-all subdomains like medicine, but also in some laboratory sciences like microbiology and genetics. despite high intra-subdomain variation, only one pair of subdomains have a jsd value that is not statistically significant at the 99% level: genetics and molecular biology. the k-means clusters divide the subdomains according to the over-use of nouns describing the objects focused on: some examples are clinical , genetic , education , oncology , public policy , cellular  and environmental .

adjective distributions  also have high divergence within and between subdomains. again, genetics-related subdomains show insignificant differences, as do virology and microbiology. in general, we see nouns and adjectives give common-sense semantic pairings  and sharply distinguish the "social sciences" from the rest. there is also a slightly less clear distinction between "patient-centric"  and "system-centric"  subdomains. the k-means clusters are similar to those for nouns, with the microscopic sciences  merged into one cluster. unlike nouns, the characteristic features include both over- and under-used terms, such as "clinical" and "medical". verb distributions  have lower jsd values, but these remain significant due to lower intra-subdomain scores. the verb clusters generally agree with the noun clusters, although sometimes emphasise different similarities . unlike nouns and adjectives, clusters are distinguished by both under- and over-use of verbs such as "conserve", "express" and "contain". it is also interesting that these particular verbs have specialised meanings in certain subdomains, suggesting a corresponding major shift in frequency when there is a shift in meaning. adverbs  have the lowest jsd values of the lemma types. the subdomains are distinguished by two types of adverbs: markers of scientific discourse, and domain-specific premodiffers. the former include lemmas like "previously", "significantly" and "experimentally", with further distinctions between more qualitative and quantitative subdomains. the latter include lemmas like "intraperitoneally" and "immunohistochemically", which are used to avoid the more complex syntax of relative clauses and leverage the specific knowledge of its audience. these information-dense terms could prove useful for tasks like automatic curation of medical ontologies, as they imply relationships between their lexical components, the verbs they modify, and so forth.

pos distributions  have low inter-subdomain jsd values, but their even-lower intra-subdomain jsd values render them universally 100% significant. biomedical engineering, medical informatics and therapeutics make particular use of present tense verbs and determiners, and markedly less of past tense. the cluster including communicable disease and critical care shows the opposite trend, perhaps reflecting certain subdomains' use of narrative. in notable contrast to the other feature sets, tropical medicine is not clustered with communicable disease, belonging instead to a cluster with distinctive overuse of comparative adjectives, foreign words and wh-pronouns. the "laboratory science" cluster also uses many foreign words, but avoids wh-pronouns. the difference between general language , social science  and the biomedical subdomains still dominates the figures. the clusters, however, are less interpretable: there are similarities with the lemma clusters, but oddities are mixed in. for example, while tropical medicine, communicable disease and veterinary medicine are still closely related, pulmonary medicine is close as well .

syntactic features
the gr features  have similarities with the pos features, and overlapping interpretations: for example, both capture the over-usage of determiners by biomedical engineering and medical informatics. more unusual is that their cluster includes ethics and education, due to high usage of clausal modifiers. this supports the claim that clauses contribute to syntactic complexity and so are typically avoided in biomedical language. it also may indicate that biomedical engineering and medical informatics retain aspects of both scientific and general language syntax. subdomains extremely far from the centre , e.g. endocrinology and vascular disease, are never grouped together in the lexical features. these outliers have particularly long average sentence length , suggesting a relationship between gr frequencies and sentence length. however, exceptions to this  indicate the relationship is more complex, and requires more detailed analysis.

the ccg categories  show the same relationship with long sentence length as grs. ethics and education are back to forming their own cluster. as mentioned previously, the distribution of intra-subdomain jsd values for ccg shows intermediate behaviour between the open-class lexical features and closed-class features, which reflects their lexical-syntactic nature. the similarities in cluster results to both the lexical and gr features demonstrates this further.

sentential and discourse features
semantic features
a reasonable first expectation is for the topic modelling results  to be similar to the lexical features. this largely holds: 8/ <dig> binary pairings of leaf subdomains in the noun dendrogram are also present in the topic modelling dendrogram, and the exceptions are generally displaced by one or two places and are attested in other vocabulary feature sets.

since the subject  and direct object  based verb cluster distributions are derived from distributions of nouns and verbs, it would be reasonable to expect them to have similarities with these feature sets. the relationship is less pronounced than between nouns and the topic models: a similar comparison of subdomain pairs has lower agreement . more significantly, the differences are often more dramatic than interpolating one or two other subdomains. for example, public health and psychiatry, which are paired in the verb feature space, are distant according to both selectional preference models. systems that use lexical frequency information at the level of syntactic arguments may need to adjust their model to account for this.

the relationship between some subdomains is stronger when considering one selectional preference versus another. for example, the similarity of critical care and vascular disease is higher for selectional preferences of subject than direct object. the reverse is true for the similarity of rheumatology and neoplasms. this may reflect higher usage of subdomain-specific vocabulary in particular argument positions, but this needs more in-depth scrutiny to draw detailed conclusions about selectional preferences. it does, however, show the potential importance of considering the semantics of different verbal arguments when adapting to these subdomains.

CONCLUSIONS
in this paper we have identified the phenomenon of subdomain variation and studied how it manifests itself in the domain of biomedical language. as far as we are aware, this is the first time that subdomain analysis has been applied to a corpus spanning an entire scientific domain and the first time that it has been performed with a focus on implications for natural language processing applications. as well as demonstrating that subdomains do vary along many linguistic dimensions in the openpmc corpus, we have shown that subdomains can be clustered into relatively robust sets that remain coherent across different kinds of features. one important conclusion that directly bears on standard training and evaluation procedures for biomedical nlp tools is that the commonly-used molecular biology subdomain is not representative of the corpus as a whole and a system that performs well on a corpus from this subdomain is not guaranteed to attain comparable performance on other kinds of biomedical text. as interest in biomedical applications of nlp continues and nlp systems are deployed in increasingly varied contexts, we expect the study of subdomain variation to become ever more important. one direction for future work is to directly measure the effect of subdomain variation on the performance of nlp systems for various tasks. a second promising direction is to investigate whether system performance can be improved by integrating knowledge of the corpus' subdomain structure; a starting point for this work would be to consider bayesian hierarchical models of the kind that have previously been suggested for modelling structural variation in a corpus  <cit> .

authors' contributions
tl and do collected and pre-processed the corpora and performed the feature extraction. tl carried out the distribution and clustering experiments and designed heat maps, dendrograms, plots and tables included this paper. ak contributed to the design of the experiments. all the authors took part in the analysis of the results and in the write-up of the paper. all authors read and approved this document.

