BACKGROUND
large cohort studies are extremely useful for discovering genotype phenotype associations and to characterize variation with great public health significance . the decreasing costs of sequencing are increasingly making it possible to sequence whole genomes in the millions in the coming years  <cit> . the past decade has also seen the development of many joint calling approaches for genomic data produced with low coverage whole genome sequencing . joint calling is necessary for low to medium coverage sequencing projects  as it further reduces false positives rate especially at the rarer end of the site frequency spectrum. it is also clear that improving the yield of variants from sequenced data across the whole spectrum of variants requires the deployment of diverse statistical and algorithmic approaches . it is also important to correct for algorithmic biases to ensure high fidelity variants  <cit> . consensus strategies on ensemble calling of low coverage sequencing data in the 1000genomes project  <cit>  has produced variants with high sensitivity and low false discovery rate . imputation strategies have also been shown to improve the variant discovery power of variant calling pipelines analyzing low coverage data  <cit> . for example, the 1000genomes phase  <dig>  project used multiple joint callers for site discovery, followed by a genotype likelihood step and an imputation and phasing step  <cit>  for ~ <dig> whole genome low coverage samples. given the projected growth in sequenced data in the coming years, variant calling pipelines will have to adapt to a computational footprint of an unprecedented scale and make it tractable both in terms of time and costs. it will require an extremely generous data storage facility and a massive number of cores. variant calling of ~  <dig> whole genome samples in the 1000gp <dig> project took multiple institutions to collaborate over months to produce the final results. huang et al.  <cit>  estimate 1– <dig> months of exclusive access on a typical local high performance compute cluster  to accomplish single nucleotide variant  calling using snptools  <cit>  for the 1000gp <dig> dataset.

the advent of cloud computing framework  <cit>  has significantly boosted the ability to tackle problems of scale, with several existing cloud based solutions to process genomic data . there has been some past work on porting state-of-the-art variant calling pipelines  <cit>  for targeted whole exome sequencing of thousands of samples to the amazon web services   <cit>  cloud  <cit> , but a cloud based ensemble calling workflow for thousands of whole genomes is lacking. instance limits on data storage is a serious limitation for joint calling of large cohorts in the aws environment, which is typically not a problem in an lhpc environment with sufficient capacity. scaling up the lhpc infrastructure to meet the computational needs can prove to be costly as the cost of maintaining just a  <dig> node cluster can run up to  <dig> $ a month  <cit> . large supercomputers typically deployed in computing centers and department of energy leadership computing facilities provide systems with large number of computing cores with specialized integer and floating point arithmetic, memory capacity, low-latency and high-bandwidth network, and high-capacity io  <cit> . however, most of these systems limit the execution time of a job to a few tens of hours. this is a major limitation to workloads such as joint calling of a large cohort of wgs samples, whose jobs are typically hundreds of hours  <cit> .

in this work, we show that variant calling pipelines using a hybrid computational environment can leverage the strengths of each architecture to process cohorts with thousands of whole genome samples in real-time while minimizing operational costs. as a proof of principle, we present performance metrics of snv calling on the cohorts for heart and aging research in genomic epidemiology wgs freeze  <dig> dataset   <cit>  using three different computational environments. there are  <dig> whole genome sequenced  samples in this dataset sequenced with 6 × –10× coverage for a total of 180 tb of aligned bam file data. the variant calling workflow is divided into four stages, where stage a is defined as the variant site identification stage involving four callers, stage b as the consensus site filtering step, stage c as the genotype likelihood step and stage d as the imputation and phasing step. for the charges-f <dig> dataset, stage a, stage b, stage c and stage d were completed on aws  <cit> , lhpc at baylor college of medicine, aws and the large supercomputers at ornl  <cit>  and rice  <cit>  respectively. the four joint callers used in stage a are snptools  <cit> , gatk haplotypecaller   <cit> , gatk unifiedgenotyper  and gotcloud  <cit> . the snptools genotype likelihood module and imputation and phasing module are used for stage c and d respectively. we developed a tool called genomic single nucleotide analysis pipeline  for this project.

there were approximately  <dig> million snvs called in charges-f <dig> dataset, with approximately  <dig> and  <dig> % novel with respect to 1000gp <dig> and dbsnp <dig> databases respectively. using a strategy which includes all sites which have been called by at least  <dig> callers , we ensured false discovery rate  <  <dig>  % and specificity of over  <dig> % in the final callset with respect to a golden dataset consisting of whole exome sequenced samples with 80–100× coverage. the entire operation was finished in 50 days with a total core hour usage of ~  <dig>  million across all the infrastructures . each aligned bam file was split into 1 mbp region for joint calling on aws. this created a cache data footprint of 360 tb with a time to live not exceeding 14 days. only 6 tb of data was transferred across all platforms. the gosnap pipeline is designed to minimize egress charges, data storage charges and data transfer costs. it optimizes on concurrent core usage to be cost effective and fast. to the best of our knowledge, ensemble calling on a wgs cohort with over  <dig> samples has not been done before and this approach can be easily scaled to  <dig>  samples .

RESULTS
the workflow for the gosnap pipeline has been designed to address the scalability challenges in large scale genomic computing, and to minimize egress charges and computational time, while ensuring high quality results in variant calling. when scaling up, the major computational bottlenecks are the joint calling and imputation and phasing step. to address these challenges, we have designed and tested a hybrid computational paradigm, which consisted of  a local high performance cluster  made up of commodity hardware;  amazon web service   <cit> ; and  and the supercomputers at ornl   <cit>  and at rice university   <cit> . in this study, we demonstrated the feasibility of using a hybrid computational paradigm in processing large-scale genomic datasets by applying this to the charge wgs data consisting of  <dig> samples .

challenges in scalabilities for large-scale genomic data processing
limitations
most lhpcs with typical research environments have few pbs of storage and millions of core-hours per month and are constrained by hardware limits on data storage, computing power and data transfer bandwidth  to carry out large computes. scalability is not a problem for the aws computing environment as it allows flexibility to increases the compute and data resources with a ‘pay per use’ model  <cit> . however, the outbound data transfers incurs a cost which scales linearly with the amount of data transferred . it is also necessary to optimize on all aspects of the compute including memory bandwidth and capacity , computing cores  and io capacity and bandwidth  to make optimal use of the instances and achieve cost-effectiveness. for projects involving big data, there is an additional cost of implementing data parallelization to overcome the limitations of local instance on hdd space. the large supercomputing infrastructure has an extremely large data store, premium hardware optimized for high io bandwidth, low-latency and high bandwidth network, and dedicated hardware and software support for cpu-intensive operations, but computing jobs have to finish within hard wall time limits. for example, titan at ornl  <cit>  requires all jobs to finish within 24 hrs. scheduling delays in allocating large number of resources can add to the turnaround times.fig.  <dig> 
a a resource constraint analysis diagram of computing resources with respect to the three available architectures. in this diagram feasibility is measured in terms of cost, time and resource bounds. b a resource constraint analysis diagram of the variant calling stages for charges-f <dig> dataset. feasibility is measured in terms of cost, time and limited computing infrastructure in the aws cloud environment, supercomputer and lhpc respectively. the measure of feasibility is for illustration purposes only and does not conform to any data



challenges
executing the entire pipeline in fig. 1b on any single platform can be challenging for many reasons. the storage and compute requirements of stage a are beyond the capacity of most lhpcs. implementing stage a in the aws environment requires a workflow which minimizes egress charges apart from splitting and replication of data for joint calling to contend with limited per instance hdd space. doing stage a on supercomputers is not scalable as memory intensive variant calling jobs get in the way of achieving high concurrency on uniform hardware . for example, our profiling suggests that gatk-ug needs approximately 16gb of ram per joint calling job across  <dig> samples. on a supercomputer like rhea  <cit>  with  <dig> nodes, and 64gb memory per node, no more than  <dig> jobs can be scheduled concurrently. unanticipated batch failures in the presence of maintenance downtimes and fair share scheduling policy can also adversely affect turnaround times for the whole project. variation in coverage can cause job failures across all the infrastructures.

stage d does not require io, ram or hdd space, but lhpc resources are still inadequate for stage d. using the aws environment for stage d will be an inefficient utilization of the instances as they are billed for the entire configuration and not just processing power. in our profiling step, best practices configuration for snptools imputation module failed to finish within the maximum 24 hrs wall time limit on titan  <cit> . while blue biou  <cit>  was successful in finishing the imputation and phasing step within 24 hrs on a sample bin, it did not have sufficient capacity to ensure timely completion of the entire stage d computation.

solution
after extensive profiling and analyzing all the pros and cons of the three infrastructures, the gosnap pipeline ported stage a on aws, stage b on lhpc, stage c on aws and stage d on rhea and blue biou . in the rest of this section, we first present snv calling results and then present performance metrics of the gosnap pipeline on the hybrid computational environment.fig.  <dig> gosnap pipeline workflow minimizes egress charges. variant calling  and genotype likelihood  calling is done on the aws cloud, consensus filtering and imputation preprocessing is accomplished in the lhpc  and imputation and phasing  is done at the supercomputers at rice and oakridge national labs



results of snv calling on  <dig> low coverage whole genomes
in table  <dig>  the results for joint calling of  <dig> low coverage whole genomes is presented. all sites which have been called by at least three callers  are included for further analysis. there are approximately  <dig>  million bi-allelic snvs which have been called using consensus 3of <dig> approach from the four variant callers gatk-hc, gatk-ug, snptools, and gotcloud. the transition-to-transversion ratio  of  <dig>  is consistent with past results  <cit> . while consensus 2of <dig> strategy gave ~ <dig> millions snvs, the fdr is  <dig>  % with respect to wes gold standard dataset . the consensus 3of <dig> callset only has a fdr of  <dig>  % with the same gold standard dataset. the sensitivity is greater than  <dig> % for allele count > <dig> , whereas the sensitivity is ~ <dig>  and ~ <dig>  % for singletons and doubletons respectively. the consensus 3of <dig> callset has higher specificity and lowest fdr when compared to  all the callers. the number of snvs recovered in 1000gp <dig> and dbsnp <dig> is also the highest among all the callers. all callers have unique variants in their callset , with gotcloud having the lowest number  of unique variants with respect to the charge-f <dig> csnp dataset. the genotype concordance is  <dig>  % for ,  <dig>  % for  and  <dig>  % for  when compared to charge csnp array gold standard dataset.table  <dig> variant calling sensitivity and specificity for the consensus 3of <dig> approach ensures high specificity and fdr without a loss of sensitivity

the gold standard dataset consists of  <dig> samples with 80–100 × coverage. all the four callers are necessary for increasing the yield of snvs



application of our hybrid computational paradigm to variant calling of  <dig> wgs dataset
in our computational model,  <dig> aligned wgs bam files  are uploaded to the aws environment. in the slicing and repacking stage, we slice the bam of each sample into windows of size 1 mbp and repack the sliced bam from all samples in the same window into the one data package  for joint calling. this size is empirically determined  to fit into aws instances with hdd space not exceeding 320gbs for the joint calling jobs, as well as to reduce the number of intermediate files, which improves the efficiency of data access and transfer between ec <dig> and s <dig>  both the slicing and repacking jobs use “xargs” parallelization to make full use of the instance cpu cores, which ensure that none of the cores remain idle and improves the runtime by up to  <dig> fold whenever possible. several configurations were tested in the xargs mode ranging from  <dig> to  <dig> cores for the slicing stage, and the most cost effective instance had  <dig> cores, 16gb ram and 160gb solid state hdd space . the repacking stage predominantly uses instances with 1660gb of hdd space to accommodate the data across all the samples . the slicing and repacking of 200 tb whole genome bams takes  <dig> and 3 days, respectively . during this process, the data is temporarily duplicated twice, and the cache data copy is removed after the joint calling.table  <dig> summary performance metrics of gosnap pipeline

the pipeline finished in 50 days and only transferred a total of 6 tb of data starting from a raw data footprint of 180 tb. cache data of 360 tb was live only for 14 days. intermediate results amount to  <dig> and are archived for future use

the pipeline used  <dig>  million core hours



in the joint calling stage, the four variant callers, namely gatk-hc, gatk-ug, snptools and gotcloud are grouped into one aws compute job to call variants in each 1 mbp region. each caller is employed in joint calling mode, and any caller specific per sample processing, for instance gvcf calling in gatk-hc and effective base depth  calculation in snptools, is fully parallelized, which effectively reduces the runtime of the joint calling step by approximately 5-folds. the joint calling of  <dig> genomes only took 2 weeks , with an average 60 hrs per 1 mbp region.

the average concurrency is  <dig> cores per hour, with the peak concurrency  <dig>  cores per hour. since any failure that occurs during the long-running instance jobs is very costly, we run all the tools in the failure checking and retry mode . the average proportion of runtime taken by gatk-ug, gatk-hc, gotcloud and snptools is  <dig> %,  <dig> %,  <dig> % and  <dig> % respectively. nine types of instances  are used in the joint calling stage, of which  <dig> % of the jobs use an instance with  <dig> cores, 61gb ram and 160gb hdd space.  <dig> % of the jobs used instances less than 320gb of hdd space, with four configurations each using at least  <dig> % of the total number of jobs. these instances are cores =  <dig>  ram = 61gb, hdd = 160gb ; cores =  <dig>  ram = 122gb , hdd =320gb; cores =  <dig>  ram = 15gb, hdd = 160gb ; cores =  <dig>  ram = 30gb, hdd = 320gb . the remaining instances were used to accommodate the variation of sliced bam size due to depth variation across the whole genome. to reduce the egress charge in the cloud, we split the joint calling results into two parts, the essential data, containing the multi-sample vcfs from all callers, and the auxiliary data, containing intermediate files, like per sample gvcfs or ebd files. the total amount of output from the whole genome joint calling is 120 tb, of which only 2 tb of essential data are downloaded to lhpc for site level consensus filtering and quality control purpose. all auxiliary data are archived for future research project.

the site level consensus and quality control is performed on the lhpcs, and a union variant site list is generated, which is uploaded back to aws for genotype likelihood calculations  using snptools . the computational resources required for the gl step is not challenging for most of the infrastructures, but since the input whole genome bam files are stored in the cloud, doing this stage  on the cloud prevents egress charges. the per sample genotype likelihood data is only 2 tb. it is downloaded to rhea and blue biou for imputation and phasing. compared to 200 tb alignment data, 120 tb variant calling data, only 4 tb calling and gl data is downloaded and charged .

imputation and phasing is the most compute intensive stage. we take the gl data as input and run the imputation and phasing on rhea and blue biou using snptools imputation engine . the optimal imputation window size is decided taking into account the population diversity and the imputation runtime without exceeding the wall time limit of the supercomputer. the imputation jobs are scheduled according to the specification of each compute node, in order to make full use of computing resources and reduce the job scheduling overhead. the reference independent imputation of  <dig> samples took 4 weeks, with  <dig>  million core hours  including the system maintenance downtime and scheduling delays due to fair-share policy. the average runtime of imputing a bin with  <dig> snvs is approximately 13 hrs on blue biou and 30 hrs on rhea.

discussion
ensemble joint calling of  <dig> wgs samples is an unprecedented undertaking to the best of our knowledge. the limitations of joint calling tools for a sample size of this scale  had not been tested for, and a successful completion of the whole compute requires all protocols to be robust to resource allocation failures and silent faults  <cit> . non uniform coverage of the samples can contribute to unanticipated failures and data replication costs can adversely affect the operational costs. for example, five bins with large sizes were removed from further analysis in the gosnap pipeline, because the jobs did not finish even after 120 hrs of runtime. the entire pipeline only replicated the data in stage a  for 14 days and it is easy to reduce the cache data size to less than 200 tb by using a strategy where sliced bins are deleted as soon as the repacking stage finishes on the sliced bins. using multiple callers can add to the challenges due to scale, but are necessary to ensure higher sensitivity as all the callers used in our pipeline have distinct algorithmic strategies. in the gosnap pipeline all the callers contributed unique variants at the end of stage a when compared to the highly polymorphic csnp array . three callers have higher sensitivity compared to consensus 3of <dig> approach but the smallest fdr value  among the callers is ~ <dig> % which is almost twice that of the consensus 3of <dig> approach . snptools has lower sensitivity than consensus 3of <dig> but recovers most common variants  compared to the other callers at only  <dig> % of the computation resource cost. using some combination of three callers may only improve the sensitivity by a maximum of  <dig> % but the fdr might also be very high, as the fdr statistics for the consensus 2of  <dig> approach indicate. suppose we add one more caller and use  <dig> of  <dig> consensus strategy, the sensitivity may at best reach the sensitivity of gatk-ug , a maximum gain of  <dig> % compared with the sensitivity of  <dig> of  <dig> consensus  as in table  <dig>  but the fdr of  <dig> of  <dig> is not likely to be much lower than that of  <dig> of  <dig> consensus , compared to the fdrs of all four callers currently employed. on the other hand, the computation cost scales at best linearly with number of callers used. therefore the strategy of using four callers for variant site identification followed by consensus filtering was necessary for our project to ensure high sensitivity while maintaining low fdr statistics. in general, the decision to include more callers should depend on a number of factors such as, number of samples, depth of coverage, computing budget and project deadlines.

the yield of novel variants compared to 1000gp <dig> and dbsnp are also likely to be of a high quality because the ti/tv ratio of  <dig>  is consistent with past results  <cit> . sequencing errors can lead to a decreased ti/tv ratio due to introduction of random noise, especially at the rarer end of the site frequency spectrum. all the four callers in our pipeline have a ti/tv ratio less than the consensus 3of <dig> approach  and higher fdr statistics, thereby indicating that consensus filtering strategy reduces random noise. the site level consensus filtering minimizes the need of any tool specific filters  as the fdr of the consensus 3of <dig> approach is at least half as low as any individual caller . while the overall sensitivity is only  <dig>  %, the sensitivity is over  <dig> % for allele frequency >  <dig>  . this behavior is consistent with past work in detecting singletons and doubletons from low coverage data  <cit> . the yield of snvs from our pipeline is less than that of 1000gp <dig>   <cit>  even though the number of samples is almost twice as large, but that can be attributed to the relatively homogeneous ancestry of our samples compared with 1000gp <dig>  in a previous paper  <cit>  on ~ <dig> samples from our dataset, the yield of snvs was ~  <dig> million, which is less than that of a comparable sample size of 1000genomes phase  <dig> project. even the uk10k snv callset  <cit>  has only ~ <dig> million snvs whereas the number of unrelated samples in that project exceeds that of 1000gp <dig> 

a cloud based joint calling framework has been discussed in shringarpure et al.  <cit>  where a single joint caller is used to call 1000gp <dig> dataset. in their work, samples for the same population are grouped together for joint calling and calling is done one chromosome at a time. this strategy may fail with increasing sample sizes even with the most powerful instances. for example, the charges-f <dig> dataset has been sequenced with higher per sample coverage than 1000gp <dig> dataset and the sample size of euam  alone exceeds that of 1000gp <dig> . the data footprint of chr <dig> in the charges-f <dig> dataset is 4 tb and of chr <dig> is 18 tb which can only be accommodated on the d <dig> dense storage instances in aws  <cit> . our binning strategy provides a more scalable alternative, as  <dig> % of the nodes used in our work used hdd space less than 320gbs and can scale easily to much larger sample sizes. we did not face any scheduling delays which could be an issue when using high memory nodes on aws instances  <cit> . in our binning strategy, all mapped read mates, unmapped read mates and 10 kbp mapped reads in the buffer region are included in the sliced 1 mbp bin for joint calling. since many indel callers like gatk-hc and gatk-ug use this information to call high quality indels, our binning strategy minimizes errors in indel calling especially near the ends of the bins.

joint calling approaches on typical lhpcs and supercomputing infrastructures has also been studied in the past. in  <cit> , the authors use a supercomputing platform with ~ <dig>  cores and 72 hrs wall time to do joint calling using gatk-hc on  <dig> whole genomes with an average of ~30× coverage. they estimate a quadratic increase in resource usage consumption as sample size increases, and to make it feasible on their supercomputing platform, they perform group variant calling on subsets of their entire cohort in an ancestry dependent fashion. however, comparing the effective number of reads per ethnicity group in their work , with the one in our dataset, about  <dig> samples in a single population with 8× average coverage, our effective number of reads in the joint calling is at least doubled. this renders it less feasible to perform per population joint calling with our sample size scale on a supercomputing platform, as the resource consumption scales quadratically with the sample size. in the paper  <cit> , the authors also include performance analysis of aligning raw read data as part of their computational footprint. in our workflow, we assume that the raw sequencing data is aligned and uploaded to aws as it is sequenced, as storing raw sequencing data of over  <dig> samples  can be infeasible for an lhpc environment attached to a sequencing facility. aligning and uploading to aws in batches also minimizes data transfer bottlenecks across computing infrastructures and can be readily tuned to match sequencing throughput. the rate limiting step in the alignment and sequencing stage is likely to be sequencing throughput, as the expected time to sequence ~ <dig> whole genome samples with the current sequencing capacity far exceeds the turn around time of the entire gosnap pipeline. since the alignment is usually performed at the per sample level and the computation time scales linearly with the sequencing depth, we consider sequencing and alignment as a single stage with well controlled turnaround time. hence the alignment can be performed gradually as the sequencing reads are available, on a local cluster at the sequencing center, and the aligned data can be uploaded to the cloud for joint calling, where the computational resources are more abundant.

in the context of variant calling, the advantages of using an lhpc environment do not supersede the remaining two resources  for any stage and an argument can be made to design future variant calling pipelines which are either solely based on an aws environment or on a large supercomputer. however, in this project, all the real time qaqc and job tracking was carried out on an lhpc environment. despite the limited resources on an lhpc, the flexible computing environment aids in the rapid development of qaqc tools which in turn mitigates risk by ensuring the integrity of the gosnap pipeline at the start and at the end of each stage. several current tools for genomic data have been already designed for an lhpc environment but have to be ported for use in an aws or supercomputing environment, thereby adding to the timelines of a large project like charge. even though we did not use lhpc for variant calling on any bin in the charge dataset, a scenario can be anticipated where variant calling on some regions of the genome may only be feasible on an lhpc, with execution parameters different than the rest of the pipeline. changing the execution parameters on some selected bins may make it infeasible to execute on either the aws or supercomputing environment without additional development and testing of software.

there are several challenges in scaling up the analysis to even larger samples sizes. in the joint calling stage, increasing sample sizes will require instances with much larger instance hdd space. in the current implementation, the sliced per region per sample bam files are grouped together using tar compression. this helps to significantly reduce the number of input files. but the downside of tar compression is that it requires almost double the hdd space for the decompression. to work around with the instance storage limitation and to cope with the upcoming larger sample size, we propose an in-place compress-decompression strategy, by binary concatenating small tarballs together for input/output data transfer, and binary truncating the data chunks before decompression. the turnaround decompression space is expected to be as large as the small tarball size. this is an important direction for the future work. however, even with the current binning size parameters, increasing the sample size to  <dig>  samples would increase the size of the bins to only 120gb for  <dig> % of the bins. this can be scheduled with the existing gosnap release version, as  <dig> % of the instances used in this run had 320gbs of hdd space and  <dig> % of instances used less than 62gb of ram. it can be projected that with  <dig>  samples, the ram requirements will double at most, and the configuration with 122gbs of ram and 320gbs of hdd space will suffice for  <dig> % of the bins. implementing the new strategy with trunc can further alleviate hdd space constraints. furthermore, a strategy involving a smaller bin size can be used to mitigate the adverse effects due to using costly instances when scaling up to large sample sizes. for example, in our simulations we observe a  <dig> × increase in run time on 100 kbp bins with respect to 1 mbp bins, with each bin taking approximately 9 hrs on each 100 kbp region for approximately  <dig> samples . with  <dig>  samples and a bin size of 100 kbp this run time is projected to last for at most 40 hrs for most of the bins and twice the memory requirement of the current run. the number of jobs to manage will scale by  <dig> times for a sample size of  <dig>  samples and bin size 100 kbp but should not cause any performance degradation on the aws system. the imputation and phasing module scales linearly with sample size and can finish easily within the 120 hrs wall time limit of rhea. reducing the bin size for imputation and phasing could be challenging for a diverse cohort, but the current bin size estimation will hold for a larger dataset with a homogeneous population. though the number of burn-in iterations cannot be changed for larger sample sizes, the number of snvs/bin can be optimized to fit into the wall time restrictions. since all the data transferred in the gosnap pipeline is variant information, increasing sample sizes will only increase egress charges proportional to the variant information in the samples.

single sample calling for low coverage data will give a very high fdr  <cit> , therefore joint calling is necessary for minimizing fdr and to improve sensitivity of variant discovery for low coverage datasets, even when the variant calling pipelines do not include stage d. even though deep coverage sequencing also does not have perfect recovery of singletons and doubletons  <cit> , single sample calling gives comparable snv callsets to multisample calling for high coverage datasets  <cit> . however, for calling indels and mnp variants, joint calling approaches may still be better than single sample calling. this is particularly relevant in the case of clinical sequencing, where increasing the sensitivity in calling indels may have prognostic significance.

the binning strategy can be used to scale up to sample sizes of  <dig>  and beyond. however, to effectively use the computing infrastructures, the tools have to evolve to emerging architectures and data sizes. first, to efficiently use the supercomputers the tools have to be adapted for heterogeneous supercomputers. a significant number of highly scalable supercomputers, including titan at ornl are heterogeneous computers i.e., they use computing accelerators such as gpus and intel many integrated core architectures for achieving computation and power efficiency. titan is the fastest supercomputer in the united states and it uses gpus as computing accelerators. as this trend in supercomputer architecture continues  <cit> , it becomes important for all tools to evolve to heterogeneous architectures. second, as the data size increases with sample sizes, it is important to have data transfer protocols to exchange data among multiple points of computation that are geographically separate. though our data transfer times in the current work were always less than  <dig> days, we anticipate the data transfer times becoming a bottleneck as we scale up to larger samples. third, a smart job scheduler that can schedule jobs on hybrid computing infrastructure which includes cloud, supercomputers, and local computing infrastructure can decrease the burden on researchers to schedule and manage jobs.

CONCLUSIONS
with increasing number of genomic datasets freely available on the aws cloud  <cit> , the next generation of variant calling pipelines will also be increasingly common in the aws environment. while the costs of storage and compute cores in the aws environment is declining, it may still be prohibitively costly to carry out many steps of standard variant calling workflow on the cloud. a hybrid computational approach involving multiple hpc systems may be an important future direction to explore. our work on the gosnap pipeline demonstrates that using a hybrid computation strategy can be cost effective and fast even with thousands of individual genomes.

