BACKGROUND
gene expression is the biological process in which the information stored in a gene is converted into protein or rna. nowadays, high-throughput expression measurements of entire transcriptomes can be easily obtained through different techniques. these techniques differ from each other on the technology and/or approach used to obtain a measurement. nonetheless, they can roughly be classified as: hibridization , sequencing  and other non high-throughput approaches .

the study and analysis of these measurements is the primary focus of functional genomics  <cit> . overall, the objectives of functional genomics include the definition of the functional roles of different genes and associated processes, the study of the interactions between gene and gene products, the study of gene expression variations in different cell types and under different conditions, among others  <cit> . once expression data is available, biologists are faced with the task of extracting  knowledge associated to the underlying biological phenomenon. most often, in order to carry out this task, biologists perform a sequence of analysis activities on the available gene expression dataset rather than a single analysis activity. analysis activities include data normalization, identification of differentially expressed genes, pathway analysis, cluster analysis and classification, functional annotation and modelling gene regulatory networks.

occasionally, an integrated analysis environment, also commonly refered as an integrated analysis pipeline , can be used to support the integrated execution of different analysis activities. however, in most cases biologists have to use different, isolated analysis tools. in those cases, biologists must frequently implement themselves, without proper guidelines, the necessary code to integrate those tools, which can hinder effective research  <cit> .

the integration of complete systems or part of systems to build a new system is a challenging task, even when these systems were developed using the same programming language and the same execution platform. system integration can be achieved at syntactical and semantical levels. on the one hand, the syntactic integration aims at agreeing on a common syntax for representing the data exchanged between different parts of the system. on the other hand, the semantic integration aims at agreeing on common meanings for the data exchanged, under possibly different syntactical formats, between different parts of the system. semantic integration is usually accomplished using a neutral  ontology to enable the translation first from the source system format into the neutral format and then, from there, into the target system format  <cit> . thus, semantic integration enables the assignment of unambiguous meanings to data intended to be shared among different systems in an integrated environment, allowing the exchange of data in a semantically consistent and meaningful way.

in the gene expression domain, the emerging of high-throughput experimental processes has lead to development of different  formats for the representation and exchange of gene expression data, such as mage-ml  <cit> , mage-tab  <cit> , soft  <cit> , miniml  <cit> , sam  <cit>  and fuge-ml  <cit> . however, the availability of a standard data representation format is not enough to guarantee semantic integration of heterogenous tools because not all tools comply with these formats, different types of data can be represented using a single format and, perhaps most importantly, analysis activities can be carried out without proper reasoning about the meaning of exchanged data, among others. for example, we can store and exchange both one-color and two-color microarray data according to the mage-tab data format. however, any analysis activity carried out on these data should take into account its meaning in order to obtain biologically significant results. thus, we should not normalize one-color microarray data in the same way as we normalize two-color microarray data.

ontologies have been used to facilitate the integration of computer systems and information in the biomedical domain. particularly, we can identify two general ontology-based approaches for the integration of bioinformatics systems and databases  <cit> . in the first approach, ontologies have been used as source for the design of a common or reference  model shared by a number of related tools or databases . in the second approach, ontologies have been used as basis for the development of mediators, i.e., software entities that encompasses a global knowledge  and, at the same time, are able to provide mappings to the specific  schemas to be integrated .

in the gene expression domain, different and often complementary activities are usually carried out using different tools during the analysis process. additionally, new biological and experimental developments frequently lead to the modification of existing data models and the development of new algorithms and analysis tools. thus, both approaches pose a number of limitations for the integration of gene expression analysis tools. the first approach lacks flexibility, since the integration of a new tool requires the adaptation of this tool data model to the reference model and/or the modification  of the reference model. the second approach lacks generality, since its primarily focus lies on the translation of queries between a mediator and local schemas. finally, none of the approaches supports dynamic processing  of the exchanged data, which is often necessary to enable the proper usage of data by a target tool.

this work aims at developing an ontology-based methodology for the semantic integration of gene expression analysis tools to support not only the access to heterogeneous data sources but also the definition of transformation rules on exchanged data. we have used software connectors as basis for our integrative solution. software connectors represent architectural elements used to model interactions among either computation or data components of a system. we have studied the different challenges involved in the integration of computer systems and the role connectors play in this task. we have also studied a number of gene expression technologies, analysis tools and related ontologies in order to devise basic integration scenarios and propose a gene expression domain ontology. finally, we have proposed a number of activities and associated guidelines to develop connectors. this methodology was then applied in the construction of a number of integration scenarios involving different gene expression data and/or tools. the proposed methodology allows the development of connectors capable of integrating different gene expression analysis tools and/or related data at a semantic level, thus assuring accurate data exchange and information interpretation from the exchanged data. additionally, our methodology can be used in the development of connectors supporting both simple and nontrivial processing requirements.

methods
the following steps were carried out in the development of our ontology-based methodology: 1) study of software architecture and integration of computer systems; 2) study of gene expression technologies, analysis tools and related ontologies; 3) definition of a reference ontology for the gene expression domain; 4) definition of activities and associated guidelines for connector development, and; 5) application of the proposed methodology in the construction of different integration scenarios.

software architecture
software architecture emmerged as a sub-discipline of software engeneering in the early 90's. this discipline is focused, among others, on the architecture description of complex systems and on the use of this description as basis for system design, development, reuse and management in general  <cit> . the architecture of a software system represents the fundamental properties of this system in relation to its enviroment, embodied in its elements, relationships, and in the principles of its design and evolution  <cit>  . an architecture is represented by means of an architecture description. architectural description languages  can be used to create an architecture description through the development of one or more architecture models .

despite existing diferences among adls, there seems to be general agreement with respect to the elements needed to describe structural aspects of an architecture, viz., components, connectors and architectural configurations  <cit> . a component represents a unit of computation or data store  in a system. a component can be as simple as a single procedure or method or as complex as an entire application. a connector represents an architectural element used to model interactions among components and rules governing those interactions. connectors also specify any auxiliary mechanism required to perform the interactions  <cit> . a connector can be as simple as a global variable  or procedure call or as complex as a p2p-based data distribution connector  <cit> . an architectural configuration represents a set of associations between components and connectors pertaining to a system's architecture. an architectural configuration can be represented simply as a graph whose nodes represent components and connectors and whose edges represent their associations ibid.

connectors are built based on basic primitives for transferring data and/or control. connectors can be classified according to the set of provided services. four basic services can be identified  <cit> : communication, which supports transmission of data among components; coordination, which supports transfer of control among components; conversion, which transforms the interaction provided by one component to the interaction required by another; and facilitation, which mediates and streamlines component interaction in order to optimize interactions and reduce interdependencies. simple connectors, which are implemented directly in programming languages, provide a single service, while composite connectors provide multiple services, often exibiting an internal architecture with computation and data storage capabilities.

most connectors provide multiple services. for example, a procedure call connector provides both communication and coordenation services, while a p2p-based data distribution connector provides all four types of services. in the context of this work, the developed connectors provide a combination of communication, coordination and conversion services in general.

gene expression analysis
in order to apply the proposed methodology we have devised three integration scenarios for the analysis of different types of gene expression data. each scenario involves the integration of different tools and/or data sources. the following tools were considered: r environment graphical user interface   <cit> , which was used for microarray data normalization and identification of differentially expressed genes; kegg mapper - search&color pathway  <cit> , which was used for searching kegg pathway maps using differentially expressed genes and then coloring regulation accordingly; datamatrixviewer   <cit> , which was used for rna-seq data selection and displaying; tigr multiexperiment viewer   <cit> , which was used for rna-seq data clustering; and david bioinformatics resources   <cit> , which was used for gene functional classification.

each proposed integration scenario involves the analysis of different source gene expression data in order to reproduce  different studies already documented in the literature. in the first scenario, two-color plasmodium vivax microarray data  <cit> , available from ncbi gene expression omnibus  under the accession number gse <dig>  were initially normalized and then analysed for differentially expressed genes using rgui. in the sequel, kegg plasmodium vivax pathways were analysed using kegg mapper - search&color pathway. this tool was used to identify which parts of pathways are associated with the list of differentially expressed genes provided by rgui, highlighting up and down regulation according to a user-defined color scheme. in the second integration scenario, sulfolobus solfataricus cdna sequencing data, obtained from a rna-seq platform  <cit> , available from geo under the accession number gse <dig> and aligned with bowtie  <cit> , were initially filtered using dmv and then clusterized using tmev. finally, in the third integration scenario, one-color microarray data taken from normal and cancer prostate cells  <cit> , also available from geo under the accession number gse <dig>  were analysed to find differentially expressed genes using a rgui implementation of htself  <cit> , a self-self based statistical method for low replication microarray data. the obtained data were then loaded into david for functional analysis.

automatic interaction with rgui from a third-party application was provided by the rserve api  <cit>  . this api allows the establishment of a  communication connection  between the r system and a java application. this connection was then used to send r commands to be processed by the r system and, after their execution, to receive the corresponding answers.

biomedical ontologies
many different ontologies have been proposed in the biomedical domain. the open biological and biomedical ontologies  foundry is a consortium that provides a repository of life-science ontologies  <cit> . these ontologies have been developed according to a set of shared principles, including openness, orthogonality and collaborative development. the obo foundry ontologies include the gene ontology   <cit> , the chemical entities of biological interest ontology   <cit> , the phenotypic quality ontology   <cit>  and the protein ontology   <cit> , among others. additionally, the foundry also includes a number of candidate ontologies and other ontologies of interest in the life-science domain, such as the sequence ontology   <cit> , the common anatomy reference ontology   <cit>  and the ontology for biomedical investigations   <cit> .

the proposed methodology uses a reference ontology for the gene expression domain called gene expression ontology . two ontologies were considered in the development of our reference ontology: gene ontology   <cit>  and sequence ontology   <cit> . go provides a set of terms and relations used for standardization of genes and their products in eukaryotic organisms using three independent ontologies: cellular component, which describes cellular structures in which genes can be expressed; molecular function, which describes activities that occur at the molecular level; and biological process, which describes collections of processes  related to the functioning of integrated living units. so provides a set of terms and relations used to describe features and attributes of biological sequences. the development of such controlled terminology aims at facilitating the exchange, analysis and management of genomic data, particularly genomic databases and flat file data exchange formats.

ontologies can be represented using different languages, such as the unified modeling language   <cit> , the web ontology language   <cit>  and the obo flat file format  <cit> . uml is a standard graphical language widely used in the specification, documentation and visualization of computer artifacts and ontologies. owl is an ontology definition language originally conceived for the semantic web. owl specifications are serialized using a machine-readable rdf/xml-based format. the obo flat file format or simply obo format is also a machine-readable, text-based ontology representation language. the obo format provides a subset of the concepts in owl, with a number of extensions.

our gene expression reference ontology has been represented using owl. in order to facilitate the visualization of the proposed ontology and the understanding of the integration scenarios, we have also created uml representations of parts of our reference ontology.

RESULTS
basic integration scenarios
in order to develop our methodology, we initially had to identify a set of basic scenarios in which integration could take place.

a tool ta can be integrated to another tool tb or to a data source d in different ways, considering both the transfer of data and/or control. in this sense, five basic integration scenarios can be identified :  data stored in d are transferred to ta;  data produced by ta are transferred to d;  data stored in d are transferred to ta and later data produced by ta are transferred back to d;  data and/or control from ta are transferred to tb; and  data and/or control from ta are transferred to tb and later data and/or control from tb are transferred back to ta.

integration scenarios c and e can be considered composite integration scenarios since they can be structured as a composition of the other  scenarios . integration scenario c can be considered a combination of scenarios a and b, while integration scenario e can be considered a combination of two scenarios d. in both cases, there is a bidirectional flow of data between a data source and a tool or a bidirectional flow of data and/or control between two tools. in this sense, the guidelines provided in this work take into account only unidirectional scenarios , but can be easily extended to cover bidirectional scenarios.

gene expression ontology 
different ontologies have been proposed in the biomedical domain. these ontologies usually focus on specific aspects of this domain, such as biological processes, cellular components, molecular functions, biological sequences, etc. however, there is no ontology whose primary focus is the gene expression domain, although many concepts of this domain are present in different ontologies. so, after identifying the basic integration scenarios we developed a reference ontology, called gene expression ontology , to be used as basis for semantic integration.

all classes  defined in our reference ontology are subclasses of the general owl class thing. figures  <dig>   <dig>   <dig> and  <dig> present the concepts and relationships defined as part of the ontology through uml class diagrams. not all concepts and relationships are graphically presented using uml, only those which were considered more relevant to the application of the developed methodology to the proposed integration scenarios. a concept in our ontology is represented by a uml class. the relationships between concepts are presented through a notation similar to the one proposed in  <cit> . the subsumption relation of owl is presented as a uml generalization stereotyped as is a. the part of relation and its inverse  are presented as a uml shared aggregation stereotyped as part of and has part, respectively. the other relations are presented as stereotyped uml associations.

particularly, the concepts reused from the gene ontology represent biological processes, such as gene expression, reverse transcription and translation, while the concepts reused from sequence ontology represent "things", such as dna, mature transcript and protein. most of the relationships used to relate these and other concepts were reused from the obo relation ontology  <cit> , except for the produced by, affected by and quantifies relations, which we have introduced in our ontology. the produced by relation was defined as a relation between a process and an entity produced by this process. the affected by relation was defined as a relation between an experimental process and an experimental condition capable of affecting this process. finally, the quantifies relation was defined as a relation between a biopolymer quantity and the biopolymer which it quantifies.

the class gene expression represents a biological process  by which the information codified by a gene is used to synthesize functional gene products, i.e., proteins and functional rnas. the classes transcription, rna processing, translation and protein maturation represent different subprocesses involved in gene expression. these subprocesses are related to the class gene expression through part of relations. the relationship preceded by defined between the classes rna processing and transcription indicates that the occurence of rna processing is preceded by the occurence of transcription. this relationship is also defined between the classes translation and rna processing, and between the classes protein maturation and translation.

the class transcription represents the production  of a primary transcript  from the information codified  by a gene . the class rna processing represents the occurence of modifications in a primary transcript , such as polyadenylation and splicing. the class mature transcript represents the result of this subprocess . there are two subtypes of mature transcript: non-coding rna  and messenger rna . the class translation represents the production  of a protein  from the information codified in a mrna molecule . finally, the class protein maturation represents the occurence of chemical modifications in a protein , leading to the attainment of its full functional capacity.

the class biological experimental process represents a general biological experimental process. the class experimental condition represents any experimental condition that can affect a biological experimental process . the class gene expression measurement represents a specific experimental process whose objective is to quantify the functional products, i.e., functional rnas and proteins, produced by a gene as result of its expression. in the context of this work, this process can be realized through a hibridization-based or sequencing-based approach . the class dna microarray experimental process specializes the class hybridization-based gene expression measurement. basically, dna microarray experimental process represents the process by which dna probes attached to the surface of microarray chips are used to hybridize with a labeled cdna sample, in order to quantify functional gene products. the classes one-color microarray experimental process and two-color microarray experimental process also specialize dna microarray experimental process. the class one-color microarray experimental process represents a specific dna microarray experimental process in which the biological samples of interest are labeled with the same fluorescent dye and hybridized in different arrays. conversely, the class two-color microarray experimental process represents a specific dna microarray experimental process in which the biological samples of interest are labeled with different fluorescent dyes and hybridized in the same array. finally, the class rna-seq experimental process specializes sequencing-based gene expression measurement. basically, rna-seq experimental process represents the process by which cdna molecules produced from a biological sample of interest are sequenced through next-generation sequencing technologies and quantified.

the class gene expression value represents a value obtained by quantifying the functional products produced by a gene as a result of its expression . the classes fluorescence intensity-based value and sequence counting-based value specialize gene expression value. additionally, the classes absolute intensity-based value and ratio intensity-based value specialize fluorescence intensity-based value to represent the values produced by one-color and two-color microarray experimental processes , respectively. specifically, the class absolute intensity-based value represents a value that quantifies the expression of a gene under a unique experimental condition, while the class ratio intensity-based value represents the ratio between the levels of expression of a gene under two different experimental conditions. the class cdna reads counting-based value specializes sequence counting-based value. cdna reads counting-based value represents a value produced by a rna-seq experimental process  to quantify the expression of a gene. the classes absolute cdna reads counting-based value and relative cdna reads counting-based value specialize cdna reads counting-based value to represent that the value obtained from a rna-seq experimental process may be based on the absolute number of cdna reads or on a relative value, respectively.

the class dna microarray experimental process represents a hybridization-based experimental process aiming at quantifying the functional products generated by a gene as result of its expression. the classes microarray manufacturing, microarray washing, microarray image acquisition and microarray gene expression profiling represent different subprocesses of a dna microarray experiment. these subprocesses are associated to the class dna microarray experimental process through part of relations, since they only occur as part of a dna microarray experimental process. the classes rna extraction, reverse transcription, cdna labeling and hybridization also represent subprocesses of a dna microarray experiment. however, these subprocesses are associated to the class dna microarray experimental process through has part relations, since they can also occur as part of other biological or experimental processes.

the class microarray manufacturing represents the process of manufacturing a microarray chip with probes of interest . the class probe represents fragments of dna used in the microarray manufacturing process. the class rna extraction represents the process by which functional rna molecules  are extracted from cells or tissues of interest . the class reverse transcription represents the biological process by which a complementary dna  molecule is produced  from a mature transcript . the class cdna labeling represents the process by which cdna molecules  are labeled with fluorescent dyes . the class hybridization represents the biological process by which two or more complementary nucleic acids establish non-covalent interactions, pairing each other. specifically, in a dna microarray experiment, the process of hybridization occurs between a dna fragment and a cdna molecule. the class microarray washing represents the washing of a microarray chip aiming at removing cdna molecules that were not hybridized to the chip probes. the class microarray image acquisition represents the process of obtaining a microarray image. a microarray image is produced by exciting the labeled cdna molecules with a laser and scanning the microarray chip in order to measure the emission of these molecules. finally, the class microarray gene expression profiling represents the measurement of gene expression levels  by quantifying the fluorescence intensities contained in a microarray image .

the relationship preceded by defined between the classes cdna labeling and reverse transcription indicates that the process of cdna labeling is preceded by the occurence of reverse transcription. this relationship is also defined between the classes microarray washing and hybridization, microarray image acquisition and microarray washing, and between the classes microarray gene expression profiling and microarray image acquisition.

the class rna-seq experimental process represents a sequencing-based experimental process aiming at quantifying the functional products generated by a gene as result of its expression. the class rna-seq gene expression profiling represents a subprocess of a rna-seq experiment. this subprocess is associated with rna-seq experimental process through a part of relation, since it only occurs as part of a rna-seq experimental process. the classes rna extraction, reverse transcription, cdna library with adaptors preparation, cdna next-generation sequencing, alignment of cdna read to a reference genome and de novo assembly of cdna reads also represent subprocesses of a rna-seq experiment. however, these subprocesses are associated with rna-seq experimental process through has part relations, since they can also occur as part of other biological or experimental processes.

the class cdna library with adaptors preparation represents the preparation of a cdna library containing cdna molecules  with adaptors. the class cdna next-generation sequencing represents the process of sequencing a cdna molecule  through next-generation sequencing technologies in order to produce cdna reads . the class cdna read represents a cdna sequence fragment obtained as a result of a sequencing experiment . the class reference genome represents a standard collection of sequences for a given organism and genome assembly. the class alignment of cdna read to a reference genome represents the process of aligning a cdna read to a reference genome . the class de novo assembly of cdna reads, in turn, represents the process of assembling cdna reads  to create a transcriptome without the aid of a reference genome. the classes alignment of cdna read to a reference genome and de novo assembly of cdna reads represent alternative processes of a rna-seq experiment . finally, the class rna-seq gene expression profiling represents the measurement of gene expression levels based on the counting of cdna reads .

the relationship preceded by defined between the classes cdna library with adaptors preparation and reverse transcription indicates that preparing a cdna library with adaptors is preceded by the occurrence of reverse transcription. this relationship is also defined between the class rna-seq gene expression profiling and the classes alignment of cdna read to a reference genome and de novo assembly of cdna reads. however, since alignment of cdna read to a reference genome and de novo assembly of cdna reads represent alternative processes of a rna-seq experiment, gene expression profiling is preceded by the occurrence of either one of these processes, not both .

the complete owl specification of the gene expression ontology, including the definition of a sage experimental process, can be found in a supplementary material .

connector development methodology
a methodology consists of a collection of systematic procedures or guidelines used to produce an intended set of artifacts  to represent the different elements of a target application domain. thus, we defined a set of activities and associated guidelines that prescribe how the development of a connector c to integrate tools ta and tb or to integrate a tool ta to a data source d should be carried out:

 <dig>  identification of main functionalities. this activity aims simply at identifying the main functionalities provided by the tools being integrated. we should develop a functional specification listing the main functionalities fa <dig>  fa <dig>  ..., fan and fb <dig>  fb <dig>  ..., fbm of tools ta and tb, respectively. this abstract specification both provides a better understanting of the services provided by each tool and serves as a starting point for the identification of possible integration scenarios. considering the integration of a data source d to a tool ta, only the functionalities of ta should be identified.

 <dig>  integration scenario initial description. since each tool being integrated provides different sets of functionalities and since these sets of functionalities can possibly be combined and used in many different ways, we need to delimit a target integration scenario. so, this activity aims at providing an initial description of this target scenario. such a description can be obtained in three steps. first, we should select among the functionalities previously identified a set of interest functionalities for the target scenario. then, we should identify any new functionality that should be provided as part of the integration scenario. such functionality will be provided by the connector under development itself. together, both sets of funcionalities form the set of relevant functionalities for the target scenario. finally, we should provide a general textual description of this scenario. such a description must include all relevant functionalities identified.

 <dig>  integration scenario detailed description. after the initial description of the integration scenario, we need to detail this scenario. so, this activity aims at providing a detailed description of the target integration scenario through the development of an activity model and a use case model. these models can be obtained through the development of uml activity and use case diagrams  <cit> , respectively. the development of an activity model aims at capturing the order in which the relevant functionalities are executed and the functional entities  responsible for their execution. the development of a use case model aims at capturing additional information regarding the execution of the relevant functionalities through the description of the interactions between the functionality users and the entities responsible for their execution.

an activity diagram describes a coordinated execution of a sequence of activities performed by one or more functional entities. each relevant functionality should be mapped to a corresponding activity. activities can be organised into swimlanes, which represent responsibility zones. thus, swimlanes, one for each entity, should be used to associate each activity to the entity responsible for its execution. the execution ordering of the identified activities should be established based on the integration scenario initial description.

a use case diagram describes an interaction scenario between a set of users and a functional entity. a use case diagram consists of a set of actors, use cases and their relationships. a use case represents a unit of functionality comprising sequences of actions that the functional entity perform to produce an observable result to one or more outside interactors, called actors. an actor represents a role that a user plays with respect to the functional entity. use cases are identified based on the developed activity model. in general, each identified activity should be mapped to a single use case. however, multiple  activities could also be mapped to a single use case. each identified use case should be associated to either one of the tools involved in the integration or the connector itself. this association is directly obtained from the activity diagram swimlanes. actors and their association to use cases should be identified based on the integration scenario initial description.

a detailed description of selected use cases should complement the use case diagram. such a description consists of a textual description of the main aspects associated to the use case in a table-like format. for each use case, the following information should be provided: use case purpose, list of associated actors, numbered account of the interactions initiated by the associated actors and the corresponding functional entity response. these interactions should be described in a typical or normal course of events, which does not preclude the description of alternative courses of events either. the selection of use cases to be described is based on the activity diagram. only use cases created based on activities directly related to the interactions between a tool or a data source and the connector should be described.

as a final and complementary step of the integration scenario detailed description, we should provide an architectural description of the scenario to facilitate the visualization of the different roles played by the involved entities. in order to produce such a description we can use an adl or simply represent graphically the different architectural elements  and their relationships using an ad hoc notation. at this point, we can structure  a complex  connector as an integrated set of simple connectors. this can be accomplished through the assignment of different use cases to different  connectors.

 <dig>  interest data detailed description. this activity aims at providing a detailed description of the integration interest data. interest data include data either consumed or produced by a connector. such information can be typically obtained from the use case detailed descriptions. however, other sources can be used to provide additional information, such as user manuals, help entries, etc. for each functionality fi previously associated to a use case, we should describe in a table format each data item either consumed or produced.

two separate tables should be created for each connector: one to describe data items that are consumed and one to describe data items that are produced. for each identified data item, a new row should be added to the table. a separate column should be used to describe the data item identification, the data item semantic description and the data item syntactic description. the data semantic description contains a textual description of the intended meaning of the data item. the data syntactic description contains all information needed to concretely represent the data, including the data storage medium , encoding format , content type  and cardinality .

 <dig>  interest data conceptual modelling. after the detailed description of the interest data, we should develop conceptual models to represent such information at a high abstraction level. two conceptual models should be developed for each  connector using uml class diagram  <cit> : one to represent the data consumed by the connector and another to represent the data produced by the connector.

each conceptual model should formalize the concepts and relationships identified in the interest data detailed description. each identified data item should be mapped to a corresponding concept  in the concept model. additionally, any identified relationship between data items should be mapped to a corresponding relationship between concepts. typical relationships include uml association and aggregation.

 <dig>  reference ontology mapping. after creating the conceptual models of interest data, we should map the concepts present in these models to concepts present in the reference ontology. so, this activity aims at identifying the correspondence between concepts representing either consumed or produced data items and concepts from the reference ontology.

the mapping can be accomplished through the construction of an equivalence table for each  connector. this table contains three columns: the first column contains concepts representing consumed data items; the second column contains concepts present in the reference ontology; and, the third column contains concepts representing produced data items. first, we should add one row for each concept representing an identified consumed data item. then, we should identify the equivalence between each one of these concepts and a corresponding concept present in the reference ontology. whenever such equivalence can be identified, the corresponding reference ontology concept should be added to the appropriate row. finally, we should also add one row for each concept representing an identified produced data item and then identify the equivalence between each one of these concepts and a corresponding concept present in the reference ontology. once again, whenever such equivalence can be identified, the corresponding reference ontology concept should be added to the appropriate row. in case the same concept appears repeatedly in different rows, it should be merged into a single row. figure  <dig> illustrates the construction of an equivalence table.

at the end of the equivalence table construction, ideally we should have mapped each concept representing a consumed data item to a corresponding concept from the reference ontology, which also corresponds to a concept representing a produced data item. however, situations in which  partial mappings are obtained are quite common and should be dealt with accordingly.

three cases in which a partial mapping is obtained can be identified: 1) a consumed data item is simply not used by the connector to produce an output; 2) a consumed data item may represent part of the information used by the connector to produce an output; and 3) a concept representing a produced item may be derived from the input data. since, in case  <dig>  the data is not used to produce an output, the lack of correspondence to a reference ontology concept is meaningless for the problem at hand, so the complete mapping is not required. in cases  <dig> and  <dig>  the definition of of semantic equivalence rules is required .

after the specification of semantic equivalence rules, table entries should be merged or split accordingly. once such correspondences can be established for all consumed and produced data items, except for unused data items, we can assert that semantic integration can be obtained.

 <dig>  grounding and semantic transformation definition. this activity aims at describing the mapping between syntactic and semantic data descriptions. additionally, this activity also aims at describing any possible semantic mapping transformation that might be required in order to establish equivalent relations between concepts representing produced and/or consumed data items and concepts from the reference ontology.

the mapping between syntactic and semantic data descriptions is often called grounding in the literature  <cit> . grounding is required because frequently the same semantic information can be represented differently at a syntactic level . grounding can be specified by means of two complementary operations: lifting, which is used to interpret semantic data from a  syntactic representation, and lowering, which is used to create a  syntactic representation from semantic data. in the context of this work, lifting and lowering operations should be specified textually for each consumed and produced data item. this specification should be based on the interest data detailed description and reference ontology mapping activities and serves as basis for the connector implementation activity.

during the specification of the lifting and lowering operations, we need to identify and represent any semantic transformation between consumed data conceptual model elements and reference ontology elements as well as between reference ontology elements and produced data conceptual model elements that might be necessary. the need for such semantic transformation might have been identified during the reference ontology mapping activity. in such case, we need to formally represent either mathematically or through a series of inference rules how a produced data conceptual model element can be derived from a reference ontology element or from a set of consumed data conceptual model elements, thus creating an equivalence relation between all these concepts. once this equivalency is defined, the equivalence table developed at the reference ontology mapping activity should be updated.

 <dig>  access policy identification. this activity aims at providing a textual description of the integration target object  access policy. this description specifies how and where the functionalities of the integration target object can be accessed by the connector under development.

the access policy description should include the identification of the mechanism to be used to either access consumed and/or produced data , file input/output , etc) or provide  transfer of data and control to the integration target object . automatic transfer of data and control is usually available only whenever an api describing the integration target object functionalities exists.

the access policy description should also include information regarding the physical and logical location  of the integration target object, as well as any existing access restriction to the object . this information can be usually obtained from user manuals, help pages, api documentation, etc.

 <dig>  connector implementation. this activity aims at implementing the connector in a systematic way. there is no restriction on the choice of implementation language to be used. the connector's developer is free to use any language she or he sees suitable for this task. however, some of the guidelines provided below take into account the use of an object-oriented programming language.

the functionalities of each connector should be structured into four functional blocks: 1) data input processing, which is responsible for obtaining the input data according to the specified data input format and producing an object-oriented representation of the data; 2) lifting, which is responsible for transforming the previously obtained object-oriented data representation into a reference  object-oriented representation of the data; 3) lowering, which is responsible for transforming the previously obtained canonical representation into an object-oriented representation of the data suitable for output; and 4) data output processing, which is responsible for producing the desired output according to the specified data format based on the previously obtained object-oriented representation of the data.

the lifting functional blocks is also responsible for implementing any required semantic transformation of the concepts according to the equivalence relations specified in the grounding and semantic transformation definition activity. the data output processing functional block is also responsible for accessing the target tool or data source being integrated and realizing the transfer of data and/or control as specified in the access policy identification activity.

the requirements for the implementation of each functional block can be identified based on the information produced by the interest data detailed description, grounding and semantic transformation definition and access policy identification activities. the execution of the functional blocks should be carried out serially according to the order in which they have been defined. figure  <dig> illustrates the timeline execution of a connector's implementation functional blocks.

the connector can be implemented completely independent from tool ta or as an integral part of this tool. the independent connector implementation is simpler to be implemented since no changes are required to tool ta. however, this implementation choice relies on the user to explicitly execute the connector. the integral part connector implementation enables the connector to be automatically executed from tool ta integrated interface. however, this implementation choice relies on the availability of the source code of this tool.

methodology application
we applied the proposed methodology in the development of three integration scenarios for the analysis of different types of gene expression data.

in the first scenario, two-color plasmodium vivax microarray data were normalized using rgui. in order to facilitate multiple condition normalization by rgui, different two-color microarray data should be combined into a single multi-column dataset. normalized microarray data was then used by rgui itself for the identification of differentially expressed genes. in the sequel, kegg plasmodium vivax pathways were analysed using kegg mapper - search&color pathway. this tool was used to identify which parts of pathways were associated with the differential gene expression analysis provided by rgui, highlighting up and down regulation according to a color scheme.

different files were used to store two-color microarray data , each representing a separate experimental condition. so basically, connector c <dig> was responsible for reading these files and for producing a single  file containing all data to facilitate further analysis activities. from a semantic point of view, the construction of the equivalence table was straightforward since the same set of concepts was used to represent data items both consumed and produced by the connector. these concepts were also directly mapped to the concepts of the reference ontology. thus, no transformation was required.

connector c <dig> was implemented as a separate java application. its different functional blocks were implemented as follows. the data input processing functional block reads a number of files containing two-color microarray data and stores these data as lists of strings . then, the lifting functional block transforms the lists of strings into three separate lists, containing instances of genes, experimental conditions and multiple lists of ratio intensity-based values. next, the lowering functional block transforms all the previous lists into a single list of strings. each string represents the concatenation  of the different instances of the elements identified in the lifting functional block. finally, the data output processing functional block simply writes the list of strings previously produced into a single text file . this general strategy has been similarly adopted in the implementation of all connectors in the context of this work.

connector c <dig> was also designed to provide both manual and automatic transfer of control to rgui . the automatic integration of the connector with rgui was provided by the rserve api. since the connector is capable of providing both forms of transfer of control to rgui, the automatic integration was implemented independently of the data output processing functional block in order to maintain a sound implementation structure. nevertheless, the interaction with rgui takes place after data output processing is complete. detailed information regarding connector c <dig> implementation can be obtained in a supplementary material .

finally, we have also developed an api, called gene expression library class , to facilitate the implementation of any semantic-based application in the domain. this api contains a number of java classes, each corresponding to a different concept of our reference ontology. examples of classes contained in the gelc api include absoluteintensitybasedvalue, cdnaread, experimentalcondition, gene, ratiointensitybasedvalue, sagetag, etc. all connectors were implemented using the gelc api in the context of this work. the gelc api binary code and documentation is also available as supplementary material .

rgui produced as output a single file containing the result of the differential gene expression analysis. these data were processed directly by connector c <dig> in order to produce an input suitable for kegg mapper - search&color pathway. connector c <dig> also received two user-provided  inputs, viz., i) the identification of the experimental condition whose expression values should be considered for further analysis; and ii) the value of a gene expression threshold that was used in the gene expression  classification . as a result, connector c <dig> produced as output a list of genes and their respective color information, viz., red, green or yellow, to be used as input by kegg mapper.

the semantical mapping between concepts representing either consumed or produced data items and concepts from the reference ontology for connector c <dig> was not as direct as for connector c <dig>  there were cases where no direct association between a concept representing a consumed data item and a concept representing a produced data item were identified. so, two equivalence relations had to be created for those cases. the first equivalence relation was straightforward and was created to associate an instance of the concept experiment-specific gene identifier with a possible instance of the concept kegg identifier. although both concepts were associated to the reference ontology concept of gene, not all instances of experiment-specific gene identifier have corresponding instances of kegg identifier. the second equivalence relation was created to associate the instances of the concepts of experimental condition, ratio intensity-based value and gene expression threshold with an instance of the concept of ratio intensity-based value. although quite simple in principle, the definition of this equivalence relation was quite elaborated. it involved the definition of a mapping over instances of ratio intensity-based value from the real numbers domain to the up, down and undefined regulation range according to a  instance of gene expression threshold.

connector c <dig> was also implemented as a separate java application. this connector provided only manual transfer of control to kegg mapper - search&color pathway. once the equivalence relations were defined, the specification and implementation of the grounding operations was straightforward. additionally, the transformation of instances of the concept experiment-specific gene identifier to instances of the concept kegg identifier was carried out using a platform mapping file , also provided as input to connector c <dig>  this mapping is usually carried out in two steps: 1) from experiment-specific gene identifier to official gene identifier; and 2) from official gene identifier to kegg identifier. however, in the specific case of plasmodium vivax, each kegg identifier corresponds directly to its official gene identifier. in the implementation of the lowering functional block instances of the concept ratio intensity-based value, viz., upregulated, downregulated and undefined, were mapped respectively to the red, green and yellow kegg mapper - search&color pathway color schema. detailed information regarding connector c <dig> implementation can also be obtained in a supplementary material .

the second scenario consisted of the integration of tools dmv and tmev in order to clusterize sulfolobus solfataricus rna-seq data. this scenario was inspired by the increasing popularity of next-generation sequencing platforms for gene expression. despite the benefits of storing the actual reads in rna-seq databases, these raw data files provide little support for high-level gene expression analysis. so, they were transformed into a numeric representation to be filtered using dmv according to some user defined criteria. next, dmv output was used as input data for clustering using tmev. however, dmv output data must be normalized to account for different library sizes  before clusterization by tmev.

different files were used to store aligned rna-seq data , each representing a different experimental condition. rna-seq data was then transformed into a numeric representation in a three-step process. the first step was the identification of known transcribed genes  using the sulfolobus solfataricus genome description  and replacing them in each source file with the associated gene identifiers . in case no match was found, the associated cdna read was removed from the source file. the second step was the counting of the total number of annotated genes present in each source file. the result was then stored in a text file . the third step was the counting of the number that each annotated gene appears in each experimental condition. once again, the result was stored in a text file .

connector c <dig> was created to integrate rna-seq data to dmv. c <dig> represents a composite connector consisting of three simple connectors: c <dig> , c <dig>  and c <dig> , which are responsible for performing steps one to three of the aforementioned transformation process, respectively. figure  <dig> shows the internal architecture of connector c <dig>  focusing on the flow of data between connectors.

the semantical mapping between concepts representing either consumed or produced data items and concepts from the reference ontology for connector c <dig> was not straightforward. in several cases, there was no direct association between a concept representing a consumed data item and a concept representing a produced data item. so, equivalence relations had to be created for those cases. for example, in connector c <dig> , an equivalence relation was created to associate an instance of the concept cdna read with an instance of the concept gene ; in connector c <dig> , an equivalence relation was created to associate an instance of the concept gene with an instance of the concept absolute cdna reads counting-based value ; finally, in connector c <dig> , another equivalence relation was created to associate instances of the concepts gene and experimental condition with an instance of the concept absolute cdna reads counting-based value .

once the equivalence relations were defined, the specification and implementation of grounding operations was carried out based on these definitions. connectors c <dig> , c <dig>  and c <dig>  were each implemented as a separate java application. thus, each connector can be executed and used independently. these simple connectors were then composed to form connector c <dig>  which is responsible for controlling the ordering in which the simple connectors are executed, viz., first c <dig> , then c <dig>  and finally c <dig> . although connectors c <dig>  and c <dig>  can be executed in any order , we have chosen that specific sequencing because performance is not an issue in the scope of this work. connector c <dig> as a whole was designed to provide only manual transfer of control to dmv, since this tool does not provide an api for automatic interaction from a third-party application.

data output from dmv must be normalized before they can be clusterized by tmev to account for different library sizes. normalization was carried out by connector c <dig> by dividing the number that each annotated gene appears in each experimental condition by the total number of annotated genes present in each source file. these normalized data produced by connector c <dig> were then used as input by tmev.

similarly to connector c <dig>  the semantical mapping between concepts representing either consumed or produced data items and concepts from the reference ontology for connector c <dig> was not straightforward either. so, an equivalence relation was defined to associate two instances of the concept of absolute cdna reads counting-based value with one instance of the concept of relative cdna reads counting-based value .

connector c <dig> was also implemented as a separate java application. this connector provided only manual transfer of control to tmev, since this tool does not provide an api for automatic interaction from a third-party application either. once the equivalence relation was defined, the specification and implementation of the grounding operations were straightforward. all data consumed and produced by this connector were stored in ascii text files .

the third integration scenario was inspired by a study where histologically normal and tumor-associated stromal cells were analysed in order to identify possible changes in the gene expression of prostate cancer cells  <cit> . in order to cope with a low replication constraint, we needed to use an appropriate statistical method, called htself  <cit> . however, this method was designed for two-color microarray data, thus a non-trivial data transformation on input data was required. one-color microarray data taken from normal and cancer cells were transformed into  two-color microarray data and then used as input for the identification of differentiated expressed genes using htself. then, the obtained data were filtered to be used as input for functional analysis carried out using david.

one-color microarray data was transformed into virtual two-color microarray data by creating sets of ratios using all possible pair-wise comparisons among experiments. this transformation process was carried out in two steps. the first step consisted of creating sets of ratios derived only from samples pertaining to the same cathegory , while the second step consisted of creating sets of ratios derived only from actual comparisons among different cathegories.

connector c <dig> was created to integrate one-color microarray data to rgui. c <dig> represents a composite connector consisting of two simple connectors: c <dig>  and c <dig> , which are responsible for performing steps one and two of the aforementioned transformation process, respectively.

the semantical mapping between concepts representing either consumed or produced data items and concepts from the reference ontology for connector c <dig> was not straightforward either. in two cases, there was no direct association between a concept representing a consumed data item and a concept representing a produced data item. so, equivalence relations had to be created in both cases. in both connectors c <dig>  and c <dig> , an equivalence relation was created to associate two instances of the concept absolute intensity-based value with one instance of the concept ratio intensity-based value according to the aforementioned transformation process .

once the equivalence relations were defined, the specification and implementation of the grounding operations for each simple connector was straightforward. connectors c <dig>  and c <dig>  consumed as input a number of one-color gene expression data files  and produced as output a single virtual two-color gene expression data file each . these connectors were then implemented each as a separate java application, so each connector can be executed and used independently. connectors c <dig>  and c <dig>  were then composed to form connector c <dig>  which is responsible for controlling the ordering in which the simple connectors are executed, viz., first c <dig>  and then c <dig> . although connectors c <dig>  and c <dig>  can be executed in any order , we have chosen that specific sequencing because, once again, performance is not an issue in the scope of this work.

similarly to connector c <dig>  connector c <dig> as a whole was designed to provide both manual and automatic transfer of control to rgui . the automatic integration of the connector with rgui was also provided by the rserve api. since connector c <dig> is capable of providing both forms of transfer of control to rgui, the automatic integration was implemented independently of connectors c <dig>  and c <dig>  in order to maintain a sound implementation structure. nevertheless, the interaction with rgui takes place after connector c <dig>  concludes its execution.

rgui was used for the identification of differentiated expressed genes on the transformed  two-color microarray data using htself. rgui produced as output two numeric values associated to each gene, viz., the resulting value of htself and an associated p-value. these data had to be filtered directly by connector c <dig> in order to select for the functional analysis study only those genes whose values for htself and p-value are above and below provided thresholds, respectively. as a result, connector c <dig> produced as output a list of genes to be used as input by david.

the semantical mapping between concepts representing either consumed or produced data items and concepts from the reference ontology for connector c <dig> was simpler than for connector c <dig>  initially during the equivalence table construction, two out of three concepts representing a consumed data item  could not be mapped to an equivalent reference ontology concept. in principle, this was not a problem because these concepts were only used as filtering criteria by the connector for the production of the output list of genes. despite this fact, an equivalence relation was defined to associate instances of the concepts of gene, htself and p-value  with instances of the concept gene.

connector c <dig> was also implemented as a separate java application. this connector provided only manual transfer of control to david, since this tool does not provide an api for automatic interaction from a third-party application either. once the equivalence relation was defined, the specification and implementation of the grounding operations was straightforward. all data consumed and produced by this connector were stored in ascii text files .

discussion
we have developed an ontology-based methodology for the semantic integration of gene expression analysis tools and data sources using software connectors. our methodology supports not only the access to heterogeneous gene expression data sources but also the definition and implementation of transformation rules on exchanged data. first, we have defined a reference ontology for the gene expression domain. then, we have defined a number of activities and associated guidelines to prescribe how the development of connectors should be carried out. finally, we have applied the proposed methodology in the construction of three different integration scenarios involving the use of different tools for the analysis of different types of gene expression data. the availability of a step-by-step methodology based on a reference ontology for the gene expression domain facilitated the development of connectors responsible for the semantic interoperability of the proposed set of data and tools.

the two general approaches used in the semantic integration of bioinformatics tools and databases do not tackle adequately the integration of gene expression analysis tools. in the first approach, ontologies have been used as a common database model to integrate a number of related tools and/or data sets . although, in principle our reference ontology can be used as basis for the development of a  database schema for a number gene expression analysis tools, this is not the main purpose of our reference ontology. gexpo is used as a reference for mapping concepts representing consumed and produced data items, so they directly or indirectly  bear the same semantics as defined in the reference ontology.

in the second approach, mediators have been used to integrate heterogeneous data sources . mediators represent software entities capable of mapping concepts of a global  schema to concepts of a local schema. the role played by software connectors in our methodology resembles the role played by mediators, viz., both bridge the gap between global  and local . however, mediators are only used to support the translation of queries to local schemas, whereas software connectors can be used not only to perform queries on local databases, but also to relate and transform  input data onto semantically equivalent output data. besides, software connectors are primarily intended to integrate analysis tools instead of  databases in the context of this work.

semantic integration can also be achieved using a non-systematic approach. in such approach, which could also be based on software connectors, the semantic integration would be achieved on an ad-hoc basis, possibly using the concepts of an existing ontology as reference. however, the lack of a systematic methodology for achieving integration would likely result on a more complex, costly and error-prone development process. furthermore, the lack of structuring guidelines for implementing a connector would possibly reduce the likelihood of reusing existing connectors. alternatively, integration can also be achieved using a semantic flexibility approach  <cit> . in this case, no formal mappings are required, i.e., data are exchanged regardless of its meaning. an adapter, associated to each integrated application, receives exchanged data and assigns specific meaning to them according to each specific context of use. both the non-systematic and the semantic flexibility approaches fall short with respect to our methodology because our systematic approach reduces the chances of misinterpreting data and consequently increases the likelihood of producing meaningful results.

despite the benefits of our methodology, we must acknowledge a few limitations to our study. in the gene expression domain, there are many different tools and data sources, which can be combined in many different ways. in this sense, the first limitation of our work was the restricted number of integration scenarios in which our methodology was applied. actually, any methodological work such as ours presents the same limitation. however, our methodology is not restricted to the proposed set of data and tools. on the contrary, it was defined to be as general as possible, so it can be applied in the integration of different tools and data sources in the domain. the proposed integration scenarios were carefully defined. they include the most representative types of gene expression data, as well as tools representing some of the most common gene expression analysis activities. we have also combined tools with different access interfaces and policies. most of all, we have demonstrated how our methodology was applied to transform actual gene expression data so that semantic integration could be achieved.

the potential limitation of the reference ontology is another source of concern. we were able to successfully map all concepts representing a data item either consumed or produced by the developed set of connectors to a concept defined in the gene expression ontology. nevertheless, the development of other integration scenarios involving additional sets of data and analysis activities could possibly result in a situation in which such mapping could not be accomplished. in such case, we can fairly assume the reference ontology would be incomplete. still, since continuous modifications of existing ontologies according to emerging new biological insights represent a common practice in the biomedical domain, our reference ontology could also be subject to a review. besides, in addition to the set of concepts already defined for the different gene expression measurement approaches described in this paper, we have also included into the reference ontology a set of concepts related to another usual high-throughput gene expression measurement approach, viz., sage .

finally, despite all structuring guidelines provided by our methodology, there is a lack of  support for the implementation of the connectors, which can potentially represent a burden for a biologist undertaking this task. still, the mere existence of systematic methodology containing a set of guidelines for the design and implementation of connectors not only facilitates the development process but also helps reducing potential  mistakes that the biologist would more likely incur using an ad-hoc development process. in order to facilitate the implementation of connectors, we have developed the gelc api containing a number of classes representing different concepts of our reference ontology. thus, the biologist can focus on the implementation of the functional blocks of the connector under development. ultimately, most of the connectors developed in the context of this work were based on nontrivial transformation rules, which are unlikely to be properly generated by any  code generation tool.

to the best of our knowledge this is the first initiative to provide a systematic methodology for the semantic integration of gene expression analysis tools and data sources using software connectors. our methodology allows not only the identification of simple equivalence between concepts representing consumed and produced data items but also the definition of  rules in order to establish an equivalence between sets of concepts representing consumed and produced data items. further, our methodology separates the connector development guidelines from the reference ontology itself. thus, the same guidelines can be used in the semantic integration of tools and data sources in different  domains, such as proteomics, metabolomics and interactomics, provided that a suitable ontology is available to be used as reference for the target domain.

our ontology-based methodology can be used in the development of semantically integrated analysis environments. the proposed methodolody facilitates the development of connectors capable of achieving semantic interoperability between gene expression analysis tools and data sources. additionally, developed connectors are capable of supporting both simple and nontrivial processing requirements on exchanged data. our methodology can be used to create an integrated environment from a set of isolated  tools and data sources, as well as to extend an existing integrated analysis environment with the integration of new tools and data sources. thus, our methodology favors the execution of a broader and richer set of analysis activities on available gene expression data.

furthermore, the set of connectors developed in the context of this work can also be adapted and reused in the integration of other tools and data sources in the domain. for example, connector c <dig> can be adapted to provide integration to other kegg pathway analysis tools; connectors c <dig> and c <dig> can easily be adapted to process sage data; and, finally, connector c <dig> can be adapted to provide integration to other functional and enrichment analysis tools  <cit> . in this way, connectors can be reused to create other similar integration scenarios.

semantical integration is pivotal for gene expression analysis. on one hand, a semantically integrated analysis environment is fundamentally important for unveiling new biological knowledge. on the other hand, the lack of semantic integration can, most likely, produce results without biological significance. in many occasions, integration is carried out by someone with insufficient knowledge of the target domain. even if a domain expert is available, the absence of a systematic approach towards integration favours the arising of  inconsistencies because integration is based only on tacit knowledge. our methodology enforces the use of explicit knowledge, since conceptual models must be developed to represent input and output data items. then, the set of identified concepts are mapped to concepts from a  gene expression ontology, hence contributing for semantic accuracy.

CONCLUSIONS
high-throughput expression measurements of entire transcriptomes can be obtained through different techniques. these data have to be analysed using different tools in order to understand the underlying biological phenomenon. in order to facilitate such analysis, guaranteeing at the same time the soundness of the results, a semantically integrated analysis environment is needed. in this sense, we have developed an ontology-based methodology to support the development of software connectors to integrate gene expression analysis tools and data sources. our results indicate that the use of our methodology requires the biologist undertaking the integration task to explicitly reason about the underlying semantics of the concepts representing connector input and output data, thus contributing for the correctness and accuracy of the resulting integration as a whole.

any design methodology can be evaluated according to some general quality properties  <cit> . results from the application of our methodology indicate that the proposed methodology adhere to the following properties, viz., simplicity, since the methodology uses a minimal set of concepts, which facilitates its learning and application as a whole; systematicness, since the methodology provides a stepwise process to guide connector development, in which details are added systematically along the development trajectory; prescriptiveness, since the methodology prescribes what should be done rather than what may be done; and, finally, flexibility, since the methodology can be used in a variety of situations, without  changes or adaptations.

recently, we have observed an increasing number of gene expression analysis tools becoming available as web services. web services represent software resources with well-defined interfaces that can be executed remotely through the internet. communication and information exchange are carried out using xml-based standard internet protocols. similarly to the integration of different tools to create an integrated analysis environment, web services can be integrated to create a composed  service. web services whose interfaces are semantically enriched through the use of ontologies are called semantic web services. the availability of semantic descriptions for these services facilitates machine interpretation and, consequently, the automated execution of service compositions. in this sense, future research includes the development and  composition of semantic web services in the gene expression domain using the reference ontology proposed as part of this work. additionally, we will also investigate the role played by software connectors to enable proper service compositions.

competing interests
the authors declare that they have no competing interests.

authors' contributions
fm defined the methodology, helped to define the reference ontology and to implement the integration scenarios. gg defined the reference ontology and helped to draft the manuscript. rv helped to define the reference ontology, defined the integration scenarios and helped to draft the manuscript. cf helped to define the methodology and the reference ontology, implemented the integration scenarios and drafted the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
owl specification of the gene expression ontology. complete specification of the gene expression ontology in owl.

click here for file

 additional file 2
connectors c <dig> and c <dig> implementation. connectors c <dig> and c <dig> source code and documentation .

click here for file

 additional file 3
gelc api. gelc api binary code  and documentation .

click here for file

 declarations
publication for this article has been funded by the brazilian ministry of education .

this article has been published as part of bmc genomics volume  <dig> supplement  <dig>  2013: proceedings of the international conference of the brazilian association for bioinformatics and computational biology . the full contents of the supplement are available online at http://www.biomedcentral.com/bmcgenomics/supplements/14/s <dig> 
