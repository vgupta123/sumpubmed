BACKGROUND
text mining of the biomedical literature has gained increasing attention in recent years, as biologists are increasingly faced with a body of literature that is too large and grows too rapidly to be reviewed by single researchers  <cit> . text mining has been used both to perform targeted information extraction from the literature, e.g. identifying and normalizing protein-protein interactions  <cit> , and to assist in the analysis of high-throughput assays, e.g. to analyze relationships among genes implicated in a disease process  <cit> . systems performing text mining of biomedical text generally incorporate processing tools to analyze the linguistic structure of that text. at a syntactic level, systems typically include modules that divide the texts into individual word or punctuation tokens, delimit sentences, and assign part-of-speech tags to tokens. it is becoming increasingly common to perform syntactic parsing of the texts as well, either with a full constituent parse or a dependency parse representation. at a more conceptual level, named entity recognition, or identification of mentions of specific types of entities such as proteins or genes, is a widely used component of systems that aim to perform entity-oriented text mining. historically, the majority of research in biomedical natural language processing has focused on the abstracts of journal articles. however, recent years have seen numerous attempts to move into processing the bodies of journal articles. cohen et al.  <cit>  compared abstracts and article bodies and found that they differed in a number of respects with implications for natural language processing. they noted that these differences sometimes demonstrably affected tool performance. for example, gene mention systems trained on abstracts suffered severe performance degradations when applied to full text.

it has been previously noted that there was inadequate linguistically annotated biological text to make domain-specific retraining of natural language processing tools feasible  <cit> . with the release of craft, we now have a large resource of appropriately annotated full text articles in the biomedical domain to enable both evaluation and retraining.

in this paper, we will introduce the linguistic annotation of a significant new resource, the colorado richly annotated full text  corpus. craft consists of the full contents of  <dig> open access journal articles, comprising nearly 800k tokens. craft has been manually annotated with a number of elements of linguistic structure, corresponding to the functions listed above. it has also been annotated with semantic content, of biological concepts from several semantic classes characterized by biological ontologies. in prior work, we established that open access journal articles do not differ in terms of linguistic structure or semantic content from traditional journal articles  <cit>  and therefore take this corpus as representative of the biomedical literature more generally. along with this paper, we are publicly releasing  <dig>  of the articles, constituting  <dig> % of the tokens in the full corpus. it is available at http://bionlp-corpora.sourceforge.net/craft/index.shtml.

the availability of the craft corpus makes it possible for the first time to evaluate a number of hypotheses with exciting implications for the near-term development of biomedical text mining. in this work, we explore several uses of the craft corpus for evaluating the performance of natural language processing tools. we specifically consider  the generalizability of training algorithms and existing models to the new corpus, and  the impact of the availability of full text training data for new model development. a priori, genre differences have plagued natural language processing for years, and full texts are clearly a different genre from that which most extant systems have been developed on — abstracts of journal articles  <cit> . those who have worked with full text have noted various ways in which full texts differ from abstracts  <cit> , mainly focusing on distributional differences of certain types of keywords and assertions. nonetheless, a few authors have developed systems to process full text. friedman and rzhetsky developed the genies system, which processes full-text journal articles  <cit> , rzhetsky’s geneways system does as well  <cit> , and the recent biocreative iii evaluation required systems to process full text  <cit> .

in this work we first introduce the syntactic annotation of the craft corpus. the annotation of genes and ontological concepts is described in more detail in bada et al.   <cit> .

next, with this sufficiently large collection of annotated biomedical full text documents, we report the head-to-head performance of a number of language processing tools selected for their difficulty, for their relevance to any language processing task, and for their amenability to evaluation with well-annotated gold standard data. specifically, we examined the performance of tools for: 

· sentence boundary detection

· tokenization

· part-of-speech tagging

· syntactic parsing

· named entity recognition, specifically of gene names

sentence boundary detection was included because it is an essential first task for any practical text mining application. tokenization was included both because it is an essential prerequisite for any practical language processing application and because it is notoriously difficult for biomedical text . part-of-speech tagging and syntactic parsing were included because the use of syntactic analyses in biomedical text mining is a burgeoning area of interest in the field at present  <cit> . finally, gene mention recognition was included because prior work has shown drastic differences in gene mention performance on full text across a range of gene mention systems and models  <cit> . we perform a broad survey of existing systems and models, and also retrain systems on the full-text data to explore the impact of the annotated training data.

previous investigations of syntactic parser performance on biomedical text  <cit>  have focused on parser performance on biomedical abstracts rather than full text publications. in particular,  <cit>  evaluates accuracy on only  <dig> manually reviewed sentences, while  <cit>  explore similarly small corpora of  <dig> and  <dig> sentences, respectively. the craft corpus, in contrast, contains over  <dig>  manually analyzed parsed sentences in the portion we are publicly releasing at this time – the full contents of  <dig> journal articles, containing over 500k tokens .

prior biomedical corpus annotation work
there has been significant prior work on corpus annotation in the biomedical domain. until the very recent past, this has focused on the biological, rather than the medical, domain. the biological corpora are most relevant to the work discussed here, so we focus on them. the biomedical corpora site at http://compbio.ucdenver.edu/ccp/corpora/obtaining.shtml currently lists  <dig> biomedical corpora and document collections. of this large selection, we review here only some of the most influential or recent ones.

the flagship biomedical corpus has long been the genia corpus  <cit> . studies of biomedical corpus usage and design in  <cit>  reviewed several biomedical corpora extant as of  <dig> with respect to their design features and their usage rates outside of the labs that built them. usage rates outside of the lab that built a corpus was taken as an indicator of the general usefulness of that corpus. these studies concluded that the most influential corpus to date was the genia corpus. this was attributed to two factors: the fact that this was the only corpus containing linguistic and structural annotation, and the fact that the corpus was distributed in standard, easy-to-process formats that the natural language processing community was familiar with. in contrast, the other corpora lacked linguistic and structural annotation, and were distributed in one-off, non-standard formats.

the genetag corpus  <cit>  has been very useful in the gene mention recognition problem. it achieved wide currency due to its use in two biocreative shared tasks. the bioinfer corpus  <cit>  is a collection of  <dig> sentences from abstracts of journal articles, annotated with entities according to a self-defined ontology and showing relationships between them by means of a syntactic dependency analysis. the bioscope corpus  <cit>  is a set of  <dig>  sentences that have been annotated for uncertainty, negation, and their scope. most recently, the various data sets associated with the association for computational linguistics bionlp workshop  <cit>  have been widely used for their annotations of multiple biological event types, as well as uncertainty and negation.

RESULTS
annotation of document structure, sentence boundaries, tokens, and syntax
syntactic annotation: introduction
although craft is not the first corpus of syntactically annotated biomedical text, it provides the first constituent annotation of full-text biomedical journal articles. penn treebank’s bioie project provided much of the basic skeleton for the workflow of this type of annotation. however, we did have to make several new policies or expand existing ptb policies for syntactic annotation in the biomedical domain .

the markup process of the craft corpus consisted of phases of automatic parsing and manual annotation and correction of all  <dig> articles in the corpus. automatic segmentation and tokenization were performed, then part of speech tags were automatically applied to every token in the data according to each token’s function in a given context . we employed penn treebank’s full part of speech tagset  without any alterations . this output was then hand corrected by human annotators.

after hand correction, the data was then automatically parsed into syntactic phrase structure trees with penn treebank’s phrasal tagset. syntactic nodes indicate the type of phrase of which a token or a group of tokens is a part. they form constituents that are related to one another in a tree structure where the root of the tree encompasses the largest construction and the branches supply the relationship between the main components of the tree  and each of these main components may contain internal phrase structure. craft added  <dig> nodes representing article structure, cit, title, heading and caption , to the original tagset. the automatically processed trees were then hand corrected. automatic parsing did not provide function tags or empty categories, which were also adapted from the penn treebank syntactic tagset, so those were added by hand during bracketing correction. function tags are appended to node labels to provide additional information about the internal structure of a constituent or its role within the parent node. craft added one new function tag, -frm . empty categories provide a placeholder for material that has been moved from its expected position in the tree, arguments that have been dropped, such as an empty subject, or material that has been elided.

the data was finalized with two iterations of quality control verification to ensure that all the data was consistently annotated and that all policy changes that were adapted at different stages of the project were properly implemented across all data. a rough estimate of the total time required to syntactically annotate the full corpus is approximately  <dig> hours a week for  <dig>  years .

given the input text, “little is known about genetic factors affecting intraocular pressure  in mice and other mammals” , the final segmented, tokenized, part-of-speech tagged, syntactically parsed and annotated output is as follows, with each phrase in parentheses and part of speech tags to the left of their respective tokens.

)





)





)





)



))





)





)))))))))

.)

we describe below the major implementations and policy adaptations that yield the above tree.

selection and amendment of annotation guidelines
for the pos annotation, we chose to follow the 3rd revision of the pos-tagging guidelines of the penn treebank project  <cit> .

for the treebanking, we have followed the guidelines for treebank ii  <cit>  and treebank 2a  <cit>  along with those for the bioie project  <cit> , which is an addendum to the treebank ii guidelines based on annotation of biomedical abstracts. employing these guidelines of the penn treebank project enables us to contribute our collection of richly annotated biomedical texts to a larger collection of treebanked data that represents a multitude of genres and that already includes biomedical journal abstracts. finally, we modified or extended these guidelines to account for biomedical phenomena not adequately addressed in them . a set of these changes was made at the beginning of the project resulting from examination of the corpus articles, and further changes were made throughout the course of the project as issues arose; descriptions and examples of these changes can be seen below.

training of annotators and creation of markup
the lead syntactic annotator , who had five years of syntactic annotation experience, first trained the senior syntactic annotators , the former of whom trained a third senior syntactic annotator . these lead and senior annotators were responsible for policy changes, documentation, quality control, and training of additional annotators, who were required to have some knowledge of syntax and semantics  and some previous experience in syntactic annotation. these additional annotators were first trained to perform pos tagging for approximately one month with penn’s newswire training files and then on a chapter of an introductory biology book  <cit> , followed by treebanking training for several weeks to one month on short training files obtained through the penn treebank project. treebanking training continued on the aforementioned book chapter and finally on the first article of the corpus. altogether, training for syntactic annotation lasted approximately six months. all training was performed on flat text .

for the syntactic annotation of the corpus, sentence segmentation, tokenization, and pos markup was first automatically generated using the genia parser. each article’s automatically generated markup was manually corrected by one annotator in the lex mode of emacs. this was followed by the automatically generated treebanking of these articles  using the parser of the opennlp project. each article’s automatically generated treebanking markup was then manually corrected by one annotator using treeeditor. since they are not generated by this parser, the annotators used treeeditor to add empty categories, which are syntactic place holders in the tree construction that indicate arguments that have been moved from their expected positions in the trees, and functions tags, which specify additional information about phrases not represented in the treebanking markup, e.g., the location of an action. additionally, sentence-segmentation errors not previously found were corrected manually outside of treeeditor, as it does not have the capability of merging sentences. the corrected output of this annotator was checked by the syntactic lead annotator.

the output of the syntactic lead then underwent the final phase of syntactic annotation, referred to as the quality-control phase. this phase consisted of automatic validation of pos tags  and of sentences  using corpussearch followed by manual correction of indicated errors. this step allowed us to confirm tree uniformity, to verify that errors had not been introduced during the manual correction of previous passes, and to ensure that changes in annotation guidelines or policy made during the project were consistently reflected in the final output. for example, during the course of annotation, the treatment of prepositional phrases beginning with “due to” changed from being annotated as recursive prepositions, i.e., ), to being annotated as flat multiword prepositions, i.e., . a validation script was written to detect recursively annotated occurrences of such prepositional phrases, an example of which is provided below.

these results explain why defective pdgf signal transduction results in a reduction of the v / p cell lineage and ultimately in perinatal lethality due to vessel instability . <dig> pp-prp:  <dig> pp-prp,  <dig> in,  <dig> due,  <dig> pp )



 ))))))

this error message indicates that there is a recursive pp error and provides the full sentence, the reference number of the element involved in the error, and the current parse of the tree. given this output, the annotator manually corrected this error in the file by deleting the extra pp node for “to”.

guidelines
full-text journal articles present issues that can be uniquely distinguished from the style of the abstracts that the penn bioie project annotated. we found that penn’s guidelines for biomedical abstract annotation did not cover the increased technical complexity of a full-length article, such as the parenthetical information, definitions, and figure and table captions found throughout a full-text article, necessitating regular policy review and addendum construction. major changes to penn’s guidelines include addition of node labels title, heading and caption to replace the -hln function tag , and cit for citations. we have added one new function tag, -frm, to the top-level constituent  of formulas, where a mathematical symbol  is treated as a verb. the use of the prn node label has been expanded from the tb2a policy  <cit> , which only allows for a clausal prn . because of the large number of nominals and other parentheticals in the craft data we have allowed any node label inside of prn. the use of the -ttl function tag has been slightly modified from ettb as well. each of these node and function label additions and expansions have been made in order to provide labeling that accurately represents the more complex structure of biomedical articles.

we have also changed how shared adjuncts are bracketed, which are now adjoined to coordinated vp or s, added more structure to single token coordinated nmls, and refined penn’s pos and tokenization policy to account for additional symbols, such as ∘  . another significant change we have made is the elimination of pp-clr. ptb <dig> allows for pp-clr on verbal arguments. however, we felt that this policy was not clearly defined and it was difficult to consistently apply. we have retained the -clr in s-clr for resultatives and secondary predicates.

the last change we implemented was the complete elimination of the empty category *p*  introduced in the penn bioie guideline addendum. with the increased complexity of full-length articles, we felt that these policies were difficult to apply consistently and greatly increased the complexity of the annotation and resulting trees. we maintain that existing policy on nml and np coordination preserves much of the same information represented by *p*.

in ptb <dig>  the -hln function tag indicates a headline or a dateline, as found in newswire texts. however, the section headings in journal articles have a slightly different function and convey different information than a news headline. since the treebanked data are journal articles, we are using more informative labels for nodes that would have been tagged with -hln  based on newswire bracketing guidelines .





))

‘‘clinton to leave'' 



)))

‘‘soldiers killed by bomb'' 

craft addenda
labels were created for information that is unique to the structuring of a journal article. the craft annotation guidelines add title, heading, and caption node labels to denote these sections of journal articles. below are several examples of usage. 

journal title: title)

section headings: heading)))figure, table, and picture captions: caption))

range)

)



mouse strains))

.))‘‘an almost two-fold range of iop between genetically distinct mouse strains.''



mice)))

:





))))

‘‘intraocular pressure in genetically distinct mice : an update and strain survey''

these nodes require internal structure the same as other main text nodes. however, title, heading, and caption nodes have only one daughter. in cases where titles, headings or captions are not complete sentences, frag may be used to make a single constituent of the daughter nodes.



:

))

while title, heading, and caption are new nodes that have been added to ptb2’s original tag set, we have also changed the scope of some existing node labels, such as prn .

prn and cit
we have expanded the use of prn to include citations that consist of other referential material such as page or footnote numbers, figure and table information, or extra-sentential details. the prn node is put inside of whichever node it seems to be modifying. sentence-final parentheticals modifying the entire sentence are placed inside the vp containing the matrix verb, mirroring the placement of sentence-level adjuncts.

adding primary label cit for inline citations
in penn bioie addendum , citations are annotated as follows:



-rrb- ))

since citations are pervasive in journal articles and books, we have added a cit node for inline citations. the internal structures for citations are flat. cit applies only to author references that occur inside of parentheses.



‘‘'' 

all other, non-parenthetical references are bracketed as normal text.

citations that are part of the argument structure of a larger sentence are annotated fully as ordinary text:



,



)))

‘‘the second paper, by davies et al'' 



)

)))

‘‘reviewed in furumura et al. 1996'' 

expansion of prn
we have expanded its use to include citations that consist of other referential material such as page or footnote numbers, figure and table information, or extra-sentential details. if the sentence contains only one parenthetical at the end of the sentence, then this is a daughter of the vp; otherwise, it is within whichever node it seems to be modifying.

)))



]))) .)

‘‘these mutations shift the spectral profiles of the translation products  <cit> .''

]))



)



))))) .)

‘‘es cells  were maintained under standard culture conditions in the presence of lif..'' 

addition of -frm function tag
we have added one new function tag, -frm, to the top-level constituent  of formulas in which a mathematical symbol  is treated as a verb.

)

)

‘‘p <  <dig> ''

the above is interpreted as “p is less than  <dig> .” orthographically, the copula is not realized—thus we have created the -frm tag to denote the difference between formulas and canonical sentence structure.

shared vp and s adjuncts
in the ptb, shared adjuncts for coordinated vps are left at the conjunction level:





)

and

)

))))))

‘‘the company expects to obtain regulatory approval and complete transaction by year-end.'' “by year-end” is shared by both vps “obtain regulatory approval” and “complete transaction”, but is attached at the same level as those two vps to form a flat structure. craft adds a layer of vp so that the pp modifier and the coordinated vp are at different levels of attachment to make more explicit the shared distribution of the pp. 



)

and

)

))))))

shared modifiers can also occur at the s level. when two clauses share a modifier, the modifier is adjoined to the coordinated s’s. 



))) ))



)

and









 ))))))))

.)

‘‘after puncture of coagulated blood from the corpora cavernosa urine retention developed and a suprapubic catheter had to be introduced temporarily for urine drainage.'' .

tokenization and part of speech tagging of hyphens and symbols
we adopted penn’s tokenization policy regarding hyphens, slashes, dashes and symbols, in which expressions containing hyphens and symbols are split into multiple tokens, with the exception of a list of bound affixes that don’t provide meaning in isolation .

hyphens, slashes, and dashes
dashes are split and are interpreted as prepositional phrases when they are used to denote a range, as in pages in a parenthetical reference:



)



)))

)

‘‘we have focused on developing the mouse system for iop and glaucoma studies  <cit> .'' 

in the above citation the dash is read as “to.” when a dash is pronounced  it is tagged sym to distinguish hyphens and dashes that perform syntactic functions from those that simply link words together.

the negative symbol is also annotated as a pronounced symbol and receives the sym pos tag:

)



 )

)))

‘‘average of - <dig>  mmhg''

we also split off all slashes in the text into separate tokens. when a slash is pronounced  it received the sym pos tag:

 )



)))

‘‘ <dig> mg/kg''

compare the above with use of hyph for hyphens and slashes that are not pronounced:

  )

)

‘‘neuron-packing density''  )

)

‘‘balb / cj strain''

in keeping with penn’s tokenization policy we decided to treat numbers in temperatures as multi-token expressions. temperatures containing a ‘°’  symbol are split into two tokens; the number is pos tagged as a cardinal number, cd, and ‘°c’ forms a constituent that is pos tagged as a singular noun as in “37°c,” ).

the percent symbol is also split. however, it receives the pos tag nn for singular noun, rather than sym.

  )

)

)

‘‘greater than 90% inhibition''  )

)

‘‘4% fat'' 

symbols in mathematical formulas are split and pos tagged as sym.

)



)))

‘‘n = 4'')



)

‘‘p <  <dig> ''

we did not split certain symbols from their names, since they are part of the name as a whole and do not serve a specific function such as joining terms together  or providing other syntactic or semantic information .

 



)

))

‘‘homozygous lethal '' 

)

‘‘the p <dig> δ pgfp exchange construct'' 

elimination of -clr
in ptb <dig> one use of the function tag -clr is to label prepositional phrases that have a closer relationship with a verb than simply that of an adjunct. whether a pp is an argument or not to a large extent depends on the specific verb and it is hard to make a general characterization about the nature of this category without referring to this verb. therefore the penn bioie addendum has a long list of verbs that take a pp that can be labeled -clr. below is an example of pp tagged -clr:

))

we believe such argument structure information is better handled in a separate layer of propbank-style annotation that focuses on the argument structure of each verb. in the treebank annotation, we avoid using this functional tag. following craft’s policy, the above example is annotated as follows:

))

note that we have retained the use of s-clr to mark resultatives and secondary predicates, as defined in the treebank 2a guidelines:







)))

modification of -ttl
the -ttl function tag was originally used only to mark the titles of created works. however, it also marks a form of nominalization, as titles whose internal structure is not nominal can behave nominally. for example, “in the heat of the night” or “one flew over the cuckoo’s nest” can function as the subject of a sentence or the object of a preposition.

the following is an example of -ttl in its more traditional usage:



))

,

‘‘

)

·



 

)



)) )))

''))

‘‘the eu project eumorphia, ‘‘understanding human molecular physiology and pathology through integrated functional genomics in the mouse model'' '' 

as in ettb, craft allows the extension of -ttl to other instances of this referential nominalization that can occur outside of titles. for example:







) )))))

and





) ))))

.)

‘‘significance was set at p <  <dig>  and suggestive refers to p <  <dig> .'' 

this sentence has two nominalizations. suggestive refers to the word suggestive itself rather than the quality of being suggestive. similarly, the formula  is functioning nominally within the larger context of the sentence, in its position as object of a preposition.

elimination of *p*
*p* is used in the penn bioie project as a place-holder for a distributed premodifier or head, and it is used exclusively in coordinated nominal constructions. here is an example where *p* is used in the penn bioie project:

)

and

))

craft annotates the tree as:  ras)

this structure would represent the fact that k- and n- are both modifiers of ras. please refer to the craft addendum to ptb <dig> and penn bioie guidelines for a more detailed discussion of *p*.

nml modification
in general, we have maintained the current policies of ptb2a regarding annotation within np: *p* is not used and the nml node label is used for sub-np nominal substrings . however, in conjunction with other policy changes that explicitly annotate the scope of coordinated structures , we have slightly expanded the use of nml in certain single-token coordinated structures. by current tb2a policy, single-token coordinated nominal heads with shared premodifiers are left flat:



in craft, we explicitly show the scope of the, which is modifying both cats and dogs, by putting a nml node around cats and dogs:

)

more examples of this expanded use of nlm from pmcid 11532192: ))

)



))

in this way, we more closely align the annotation of these single-token coordinated heads with existing ptb2a policy regarding the use of nml in multi-token coordinated phrases with shared premodifiers:



and

))



and

))

‘‘the pupil and optic nerve'' 

as in ptb2a, we also use nml in multi-token nominal premodifiers of nouns, as in:



destruction)

‘‘red blood cell destruction'' …where red and blood modify cell, forming the sub-np constituent red blood cell, which modifies destruction.

some other examples of nml in craft marking this type of complex nominal modifier:



profiles)

‘‘the overall gene expression profiles'' 

and

)

inbred strains)

‘‘c57bl/6j and 129x1/svj inbred strains'' 

%)

inhibition)

‘‘a greater than 90% inhibition'' 

annotation products and quality assessment
the syntactic annotation of the craft corpus consisted of manual annotation, including manual correction of automatic parsing, of  <dig> full-text biomedical journal articles. the  <dig> fully syntactically annotated articles yielded  <dig>  sentences and  <dig>  tokens, and include  <dig>  syntactic nodes,  <dig>  function tags, and  <dig>  empty categories. the initial release of  <dig> articles yielded  <dig>  sentences and  <dig>  tokens and include  <dig>  syntactic nodes,  <dig>  function tags, and  <dig>  empty categories. see additional file  <dig> for counts of each node, tag, and empty category.

when the opennlp syntactic parser output was compared to the gold standard it achieved  <dig>  recall and  <dig>  precision, whereas the average accuracy of annotators when compared to the gold standard is  <dig>  recall and  <dig>  precision . this indicates a large human annotation effort to correct automatic output. automatic parsing of biomedical literature is not consistent enough to rely only on automatic methods to provide precise data. average inter-annotator agreement is  <dig>  recall and  <dig>  precision. full details of iaa are available in table  <dig> 

evaluation of named entity recognition systems and syntactic parsers
we consider the performance of existing systems on several tasks important for biomedical natural language processing: gene/protein mention recognition and syntactic analysis of text, including the syntactic pre-processing steps of sentence boundary detection, tokenization, and part of speech tagging.

for each tool assessed, we produced results over the craft text using the models with which they are distributed and compared the produced annotations to the craft gold standard annotations using standard measures. we also retrained several of the tools on the craft data to assess the impact of retraining, performing five-fold cross-validation of a training set sub-selected from craft. we report the performance on a held-out development set in both cases .

gene mention recognition
the craft corpus semantic annotations include annotation of gene mentions. the sequence ontology  <cit>  was used as the target ontology for mentions of sequences, including genes and gene products. entrez gene  <cit>  identifiers are used to associate gene mentions with a specific gene. we utilized these annotations to evaluate several popular named entity recognition  systems that focus on recognition of protein or gene names. ner of gene/protein mentions has been the subject of several shared tasks  <cit>  and is a common step in other bionlp applications, such as protein-protein interaction extraction or gene-disease relation extraction. ner systems aim to identify relevant names in text, and delimit the boundaries of those names. they do not typically attempt to map those names to a specific database identifier  and therefore our evaluation focuses only on the detection of relevant strings in the text.

bada et al. describes the semantic annotation of craft  <cit> . the annotation identifies mentions of genes and their products, including a determination of type . we compared these annotations with mentions found by the ner systems. the sequence type annotations, however, are very detailed and the set of annotations for a single type do not in every case correspond to a cohesive set of annotation categories from a given ner system/model, such as the “protein” category of the biocreative datasets  <cit> , or nlpba’s “dna” and “rna” categories  <cit> . the problem of inconsistency among annotation category sets has also been investigated by  <cit> , who introduced an aggregate tag, gene-or-gene-product .

to achieve better coverage, we also aggregated several semantic classes utilized in craft based on domain knowledge, for the purpose of evaluation. the mappings are listed in table  <dig>  the names reflect a combination of the main focus of the class and the kleene star  character used in regular expressions. in the final form, the aggregations are explicitly defined in terms of specific classes and do not make use of regular expressions . we tested various combinations of ner system categories to craft semantic classes for each system, depending on the categories used by the system. in the tables below, we use a lowercase descriptor for the source ner system categories and an all caps descriptor for the target craft semantic classes. for instance, table  <dig> refers to “protein-polystar”. this means that the abner category of “protein” was allowed to match any of the craft classes listed for polystar in table  <dig>  including “polypeptide”, “macromolecular complex”, or “transcript”. that is, if abner produced an annotation with the category “protein” where the craft gold standard has e.g. an annotation with the class “transcript”, this was counted as a true positive in the evaluation.

tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> show the results for the gene mention systems with the distributed models. distributed models are trained on one of several available gene mention corpora: the nlpba corpus  <cit> , the genia corpus  <cit> , the biocreative i gene mention corpus  <cit> , or the biocreative ii gene mention corpus  <cit> . two sets of results are provided, based on a comparison of the system output on  the development portion of the craft public release data set and  the complete initial public release of craft of  <dig> files . we performed experiments with several variables: 

·system: the system used to produce the results.

·model: the specific model used by the system to produce the results.

·annotation comparison strategy: the specific strategy used in assessing precision and recall of gold standard annotations .

·annotation class mapping strategy: the defined mappings from annotation classes in the source system model to the craft model .

in performing annotation comparison, gene mentions were scored with respect to four progressively less strict types of mention boundaries due to differences in what the different automatic taggers considered proper mention boundaries. the various strategies are summarized in table  <dig> 

the specific results shown in tables  <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> represent only a few of the possible semantic class mappings. while full results are available as additional material , here we have selected the  <dig> top-performing mappings for each system/model combination. examination of the tables shows significant variability in performance, depending on the different variable settings. some variability can likely be attributed to differences in size or quality of the underlying training corpora , while some likely has to do with differences in the learning algorithms. with regard to overall system performance, the banner system produced consistently higher results than other systems with various semantic class mappings, though it was only tested with one distributed model.

a comparison with the abner system, also using a biocreative-derived model, shows that banner outperforms the abner biocreative model. abner with the biocreative model outperforms the same system with the nlpba model; this indicates that the biocreative gene mention data is more similar to craft gene annotations than the nlpba data. in general, increasing the scope of the semantic classes considered to be a gene mention in craft increases precision. this indicates that most gene ner systems employ a generous definition of a “gene”, while the craft annotations are more fine-grained and semantically precise. finally, we note that the results of all systems and models are consistently worse on the full craft initial release set than the smaller development set. this suggests that despite our best efforts to partition the craft data into unbiased subsets, there may still be some important variation.

for those ner systems that were straightforwardly trainable, we retrained them on the craft data . tables  <dig> and  <dig> show the performance of the retrained gm systems on the development portion of the craft corpus. the retrained systems are trained with the aggregations of the semantic classes derived from the sequence ontology, shown in column  <dig> of table  <dig>  the systems were trained only with annotations in the relevant aggregation, and evaluated on the corresponding annotations . for abner in particular , we see that some combinations of categories seem to perform particularly badly, indicating that those categories may be particularly difficult to recognize. lingpipe  has somewhat more consistent results across the various category groupings, through there is still significant variability. a possible explanation for lower performance with the polystar mappings may be that there is insufficient training data in those aggregations to derive a good model. all system performances were statistically different than the others, ; see methods section statistics used for nlp tools performance differentiation for the details of the test performed.

for both systems, the best results are obtained when all of the various semantic classes are grouped together both for training and for evaluation, suggesting that the systems have done a reasonable job of generalizing over the different types of sequence mentions. when compared with the performance with distributed models, the lingpipe system performed better upon re-training, achieving a highest f-score of  <dig>  as compared to  <dig>  on the development set with the distributed models . in contrast, the abner system had an overall drop in performance on retraining; it was able to achieve much better precision at a substantial cost to recall.

the lingpipe results after retraining are encouraging, and do slightly outperform the best out-of-the-box results we achieved with banner. we believe the modest improvements upon retraining may be due to how we structured the learning problem: due to overlaps among the different aggregation sets, we removed any existing annotations not in a given aggregation set before training. this means that the system cannot take advantage of constraints among different annotation types to improve the model for the target category. it is well-established in the machine learning community that learning multiple categories simultaneously generally results in better overall performance of the model. we look forward to more experimentation with learning ner models over the craft data to better understand this behavior.

syntactic pre-processing: sentence boundary detection, tokenization, and part of speech tagging
a number of steps in any text mining pipeline or machine learning algorithm are dependent on the accuracy of lower-level task performance. for this reason, we evaluated the performance of systems for sentence boundary detection, tokenization, and part of speech  tagging.

the input to the sentence detectors was the original plain text  articles with no markup. the sentence-detected output was the input to the tokenizers. each tokenizer was paired with its own sentence detection tool . similarly, the input to the pos-taggers was sentence and token annotated data from the corresponding tools. all token, sentence, and pos annotations from the various tools were evaluated using the strict span matching criteria .

sentence boundary detection was evaluated on the basis of precision/recall of character-based sentence boundary placement. post-processing was performed that removed whitespace from character span counts at the end of sentence annotations and that removed empty-span sentence annotations. table  <dig> shows the results for sentence boundary detection. the permutation test  showed that the difference in performance between lingpipe and the other two tools was significant ; the difference between opennlp and uima was not. the major difference between the high performance of lingpipe and the lower performances of opennlp and uima is that the former is able distinguish section headings from the surrounding text.

tokenization and pos tagging were evaluated likewise. table  <dig> shows the results for tokenization. the permutation test showed the performance of each tool to be significantly different than the others . here, we see that the default uima tokenizer actually outperforms the more specifically biomedical tokenizer of the pennbio framework; this likely stems from the treatment of punctuation in our annotation guidelines.

the results for pos tagging are in table  <dig>  the permutation tests showed that all system performances were significantly different , except for lingpipe with the genia model against opennlp. here we see surprisingly poor performance, with none of the systems reaching even  <dig>  f-score on the craft data, well below state of the art for general english pos tagging. the highest-performing system  is a model specifically trained on biomedical text, indicating the importance of domain-relevant training material. the lowest performing models, lingpipe with the brown model and with the medpost model, have different tag sets, which greatly impairs their apparent performance when compared against the penn tagset used in craft. to adjust for the different tagsets and provide an upper-bound notion of pos tagger performance, those tags that did not align with the gold standard set were removed from the evaluation for all four tools. these adjusted values are presented in parentheses in table  <dig>  note that even opennlp and lingpipe with the genia model have a higher upper-bound than their actual performance; this is because each have a small set of tags that do not align to the gold-standard tagset. the overall low performance even with those tools using the penn tagset, , is of concern for bionlp systems, since much downstream processing makes use of pos-tagged data .

numbers in parentheses indicate the upper-bound performance potential of the tools, calculated by removing occurrences of tags that did not align to the gold-standard tagset.

parsing
we compared parsers under a variety of conditions related to  type of model and  type of output. we differentiated between parsers distributed with models built on non-biomedical text and parsers with models built on biomedical text. we differentiated between dependency parsers and constituency parsers. for parsers that could be trained, we retrained them on the craft data .

constituency parsing
constituency parsers vary in their required input formats and allowable configuration. the required input format for each parser, which varied from one token/pos-tag pair per line to one sentence per line with specific delimiters between tokens and pos-tags, was extracted from the gold-standard treebanked parses from the public release craft set. parsers that could be configured to accept sentence-split, pre-tokenized, pos-tagged input were provided this pre-processed input derived from the gold standard. parsers that could not were provided just the gold standard sentence-split input . the charniak-lease and charniak-johnson parsers are very similar. we show results from the older charniak-lease version of the parser because it was distributed with a model that was trained on biomedical text. the stanford parser accommodated a character encoding configuration and that was set to handle the input as utf- <dig> 

to evaluate full syntactic parses, we used the version of evalb provided with the stanford parser java  <dig> . <dig> package  <cit> . the evalb scoring categories are labeled bracket precision, , labeled bracket recall , and f-score , applied to each sentence as a whole. the values presented here are the sentence scores averaged over the section of corpus being tested. except where noted, all comparisons of tools on the same dataset were statistically different  using the permutation test .

each parser struggled with a different small set of sentences that it could not parse, and the parse output of these sentences varied per parser. in some cases the parser output had to be manipulated manually to conform to a format that evalb could handle. evalb skips any sentence for which the token count between the gold-standard sentence and the automatically parsed sentence does not match, and sentences that could not be parsed fall into this category. additionally, some parsers retokenized input containing punctuation despite being given gold-standard tokenization and pos information; in some cases these alterations changed the token count, leading to higher counts of sentences that were not evaluated by evalb; this figure is shown in the ‘unevaluated count’ column of the tables.

parsing results for parsers distributed with general english  parsing models appear in tables  <dig> and  <dig>  the mogura, charniak-lease and charniak-johnson parsers are distributed with models trained on biomedical annotated text . also, the stanford  <dig> . <dig> parser is released with a default model that includes training from sections of the genia corpus in addition to general english text  <cit> . the results from evaluation of the development set using these biomedical models are presented in table  <dig>  comparing tables  <dig> and  <dig>  we see that, perhaps counter-intuitively, on the craft development set the general english models outperform the biomedical models, even when the same underlying system is used. the exception is the mogura parser, which had nearly identical performance in both cases. the results of the parsers using the biomedical models on the release set appear in table  <dig>  note that we were unable to obtain successful parses on craft with the enju parser using the distributed biomedical model and so no results for that parser/model combination are included here.

for parsers that allowed retraining, we performed 5-fold cross-validation on the training set and report the performance on the development set; see table  <dig>  not surprisingly, the parser performance using the craft-retrained models showed a large improvement over those using the distributed models. the berkeley parser showed greater improvement than the stanford or bikel parsers, with the best results of about 83% lb-f.

dependency parsing
while the craft corpus has been syntactically annotated with constituent trees, the use of dependency parses rather than constituent parses is becoming increasingly common in biomedical natural language processing. clegg and shepherd  <cit>  have argued that measuring parser performance through constituent-based accuracy fails to adequately distinguish between real differences in meaning derived from incorrect syntactic analysis and minor differences of convention that do not truly affect the output of text mining systems. hence, we perform an analysis of constituency parses that have been translated to dependency structures. this also enables comparison of the craft trees with the output of the dependency parsers. to do this comparison, the gold standard constituency parse was translated to a dependency representation.

two kinds of dependency parses are evaluated here: parses that originated from a dependency parser and parses that originated from a constituency parser and were converted to dependency representations. performing constituent-based parsing followed by conversion of the outputs to dependency trees has been shown to give higher accuracy than performing parsing directly to dependency trees for stanford dependencies  <cit> . this is mostly because the dependency structures we are evaluating against are themselves converted from constituent-based trees. on the other hand, performing constituent-based parsing and doing the conversion is literally  <dig> times slower than performing dependency parsing directly .

like the constituent parsing, dependency parsers were provided gold standard tokenization and pos-tags extracted from the gold standard public release set of craft. the output was evaluated using the standard measurements typically used at conll for dependency parse evaluation. the labeled attachment score  corresponds to a complete comparison of the dependency structures in the system to the structures in the goal, for each sentence, requiring that individual tokens are assigned to the correct head, with the correct dependency relation. the unlabeled attachment score  relaxes the requirement that the dependency relation matches, only requiring association with the correct head. the labeled accuracy score  requires that the dependency relations match, but relaxes the requirement of being assigned to the correct head.

micro accuracy of a fold is calculated as in equation  <dig>  i.e. the accuracy across all individual gold standard dependencies in the fold. macro accuracy is calculated as the average of accuracies across all trees in the relevant fold. we have not calculated accuracy averaged across individual documents, due to the differences in the number of sentences in the documents. 

  microaccuracy=÷ 

for the dependency parser output, we report the individual score on each training fold, the average across the training folds, the score on the development set data for a model trained on the complete craft training set, and the score on the development set data for the standard model for each parser trained on the penn treebank wall street journal corpus . tables  <dig> and  <dig> show the results for the dependency parsers we tested.

CONCLUSIONS
we began this work by introducing two use cases for the craft corpus,  evaluation of existing tools and  retraining of those tools. our investigations have led to several conclusions.

algorithms and models differ in their generalizability
it is not controversial to state that different algorithms differ in their ability to train models that generalize to novel corpora. however, as the work of banko and brill  <cit>  has shown, these differences may become apparent only as increasing amounts of data become available. we suspect that it is also the case that these differences may become apparent only as increasing numbers of genres become available. prior work has looked at differences in performance based on training on the wsj versus biomedical abstracts; the work reported here adds a new dimension to genre variability by introducing the full text of biomedical articles, which differ with respect to structure and content from both wsj articles and biomedical abstracts  <cit> .

tool performance is increased
as was shown in the sections on parsing, tool performance is increased when applications are re-trained on the data in the craft corpus. this means that the bottleneck in performance that the field previously faced when trying to move from processing abstracts to processing full text can be overcome.

our current results for retraining the gene mention recognition systems unfortunately did not show much improvement. we anticipate that these will improve significantly after some reconfiguration of the learning problem posed to the gene mention recognition systems, as described at the end of section gene mention recognition.

craft is a high quality resource
the work reported here has demonstrated that the data in the craft corpus can be used to train high-performing models for a variety of language processing tasks. in addition, we have shown that there is high inter-annotator agreement for the syntactic annotation of the corpus . taken together, these results support the conclusion that the craft corpus is itself of high quality.

building a state-of-the-art bionlp system
based on the experiments described here, there are several tools that stand out for consideration for inclusion within a bionlp system targeted at full text biomedical publications. for sentence boundary detection, the lingpipe sentence boundary detector out-performed others by a significant margin. for tokenization, the default tokenizer within uima does a good job. none of the part-of-speech taggers did a great job without retraining, though the opennlp tagger had the highest precision. given that gold standard pos tags were provided to the parsers in most cases, it could be expected that use of a low-performing tagger would result in lower than reported parsing accuracy in a natural setting. however, the clearparser dependency parser with craft-trained model would be an excellent choice; the berkeley parser with craft-trained model should work well for constituency parsing of full text. finally, for gene mention recognition banner appears to provide good out-of-the box performance, while lingpipe responded well to re-training. we hope to retrain banner on craft in the near future to see additional performance gains.

the effect of differing annotation guidelines
a possible reason for the differing performance of various tools on this full-text corpus is differences in annotation guidelines. however, this can be ruled out as the explanation for all differences. in previous work, we showed that performance differences, and sometimes quite drastic ones, manifest themselves when tools are evaluated separately on paper abstracts and paper bodies  <cit> . since the annotation guidelines were identical for all parts of the articles, these differences cannot be due to differences in annotation guidelines—the only variable in this study was abstracts versus article bodies. we also note that although differences in tag sets could explain some of the differences in performance of part of speech taggers when applied to our full-text corpus, it clearly cannot explain all of it, since performance differences were noted even when the tag sets were the same.

the future of bionlp with the availability of craft
we retrained a relatively small set of tools for this study ; it is exciting to think what advances could be made if additional tools are retrained on this corpus, and if different strategies are explored for taking advantage of the annotations. furthermore, we look forward to still more annotation of this material by us and by other groups to support richer models integrating different aspects of language, including discourse and pragmatics.

ongoing and future work
we are currently producing a number of additional sets of annotations for the craft corpus: 

relations: assertions of relationships between semantic types already annotated in the corpus are in progress.

coreference: all coreference in the corpus is being annotated. the process and guidelines are discussed in  <cit> .

discourse: discourse functions have been marked at the sentence level.

parentheses: all parenthesized text is being classified according to an ontology of parenthesis contents in scientific journal articles. the ontology and preliminary scores for a classifier for the ontology concepts are described in  <cit> .

evidence sentences: all sentences used as evidence for go annotations at mgi are being marked.

these new sets of annotations will be released as they are completed.

