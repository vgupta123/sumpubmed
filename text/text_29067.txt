BACKGROUND
models in computational biology are becoming increasingly complex, as in-silico frameworks are expanded to account for our rapidly increasing knowledge of physiological mechanisms  <cit> . this poses considerable challenges for uniquely linking model parameters to experimental data. the desire to capture this complexity to simulate physiological function increasingly results in models where the identifiability of parameters from available experimental data is relatively low. this situation is exacerbated by the lack of consensus on the optimal method for fitting model parameters to data, taking into account the, often, poor signal to noise ratio in these measurements. furthermore, in many cases the model structure is such that the inverse problem of parameter fitting is ill-posed due to multiple parameter values producing the same model output. finally, measured data in the literature is often incomplete, and state-of-the-art models are consequently based on a synthesis of data measured at different temperatures, from different laboratories and often from different species  <cit> .

the reuse, combination and extension of existing models are necessary components of the physiome approach  <cit> . in particular, as new datasets become available, and as models are applied to address new hypotheses and understand physiological situations, model developers are likely to need to augment or extend models or model components. this implies a requirement for a methodology for comparing model predictions with experimental data in a robust and automated fashion, efficiently incorporating new knowledge to better constrain the model parameters, systematically searching for the perturbation of the system that highlights parameter sensitivities and constrains the system, as well as reducing models to the minimal applicable version .

we believe that reduction in model complexity is important in that it typically increases the sensitivity of model outputs to the various parameters and hence the consequences of introducing changes to the model become more transparent. it also improves the likelihood that the models will be predictive outside the regime of the parameterising data. specifically, if the identifiability of model parameters can be increased, this will enhance the ability to find the most relevant experimental measurements to use in order to constrain parameters within a given model framework, decreasing the uncertainty in parameter estimates.

in this study we address the issue of ill-posed inverse problems through the development of a generic framework for combined model parameterisation, comparison of model alternatives and analysis of model mechanisms. the fitting of model parameters from measured data is based on a combination of inverse metamodelling  <cit>   and iterative cost-function-based identification  of the simulations resulting in values of the output metrics in close proximity of the measured values, and subsequent zooming into relevant regions of the parameter space. in contrast to conventional nonlinear fitting or minimisation algorithms that only estimate parameter values, this method provides an overview of the parameter space and identifies regions in the parameter space where model outputs match measured data. the variation in possible solutions thereby provides an estimate of the uncertainty in the parameter values. moreover, the inverse metamodelling component of the fitting pipeline provides an implicit sensitivity analysis and a quantification of the identifiability of model parameters from measured data.

in the look-up component of our proposed pipeline, the output spaces of model alternatives are analysed using principal component analysis   <cit> , providing effective visualisation of the consequences of introducing changes to models and allowing identification of redundant model components. hence, this modelling framework represents a combined parameter fitting and systematic analysis of model behaviour and model mechanisms for possible model reduction. this has the clear advantage that it provides a transparent link between parameter values and experimental data in comparison to alternative methods such as simplex optimisation  <cit> , simulated annealing  <cit>  and levenberg-marquart optimisation  <cit> , which only provide parameter value estimates without increasing the understanding of the underpinning model mechanisms.

we demonstrate our proposed approach by applying our parameter fitting pipeline to re-parameterise the cardiac cell contraction model developed by niederer et al. <cit> , originally fitted to rat experimental data at room temperature, to represent mouse functionality at 37°c by iteratively matching the output from the niederer-model to a combination of measured data and the outputs of the land-model  <cit>  . the lack of a complete and self consistent data set of all output metrics of interest from a single species, temperature and laboratory motivated the use of simulated outputs from one model as a substitute for measured data in the parameter fitting. using in silico data also provides the opportunity to analyse how the parameter identifiability can be increased by introducing additional output metrics for which measured values are not available in the literature, guiding future measurements.

following re-parameterisation of the niederer-model, we apply the same methodology for finding reduced model versions through the identification of redundant model components. specifically, we demonstrate how our methodology can be used for systematically comparing model versions, analysing the sensitivity of the model outputs to the input parameters, and choosing the most reduced version giving outputs matching measured data.

methods
application system
as outlined above we demonstrate our methodology by applying it to two models of cardiac cell contraction, consisting of differential equations describing contractile force, including length-dependence and velocity-dependence. the choice of application system was motivated by the high degree of maturity of cardiac models; the heart is arguably the most advanced example of a multi-scale framework for biology. both these models represent cardiac muscle cells which consist of many contractile sub-units, sarcomeres, each again organised into thin and thick filaments  <cit> . the thick filaments contain myosin crossbridges that bind to the thin actin filament to generate force. electrical activation results in an increase in cytosolic calcium , and binding of calcium to the regulatory calcium binding site on troponin c  within the sarcomeres. this causes a conformational change in the associated tropomyosin complex that unblocks the thin filament actin sites for binding to the thick filament myosin crossbridges. in a crossbridge cycle, a myosin crossbridge on the thick filament attaches to the actin thin filament, performs a power stroke to generate force, and then detaches using adenosine triphosphate . both models applied in this study consist of equations representing the influence of the muscle’s length on the tension it generates , and the sensitivity of the generated force to the rate at which the muscle is stretched . the velocity-dependence parts of the two models are based on the same mathematical formulation, which is therefore not considered in this study . both models, parameterised from a range of data, are biophysically based, and represent two different frameworks for simulating the generation of contractile force in cardiac cells as a consequence of calcium binding . a description of the two contraction models including the differential equations is given in additional file  <dig> 

both the land-model and the niederer-model were developed specifically for use with organ-scale simulations, and therefore have a relatively low level of detail compared to many other contraction models. specifically, they do not include many sub-states for the attachment of atp and the position of crossbridges. however, both of these models do include enough detail to enable the direct linking of parameters to biological data and exploration of different mechanistically based hypotheses.

the niederer-model was originally parameterised using data for rats at 25°c, the calcium/tnc dynamics are modelled by a simple molecular binding model, and tropomyosin/crossbridge dynamics are represented by the transient changes in the proportion of available actin sites, while the binding sensitivity is length-dependent. with the default parameter values, the niederer-model is unable to capture the fast relaxation kinetics of mouse cardiac muscle at higher pacing frequencies.

the land-model uses a standard cooperative binding equation which has a hill curve as its steady state solution to represent troponin binding, where the calcium half activation of maximal steady state tension generation is length-dependent, combined with a modified version of the crossbridge dynamics component from the model developed by rice et al. <cit> , which uses a 4-state markov model. the land-model uses only  <dig> of these states, the so-called non-permissive and permissive  states.

evidence of the velocity-dependence of tension generation and the dynamic response to step changes remains controversial in the experimental literature. the fading memory model   <cit>  provides a succinct representation of these dynamics without being tied to a specific underlying mechanism, and is exploited by both models. the fmm represents the velocity response as several strain-rate dependent variables which all decay with time. an advantage of this model is that it is independent of the contraction model, and can be added after modelling isometric tension and length-dependence.

our analysis of the two contraction models consists of the following steps:1) sensitivity analysis and parameter identifiability analysis based on statistically designed model simulations and metamodelling. this was carried out to test whether the niederer-model parameters could be predicted directly from the land-model outputs using regression, and to identify redundant model components for both models. this is illustrated in figure  <dig> ) due to the relatively low prediction accuracy of the resulting inverse metamodel for several of the niederer-model parameters, the inverse metamodelling approach was combined with a cost-function based look-up of simulations resulting in model outputs in close proximity to the target values. this was carried out iteratively as shown in figure  <dig>  resulting in a zooming into the region of the parameter space where the target outputs were replicated by the simulations.

3) model reduction by repetition of step  <dig> using reduced model versions. the reduced model versions were made based on the results from the parameter identifiability analysis, which was done for both models.

sensitivity analysis and parameter identifiability analysis of the niederer-model
in order to obtain an overview of the relationships between input parameters and dynamic outputs of the model, an experimental design of the niederer-model parameters using relatively wide parameter ranges was made using a latin hypercube design   <cit> . lhd is a semi-random sampling procedure that is especially suitable for use on high-dimensional data, since it separates the data into several hypercubes, and samples randomly within each hypercube. this ensures that all regions of the parameter space are sampled. within our implementation, the parameter ranges in table  <dig> were used to generate a lhd of  <dig> parameter value combinations, and simulations where run with the niederer-model using cell lengths of  <dig>   <dig> and 110% of resting sarcomere length. an input ca-transient measured for mouse at 37°c   <cit>  was used in all simulations. all simulations and subsequent analyses were done in matlab® version r2012b  <cit> .

ca
50ref
k
refoff
k
on
n
r
β
0
β
1
t
ref
α
0
α
r1
α
r2
k
z

output metrics used to represent the model behaviour

tension transients were simulated using both the land and niederer contraction models, and described by routinely experimentally measured descriptors of the transient morphology. a list of the descriptors and their recorded experimental values for mouse at 37°c is shown in table  <dig>  tension transients were simulated at three cell lengths  activated by the experimentally measured ca-transient in figure  <dig> 

*
these metrics were calculated from simulations using a ca-transient.

*from a simulation at resting sarcomere length.

preliminary analyses of the results achieved by fitting the model parameters to the metrics in table  <dig>  using data obtained by simulations using the experimentally measured ca-transient scaled by  <dig>   <dig> and 110%, showed that the model outputs were highly sensitive to the calcium concentration. in order to take this into account we also matched the force-pca  relationships of the two models, using metrics from simulations run with fixed ca-concentrations as additional model characteristics to constrain parameters. the ca-concentrations used were a logarithmically spaced series of  <dig> different concentrations from  <dig>  to 1 μm together with the concentration 10 μm. the resulting steady state tensions were normalised by the maximal simulated tension value.

model and experimental steady state force-calcium curves are routinely approximated by a hill-curve that can be logarithmically transformed to be linear. the relationship between pca and log) was therefore fitted to a straight line using ordinary least squares  regression  <cit>   < 10- <dig> were removed in order to avoid numerical errors), and the metrics given in table  <dig> were calculated to represent the properties of the force-pca relationship. the f-pca curves were simulated for  <dig>   <dig> and 110% of resting sarcomere length, and the resulting f-pca metrics used as additional output constraints  to fit the parameters of the niederer-model. similarly, the final set of target output measures included both the metrics in table  <dig> and those in table  <dig>  all calculated from simulations with  <dig>   <dig> and 110% of resting sarcomere length for the land-model.

*
r
2
these metrics were calculated from simulations using fixed ca-concentrations.

*from a simulation at resting sarcomere length.


sensitivity analysis by classical metamodelling

partial least squares regression   <cit>  was then used for regression-based sensitivity analysis. plsr is a subspace-based regression method based on decomposing the data into a subspace representing the main features of covariance between the regressors  and the response variables . this subspace is represented by latent variables called score- and loading vectors. plsr can be seen as a regression analogue to pca, and can handle numerous input and output variables simultaneously. unlike for example ols regression  <cit> , plsr does not require the regressor variables to be linearly independent. coupling between parameters can be revealed using plsr-based classical metamodelling through analysis of the regression coefficients for cross-terms between the parameters. in addition, the correlations  between all input parameters and output metrics included in the analysis were evaluated to obtain overview of the model system.

based on the parameter-simulated output data for the niederer-model, a classical metamodel was first constructed to predict the output metrics as functions of the parameters using plsr. this classical metamodel was used for sensitivity analysis, using the regression coefficients as sensitivity measures , as described in  <cit> . the metamodelling procedure is illustrated schematically in figure  <dig>  here, both parameters and output metrics were centred and standardised by subtraction of the mean value and dividing by the standard deviation of each variable prior to the regression, making the regression coefficients independent of the scales of the variables and thereby easier to compare in the sensitivity analysis. cross-terms and second order terms of the input parameters  were included in the metamodelling to take nonlinearities into account when predicting the output metrics.


parameter identifiability analysis by inverse metamodelling

to evaluate whether it would be possible to get a reasonable estimate for the niederer-model parameters by direct prediction using regression, an inverse metamodel, predicting the input parameter values from the simulated output metrics in table  <dig> and table  <dig>  was generated using hierarchical cluster-based partial least squares regression   <cit> . hc-plsr is a nonlinear extension of plsr. as described above, plsr can handle correlated regressor variables, which makes it especially useful for inverse metamodelling of large, complex dynamic models, which contain large sets of highly coupled differential equations producing correlated model outputs. hc-plsr is a locally linear regression method based on separating the observations into groups using fuzzy c-means  clustering  <cit>  on the latent variables from a global plsr model including all observations, and making local plsr models within each cluster. this allows prediction of highly nonlinear input–output relationships. the inverse metamodelling procedure is also schematically illustrated in figure  <dig>  while the hc-plsr method used for the metamodelling is outlined in additional file  <dig> 

both parameters and output metrics were centred and standardised by subtraction of the mean value and dividing by the standard deviation prior to the regression, and  <dig> clusters where used in the hc-plsr. the number of clusters was chosen based on a comparison of predictive ability between different hc-plsr metamodel complexities ranging from models using 1– <dig> clusters. this comparison showed that  <dig> clusters was the minimum number of clusters providing maximal predictive ability, and  <dig> clusters were therefore used to limit the metamodel complexity. cross-terms and second order terms of the output metrics were included in the inverse metamodelling to predict the input parameters, in order to better handle nonlinearities in the input–output relationships of the model.

due to the relatively large differences between the default outputs from the land-model and the niederer-model, it was necessary to obtain a robust estimate of the predictive ability of the metamodel to evaluate whether it could be used to directly predict new parameter values for the niederer-model. the inverse metamodel was therefore validated using 33% of the simulations from the experimental design of  <dig> simulations as a separate test set. hence, the metamodel was calibrated using only 2/ <dig> of the simulations, while the rest of the simulation results were kept aside for the purpose of prediction and thus not included in the parameterisation of the metamodel. this process produces a valid estimate of the ability of the metamodel to predict the parameter values from a new set of measured data.

fitting of the niederer-model parameters
the results from the sensitivity analysis and the parameter identifiability analysis above showed that the identifiability was relatively low for several of the niederer-model parameters . we therefore combined the inverse metamodelling with an iterative generation of new experimental designs in the parameters, and identification of the simulations resulting in output metrics in close proximity to the target values. the target output metrics were found from simulations run with the land-model using the default parameter set and otherwise the same settings as for the niederer-model simulations. these were used as substitutes for measured data in the parameter fitting pipeline. a schematic representation of the parameter fitting pipeline is shown in figure  <dig>  the initial niederer-model parameter ranges are given in table  <dig>  and were used to generate the initial experimental design . following simulations with the contraction model, the output metrics described above were calculated from the model outputs generated using the parameter values from the experimental design .

the next step of the fitting pipeline is to generate an inverse hc-plsr metamodel, predicting the niederer model parameters as functions of the output metrics in table  <dig> and table  <dig>  based on the simulation results. this metamodel is then applied to the target outputs  to generate an initial estimate of the model parameters . the inverse metamodelling is performed in the same way as described above under “parameter identifiability analysis by inverse metamodelling”.

for each set of output metrics corresponding to one of the parameter sets in the experimental design, the proximity to the target is found , and the predicted parameter set from the inverse metamodelling is then combined with the  <dig> simulations that generated observations in the closest proximity to the experimental measurements . the predictions from the metamodelling were only included for those parameters for which the inverse metamodel could provide a prediction accuracy of >70% in a test set validation. together, these  <dig> parameter sets  are used to identify the direction or localised region of the parameter space that allows the model to best match the target observations. using the  <dig> simulations having the lowest distances to the measured metrics in the guideline set was considered sufficient in order to balance between zooming into the relevant parameter space region and keeping the possibility of identifying alternative regions giving feasible output metrics. this increases the likelihood that all possible regions in the parameter space that can produce physiologically feasible simulations are found during the parameter fitting. this, preferably together with constraints on the parameter values according to a priori knowledge about possible ranges, can generate robust/unique parameter estimates. the size and number of identified regions of the parameter space producing model outputs that replicate measured data give an indication of the uniqueness of the parameter estimates.

the achieved distances to the target outputs are found by pca of the output metrics resulting from the simulations together with the target output , and calculation of the root mean square distances  of each simulation to the target in the pca scores. the pca scores are used to evaluate the distance to the target both in order to decrease the dimensionality of the data and to weight the metrics according to their contribution to the variation in the data. the pca approach decomposes the data into latent variables describing the main variance directions in the data, and each score vector is a weighted sum of the original variables. hence, the metrics having the largest contributions to the variation in the data have the highest weights for the first principal components  resulting from the pca. the minimal number of pcs explaining 99% of the variance are included in the distance calculations in our fitting pipeline.

for each parameter, the new parameter range for the next iteration is set to the value span over the guideline sets  ± an additional span defined by a variable called stepsize
new
 ). the ranges for the new experimental design are calculated using equations  and .

  maximumvaluesi=maxix1+x1i¯stepsizenew 

  minimumvaluesi=minix1-x1i¯stepsizenew 

the variable stepsize
new
 was introduced to allow adjustment of the spread in parameter values according to the degree of proximity to the target outputs. initially, the value of stepsize
new
 is  <dig> in order to analyse a large part of the parameter space. in each following iteration, the minimum achieved rmsd in the pca score space is compared to that for the previous iteration, and stepsize
new
 is increased by  <dig> if the value has decreased, until it reaches a maximum value of  <dig>  hence, the value of stepsize
new
 is increased as the distance from the target decreases, strengthening the zooming effect. if stepsize
new
 reaches the value  <dig> before the results are sufficiently close to the target metrics values, stepsize
new
 is decreased by  <dig> for the next iteration design.in each iteration, a new experimental design of parameter value combinations is generated using lhd in the region of the parameter space defined by the new parameter ranges. the number of simulations for each iteration is given as input to the fitting pipeline. here, using a lhd size of  <dig> simulations was regarded sufficient in order to sample the parameter space relatively densely, while limiting the computational time used in each iteration. this procedure  is repeated iteratively until the success criterion is met .

for our specific application, the criterion for success of the parameter fitting was defined as follows:

1) for resting sarcomere length simulations: the tension transient metrics should be within the error bars for the measurements in table  <dig> 

2) for 110% of resting sarcomere length simulations: the peak tension should be between  <dig> and  <dig> kpa  and the minimum tension should be less than  <dig> kpa.

3) for 90% of resting sarcomere length simulations: the peak tension should be between  <dig> and  <dig> kpa .

4) for the force-pca curve simulations: the rmsd between the simulated force for the niederer-model and the target land-model force  should be less than 15%.

the test set prediction accuracy of the inverse metamodel was relatively low for several of the parameters , so the metamodel was used only in the first iteration to provide an initial indicator of the direction in the parameter space to move . the constraints given in table  <dig> were set on the parameters based on the variation in measured values in the literature.

k
refoff
k
on
n
r
β
0
t
ref
the fitting pipeline was written in matlab® version r2012b  <cit>  as both a parallelised and a non-parallelised version, and both can be obtained from the authors upon request.

reduction of model complexity for the niederer-model and the land-model

parameter identifiability analysis for the land-model

in the same way as for the niederer-model, the possibility for reducing the land-model was tested based on a similar test set validated inverse hc-plsr metamodelling. the metamodel was made using data from simulations in a lhd of  <dig> observations within the ranges given in table  <dig>  using the output metrics in tables  <dig> and  <dig> to predict the land-model parameters. all variables were centred and standardised prior to the regression, and cross-terms and second order terms of the output metrics were included.

t
ref
ca
50ref
trpn
50
n
trpn
k
trpn
n
xb
k
xb
β
1
β
0

model reduction


reduction of the niederer-model the parameter fitting procedure described above was repeated with parts of the niederer-model omitted in order to see whether the model could be reduced while keeping the replication of the simulated output data from the land-model. the choice of model parts to omit was based on the results from the sensitivity- and parameter identifiability analysis, indicating very low sensitivity to the parameters n
r
, α
r2
 and k
z
. these parameters were assumed to have minor effects on the model outputs, and were therefore omitted by making the model independent of these model parts. this omission was achieved by giving the parameter α
r2
 the value zero, making the model independent also of n
r
 and k
z
.

the initial parameter ranges in the fitting pipeline were the same as in the previous parameter fitting , and all output metrics in tables  <dig> and  <dig> were included to fit the model parameters.


reduction of the land-model based on the parameter identifiability analysis of the land-model, k
trpn
, n
xb
 and k
xb
 were successively set equal to  <dig> , in order to analyse the model output effects of variations in these parameters. the simulations were run as described above, and all output metrics in tables  <dig> and  <dig> were included in the analysis.

RESULTS
as described in the methods section, we have analysed the sensitivity of the model outputs to variations in the input parameters, verified parameter identifiability and compared and matched the model outputs for the two cardiac contraction models. the analyses were based on both simulations run using a measured ca-transient for mouse at 37°c to generate dynamic tension transients, and fixed, individual ca-concentrations to simulate the steady state f-pca curve. the niederer-model was re-parameterised using the presented parameter fitting pipeline in figure  <dig> using a combination of measured data and synthetic data from land-model simulations. reduced versions of both models were identified based on the parameter identifiability analysis and comparison of the outputs from reduced model versions with the land-model default outputs. the results are detailed below.

sensitivity analysis and parameter identifiability analysis of the niederer-model

sensitivity analysis by classical metamodelling

sensitivity analysis based on a classical plsr metamodel indicated that physiological simulations using the niederer-model had low sensitivity to the parameters n
r
, γ, α
r2
 and k
z
, while they were most sensitive to ca
50ref
, k
refoff
, β
0
, β
1
, nh and t
ref
. the regression coefficients from the plsr showing the sensitivity of the output metrics to the input parameters are shown in figure  <dig>  these results indicate that parts of the niederer-model tropomyosin kinetics component can be simplified by omitting the low sensitivity parameters. the model equations in additional file  <dig> show that giving α
r2
 the value zero would make also n
r
 and k
z
 redundant, significantly reducing the model complexity. this possibility was therefore analysed further below.the sensitivity patterns described above were confirmed by the plot of the correlations  between all input parameters and model output metrics included in this analysis, shown in figure  <dig>  as expected due to the sampling procedure used to generate the experimental design of parameter sets, figure  <dig> shows no strong correlations between the input parameters in the model. however, there are several strong correlations between the output metrics. this was also expected, since they are metrics representing curve shapes. however, the results also show correlations between the metrics representing the tension transients and those representing the force-pca-relationship.


parameter identifiability analysis by inverse metamodelling

the parameter prediction accuracies from the inverse hc-plsr metamodel are shown in figure  <dig>  represented by the correlation coefficients  between the simulated and the predicted parameters from a test set prediction. the test set consisted of 33% of the simulations from the lhd of  <dig> simulations. these simulations were not included in the calibration of the metamodel, and therefore represent new data, so that the resulting predictive ability would be what we can expect from a prediction using new measured data . as figure  <dig> shows, the inverse metamodel was not able to predict all parameters accurately, but could give useful information about the parameters β
1
, β
0
 and t
ref
. the reason why some of the model parameters that the sensitivity analysis indicated a model sensitivity to were predicted incorrectly by the inverse metamodel is probably that the model is sloppy, meaning that many parameter value combinations can generate the same output metrics values. this characterises most dynamic models  <cit> . the model can still be sensitive to variations in these parameters, but it is difficult to predict parameter values from the output metrics for sloppy models. however, our results demonstrate the value of using inverse metamodelling to give an indication of the best direction in the parameter space to move to approach reasonable estimates for the values of the three parameters for which the prediction accuracy was relatively high. for the other parameters the inverse metamodel alone will not provide an estimate that can be trusted. the fitting procedure therefore had to be extended by including the look-up approach to guide new simulations.

fitting of the niederer-model parameters
figure  <dig> shows a comparison of the outputs from simulations with default parameter values and resting cell length for the two models. as the figure shows, the niederer-model is not able to capture the faster relaxation kinetics of the mouse at higher pacing frequencies. this was expected, since the niederer-model was originally parameterised using a different ca-transient and fitted to experimental measurements from a different species. figure  <dig> shows the results from a pca of the simulation results based on the parameter value combinations generated by the initial experimental design using the parameter ranges in table  <dig>  together with the results from the land-model. this was the pca used in the first iteration of the fitting pipeline.

using our presented parameter fitting pipeline, three niederer-model parameter sets were identified that fitted the land-model data. the three successful parameter sets found  gave outputs from the niederer-model matching the land-model outputs for all three cell lengths used, including the force-pca relationships. the force-pca relationship for parameter set  <dig> in table  <dig>  which gave the best match to the land-model outputs, and the tension transients for all parameter sets in table  <dig> are shown in figure  <dig>  the force-pca relationships for the remaining parameter sets in table  <dig> are shown in additional file 3: figure a <dig> . the spread in parameter values provides an indication of how constrained a parameter is for a given set of output metrics. in this specific case, the niederer-model parameters could be constrained to a standard deviation of on average  <dig> % of the mean values over the succeeding parameter sets. figure  <dig> shows the  <dig> simulations from the lhd used in the last iteration together with the land-model output in the score space from a pca of all output metrics. as expected, the simulation results are significantly closer to the region of the land-model outputs than in the first iteration .

ca
50ref
k
refoff
k
on
n
r
β
0
β
1
t
ref
α
0
α
r1
α
r2
k
z
model outputs were matched to target data for  <dig>   <dig> and 110% of resting sarcomere length.

reduction of model complexity for the niederer-model and the land-model

parameter identifiability analysis for the land-model

in order to identify a reduced version of the land-model, a lhd of  <dig> simulations were made with the parameter ranges given in table  <dig> for the nine length-dependence parameters of the land-model. an inverse metamodel was made in the same way as for the niederer-model, and the test set parameter prediction accuracies achieved are shown in figure  <dig>  the results in figure  <dig> show that only t
ref
 and β
0
 had r
2
-values above  <dig> , but also ca
50ref
 and trpn
50
 had r
2
-values above  <dig> , which is a reasonably good prediction accuracy considering the large span in parameter values utilised here. n
trpn
 and β
0
 had r
2
-values around  <dig> . hence, most of the parameters from the land-model could be constrained by the output metrics considered. however, k
trpn
, n
xb
 and k
xb
 were not as well constrained, having r
2
-values below  <dig> . hence, the possibility for reducing the model complexity by making a steady state approximation by increasing k
trpn
 and k
xb
 to  <dig> times the default value was analysed as described below. the low sensitivity to n
xb
 may be explained by the coupling to n
trpn
, which was illustrated in  <cit> . the effects of removing this parameter by setting it to  <dig> are analysed below.


model reduction


reduction of the niederer-model the values of α
r2
 in table  <dig> are close to zero, and according to the analysis above the niederer-model has low sensitivity to this parameter. hence, we tested whether the model can be simplified by giving this parameter the value zero. this gives k
1
 =  <dig>  k
2
 =  <dig> , and thereby a simplified equation for z
max
. the parts of the equation system containing the parameters n
r
 and k
z
 would then also be zero, making these parameters redundant. a new parameter fitting was therefore carried out, starting from the same initial parameter ranges as in the first parameter fitting, but now with α
r2
 =  <dig> in all parameter sets. the same parameter fitting procedure as described above was used, and four parameter sets  were found to give values of the output metrics close to the target values. comparison of the parameter sets in tables  <dig> and  <dig> shows that the values are relatively similar for most parameters. hence, two separate parameter fittings identified the same parameter space region, giving confidence in the parameter estimates.

ca
50ref
k
refoff
k
on
β
0
β
1
t
ref
α
0
α
r1
model outputs were matched to target data for  <dig>   <dig> and 110% of resting sarcomere length.

the force-pca relationship for parameter set  <dig> in table  <dig>  which gave the best match to the land-model outputs, and the tension transients for all parameter sets in table  <dig> are shown in figure  <dig>  the force-pca relationships for the remaining parameter sets in table  <dig> are shown in additional file 3: figure a <dig>  and figure a <dig> . our results therefore indicate that it is possible to reduce the niederer-model by setting α
r2
 to zero while keeping the same model behaviour.

for this reduced model version, the parameters could be constrained to a standard deviation of on average  <dig> % of the mean values over the succeeding parameter sets, as compared to  <dig> % for the original model version. this is not a very large decrease in the spread of resulting parameter sets, but this model reduction process has clear advantages in terms of ultimately increasing the capacity to derive physiological insight from the model behaviour and identification of feasible measurements to make in order to constrain parameters.


reduction of the land-model the parameter identifiability analysis indicated that the land-model had relatively low sensitivity to the parameters k
trpn
, n
xb
 and k
xb
 in the part of the simulation space analysed here. these three parameters were therefore successively given the value  <dig>  while all the other parameters were kept at the default values, and simulations were run in order to analyse the consequences these changes had for the model outputs. giving these parameters the value  <dig> simplifies the equation system for the land-model . setting n
xb
 =  <dig> led to relatively large changes in model behaviour , as expected considering the importance of thin filament cooperativity. however, setting k
trpn
 =  <dig> or k
xb
 =  <dig> had only relatively small consequences for the behaviour; the force-pca relationships were identical to the default output, and the tension transients were still within the measurement error compared to the default tension transients . hence, this indicates that it is possible to speed up these components of the land-model to near steady state by setting k
trpn
 =  <dig> or k
xb
 =  <dig> while keeping approximately the same model behaviour. this result was probably caused by the fact that the measured time to peak is relatively low for mouse, giving these two parameters undefined upper bounds given the metrics included in this analysis . setting both to  <dig> simultaneously caused the time to peak to be too low compared to the measured data, as expected. this indicates that it is difficult to identify the rate-limiting step using the metrics included in this study, something that is consistent with the coupling of k
trpn
 and k
xb
 found previously  <cit> .

discussion
in this study, we have presented and demonstrated the value of a generic and robust methodology for combined parameter fitting and analysis of model mechanisms. to demonstrate this method, we have adjusted the parameters of the niederer-model to fit data for mouse at 37°c. we also succeeded in finding reduced versions of both the land-model and the niederer-model through comparison of model alternatives and fitting of reduced model versions to measured data. our results indicate that this is an effective approach for comparing model alternatives and reducing models to the minimum complexity replicating measured data.

in our analysis we make the assumption that the equations capture the salient first order dynamics of our system of interest. both models applied here are biophysically based. by understanding the relationships between the parameters and model predictions, we gain further insight into the regulation and physiology of our system. the niederer-model has two relaxation terms, but setting α
r2
 to zero leaves only one relaxation term. the omitted relaxation term was designed to fit rapid relaxation rates following a step change in calcium due to the activation of a calcium chelator. however, our analysis shows that a simpler model suffices for contraction under conditions of regular changes in calcium, which includes most physiological conditions. the land-model starts with only one relaxation term, so it cannot be removed. setting the parameters k
trpn
 or k
xb
 to  <dig> are approximations for very fast or near steady-state kinetics.

the fitting pipeline includes an implicit sensitivity analysis and analysis of parameter identifiability, making it suitable for testing hypotheses for model reduction. hence, an advantage of this method compared to alternative methods is that it not only provides the parameter values, but also gives an estimate of the identifiability of parameters and the uncertainty in the parameter estimates through both the range of values in the feasible parameter sets and the ability of the inverse metamodel to predict the different parameters. combining the analysis of model mechanisms with parameter fitting makes it possible to automatically detect how the behaviour of the model as well as the parameter identifiability changes as a consequence of moving to different parts of the parameter space, and whether adjusting certain parameters makes other parameters or model components redundant.

sensitivity analysis
biological models typically contain numerous output metrics resulting from large sets of coupled equations, and complex covariance patterns often exist between the outputs. choosing the measurements to make in order to constrain biological parameters thus requires sensitivity analyses and parameter fitting methodologies that can take numerous output variables into account simultaneously and evaluate the impact of parameter value perturbations on the entire model system. regression-based sensitivity, as used here, is based on deriving a selection of data points by experimental design or semi-random sampling, and analysing the resulting input–output relations using regression  <cit> . the regression coefficients then provide direct measures of the impact of variations of the individual inputs on the output. most regression-based sensitivity analyses published are based on relatively simple linear models fitted by ols regression  <cit> . in this study, the sensitivity analysis was based on classical plsr metamodelling due to its ability to handle linearly dependent regressor variables, several response variables simultaneously and to utilise inter-correlations between the response variables for regression model stabilisation.

metamodelling has been widely used in e.g. engineering, for speed-up of computations, sensitivity analysis and uncertainty assessment  <cit> , and recently, multivariate metamodelling using plsr  <cit>  and hc-plsr  <cit>  has been shown to be effective for analysis of the complex, nonlinear input–output relationships of biological models. classical plsr metamodels, where the model outputs are predicted as functions of the input parameters, are useful for sensitivity analysis and analysis of interactions between input parameters and covariance patterns between multiple model outputs  <cit> .

several alternatives to regression-based sensitivity analysis exist, such as rank transformation, first- and second order reliability algorithms  and variance-based methods  <cit> . rank transformation is an alternative to conventional regression-based sensitivity analysis in cases where the input–output relations are monotonically nonlinear, while reliability algorithms are used in cases where the primary focus is on a particular mode of failure of the system rather than the entire spectrum of possible outcomes. variance-based methods, such as sobol's method  <cit> , use analysis of variance -type decomposition of the output function into a polynomial expression including cross-terms between the input parameters. partial variances are computed from each of the terms in the decomposition, and the sensitivity of each term is defined as the partial variance divided by the total output variance. however, these methods concentrate on the effects on one output variable at a time, and are therefore not as useful for analysis of biological systems that typically contain intricate feedback loops.

parameter fitting
as described above, in order to re-parameterise the niederer-model, we used a combination of inverse metamodelling  <cit> , predicting the input parameter values directly from the model output metrics, and iterative zooming into the relevant region of the parameter space based on a look-up approach. however, numerous alternative methods exist to fit model parameters from measured data. optimisation of the parameter values based on simplex optimisation  <cit>  is a widely used approach. however, the results become unreliable when many parameters are required to be fitted simultaneously, and the most common approach is to fit a few parameters at a time. the result from simplex optimisation is highly dependent on the starting values used, and this method is thus often not able to find the global optimum. the optimisation itself is computationally non-expensive, but the optimisation might become time consuming if the dynamic model is large, since the optimisation has to be run many times with different starting values to provide reliable results.

in order to compare our method to the more conventional simplex optimisation, we ran optimisations with the nelder-mead simplex  method  <cit>  using the “fminsearch” function in matlab® . optimisations were run using  <dig> different starting values , adding a penalty to the cost function value for moving outside the feasible parameter ranges given in table  <dig>  the cost-function we used was the rmsd between the simulated and reference model outputs . the rmsd was calculated using output variables that had been scaled by subtracting the mean and dividing by the standard deviation for all model outputs from simulations using the initial experimental design described under “fitting of the niederer-model parameters” in the methods section. none of the optimisations could identify any parameter sets within the feasible ranges producing model outputs replicating the reference data. even though we used a wide variety of starting values and penalty functions, all optimisations were driven outside the feasible region, and were unable to move back into the feasible region, in spite of the penalty added to the cost function. it therefore seems that with a very complex cost function with many local minima like the one used in this study, our statistical approach is more useful than the simplex optimisation for constraining the model parameters.

alternative optimisation methods include simulated annealing  <cit>  and levenberg-marquart optimisation  <cit> . these methods generally give more reliable results, and are more likely to find the global optimum. however, they are also significantly more computationally expensive, and are therefore not very suitable for parameter estimation of large, dynamic models. moreover, neither of these methods or the simplex optimisation provide an increased understanding of the underlying model mechanisms, they result in a parameter estimate only, and the results can often be non-physiological when no constraints on the parameter values are used.

as an alternative, artificial neural network-based methods  <cit>  are computationally non-expensive and can fit input–output relations including several outputs successfully. however, the neural network models often become extremely complex and difficult to interpret. they are also highly dependent on the quality of the data, and since they have the flexibility to adjust to small nuances in the data there is a risk of fitting to noise. physiological measurements often lack a sufficient signal-to-noise ratio, giving non-robust approximations of the parameter values when these methods are used for parameter estimation. kalman filtering  <cit>  and derivative-based methods give an estimate of parameter confidence, but can be computationally expensive, and derivative-based methods may in addition display convergence problems.

sarkar and sobie  <cit>  recently published a regression-based approach for constraining free parameters in dynamic models, based on inverting the regression coefficient matrix of a classical metamodel, and using this inverted regression coefficient matrix to predict the parameters from the model outputs. this resembles inverse metamodelling, but in inverse metamodelling the input parameters are predicted directly from the output metrics from simulations using regression, avoiding the need for an invertible  regression coefficient matrix. both the approach presented by sarkar and sobie and the inverse metamodelling approach require a non-ambigous  relationship between input parameters and model outputs. this, however, is often not the case for many biologically based models , creating a need for an alternative approach to constrain model parameters. this model sloppiness was also demonstrated in the application in this study, where low parameter identifiability resulted from the initial analysis .

in spite of model sloppiness, inverse metamodelling can effectively identify the direction in the parameter space to move in order to approach measured data in cases where the baseline is far from the target. this can limit the search space compared to what is needed with alternative methods such as simplex optimisation. without prior knowledge of suitable starting values for the optimisation, a simplex optimisation requires numerous simulations to give reasonable results. in contrast, the inverse metamodelling component of our method effectively guides the design of new simulations towards the most relevant parts of the parameter space, and the search space can thereby be reduced. this can also be achieved with methods like genetic algorithms or levenberg-marquardt optimisation. however, these methods provide no implicit, easily interpretable analysis of model mechanisms.

if the inverse metamodel is not calibrated using relevant simulation results, it has the potential to identify an incorrect search direction in the parameter space. however, the look-up process will automatically detect this error, since the closest simulations will then be further from the measurements than in the previous iteration. in such cases, the inverse metamodelling component can be omitted, and the look-up part of the algorithm used alone to guide the design of new simulations. the method often results in a set of possible solutions that can be restricted according to known physiological ranges of the parameters. accordingly, as new measurements of output metrics or parameters become available, they can further constrain the set of possible solutions. hence, prior knowledge can easily be taken into account in the procedure. moreover, other cost-functions can easily be incorporated in the pipeline, in addition to, or instead of, the rmsd calculated in the pca score space. hence, a weighting of the output metrics according to, for example, relevance for clinical use can be utilised.

due to the dependency of the results from each fitting iteration on the previous iteration, there may be other directions in the parameter space that could also give possible solutions. hence, the parameter space needs to be sampled densely in the initial experimental design to ensure that all possible solutions are found. however, in each iteration the experimental design is extended slightly beyond the ranges of the guideline set. hence, alternative directions in the parameter space that would allow model outputs replicating the measurements are likely to be found during the procedure. in cases where the target is very far from the output of the baseline parameter set, the method may need numerous simulations to make sure the parameter space is sampled sufficiently and that all possible clusters of feasible solutions are found, but due to the effective identification of a reasonable direction in the parameter space to move by the inverse metamodelling, the method is still likely to be more efficient in most cases than a “brute force” optimisation using, for example the simplex method, with a large number of different starting values. the method gives no clear answer as to when to stop, how many parameter values are enough or how we can know whether we have found all possible clusters/manifolds of solutions, but this is a problem with any parameter estimation method. likewise, if the data used to fit the parameters does not cover the complete space of system behaviour, the model parameters will not be constrained by the data, which also means that the model is too complex for the data it is being used to understand. this is true for all models and parameterisation methods.

model reduction
parameterising cardiac cell models in a whole-organ context is important for multi-scale modelling and ultimately for clinical use of the models, and requires the ability to control and foresee the whole-organ consequences of variations in cell-level model parameters. this makes it easier to determine how to pass on parameters between the scales, and eases the parameterisation of the cell-models in a whole-organ context. this again requires compact cell models with relatively few parameters and equations for which overview of the input–output relationships can be easily gained. by reducing models to a minimum number of parameters and equations, using detailed biophysical data we can reduce the number of free parameters that can then be efficiently fitted when these cellular models are embedded within whole organ models and fitted to compatible data. in many cases, and in particular clinical contexts, only whole organ data will be available. consequently, there is a need for efficient comparison of model alternatives in order to find the most reduced version that is able to replicate experimental measurements. for biochemical reaction networks, several methods have been developed for reducing the networks to the minimal complexity required  <cit> . we present here a generic framework for combined sensitivity analysis, parameter identifiability analysis, parameter fitting and model reduction, which can be applied to all types of deterministic models generating a set of outputs from a set of input parameters.

our results indicate that the presented approach is effective for model reduction and automatic updating of models according to new measurements, allowing identification of models that are more specific to e.g. certain species, temperatures or individuals. this is likely to be important in large modelling initiatives like the physiome project , since compact cell models can be more confidently and effectively applied as parts of large multi-scale whole organ models. we therefore believe that the presented methodology will be of great value for future model development, including the search for patient-specific or patient group-specific parameter values, something that is likely to highly increase the clinical applicability of models.

CONCLUSIONS
we have presented a new method for parameter estimation, which combines parameter fitting in relation to measured data and analysis of the mechanisms of the model system. the pipeline contains an implicit analysis of the model sensitivity and the parameter identifiability for model reduction. using our approach, different model alternatives can be compared, allowing effective analysis of the consequences of introducing changes to the models and identification of redundant model components that can be omitted without affecting the fit to measured data. we have applied the methodology to show that we can make two alternative model frameworks for cardiac contraction give the same outputs, and that we can generate reduced versions of both these models using this approach. we show that despite model sloppiness, inverse metamodelling can identify a reasonable direction in the parameter space to move in order to approach measured data. combined with a look-up of simulations in the proximity of the measured data and iterative generation of new experimental designs, this provides an accurate and effective approach for constraining model parameters.

the presented parameter fitting pipeline can effectively fit numerous parameters simultaneously, and through the iterative generation of new experimental designs for simulations, the method provides an overview of the spread of possible solutions, as well as possible clusters of suitable parameter values. this indicates the ability of a set of output metrics to constrain the parameters and gives an estimate of the uncertainty in the parameter estimates. in this study we showed that the niederer-model parameters could be constrained to a standard deviation of on average  <dig> % of the mean values over the succeeding parameter sets. this was decreased to  <dig> % for the equivalent reduced model. as new measurements become available, these can be incorporated to further constrain parameter values.

given measured data for a number of patients in a clinical context, this methodology can also be used to find sets of parameter values replicating the measured data for each patient, allowing identification of clusters in the parameter space corresponding to different patients or patient groups for personalised medicine. similarly, clusters of parameter values for different species, different measurement conditions etc. can be identified. the presented method thus has a clear potential in both multi-scale model development and clinical use of models.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
kt contributed to conception of the study and design of the computer experiments, wrote the matlab® code for the parameter fitting pipeline, performed the computer simulations, analysed the data and wrote the paper. san contributed to conception of the study and to writing of the paper. sl participated in designing the computer experiments and in debugging of the matlab® code. nps contributed to conception and coordination of the study and to writing of the paper. all authors read and approved the final manuscript.

supplementary material
additional file 1
description of the contraction models. a <dig> . length-dependence equations of the land-model. a <dig> . length-dependence equations of the niederer-model.

click here for file

 additional file 2
description of hierarchical cluster-based plsr.

click here for file

 additional file 3
additional figures. figure a <dig> . force-pca relationships for parameter sets  <dig> and  <dig> in table  <dig>  figure a <dig> . force-pca relationships for parameter set  <dig> and  <dig> in table  <dig>  figure a <dig> . force-pca relationships for parameter set  <dig> in table  <dig> 

click here for file

 acknowledgements
the research leading to these results has received funding from the seventh framework programme  under grant agreement n° 611823; fp <dig> marie curie actions intra-european fellowship for career development  n° 298494; the department of health via the national institute for health research  comprehensive biomedical research centre award to guy's & st thomas’ nhs foundation trust in partnership with king's college london and king’s college hospital nhs foundation trust; the united kingdom epsrc ; welcome trust  and biotechnology and biological sciences research council bbsrc .

alex lewalle is thanked for helpful discussions.
