BACKGROUND
protein complexes participate in many important cellular functions, so finding the set of existent complexes is essential for understanding the mechanism, organization, and regulation of processes in the cell. since protein complexes are groups of interacting proteins, many methods have been proposed to discover complexes from protein-protein interaction  data, which has been made available in large amounts by high-throughput experimental techniques. typically, complexes are predicted based on topological characteristics in the ppi network. for example, many approaches search for regions of high density or connectivity  <cit> . other approaches further incorporate subgraph diameters of known complexes  <cit> , and core-attachment models of connected clusters  <cit> . qi et al. used a set of topological features including density, degree, edge weight, and graph eigenvalues, with a supervised naive-bayes approach to learn these feature parameters from training complexes  <cit> .

the performance of these complex discovery algorithms is reliant on the quality of the protein interaction data, which is often associated with substantial numbers of spuriously-detected interactions  and missing interactions . furthermore, many protein pairs that actually do interact with each other are not located in the same complex, for example, protein pairs that bind temporarily to perform a function. we refer to such interactions as transient interactions. finally, not all proteins in the same complex may interact with each other, making its ppi subgraph far from complete. figure  <dig> provides an illustrative example of these challenges. the mitochondrial cytochrome bc <dig> complex is a well-known complex involved in the electron-transport chain in the mitochondrial inner membrane. in saccharomyces cerevisiae , this complex is composed of ten proteins. figure  <dig> shows the ppi subgraph around these ten proteins, using ppi data obtained from biogrid  <cit> , intact  <cit>  and mint  <cit> . nineteen ppis  were detected between these ten proteins; the rest remain undetected, likely due to the difficulty of detecting interactions between membrane proteins, or because not all proteins in this complex interact with each other.  <dig> extraneous interactions were detected between the proteins from this complex and  <dig> proteins outside the complex. while some of these extraneous interactions might be spuriously detected, others constitute transient interactions. five proteins likely involved in such transient interactions are shown: nab <dig> and ubi <dig> are involved in mrna polyadenylation and protein ubiquitination respectively, and bind to many proteins to perform their functions; pet <dig>  shy <dig>  and cox <dig> are mitochondrial membrane proteins that are also involved in the electron-transport chain, and interact with proteins of the complex, although they are not part of it. the density of the complex is lost amidst the noise of the extraneous interactions, making the discovery of this complex from ppi data extremely difficult: none of the six complex discovery algorithms we use in this paper successfully detected it.

many algorithms have been developed to assess the reliability of high-throughput protein interactions  <cit>  or predict new protein interactions  <cit> , using various information such as gene sequences, annotations, interacting domains, 3d structures, experimental repeatability, or topological characteristics of ppi networks. these approaches have been shown to be effective in reducing false positives or false negatives. in our previous work  <cit> , we have shown that using the topology of the ppi network to weight interactions, remove unreliable interactions, and posit new interactions improves the performance of several complex discovery algorithms. while such approaches are effective in reducing the impact of spuriously-detected and missing interactions, they do not directly address transient interactions and non-interacting complex proteins.

researchers have also proposed integrating heterogeneous data sources with supervised approaches to predict co-complex protein pairs , using a reference set of training complexes. data integration leverages on the fact that diverse data sources other than ppi can also reveal co-complex relationships, while a supervised approach targeted at predicting co-complex protein pairs can be trained to discriminate between actual co-complex interactions and spuriously-detected or transient interactions. qiu and noble  <cit>  integrated ppi, protein sequences, gene expression, interologs, and functional information, to train kernel-based models, and achieved high classification accuracy in predicting co-complex protein pairs. however, they did not apply or test their method on reconstructing and predicting complexes. wang et al.  <cit>  integrated ppi, gene expression, localization annotations, and transmembrane features, and applied a boosting method to predict co-complex protein pairs. they showed that this approach, combined with their proposed clustering method haco, achieved higher sensitivity in recovering reference complexes compared to unsupervised approaches. however, they did not explore how well their classification approach works when used in conjunction with other clustering methods: while sensitivity was improved, many reference complexes were still unable to be predicted in part due to limitations of haco, thus raising the question of whether other clustering methods may also see an improvement when used with their co-complex predictions. furthermore, these approaches directly produce co-complex affinity scores between protein pairs, without providing measurements of the predictive strengths of the different data sources, nor how the different score values of each data source indicate co-complex relationships. in our view, this is important when integrating different data sources: while using ppi for complex prediction is biologically reasonable because proteins in a complex interact and bind with each other, using other data sources such as sequences, expression, or literature co-occurrence is not as biologically intuitive, even if they do reveal co-complex relationships. providing a measurement of how these data sources contribute to co-complex predictions allows human judgment of the validity and credibility of predicted novel complexes.

we propose a method to address these challenges of complex discovery: first, the ppi network is integrated with other heterogeneous data sources that specify relationships between proteins, such as functional association and co-occurrence in literature, to form an expanded, composite network. next, each edge is weighted based on its posterior probability of belonging to a protein complex, using a naive-bayes maximum-likelihood model learned from a set of training complexes. a complex discovery algorithm can then be used on this weighted composite network to predict protein complexes. our method offers several advantages over current unsupervised or non-integrative weighting approaches. first, a composite protein network constructed from multiple data sources is more likely to have denser subgraphs for protein complexes, as it not only reduces the number of missing interactions, but also adds edges between non-interacting proteins from the same complex, because such proteins are likely to be related in ways other than by physical interactions. second, learning a model from training complexes not only provides a powerful method to assess the reliability of interactions, but also allows the discrimination between transient and co-complex interactions. third, utilizing multiple data sources to assess the reliability of interactions is likely to be more accurate than using just ppi data.

our choice of a naive-bayes maximum-likelihood model also offers several advantages over other supervised data-integration approaches. firstly our model is transparent, in that learned parameters can be validated and analyzed, for example to reveal the predictive strengths of the different data sources. furthermore, for a predicted complex, the learned parameters can then be used to visualize the component evidences from the different data sources, allowing human judgment of the credibility of the prediction. second, maximum-likelihood models are known to be robust and have low variance, even when few training samples are available. although we describe our experiments using yeast and human, this is important when we apply our approach to less-studied organisms with fewer known complexes available for training. finally, we utilize different clustering algorithms as well as a simple aggregative clustering strategy to evaluate the performance of our method, and show that we improve the performance of complex prediction compared to other weighting methods.

methods
building the composite network
heterogeneous data sources are combined to build the composite network. each data source provides a list of scored protein pairs: for each pair of proteins  with score s, u is related to v with score s, according to that data source. for both yeast and human, the following data sources are used:

• ppi data is obtained by taking the union of physical interactions from biogrid  <cit> , intact  <cit>  and mint  <cit>  . interactions are scored using a topological function, iterative adjustcd , which has been shown to improve the performance of complex discovery  <cit> . iterative adjustcd uses expectation maximization to score each interaction  based on the number of shared neighbors of u and v. interactions between proteins that have no shared neighbors are regarded as unreliable and are discarded. protein pairs that do not directly interact but have shared neighbors are also scored; such pairs with scores above  <dig>  are added as new interactions, and are called level  <dig> or l2-ppis. we consider ppis and l2-ppis as two separate data sources.

• predicted functional association data is obtained from the string database  <cit>  . string predicts each association between two proteins u and υ  using the following evidence types: gene co-occurrence across genomes; gene fusion events; gene proximity in the genome; homology; coexpression; physical interactions; co-occurrence in literature; and orthologs of the latter five evidence types transferred from other organisms . each evidence type is associated with quantitative information , which string maps to a confidence score of functional association based on co-occurrence in kegg pathways. the confidence scores of the different evidence types are then combined probabilistically to give a final functional association score for . only pairs with score greater than  <dig>  are kept.

• co-occurrence of proteins or genes in pubmed literature . each pair  is scored by the jaccard similarity of the sets of papers that u and υ appear in:

 s=au∩avau∪av 

where ax is the set of pubmed papers that contain protein x. for yeast, that would be the papers that contain the gene name or open reading frame  id of x as well as the word "cerevisiae"; for human that would be the papers that contain the gene name or uniprot id of x as well as the words "human" or "sapiens".

while there seems to be overlap between string's use of ppi and literature co-occurrence data with our use of them as separate data sources, note that string uses these data as only as component evidences for functional association and scores them accordingly. thus we treat the string data as a representation of functional association between proteins, regardless of how this association was derived. table  <dig> gives some summarizing statistics for these data sources.

in the composite network, vertices represent proteins and edges represent relationships between proteins. the composite network has an edge between proteins u and v if and only if there is a relationship between u and v according to any of the data sources.

edge-weighting by posterior probability
next, each edge  is weighted based on its posterior probability of being a co-complex edge , given the scores of the data source relationships between u and v.

we use a naive-bayes maximum-likelihood model to derive the posterior probability. each edge  between proteins u and v of the composite network is cast as a data instance. the set of features is the set of data sources, and for each instance , feature f has value f if proteins u and υ are related by data source f with score f. if u and v are not related by data source f, then feature f is given a score of  <dig>  using a reference set of protein complexes, each instance  in the training set is given a class label co-complex if both u and υ are in the same complex; otherwise its class label is non-co-complex learning proceeds by two steps:

 <dig>  minimum description length  supervised discretization  <cit>  is performed to discretize the features. mdl discretization recursively partitions the range of each feature to minimize the information entropy of the classes. if a feature cannot be discretized, that means it is not possible to find a partition that reduces the information entropy, so the feature is removed. thus this step also serves as simple feature selection.

 <dig>  the maximum-likelihood parameters are learned for the two classes co-complex and non-co-complex:

 p=nc,f=fncp=n¬c,f=fn¬c 

for each discretized value f of each feature f. nc is the number of edges with class label co-complex, nc,f=f is the number of edges with class label co-complex and whose feature f has value f, n¬c is the number of edges with class label non-co-complex, and n¬c,f=f is the number of edges with class label non-co-complex and whose feature f has value f

after learning the maximum-likelihood model, the weight for each edge e with feature values f <dig> = f <dig>  f <dig> = f <dig>  . . . is calculated as its posterior probability of being a co-complex edge:

 weight=p=ppz=∏ippz=∏ipp∏ipp+ ∏ipp 

where z is a normalizing factor to ensure the probabilities sum to  <dig>  although the second last equality makes the assumption that the features are independent, naive-bayes classifiers have been found to perform well even when this assumption is false  <cit> . specifically, while the probability estimates are frequently inaccurate, their rank orders usually remain correct, so that edges with likelier co-complex feature values are assigned higher scores than edges with likelier non-co-complex feature values.

complex discovery
after the composite network is weighted, the top k edges are used by a clustering algorithm to predict protein complexes. we use the following clustering algorithms in our study:

markov cluster algorithm   <cit>  simulates stochastic flow to enhance the contrast between regions of strong and weak flow in the graph. the process converges to a partition with a set of high-flow regions  separated by boundaries with no flow.

restricted neighborhood search clustering   <cit>  is a local search algorithm that explores the solution space to minimize a cost function, calculated according to the number of intra-cluster and inter-cluster edges. rnsc first composes an initial random clustering, and then iteratively moves nodes between clusters to reduce the clustering's cost. it also makes diversification moves to avoid local minima. rnsc performs several runs, and reports the clustering from the best run.

ipca  <cit>  expands clusters from seeded vertices, based on rules that encode prior knowledge of the topological structure of protein complexes' ppi subgraphs. whether a cluster is expanded to include a vertex is determined by the diameter of the resultant cluster and the connectivity between the vertex and the cluster.

clustering by maximal cliques   <cit>  first generates all the maximal cliques from a given network, and then removes or merges highly overlapping clusters based on their inter-connectivity as follows. if the overlap between two maximal cliques exceeds a threshold overlap_thres, then cmc checks whether the inter-connectivity between the two cliques exceeds a second threshold merge_thres. if it does, then the two cliques are merged; otherwise, the clique with lower density is removed.

hierarchical agglomerative clustering with overlap   <cit>  first considers all vertices as individual clusters, then iteratively merges pairs of clusters with high connectivity between them. at each merge, the two constituting clusters are remembered; when the merged cluster a is later merged with another cluster b, it also tries to merge the remembered constituting clusters of a with the cluster b, and keeps the  resultant clusters if they are highly connected.

clustering with overlapping neighborhood expansion   <cit>  greedily expands clusters from seeded vertices to maximize a cohesiveness function, which is based on the edge weights within a cluster and the edge weights connecting the cluster to the rest of the network. it then merges highly-overlapping clusters.

cmc, mcl, haco, and clusterone are able to utilize edge weights in their input networks, whereas rnsc and ipca do not; in this case, the selection of the top k edges provides less noisy networks as inputs to the algorithms.

cmc, mcl, and haco utilize parameters whose optimal values are at least partly dependent on the input networks' distribution of edge weights. for example, given an input network with high edge weights, using cmc with too low a merge_thres produces too many clusters consisting of merged cliques. thus, we run these algorithms with a range of values for their respective parameters, so as to obtain a more comprehensive picture of their performances across different weighting approaches. we run clusterone, rnsc, and ipca with mostly default or recommended parameters. the parameter settings used in our experiments for the six clustering algorithms are given in table  <dig> 

for any cluster c produced by any of these clustering algorithms, we define its score as its weighted density:

 scorec=densc=∑u∈c,v∈cwu,vcc- <dig> 

we also use a simple voting-based aggregative strategy combined, which takes the union of the clusters produced by the six algorithms above. if two or more clusters are found to be similar to each other, then only the cluster with the highest weighted density is kept, and its score is defined as its weighted density multiplied by the number of algorithms that produced the group of similar clusters; otherwise its score is its weighted density as usual. we define two clusters c and d to be similar if jaccard >=  <dig> , where jaccard is the jaccard similarity between the proteins contained in c and d:

 jaccardc,d=vc∩vdvc∪vd 

where vx is the set of proteins contained in x.

RESULTS
experimental setup
in our main experiment, we compare the performance of five weighting approaches:

 <dig>  swc: supervised weighting of composite network 

 <dig>  boost: supervised weighting of composite network using logitboost  <cit> 

 <dig>  topo: unsupervised topological weighting of ppi network with iterative adjustcd  <cit> , including level- <dig> ppis 

 <dig>  str: network of predicted and scored functional associations from string  <cit>  

 <dig>  nowei: unweighted ppi network

we perform random sub-sampling cross-validation, repeated over ten rounds, using manually curated complexes as reference complexes for training and testing. for yeast, we use the cyc <dig>  <cit>  set which consists of  <dig> complexes. only complexes of size greater than three proteins are used for testing; there are  <dig> such complexes in cyc <dig>  for human, we use the corum  <cit>  set which consists of  <dig> complexes, of which  <dig> are of size greater than three. in each cross-validation round, t% of the complexes of size greater than three are selected for testing, while all the remaining complexes are used for training. each edge  in the network is given a class label co-complex if u and v are in the same training complex, otherwise its class label is non-co-complex. for swc and boost, learning is performed using these labels, and the edges of the entire network are then weighted using the learned models. topo, string, and nowei require no learning, so the labels are not used; instead, for topo the edges of the network are weighted with topological scores, for string the edges are weighted with functional association scores, and for nowei all edges are given weight  <dig>  the top-weighted k edges from the network are then used by the clustering algorithms to predict complexes. for nowei we use k=all edges, while for swc, boost, topo, and string, we use k= <dig> . we do not use all edges for these four weighting methods, because weighting enriches the network in dense clusters, which causes some of the clustering algorithms to require too much time to run when all edges are used; moreover, our experiments indicate that the performance of these methods drop when more than  <dig> edges are used. the predicted clusters are evaluated on how well they match the test complexes.

we designed our experiment to simulate a real-use scenario of complex prediction in an organism where a few complexes might already be known, and novel complexes are to be predicted: in each round of cross-validation, the training complexes are those that are known and leveraged for learning to discover new complexes, while the test complexes are used to evaluate the performance of each approach at this task. thus we use a large percentage of test complexes t=90%. in yeast, this gives  <dig> test complexes , and  <dig> training complexes ; in human, this gives  <dig> test complexes , and  <dig> training complexes .

evaluation methods
to evaluate the predicted clusters. first, a cluster p is said to match a complex c at a given match threshold match_thres if jaccard >= match_thres. each cluster c is ranked by its score. to obtain a precision-recall graph, we calculate and plot the precision and recall of the predicted clusters at various cluster-score thresholds. given a set of predicted clusters p={p <dig> p <dig> …}, a set of test reference complexes c={c <dig> c <dig> …}, and a set of training reference complexes t={t <dig> t <dig> …}, the recall and precision at score threshold d are defined as follows:

 recalld=ci|ci∈c∧∃pj∈p,denspj≥d,pjmatchescicprecisiond=pj|pj∈p,denspj≥d∧∃ci∈c,cimatchespjpk|pk∈p,denspk≥d∧∄ti∈t,timatchespk∨∃ci∈c,cimatchespk 

the precision of clusters is calculated only among those clusters that do not match a training complex, to eliminate the bias of the supervised approaches  for predicting training complexes well. the precision-recall area under curve  is used as a summarizing statistic for each method's performance. besides evaluating the performance of complex prediction, we also evaluate the performance of edge classification, in which the edge weights are used to classify edges as co-complex or non-co-complex edges.

to evaluate the quality of novel predicted complexes, we define three measures of semantic coherence for each complex: its biological process , cellular compartment , and molecular function  semantic coherence. these are calculated from the proteins' annotations to gene ontology  terms, which span the three classes bp, cc, and mf  <cit> . we use the most informative common ancestor method of calculating the semantic similarity between two proteins, as outlined in  <cit> . briefly, the semantic similarity of two go terms is first defined as the information content of their most informative common ancestor. next, the bp semantic similarity of two proteins is defined as the highest semantic similarity between their two sets of annotated bp terms. then, we define the bp semantic coherence of a predicted complex as the average bp semantic similarity between every pair of proteins in that complex .

classification of co-complex edges
yeast
we first evaluate each approach in classification of co-complex edges. here, each weighting approach is used to weight the network edges, and the edges are classified as co-complex by taking a threshold on their weights. we obtain precision-recall graphs  by taking a series of decreasing thresholds; at each recall level, we also indicate the proportion of test complexes covered by at least one predicted edge .

on the other hand, swc is more accurate than string in predicting co-complex edges with high weights, because many proteins that are highly functionally associated are not co-complex, while swc's supervised learning approach produces weights that are targeted at predicting co-complex edges, so highly-weighted edges are likelier to be co-complex. however, to retrieve even more co-complex edges by lowering the weight threshold, string's precision rises above swc's, indicating that finding co-complex edges in this region might be better served simply by functional association.

boost integrates the same data sources as swc, but uses logitboost instead to learn to classify co-complex edges. its points in the graph are clustered in two regions: one set of edges are given high scores, achieving about 40% recall and 35% precision , while the remaining edges are given low scores. thus boost performs classification in a categorical manner, whereas swc produces co-complex scores that reflect a wide range of confidence.

finally, the performance of nowei, which uses unweighted ppi edges, appears as a single point on the graph, and shows that the ppi edges cover only 53% of co-complex edges, with a precision of 5%.

human
compared to topo, swc has lower precision along topo's entire recall range. however, once again topo's predicted edges are clustered in fewer complexes, giving lower complex coverage: for example, to cover 80% of complexes requires topo to recall 22% of edges at a precision of 8%; swc has to recall only 13% of edges at a higher precision of 11% to cover the same amount of complexes. thus, for human as well as yeast, swc is able to predict co-complex edges for a wider range of complexes compared to topo, whose range is limited to fewer complexes that are densely connected.

for human, string's functional association scores are the least accurate for predicting co-complex edges, giving the lowest precision among all the weighting approaches.

just like in yeast, boost performs classification in a categorical manner: a set of edges are predicted as co-complex with high scores, achieving 7% recall and similar precision levels as swc, while the remaining edges are predicted as non-co-complex with low scores.

prediction of complexes
yeast
we compare the performance of the five weighting approaches in complex prediction, when each of the six clustering algorithms is used separately, and when all the clustering algorithms are used together with the combined strategy. figure  <dig> shows the precision-recall auc for prediction of yeast complexes, and demonstrates that swc outperforms the other approaches in most cases: using the best clustering parameter settings for each approach, swc achieves the highest auc with all clustering algorithms except for ipca  and haco . string achieves higher auc compared to boost for all clustering algorithms except for cmc, while boost outperforms topo for all algorithms except for haco. finally, nowei performs dismally in all clustering algorithms except for rnsc. the combined strategy achieves higher auc compared to using each individual clustering algorithm, for all weighting approaches except for nowei . using the combined strategy, swc achieves the highest auc, followed by string, boost, topo, and finally nowei.

we analyze the clusters from the combined strategy to determine how it achieves greater complex-prediction performance by aggregating clusters from the different clustering algorithms with simple voting. figure 4a shows how clusters from the combined strategy are distributed among any single or multiple number of clustering algorithms that generated them, as well as their precision , in yeast. for brevity we present only the figures for the swc weighting approach. it reveals that the different algorithms produce different sets of clusters: around 85% of clusters are uniquely generated by a single algorithm, 7% of clusters are generated by two algorithms, and the remaining 8% of clusters are generated by three or more algorithms. thus, taking their union increases the recall substantially. furthermore, the precision of clusters increases with the number of algorithms that generated them: among clusters generated by a single algorithm, the highest precision is 20%; clusters generated by two algorithms have a precision of 28%; the precision increases to 78% among the clusters generated by all six algorithms. thus, voting helps to increase precision by giving greater scores to those clusters predicted by multiple clustering algorithms.

swc recalls the most test complexes, with the highest precision at almost all recall levels, especially with the stricter match_thres =  <dig> . thus it outperforms all other weighting approaches, especially at predicting complexes with ne granularity.

at match_thresh =  <dig> , str achieves almost the same recall as swc with only slightly lower precision levels, but its recall and precision are much worse at a higher match_thresh =  <dig> . since str classifies co-complex edges across a large range of clusters, it is able to recall many test complexes; but its lower accuracy in edge classification means that many of its clusters include extra or missing proteins, causing them not to be matched at a stricter matching threshold. boost achieves similar recall as str but with substantially lower precision levels at both match thresholds. since it classifies edges categorically, many edges have similar scores that do not vary with classification accuracy; thus the ranking of clusters  does not correlate as well with their correctness, giving lower precision levels. topo achieves the lowest recall of all approaches. while its precision for its highest-scoring clusters is comparable to swc's at match_thresh =  <dig>  , it drops rapidly for the remaining clusters. this is because topo classifies co-complex edges accurately for a limited number of complexes which are thus easy to predict, while the remaining complexes' edges are not as accurately classified, creating many false positive clusters and low recall. finally, although nowei achieves slightly higher recall than topo, it generates a great number of false positives, giving extremely low precision.

human
swc attains the highest recall at both match_thresh, with higher precision at all recall levels . the performance advantage is even more pronounced at match_thresh =  <dig> , where swc recalls 50% more test complexes compared to the other approaches, and maintains almost twice the precision throughout its recall range. boost attains the next highest recall, but with substantially lower precision at all recall levels. just as in yeast, its categorical edge classification reduces the correctness of the ranking of its clusters, giving lower precision levels.

topo achieves lower recall, but at match_thresh =  <dig>  its precision for its high-scoring clusters is higher than that of boost, and even comparable to swc's for its highest-scoring clusters. once again, topo's high accuracy in classifying edges for a limited number of complexes means it is only able to predict a few complexes well at rough granularity.

unlike in yeast, here str performs extremely poorly with the lowest recall and precision levels of all weighting approaches. this is not surprising given that str performs poorly in edge classification as well. indeed, even nowei achieves higher recall and precision at match_thresh =  <dig> , with a similar recall at the higher match threshold.

prediction of novel complexes
we evaluate the five weighting approaches  on the number and quality of high-confidence novel complexes predicted in yeast and human. for the supervised approaches , we use the entire reference set of complexes  for training. next, the edges of the entire network are weighted, and the top k edges are used to predict complexes with the combined clustering strategy, which combines clusters predicted by the six clustering algorithms. we use k =  <dig> for swc, boost, and topo, k =  <dig> for string, and k = all edges for nowei.

we filter the set of predicted complexes to obtain a set of unique, novel, high-confidence predictions. first, complexes that are too similar are removed: if any two predicted complexes match with match_thres =  <dig> , then the complex with the lower score is removed. next, only novel predictions are kept: if any predicted complex matches any reference complex with match_thres =  <dig> , then that predicted complex is removed. finally, only high-confidence predictions are kept: for each weighting approach, using the cross-validation results, the score of each predicted complex is benchmarked to a precision value, and predicted complexes whose estimated precision are less than a confidence threshold are removed. for yeast, this confidence threshold is  <dig> ; for human, since much fewer complexes are predicted with high precision, we use a  <dig>  confidence threshold.

yeast
human
analysis of learned parameters
figures 10a and 10b show the learned likelihood parameters for yeast and human respectively, when the entire reference sets of complexes  are used for training. the likelihood parameters are expressed as likelihood ratios, or how many times likelier is an edge co-complex rather than not co-complex, given the feature value:

 likelihoodratio=pp 

the likelihood ratio is a reflection of "co-complexness strength". in general, the likelihood ratios increase as the scores for the data sources  increase. for the ppi and l2-ppi data sources, protein pairs with higher scores have greater number of shared neighbors, and are likelier to be co-complex: when the score of ppi is close to  <dig>  indicating that almost all of the protein pair's neighbors are shared, the pair is  <dig> times likelier to be co-complex in yeast and  <dig> times likelier to be co-complex in human. l2-ppi scores are imputed in edges whose proteins do not actually interact according to ppi databases, yet who share many interaction partners. these scores have corresponding lower likelihood ratios compared to ppi scores: with a score close to  <dig>  the pair is less than  <dig> times likelier to be co-complex in yeast and less than  <dig> times likelier to be co-complex in human.

for the string data source, only protein pairs with very high functional association scores are likelier to be co-complex: those with the highest scores are almost  <dig> times likelier to be co-complex in yeast and  <dig> times likelier to be co-complex in human, whereas protein pairs with lower functional association scores do not seem any likelier to be co-complex.

for pubmed data, protein pairs that co-occur in literature, even infrequently, are already much likelier to be co-complex: about  <dig> times likelier in yeast and  <dig> times likelier in human. however, pairs that co-occur more frequently in literature are not any more likelier to be co-complex compared to pairs that co-occur less frequently.

the likelihood ratios for the different data sources show that the co-complexness strength of each data source does not increase linearly with its score. moreover, between the different data sources, the relationships between data score and co-complexness are different. thus, combining data scores across different data sources without factoring their dissimilar co-complexness relationships is evidently unsound, while our supervised approach scales the heterogeneous scores to a uniform co-complexness score in terms of likelihoods, which can then be combined probabilistically using the naive-bayes formulation.

the high likelihood ratios for the data sources also demonstrate that they are indeed indicative of edges belonging to complexes: during cross-validation for both yeast and human, none of the data sources were removed by feature selection in any round.

visualization of example complexes
yeast cytochrome bc <dig> complex
in this section we use two example complexes to illustrate the power and mechanism of swc. figure 11a shows the ppi subgraph of the yeast mitochondrial cytochrome bc <dig> complex discussed earlier, which is involved in the electron-transport chain in the mitochondrial inner membrane. the complex's ppi subgraph has  <dig> co-complex interactions, and  <dig> extraneous interactions with  <dig> external proteins, among which five are labeled: nab <dig> and ubi <dig> are involved in mrna polyadenylation and protein ubiquitination respectively, and bind to many proteins to perform their functions; pet <dig>  shy <dig>  and cox <dig> are mitochondrial membrane proteins that are also involved in the electron-transport chain, and interact with proteins of the complex, although they are not part of it. in the composite network , the edges from the other data sources induce a full clique among the complex proteins, although the number of extraneous edges and number of neighbors outside the complex increase to  <dig> and  <dig> respectively. after weighting by swc and selecting the top k =  <dig> edges , the complex's subgraph is still relatively dense; furthermore, only  <dig> extraneous edges and  <dig> neighboring proteins remain. note that among the five labeled external proteins, the two involved in unrelated processes  have been disconnected at this point, while the three also involved in the electron transport chain with the complex  are still connected to the network. with this network, both ipca and rnsc detect the cluster shaded in gray, which matches the complex with jaccard similarity of  <dig> .

the likelihood network for the cluster  visualizes the component evidences for the prediction: the contribution of each data source to an edge's swc score is reflected in the edge thickness, which is scaled with its likelihood ratio, or co-complexness strength. the likelihood network reveals that diverse data sources connect many proteins within the cluster with high swc scores. cyt <dig>  rip <dig>  and qcr <dig> are fully connected with each other via all three data sources, making them the strongest co-complex triplet that is centrally embedded in the cluster, while cyt1-cor1-qcr <dig> and cyt1-qcr7-qcr <dig> are connected with two or more data sources, making them highly co-complex and deeply embedded as well. the other proteins appear less central in the cluster, especially cob, a fringe member which is only connected via functional associations to four proteins.

human brca1-a complex
two novel predicted complexes
we select two novel complexes predicted with the combined strategy using the swc network, with the entire reference set of complexes for training.

one high-scoring novel yeast complex, generated by all six clustering algorithms, is composed of four proteins, mms <dig>  mms <dig>  rtt <dig>  and rtt <dig>  and is annotated with two high-level bp terms, dna metabolic process and response to stress. figure 13a shows its likelihood network. the four proteins are fully connected by six literature co-occurrences with strong co-complexness, and six functional associations with strong or moderate co-complexness. five ppi edges with moderate or weak co-complexness also connect the proteins. the diverse mix of data sources provides convincing evidence for this complex. a scan through the literature reveals that these four proteins form a complex named cul8-ring ubiquitin ligase complex  <cit> , thought to be involved in dna repair and regulation of chromatin metabolism, which the yeast reference complexes set cyc <dig> has apparently failed to include.

CONCLUSIONS
in this paper, we introduce a maximum-likelihood supervised approach for weighting composite protein networks for predicting protein complexes, called swc . first, we construct a composite protein network using three heterogeneous data sources: ppi, predicted functional association, and co-occurrence in literature abstracts. next, we weight each edge of the composite network based on its posterior probability of belonging to a protein complex, using a naive-bayes maximum-likelihood model learned from a set of training complexes. the weighted composite network is then used by clustering algorithms to predict new complexes. we also propose a simple aggregative clustering strategy that combines clusters generated by multiple clustering algorithms, using simple voting. we evaluate our weighting scheme using six clustering algorithms, as well our aggregative clustering strategy, on the prediction of yeast and human complexes. we demonstrate that our proposed method outperforms a supervised data-integration approach using boosting, a predicted functional-association network from string, an unsupervised approach using a topological function to weight ppi networks, as well as a baseline approach using unweighted ppi networks: our approach predicts more correct complexes at higher precision levels, and generates more high-confidence novel complexes with similar or better semantic coherence. using a few example complexes, we show that our approach increases the density of the complexes' subgraphs, and filters them to remove extraneous edges. furthermore, our approach allows visualization of the evidence of predicted complexes, using learned likelihood parameters to express strengths of co-complex relationships of each data type. this aids human evaluation of the credibility of predicted complexes.

finally, we present two novel predicted complexes: a four-protein yeast complex possibly involved in dna metabolism and stress response, and a four-protein human complex possibly involved in transport processes. we show that these predictions appear credible from their evidences, being supported by diverse data sources with strong co-complexness. indeed, a recent paper presents the predicted yeast complex as the cul8-ring ubiquitin ligase complex, while the uniprot database provides evidence that the predicted human complex may exist as a potassium channel complex.

swc software package and data files are available at http://compbio.ddns.comp.nus.edu.sg/~cherny/swc/.

competing interests
the authors declare that they have no competing interests.

authors' contributions
chy derived and implemented the algorithms, performed the experiments, and drafted the manuscript. gl and chy designed the experiments. hnc and chy selected and prepared the data sources. lw conceived and directed the study. all authors read and approved the final manuscript.

supplementary material
additional file 1
novel, unique, high-confidence predicted yeast complexes.

click here for file

 additional file 2
novel, unique, high-confidence predicted human complexes.

click here for file

 acknowledgements
this work was supported in part by singapore national research foundation grant nrf-g-crp-2007-04- <dig> and a national university of singapore ngs scholarship.

this article has been published as part of bmc systems biology volume  <dig> supplement  <dig>  2012: proceedings of the 23rd international conference on genome informatics . the full contents of the supplement are available online at http://www.biomedcentral.com/bmcsystbiol/supplements/6/s <dig> 
