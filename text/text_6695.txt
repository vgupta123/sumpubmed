BACKGROUND
in mass spectrometry  based proteomics, proteins in a sample are digested and the resulting peptides are separated by high-performance liquid chromatography  before injecting them into the mass spectrometer  <cit> . in this work, we focus on data from lc-ms experiments, as opposed to lc-ms/ms experiments where a fragmentation of selected sample compounds is performed to obtain ion ladders which can be used for the identification of the compound  <cit> . pure lc-ms experiments do not directly give information about the sequences of the peptides in a sample but we can still use the information on the lc-ms level to perform a quantification of the sample proteins  <cit> . in this application, algorithms detect peptide ion signals  in lc-ms spectra and estimate their abundances by integrating the signal area. different charge variants of the same peptide are summarized  and the peptides are mapped back to their parent protein to obtain abundance estimates at the protein level.

modern mass spectrometers can easily generate thousands of mass spectra in a short time. this wealth of information has sparked off the development of new, fully automated methods to analyze and process it. fig.  <dig> gives an example of a generic data analysis workflow in a study using lc-ms. stages of mass spectrometry-based proteomics in which algorithms can be applied are, among many others, low-level preprocessing such as the abovementioned feature detection and quantification  <cit> , alignment and registration of lc-ms data sets  <cit>  as well as the statistical evaluation  <cit>  of the experiments. there also exist some software tools that offer all  of these steps in one program  <cit> . for a recent review, we refer the interested reader to müller et al.  <cit> .

this plethora of tools is often confusing for the user who needs to decide which algorithms to apply for his data. but also developers of new algorithms need standardized benchmark data to compare their approach to existing ones. this is a difficult task, since only few quality metrics  <cit>  and only limited benchmarks exist so far. carefully compiled databases of annotated test data are standard in other fields such as dna sequence  <cit>  or rna structure analysis  <cit> . but they are not yet available for mass spectrometry based proteomics. only few researchers make their lc-ms data publicly available and all proteomic databases so far focus on data for the identification of peptides from ms/ms spectra  <cit>  and not on broader applications such as quantitative experiments.

an ideal lc-ms data set for the evaluation of feature detection, alignment and quantification algorithms would contain annotations with the positions of all peptide ion signals, their charge states, monoisotopic masses and abundances. only this information would allow meaningful comparisons between different methods and fair benchmark studies. of course, this information can be partially obtained by peptide identifications using ms/ms fragmentation. unfortunately, only a few of the peptide ions present in a sample are selected for fragmentation. furthermore, even of those fragmented, many cannot be identified due to noise, mutations or posttranslational modifications. for these reasons, annotations by ms/ms will always be incomplete. manual annotations by a human expert have been performed for single data sets  <cit>  but are clearly infeasible if our aim is to generate larger benchmarks. we believe that the simulation of lc-ms spectra is a valid approach, to be supplanted by the accumulation of annotated real-word spectral databases.

in the following sections, we introduce our software lc-mssim and describe its implementation details. we would like to emphasize that our aim was not to create a detailed physical model of mass spectra generation as, for instance, attempted in  <cit> . but we want to simulate data that is reasonably close to reality and provides a fair testing ground for data analysis methods. the idea of simulating esi mass spectra to assess the performance of ms feature detection algorithms was pioneered by wong et al.  <cit>  who presented a straightforward model for the simulation of esi mass spectra. they simulate spectra as mass lists derived from theoretical digests of protein sequences with normalized intensities without prediction of ion intensities, retention times or simulation of isotopic pattern. they also restrict their comparison to their own algorithm which implements a very specific task, the detection of protein-ligands and other macromolecular complexes in mass spectra. of course, the applications of lc-mssim are not restricted to feature detection benchmarks. the next obvious step would be to compare alignment algorithms, but even the comparison of a full quantification workflow is an interesting scenario.

to our knowledge, lc-mssim is the first software that models the whole lc-ms data acquisition process and delivers an output  that can directly be used for the assessment of proteomics algorithms. there are, of course, some programs that simulate individual parts of the lc-ms data acquisition process, such as the estimation of isotopic peak patterns  <cit> , the prediction of peptide retention times  <cit>  or detectability  <cit> . however, these tools are written in different programming languages and they have different output formats that cannot be easily combined. therefore, to simulate a full lc-ms run, it is clearly desirable to have all of these tools combined in a single application.

methods
lc-mssim is written in c++ as an add-on for openms  <cit> , our software library for computational mass spectrometry. lc-mssim uses openms data structures for file reading, writing and the calculation of isotopic patterns. it is also compatible with the openms proteomics pipeline   <cit>  and can readily be integrated into its workflows. this makes it very easy to generate large numbers of simulated data sets and to pipe them directly into a topp data analysis pipeline. lc-mssim is compatible with the current openms release version  <dig> .

furthermore, lc-mssim supports the topp ini  file format. this format is xml-based and can be edited using common xml editors or the inifileeditor supplied with topp. lc-mssim, openms and topp are all published under the lesser gnu public license. the source code can be downloaded from .

an artificial lc-ms data set is generated by the following steps: digestion of proteins, prediction of peptide detectability and retention time, relative abundances of charge states, modeling of isotopic and elution profiles and addition of shot noise to spectra. key parameters that influence the outcome of the simulation are the minimum accepted peptide detectability which influences the number of theoretical peptides appearing in the lc-ms spectra, mass accuracy and resolution, as well as the full-width-at-half-maximum  of the peptide peaks and the percentage of non-peptide contaminants added by the simulation software. in the following sections, we give an overview of all simulation steps and explain their parameters in more detail.

protein digestion
the user can supply a list of protein sequences in a fasta file and define their relative abundances in the sequence header. if no abundance is given, we assume that each protein, and thus its peptides, will appear in equal abundances in the mass spectra . lc-mssim supports only tryptic digests in this version, but new proteases can be added easily by extending the corresponding openms classes by new regular expressions. we can also simulate missed cleavages and self-digestion of the protease.

detectability and retention time prediction
after the enzymatic digest of all protein sequences, we need to determine the retention time of each peptide. pfeifer et al.  <cit>  recently introduced the paired oligo-border kernel  for machine learning problems in computational proteomics. support vector regression  <cit>  using this kernel function yields very accurate retention time predictions while requiring only a small number of training samples. we use the pobk for retention time prediction in our simulator. we trained the svm on the test set of petritis et al.  <cit>  and determined the best parameters using nested cross-validation. the data set consists of  <dig> peptide identifications of capillary reversed-phase liquid chromatography runs.

it was previously shown that not all peptides of a digested protein actually appear in the lc-ms sample  <cit> . there are numerous reasons for that. some peptides will only poorly ionize in the electrospray and others are simply not soluble, just to name a few. to account for this fact, we determine the likelihood of detectability of each peptide using a support vector machine  <cit>  and the pobk as a kernel function. we performed a nested cross-validation on balanced samples of the mudpit-esi data set of mallick et al.  <cit> . this means that we selected all n positive examples and chose n negative examples randomly out of all negative examples. the whole process was repeated ten times to minimize random effects. a roc curve of all predictions on the excluded test partitions can be seen in fig.  <dig>  mallick et al.  <cit>  evaluated their method in terms of  <dig> – positive predictive value  against coverage. for the mudpit-esi data set they got a coverage of about  <dig>  at  <dig> - ppv =  <dig>  and a coverage of about  <dig>  at  <dig> - ppv =  <dig> . in our evaluation  we get a coverage of about  <dig>  at  <dig> - ppv =  <dig>  and a coverage of about  <dig>  at  <dig> - ppv =  <dig> . this means that our method performs comparable to the methods of mallick et al.  <cit>  although requiring just a fraction of the negative data set for training , which drastically decreases the time needed for training the classifier. we used the probability estimates  <cit>  of the libsvm  <cit>  to compute the likelihood of a peptide to appear in the lc-ms spectra.

charge distribution model
after protein digestion, peptide detectability and retention time prediction, we need to determine the relative abundances of the ions created for each peptide. lc-mssim models an electrospray ionization  mass spectrometer. esi ionizes peptides and other sample compounds by applying a strong electric field to the sample. this field induces a charge accumulation at the liquid surface which will form highly charged droplets. as a result, we expect to see one to four ions per peptide, but charge states two or three are the most common. there are several chemical models describing the charge distribution for molecules after esi and numerous factors influence this distribution such as the ph, sample composition and conformation of the peptide  <cit> . however, our experiments have shown that a simple model gives a good approximation of real data.

for this reason, we decided to stick with a straightforward model of an esi mass spectrometer in positive ion mode. we follow an approach by schnier et al  <cit>  and assume that each basic amino acid in a peptide can receive at most one charge unit . consequently, most tryptic peptides have a maximum charge state of  <dig> –  <dig> which matches observations of real data. we determine the relative abundances of each charge state by sampling from a binomial distribution. as a result, low charge states are much more likely to occur than higher ones.

ion signal model
the position of a peptide ion signal in the lc-ms map is determined by three parameters: monoisotopic mass, charge and retention time. we calculate the mass from the amino acid sequence, charge is given by our binomial charge distribution model and the retention time predicted by the svm.

usually, a peptide ion gives rise to several peaks in a mass spectrum due to the fact that some of its atoms will occur in heavier isotopic states. given the sequence of the peptide, we calculate its monoisotopic mass from its empirical formula. the relative heights of the isotopic peaks are calculated using a simple but fast algorithm  <cit> . this algorithm gives us the relative intensities of the isotopic peaks. we model the peak shape using a gaussian distribution. the user can choose the peak width in terms of the full-width-at-half-maximum . the fwhm of a peak in a mass spectrum is given by the difference of the m/z values at which the ion count equals half of the maximum ion count of this peak. note that we assume the peak shape to be gaussian and the fwhm of a gaussian function is given by 22ln⁡2σ, where σ is the standard deviation of the gaussian.

whereas the shape of peaks in the m/z dimension is relatively stable during one experiment, the peak shape in retention time might vary considerably, but has often a gaussian-like shape. to account for this fact, we model the elution profile of a peptide signal using different chromatographic functions: a simple gaussian distribution and an exponentially modified gaussian distribution   <cit> . whereas the gaussian function represents a perfect chromatographic condition, the emg can model different distortions of the elution peak. its exponential component introduces tailing and fronting effects. several studies have shown that it provides a good fit for chromatographic peaks in reverse-phase chromatography  <cit> . it is defined as

 emg=ac2π2dexp⁡) 

where b is the standard deviation of the gaussian part, d is the expected shift of the exponential modifier, a the amplitude and c the center. for d <  <dig>  we obtain a fronted peak and for d >  <dig>  the peak is tailed. furthermore, we add uniformly distributed noise to single sampling points of the emg but smooth the noisy elution profile afterwards to obtain ragged chromatographic peaks. this allows us to model more realistic chromatographic peaks as an elution profile in a real lc-ms run is never entirely smooth. on the other hand, this introduces several additional parameters into the simulation. to make the software more user-friendly, we supply a set of pre-defined parameter sets for the emg, entitled poor,medium and good chromatographic conditions. a choice of good conditions leads to almost perfect gaussian-shaped peaks, like they will almost never appear in a real experiment. accordingly, medium and poor conditions lead to far more noisy elution peaks, including tailing and fronting effects. the corresponding peptide signals will be more difficult to trace across all their scans since most algorithms have problems to trace features with very rough elution peaks. fig.  <dig> shows three elution peaks from a reversed-phase column and as well as three simulated elution profiles, one for each parameter set. as we can see, the simulated peaks are close to real elution peaks and cover a sufficiently broad range of chromatographic conditions.

putting all this together, we can model lc-ms experiments with different mass resolutions and chromatographic conditions. to exemplify this, the right part of fig.  <dig> gives a bird's eye view of an lc-ms map created by lc-mssim. this map represents a tryptic digest of bsa  with some contaminations . the left part of fig.  <dig> shows a simulated bsa peptide ion from this map. this peptide occurs in the charge variant + <dig> and + <dig> 

noise and contaminations
no real lc-ms data set consists only of true signals i.e. signals caused by sample compounds. there is always some  noise in each spectrum. lc-mssim has several parameters that allow the user to introduce noise of various forms into a data set. users can simulate almost perfect lc-ms runs and runs with high amount of noise posing severe challenges to data analysis algorithms.

first, the user can define error bounds on the theoretically predicted retention times. by doing so, we simulate retention time shifts between different experiments and, for instance, can evaluate the performance of lc-ms alignment algorithms that are used to correct for these shifts as illustrated in fig.  <dig>  lc-mssim assumes these errors to be gaussian-distributed and the user can define medium and standard deviation in each case.

mass analyzers with different mass accuracies and resolutions are simulated by changing the fwhm of the peptide peaks as described above and by altering the sampling step size of the peptide models. furthermore, lc-mssim simulates inaccuracies in peak intensity measurements by adding gaussian-distributed noise to peptide peaks. finally, esi mass spectra frequently contain high-frequency noise signals of low to medium intensity, often referred to as shot noise. this term stems from electronics and physics  <cit>  and describes statistical fluctuations occurring if the number of particles measured by a detector is very small. its strength increases with the average intensity of the detected signal but is usually only detectable if the measured signal is very weak. the common assumption is that shot noise is poisson-distributed  <cit> .

to our knowledge, the notion of shot noise in mass spectrometry is much less well defined than in physics but usually loosely refers to high-frequency noise of low intensity in a mass spectrum. noise models for mass spectra have been the topic of several publications, but no consensus on the most suitable model exists so far  <cit> . however, recent publications suggests that noise in both q-tof and ion trap spectra can be modeled using a poisson distribution  <cit>  and therefore we decided to do the same. we split each spectrum in our simulated lc-ms map into segments of uniform size. we determine the number of shot noise signals by sampling from a poisson distribution, though m/z and intensity of these particles are given by a gaussian and exponential distribution, respectively. fig.  <dig> shows the peak intensity distribution of a real ms scan. the distribution is approximately exponential with some signals  having a high intensity. this shows that our model with exponentially distributed noise intensities well approximates real signals.

another typical phenomenon in mass spectra is a so-called baseline signal which usually decays with increasing m/z. this is usually a problem for maldi instruments but less in esi mass spectrometry. lc-mssim can simulate baseline signals by adding an exponentially-decaying baseline to each mass spectrum, but this feature is turned off by default.

shot noise and a baseline are both factors that hamper a computational analysis. but of equal concern for feature detection algorithms are non-peptidic contaminations in an lc-ms experiment or peptide signals arising from modified peptides. hoopmann et al.  <cit>  demonstrated that the detection of modified peptides is difficult and requires additional computational effort since the isotopic pattern of these peptides does not follow the typical averagine pattern assumed by most algorithms. in short, an averagine is an average amino acid with a composition estimated from a large number of protein sequences. using the averagine, we can estimate the average isotopic pattern for a peptide of a given mass. furthermore, contaminations such as salt molecules or metabolites are of lesser interest in proteomics studies and should not be reported by peptide feature detection algorithms. for these reasons, we decided to simulate these interferences as well. lc-mssim comes with a list of sample contaminants that can easily be extended by editing the corresponding text file. the current list of available contaminants comprises a snapshot of metabolites downloaded from the human metabolome database  <cit> . the user can set the percentage of added contaminants with respect to the number of peptides.

lc-mssim also includes a list of typical modifications such as oxidations or demethylations together with a list of affected amino acid residues. for each peptide containing a matching amino acid, lc-mssim determines at random whether the amino acid is modified or not. the user can set the corresponding probabilities and desired relative frequencies of modified peptides. fig.  <dig> shows an ms scan from a simulated bsa digest with added metabolic contaminants and shot noise. it is only a single scan from an lc-ms experiment, therefore not all bsa peptides are visible.

RESULTS
in this section, we present exemplary applications of lc-mssim. the advantage of our simulator is that we can generate lc-ms maps for which the exact mass, charge, retention time and bounding box of all compounds are known. the bounding box is the smallest axis-parallel rectangle that fully encloses the raw data points constituting the peptide feature. we can also deliberately introduce noise or change instrument parameters such as resolution or chromatographic behavior. this allows scientists to perform fine grained comparisons of lc-ms data analysis algorithms.

we decided to focus on peptide feature detection algorithms and compare the algorithms msinspect  <cit> , superhirn  <cit> , specarray  <cit> , mzmine  <cit>  and decon2ls  <cit> . decon2ls is an implementation of the thrash algorithm  <cit> . we also report on results for the algorithm implemented in openms  <cit> . this is for reference only, since we use some of the simulation models in our feature detection algorithm as well, which would make benchmarking of the openms algorithm biased.

the algorithms we compared differ heavily in the type and number of parameters they accept. some require only m/z and retention time range in which to search for features, others require lots of parameters such as confidence cutoffs, bin width or minimum signal-to-noise levels, to name just a few. parameters are also not always well documented. to achieve a comparison as fair and unbiased as possible, we chose for each algorithms settings that seemed suitable for each simulation run , but apart from that we decided to stick with the standard parameters and not to further optimize.

quality of simulation
performing simulations always raises the question whether the simulated data is sufficiently close to reality. in this section, we will demonstrate that our simulations are realistic.

we already showed that our model of elution peaks and shot noise match real data well . to illustrate the quality of our isotopic peak model, we simulated a mixture of standard proteins and generated a real lc-ms run of the same mixture on an esi-tof mass spectrometer . details of the sample preparation are given in  <cit> . we manually extracted peptide feature signals from the real data set and the simulated lc-ms run and computed spearman's rank correlation coefficient for three simulated isotopic peak patterns. the correlation coefficients were high, namely  <dig> ,  <dig>  and  <dig> . fig.  <dig> gives an example. we repeated this experiment with a low resolution lc-ms run. we recorded a mixture of human serum on an esi iontrap instrument and simulated an lc-ms map of similar resolution. details of the sample preparation are given in mayr et al.  <cit> . the correlation between real and simulated isotopic pattern was high . this shows that our isotope distribution model based on the algorithm by kubinyi  <cit>  and a gaussian peak shape generates realistic signals.

influence of mass resolution on feature detection
we downloaded the mouse ipi protein sequence set  and randomly selected  <dig> protein sequences from this set. a tryptic digest and filtering for detectability at a threshold of  <dig>  resulted in  <dig> peptides. the chosen threshold corresponds to a false discovery rate of 10%. we opted for this mixture of moderate complexity to avoid a high number of overlapping peptides. still, manual annotations of all these data sets would be tedious. in our first experiment, our goal was to determine to what extent the performance of current feature detection algorithms depends on the mass resolution of the instrument. we simulated different mass resolutions by changing the fwhm of the peptide isotopic pattern. we generated data sets for fwhm values of  <dig> ,  <dig> ,  <dig>  and  <dig> . a peak fwhm of  <dig>  roughly corresponds to an orbitrap instrument whereas the  <dig>  results in spectra similar to typical ion trap measurements. to each data set, we added shot noise with a mean intensity of  <dig> and a poisson rate of  <dig>  this noise level was chosen such that all peptide signals would be well above the noise level. the challenge of this benchmark was to detect poorly resolved and possibly overlapping peptide signals. the full result lists are contained in the supplemental material  as well as the settings for each feature detection algorithm . we also described our strategy to find suitable parameters for each algorithm . fig.  <dig> and fig.  <dig> show the simulated lc-ms run for fwhm  <dig>  and the spectrum at retention time  <dig> , respectively. all simulated lc-ms maps were uploaded to the pride database  and are available under the accession numbers  <dig> to  <dig> inclusive.

we noticed early on that each algorithm follows a different strategy. some algorithms report a lot of potential peptide features even for simple data sets, rather than missing an important signal. the rationale seems to be that it is better to obtains many false positives than to miss a potentially crucial signal. of course, spurious noise signals can be removed during later stages of the workflow. for instance, by removing signals that do not appear at consistent positions during alignment. nevertheless, this makes matters unnecessary difficult. in contrast, some algorithms are highly specific but tend to miss poorly resolved signals. which strategy is best might depend on the specific task to be performed and the complexity of the data.

furthermore, not every algorithm associates a quality or confidence measure with a feature that could be used as a cutoff. it is therefore not possible to give the classical receiver operating characteristic curves frequently used when comparing signal detection methods. consequently, we decided to give the results in terms of the false discovery rate  and true positive rate . we approximate the fdr by

 fdr=falsepositivestruepositives+falsepositives 

and compute the tpr as

 tpr=truepositivestruepositives+falsenagative. 

we count a peptide signal as detected correctly if the algorithm found a feature with the correct monoisotopic m/z  and an estimated bounding box within the true signal bounding box. it happens frequently that an algorithm splits a feature eluting over a longer period of time into several parts, i.e., loses track of the elution peak. in this case, we counted only one true positive hit for this feature but did not count the remaining features as incorrect hits. table  <dig> shows the results of this experiment. all algorithms recover most of the peptide signals at high mass resolutions but the true positive rate decreases for all algorithms with the resolution. specarray performs best on the high resolution data but with decreasing performance at lower resolutions. we would like to emphasize that specarray performs very well out-of-the-box, i.e., without parameter tuning, whereas other algorithms required a filtering of signals. our algorithm, implemented using openms, seems to be less affected by a poor mass resolution but fails to detect some signals even at high resolutions.

 false discovery and true positive rates on four data sets with changing mass resolutions.

we also note that some algorithms, especially msinspect and decon2ls, compute huge numbers of false positives and consequently, their false discovery rates are poor. on the other hand, both algorithms find almost all true signals, especially on the high resolution data set.

fig.  <dig> displays the running times of all algorithms on each data set. the time measurements were performed on a  <dig>  ghz intel xeon cpu with  <dig> gb memory running debian or windows server  <dig> r <dig> . note that the running times of decon2ls and mzmine are approximate results only, since both tools are gui-based and therefore do not allow direct time measurements. superhirn stands out as the fastest algorithm, whereas all other tools yield similar running times. decon2ls is a bit slower than the rest, but not significantly. to summarize, different algorithms have different strengths: some recover nearly all true signals even under poor conditions but at the expense of large numbers of false positive hits. one might argue that many of this false positive signals could be removed by removing features of low intensity or of unlikely masses. but this clearly has its disadvantages if we examine complex mixtures with large dynamic ranges and many compounds at low intensities.

influence of chromatographic conditions
in this experiment, we tested if the algorithms could deal with noisy elution profiles. we simulated three lc-ms runs, one for each predefined chromatographic condition  but kept the mass resolution in terms of the fwhm constant at  <dig> . if a peak elution profile gets noisy, we expected most algorithms to lose track of the isotopic pattern over time or maybe even not to detect it at all. table  <dig> shows the results of this experiment, again in terms of false discovery and true positive rate.

 false discovery and true positive rates on three data sets under changing chromatography conditions.

the performance of most algorithms remains stable across chromatographic conditions. there are only two algorithms whose performance lags behind if the elution peaks become noisier, openms and mzmine. the first simulated run with good column conditions contains many overlapping isotopic pattern, and openms is not able to separate strongly overlapping signals. furthermore, openms uses a gaussian model to fit the elution curve of a feature and discards features having a poor probability under this model. obviously, this dampens the performance of openms in this experiment. mzmine does not perform well on high resolution data, as shown in the previous section. this might be due to unfavorable parameter settings. the false discovery rate of specarray increases slightly at poorer chromatography conditions. all other algorithms are not affected. note that decon2ls is not affected by changes in the chromatographic condition since this tool detects isotopic patterns in a scanwise matter and does not take the elution profile into consideration.

metabolites
finally, we tested to what extent current peptide feature detection algorithms can discriminate between peptide signals and signals of other sample compounds. to this end, we generated an lc-ms map consisting of  <dig> metabolites, but no peptides. these metabolites represent a random subset of compounds from the human metabolome database . for each metabolite, we computed its isotopic distribution and placed it at a randomly-determined retention time in the lc-ms map. we modeled the elution profile using a gaussian function.

percentage of metabolites declared as features. we also report the number of features detected in total.

we computed the pearson correlation coefficient for both, metabolite and peptide isotopic profiles. the lowest correlation is  <dig> , which is still high. but this means that current algorithms that try to detect peptide signals using the averagine method will only poorly be able to distinguish peptides from other biomolecules.

this problem might not be grave. if we simply search for signals that discriminate between two conditions e.g. control and disease, it might at first not be that important whether this signal is caused by a peptide or a metabolite. but it is a fact that users have to keep in mind: most feature detection algorithms detect a lot of features in a real world data set, many more than are sequenced. this has usually been attributed to the fact that the data dependent acquisition process is a semi-random sampling of sample compounds and many peptides will never be identified. but users need to be aware that not all detected features will be caused by peptides, but also by other biomaterials including metabolites.

discussion
lc-mssim simulates mass spectrometry experiments with a wide range of instrument settings and column performances. there are some ways this software could be improved. to give an example, we trained our svm predictor for the detectability of peptides on data obtained from ms/ms identifications. that is, our model actually predicts whether a peptide is detected and identified using ms/ms. but we use it to predict whether a peptide occurs at all in an lc-ms data set or not. this is clearly a less stringent criterion since not all peptides visible in a mass spectrum will be identified by ms/ms.

it would also be interesting to test another important class of lc-ms data analysis algorithms, namely alignment methods. there is a similar diversity of approaches  <cit>  as for feature detection algorithms and it would be highly beneficial to the computational proteomics community to know about their individual strengths and weaknesses. the next step, as already mentioned in the introduction, would be to test full data analysis pipelines for accuracy of quantification, robustness in the presence of noise and contaminants, etc. obviously, finding good parameters for each and every pipeline will become even more difficult than it was already in this smaller study. it might be a good idea to compile a benchmark data set consisting of some real and manually annotated lc-ms runs, complemented by a large number of simulated runs. this would be an ideal testing ground for the proteomics community to compare and assess different analysis methods.

to summarize, our aim was not to develop a simulation capturing all physical aspects of an lc-ms experiment. this is hard since not all these aspects are entirely understood. but our aim was to develop a tool which yields benchmark data that are sufficiently close to reality. furthermore, we tried to keep the source code as modular as possible such that the community can adopt it or add new ideas and simulation models.

CONCLUSIONS
we presented lc-mssim, a simulation software for lc-esi-ms spectra. our software contains predictors for peptide retention time and detectability as well as models for charge distribution, peak shapes and isotopic intensity distributions. it has already proved to be valuable for in-house studies and we make it publicly available in the hope that it will be useful to the wider community.

lc-mssim is implemented as an add-on to the openms c++ software library and available for free under an open source license . both openms and lc-mssim can be downloaded from the sourceforge software repository. from a software engineering point of view, lc-mssim is an example how mass spectrometry-related software can easily be built using the openms library.

in this work, we demonstrated the versatility of lc-mssim for the benchmarking of peptide feature detection algorithms. this is a difficult task on real lc-ms data since there is no clearly defined ground truth in this case. we were able to probe the capabilities of currently available algorithms to a deeper extent than previously possible.

availability and requirements
lc-mssim runs under linux and windows . sourcecode is available from . installation instructions can be found at . the software depends on several data structures in the openms software library which can be downloaded at .

list of abbreviations used
fdr: false-discovery-rate; fwhm: full-width-at-half-maximum; lc-ms: liquid chromatography coupled to mass spectrometry; lgpl: lesser gnu public license. available at ; libsvm: an integrated software for support vector classification, available at ; mudpit: multidimensional protein identification technology, it combines 2d chromatography, i.e. two coupled columns, with a mass spectrometer; pobk: paired oligo-border kernel; svm: support vector machine; tpr: true positive rate.

authors' contributions
o.s.t. implemented the simulator, performed the experiments and wrote the manuscript. n.p. and o.k. devised the retention time and detectability prediction. c.g. developed the peak elution profiles and the isotopic peak model. k.r. conceived and initiated the project, provided feedback and directions on the results and the design of the simulator. all authors have read and approved this manuscript.

supplementary material
additional file 1
our strategy to choose the feature detection parameter. information about the versions of the tested software tools and their parameter sets.

click here for file

 additional file 2
feature detection parameter. the parameter settings of the different algorithms we compared.

click here for file

 additional file 3
results of the feature detection algorithms. result lists of the feature detection algorithms executed on our benchmark data.

click here for file

 acknowledgements
o.s.-t. acknowledges funding by the international max planck research school for computational biology and scientific computing  and by a grant of the german federal ministry for education and research , grant no. 031369c.

we thank parag mallick  for providing us with the data sets for peptide detectability prediction. we also thank alexander haupt who implemented a preliminary version of the simulator and marcel grunert who implemented the peak elution profiles. many other researchers and students in berlin, saarbrücken and tübingen contributed to the openms software library of which we made heavy use in this work. we also thank the anonymous reviewers who helped to improve this manuscript.
