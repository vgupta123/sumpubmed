BACKGROUND
in most phylogenomic studies supermatrices of concatenated presumably orthologous genes are used for tree inference  <cit> . due to the failure of consistently identifying orthologous genes among taxa  <cit>  and/or due to general sparse sequence data availability these supermatrices frequently display a low data coverage, down to 8%  <cit> . simulation studies showed that in these instances chances of recovering a correct and robust tree can drastically decrease  <cit> . additionally, wiens  <cit> , philippe  <cit> , sanderson  <cit> , driskell  <cit> , hartmann  <cit>  and colleagues showed that low gene data coverage of single taxa can already impede the success of tree reconstructions. in contrast, other simulation studies demonstrated that single taxa with low gene data coverage can help breaking up long branches and thus improve tree reconstructions  <cit> . these mentioned analyses of empirical and simulated data demonstrate that confounding effects of low gene data coverage on tree inference can hardly be generalized  <cit> .

despite these unresolved issues many investigators select sets of taxa with high gene data coverage assuming that the high gene data coverage will improve the robustness of tree inferences  <cit> . however, these threshold criteria are arbitrary and do not take into account potential phylogenetic signal of the data. those approaches might not lead to the desired increase of tree robustness. for example, tree robustness will not increase, if high gene data coverage is achieved by selecting highly conservative orthologous genes with low phylogenetic signal. alternatively, a robust tree might result if taxa with low gene data coverage but highly informative genes have been selected, driskell et al.  <cit>  e.g. report an example of plausible tree reconstructions based on a supermatrix with a gene data coverage of just 8–16%. both cases illustrate that gene data coverage and phylogenetic resolution are not necessarily correlated. consequently, the practice of selecting data based solely on data coverage is potentially problematic. therefore, we have developed an approach which focuses on the analyses of selected optimal data subsets  which have high data coverage and phylogenetic signal. crucial for this approach is the assessment of potential signal of genes and the development of a heuristics to select such an sos.

different quartet mapping approaches have been used to assess potential signal within genes  <cit> . among these, geometry mapping is demonstrably the most conservative estimator  <cit>  and the application to genes of supermatrices is straightforward. consequently, we have chosen the geometry mapping approach  <cit>  to assess potential signal of genes in the development of our heuristics.

in order to select an optimal set of taxa and genes, sanderson and colleagues  <cit>  suggested selecting sets of full data coverage . however, the identification of the maximal  biclique is a np-complete problem  <cit>  and, thus, there is no guarantee to find the maximal  biclique. additionally, sanderson et al.  <cit>  found that selections of maximal bicliques resulted in very small subsets of size <  <dig> taxa and <  <dig> genes. sanderson’s approach is, thus, not suitable to reconstruct phylogenetic relationships of many taxa. a possible solution might be the selection of quasi-bicliques  <cit> , which potentially combine a much larger set of taxa and genes accepting a predefined level of missing data. this promising direction however has the drawback that it is not time-efficient.

alternatively hartmann et al.  <cit>  and cheng et al.  <cit>  introduced two approaches directly applicable to sequence data. the first approach of hartmann et al.  <cit>  is a masking technique  which masks multiple sequence alignments according to predefined thresholds of gap frequencies of sites. the approach of cheng et al.  <cit>  is a statistical correction for missing data . a comparison of these two approaches demonstrated that reap performed better, a result which is compatible with the results of sanderson’s biclique approach. however, both, alignment masking  and the biclique approach optimize data only with respect to data coverage and without considering potential signal among genes.

here, we introduce a simple hill climbing algorithm to select optimal data subsets  which are assembled by considering data coverage and potential signal of genes. we start with the assumption that any taxon and gene can potentially contribute to the total signal of the matrix. however, taxa or genes with incomplete data coverage and low signal can potentially also contribute noise or cause biases to the total signal of the supermatrix. therefore, we successively mask taxa and genes of low signal and/or data coverage generating a submatrix of higher data coverage and signal. with this approach we deliberately discard taxa and genes because of their low data coverage and/or potential low signal. the proposed hill climbing algorithm delivers an optimal solution of this trade-off. using simulated and empirical data, we compare the performance of the herein proposed approach with an often applied approach of simply selecting data subsets using predefined thresholds of data coverage only.

methods
the approach can be separated into two parts:  the determination of information content of genes, taxa and the concatenated supermatrix and  the selection of an optimal subset  of taxa and genes.

information content of genes, taxa and matrices
before we define the information content of genes, taxa and matrices used in our approach, we have to introduce the concepts of data coverage representation matrices.

a concatenated supermatrix of n taxa and n gene nucleotide/amino acid sequence alignments can be represented as a matrix b with entries b
ij 


  b:bij=,∀ 

with b
ij 
=  for a present and b
ij 
=  for an absent gene nucleotide/amino acid sequence j for a taxon i. we call this matrix b the data coverage representation matrix.

we define the information content of a gene j, q
j
, as the relative data coverage of this gene, defined as 

  qj=∑i=1nbijn,∀taxa:i:1…n. 

likewise, the information content of a taxon i, p
i 
is defined as 

  pi=∑j=1nbijn,∀genes:j:1…n. 

we define the information content, p, of a matrix b as 

  p=∑i=1n∑j=1npin×n=∑i=1n∑j=1nqjn×n 

with 0 ≤ p ,p
i
,q
j 
≤  <dig>  to determine the potential signal of genes we use geometry mapping  <cit>  extended to the amino acid level. nieselt-struwe et al.  <cit>  showed that for a given quartet of sequences, relative support for each of the three possible topologies s <dig> s <dig> s3 can be computed as 

  si=δi/ 

with δ
i 
support for tree t
i
, 0 ≤ s
i 
≤  <dig> and ∑isi= <dig>  support values δ
i 
can be computed with any optimality criterion. relative support values can be interpreted as baricentric coordinates of a bipartite simplex graph s  with vectors s = : 

  s=∑i=13siei|s1+s2+s3= <dig> ≤s <dig> s <dig> s3≤ <dig> 

with e
i 
as unit vectors. within s, areas t <dig> t <dig> t3 at vertices can be defined for resolved quartets, t <dig> ,t <dig> ,t <dig>  for partly resolved quartets, and t∗ for star-like, unresolved topologies of quartets . for all possible quartets k
j 
of a gene j, kj=n <dig> with n the number of taxa, all vectors s
m 
= , can be calculated, and the frequency of vectors in areas t <dig> t <dig>  and t3 determine potential signal, t
j 
of a gene j <cit> . 

  tj=t1+t2+t3t1+t2+t3+t <dig> +t <dig> +t <dig> +t∗ 

we relaxed the definition of signal by calculating the frequency of vectors in areas t <dig> t <dig> t <dig> t <dig> ,t <dig> ,t <dig> . 

  tj^=t1+t2+t3+t <dig> +t <dig> +t <dig> t1+t2+t3+t <dig> +t <dig> +t <dig> +t∗ 

our approach will, thus, be a more optimistic estimator of potential signal. signal tj^ will be 0≤tj^≤ <dig> .

geometry mapping is a conservative estimator of tj^, however, within a narrow range of short internal and long terminal branch lengths, geometry mapping opts for the wrong tree, a classical case of long branch attraction  <cit> . this phenomenon might inflate the estimation of tj^ under certain circumstances.

nieselt-struwe and colleagues  <cit>  showed that for any alphabet of characters of finite length, e.g. nucleotides or amino acids, an enumeration of character states among four sequences can be used to calculate support for all three possible topologies. they further showed that a weight matrix m, defining dissimilarity measures between characters, can equivalently be used to calculate distances between sequences. therefore, we used blosum <dig>  the amino acid substitution matrix introduced by henikoff  <cit> , to calculate distances between sequences in correspondence to equation  in nieselt-struwe et al.  <cit> .

we use tj^ of each gene j to update entries of matrix b. for each gene j, entries of matrix b =  are scaled with the corresponding tj^ values. we call this matrix a weighted data coverage representation matrix b∗, in short, a weighted matrix b∗, in the following: 

  b∗:bij∗=,∀ 

substituting bij∗ for b
ij 
results in weighted forms of equations  <dig> and  <dig>  the information content of a gene j, q
j
, represents in its weighted form a product of relative data coverage and potential signal of genes.

selection of an optimal subset  of taxa and genes
we consider a subset of taxa and genes optimal, if it has a high information content, p  and contains as many taxa and genes as possible. if we discard genes or taxa with low q
j 
or p
i 
respectively, we will increase p of the matrix, but will loose information on the excluded taxa and genes. a simple optimization can be performed, searching for the highest possible p while excluding as few taxa/genes as possible.

first, a data coverage representation matrix b is generated from the concatenated supermatrix of multiple gene nucleotide/amino acid sequences corresponding to equation . secondly, for each gene j, ≤ <dig>  quartets are randomly drawn without duplication and tj^ is calculated. for each gene j, entries of b =  are scaled with the corresponding tj^ values, generating a weighted matrix b∗ corresponding to equation . thirdly, we use a simple hill climbing procedure to select an optimal subset  of taxa and genes. elimination of taxa or genes starts with dropping either a taxon or gene with the lowest information content p
i 
or q
j
, generating a new matrix b′ with p′ . in case of ties between q
j 
and p
i
, genes will be excluded. since taxa or genes with lowest information content will be dropped, p′  > p  . after each elimination step, information content of taxa  and genes  are recalculated. every gene represented by less than  <dig> taxa is automatically dropped from the matrix. gene overlap between taxa is monitored to a minimum of three taxa and two genes. if the matrix b′ does not fulfill this criterion, the next best b′ in terms of p′ is selected.

continuous elimination of taxa or genes with low p
i
 or q
j 
will generate a ‘trivial’ sos containing few taxa and one gene. therefore, we define an optimality function f  

  f=1-|)|ifp< <dig> 

with α as a scaling factor  and λ as the size ratio between reduced b′ and original matrix b

  λ=nb′×nb′nb×nb. 

during the process of elimination of taxa and/or genes, p′ will continually increase, and λ will continually decrease. f will reach a maximum of  <dig>  with a scaling factor α =  <dig>  the maximum will be at the intersection of p′ and λ, with α =  <dig> it will be reached later, favoring an sos with a higher p . if f  =  <dig> the process of elimination stops.

the outlined procedure is a simple hill climbing heuristics without guarantee of finding a globally optimal solution due to the interaction of p
i 
and q
j
. the approach can be applied either to b or b∗. it should be pointed out that removal of taxa will have an influence on the calculation of tj^ which is not recalculated during the process of matrix reduction. this simplification greatly speeds up the heuristics. an iterative recalculation of tj^ can potentially improve the selection of an informative dataset and will be further studied.

calculation time for this heuristics grows with the number of taxa  and genes . therefore, it is time efficient, o  <dig>  the algorithm reduces matrices in a deterministic way which makes matrix reduction reproducible. however, different equally optimal solutions will not be found under identical parameter settings.

by varying the scaling parameter α, however, an sos of high p , versus an sos of more taxa and genes with lower p  can be found.

simulated data
our simulations were not set up with the intention of fully exploring the performance of matrix reductions depending on super matrix characteristics, but were set up in order to illustrate the potential of the method in four different cases, resembling observed situations of empirical data.


simulated data with random distribution of missing data

for two different sets of genes, differing in relative evolutionary rates among genes , we simulated  <dig>  supermatrices each, composed of genes with  <dig> amino acids , concatenated for each taxon to  <dig>  aa length using seq-gen  <cit>  and the blosum <dig> matrix. for these simulations, we used a topology derived from empirical data with realistic distribution of branch lengths . evolutionary rates of genes varied from  <dig>  to  <dig>  relative rate differences, to mimic different signal strength . within each gene, site rates were homogeneous. in order to generate supermartices with missing data, we removed amino acid sequences of taxa using a binomial distribution with a probability of retaining data entries for each taxon and gene of  <dig>  . this set up generated supermatrices with randomly distributed missing data, closely resembling the observed data coverage of published concatenated supermatrices of dunn and colleagues  <cit> .

∗
d


qs

d


qs

f
all values are medians of  <dig> simulations.

∗ total information content  of un-weighted matrices is allways higher due to the fact that all genes are coded as present/absent .

‡ f refers to the frequency of correct trees per  <dig> simulations.


simulated data with power-law and non-random distribution of missing data

for two different sets of genes, differing in relative evolutionary rates among genes , we further simulated  <dig>  supermatrices each, composed of genes with  <dig> aa, concatenated for each taxon to  <dig>  aa length. we used again the topology derived from empirical data with realistic distribution of branch lengths . we changed seven branch lengths to introduce potential long branch attraction . in order to generate supermartices with missing data, we followed a proposal of li and colleagues  <cit> . these authors showed that the distribution of missing data in many empirical supermatrices is best described by applying a power law function of the probability of having data. following their observation, we assigned to each taxon and gene a probability of having data randomly drawn from f  =  -  <dig> , for x randomly selected with equal probability from, 0 ≤ x ≤ ∞. additionally, we constrained data assignment to having at least one gene for each taxon. following this approach, we concatenated supermatrices with a distribution of missing data approximately similar to observed empirically supermatrices  . finally, we raised the probability of data coverage for four predefined taxa, mimicking the often seen high coverage of a few taxa for which genomes are available.

selecting subsets from simulated data and tree reconstructions

selecting subsets with the hill climbing algorithm

sos’s were selected using the mare software  which implements the herein described novel approach. for each supermatrix, trees were reconstructed 1) using the original supermatrix , 2) an sos of b and 3) an sos of b∗. trees were reconstructed with raxml  <dig> . <dig>  <cit> . the blosum <dig> amino acid substitution matrix with Γ distributed among site rate heterogeneity was used to account for different substitution rates among genes.

to compare reconstructed trees with the correct trees used in data simulations, we used standardized quartet distances between shared taxa  <cit> . qdistances  were standardized in relation to all quartets of shared taxa. we recorded d
qd
’s of trees inferred from the unreduced matrix and of the two sos’s derived from b and b∗.


selecting subsets with predefined thresholds of data coverage

from supermatrices with power-law and non-random distribution of missing data we selected subsets in two different ways:  we selected all genes with data coverage above or equal to  <dig>  and  we selected all taxa with data coverage above or equal to  <dig>  and all genes with data coverage above or equal to  <dig>  . we recorded d
qd
’s of trees inferred from unreduced matrices and from subsets.

selecting subsets from empirical data and tree reconstructions
we studied the performance of using the hill climbing algorithm with matrices b and b∗ using the published empirical metazoan data set of driskell et al.  <cit>  comprising  <dig>  putative orthologous genes for  <dig> taxa . additionally, we selected data subsets of the driskell supermatrix applying predefined thresholds of gene - and taxa coverage . all ml analyses using raxml v <dig> . <dig> or  <dig> . <dig> were executed with rapid bootstrapping  and best tree search  in one step  and the empirical substitution matrix wag  <cit> . a posteriori bootstop tests were performed to test for a sufficient number of bootstrap replicates  <cit> . all analyses were conducted using raxml hybrid and pthreads versions on hpc linux clusters,  <dig> nodes with  <dig> or  <dig> cores each, at the regionales rechenzentrum köln  using cologne high efficient operating platform for science . further, we compared the effects of data reduction on tree robustness with the resolution score as introduced by holland and colleagues  <cit> . this resolution score, rs, calculated as the sum of bootstrap support values ≥ <dig> divided by the number of taxa n -  <dig>  represents a measure of average bootstrap support and, thus, robustness of trees.
mare 
and simple predefined thresholds

RESULTS
performance with simulated data
tree reconstructions based on unreduced supermatrices with a gaussian distribution of missing data did not yield correct trees except for one case in set  <dig>  for set <dig> and set <dig>  gaussian distribution of missing data in figure 7a,b, table 1). the variability of d
qd 
values was low  for set <dig> and set <dig>  gaussian distribution of missing data in figure 7a, table 1). tree reconstructions based on all soss  of these supermatrices performed much better ,  for set <dig> and set <dig>  gaussian distribution of missing data in figure 7a,b, table 1). compared with trees derived from unreduced supermatrices, soss supported more often correct trees, but had a higher frequencies of wrong quartets ,  for set <dig> and set <dig>  gaussian distribution of missing data in figure 7a,b, table 1). however, there was no clear difference of mean d
qd 
values between trees based on soss derived from b  or b∗  ,  for set <dig> and set <dig>  gaussian distribution of missing data in figure 7a, table 1). trees based on soss of b∗  had a much lower amplitude of d
qd 
values ,  for set <dig> and set <dig>  gaussian distribution of missing data in figure 7a, table 1). soss derived from b∗ contained on average more taxa .

tree reconstructions based on the unreduced matrix with power-law non-random distribution of missing data did not recover correct trees for set  <dig> and set  <dig>  in both cases variability of d
qd 
values was high ,  for set <dig> and set <dig>  power-law non-random distribution of missing data in figure 7a,b, table 1). tree reconstructions based on all soss  clearly outperformed reconstructions based on the unreduced matrices , ,  for set <dig> and set <dig>  power-law non-random distribution of missing data in figure 7a,b, table 1). the absolute number of correct trees was again higher for all soss  compared with the number of correct trees inferred from the unreduced matrices. in cases of low relative rate differences among genes, set  <dig>  soss derived from b  performed worse compared to soss derived from b∗ , in cases of high relative rate differences among genes, set  <dig>  the opposite was observed , ,  for set <dig> and set <dig>  power-law non-random distribution of missing data in figure 7b, table 1).

data subsets derived from matrices with power-law non-random distribution of missing data using predefined thresholds of gene coverage supported trees with lower mean d
qd 
values ,  in figure 7a) in comparison with mean d
qd
 values of trees inferred from soss selected with our approach ,  for set  <dig> and set  <dig> of the power-law data in figure 7a, table 1). the mean d
qd 
values were higher and the amplitude of d
qd 
was large ,  in figure 7a). data subsets from matrices with power-law non-random distribution of missing data using combined thresholds of data coverage for genes and taxa did support trees with mean d
qd 
values ,  in figure 7a) comparable with mean d
qd 
values of trees inferred from soss of set  <dig> and set  <dig> selected with our approach ,  for set  <dig> and set  <dig> of the power-law data in figure 7a, table 1). the amplitude of d
qd 
values however was large ,  in figure 7a). applying only thresholds for gene data coverage yielded a lower absolute number of correct trees ,  in figure 7b) compared with our approach, but the absolute number of correct trees was comparable or even higher if combined thresholds of taxa and genes were used ,  in figure  <dig>  table 1).

in summary, reduction of supermatrices often increased the chance to find a correct tree, but not consistently. soss derived from b∗ did not always support correct trees more often compared with soss derived from b, but had a much smaller amplitude of d
q 
values. data subsets derived from predefined thresholds supported fewer correct trees if only applied to genes but supported comparable numbers of correct trees if used with combined thresholds of data coverage for taxa and genes.

performance with empirical data
we applied our approach to the published metazoan data set of driskell et al.  <cit>  comprising  <dig>  genes for  <dig> taxa . the data coverage was low , the matrix information content was low . most genes are represented only by few taxa . we excluded six taxa of which the complete genome was available from the original matrix showing the highest coverage  and selected an sos from these data. with this procedure we removed the most extreme heterogeneity of data coverage among taxa prior to the selection of an sos.

selecting an sos resulted in a data subset of  <dig> taxa and  <dig> genes with a data coverage of  <dig>  and p =  <dig> . thus, a sos was found with a  <dig> % loss of taxa and a  <dig> -fold increase in data coverage and a  <dig> -fold gain in p. however, all outgroup taxa including slime molds, fungi and nematodes had been excluded. we compared tree reconstructions based on 1) the original unreduced supermatrix with  <dig> taxa  and 2) the sos of  <dig> taxa and  <dig> genes . an a posteriori bootstop test  revealed that  <dig>  bs were by far sufficient for both analyzed data sets.

tree reconstructions with the 64-taxa set resulted in trees with polyphyletic tetrapoda, actinopterygii, monophyletic marsupialia + monotrema, and largely unresolved basal splits within theria .

the tree based on the sos was more congruent to general taxonomic views. the topology showed moderately supported monophyletic tetrapoda, and resolution within ungulates and carnivora . however, for example actinopterygii remained paraphyletic and relationships of marsupialia and monotrema were not resolved. the resolution score rs increased from  <dig> %  to  <dig> % . we also compared reductions of the original driskell supermatrix using different parameter settings in our approach and simple thresholds of data masking . applying predefined thresholds of gene and taxa coverage never resulted in matrices with comparable resolution scores and comparable number of taxa. our approach outperformed the application of simple thresholds.

discussion
we show that supermatrices of simulated amino acid sequence data with low data coverage and relative rate differences among genes can support biased tree inference or low robustness of trees. it can be suspected that these effects will even be stronger for empirical data. these conclusions corroborate results of hartmann  <cit> , in many aspects philippe  <cit>  and wiens and colleagues  <cit> . effective techniques to reduce these potential biases in tree inference are therefore clearly needed.

masking supermatrices and deleting rogue taxa after tree reconstructions could be suitable measures as has been applied by dunn and colleagues  <cit> . in their analysis these authors selected taxa and genes according to predefined cutoff values of data coverage. the application of cutoff values considers only the extent of missing data which might favor the selection of the most conserved genes readily identified among all taxa in the data. additionally, dunn et al.  <cit>  deleted rogue taxa after tree reconstruction based on an idea introduced by thorley and colleagues  <cit> . the major drawback of their approach is that robustly misplaced taxa will not be identified. in this respect, a formal approach to masking of supermatrices as proposed here could be an alternative worth to consider.

we propose to select a subset of taxa and genes with a maximal information content. in doing so, it is necessary to first assess potential signal of genes, for which we use extended geometry mapping   <cit> . we opted for geometry mapping, because it tends to be more conservative in discriminating between resolved and star-like trees in contrast to likelihood mapping  <cit> . additionally, egm is easily applied to nucleotide and amino acid sequence data without the need of tree reconstructions. it is, thus, a technically convenient but, admittedly, coarse way of estimating potential signal.

secondly, it is necessary to select optimal subsets of supermatrices based on the information content of taxa and genes. the information content of taxa and genes is calculated as the ratio of potential signal and data coverage. by introducing this optimality criterion we can select taxa and genes which contribute most signal in tree reconstructions. we select a data subset in a stepwise function penalizing size reduction of the supermatrix and favoring higher matrix information content, monitoring but ignoring optimization of connectivity in the matrix. our approach is time efficient but will not be effective in discovering a globally optimal subset in terms of taxa/gene overlap  and information content. this is in contrast to the approach of yan  <cit>  in which the quasi-biclique with the highest level of connectivity  is searched for.

improved heuristics considering information content and  connectivity in our approach are certainly conceivable. however, the distribution of missing data following a power-law distribution in empirical data suggests that simple hill climbing procedures will be effective in identifying a good  subset of taxa and genes in terms of matrix information content. the flexibility of our approach offers even the chance to use different parameter settings of the optimality function to identify alternative soss.

we observed high amplitudes of d
qd 
values of trees based on soss in our simulations. these amplitudes were even higher in sos’s based on simple data coverage representations. we interpret this occasional high error rate as a possible phenomenon of insufficient taxon sampling in soss which might pronounce long branch attraction , or, alternatively, that connectivity in soss was not sufficient to potentially support just one tree  <cit> . this interpretation highlights a problem of all methods of data reduction. every reduction process, at least partially, counteracts efforts to reduce biases in tree reconstructions due to insufficient taxon or gene sampling. the analyses of wiens and colleagues  <cit>  showed that lba effects can disappear, if data exhibiting lba are recoded as missing. this implies that an identification of lba taxa before concatenation and reduction of data would be important. however, we do not have a grip yet on a reliable identification of biases in tree reconstructions which could guide a preselection of taxa. an immediate, however unsatisfying, solution is probably the reconstruction of trees with and without suspect taxa.

our simulations showed that in the presence of heterogeneous signal among genes the new heuristics increased the chance of finding a correct tree. it is, thus, an alternative to the computationally much more demanding quasi-biclique approach  <cit> . soss derived from b or b∗ matrices did not differ extensively in their success rate of correct tree reconstructions with simulated data, with small advantages for the b in cases of power-law non-random distribution of missing data. however, the analyses of the empirical data imply that tree reconstructions based on soss derived from b∗ will result in improved tree robustness.

CONCLUSIONS
our analyses of simulated and empirical data demonstrate that sparse supermatrices can be reduced on a formal basis outperforming the usually used simple selections of taxa and genes with high data coverage. the approach prresented here is will be of general inportance in phylogenomic studies based on large concatenated superalignments with incomplete data coverage. it clearly offers an alternative to threshold based data selection.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
b.mi., b.me. conceived the study, designed the setup and performed all analyses. b.mi. wrote the paper with comments and revisions from k.me., k.mi., p.k., b.v.r. and b.me. all authors read and approved the final manuscript.

