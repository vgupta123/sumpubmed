BACKGROUND
the prediction of the 3d structure of protein chains, known as protein structure prediction , is a key challenge in structural bioinformatics. not only there is a lack of consensus on how to approach psp, but for some of the current methods, especially ab-initio ones, computations are exceedingly demanding. rosetta@home  <cit> , one of the top predictors in the casp <dig>  experiment, used up to  <dig> computing days to model a single protein. one way in which psp calculations might be accelerated is by using a divide-and-conquer approach, where the problem of predicting the tertiary structure of a given sequence is split into smaller challenges, such as predicting secondary structure, solvent accessibility, coordination number, etc. and then using the solutions for these simpler problems as constraints, i.e. building blocks, for the original 3d prediction. a complementary strategy would be to reduce the size of the alphabet of the variables that are involved in the prediction of tertiary structure, and solve the previously mentioned sub-problems using the reduced alphabets. the alphabet by which the sequence of a protein is represented would be an obvious focus for any reduction mechanism.

an example of a widely used reduction is the two-letter hydrophobic/polar  alphabet  <cit> . this reduction is usually followed by constraining the residue locations of the protein to those of a 2d/3d lattice  <cit> . sometimes this hp alphabet reduction is applied to off-lattice proteins. for example, in a recent paper  <cit>  we compared predictions of residue contact numbers  for lattice and off-lattice proteins using both the hp alphabet and the amino acid  alphabet. the hp alphabet gave predictions significantly less accurate than the aa alphabet, although the difference was not large . the reduction of alphabets for psp related problems brings to the fore some important questions.  is it possible to reduce alphabet size without significantly losing information and hence degrading performance?  is there a single reduced alphabet for all problems? or would, for example, the prediction of disulfide bonds necessitate a different alphabet than, for instance, the prediction of contact numbers?

previous work
the alphabet reduction problem in the context of psp can be summarized in a simple question: which is the minimum number of aas that is able to represent the structural information of a protein? that is, when is the loss of information intrinsic in the alphabet reduction problem going to affect crucial data? in general this kind of process involves deciding on three aspects:  how are we going to represent the protein structural information?  how are we going to quantify how good is the reduced alphabet to maintain the crucial information held in the representation from point  <dig> and  how are we going to obtain the optimal reduced alphabet based on the metric of point  <dig> 

the work of solis and rackovsky  <cit>  is one of the earliest alphabet reduction methods that, like our approach, uses information theory to quantify the quality of the reduced alphabets. they propose an information gain function that measures how different the entropy of the structural patterns represented by a given reduced alphabet is from a random distribution of patterns, and then optimize this function using a monte carlo method to obtain their optimal reduced alphabet. two representations of a protein structure were evaluated in that work, one based on bond lengths, bond angles and dihedral angles of the protein's backbone and another one based on secondary structure states of the protein residues. other work of these authors combining information theory and reduced alphabets includes  <cit>  and  <cit> . the latter one also introduces the simultaneous generation of multiple reduced alphabets for different parts of the protein representation, which is similar to our dualrmi strategy.

other methods  <cit>  base their metrics on the miyazawa-jernigan interaction matrix  <cit> , while others  <cit>  are based on the blosum substitution matrix  <cit> . another work  <cit>  uses the kullback-leibler distance  <cit>  between probability distributions as their metric to evaluate reduced alphabets, applied to a four-state secondary structure representation. one example of the application of an evolutionary algorithm to alphabet reduction is  <cit> . in this case, a genetic algorithm  was used to optimize a five-letter alphabet for sequence alignment. the ga was used to maximize the difference between the sequence identity of the training alignments and a set of random alignments based on the same sequences. another approach  <cit>  applied mutual information to optimize alphabet reduction for contact potentials. meiler et al.  <cit>  proposed a method whereby, instead of treating each aa type at a symbolic level, they were characterized through several physical features and a neural network was applied to generate a lower dimensionality representation of these features. wrabl and grishin  <cit>  used the monte carlo method to optimize groups of amino acids that maximize the total variance of amino acid frequencies applied to multiple sequence alignments. a different structural representation was used by  <cit> , where the 3d structure of a protein is mapped into protein blocks of five residues long containing eight consecutive  dihedral angles. the distributions of aas in the protein blocks were used to group together those having similar local structure.

reduced alphabets for protein sequences have other traditional uses besides psp, such as protein design and mutation, where the goal is to obtain sequences, either generating them from scratch or modifying previously known proteins, that fold in a certain specific way or to obtain/maintain certain functional properties. to that end, it is important to known which aas can be exchanged with which others without impacting in the fold or the function/stability/etc. of the protein. that is, which aas can be grouped together because they behave similarly. a classic example of this kind of research is  <cit> , where several proteins with a certain specific alternation of polar/nonpolar residues were designed and all of them folded into four helix bundle proteins. the specific aa sequence of each protein varied, but the pattern of polar/nonpolar residues was always the same. another example is  <cit> , where an enzyme used for d-amino acids production was randomly mutated, and the individual aa substitutions that maintained the enzyme function and increased its thermostability were identified. in  <cit>  a 213-residue e. coli enzyme was mutated with the objective of obtaining a functionally similar protein having a reduced set of amino acids. in the final variant of the enzyme, after  <dig> substitutions, nine aa types occupied 88% of its sequence, and seven aa types were never present.

in recent work  <cit>  we proposed an automated method to perform alphabet reduction. this method uses ecga  <cit>  to optimize the distribution of the  <dig> letters of the aa alphabet into a predefined number of categories, using the mutual information  metric, as an objective function. this measure relates the dependence between two variables: the input attributes and the feature we are predicting. by optimizing this measure we are looking for the alphabet reduction that maintains as much of the useful information as possible existing in the input attributes related to the predicted feature. we applied this measure to predict cn  <cit> , optimizing alphabets ranging from two to five letters. afterwards, the dataset with reduced representation was fed into the biohel machine learning method  <cit>  to determine if the reduced alphabet was able to perform competently. it was possible to obtain an alphabet of three letters with similar performance to the full aa alphabet using a protein-wise accuracy metric.

however, when the sample size was small, e.g. trying to optimize alphabets with more than three symbols, the mi was unable to find a reduced alphabet without losing performance. table  <dig> contains, for each tested alphabet size in our previous work, the percentage of possible input patterns actually represented in the dataset. we were predicting the cn of a residue using as input information the aa type of a window of ±  <dig> residues around the target. therefore, if a two-letter alphabet was used to represent these data, there would be a total of  <dig> possible input patterns . if we were using a three letters alphabet, we would have  <dig> possible input patterns. as the training set contained around  <dig> instances, with two and three letters it was  possible to have at least one occurrence of all possible input patterns. however, beyond three letters only a fraction of the input patterns is available, and mostly with a single instance per pattern. an extreme case that illustrates the small sample size issue is if we were to compute this percentage of represented patterns for the original 20-letter aa alphabet. the difference between the size of the, already large, training set and the size of the input space is many orders of magnitude. mi needs redundancy in order to estimate properly the relation between inputs and outputs, and there is almost no redundancy in the dataset for alphabets with more than three letters. this objective function cannot provide appropriate information for a successful alphabet optimization. our experiments confirmed a performance degradation for alphabets of four and five letters and, moreover, it was difficult to extract meaningful explanations for the resulting reduced alphabets. in this paper we use an existing method  <cit>  to increase the robustness of the mi metric, which is implemented into two of our alphabet reduction strategies, rmi and dualrmi .

our contribution
in this paper, we address these issues by extending our previous work  <cit> . given a dataset and a feature to predict  one may ask "what is the optimal alphabet that must be used to represent and exploit the available data?" this is, in essence, an optimization problem for which, as stated above, a suitable objective function and optimization algorithm must be found. in this paper we propose that an existing robust mi estimation method  <cit>  is a very strong candidate for a good objective function, while a state-of-the-art evolutionary algorithm, ecga  <cit> , is used to explore the vast and complex search space associated with alphabet minimization. we test our protocol on two structural bioinformatics problems, namely, contact number  and relative solvent accessibility  prediction. that is, we used the automatically reduced alphabets to predict cn and rsa profiles for proteins and compared their accuracy against those of the full alphabet, reduced alphabets found in the literature  <cit>  and with some expert-designed ones. our results indicate that we can obtain reduced alphabets of only five letters that give an accuracy within 1% of that obtained with the full aa alphabet, and higher accuracy than the other reduced alphabets included in the comparison. the differences between the reduced alphabets are quantitatively analyzed. as a final experiment to show the generality and scientific relevance of this work, we used five-letter alphabet obtained with our protocol to reduce a protein representation using evolutionary information, namely a position-specific scoring matrix   <cit>  representation, and then we repeated the process of learning from both the reduced and the original representation and compare their performance.

methods
dataset and predicted psp features
contact number
the cn of a certain aa is a specific feature of a protein's 3d structure. that is, in the native state, each residue will have a set of other residues that are its spatial nearest neighbours. the number of nearest neighbours of a given residue is its contact number. a variety of machine learning paradigms have been used to predict this feature  <cit> . the cn definition we have used is the one proposed by kinjo et al.  <cit> . it is defined using the cβ atom  of the residues. the boundary of the sphere around a residue, defined by the distance cutoff dc ∈ ℜ+, is made smooth by using a sigmoid function. a minimum chain separation of two residues is required. formally, the cn, nip, of residue i in protein chain p is computed as:

  nip=∑j:|j−i|>211+exp) 

where rij is the euclidean distance between the cβ atoms of the ith and jth residues. the constant w determines the sharpness of the boundary of the sphere. in this paper we used a distance cutoff dc of  <dig> Å and a w of  <dig>  the real-valued definition of cn has been discretized in order to transform the dataset into a classification problem that can be mined by machine learning methods. we divide the range of cn values into two states  by cutting the cn range by its middle point. to predict the cn of each residue in a protein we use the set of input attributes labelled as cn <dig> in  <cit> : the input data consist of the aa type of the residues in a window of four residues at each side of the target.

in the final experiment using the pssm representation, this information was computed by using the psi-blast program  <cit>  following the procedure suggested in  <cit> . each residue in the chain is represented by  <dig> continuous variables. we use the same window size of ±  <dig> residues. thus, each instance has  <dig> attributes.

relative solvent accessibility
another interesting psp feature is the solvent accessibility  of residues. prediction of this feature is usually addressed after a certain aa-wise normalization, where the sa of a residue is divided by the maximum accessible surface in the extended conformation of its aa type in what is known as relative solvent accessibility   <cit> . in order to predict this continuous feature some works use regression methods  <cit>  while other works predict whether the rsa of a residue is, for instance, lower/higher than some threshold , treating this prediction as a classification problem  <cit> . we have used the dssp program  <cit>  to obtain the actual sa of each residue in the dataset. next, we compute the rsa by dividing the sa of each residue by the maximum sa values specified in  <cit>  for each aa type. the continuous rsa was transformed into a classification problem, by dividing the domain into buried/exposed states, placing a cut point at 25% rsa, as in  <cit> . the input data used to predict rsa are analogous to the data used for the cn dataset: the aa type of the residues in a window of ±  <dig> residues around the target. the pssm representation for rsa is also equivalent to the one used for cn:  <dig> continuous attributes.

protein dataset
we have used the dataset and training/test partitions proposed by kinjo et al.  <cit> . the protein chains were selected from pdb-reprdb  <cit>  with the following conditions: less than 30% of sequence identity, sequence length greater than  <dig>  no membrane proteins, no nonstandard residues, no chain breaks, resolution better than  <dig> Å and a crystallographic r factor better than 20%. chains that had no entry in the hssp database  <cit>  were discarded. the final dataset contains  <dig> protein chains and  <dig> residues. data were partitioned into training and test sets using an iterated hold-out procedure . the proteins included in training/test pair of sets are reported in . the original and alphabet-reduced datasets for both cn and rsa prediction and all the training/test partitions are available at  .

automated alphabet reduction protocol
the experimental protocol follows two main stages. in the first one, ecga is used to optimize a reduced alphabet. mi is used as the objective function. in the second stage, biohel  <cit>  is used to validate the reliability of the optimized alphabet found in the first stage by training classifiers, i.e. predictors, for cn and rsa. the following protocol was used for our experiments. for each dataset  and for each tested alphabet size  ecga is used to find the optimal alphabet reduction of the predetermined size using the mi-based objective function. three mi strategies are used: mi, rmi and dualrmi . next, the alphabet reduction policy is applied to the dataset. finally, the biohel classification method is applied to the dataset with the optimally reduced alphabet found in the previous stage. we have tested alphabets of up to five letters only, because we are  interested in determining the smallest possible alphabet size that suffers from only a marginal information loss and  because recent literature focuses on alphabets of similar sizes  <cit> .

optimization of reduced alphabets
ecga  <cit>  is an optimization method belonging to the family of estimation of distribution algorithms   <cit> . edas are evolutionary computation techniques that employ statistical learning or machine learning methods to estimate the structure of the problem, and employ these estimations to perform an intelligent exploration of the search space. in the ecga, the structure of the population is modelled as a set of non-overlapped groups of variables. the variables in each group interact strongly among themselves, and interact less with variables in other groups. next, an exploration mechanism seeks new solutions, exploiting the estimated problem structure model. we have extended a public implementation of ecga  to optimize reduced alphabets. the source code of the program is available from .

objective function
the aim of the alphabet reduction optimization is to simplify the representation of the dataset with the goals of  making the problem easier to learn and  enhancing the interpretability of the resulting classifiers. these two goals are, of course, counterbalanced with the need to maintain the essential information contained in the original dataset. therefore, the objective function for such a process should give an estimation of what the reduced input information can infer about the output.

mi is a measure that quantifies how much information one variable holds about the other  <cit> . it is defined as:

  i=∑y∈y∑x∈xplogppp 

where p and p are, respectively, the probabilities of appearance of x and y, and p is the probability of having x, y at the same time in the dataset. in our case, we use mi to measure the quantity of information that the input variables of the alphabet-reduced dataset have in relation to the states in which the protein structural feature is partitioned. that is, for a given instance, x represents a string that concatenates the input variables of the instance, while y encodes the associated class for that instance.

alphabet reduction strategies
we describe next the three studied reduction strategies. the first corresponds to the strategy tested in our previous work  <cit> , while the latter two represent substantial improvements.

mutual information strategy
this strategy is composed of a representation of the reduced alphabet and an objective function that evaluates such reduction. the representation is simple: it has one variable for each letter of the original alphabet  encoding the group of letters where this aa type is assigned. this variable can take a value in the range  <dig> .n -  <dig>  where n is the predefined number of symbols of the reduced alphabet. table  <dig> illustrates an example of such encoding for a reduction process into two groups. the objective function that evaluates the reduction is the original mi metric as per eq.  <dig> 

each objective function computation follows these steps: first, the reduction mappings are extracted from the representation. next, the instances of the training set are transformed into the low cardinality alphabet based on the extracted mappings creating a set of pairs , , ..., . then, mi is computed from the data, mi = i. the objective function of this alphabet reduction is its dataset mutual information.

robust mutual information
as discussed in the introduction, the performance of mi as a good objective function degrades when applied to small samples. hence, we also use the approach proposed in  <cit> . the strategy is known as robust mutual information . it uses the same representation encoding as in the mi strategy, but a different objective function, that is computed as follows: first, the reduction mappings are extracted from the representation and the mi measure is computed as in the previous strategy. next, we scramble the pairs of , i ∈  <dig> .n joining randomly some xi with some yj, but maintaining all x and y from the original dataset. the shuffling process is repeated n times with different random seeds, and the mi measures computed from each shuffling are averaged. mis=1n∑i=1npermmii. finally, the objective function of this alphabet reduction is the dataset mi minus the average shuffled mi: obj func = mi - mis. intuitively, mis is an estimation of the sampling bias existing in the data, unrelated to the relationship between x and y. by removing it from the mi, we obtain a better estimation of the amount of information that x and y share.

dual robust mutual information
the third reduction strategy is motivated by our findings  <cit> , that suggested that the target aa and its environment  might benefit from two different reductions. this observation leads us to think that it might be sensible to test the performance of a dual alphabet reduction: one reduction policy specifically for the target residue and another reduction policy for the other residues in the window. this strategy will be used in combination with rmi with the name dual robust mutual information  strategy.

verification of the reduced alphabets
to verify the results of the first stage of our protocol, we use the biohel machine learning method to learn the dataset with reduced representation. biohel  is an evolutionary-computation based machine learning system following the iterative rule learning approach  <cit> . biohel is also strongly influenced by gassist  <cit>  which is a pittsburgh gbml system. the system applies a generational genetic algorithm  with elitism, which evolves individuals that are classification rules. rules are obtained by an iterative process. after each rule is obtained, the training examples that are covered by this rule are removed from the training set, to force the ga at the next iteration to explore other areas of the classification space. the performance of biohel is enhanced by running it several times with different initial random seeds on the same data. afterwards, the rule sets obtained by each run are combined to form an ensemble that generates consensus prediction using a simple majority vote. for specific details of the design and objective function of biohel, see  <cit> . the source code of the program is available from .

RESULTS
contact number prediction
ecga was used to find alphabet reductions into alphabets of two, three, four and five symbols following the mi, rmi and dual rmi strategies. figure  <dig> shows the reductions obtained; for visualization of the physico-chemical properties of the aa groups obtained, we have colored each aa type differently according to the properties discussed in  <cit> . we have aligned as much as possible the amino acids between the reduction groups of increasing alphabet size. this allows us to observe how the optimization method rearranges the groups when the alphabet size grows. also, we have marked with a solid rectangle the amino acids that remain within the same group with at least one other amino acid for all the four tested alphabet sizes. for simplicity, we show only the reductions obtained from the first of the ten training sets. the reduction groups for the other training sets are reported in additional file  <dig>  these reduction groups, although slightly different for each training set , provide similar statistical and predictive properties.

when ecga is used with the rmi strategy to search the space of possible reduced alphabets with the goal of designing a five letters alphabet, it does not find a solution with five letters and reports instead a candidate alphabet with four letters. dualrmi did find a five-letter alphabet for the target residue, but only a four-letter alphabet for the other residues. thus, we overcame the rmi limitations with this heuristic approach.

counting the number of framed amino acids for each reduction strategy provides a simple metric of the results of these experiments. for the mi strategy, only nine aa always stay in the same group, while for the rmi strategy  <dig> of the  <dig> aa do. this simple metric shows how the objective function is more stable and thus the obtained alphabets are easier to understand, . moreover, later in this subsection we will show how this strategy also gives better performance when learning from the reduced alphabets. for the dualrmi strategy,  <dig> aa of the target residue and  <dig> of the other residues in the window remain in the same group. these numbers are less than the  <dig> aa of the rmi strategy, but rmi had no five-letter alphabet and therefore it is easier for this strategy to maintain the groups.

when optimizing for a two-letter alphabet, both mi and rmi find two identical groups of aa types that separate the hydrophobic residues  from the rest . hydrophobicity is one of the main factors in the folding process of proteins, so it is natural that a reduction process into only two symbols is equivalent to identifying the hydrophobic residues. the dualrmi strategy also gives similar reduction groups, but with small differences between the reduction of the target residue, where the hydrophobic group is smaller, and the reduction for the other residues, where the hydrophobic group contains more aa types.

looking at the physico-chemical properties of the four and five-letter alphabets a clear difference emerges  between the reduction groups for the mi strategy and those for the other two strategies. all mi groups have very mixed properties, while in rmi and dualrmi there is a clear difference between groups that include hydrophobic residues or not. however, some groups are difficult to explain, such as the ghts group for the dualrmi five-letter alphabet. g, t and s are small amino acids, h is large. g and h are hydrophobic, while the other two are not. h is aromatic and has a high coil propensity.

a retrospective analysis of the dataset reveals why ghts are clustered together. table  <dig> shows, for each aa type, the proportion of residues in the dataset that belong to the high cn class, sorted by increasing order. the groups correlate almost perfectly with the sorted residues in the table, showing that g, h, t and s have similar properties in terms of cn.

table also shows the reduction groups to which they belong for the dualrmi five-letter strategy.

the reduced alphabets were used to represent the cn datasets, which were then used to train and test biohel. table  <dig> contains the results of the learning process. for each reduction strategy and alphabet size we have reported three measures. as a performance measure we have included the test accuracy. to illustrate the interpretability and explanatory power of the reduced datasets, we have included two complexity metrics for the classifiers: average number of rules and average number of relevant attributes in each rule. as a baseline, the performance and complexity obtained with the full aa type representation is also included . these results were analyzed using statistical t-tests  to determine if the differences in accuracy were significant. the bonferroni correction for multiple comparisons was employed.

 accuracy is the average test accuracy from the ten cross-validation folds. a • marks reduced datasets where performance is significantly worse than the original full aa representation according to statistical t-tests with 99% confidence level.

the results for the mi strategy are similar to those presented in previous work. only the dataset with the three-letter alphabet gives a performance statistically comparable to the performance given by the original dataset. larger alphabet sizes degraded the results. on the other hand, both rmi and dualrmi obtain their best results in the four and five-letter alphabets, closing the performance gap with the full aa alphabet to  <dig> % . dualrmi always obtains better results than rmi, except for the three-letter case, showing the usefulness of this strategy. these results  answer the first of the questions that we wanted to address in this paper, determining the minimum alphabet size that obtains similar results to the full aa representation. moreover, the rules learned using the reduced datasets are more compact and human readable, in both the number of rules and the number of expressed attributes, than the solutions produced from the original dataset.

relative solvent accessibility prediction
ecga, with each of the three reduction strategies  was also used to find alphabet reductions to two, three, four and five symbols for rsa prediction. figure  <dig> describes the reductions obtained, using the same visualization techniques employed previously. the comparison between the alphabets in figure  <dig> with those in figure  <dig>  generated for cn, shows several differences. for instance, the alphabets for rsa contains more groups of polar residues, while most of the hydrophic residues are grouped together. on the other hand, several groups of hydrophobic residues exist in the cn alphabets, while the polar residues are grouped together.

given that both features are strongly anti-correlated , one could expect that the reduced alphabets found for cn would be useful for rsa too. this turns out not to be the case and the differences between the two features are successfully captured by the alphabet reduction process. moreover, these differences further justify the automated procedure presented in this paper, which tailors the reduction specifically to each problem.

the number of amino acids that remain grouped together in the mi strategy is  <dig>  while in the rmi strategy it is  <dig>  for the dualrmi strategy,  <dig> aa types also stay in the same group for the target residue, while  <dig> of them do for the other residues. again, no alphabet with five letters was found for the rmi strategy and we find, again, a group of aa types formed by ghts in rmi and dualrmi alphabets. if we sort the amino acids by their proportion of exposed instances in the dataset  we can see how the reduction groups match perfectly the order of aa types in the table, similarly to what happened in the case of the cn dataset.

also showing the reduction group to which each aa type belongs for the dualrmi strategy and five-letter alphabet.

we also observe some interesting facts that are particular to this dataset, namely the presence of reduction groups including only one amino acid type: the a type for the dualrmi strategy with five-letter alphabet and target residue and g for the other residues, and x in rmi with four letters and dualrmi with four and five letters. the end-of-chain symbol, x, only appears in  <dig> % of the instances in the dataset. however, this symbol has the highest proportion,  <dig> %, of exposed instances of all  <dig> alphabet letters. in comparison, the second letter with highest proportion of exposed residues is lys with only  <dig> % of exposed instances. thus, it makes sense for the protocol to create a reduction group containing only x. we also observe other very small groups, like ek.

the reduced alphabets were used to represent the rsa datasets, which were then used to train and test biohel. table  <dig> contains the results of the learning process, which are quite different from the results for cn. here only the dualrmi strategy  achieves a performance statistically similar to that obtained by learning the dataset with full alphabet. there is a significant performance difference between rmi and dualrmi in the rsa, which was not observed for the cn dataset. moreover, the performance gap between the best reduced alphabet, using dualrmi and five letters and the original alphabet with full aa type representation is  <dig> %, even less than for the cn dataset. for the predictions of rsa, an alphabet representation with only two or three symbols degrades the classification performance severely. this matches with our previous observation of having very small reduction groups in some of the dualrmi alphabets . if an alphabet of two or three letters contains one letter with such small groups it means that there must be another letter grouping a large number of letters, and the likelihood that such large group is meaningful is very low, because it is over-simplifying the representation. table  <dig> shows two rule sets generated by biohel, one for cn, the other for rsa, both obtained from the full aa alphabet. we can observe very specific predicates associated to the target residue, especially for the rsa dataset. it would be practically impossible to generate equivalent rules when learning from alphabets with only two or three letters.

 accuracy is the average test accuracy from the ten cross-validation folds. a • marks reduced datasets where performance is significantly worse than the original full aa representation, according to the statistical t-tests with a 99% confidence level.

rule set at the left is for cn prediction. rule set at the right is for rsa prediction. aa± n means aa type for residue in position ± n in respect to the target residue. x means end of chain, in case one of the residues of the window overlaps with either one of the two ends of a chain.

comparison against other reduced alphabets
we compared the performance of the five-letter reduced alphabets obtained using the dualrmi strategy, the most successful of those presented in this paper, against other alphabets summarized in table  <dig>  the first four are taken from the literature  <cit> . for the specific usage in this work it has been necessary to extend them with an extra letter, the end-of-chain symbol , which is used to identify when the window of residues around the target overlaps with one end of the chain. we would like to clarify that these alphabets from the literature were not optimized for cn or rsa prediction, but for other tasks such as sequence alignment. thus, we expect our alphabets to perform better because they have been explicitly optimized for the features at hand. our goal in this experiment is to check how adaptable these reduced alphabets are across different psp-related problems. our aim is, by no means, to find a single best alphabet that is good for all protein problems.

these alphabets have been extended with a letter corresponding to the end of chain symbol. hd n = human designed alphabet n.

moreover, we have manually designed three reduced alphabets based on physico-chemical properties. the first alphabet uses four properties: size, hydrophobicity, coil propensity and aromatic nature. the second alphabet adds another property to the first one: charged residues, and the third alphabet adds another property to the second one: those amino acids having sulphur atoms. the goal of this comparison is to check how far in performance terms a generic reduced alphabet is from an optimized reduced alphabet. table  <dig> contains these comparisons. the automatically derived reduced alphabets obtain higher performance than all the other reduced alphabets. the performance degradation of these alternative alphabets, when compared to the full aa representation, was significant according to the t-tests for all the alphabets in the rsa dataset, and for the mu <dig> alphabet in the cn dataset. the three human-designed alphabets, despite having more letters than the other alphabets, perform poorly. a less reduced alphabet, if improperly designed, will lose as much information or more than a more compact alphabet.

 a • marks reduced datasets where biohel performs significantly worse than the aa type dataset according to the t-tests with a 99% confidence level. a ▼ marks the alphabets that performed significantly worse than the dualrmi strategy.

even though that the performance differences between the other reduced alphabets and the aa representation were significant, these differences are not very large. this observation might be taken to suggest that developing new reduced alphabets adapted for cn or rsa may not be worthiwhile. we do not agree with this contention for several reasons. first of all, the automated alphabet reduction proposed in this paper is very generic and flexible. thus, generating new reduced alphabets tailored to new features can be done easily and without much effort. secondly, the previous alphabets, despite still being able to hold quite a large amount of information, are not able to capture totally the crucial information for an optimal prediction of cn and rsa. to illustrate this claim we have regenerated tables  <dig> and  <dig>  to compare our dualrmi alphabets to the four alphabets from the literature. the new tables are table  <dig>  and table  <dig> .

trans. = number of transitions between groups. ave. range = average range of each reduction group, range is the difference between the maximum and minimum high cn ratio of the aas of a group.

 ave. range = average range of each reduction group, range is the difference between the maximum and minimum exposed ratio of the aas of a group.7

to quantify the differences between the reduced alphabets we have computed two simple statistics:  counting the number of transitions between reduction groups through the sorted list of aa and  computing the range of each reduction group, that is, the difference between the maximum and minimum high cn/exposed ratio of the aas in the same group, and averaging these ranges. both a low number of transitions and a small average range indicate that the aa grouped together have similar behavior in terms of cn or rsa. the alphabets generated by our dualrmi strategy present always the lowest number of transitions and average range from all the compared alphabets. except for the mm <dig> alphabet, all the other three alphabets from the literature have a number of transitions that, at least, almost doubles the number of groups of dualrmi.

with these statistics we have quantified the differences of the compared alphabets in relation to cn and rsa. however, why do several of these alphabets have poor performance for cn or rsa? tables  <dig> and  <dig> can also answer this question. in mm <dig>  lys  and thr  are in the same group. for mu <dig>  lys and his  are in the same group. for sr <dig>  lys is in the same group as ala . these ranges of exposed ratios are very large, effectively treating equally aa types with very different behavior. in dualrmi this situation does not happen, and the largest range is only  <dig> % wide. for the cn dataset all ranges of high cn ratio for all alphabets are much smaller, and this is reflected in the much smaller performance differences between all reduced alphabets. the principal difference between the alphabet with worse performance  and the others, besides having one less letter, is having clustered together ala and pro. none of the other alphabets has both aas in the same group. a further investigation would be necessary to quantify the impact of such a group.

as a final experiment, we test the performance of the reduced alphabet obtained from the rsa dataset in learning cn and also using the cn reduced alphabet to learn rsa. this latter option is labelled dualrmi-alt. the results of this experiment are in table  <dig>  the optimized alphabet using dualrmi for rsa performed well when applied to the cn feature. however, the reverse is not the case. the alphabet optimized for cn data performs poorly when applied to the rsa data. the reason for this is that the cn alphabet misses a specific reduction group only for glu and lys and, in consequence, having the same problem that we have discussed above about the alphabets from the literature. the rule sets in table  <dig> also showed this difference between cn and rsa. the predicates associated to the target residue were quite different between features. this observation shows that reduced alphabets should be tailored to the specific problem at hand, even if the problems are as similar as cn and rsa.

 a • marks reduced datasets where biohel obtains a performance significantly worse than the aa type dataset according to the statistical t-tests with a 99% confidence level. no significant differences were detected between the reduced alphabets.

extrapolating the obtained alphabets to reduce an evolutionary information-based representation
the experiments performed so far have been applied to proteins represented using their primary sequence. this representation contains limited amount of information when compared to other, more modern, approaches such as pssm representations that take into account evolutionary information of proteins. in the final experiments reported in this paper we now evaluate the generality of the results  obtained so far by our protocol. to do so we are going to adapt the alphabet reduction process from the primary sequence representation to a pssm representation.

in the primary sequence representation, each residue is characterized by a single discrete variable with  <dig> possible values . on the other hand, in the pssm representation, each residue is characterized by  <dig> continuous variables. each of the  <dig> variables indicates, for the associated aa type, its degree of preference for the corresponding residue position in the chain. each letter in a reduced alphabet groups a set of aa types that should be treated as if they all were the same. given all this, the application of our current reduced alphabet to a pssm representation  is very simple:

• each residue will be characterized by n continuous variables, where n is the number of letters of our reduced alphabet.

• the value of each of these attributes will be computed by averaging the variables of the pssm profile associated to the aa types that were grouped to form that letter of the reduced alphabet.

• in the case of using a dual alphabet , we will apply one reduction to the pssm profile of the target residue and the other reduction to the pssm profiles of the other residues in the window.

as an example, if we were to apply this process, using the five-letter dualrmi alphabet, we would take the original dataset of  <dig> variables  and transform it into a dataset of  <dig> variables, that is  <dig> variables for the target residue and  <dig> variables for the other  <dig> window positions. as we explained previously, the rmi and dualrmi  strategies converged to four-letter alphabets even if they were trying to optimize a five-letter alphabet.

the validation of this reduced pssm representation is the same as used before. we will train biohel to predict cn and rsa using two representations:  the original pssm representation of  <dig> attributes and  the reduced pssm representation  consisting of  <dig> attributes and we will compare the obtained accuracies. table  <dig> contains the results of these experiments. we show four different performance metrics. besides reporting test accuracy, and number of rules and number of expressed attributes per rule, as we did in the previous experiments, we also report the average run-time of each biohel training process. a full experiment consists of  <dig> runs of biohel:  <dig> cross-validation training sets ×  <dig> models in each ensemble. the reported run-time metric is the average of these  <dig> runs. this metric is relevant in this context, because the training process using the reduced representation is much lower, and given the magnitude of the time values, this run-time reduction becomes an important factor. all experiments were performed on the jupiter supercomputer of the university of nottingham, using opteron- <dig> processors running at  <dig>  ghz, the linux operating system and a c++ implementation of biohel.

the results reported in table  <dig> indicate that the performance gap between both representations, like in the previous experiments is small:  <dig> % for cn,  <dig> % for rsa. moreover, the solutions generated by biohel when learning from the reduced representation are more compact in terms of number of rules and number of variables used in each rule. the number of variables per rule for these experiments is only a fraction  of the attributes. moreover, the run-time of the training process was approximately  <dig>  times faster for both datasets, and considering that the run-times are reported in hours, this reduction is quite considerable in absolute terms.

the predictability of these models is quite comparable to other work from the literature. kinjo et al  <cit>  report  <dig> % accuracy on two-state cn prediction, although their class definition criterion is slightly different from ours, and they use a distance threshold of  <dig> Å instead of  <dig> Å. dor and zhou  <cit>  recently obtained  <dig> % accuracy on two-state rsa prediction from a pssm representation  with a 25% rsa cutoff, the same as we have employed in this paper. thus, our alphabet reduction protocol can be applied to state-of-the-art representations and obtains accuracy levels that are quite similar to the performance reported in the literature for predicting cn and rsa. this last experiment has helped to illustrate the generality of our findings.

CONCLUSIONS
this paper develops the use of information theory based automated procedures for alphabet reduction in psp datasets. our investigations indicate that:  finding a reduced alphabet with a performance that is statistically equivalent to the performance obtained with the full aa type representation is possible,  this does not compromise accuracy and enhances interpretability and  different problems might require different reductions and  the alphabets obtained from primary sequence data can be successfully adapted to richer representations using evolutionary information. these four observations taken together point to the need for a robust automated method, such as the one described in this paper, for tackling alphabet reduction. the tests with the three reduction strategies tell us that the dualrmi strategy can give a significant advantage, as has been shown for the solvent accessibility dataset. the reduction groups that our automated procedure finds translate quite well to physico-chemical characteristics such as hydrophobicity, but not completely. we find groups of letters such as ghts that are difficult to explain. a retrospective analysis of the dataset shows that the reduction groups are sound, because they are a consequence of the underlying data that we are mining. moreover, the learned reductions suggested a new way of interpreting the data, which a priori would have made no sense. that is, our results show that this automated procedure is sound, is not bound by any preconceptions the experimenter might have, tailors the alphabet reduction specifically for each dataset and obtains higher performance than other reduced alphabets available in the literature or human-design alphabet reductions. it can be applied with performance comparable to state-of-the-art protein representations, while at the same time it is able to provide new insight into the available data.

in future work, we will test alternative objective functions as well as other robust mi estimations. it would also be interesting to check if it is possible to find alphabets of slightly higher cardinality than the ones studied in this paper that are able to close even more the performance gap with the original representation. we also would like to apply this protocol to protein design problems. finally, it would be interesting to apply this protocol to datasets with more than two classes or, even, regression datasets, although we expect that this option will require a much more robust fitness function, as this means  making worse the sample size problem of the mi metric and  mi has to be estimated in some way if applied to regression problems, as it originally only deals with discrete variables.

authors' contributions
jb participated in the conception and design of the study, carried out the experiments and statistical analysis reported in this paper and drafted the manuscript. ms helped to draft the manuscript. jh participated in the design of the study and helped to draft the manuscript. rs and av participated in the design of the study. nk participated in the conception and design of the study and helped to draft the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
reduction groups obtained for all the training sets. this document lists  <dig> tables  containing the details of the reduction groups generated by our protocol for each of the ten training sets.

click here for file

 acknowledgements
we acknowledge the support of the uk engineering and physical sciences research council  under grant gr/t07534/ <dig>  we are grateful for the use of the university of nottingham's high performance computer.
