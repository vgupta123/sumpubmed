BACKGROUND
in the last several years, virtual screening has become an accepted tool in drug discovery. it has been successfully applied in a number of therapeutic programs, in particular, at the lead discovery stage, where high-throughput molecular docking can play an important role  <cit> . in concert with the continued need for improvements of in silico docking accuracies, the explosive growth of commercial and publicly available chemical databases requires computational techniques to efficiently implement docking protocols and rapidly screen millions of compounds in a timely fashion. here, we are focusing on the techniques to enable large scale docking using linux-based hpc platforms.

several commercial docking programs, such as glide  <cit> , ligandfit  <cit>  and flexx  <cit> , can distribute docking jobs to computers over the network. however, protocols that can seamlessly dock millions of compounds and capture the top percentage of high-scoring ligands are not standard. there are two major requirements for such a protocol running on a linux cluster:  the ability to launch parallel docking jobs through a queuing system; and  the ability to process millions of compounds in a reasonable time. since the latter requirement may call for hundreds of central processing units  working simultaneously, the protocol should effectively handle the associated data flow through the file system without affecting the performance of the cluster. in this note, we describe a linux cluster-based protocol using autodock  <cit>  as the docking engine.

autodock is a widely used docking program developed at the scripps research institute. application of autodock requires several separate pre-docking steps, e.g., ligand preparation, receptor preparation, and grid map calculations, before the actual docking process can take place. existing tools, such as autodock tools   <cit>  and bdt  <cit> , integrate individual autodock steps within a graphical user interface , and provide automated features for docking runs. however, they do not contain the capability to effectively process millions of compounds in a single execution. at the autodock website  <cit> , there is a tutorial with scripts to teach users how to use autodock and how to employ unix commands to perform virtual screening. in the tutorial, first a user needs to manually prepare the receptor, ligands, energy grids and a list of ligands names; then every compound in the list is looped through by a shell script, where the compound can be docked either by executing autodock commands inside the loop or by submitting the corresponding commands to a queuing system. this approach works well for thousands of compounds, however, it may not work for millions of compounds due to the limitations of the file and queuing systems. concurrently, autodock has also been used in grid-computing projects, fightaids@home  <cit>  and wisdom  <cit> . in a grid-computing infrastructure, tens to hundreds of thousand cpus are used to setup a computational grid. through specialized middleware, servers in a grid can schedule jobs, send applications and data to a cpu, and retrieve results. because of the large number of cpus involved, the computational power of a grid is phenomenal. for example, in the wisdom project,  <dig> million docking experiments were performed using autodock in  <dig> days. however, due to the nature of its infrastructure, the job success rate was 65% and the percentage of time spent running the application was ~50%. on the other hand, using a dedicated hpc platform  with specialized software tools for autodock, it is possible to screen millions of compounds in a reasonable time at a much higher job success rate and cpu efficiency.

here, we describe a docking-based virtual screening  pipeline, based on autodock , where perl and shell scripts are used to integrate executables and scripts from adt, openbabel  <cit>  and autodock. this implementation has the following advantages:  a scalable parallelization scheme for autodock integrated with queuing systems, such as the load sharing facility , platform computing inc.  and the portable batch system , altair grid technologies ;  a protocol to retain user-specified top percentage of docked ligands based on their docking scores;  a x-window's-based gui for users to specify docking parameters, submit docking jobs and query/visualize docked ligands; and  a collection of pre-processed compounds from the zinc database  <cit>  and the national cancer institute  diversity dataset  <cit>  in native autodock pdbq format.

implementation
a high-throughput screening campaign typically has many more ligands  than the number of cpus  available. in this case, a straightforward approach to process the ligands is via a parallelization scheme where the input ligands  are divided into n equal partitions corresponding to the number of cpus available for docking. for each cpu, a single partition of ligands is docked to the target receptor. after all docking jobs are completed, the results from the n cpus are consolidated and the final  top m scoring ligands are returned to the user for further analysis.

the input parameters for screening are specified via a gui, which creates one master parameter file that is used to drive all related scripts and programs. inside dovis there are three distinct implementation steps, pre-docking, parallel docking, and post-docking, which are integrated with a queuing system.

pre-docking
in this step, the receptor is converted to the native autodock format and the ligands are partitioned into n files. dovis accepts receptors with all hydrogen atoms specified in standard pdb or mol <dig> format. a python script, "prepare_receptor.py" from adt, is used to convert the receptor into the pdbqs format required by autodock. the acceptable input formats for ligands include sd, mol <dig>  and pdbq. when the ligand is provided in the sd format, it is converted into mol <dig> by openbabel. the mol <dig> files are then partitioned into n files with roughly the same number of molecules in each file. if the input ligands are provided as pre-processed pdbq files, the list of ligand file names is simply partitioned into n lists of roughly equal length.

parallel docking
from the previous step, either a ligand file  or a list of ligands is assigned to each cpu, where the ligands in each set are separately processed and docked to the receptor, one ligand at a time per cpu. for ligands in the mol <dig> format, a python script, "prepare_ligand.py" from adt, is used to generate the corresponding pdbq file. we pre-compute energy grid maps for all possible ligand atom types at the beginning of each parallel job. thus, any grid map needed by autodock can be directly loaded without a separate calculation. after each docking, the docking log-file  is parsed by a perl script to extract the "estimated free energy of binding," which is used as the criterion to select the top m scoring dlg files from each cpu.

during testing, we found that there are more than  <dig> file operations and over  <dig> mb of data flow associated with the docking of each ligand. if a network file system  is used with more than  <dig> cpus running concurrently, the data flow degrades the performance of the entire linux cluster. to overcome this problem, dovis provides the option to use the local disk drive on each node or cpu to store the energy grid maps and other intermediate files.

post-docking
after all docking jobs are completed, a perl script is used to combine all saved results from each cpu and to collect the top m scoring ligands from the consolidated list as the final result. the dlg files of all selected top-scoring ligands are collected and compressed into a directory.

queue integration
on large linux clusters with distributed memory, it is essential to integrate dovis with a queuing system. with the job dependency support of a queuing system, the three steps discussed above can be sequentially executed and parallel jobs can be automatically launched. for the parallel docking job, at each cpu, the same executables are used to process different ligands. this computational strategy is most efficiently handled by queuing systems that support the functionality to bundle many single-processor jobs using the same executables into a single job, typically referred to as job array. thus, dovis is suitable for any queuing system that supports job array and job dependency. currently, we have integrated dovis with the lsf and the pbs queuing systems.

in order to run dovis on linux clusters without a queuing system, we provide a version of dovis that uses multi-threading to run parallel docking jobs. this scheme is especially suitable for shared-memory linux clusters.

there is another issue related to shared queuing systems. usually, every job in the queue has a runtime limit. once the limit is reached, the job is terminated. when docking large number of ligands, job array may not be able to complete all tasks within the allowed runtime limit. therefore, we implemented a restart function, which tracks the progress of each cpu and gives the user the option to manually restart dovis from where it left off.

graphic user interface
a gui was developed for dovis using java swing to provide a convenient way to specify the target receptor, the ligand database and the docking parameters. using the gui, users are also able to submit the job to the queuing system. in addition, the jmol  <cit>  molecular viewer is embedded in the gui to enable visualization of docked ligands with the target receptor.

RESULTS
in order to build a ligand database for testing and scientific research purpose, we pre-processed the zinc database  and the nci diversity dataset  into autodock pdbq format. using the pre-processed zinc database, we tested how many ligands dovis could dock to a receptor per day with varying numbers of cpus. we performed tests with up to  <dig> cpus on a linux cluster and observed a near-linear speedup as a function of number of cpus, for the zinc database. this indicates that our implementation achieves near-optimal performance. in addition, we carried out the virtual screening of the zinc database against the ricin a chain  as a receptor target with  <dig> cpus. the task was completed in  <dig> days, corresponding to ~ <dig> ligands per cpu per day, with two manual restarts due to a four-day runtime limit in our queuing system. see additional file  <dig> for the detailed parameter choices.

discussion
we developed dovis as a utility software to automate docking jobs with autodock. it can reliably screen millions of compounds against a receptor and automatically save the top percentage of high-scoring hits. the parallelization scheme employed here provides a straightforward approach to such a problem. when all cpus are started around the same time, this scheme works well and all cpus shall complete their jobs at about the same time. however, when the queuing system does not allow all cpus to be launched at the same time, with the equal number of ligands assigned to each cpu, the efficiency of dovis is compromised. this may become a problem with a queuing system when a large number of cpus are requested for a calculation. to solve this problem, the number of ligands assigned to each cpu should be re-distributed in a way that the cpus launched earlier shall docked more ligands than the cpus launched later. currently, we are working on an improvement to address this issue.

CONCLUSIONS
using autodock as its docking engine, dovis provides an automated parallel docking package that is integrated with a queuing system. this application is suitable for conducting large-scale high-throughput virtual screening on linux cluster platforms. the dovis package is freely available .

authors' contributions
sz implemented and tested the software. kk implemented the gui and tested the software. xj participated in the software design and testing, and drafted the manuscript. jr and aw conceived the project, participated in the software design and project coordination. all authors read and approved the final manuscript.

supplementary material
additional file 1
supplementary_material_dovis_paper. this file describes the software packages required to setup dovis, the autodock parameters used to run a virtual screening reported in the paper, and the dovis package installation options.

click here for file

 additional file 2
dovis_alpha_release.tar.gz. this file contains the dovis package , release note and user manual.

click here for file

 acknowledgements
we would like to thank drs. m. lee and m. olson for helpful suggestions. this work was sponsored by the us department of defense high performance computing modernization program , under the high performance computing software applications institutes  initiative. the opinions or assertions contained herein are the private views of the authors and are not to be construed as official or as reflecting the views of the us army or the us department of defense. this paper has been approved for public release and distribution is unlimited.
