BACKGROUND
a fundamental goal of human genomics is to identify and describe differences among human genomes. besides the detection of single nucleotide polymorphisms, larger mutation events, such as deletions, insertions, inversions, or inter-chromosomal rearrangements, can have a crucial impact on genomes function. they can for instance result in loss, mutation or fusion of genes that can be linked to certain diseases such as cancer. the characterization of structural variations can thus help shed some light on the complex mechanisms in cancer biology  <cit> .

structural variations discovery
current sequencing technologies enable fast sequencing of human genomes at high coverage and low cost. usually, multiple copies of a genome are randomly broken into small fragments which are then sequenced. most of these techniques allow to read dna fragments from both sides, resulting in a large set of paired-end reads. saving the time and cost intensive assembly and finishing steps which would be necessary to determine the full genome sequence, the read pairs can directly be used to detect structural variations by paired-end mapping. the paired-end reads from a newly sequenced donor genome are mapped onto a reference genome which is already assembled to a complete dna-sequence  <cit> . in a region where the two genomes do not differ, the mapped reads have the correct orientation and their distance coincides with the fragment length in the donor genome. such a mapping is called concordant. however, if a mapping is discordant, i.e. the orientation is incorrect or the distance differs significantly from the expected fragment length, this indicates a putative structural variation in the donor w.r.t. the reference. this concept was fist introduced by volik et al.  <cit>  and tuzun et al.  <cit> . in recent years, many tools have been developed that follow this approach to identify putative structural variations .

another approach is to actually assemble all reads to obtain the exact sequence. to avoid the computational expense of full de novo assembly  <cit> , one can restrict the process to only reads from regions suspect to harbor a variation, as for instance done in  <cit> .

tumor genomes analysis
besides problems in the accurate prediction of variations due to ambiguous mappings, mappings in repeat regions etc.  <cit> , these approaches have a fundamental shortcoming for the analysis of cancer data: they do not differentiate between inherent, patient-specific variations and those which are tumor-specific. even if the data of both a tumor sample and a normal sample  from the same individual is available, this is not a trivial task  <cit> . in particular, considering any discordant mapping from the tumor data overlapping a structural variation found in the normal data as non-tumor-specific could result in missing tumor-specific variations since different structural variations can be overlapping or very closely located. another difficulty in analyzing cancer data is that a cancer sample is most likely a mixed sample: although taken from tumor tissue, it usually contains also normal cells  <cit> . hence, we have to tackle the "need to simultaneously analyze data from tumor and patient-matched normal tissue … and the ability to handle samples with unknown levels of non-tumor contamination"  <cit> .

to our knowledge, very few methods allow a combined analysis of pooled data sets, such as a normal and a tumor sample, to detect deletions of arbitrary length indicated by discordant mappings. breakdancer  <cit>  was used in  <cit> , although it was not designed explicitly for such a task. in  <cit> , it was proposed to cluster together only mappings from the tumor data set which do not overlap any discordant mapping from the normal data set. this might result in discarding tumor-specific variations that overlap or are close to normal variations, a problem we address in the present work. they also suggest pooling the mappings obtained from both samples into one data set and to assemble them into clusters calling for the same structural variations. clusters containing no mappings from the normal data set are then considered being tumor-specific. recently, hormozdiari et al.  <cit>  introduced an approach to detect common variations among several genomes . however, their method focuses on ambiguously mapped read pairs and does not consider concordant mapped pairs. other methods that consider pooled data sets are for analyzing only small scale deletions of a few base pairs in size  <cit> .

contribution
the goal of the present paper is to address a very specific problem in the analysis of matched tumor/normal genomes: the detection of putative tumor-specific deletions that overlap or are closely located to normal deletions.

the motivation for this question stems from the preliminary analysis of data from an adenocarcinoma patient  <cit> , obtained by sequencing both normal cells and tumor cells. we found that, for the tumor data, many overlapping deletions were called. moreover we observed that, in some regions, more deletions were called than there could actually be on a diploid genome. especially in regions where the normal data indicate a possible deletion, we found a high signal of inconsistency for the deletions in the tumor sample, whereas in the normal data this signal was very low.

to detect such regions, we rely on a combinatorial notion of conflicting deletions that represents deletions, defined by clusters of discordant mappings which overlap significantly and could then not happen together on the same diploid genome. the notion of conflicting structural variations was introduced in  <cit>  to handle in particular reads that can be mapped to several locations. we extend the work of  <cit>  by describing a rigorous definition of conflicts that accounts for the difference between deletions and breakpoint region, and by showing that conflicts in an haploid genome are limited to sets of two or three deletion clusters.

in order to extract from these data putative tumor-specific deletions that would be consistent with the conflict-free deletions detected in the normal sample, we designed a method based on the combined analysis of both the tumor and normal data. our approach assumes a mixture of normal and cancer cells in the tumor sample, i.e., four copies of each chromosome . our combinatorial objective is to find a partition of the mappings from the tumor data set into four sets  that corresponds to a consistent set of deletions. if not all mappings can be incorporated into a consistent deletion call, we aim at minimizing the number of mappings required to be discarded to reach consistency. this approach allowed us to consistently explain most of the data we analyzed: very few mappings needed to be discarded to be able to both refine normal deletions and detect putative tumor-specific deletions that were both conflict-free and consistent with the normal deletions. an implementation of our method can be found in additional file  <dig> 

methods
mappings and deletion clusters
we first recall the standard combinatorial approach to detect deletions in a set of mapped paired-end short-reads, based on clustering together pairs of reads that indicate a similar deletion. we assume here that the sequenced short reads were first mapped onto the reference genome, resulting in a unique location for each read; several tools exist for this task, including some that can handle reads that map in several locations of the reference genome  <cit> .

a deletion is a segment of the reference genome that is not present in the donor genome. the precise location of the deletion in the donor genome is called its breakpoint. a pair of reads spanning the breakpoint in the donor genome will be mapped to the reference with a distance increased by the size of the deletion, and with proper orientation of its two reads. for such a mapping m, let span be the interval from the left end of the left read to the right end of the right read. we assume the size range  of the sequenced fragments  is known and comprised between minlen  and maxlen . a deletion has then to be of size at least delmin := |span| – maxlen and at most delmax := |span| – minlen. obviously, the deleted segment has to be between the right end of the left read and the left end of the right read—we call this the breakpoint region, br, of the mapping. see figure  <dig> for an illustration. note that the approach described above does not aim at detecting very small deletions , which can be done by analyzing directly the read mappings  <cit> .

if the coverage of the sequenced genome by the reads is high, one can expect that a deletion breakpoint is covered by several pairs of reads. a large number of mappings indicating the same deletion increases naturally the confidence in this potential deletion. furthermore, the specification of the deletion, its location and length, will generally be more precise if it is based on more observations. therefore, it is common to cluster mappings that can be explained by the same event. more precisely, in the case of a deletion, a set c = {m <dig> …, mn} is a valid cluster of mappings if there is a possible deletion length compatible with all mappings. let  and  . then there exists dellen ≤ |br| such that for each m ∈ c : delmin ≤ dellen ≤ delmax. for a valid cluster c, we can infer the size range of the deletion:

delmin := max {delmin | m ∈ c} and

delmax := min {delmax | m ∈ c} .

a cluster c is called maximal if it is valid and there is no mapping m ∉ c such that c ∪ {m} is valid. figure  <dig> shows an illustration of a cluster.

haploid conflicts
to introduce the notion of conflict, we first consider that all deletions may have taken place on a haploid genome. we discuss the diploid case in the next section.

the notion of conflicting clusters was introduced in  <cit>  and we refine it here. first, as discussed previously, we assume that a deletion is associated to a cluster  of discordant mappings. the deletion associated to a cluster is then located within the breakpoint region of the cluster. further, if any read—belonging to either a concordant or a discordant pair—is mapped onto a segment in the reference sequence, this segment can obviously not be part of a deletion. our last assumption is that each deletion is covered by exactly one cluster. hence, our model does not allow scenarios as shown in figure  <dig>  and considers such overlapping clusters as conflicting. this approach is motivated by several reasons. first, such configurations, where two overlapping clusters can be neither combined into one valid cluster nor consistently explained by two separate deletions within each cluster, require very specific combinations of fragment length, read length and deletion sizes, and are then quite unlikely to be observed. also, from a computational point of view, deciding if such overlapping clusters are not conflicting require to investigate splitting the deletion indicated by a cluster into several smaller deletions. as far as we know, this splitting deletion problem has not been considered and there does not seem to be an obvious algorithm to address it. finally, the additional conflicts stemming from our assumption did not impact the analysis of the data we considered. so disregarding these pathological exceptions leads to a simpler combinatorial model, that proves to be sufficient to explain the data set we analyzed.

definition  <dig>  let c be a set of maximal deletion clusters. then c is consistent if for all c ∈ c, contains an interval of size delmin. otherwise it is conflicting.

if a set of clusters is conflicting, any set containing all these clusters is also conflicting. to detect all conflicts, it is sufficient to find the minimal conflicting sets, a general notion that was used to deal with inconsistency in reconstructing ancestral genomes  <cit> .

definition  <dig>  a set of maximal deletion clusters c is minimal conflicting if and only if it is conflicting and, for all c ∈ c, the set c\{c} is consistent.

lemma  <dig>  a minimal conflicting set in a haploid genome contains exactly two or exactly three maximal deletion clusters.

proof. first, it is not difficult to see that there can be minimal conflicting sets containing exactly two or exactly three maximal clusters . it then remains to be shown that there is no minimal conflicting set that contains more than three clusters.

to prove this fact, we rely on the following observation: if the breakpoint region of a maximal cluster c is covered completely by other maximal clusters c <dig>  …, ck, i.e., , then {c, c <dig> …, ck} is conflicting.

now, assume there exists a minimal conflicting set of clusters c = {c <dig> …, ck} with k >  <dig> which are all assigned to one chromosome copy. as they are in conflict, they overlap. according to the above observation, if any cluster overlaps with two clusters on one side, these three clusters would create a conflicting set, which contradicts the assumption that c is minimal conflicting. thus, they can only overlap according to a chain structure: each cluster can overlap with at most one cluster on each side. without loss of generality, assume that each ci overlaps with ci+ <dig> for  <dig> ≤ i ≤ k.

if c is minimal conflicting, {c <dig> …, ck–1} is consistent. since the remaining cluster ck only overlaps ck– <dig>  there are only two possibilities that result in c being conflicting:

either there is not enough space for the deletion called by ck– <dig> within its breakpoint region in between the spans of ck– <dig> and ck: |br\{span ∪ span} | <delmin. in this case, {ck– <dig>  ck– <dig>  ck} is conflicting, which contradicts the assumption that c is minimal conflicting.

or, there is not enough space for the deletion called by ck within its breakpoint region to the right of the end of ck–1: |br| <delmin. in this case, {ck– <dig>  ck} is conflicting, which again contradicts the assumption that c is minimal conflicting.

since concordant mappings define segments in which no variation occurred, they also restrict the space for putative deletions and can thus be involved in conflicts. we can simply extend the above definition of consistency by considering a concordant mapping as a cluster with minimal deletion length zero. any follow-up definition can be extended analogously.

definition  <dig> let c be a set of maximal deletion clusters and m be a set of concordant mappings. then c and m are consistent if for all c ∈ c, the remaining positions incontain an interval of size delmin, otherwise they are conflicting.

in summary, we can identify all haploid conflicts by testing each pair and each triplet of overlapping clusters or mappings. these combinatorial results refine  <cit> , where  conflicts were defined only in terms of the length of the breakpoint regions and not of the deletions included in these regions and  only conflicts between pairs of clusters were considered.

diploid conflicts
a normal human genome is diploid, i.e., there are two copies of each chromosome. if a structural variation affects both copies identically, it is homozygous. but a variation might also affect only one of the copies, in which case it is heterozygous. if a set of clusters is conflicting as defined in the previous section, it is not necessarily conflicting when we consider heterozygous deletions. the conflicting clusters could be assigned to separate chromosome copies and thus be explained by independent, consistent deletions. however, in some cases, even such a separation is impossible. to check consistency for a given set of clusters on a diploid genome, a graph-theoretic approach can be used, that was introduced in  <cit> .

a hypergraph model
we define a hypergraph containing each cluster as a node and each minimal conflicting set  as a hyper-edge. a set of clusters is consistent if and only if the corresponding graph is two-colorable, i.e., we can assign one of two colors to each node such that each hyper-edge contains at least one node of each color. in this case, each cluster can be assigned to one of the two chromosome copies such that none of the two copies contains a complete conflicting set. analogously to the definition of consistency, we can include concordant mappings into this framework by adding a node for each mapping and further edges corresponding to the minimal conflicting sets containing this concordant mapping.

although this model is a natural and simple way to formally define the consistency problem, it does not provide a simple solution for it right away. in fact, the hypergraph two-colorability problem is known to be np-hard. the hardness result also holds in our case, where each minimal conflicting set and thus each hyper-edge is of size two or three  <cit> .

a heuristic approach to detect diploid conflicts
therefore, we relied on the following straightforward observation to detect at least a certain type of conflict in polynomial time: a set of three maximal clusters, or two clusters and one concordant mapping, which are pairwise in conflict with respect to one chromosome copy  cannot be conflict-free in a diploid genome. hence, to get an approximation of the amount of inconsistency in our data, and in particular a lower bound on the number of clusters involved in at least one conflict, we computed all triplets of three clusters, and those containing two clusters and one concordant mapping where the three elements are pairwise in conflict. we are aware that these are not all possible conflicts. actually, there are other configurations of overlapping clusters causing a diploid conflict which could theoretically exist. however, these configurations require a quite large number of clusters  overlapping in a very specific way, and we expect these combinations being less likely to occur in real data.

analyzing matched normal/tumor data sets
we now turn to our specific problem: given a deletion cluster detected in the normal data set and one or several deletion clusters detected in the contaminated tumor data set, involved in some conflict, how can we provide, if possible, a consistent explanation of the reads  obtained from this genome region?

in the following, we describe a framework designed to analyze the tumor-subset, whose goal is to provide a set of conflict-free deletion clusters while discarding as few reads as possible.

overlapping component of a normal deletion cluster
to analyze these regions, we define the following subset of the tumor data set, denoted from now as "tumor-subset": first, we collect all discordant mappings from the tumor data set overlapping a normal deletion . we then iteratively add further deletion mappings which overlap those already added to the subset, which we say are indirectly overlapping the initial normal deletion cluster. we call all mappings which  overlap the same normal deletion an overlapping component.

in all generality, the overlapping components are not necessarily disjoint and could thus not be treated independently. however, in the considered data set  the majority  of the overlapping components were disjoint, and we discarded the few remaining components.

as we will see in results section , the maximal valid clusters in the tumor data and tumor-subset are highly conflicting. when we consider only tumor-specific deletions found by pooling, i.e., a combined processing of the normal and tumor data, the ratio of conflicting clusters increases significantly.

a general framework to clear conflicts: assumptions
a central assumption in our approach is that the tumor data originated from a mixed sample containing tumor as well as normal cells, hence two sets of diploid chromosomes. this implies that the read pairs have actually been obtained from four chromosome copies. this assumption is natural for tumor cell sequencing  <cit> . it is well known that tumor genomes can be affected by ploidy abnormality, such as loss of heterozygosity   <cit>  or increased copy number of some chromosomes. such information can naturally be integrated in our model if known, but they were not available for the data we analyzed, which justified our choice to assume four copies of each chromosome. this proved to be sufficient, as we show later in the results section.

next, a deletion can be either heterozygous , or homozygous . in general, deciding if a deletion is heterozygous or homozygous from paired-end mappings is difficult. probabilistic methods have been designed  <cit> , requiring certain parameters, a-priori probabilities or other assumptions. to actually verify the nature of deletions, further experiments, like pcr-analyzes, need to be carried out  <cit> . here we assume that all deletions are heterozygous. our framework could be expanded appropriately by assuming that some deletions are located on all four chromosome copies in the mixed data set. preliminary investigations showed that this would only slightly increase the number of mappings which cannot be explained by our approach.

a general framework to clear conflicts: description
hence, the problem can be generally described as assigning the largest set of mapping from the tumor data set to one of the four chromosome copies in such a way that no conflict between deletion clusters occur. more precisely, for each overlapping component of a normal deletion cluster c and its overlapping mappings m = {m <dig>  …, mn} in the tumor data set, we solve the following optimization problem.

problem  <dig> . given a set m of mappings and a valid cluster c, find a partition of m into disjoint sets t <dig>  …,tl, nc, tc, , d such that:

• nc ∪ tc contains exactly the concordant mappings from m,

• is a valid cluster,

• t <dig> …, tlare valid clusters,

• {c′, t <dig>  …, tl} and tc are consistent w.r.t. two chromosome copies, and

• d is of minimal size, possibly empty.

in the description above, each tj denotes a cluster of mapping assigned to a tumor chromosome, nc and tc denote subsets of concordant mappings from m,  denotes a set of mappings from m added to cluster c, and d is the set of mappings which could not be assigned to any of the four chromosomes without creating a conflict.

the method we designed to solve this problem is summarized in algorithm  <dig> 

we start from the overlapping components of the tumor-subset, and for each component, we proceed according to the following steps.

1) any concordant mapping can be explained by assuming that the read pair was sequenced from the normal chromosome copy not carrying the deletion. since no other mapping can stem from this copy, this will not influence the assignment of the discordant mappings.

2) we say that a set of mappings supports the normal deletion if, together with the deletion cluster from the normal data set, they call for a single deletion. we assign a maximal supporting subset of the discordant mappings to the normal chromosome. note that, instead, we could also assign it to the respective tumor chromosome, but, in order to minimize the number of discarded mappings, it is better to leave as much space for further deletions on the tumor chromosomes as possible, as adding further mappings to a cluster refines the location and range of the deletion it indicates.  

3) the remaining mappings have to be assigned to the two tumor chromosome copies, thus defining tumor-specific deletion clusters lying close to the refined normal deletion or on the other chromosome copy. in some cases, not all mappings can be embedded into a consistent assignment. we compute a partitioning with a minimal number of discarded mappings.

the result is a refined normal deletion c′, a set of tumor-specific deletions  and, in some cases, a set of discarded mappings.

obviously, steps  <dig> and  <dig> are only stated for reasons of completeness and have not really to be carried out. step  <dig> could be computed efficiently using a geometric approach  <cit> . here, we relied on a simple exhaustive search. for step  <dig>  we implemented a branch-and-bound search, which finds an optimal solution in general but only reports the best solution found so far if the search takes more than ten minutes. experiments showed that this bounding has only a minor impact on the results.

finally, note that, instead of building on maximal valid clusters, one could also use other models or methods, or a pre-filtered or manually curated set of normal deletions, provided that they correspond to proper deletion clusters.

RESULTS
the genome science centre of the british columbia cancer agency generated and provided paired-end mapping data from different samples taken from one adenocarcinoma patient  <cit> . genome sequence data have been deposited at the european genome-phenome archive   <cit> , accession number .

we analyzed data from a blood sample, from now referred to as the normal data set, and from a skin metastasis sample, referred to as the tumor data set. the normal data set contained about 70-million concordant mappings  and about 100-thousand discordant mappings indicating a deletion in the donor. sequenced at higher coverage, the tumor sample contained about 160-million concordant  and about 310-thousand deletion mappings.

data
sequencing was done using the illumina genome analyzer ii, and mapping the paired-end reads onto the human reference genome hg <dig> was done with the software maq version  <dig> . <dig>  <cit> .

the lengths of the fragments obtained and processed in a sequencing procedure are usually not all the same but distributed around a certain value. the distribution of the mapping lengths was used to estimate cutoffs to differentiate between discordant mappings and concordant mappings. we filtered for a minimum quality value of  <dig>  and used the maq-preprocessor provided by the gasv software package  <cit>  to determine the minimum  and maximum length  for concordant mappings. using the default options and parameters, minlen was defined as the  <dig> %-quantile, and maxlen as the  <dig> %-quantile. the results are summarized in table  <dig> 

inconsistency of predicted deletions
for the different data sets under consideration, we computed all maximal clusters, as defined in methods section , for the given discordant mappings using gasv  <cit> .

the normal and the tumor data were processed separately as well as pooled. in the normal data set, we found  <dig>  deletion clusters. for the tumor and the pooled data set, the computation of the maximal clusters for chromosomes  <dig>   <dig>   <dig>   <dig>   <dig> and  <dig> did not finish within reasonable time due to a combinatorial explosion of the number of maximal clusters. on the remaining chromosomes, we found  <dig>   and  <dig>   deletion clusters, respectively. deletions found in the normal data set were assumed to be patient-specific. the pooling results contained  <dig>  clusters solely composed of mappings from the tumor data set, thus indicating a candidate tumor-specific deletion. we found that only  <dig>  of the deletion clusters in the whole normal data set were involved in conflicts. in the tumor data set and among the candidate tumor-specific deletions in the pooled data, we found a higher level of inconsistency, as can be seen in table  <dig>  note that the very large number of overlapping clusters in the disregarded chromosomes in the tumor and pooled data set comprise a high amount of conflicts, confirming this observation.

further analysis revealed that in the tumor-subset, i.e., regions with closely located patient-specific and tumor-specific deletion clusters, the level of inconsistency was especially high: more than 70% of all putatively tumor-specific deletions were conflicting. in general, filtering for large deletions had no significant effect on the ratio of conflicting clusters. this indicates that there are inherent conflicts, not just caused by false positives candidate deletions.

to confirm these observations, we repeated the analysis on the deletion clusters obtained by breakdancer  <cit> . the clusters reported by this tool are not necessarily valid according to our definition. thus, a single cluster can be self-conflicting, i.e., a minimal conflicting set of size one. in table  <dig>  we report the number of self-conflicting clusters, and of those clusters contained in conflicting subsets of valid clusters. again, we observe that there are very few conflicts in the normal data, a signal of inconsistency in the tumor data, and a higher ratio of conflicts in the tumor-subset. in general, the number of conflicts is lower compared to the analysis of the maximal valid clusters. this is because invalid clusters cannot pair with other clusters and thus increase the level of inconsistency by one. in contrast, at the same spot, several overlapping maximal valid clusters could constitute to larger conflicting sets, yielding a larger increase.

consistent explanation of the data
we applied the method described in algorithm  <dig> to the tumor data. as our approach does not rely on computing maximal clusters from these data , all  <dig> chromosomes could be processed. only  <dig> of the  <dig>  discordant mappings in the tumor-subset had to be discarded to remove all conflicts. from the remaining mappings , a large majority  supported and refined  <dig> normal deletions. the remaining  <dig> mappings defined  <dig> candidate tumor-specific deletions. as can be seen in figure  <dig>  the range of deletion length for these  <dig> deletion clusters lies in the order of a few hundred bases.

in figure  <dig>  we show a histogram of the sizes of the cluster  supporting these candidate tumor-specific deletions. we can notice a large number of small size clusters. it is however important to note that a tumor-specific deletion might be supported by further mappings which have been assigned to the normal deletion by our greedy approach. that is why we refrained from filtering the obtained tumor-specific clusters for a minimal size. this raises the interesting problem to refine and increase the support for potential tumor-specific deletions by re-assigning to the tumor chromosomes mappings assigned to the normal chromosome.

we also looked at the refined normal deletions, i.e., the clusters of mappings from both the normal and tumor data sets that agreed with a normal deletion cluster. figure  <dig> below shows how the deletion length and breakpoint region were modified by including possible mappings from the mixed data into the normal clusters. we can observe a significant refinement of the characteristics of normal deletions.

for  <dig>   normal deletions, we did not find any supporting mapping in the tumor data set. further investigations revealed that almost all of these were either very small deletions , or were defined by only two mappings in the normal data set, or overlapped segmental duplicated regions of a chromosome. this suggests that they might be false positives or that the region in the tumor data set was not sufficiently covered .

however, a closer look at the many normal deletions which were defined by only a small number of mappings from the normal data set showed that many of them are supported by mappings from the tumor data set. for example, figure  <dig> shows that a significant number of normal singletons  are well supported by mappings from the tumor data set . this indicates that many singletons might be true positive rather than false positive normal deletions.

to some extent, this can be a result of the roughly four times higher coverage of the tumor sample. another possible explanation for the observation of many singletons with a significantly high support by the tumor data might be a contaminated normal sample: if the normal sample contained only very few tumor cells, then, in the sequencing process, some tumor-specific deletions might have been sequenced with very low coverage, resulting in only a few or even single discordant read pairs calling these deletions. in contrast, the cancer sample contained many more tumor cells, yielding a high coverage and thus high support for the same deletions. recall that the tumor sample was actually taken from a skin metastasis. since cancer spreads via the lymphatic system or the bloodstream, this also supports the hypothesis. in all cases, this points to candidate tumor-specific deletions and specific cases that might require improved detection methods. so here again, the question of assigning the mappings from a contaminated  tumor sample between the normal and tumor chromosomes copies comes naturally to mind.

CONCLUSIONS
we analyzed two data sets obtained from different tissues of the same patient: a tumor and a normal sample. paired-end reads were mapped to a reference genome and discordant mappings were assembled to maximal valid clusters calling for deletions . these deletions were then analyzed in terms of consistency: can all mappings and deletions be explained by assuming that all read pairs were read from the mapped positions in a diploid set of chromosomes? we described a simple combinatorial model of deletion consistency that refines the previous work of  <cit> . we found that, in this model, the deletions predicted for the normal data are almost consistent. in contrast, the tumor data set showed a higher level of inconsistency. for those regions of the genome harboring a normal deletion, we found the highest rate of conflicting deletions among putatively tumor-specific deletions. we thus focused our study on this tumor-subset.

usually a tumor sample also contains normal cells. thus, instead of a diploid set of chromosomes, we expect four copies of each chromosome . based on this assumption, and our consistency model, we described the problem of consistent explanation of the data as an optimization problem and described a method to solve it. applying this method on our data, we were able to explain almost all the considered mapping data in a consistent way. only very few mappings could not be explained , most of the mappings supported inherent normal deletions , and  <dig> % were clustered to  <dig> tumor-specific deletions.

summarized, we revealed that deletion calls obtained by standard methods possess a high rate of inconsistency, and we present a model that, even though built on strict assumptions, such as diploidy of the genomes and each deletion cluster spanning exactly one deletion, allows for an almost consistent explanation of the data.

although these results look promising, they raise some questions. possible reasons for false positive deletion calls and thus inconsistencies are misleading pairs of reads, for instance caused by chimeric fragments in the sequencing procedure, or erroneous mappings, for instance caused by repeated regions in the genome. even though we filtered for unambiguous mappings of high quality, some wrong mappings were probably left. introducing read error-correction in the general process of mapping assignment is a natural extension. next, it is not unlikely to find polyploid chromosomes with three or more copies in tumor cells. furthermore, the sample could contain cells from different tumor lineages—also indicating a higher copy number. certainly, assuming further chromosome copies, all mappings could eventually be assigned consistently. however, including copy number information to refine the assumptions would be a first way to bring our model closer to reality. and, as already mentioned, our model does not include loss of heterozygosity  so far. regions which remain in conflict might also be putative loh sites.

another approach to explain more mappings is to relax the definition of consistency. in particular, the assumption that each cluster only spans exactly one deletion is mainly of technical origin and arguable. if we would allow any deletion being split into any number of smaller fragments, the problem complexity would increase. however, we believe the problem might be somewhat tractable for an intermediate model where at most k splits are allowed.

one issue which is not addressed by our study so far is the discrimination of heterozygous and homozygous deletions. a deletion affecting only one copy of a chromosome allows further deletions on the second copy. in contrast, a homozygous deletion residing on both copies, might also occur on both copies of the tumor chromosomes, leaving only limited space for further, tumor-specific deletions. this creates a higher potential for conflicts. hence, any mis-classification as homozygous could falsely increase the number of discarded mappings dramatically. we currently investigate how homozygosity can be securely included into our method. a possible approach would be to first consider normal deletions as heterozygous, as we do in the present work, then to analyze each configuration produced by our method  and to investigate the homozygous/heterozygous issue in a post-processing phase. such a post-processing could also address the issue of re-assigning mappings from the normal chromosomes to the tumor chromosomes that we raised in discussing our results. more generally, our approach is more intended to provide a first level of conflict clearing and to highlight potential region where tumor-specific deletions could have occurred close to a normal deletion. post-processing the results of our method to refine them, without re-creating conflicts, would be a natural extension.

we also plan to extend our framework to cover overlapping normal deletions and other more complicated cases which we discarded so far. although these only constitute a minority , their complex structure might comprise a lot of conflicts and is thus worth to study.

next, the combinatorics of mappings, clusters and conflicts is still very poorly understood and require further investigation, in particular to find properties that will lead to efficient algorithms to detect conflicts. also, several combinatorial optimization problems we considered in the present work still await for efficient algorithms .

in general, paired-end mapping approaches can be used to analyze different types structural variations. deletions certainly belong to the simplest classes, well suitable as a starting point to study structural variation, but far from representing the whole spectrum of cancer development. as a next step, we intend to include further types of variations to approach a more complete model, which will however require combinatorial developments.

list of abbreviations
loh: loss of heterozygosity.

competing interests
the authors declare that they have no competing interests.

authors' contributions
r.w. and c.c. designed the method. r.w. implemented the method. r.w. and c.c. wrote the manuscript. all authors read and approved the final manuscript.

appendix a: statements of algorithmic and combinatorial optimization problems
problem 1: detection of minimal conflicting sets.

given: set of mappings m, set of valid clusters c.

task: find all minimal conflicting subsets of m ∪ c w.r.t. a diploid genome.

remark: general, but exponential time, techniques were described in  <cit> , for the different problem of reconstructing ancestral genomes and gene clusters, but they can be applied for deletion clusters. as far as we know, ad-hoc approaches for the specific case of deletion clusters have not been investigated.

problem 2: test for consistency of clusters in the haploid case allowing for splitting each given deletion into k smaller deletions.

given: set of clusters c.

task: find  a set d of positions such that for each cluster c ∈ c:

• delmin ≤ |d ∩ br| ≤ delmax, and

• d ∩ br is a set of at most k intervals.

remark: the set d represents the genomic positions deleted in the donor genome. strictly speaking, positions onto which any read maps have to be excluded from d. to simplify the verification one can simply discard all these position in general and adjust delmin, delmax and br for each cluster c correspondingly.

problem 3: finding a maximum set of concordant mappings for a deletion cluster

given: set of discordant mappings md, valid deletion cluster c.

task: find a maximum subset  of md such that  is a valid cluster.

remark: the geometric framework described in  <cit>  allows to solve this problem.

problem 4: consistent assignment to four chromosome copies.

given: set of mappings m, valid cluster c.

task: find a partition of m into disjoint sets , d such that:

•  are valid clusters,

•  is consistent w.r.t. one chromosome copy, and

•  is consistent w.r.t. one chromosome copy, and

• d is of minimal size, possibly empty.

supplementary material
additional file 1
implementation of the method to find a consistent explanation for a  tumor-subset under consideration of associated normal data . this zip-archive contains the main algorithm and a converter as java archives , and a text file with detailed instructions.

click here for file

 additional file 2
illustration of deletion clusters inferred by algorithm  <dig>  gasv and breakdancer. note that many deletions in close proximity may appear as a single dot, and the size of a dot is in general larger than the respective deletion. for some data sets, the computation of all maximal clusters was infeasible. this zip-archive contains a pdf file for each chromosome.

click here for file

 acknowledgments
the authors thank the genome sciences centre and the british columbia cancer agency, especially yaron butterfield, for providing the data and discussions. this research was partially supported by an nserc discovery grant to c.c.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2011: proceedings of the ninth annual research in computational molecular biology  satellite workshop on comparative genomics. the full contents of the supplement are available online at http://www.biomedcentral.com/1471-2105/12?issue=s <dig> 
