BACKGROUND
high-throughput experimental methods, including dna and protein microarrays and other omics techniques, have become ubiquitous, indispensable tools in biology and biomedicine. the number of high-throughput technologies is constantly increasing. they provide the power to measure thousands of features of a biological system in a single experiment, and they have the potential to revolutionize our understanding of biology and medicine. however, the high expectations for omics methods have fallen short of realization, due to the challenges the data present for statistical modeling. thus, the wealth of data produced is difficult to translate into concrete biological knowledge, new drugs, and clinical practices  <cit> . a recurring problem is that few experimental samples are generated relative to the number of model parameters  <cit> . this leads to imprecise parameter and performance estimates, and models prone to overfitting . in fact, it is difficult to even obtain a trustworthy measure of how imprecise performance estimates are with standard techniques like confidence intervals based on holdout estimates in cross-validation or bootstrapping  <cit> . another problem is that classical statistical models are too restrictive to account for the complexities of integrating omics data with previously acquired data. this has forced us to resort to ad hoc approaches for solving very specific problems  <cit> . statistical approaches that use prior distributions over the model parameters are known as bayesian methods  <cit> . the prior distributions summarize one's belief about the parameters before seeing the data. after performing an experiment, the parameters are updated to a posterior distribution according to bayes' theorem:  

the posterior distribution reflects the contribution of both our prior beliefs and the experimental data .

bayesian methods have a number of advantages that allow us to address the problems inherent in omics data analysis. firstly, they afford formal and coherent incorporation of prior information and integration of data sources. apart from the philosophical appeal of being able to use all available information when analyzing a problem, this allows us to tackle the curse of dimensionality in two ways:  prior information can be used to reduce model dimensionality with bayesian variable selection and regularization, and  data from different research studies can be coherently combined to increase the number of available observations. bayesian modeling thus allows us to "borrow information" across studies  <cit> . secondly, bayesian methods correctly summarize the model's predictive distribution  <cit> . thus, we can obtain reliable bayesian confidence intervals  of the estimates of model performance in small-sample problems; this avoids the problem of having to rely on imprecise confidence intervals based on holdout estimates from cross-validation or bootstrapping  <cit> . thirdly, bayesian methods may be applied to problems with structures too complex for classical statistical methods  <cit> . bayesian models can be made increasingly elaborate to accommodate for the integration of prior information and omics data from multiple data sources.

however, the advantages of bayesian modeling come at a price. we have to specify prior distributions for all parameters in a model. for large models, like those for analyzing omics data, it is unrealistic to expect practitioners to manually collect prior information for all individual parameters. in practice, this limits the powerful advantage of using available prior knowledge. in addition, bayesian methods require numerical handling of analytically intractable integrals. this is typically performed with markov chain monte carlo  methods, which are computationally very expensive and therefore restricts the size of the modeling problems that we can address.

recent developments in distributed information and computational resources have given rise to the notion of escience, i.e. computationally intensive science that is carried out in highly distributed network environments  <cit> . advances in escience can be roughly grouped into:  developments in semantic technologies  and methods for retrieving standardized data ; and  construction of high-performance computing  facilities and development of middleware for using the hpcs. these two aspects of escience equip us for efficient use of the bayesian approach to omics data analysis. standardized data and web services allow us to use machines to harvest the internet for information to use in prior distribution specifications, and hpc resources provide the computational power required to fit large bayesian models to high-throughput data . in this paper we demonstrate how escience permit us to leverage on the advantages of bayesian analysis when modeling omics data. we show by two examples that the approach can improve predictive performance, and that it permits a deeper analysis of the data by accommodating more complex model structures.

RESULTS
transcriptomics data: predicting distant metastasis development in breast cancer patients
microarray gene expression profiling has shown promise for supporting the prognostication of breast cancer patients based on the expression pattern of specific gene sets . a number of studies have reported different signatures  <cit> . however, concerns have been raised against the prognostic capacity of these signatures when applied to new data  <cit> . these concerns include the following:

 <dig>  signatures from different studies share almost no genes, which indicates that the signatures depend to a large degree on the datasets rather than being prognostic for breast cancer  <cit> . in fact, michiels et al.  <cit>  showed that, even within a study, the subset of patients used for signature derivation strongly influenced which genes were selected.

 <dig>  the reported performance estimates of the signatures have been questioned  <cit> ; this suggests that worse performance is likely to be obtained when applied to new data. also, the confidence intervals of the performance estimates  have been disputed  <cit> . this makes it difficult to assess the level of accuracy in the reported results.

in fact, both these concerns arise due to the small sample sizes typically used in microarray studies relative to the number of profiled genes . we employed the escience-bayes strategy described in fig.  <dig> to address this problem. in accordance with the practice in most previous articles, we modeled the development of distant metastases within five years. to increase the sample size, we used the gene expression omnibus  web service  <cit>  to collect data from five previously published breast cancer studies  <cit> . to reduce the model dimensionality we employed a bayesian variable selection procedure, giving higher prior probabilities of including a variable  the higher our prior belief that the gene affects development of distant metastases. the prior belief for each gene to affect development of distant metastases was based on information retrieved by connecting three web services: netpath http://www.netpath.org/, dictservice http://services.aonaware.com/ and entrez utilities http://eutils.ncbi.nlm.nih.gov/entrez/eutils/soap/v <dig> /doc/esoap_help.html. netpath is a curated resource of genes reported to be transcriptionally regulated by cancer-signalling pathways. all genes on the hg-u133a array listed in netpath were used to text mine pubmed for all free fulltext articles where the gene names were mentioned in combination with breast cancer. to reduce the number of spurious hits, we discarded genes with names that represent english words by using the dictionary definition web service dictservice. this gave us a list of integers, associated with the genes on the hg-u133a array representing the number of times a cancer-pathway regulated gene is mentioned together with breast cancer in the literature . these informative prior distributions were thus based on the assumption that the probability of a gene being related to breast cancer was reflected in the number of times a gene reported in netpath was mentioned in combination with breast cancer in pubmed articles. a work flow of the procedure is shown in fig.  <dig> . the five datasets and the prior distributions were incorporated in a multilevel bayesian probit model . to assess the discriminative power of model i, we fitted it five times; each time using three of the datasets as training sets and the remaining two datasets as independent testsets . it may be noted that model i achieved high prediction accuracy on independent test data produced in different research groups. to the best of our knowledge, this is the first time that a modeling approach has demonstrated consistently good predictive ability across multiple independent testsets for predicting breast cancer metastasis development from gene expression data.

to validate the value of using prior information, we tested model i with non-informative prior distributions  and compared the results to those obtained with informative priors . this clearly showed that the use of relevant prior information significantly improves the classification accuracy of model i . the dramatic improvement was due to the fact that the prior information focused the model to include relevant genes in the signature, thus reducing the risk of detecting spurious relationships and overfitting the model. analogously, the use of prior information enforced consistency in signature selection across disparate datasets:  <dig> % of the genes were selected all five times model i with informative priors was fit, which may be compared to 1% when noninformative priors were used, and 0% among the original signatures. the median of the pairwise overlap between two different model fits was  <dig> %, compared to 3% among the original signatures and 12% reported in chuang et al.  <cit> .

we also assessed the advantage of using multiple datasets by using only single datasets for training model i ; results are shown in fig. 3a , additional file  <dig>  fig. s <dig>  and table s <dig>  a significant improvement was found when multiple datasets were used for model training compared to when only a single dataset was used . in a final comparison, we modified model i to use only the expression data that corresponded to the respective gene signatures derived in wang et al.  <cit> , miller et al.  <cit> , sotiriou et al.  <cit> , and pawitan et al.  <cit> , together with noninformative priors . again, we found that model i  performed significantly better than when the signatures from wang et al.  <cit> , miller et al.  <cit> , sotiriou et al.  <cit> , and pawitan et al.  <cit>  were used. results are displayed in additional file  <dig>  fig. s <dig> and table s <dig>  to derive a "final" gene signature, we fitted model i using all five datasets and informative priors. some genes were included in this signature that a priori had been regarded as non-relevant to breast cancer; these genes were strongly supported by the data and may be of interest for future experimental investigations. the genes included in the signature are shown in additional file  <dig>  table s <dig>  stratified into genes that were relevant a priori as well as a posteriori, and genes that were regarded non-relevant a priori but relevant a posteriori.

in order to further demonstrate the escience-bayes approach, we used it to create a model that predicted the probability of distant metastasis development as a function of time. this model would provide finer granularity in the prognosis predictions of future patients compared to the current practice of stratifying patients into only two classes . patient stratification into two classes discards information by discretizing a continuous variable  into an arbitrarily defined binary variable . because not all patients had developed distant metastases before the last follow-up in the studies, this required a consideration of the censoring of the data . we created a multilevel bayesian accelerated failure time   model with the expression data as explanatory variables and the informative prior distributions . analogously to the assessment of models i, we assessed model ii by fitting it five times, each time using three of the datasets as a training set and the remaining two datasets as test sets to assess the performance of model ii. the results implied that we indeed could provide more fine-grained prognoses predictions, compared to the binary classification afforded with model i .

in this example, we provided evidence for that adopting the escience-bayes approach can significantly improve consistency in gene signature selection and in the predictive performance of the constructed models. in contrast to previous studies, our results were presented together with bayesian confidence intervals, which permit assessment of the level of accuracy in the results  <cit> .

proteomics data: analyzing pdz domain-peptide interactions
pdz domains mediate protein-protein interactions and have been extensively studied over the last fifteen years. recently, a large-scale pdz domain-peptide interaction dataset was published in stiffler et al.  <cit>  and follow-up studies were later described in chen et al.  <cit> . the combined data from these two publications comprise  <dig> observations of interactions between peptides and pdz domains from mouse, c. elegans, and d. melanogaster. we reanalyzed these data using the escience-bayes approach. to this end, we modeled the pdz domain-peptide interactions using a multilevel bayesian probit model with physicochemical properties of the pdz domains and peptides as explanatory variables and a binary variable as response, indicating whether or not a pdz domain and a peptide bound to each other . we derived informative prior distributions from 3d structural information using the sequence annotated by structure  <cit>  , a tool for annotating a protein sequence with structural information based on all solved 3d structures of the proteins in the protein data bank  <cit>  . using the sas web service  <cit>   we estiamted the number of contacts made between residues in each pdz domain and in the peptide ligands. similarly to the breast cancer demonstration, we used the informative prior distributions to "guide" the variable selection in order to reduce the dimensionality of the model based on the assumption that the importance of a pdz domain residue for the interaction is reflected in the number of contacts it makes with the ligand . fig.  <dig> shows the work flow for the prior elicitation and modeling. fig. 5a summarizes the predictive performance of model iii in a receiver operating characteristics  curve based on a 10-fold cross-validation, and shows a comparison with the predictive performance when noninformative prior distributions were used . we observed a nonsignificant  improvement in the predictive performance with informative compared to noninformative priors .

in this example, the main advantage of adopting an escience-bayes approach was the increase in information that we obtained. for example, model iii enabled an analysis of the differences between pdz domains. by allowing the regression coefficients to vary between pdz domains , we found differences in the estimated nonzero coefficients for different pdz domains . this suggests that there may be variations among the pdz domains in the networks of residues that are important for interacting with ligands  <cit> . interestingly, some of these residues are located far away from the binding site. although the tertiary structures of pdz domains are very similar, their primary structures vary substantially  <cit> . this may cause differences in the intramolecular interactions within different pdz domains. networks of such intramolecular interaction have previously been reported for pdz domains  <cit> . the results from studies by gianni et al.  <cit>  and chi et al.  <cit>  support the notion that the networks may differ among pdz domains; for example, a network present in a pdz domain from mouse tyrosine phosphatase bl was not found in the human psd- <dig> pdz <dig>  the escience-bayes approach allowed analysis of these differences across a large set of pdz domains.

model iii further indicated that the pdz domains from c. elegans and d. melanogaster may form two clusters . given the small datasets from c. elegans and d. melanogaster, which contain only seven pdz domains each , the clustering might be a sampling or an experimental artifact. nevertheless, it is tantalizing to hypothesize that there are general trends in the intramolecular interaction networks in pdz domains that differ between species. these observed differences are further illustrated in fig 5c, where five residues, estimated to be allosterically linked to residues in the binding peptide, are shown in one representative pdz domain from mouse, c. elegans, and d. melanogaster. as shown in the figure, different amino acids, with different positions and physicochemical properties, are estimated to be allosterically coupled to the binding peptide in the three different pdz domains.

this example shows that an escience-bayes approach to modeling omics data can straightforwardly accommodate grouping structures in the data. this enables the analysis to reveal tentative differences among pdz domains that allowed the inference of putative allosteric networks.

discussion and 
CONCLUSIONS
in a recent nature horizons article  <cit> , cambridge chemistry professor peter murray-rust sketched an escience world where the answer to any question is at the fingertips of every person; the complexity of the question and the size of the data to analyze do not matter. computers perform all trivial and time-consuming tasks, like searching through millions of research articles or performing massive algorithmic calculations. in this proof-of-principle paper we have shown that some aspects of this vision in fact have been realized: machines can via web services be used to retrieve background information that together with multiple data sources  can be coherently exploited in a bayesian framework to create complex models by employment of hpc resources, thus allowing us to provide answers to biologically relevant questions. this modus operandi contrasts to non-bayesian approaches for merging multiple omics datasets and for integrating omics data with prior information, which to a large degree have relied on ad hoc methods for solving specific problems . bayesian statistics provide the flexibility to tailor statistical models to complex data structures, and can simultaneously accommodate prior information. the rather complex models described here would be diffcult to fit using classical statistics. we demonstrated the value of the bayesian approach with the quite drastic improvements in prediction accuracy in the breast cancer example, and the enhanced depth of the data analysis in the pdz domain-peptide interaction example. however, presently the escience-bayes approach is not easy to apply because it requires manual procurement of suitable information resources and web services, and it requires programming skills to implement the web service clients, parse the output, and connect them together. moreover, the escience-bayes approach requires implementation of mcmc-algorithms to fit bayesian models and knowledge of middleware and non-user-friendly interfaces for deployment of the mcmc-algorithms on hpc facilities. however, the most crucial difficulty is the prior elicitation process. since the prior distributions are central in the bayesian paradigm, a key aspect of applying the esciencebayes approach is to summarize retrieved prior knowledge as distributions. in both the breast cancer and pdz domain demonstrations we used prior information quantified as integers to represent our a priori belief of the independent variables relevance for modeling the response. although care was taken to design the prior distributions in a sensible way, the prior specifications reflects a somewhat arbitrary step in the work flow, and there is presently no clear way to standardize prior elicitation and make the process transparent, exchangeable, and completely general. related to this difficulty, another problem is that useful data and prior information in today's scientific practice are lost in objects that are inaccessible to computers, such as figures, tables, and summary statistics.

thus, there is a distinct need for a standardized system of technologies and tools for working efficiently with computer representations of biological entities, data, information, probabilities, and statistical models - both locally and on the network. great efforts are indeed being made in this direction. these include the increasing degrees of semantically annotated data that are deposited in public repositories in machine readable formats  <cit> , new web service protocols that allow for service discovery  <cit> , semantic web technologies to adhere information with probability distributions that can be directly used as priors in bayesian statistical models  <cit> , and high-level languages for specifying fast implementations of mcmc algorithms  <cit> . moreover, graphical workbenches, like bioclipse  <cit> , aim to handle all these tasks from a single point of entry. when these technologies and software have matured, it will be straightforward to adopt an escience-bayes approach to virtually any biological or clinical question. it would then be easy to obtain prior information and data by performing semantic queries, for example, "give me all breast cancer susceptibility genes and the probability distributions describing their association with increasing risk of distant metastasis development, together with all breast cancer affymetrix hg-u133a datasets where distant metastasis development was the clinical end-point". the retrieved priors and data could then be used directly in a bayesian statistical model specified in a high-level language and converted to an algorithmic fitting process deployed on an hpc facility. finally, the results could be summarized graphically to increase the likelihood of making correct clinical prognostic and treatment decisions.

