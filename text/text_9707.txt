BACKGROUND
evolutionary algorithms  have been widely used for data mining tasks in
bioinformatics and computational biology  <cit> . they are random search methods inspired by natural mechanisms existing in
the biological world  <cit> . eas originally comprised four types of paradigms, namely, genetic algorithms
, genetic programming , evolution strategies , and evolutionary programming
, with gas being the most popular one. data analysis tools traditionally used in
bioinformatics were mainly based on statistical techniques, such as regression and
estimation, and eas played significant roles in handling large biological data sets in a
robust and computationally efficient manner  <cit> .

currently, evolutionary computing techniques mostly comprise conventional eas , swarm intelligence algorithms, artificial immune systems, differential
evolution, as the main representative classes of evolutionary computing approaches <cit> . swarm intelligence is a class of evolutionary computing techniques
simulating natural systems composed of many individuals that coordinate one another
using decentralized control and self-organization. two most influential and classical
examples of swarm intelligence approaches are particle swarm optimization  and ant
colony optimization  algorithms, which have been widely used in many different
fields  <cit> . particularly, pso algorithms have shown their effectiveness in data mining
tasks in bioinformatics due to their performance in solving difficult optimisation tasks  <cit> .

biochemical modelling can be considered a generic data-driven regression problem on the
given experimental data. the goal of biochemical modeling is to build the mathematical
formulations that quantitatively describe the dynamical behaviour of biochemical
processes. for example, metabolic reactions are formulated as rate laws and described as
a system of differential equations, the kinetic parameters of which are identified from
a set of experimental data. finding the solution of the parameter estimation problem,
thus, plays a key role in building a dynamic model for a biochemical process, which, in
turn, can help understand the functionality of the signalling pathways at the system
level  <cit> .

since solving the inverse problem in biochemical process modelling involves a task of
nonlinear programming, many numerical optimization methods have been used to determine
the parameters of biochemical models. these methods can be generally classified into two
categories, namely, local optimization methods and global optimization methods  <cit> . the widely used local optimization tools for inverse problems are those
based on gradient descent methods, the most popular being the newton method  <cit> . this type of approaches, however, cannot be applied to non-smooth problems,
since the objective functions of the problems are discontinuous or have discontinuous
derivatives. direct search methods, such as the hooke-jeeves method, the needler-mead
simplex algorithm and the downhill simplex algorithm, are also a kind of local
optimization techniques that could be used to find a local minimum without the
information from derivatives  <cit> . normally, most local optimization approaches are used as single shooting
methods. for each of them, the path of its optimization process leading to a final
solution is determined by the initial conditions for the state variables. therefore, the
algorithm will lead to a wrong minimum, particularly if the initial conditions depend on
model parameters. to overcome this shortcoming, one can adopt multiple shooting methods
in which the time interval is partitioned and new initial conditions are used at the
start of each time interval part  <cit> . the methods can offer the possibility to circumvent local optima by
enlarging the parameter space during the optimization process.

the aforementioned local search methods are generally less efficient for the inverse
problems of biochemical models, which are multimodal and high-dimensional. in order to
solve these hard inverse problems efficiently, one can turn to global optimization
methods, most of which incorporate stochastic search strategies to prevent the search
process from being stuck into the local optimal or suboptimal solutions. the
branch-and-bound approach is a global optimization method that converts the inverse
problem into a convex optimization problem so that a global optimal solution can be
obtained  <cit> . this method requires a finite search space that can be divided into smaller
subspaces. a remarkable disadvantage is that it is applicable only if the lower and
upper bounds of the objective function can be computed. simulated annealing  can be
effectively used for parameter estimation from time-course biochemical data as shown in  <cit> . however, it has a slow convergence speed and high computational cost, and is
not easy to be parallelized. genetic algorithms  represent a widely used global
search technique that could be employed to predict the parameters of dynamic models  <cit> . nevertheless, gas are always complained of slow convergence speed and high
computation cost. the evolutionary strategy  approach showed its ability to
successfully solve inverse problems in a performance comparison made by moles et al.  <cit>  among a number of global optimization techniques on biochemical system
identification problems. in contrast to sa, evolutionary algorithms, including es and
gas, can be implemented as self-tuning methods and can be parallelizable, with the
stochastic ranking evolutionary strategy  method being a very successful example  <cit> . scatter search  is known as a population-based random search approach
that was proposed to identify the appropriate parameters for nonlinear dynamic
biological systems  <cit> . as an evolutionary algorithm method, the ss method, as well as its hybrid
with a local search step after the recombination operation, showed to be efficient in
solving inverse problems. particle swarm optimization , also a population-based
optimization technique from swarm intelligence and evolutionary computation area, has
demonstrated its better performance than gas in solving inverse problems  <cit> . hybrids of pso with other methods have also shown their effectiveness in
modelling biochemical dynamic systems  <cit> . however, pso shows to be sensitive to the neighbourhood topology of the
swarm, as commented in  <cit> .

other methods for parameter estimation include the newton-flow analysis  <cit> , the alternating regression technique  <cit> , decoupling approaches  <cit> , the collocation method  <cit> , the decomposing method  <cit> . these approximation techniques, when incorporated into an optimization
algorithm, can help reduce the number of objective function evaluations, which are very
computationally expensive. additionally, radial basis function neural networks  <cit>  and a quantitative inference method  <cit>  have also been employed to solve inverse problems in biochemical process
modelling.

in all of the above cases, the optimization approach is used to minimize to the residual
error of an inferred model against experimental data. smaller error means that the model
describes the dynamic behaviour of the biochemical system better and has more
justification to be accepted as a valid mathematical representation of the system.
theoretically, the prediction error diminishes with the accuracy of the model
increasing. this study focuses on developing an efficient optimization method for
parameter estimation of a given dynamic biochemical model. however, since parameter
estimation problems of complex dynamic systems  are high-dimensional, multimodal and more challenging to solve,
but which allow to depict more complex biochemical processes, our goal in this study is
to develop an efficient global optimization method for solving such inverse problems of
complex biochemical dynamic systems.

after extensive and in-depth study, we selected the pso algorithm as a candidate to be
modified in order to achieve our goal of solving complex inverse problems. the reason
why pso attracted us is that pso has many advantages, such as faster convergence speed,
lower computational need, as well as being easily parallelizable and having fewer
parameters to adjust. however, pso has the following shortcomings. first of all, it was
theoretically proven that the pso is not a global convergent algorithm, even not a local
convergent one, against the convergence criteria given in  <cit> . practically, the algorithm is more prone to be trapped into local optimal or
suboptimal points for a high-dimensional problem, due to the weakening of its global
search ability during the mid and later stages of the search process. next, pso is
widely known to be sensitive to its search parameters including upper limits of the
velocity, and even to the "swarm topology", so that users may feel awkward when
selecting the parameters and the topologies when using the algorithm  <cit> . finally, the performance of pso appears to be very sensitive to the setting
of upper and lower bounds of the search scope  <cit> . if the global optimal solution is located near the boundary of the search
scope, the algorithm may have little chance to catch it. we have found that these
shortcomings are mainly attributed to the velocity update equation, which is the essence
of the pso algorithm, and where it seems to be much room for improvement so as to boost
the global search ability of the pso.

in this study, inspired by the free electron model in metal conductors placed in an
external electric field  <cit> , we propose to use a variant of the pso algorithm, called the random drift
particle swarm optimization , in order to achieve our goal of effectively
estimating the parameters of complex biochemical dynamical systems. the motivation of
the rdpso algorithm is to improve the search ability of the pso algorithm by
fundamentally modifying the update equation of the particle's velocity, instead of by
revising the algorithm based on the original equation so as to probably increase the
complexity of the algorithmic implementation as well as its computational cost. it is
different from the drift particle swarm optimization  proposed by us in  <cit>  in that it can make a better balance between the global search and the local
search of the particle swarm.

the original and basic rdpso version was recently introduced by us in  <cit> , which was used for solving other problems in  <cit> . a novel variant of rdpso algorithm is being proposed in this work to solve
the parameter identification problem for two biochemical systems. the novel variant
proposed here is different from the original one in that it employs an exponential
distribution for sampling the velocity of the particles, whilst the original one used
the gaussian distribution.

the novel rdpso variant is used for estimating the parameters of two benchmark models,
one of which describes the thermal isomerization of α-pinene with  <dig> parameters  <cit> , the other of which has a three-step pathway with  <dig> parameters  <cit> . the results of rdpso and some other well-known global optimization
algorithms are then compared and discussed. it should be noted that although this paper
is focused on the parameter estimation for biochemical modelling, just as pso and other
eas, the proposed rdpso variant can be employed as a general-purpose tool for
optimization problems in data miming tasks, such as clustering, classification,
regression, and so forth, which widely exist in bioinformatics and computational biology  <cit> .

methods
problem statement
the inverse problem of a nonlinear dynamic system involves finding proper parameters
so as to minimize the cost function of the model with respect to an experimental data
set, with some given differential equality constraints as well as other algebraic
constraints. such a data-driven regression problem can be approached with statistical
techniques, using the given experimental data and the proposed models with unknown
parameters. as stated by moles et al.  <cit> , the problem can be mathematically formulated as a nonlinear programming
problem  with differential-algebraic constraints, whose goal is to find
θ so as to minimize

  j= ∫ 0tf-y)tw-y)dt 

subject to

  fdxdt,x,y,θ,v,t= <dig> 

  x=x <dig> 

  h= <dig> 

  g≤ <dig> 

  θl≤θ≤θu 

where j is the cost function of the model, θ is a vector of
model parameters to be estimated, ymsd is the experimental measure of a subset of the output
state variables, y is the prediction of those outputs by the model, x
is the differential state variables and v is a vector of other  parameters that are not to be estimated. in equation ,
w is the weighting  matrix, and the equation
can be discretized into a weighted least-square estimator. in equation , f
is the set of differential and algebraic equality constraints describing the
system dynamics . equation  gives the initial
value of x. in equations  and , h and g are equality and
inequality path and point constraints on system performance. in addition, θ
is subject to upper and lower bounds, which are described by inequality
constraints .

the above defined inverse problem is generally a multimodal  optimization
problem with multiple local optima due to the nonlinearity and constraints of the
system dynamics. even though many local and global optimization methods have been
proposed to solve the problem as mentioned in introduction, it is still challenging
and very necessary to develop efficient optimization algorithms to deal with the
parameter estimation problems, especially those for the dynamic systems with many
parameters and many equations. therefore this study focuses on the optimization
approach for the inverse problem using the proposed variant of random drift particle
swarm optimization  and other global optimization methods.

particle swarm optimization
the original pso algorithm was introduced by kennedy and eberhart in  <cit> . the algorithm was inspired by the observed social behavior of bird flocks
or fish schooling, and it roots its methodology both in evolutionary computing and
artificial life. it shares many similarities with eas, in that both the pso and the
eas are initialized randomly with a population of candidate solutions and then update
the population iteratively, in order to approximate the global optimal solution to
the given problem. however, unlike eas, pso has no evolution operators such as
crossover and mutation, but perform optimization tasks by updating the particles'
position  according to a set of discrete differential equations.
it was shown that the pso algorithm has comparable and even better performance than
gas  <cit> 

in the pso with m particles, each particle i
, representing a potential solution of the given
problem in a d-dimensional space, has three vectors at the kth
iteration, namely, the current position xik=, the velocity vik= and its personal best  position
pik=, which is defined as the position with the best
objective function value found by the particle since initialization. a
vectorgk=, called the global best  position, is
used to record the position with the best objective function value found by the all
the particles in the particle swarm since initialization. with the above
specification, the update equations for each particle's velocity and current position
are given by:

  vi,jk+1=w⋅vi,jk+c1ri,jk+c2ri,jk 

  xi,jk+1=xi,jk+vi,jk+ <dig> 

fori= <dig> ,⋯m;j= <dig> ⋯,d, where c <dig> and c <dig> are known as acceleration coefficients and w
is called the inertia weight, which can be adjusted to balance the exploration
and exploitation ability of each particle  <cit> . without loss of generality, we assume that the pso is used to solve the
following minimization problem:

  minimizef,s.t. x∈s⊆rd  

where fis an objective function  and s
denotes the feasible space. consequently, pik can be updated by:

  pik={xik    iff<fpik−1iff≥f 

and gk can be found by:

  gk=pgk, where g=arg min1≤i≤m 

in equation , ri,jk and ri,jk are the sequences of two different random numbers with
uniform distribution on the interval , namely, ri,jk,ri,jk~u. in order to prevent the particle from flying away out
of the search scope, vi,jk is restricted on the interval , where vmax is also a user-specified algorithmic parameter.

many researchers have proposed different variants of pso in order to improve the
search performance of the algorithm and proved this through empirical simulation  <cit> 

the proposed random drift particle swarm optimization
in  <cit> , it was demonstrated that if the acceleration coefficients are properly
valued, each particle converges to its local attractor,pik=, so that the convergence of the whole particle swarm
can be achieved. each coordinate of pik is given by:

  pi,jk=c1ri,jkpi,jk+c2ri,jkgjkc1ri,jk+c2ri,jk,1≤j≤d 

which can be restated as

  pi,jk=ϕi,jkpi,jk+,1≤j≤d 

where

  ϕi,jk=c1ri,jkc1ri,jk+c2ri,jk 

in the pso algorithm, c <dig> and c <dig> are set to be equal, and thus
ϕi,jk is a random number with uniform distribution on the
interval , i.e. ϕi,jk~u.

during the search process of the pso, as particles' current position are converging
to their own local attractor, their current positions, pbest positions,
local attractors and the gbest positions are all converging to one single
point. the directional movement of each particle i towards
pik resembles the drift motion of an electron in metal
conductors placed in an external electric field. according to the free electron model  <cit> , the electron has not only drift motion caused by the external electric
field, but also a thermal motion, which appears to be a random movement. the
superposition of the drift thermal motions makes the electron careen towards the
location of the minimum potential energy. thus, if the position of an electron in the
metal is regarded as a candidate solution and the potential energy function as the
objective function to be minimized, the movement of the electron resembles the
process finding the minimum solution of the minimization problem.

the above facts can lead to a novel variant of pso if the particle in pso is assumed
to behave like an electron moving in a metal conductor in an external electric field.
more specifically, it can be assumed that at the kth iteration, each
particle i has drift motion towards pik as well as a thermal motion, with their velocities in
each dimension j denoted as v1i,jk+ <dig> and v2i,jk+ <dig>  respectively. as a result, the velocity of the
particle is given by vi,jk+1=v1i,jk+1+v2i,jk+ <dig>  in the drift particle swarm optimization proposed in  <cit> , we assume that v1i,jk+ <dig> follows a maxwell distribution, say a gaussian
probability distribution, and the v2i,jk+ <dig> is given by the social part plus the cognitive part in
equation . this velocity update equation appears to add some effectiveness to the
search performance of the particle swarm but has some shortcomings. firstly, the
gaussian distribution has a thin tail so that it has less opportunity to generate
outliers. as a result, the thermal motion of the particle has less randomness and
cannot significantly improve the particle's global search ability. secondly, although
the update equation of v2i,jk+1:

  v2i,jk+1=c1ri,jk+c2ri,jk 

can guarantee the particle to converge towards its local attractor, the two random
scaling coefficients add randomness to its motion, which means that the particle's
position is sampled at uniformly random positions within the hyper-rectangle around
the gbest position and its personal best position. it is not able to enhance
the particle's global search ability because of the finite scope of the
hyper-rectangle, but it may weaken its local search ability, which is the
responsibility of the directional motion. therefore, the velocity of the particle,
which is given by the sum of v1i,jk+ <dig> andv2i,jk+ <dig>  may not be able to make a good balance between the
global search and the local search of the particle. in the present study, we employ a
new way of determining v1i,jk+ <dig> andv2i,jk+ <dig> 

here, we assume that the velocity of the thermal motion v1i,jk+ <dig> follows a double exponential distribution, whose
probability density function and probability distribution function are

  fv1i,jk+1=1σi,jke-2|v|σi,jk 

and

  fv1i,jk+1=1-e-2|v|σi,jk 

respectively, where v represents the value of the random variable
v1i,jk+ <dig> and σi,jk is the standard deviation of the distribution. by
employing a stochastic simulation method, we can express v1i,jk+ <dig> as

  v1i,jk+1=σi,jk2ϕi,jk 

  ϕi,jk={+lnifs> <dig> −lnifs≤ <dig> , 

where

where s and ui,jk are two different random numbers uniformly distributed
on the interval , i.e. s,ui,jk~u. as for the value of σi,jk, an adaptive strategy is adopted to determine
σi,jk by σi,jk=2α|cjk-xi,jk|, where ck= is called the mean best  position,
defined as the mean of the pbest positions of all the particles, i.e.
cjk=∑i=1mpi,jk.

the velocity of the drift motion v2i,jk+ <dig> may have many possible forms. however, the following
simple linear expression is adopted in this study:

  v2i,jk+1=β 

where pi,jk is determined by

  pi,jk=ϕi,jkpi,jk+,ϕi,jk~u,1≤j≤d 

it can be immediately proven that if vi,jk+1=v2i,jk+ <dig>  when k→∞, xi,jk→pi,jk. therefore the expression of v2i,jk+ <dig> in equation  can indeed guarantee that the particle
move directionally to pik as an overall result.

with the definitions of the thermal motion and the drift motion of the particle, we
can obtain a novel set of update equations for the particle:

  vi,jk+1=α|cjk-xi,jk|ϕi,jk+β 

  xi,jk+1=xi,jk+vi,jk+ <dig> 

where α is called the thermal coefficient and β is called
the drift coefficient. the pso with equations  and  is a novel variant of
rdpso, which employs a double exponential distribution instead of a gaussian one. the
procedure of this rdpso variant is outlined below.

step 0: randomly initialize the current positions and the pbest position of
all the particles;

step 1: set k = 0;

step 2: while the termination condition is not met, do the following steps;

step 3: set k=k+ <dig> and compute the mbest position
ck, which is the centroid of the pbest positions
of all the particles at iteration k;

step 4: from i= <dig>  carry out the following steps;

step 5: evaluate the objective function value f, and update pik and gk according to equation  and equation ,
respectively;

step 6: update the components of the velocity and current position of particle i
in each dimension, respectively, according to equations ,  and ;

step 7: set i=i+ <dig>  and return to step  <dig> until
i=m;

step 8: return to step 2;

in the rdpso algorithm, in addition to the population size m, α
and β are two very important user-specified algorithmic
parameters, which play the same roles as the inertia weight w and
acceleration coefficients in the basic pso algorithm. that is, the can be tuned to
balance the exploration and exploitation ability of the particle. how to select the
values of these parameters to prevent the particles from explosion is an open
problem. here, we performed the stochastic simulations for the one dimensional case,
in which the local attractor was fixed at the origin and the mbest position
was at x =  <dig> . the results of two simulations are visualized in figure
 <dig> and figure  <dig>  in which the
logarithmic value of the absolute of xk was recorded as ordinate, and the iteration number was
the abscissa. figure  <dig> shows that the particle's position was
bounded when α= <dig> and β= <dig> . however, when α= <dig>  and β= <dig> , the particle diverged to infinity. to obtain the
sufficient and necessary condition for the particle to be bounded, we will focus our
attention on a theoretical analysis in terms of probability measure in future.
bounded.
as the iteration number increases.

setting large values for α and β implies better global
search ability of the algorithm, while setting small values means better local
search. when the rdpso is used for solving a problem, a good balance between the
global search and the local search of the algorithm is crucial for the algorithmic
performance. however, in order to find out how to tune the parameters to generate
generally good algorithmic performance we need a large number of experiments on
benchmark functions, which will be performed in our future tasks. here, we recommend
that when the rdpso is used, α should be set to be no larger than  <dig> 
and β to be no larger than  <dig> . more specifically, when the problem at
hand is complex, the values of the two parameters should be set to be relatively
large in order to make the particles search more globally, and on the other hand,
when the problem is simple, relatively smaller values should be selected for the
parameters, for the purpose of faster convergence speed of the algorithm. in the
present study, the value of α and β were set to be  <dig> 
and  <dig> , respectively.

in addition, the population size and the maximum number of iterations  can
also affect the performance of a population-based technique. just as for other pso
variants, it is suggested that the population size should be larger than  <dig> for the
rdpso as well. the value of the maxiter depends on the complexity of the problem.
generally, a smaller maxiter value is used for simple problems, while a larger one is
used for complex problems.

moreover, vi,jk is also restricted within the interval
during the search process of the rdpso algorithm, just
as in the original pso algorithm.

the optimization methods compared
besides the pso and rdpso algorithms, the differential evolution , scatter search
 method and two versions of evolution strategies were also used to solve the
selected inverse problems, for performance comparison purposes. the de method, as
presented by storn and price  <cit> , is an evolutionary computing method, which has a faster convergence speed
than gas and can find the global optimal solution of a multidimensional and
multimodal function effectively  <cit> .

the ss method is also a population-based search techniques originally developed by
glover  <cit> . in  <cit> , a novel meta-heuristic method, which is the combination of the original
ss method with a local search technique, was proposed to solve inverse problems. it
was shown that the local search operator can accelerate the convergence speed
significantly. thus, in our experiments, we used this novel ss method for performance
comparison.

evolutionary strategy  is an important paradigm of eas, which imitates the
effects that genetics produces on the phenotype, rather than the genotype as in gas  <cit> . the two canonical versions of es we used in this study are denoted by
-es and -es,
where μ denotes the number of parents and λ the number of
offspring. in the -es, the parents are
deterministically selected from offsprings , while in -es, the parents are
selected from both the parents and offsprings.

in addition, the performances of the above mentioned algorithms, including the rdpso,
are also compared with those of the sres method. the sres is a version of
-es that uses stochastic ranking to handle the
constraints, by adjusting the balance between the objective function and the penalty
function on the course of the search  <cit> ,  <cit> .

case studies
two case studies involving two benchmark systems were carried out. for each system,
we performed two groups of numerical experiments, one with noise-free simulation
data, and the other with noisy simulation data.

case study 1
the goal of this case study is to estimate the five rate constants of the
homogeneous biochemical reaction describing the thermal isomerization of
α-pinene, which is an organic compound of the terpene class, one of two
isomers of pinene  <cit> ,  <cit> . the mathematical model of this process is formulated with the
following linear equations:

  dy1dt=-y <dig> 

  dy2dt=p1y <dig> 

  dy3dt=p2y1-y3+p5y <dig> 

  dy4dt=p3y <dig> 

  dy5dt=p4y3-p5y <dig> 

where  is the vector of unknown coefficients to be
estimated, y <dig>  y <dig>  y <dig>  y <dig> and y <dig> denote the concentrations of the α-pinene,
dipentene, alloocimen, β-pyronene and a dimer, respectively.

case study 2
this case study involves the inverse problem to identify  <dig> kinetic parameters of
a nonlinear biochemical dynamic model formed by the following  <dig> ordinary
differential equations that describe the variation of the metabolite concentrates
with time  <dig>  <cit> .

  dg1dt=v11+pki1ni1+ka1sna1-k1⋅g <dig> 

  dg2dt=v21+pki2ni2+ka2m1na2-k2⋅g <dig> 

  dg3dt=v31+pki3ni3+ka3m2na3-k3⋅g <dig> 

  de1dt=v4⋅g1k4+g1-k4⋅e <dig> 

  de2dt=v5⋅g2k5+g2-k5⋅e <dig> 

  de3dt=v6⋅g3k6+g3-k6⋅e <dig> 

  dm1dt=kcat1⋅e1⋅1km1⋅1+skm1+m1km2-kcat2⋅e2⋅1km3⋅1+m1km3+m2km <dig> 

  dm2dt=kcat2⋅e2⋅1km3⋅1+m1km3+m2km4-kcat3⋅e3⋅1km5⋅1+m2km5+pkm <dig> 

wherem <dig>  m <dig>  e <dig>  e <dig>  e <dig>  g <dig>  g <dig> and g <dig> are the state variables representing the
concentrations of the species involved in different biochemical reactions, and
s and p are controlling parameters which are kept fixed at
the initial values for each experiment. the inverse problem is then reduced to the
optimization problem that fits the remaining  <dig> parameters represented
byθ=.

objective functions
the objective function  for the inverse problem in either of the
two case studies is the discretization of equation , which is formulated as the
weighted sum of squares of the differences between the experimental and the predicted
values of the state variables:

  j= ∑i=1n∑j=1lwij{j} <dig> 

where n is the number of data for each experiment, l is the number
of experiments, yexp is the vector of experimental values of the state
variables, and ypred is the vector of the values of state variables
predicted by the model with a given set of parameters. in case study  <dig>  each
wij was set to be  <dig>  <cit> , while in case study  <dig>  wij was set as wij={1/maxj} <dig>  which was used to normalize the contributions of each
term  <cit> .

obtaining simulation data
in order to evaluate the performances of the global optimization methods in finding
the solution of the inverse problems, we chose a set of parameters for each model,
which are considered as the true or nominal values. for case study  <dig>  the true values
of the parameters are p <dig> =  <dig> e- <dig>  p <dig> =
 <dig> e- <dig>  p <dig> =  <dig> e- <dig>  p <dig> =  <dig> e- <dig> and
p <dig> =  <dig> e- <dig>  for case study  <dig>  the nominal values of the
model parameters are shown in table  <dig> 

 measurements)

the pseudo-experimental data  in either case were
generated by substituting the chosen parameters into the dynamic model and performing
fourth order runge-kutta method on the corresponding differential equations. for case
study  <dig>  the pseudo-measurements of the concentrations of metabolites, proteins, and
messenger rna species were the results of  <dig> different pseudo-experiments, in which,
with the given nominal values for the parameters, the initial concentrates of the
pathway substrate s and product p were varied for each experiment
 as shown in table  <dig>  these simulated data
represent the exact experimental results devoid of measurement noise and they were
used as noise-free data for the first group of numerical experiments in each case
study. in order to test the optimization methods for noisy data, we added a white
noise to each of the original noise-free data:

  z′=z+σε 

where z and z′ represents the original noise-free data and the
resulting noisy data, respectively, ε is an random number with standard
normal distribution, namely, ε~n, and σ is the standard deviation of the
white noise. in our case studies, σ was set to  <dig>  for both
systems.

initial problem solver used
during the search of each global optimization algorithm, each potential solution
 was substituted into the
dynamic model. then, the fourth order runge-kutta method was performed on the
corresponding system of differential equations to generate a set of predicted values
of the output state variables, from which the objective function value  of the potential solution could be evaluated according to equation  with
the obtained pseudo-experimental  data. this process is known as the
solution to the forward problem, which was embedded in the iterations of the search
during the solving of the inverse problem with the algorithm.

experimental settings
for the sake of performance comparison, all the tested global optimization methods
except the ss method -es,
and -es) were programmed in c++ on a vc++ <dig> 
platform in windows xp environment, and implemented on intel pentium dual-core e5300
 <dig> ghz pcs, each with  <dig> mb cache and  <dig> gb main memory. the ss method was implemented
in matlab  <dig> , on the same platform, for the purpose of calling the local solver sqp
in matlab during the search process. the software for ss for inverse problems can be
found on http://www.iim.csic.es/~gingproc/ssmgo.html.

the configuration of the algorithm parameters including the population sizes are
listed in table  <dig>  in case study  <dig>  each optimization algorithm
ran  <dig> times with each run executed for  <dig> iterations; that is, the maximum number
of iterations  is  <dig>  in case study  <dig>  each algorithm also ran  <dig> times
with each run executed for  <dig> function evaluations, which is the same as that
for the de in  <cit> . since the population size of each algorithm was  <dig>  the value of maxiter
was  <dig> in case study  <dig>  for the ss method, the initial population size was  <dig> 
and  <dig> individuals were selected to perform the iterative search after
initialization. other parameters were selected according to recommendations from the
corresponding references and/or our preliminary runs. for all the algorithms tested
on the inverse problems, the statistical values of j were figured out and
the results with best values of j were selected, processed and visualized
with matlab  <dig> .

α =  <dig> 
w =  <dig> 
f =  <dig> 
μ = 10
μ = 10
RESULTS
for case study  <dig>  the statistical values of j from  <dig> search runs with 500
iterations by each algorithm are listed in table  <dig>  the best value
of j  for the numerical experiments with noise-free
simulation data was obtained by using our proposed rdpso algorithm after running for
 <dig> h . for the experiment with noisy data, the rdpso generated the
best j value  as well. the proposed algorithm also showed the best
performance on average among all tested methods, as shown by the mean value of j
over  <dig> runs. in this case study, the basic pso algorithm showed good performance
on low-dimensional inverse problems.
case study 1

the convergence process of each tested algorithm averaged over  <dig> runs in the numerical
experiment with noisy data in case study  <dig> is shown by the convergence curve in figure
 <dig>  which is plotted in the log-log scale with objective
function values versus the iteration number. evidently, the ss method showed a better
convergence property than other algorithms. the best solution vector corresponding to
the best value of j  obtained by the rdpso in the
numerical experiment with noise-free data was p <dig> =  <dig> e- <dig> 
p <dig> =  <dig> e- <dig>  p <dig> =  <dig> e- <dig> 
p <dig> =  <dig> e- <dig>  p <dig> =  <dig> e- <dig> 
extremely close to the real values of the parameters, and the best solution vector
obtained by the rdpso for the numerical experiment with noisy data was  p <dig> =  <dig> e- <dig>  p <dig> =
 <dig> e- <dig>  p <dig> =  <dig> e- <dig>  p <dig> =
 <dig> e- <dig>  p <dig> =  <dig> e- <dig>  figure 4andboth noise-free and noisy data.
all the algorithms averaged over  <dig> runs in the numerical experiments with
noisy data for case study  <dig>  it is shown that the rdpso, pso and ss methods
had better convergence properties than other methods.
model  for case study  <dig>  it is shown that the predicted model
obtained by rdpso fits the experimental data well
model  for case study  <dig>  it is also shown that the
predicted model obtained by rdpso fits the experimental data well.

for case study  <dig>  the obtained values of j resulted from  <dig> runs with 22500
iterations by each algorithm are listed in table  <dig>  for the
numerical experiments with noise-free data, the best result of j  was obtained by using the ss method, which also had the better average
performance than any other compared algorithm, as shown by the mean value of j
over the  <dig> runs of the algorithm. the second best method was the rdpso algorithm,
which could converge to a value of j =  <dig>  and had a mean value of j
=  <dig>  over  <dig> runs. table  <dig> lists the estimated values
of the model parameters corresponding to the best j value 
found by the rdpso algorithm. the results also show that -es is the winner in this inverse problem compared to -es, whose best and mean results of j were  <dig>  and
 <dig> , respectively. the pso and de are two well-known efficient population-based
optimization methods, which, however, could not arrive at the vicinity of the
aforementioned solutions. when the experimental data  was noisy, the ss
method and rpso obtained similar results for the best j value over  <dig> runs, but
the former had a better average algorithmic performance. table 6
shows the identified model parameters corresponding to the best j value  obtained by the rdpso for the noisy data.
case study  <dig>  including the results for the noise-free and noisy data.



we plotted in figure  <dig> the convergence curve of each method
averaged over  <dig> runs. it is shown that the ss method had a remarkably better
convergence rate than others, probably due to its local solver that can enhance the
local search ability of the algorithm significantly. figures 7andfigure  <dig> show comparisons between the predicted data
and the experimental  data for the decision vectors found by the rdpso in
both groups of numerical experiments .
it can be observed that there is good correlation between the experimental and predicted
data.
all the algorithms averaged over  <dig> runs of the numerical experiments with
noisy data in case study  <dig>  it is shown that the ss method had the fastest
convergence speed and the rdpso had the second fastest one.
 <dig> , s =  <dig> . it is shown that the predicted concentration had good
correlation with the experimental data.

CONCLUSIONS
in this paper, a variant of rdpso algorithm was proposed and showed to be able to
successfully solve two inverse problems associated with the thermal isomerization of
α-pinene and a three-step pathway, respectively. the results indicate that the
proposed rdpso algorithm outperformed its pso predecessors and some other competitors in
the first problem, and also had the second best algorithmic performance among all the
compared algorithms.

like other stochastic optimization methods, a possible drawback of the rdpso method is
the computational effort required. this is mainly because most of the computational time
was spent on solving the forward problem. one measure that can be taken is to
incorporate the local search technique into the algorithm in order to accelerate its
convergence speed. another is to develop a parallelized rdpso implementation to solve
inverse problems on computer clusters to reduce the computational cost to a reasonable
level. our future tasks will focus on these two ways of improving the algorithmic
effectiveness of the rdpso algorithm.

availability and requirements
in the additional file  <dig> the source codes of five of the
tested algorithms on the two benchmark systems are provided. it includes two file folds,
one for benchmark system  <dig> and the other one for benchmark system  <dig>  all the algorithms
are programmed with c++ in microsoft visual c++  <dig> .

in additional file  <dig> the data files for the five algorithms
used in the two case studies are provided. for each case, the data corresponding to the
best results generated by  <dig> runs of each algorithm are provided in a .txt file.

authors' contributions
js and vp designed the algorithm and drafted the manuscript. yc processed the
experimental results. wf and xw revised the manuscript critically. all of the authors
have read and approved the final manuscript.

competing interests
the authors declare that they have no competing interests.
=  <dig> . it is shown that the predicted concentration had good correlation with
the experimental data.

supplementary material
additional file 1
source. codethis file includes the source code of all tested algorithms on
the two benchmark problems, programmed in c++ on microsoft visual vc++  <dig> . all
the source codes are compressed into a single .rar file.

click here for file

 additional file 2
data. this file includes the data of the best solution out of  <dig> runs of
each tested algorithm.

click here for file

 acknowledgements
this work is supported by the natural science foundation of jiangsu province, china
, by the natural science foundation of china , by the program for new century excellent
talents in university , by the rs-nsfc international
exchange programme , and by the key grant project of
chinese ministry of education .

declarations
funding for the publication of this article comes from the natural science foundation
of china  grant  <dig> 

this article has been published as part of bmc bioinformatics volume 15
supplement  <dig>  2014: knowledge discovery and interactive data mining in
bioinformatics. the full contents of the supplement are available online at
http://www.biomedcentral.com/bmcbioinformatics/supplements/15/s <dig> 
