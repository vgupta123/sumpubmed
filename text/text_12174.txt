BACKGROUND
probabilistic models are widely used in biological sequence analysis. they are essential mechanisms to pre-process the plethora of data available, creating hypothesis for biological validation. examples are hidden markov models   <cit> , weight array matrices   <cit>  and covariance models   <cit> . in this context, probabilistic models can be used to represent known families of sequences and to create programs to predict if specific sequences belong to the family of interest. however, these models assign non-zero probability values to most input sequences. therefore, we need a criteria to decide when a given probability value is sufficient. one of the most commonly used technique is bayesian classification using two probabilistic models: f, that represents a family of sequences, and a, an alternative model. the likelihoods of each of the two models is measured and the sequence is classified as belonging to f if the likelihood of f is greater than the likelihood of a.

the choice of the alternative model is essential to reduce the number of false predictions and depends on the problem. an alternative model can be either a negative model representing the complementary set of the sequences of interest, or a null model, representing random sequences. negative models are used when there is a deeper biological understanding of the particular problem and it is possible, with a high degree of certainty, to characterize the sequences that are not part of the family. therefore, the choice of the probabilistic model to be used as the negative model depends on a strong biological hypothesis about the complementary set. null models are used when we do not have sufficient information to characterize the complementary set of the sequences we want to classify. this situation is generally the rule for annotation software, where we want to characterize a sequence family  against all other sequences. this is the scoring technique used by sequence analysis tools such as hmmer  <cit> , sam  <cit>  and infernal  <cit> . null models is the chosen strategy for alternative model considered in this work.

more technically, we want to compute, given a nucleotide sequence x, which model better represents the sequence: the family model  or the null model . the sequence x is classified as belonging to the family represented by the model f if p > p or, alternatively, if . considering p = p, the classification of x simplifies to the comparison of the likelihoods:  . to cope with the very small probability values when sequences are long, log values are used. so, we use the log-odds score s:   

we want null models that help classifiers reject sequences that do not belong to family f . therefore, such a null model n should score higher than the family model f for any negative sequence. in other words, with a good null model, log-odds score for negative sequences will have value zero or less.

null models, due to their very generic nature, should not present any structure. therefore a convenient model to describe random sequences in a null model n is a position-independent probability distribution, which imposes no structure on the sequences. for nucleic acids sequences, the null model assigns a fixed probability value pn to each nucleotide . therefore, the probability value of a sequence x of length l is given by the formula:

p = ca*cc*cg*ct

where ci is the count of the nucleotide i in the sequence x, .

there are many possible strategies to set up a null model discussed in literature  <cit> , all of which seem to make sense biologically. some of them are: i. using a uniform distribution, ii. using the genomic background distribution, iii. using the training set distribution, iv. using the target sequence distribution. each strategy uses a different reasoning to minimize false positives. the reasoning behind each strategy is based on how we will characterize "random sequences". with the uniform distribution, we define randomness by the absence of information, even about the nucleotide composition. with the genomic background distribution, we define randomness by what should be the standard nucleotide distribution of a sequence in a specific genome. with the training set distribution, we assume the family model will favor a certain specific nucleotide distribution ; so if we use the nucleotide distribution of the training set as a null model, this will help the classifier reject sequences with a high score only due to their base composition. finally, with the target sequence distribution, random sequences are those with the same base distribution of the target sequence . independently of the rationale chosen, the null model will fall in one of three classes: a uniform distribution, a fixed non-uniform distribution or a target-dependent distribution.

the goal of this study is to evaluate the impact of each of these three classes of null models in the false positive rate of classifiers. we found only two studies in literature that analyzed the performance of null models  <cit> . each study evaluates one specific benchmarks of aminoacid sequences and only one probabilistic model . this approach limits the generality of their conclusions. first, they do not address the problem for a wider amplitude of classification models. second, and more important, they only analyze the final accuracy results for their specific benchmarks, without any consideration on why these can be generalized to other sequence families.

to make this study more general than previous works, we use random sequences and two different probabilistic models. using random sequences guarantees there is no bias in the study towards any particular benchmark, so we expect the results to be of broad application. also, the simulations used random sequences across the whole gc spectrum, in an effort to make the results applicable to any real-life situation. the two probabilistic models chosen are very different, aimed at covering a wide range of models: one with very simple architecture and one able to represent more structured sequences. the studies were performed using weight array matrices   <cit>  and covariance models   <cit> .

wams record only fixed-distance content dependencies, useful to represent sequence motifs. cms are able to characterize indels and register dependencies in non-adjacent bases at arbitrary distances, which can be used to characterize secondary structure. we evaluated wams in the context of splice site prediction and cms in the context of predicting rna or other genomic elements with secondary structure. splice sites were used for three reasons: first, splice site prediction is at the heart of gene prediction, an biologically important problem in bioinformatics, second, the abundance of data in public databases, third, because many successful predictors use position-dependent models, which is the base of our probabilistic model range. the spectrum of gc content in the dataset enabled using a single sequence family  for all experiments with wams. in this context, the same was not possible for cms, where training sets are generally small and concentrated on a small spectrum of gc content. in this case we had to use three different sequence families .

we will see below that the training set and the genomic background are not good choices for a null model. in fact, no fixed, non-uniform distribution is, as a quick mathematical analysis can demonstrate. as we will see below, two probabilistic i.i.d. models are best suited for classification: the uniform model and the target model. however, we also show that the uniform distribution can also have a deleterious effect in sequences with biased gc distribution. this is particularly relevant, since it has not been described before and since uniform models are widely used in the context of nucleotide sequences. the final conclusion is that the target model is more dependable when choosing candidates for biological validation due to its higher specificity. this is reinforced by the real data experiment using plasmodium falciparum, a highly at-rich genome. the study was performed in dna sequences using gc content as the measure of content bias, but the results should be valid also for protein sequences.

RESULTS
since we are interested in minimizing the number of false positive predictions, we used randomly generated sequences for evaluation. random sequences should receive negative log-odds scores in probabilistic classifiers for any specific sequence family. in other words, a better performance in terms of specificity means fewer random sequences with positive scores. we evaluated six null models: 5%gc, 25%gc, uniform, 75%gc, 95%gc and the model obtained from the base frequencies in the target sequence  <dig> 

1we used gc content as a simplified measure of nucleotide composition, which allows the visualization of 2d plots.

initially, for illustrative purposes, we computed the log of the probability values of the test sequences given the null models alone . this illustrates the values produced by these models for sequences at different gc compositions. we called these “raw scores”. next, we used each of these models as null models in log-odds scoring classification for two different types of family models, wams and cms. since we are using only random sequences, the log-odds scores should be negative. positive scores indicate false positives <dig> 

2we assume that the chance of one of the random sequences being an actual family sequence is negligible.

raw score behavior on random sequences
we have plotted the raw scores  of random sequences using the fixed distribution and target models alone. the results are shown in figure  <dig> 

as it was expected, the uniform model produces no bias along the gc content , producing a constant score, consistent with the fact that all analyzed sequences have the same size. the raw scores using the biased fixed distribution models  show a linear dependence on the gc content of the analyzed sequences; the gc content of the model only determines the inclination of the linear plot. the target model presents a less intuitive result, a curve with the lowest scores at 50%gc and higher scores towards more extreme gc distributions.

effect of different null models in log-odds scoring
probabilistic models such as wams and cms also capture the base composition of the sequences of the training set. therefore, when we use log-odds scoring, the gc bias recorded by the family models should also influence the final score and we have to analyze the combined influence of the family and null models. we embedded the null models used in the previous section in classifiers using two different probabilistic techniques, weight array matrices   <cit>  and covariance models   <cit> . for each technique we created classifiers using training sets with different gc average compositions. for clarity, we only show the results for  <dig> null models: target, 5%gc, 50%gc and 95%gc. data for the null models corresponding to the 25%gc and 75%gc is consistent with the presented results .

weight array matrices
we used sequences of acceptor splice sites to create three distinct training sets with different average gc content: 38%gc, 50%gc and 65%gc . for each training set, a weight array matrix was trained and used to score random sequences using the six null models:  <dig> fixed gc models and the target model. the results are shown in figure  <dig> 

as we can see, log-odds scores of random sequences using fixed gc null models, including the uniform model, present a quasi-linear dependence on their gc content. this means that, no matter what is the composition of the sequences used to characterize the family , any random sequence at the ends of the gc spectrum will score consistently higher  than any other sequence. this effect is so relevant that random sequences in one of the gc content extremes have positive scores when any of the fixed gc models is used, which indicates a strong tendency to generate false positives in the classification of sequences with extreme gc compositions. on the other hand, the target null model presents higher scores for sequences with gc content similar to the average gc content of the training set and lower scores for sequences with extreme gc content. the target null model presents the lowest number of positively scored sequences. the consequence in real-life classifications would be a lower number of false positives.

covariance models
covariance models  are usually used to characterize families of rnas or other genomic elements with secondary structure. training sets for cms tend to be much smaller. therefore, instead of dividing the training set of a single family in different training sets separated by gc content , we used three different cms obtained from the rfam database  <cit> , each one targeting a family with a distinct compositional bias: i. the cmlow family with  <dig> % gc , ii. the cmmedium family, with  <dig> % gc , iii. the cmhigh family, with  <dig> % gc .

we can observe in figure  <dig> that log-odds scores using the fixed gc distribution null models show, again, a quasi-linear dependence on the gc content, resulting in a large number of false positives in one side of the gc spectrum. similar to what happened with wams, the uniform model also shows a reduced, but still present quasi-monotonic bias . the target model, on the other hand, shows a non-linear dependence on the gc content, with a peak towards the gc content of the training set.

specificity of the different null models specificity of the different null models



number  of positively scored sequences for each null model. wamlow, wammed and wamhigh designate the wam models generated by the training set with low , medium  and high  gc content, respectively. cmlow, cmmed and cmhigh designate the cm models generated by the training set with low , medium  and high  gc content, respectively.


testing in plasmodium falciparum real data
as we have seen above, the target null model presented much better performance against the other models when testing against random sequences. to validate these results in a realistic environment, we have tested the performance of  <dig> null models in the context of acceptor site prediction for plasmodium falciparum. this organism was chosen due its well known gc bias . we tested four null models used in prediction: target null model, uniform null model, genomic background null model  and training set null model . the precision-recall curves are depicted in figure  <dig>  as expected, splice site prediction with the target null model shows a significantly better performance, with a higher precision  in most marks. considering all the tested sequences  the target null model presented the best precision , the best specificity , and the best balance between precision and sensitivity  . a roc curve has also been generated and reinforced the best performance of the target null model .
 performance of the null models on the p. falciparum data



this table shows the precision, specificity, sensitivity, and f-score for the entire testing set of the p. falciparum using the target, uniform, genomic background and the training null models. the best performance for each measure is in bold face. the f-score, is the harmonic mean between precision and sensitivity values.


discussion
the results of raw scores presented above show that all but the uniform null model produce a score biased by the gc content of the analyzed sequence. the problematic aspect is not the gc dependence per se, but when this dependence produces a raw score curve linear in the whole gc spectrum. so, for instance, if the null model is 75%gc, the greater the gc content of the sequence, the higher the final raw score, including the sequences with gc content higher than 75%. on the other hand, the dependence introduced by the target model is a curve with higher values at the extreme gc contents and with the lowest value at 50%gc. so high scores will always be attained by sequences with any kind of bias . therefore, raw score curves seem to indicate that the only adequate null model is the uniform model, since it is the only one that does not introduce a dependence on the gc content.

indeed, when the models were used in log-odds scoring, the uniform model showed the lowest dependence on the gc content , but still showed a quasi-linear dependence . this indicates that the gc content registered by the family models also have a small deleterious effect when working with sequences with high gc bias, since positive scores were ascribed to some random sequences. the curve associated with the target model, on the other hand, presented a peak near the average gc content of the training sequences, “canceling” the monotonic gc-dependence introduced by the family models.

this is an interesting feature, since the gc content can be a meaningful characteristic of a sequence family. in fact, this is a more appropriate classifier behavior than the effect associated to other null models, such as a training set null model, that assign high scores to sequences with a gc bias opposite to that presented by the sequences of the targeted family. if a family of sequences has low compositional variation, gc content can be considered relevant information during the classification process. what we want in these cases is a dependence that will “center” at the characteristics of our training sets, that is, that rewards gc contents similar to those of the known sequences of the targeted family and that “punishes” gc contents that are not. that is exactly what the target null model does, without producing too many false positives.

the location of the “peaks”  is not a coincidence. in particular, if the family model f is also a position-independent fixed-distribution model, then the log-odds peak is exactly at the nucleotide distribution represented by the family model. this happens because, by definition, the target model is a distribution model trained using the target sequence with the maximum likelihood method, i.e, the probability of the sequence given the target model is the maximum value possible in the family of distribution models. so the peak observed in the log-odds curve occurs when both models have the same nucleotide distribution and it occurs when the target sequence has the same nucleotide composition as represented by the family model. in other words, when only gc composition is concerned, log-odds scoring with the target model peaks at the family’s gc content.

also important is that the peak scores presented in the target null model do not necessarily correspond to positive scores. in fact, the target null model presented the best specificity results  in all tests. moreover, this effect is still in place even for models that register secondary structure such as the cms. in this case, although the log-odds score peak is moved towards the average gc content of the family, they do not coincide exactly . the explanation is probably related to the structural component of the cm score, which is not so directly dependent on the sequence gc content.

if on one hand the target null model presents the best specificity, on the other hand it may impair sensitivity in detecting true sequences that have the base composition very different from the average composition of the training set. when a high gc variation is expected within the family of interest, it is possible that the target model will generate a higher number of false negatives, in which case the uniform model should also be considered. this phenomenon was observed in covariance model tests performed with a benchmark of transfer rna sequences  . for a test sample of  <dig> trna sequences <dig> with gc content evenly distributed over the gc range of the trna family , the specificity values achieved using the uniform and target null models were, respectively,  <dig> % and 100% and the sensitivity values were, respectively, 100% and 93%, corroborating the fact that the target model tends to have higher specificity and lower sensitivity than the uniform model. the same behavior was observed in the plasmodium falciparum data experiment presented in this work. the target null model presented the best precision , but its sensitivity was  <dig> % . the most sensitive model was the uniform null model  but at the cost of a very low precision . so in this specific context the use of the uniform null model is not recommended. we do not evaluate false negative rates, since this evaluation cannot be performed using random sequences and is, therefore, highly dependent on the benchmark used.

3sequences downloaded from rfam database release  <dig>   <cit>  under the rfam accession number rf <dig> 

the gc percentages of the fixed distribution null models shown in this article do not correspond to the specific gc contents that would constitute a “training set” null model on each experiment using simulated data. but, in fact, “training set” null models are fixed-distribution models, where the distribution is determined by the training set. therefore, a training null model is not suitable because of its fixed distribution. the homogeneous behavior of the performance of fixed-probability null models and the inferior performance of the training null model in the real data experiment support our conclusions. also, for the covariance models, the training set percentages  were very close to the percentages used in the tests . the same is not true for the wam tests, in which case we did run tests for null models with the training set percentages, and the results were consistent .

our study was performed in the context of nucleotide sequences, however we expect similar results for aminoacid compositions. this is supported by the fact that the analytical reasoning we performed are also valid for aminoacid sequences. in other words, when using any fixed distribution model against the target null model in log-odds scoring, the highest scores are obtained for sequences with the same aminoacid composition as that described in the fixed model. due to the number of possible aminoacids, a similar study would be harder to perform and interpret it as 2d plots would not be helpful. as a matter of fact, two hmm-based tools used for protein domain identification, sam  <cit>  and hmmer  <cit>  also make use of target sequence data in some way to compose their null model. sam scores the reversed target sequence with the same hmm. hmmer combines the database background frequencies with a second null model derived from the analysis of the target sequence in an ad hoc way  <cit> . their success seems to reinforce our belief.

CONCLUSIONS
in this paper we evaluated the performance of  <dig> different types of null models in profile-based probabilistic models: uniform null model, fixednon-uniform gc null model , and target null model on the analysis of random nucleic acids sequences of various gc contents. we presented both the independent behavior of each model in the form of raw scores and their behavior when used in log-odds scoring in conjunction with  <dig> different probabilistic techniques: weight array matrices   <cit>  and covariance models   <cit> .

all our results indicate that, when the sequence family presents low variation on the gc content, the target model is a more dependable model to generate hypothesis for biological verification due to its high specificity when compared to any fixed-distribution model, in particular for organisms that present genomic sequences with high gc bias. detecting acceptor splice sites in the gc-poor plasmodium falciparum genome , the target null model presented the best precision , the best specificity  and the best balance between precision and sensitivity . the use of the target residue composition in the null model construction was also shown beneficial in substitution score matrices. yu, altschul and colleagues proposed matrix modifications taking into account the amino acid frequency of both query and target sequence  <cit> . accuracy of the psi-blast program  <cit>  is also improved by re-evaluating promising alignments using statistics based on the composition of target sequences  <cit> .

this study was performed using  <dig> probabilistic techniques, wams and cms. however, we expect the results to hold for other techniques such as weight matrix models  <cit>  and hidden markov models  <cit> , which are particular cases of wams and cms, respectively.

