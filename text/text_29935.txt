BACKGROUND
labs and bioinformatics platforms have to manage large amounts of data, coming from different sources, in different formats and holding different kinds of meta-data.

while data storage by itself is a new challenge, finding where a specific datum is stored, and what is its meta-data is another one. people need a way to search in their data sets, should it be visually with a graphical interface for analysis, or with an api to integrate it with other applications.

different tools are available on public bank web sites  to find a specific gene, for example, from its name or id however those tools are available only for their hosted data; they cannot be mixed up with a specific set of data coming from a lab . those tools usually limit intensive usage with a risk of being blacklisted. they can prevent automated, frequent requests for large data sets analysis.

the idea was to provide those labs with a tool to access efficiently the data generated in their lab, the same way they would with those online tools, with the additional possibility of mixing them with other data sources. the tool had to be open source, to allow new additional file formats or software extension; flexible, to adapt the web interface to one‘s own need, and scalable to support the load requests and the size of the managed data sets.

bioinformaticians are the primary target of the project; they need to link their data with the tools they use in their daily tasks. the software gives them access to original document location, the meta-data and some raw data , all of this in a machine-readable format. a bioinformatician could query the software from a blast result, for example, to extract the features present at alignment positions or he could extract all the information matching a specific criteria. he can also build custom banks from a query matching specific criteria.

the web search interface targets both the bioinformaticians and the scientists for easy access to the information and direct visualization of a set of data .

the seqcrawler project started after some testing on lucegene  <cit> . lucegene wrote some base code to link the gmod gbrowse browser  <cit>  to a search engine  as well as some translation scripts to ease the indexing of a few biological data formats. however the solution was not scalable to large indexes, and raw data storage was not suitable to hold a very large number of sequences. while srs  <cit>  offers very interesting features, it is commercial software and does not allow customization or multi-server scalability.

the seqcrawler goal is to extend those features in an easy to use solution, scalable to fit any need, and implementable in the cloud.

seqcrawler has been developed to:

 <dig>  enable full text search but also complex queries on meta-data .

 <dig>  download original file 

 <dig>  provide an integrated, extensible and customisable visualization environment above the indexed data.

 <dig>  provide a search engine and storage back-end scalable on multiple servers

using a search engine as a central system for the meta-data is interesting for several reasons:

· search engines provide exact but also approximate text-based search, a useful feature when the user is in front of a search interface.

· search engines enable storage of information beyond search usage, this means that some data can be stored and accessed even if not used directly for the search.

· search engines offer scoring and paging results.

· search engines allow flexible schema, i.e. new fields or document types can be added/indexed with no redefinition.

· gmod browsers use some drivers to access the data . with an appropriate driver, they can be connected with the search engine.

this work will explain how seqcrawler can help labs to manage their data and how a biological information system can be built upon it.

implementation
architecture
the software is made up of several independent components figure  <dig> 

 <dig>  the indexer is a command-line tool to load into the search engine biological data files, from many common known formats  and solr native format.

 <dig>  the search engine, apache solr  <cit> , manages index queries with a rest- like interface

 <dig>  the storage backend, optional, stores sequence data in a key/value based database . other backends can be easily added.

 <dig>  the web interface triggers the search engine to get results from user query, it manages pagination and provides specific rendering according to original content-type .

the software uploads data  and ties the components  to share and extract data. however, each component can be queried or activated independently.

components
indexer
the indexer program is a set of perl/java command-line tools used to load a document , containing meta-data and/or data , in the search engine and the storage backend. it supports multiple biological formats as input and maps the document to key/value pairs. each key is defined as a field in the search engine. if indexed, a field can be used as a filter with the search engine. if only stored, it will be included in the answer but will not be part of the filter.

indexer can index data either offline for large inserts , or online on a remote server. the online mechanism can be used for data updates, with no server restart. however, for a very large index, it is advised to update the index in offline mode. the program also supports partial update of the index.

some formats are managed directly by the software while others are managed with the readseq software.

the indexer also adds some additional information to keep track of the original file location as well as the location of the data in the file. it also adds a specific content-type to the indexed document, which depends on the original input. this content-type will be used by the web interface to display the data according to its type.

all the search engine fields of the document are indexed by default. this can be customized in the search engine configuration.

while adding a new file format support to the code is quite easy, a plug-in mechanism is provided to dynamically add new ones. to add a new format, the developer writes a new file , written in javascript, with dedicated methods to read the file and transform it in a key/value set. then, the indexer program can be executed with the new type  to read the new file format. this plug-in mechanism allows easy integration of new or custom data types.

another feature is field recoding: a file document field  can be recoded  dynamically. to do so, a new java file can be added and declared in the configuration. this feature helps to customize the parsing of a document.

search engine
the search engine is apache solr. it provides a rest-like interface to index a document or query the index. a schema, flexible and customizable, defines how data should be indexed and/or stored for later retrieval. this server supports index shards, i.e. splitting an index into multiple smaller ones while keeping a single point of access to query them. the server manages the querying of all the shards and the merge of the results.

it also supports paging, caching and complex queries on fields with the lucene  <cit>  syntax. with ranged queries, a field can be queried with upper and lower bounds.

with the rest interface, an application can directly query the system to find an element or a subset of elements .

while providing a filter for each indexed field, the system also supports queries on all the fields with a full text search. finally, exact but also fuzzy and starts-with searches are allowed.

storage backend
to store large data sets , in addition to the meta-data, the software uses a key/value based storage backend. currently designed for mongodb  and riak, the software can be extended to support other backends.

the reason for such a storage backend is an easy api access and its scalability with a ring architecture. in ring architectures, each node can be queried to get some data, even if the data are not hosted on this node. data can also be replicated automatically among other nodes to ensure higher availability and performance.

data are stored as objects with a unique identifier, and can be stored on multiple servers, replicated if needed. this solution provides storage for a very large amount of data elements, with no storage/size limitation.

datum is split in small chunks in the backend and the application can retrieve it from its id .

if the original file cannot be accessed by the application, using this storage is a way to keep the data available. it can also be used to store additional data related to the original document .

the software provides a web interface and a rest- like interface to query the storage backend. it also supports additional start and stop parameters to extract a part of the data .

web interface
the web interface is the end-user entry point. this module is not needed for software interaction, as all other components are reachable via a http-based interface. accessible with any recent browser, the web interface uses ajax calls to query the index engine and offers export mechanisms to extract all or part of the results. it displays a google-like result presentation with paging, ordered by score. the search engine calculates the score with internal rules. a few additional meta-data can be displayed depending on the original content type. the display of the results per content-type can be customized  to add new content-type support or modify the rendering for a specific content-type with the inclusion of a custom javascript file.

for each result, additional links are available to:

 <dig>  get the details of a result where all key/value stored elements will be displayed as well as a link to the original document for download.

 <dig>  display the data recorded in the storage backend, depending on content-type.

 <dig>  link to a visualization tool, dependent on content-type.

the content-type , recorded at indexing time by the indexer component, gives some extra information on the original data format. it defines the visualization tool to use . the software currently embeds  <dig> visualization tools: gbrowse and chemdoodle  <cit> .

gbrowse is a well-known sequence browser from gmod. linked to the biosequence/gff type , it displays the sequence with information extracted from the search engine.

features information and position are queried via a dedicated dbi interface, and are used by gbrowse to display the information. the dbi interface also queries the storage backend to extract the sequence data to be displayed in the “details” window.

we have developed this specific driver to link the gbrowse software to the solr engine as an extension of the driver developed by the lucegene project.

the chemdoodle renderer is an experimental viewer we have plugged in the system. it displays in the browser a 3d, rotating, image of a protein from a pdb file, in pure javascript. this feature needs a recent browser supporting webgl and is quite cpu consuming.

scalability and high availability
the search engine component supports index sharding, i.e. having multiple parts of an index located on different servers but managed as a single, large, index.

the other components are stand-alone components using the search engine to collect the required data.

each component  can be scaled and extended on multiple servers to reach expected dimension/load. we could have, for example,  <dig> search engine shards  installed on  <dig> different servers and a single browser  collocated with  <dig> storage system . if the system were to manage additional data after a few months and were in need of additional servers, an additional search engine  server could be added and/or an additional storage backend  with no breakage on current installation.

the search engine shards can be scaled  to add new data on new shards, with no impact on existing shards. one shard  will query the other shards and merge the results. each shard can be an index handler.

the data browser /visualization components are independent, as they hold no data/visualization, and can be scaled to face load requests .

the storage backend supports a very large quantity of data on a single server , and adding new servers can increase its capacity. the data will automatically be dispatched and replicated, with backend support for node failure and multi-master nodes.

to provide high availability and dispatch the requests to the components, a web load balancer can be set in front of the other servers, balanced per component.

RESULTS
the software is successfully installed and used on the genouest platform, in a private cloud, hosted on  <dig> virtual servers . several public data banks  are indexed for more than  <dig> millions of records.

the current implementation manages  <dig> index shards,  <dig> web dispatcher and  <dig> server combining storage backend and genome browser.

we have also successfully tested the high availability with duplication of the servers/components and a load balancer.

this high availability test was made with 3 +  <dig> servers hosting the index shards, 1 +  <dig> servers hosting the storage backend components with the genome browser software, and one web load balancer. this architecture reduces node failure impact and increases load support.

documents update, after a remote bank update for example, is automatically managed with the biomaj  <cit>  tool, providing a seamless update mechanism.

the software has been integrated with other tools  to automate the extraction of additional meta-data from blast results, or to extract the nucleic sequence of a specific id  in the mobyle portal  <cit> .

query time can take from  <dig> to  <dig> seconds according to the query and the index size. a cache mechanism helps to reduce response times. each system has to be optimally designed according to the volume of data to manage.

indexing time also depends on the volume of data. on our platform, using a cluster to parallelize indexing tasks, we index uniprot/swiss-prot files in 2 h <dig> 

future developments
expected future developments will add an ontology layer to the data. it is expected to link meta-data and ontologies to be able to extract subsets of data. a query like “get all chromosome identifiers for the species fish” would match all fish sub-elements in the species ontology. users of the software will have the possibility of extracting custom banks with a more accurate selection than the current one.

we also expect to add a das   <cit>  interface to the system to enable das compatible tools queries.

CONCLUSIONS
seqcrawler provides a free and open source solution to meta-data storage and search. while some other solutions on specific web sites will be more accurate, because it is focused on a specific set of data, seqcrawler offers a flexible solution to locally manage the data, with no restriction on data type and data mixing.

the components of the software are modular, i.e. are component-optional and can be inactivated if not used. additional components can be introduced to extend the system for other usages; one just needs to query the search engine to extract the meta-data matching one’s criteria.

the system is customizable to fit other requirements, and the plug-in mechanism of the indexer eases the addition of new file formats.

limitations, from a hardware point of view, are the disk requirements. index and backend storage require a substantial amount of disk space in addition to the original data. though, with the sharding support and the ring architecture, data can be split to remove single server disk space restrictions.

the software can scale from single user and small data, up to multi-user and larger data for large organizations. it can also easily be used in a cloud with its packaged installation .

availability and requirements
the seqcrawler software is freely available for download with installation instructions at http://seqcrawler.sourceforge.net

· project name: seqcrawler

· project home page: seqcrawler.sourceforge.net

· operating system: platform independent

· programming language: java/javascript

· other requirements: java  <dig>  or higher, tomcat  <dig>  or higher

· license: cecill

· any restrictions to use by non-academics: none

· demo: http://seqcrawler.genouest.org

while the software depends on multiple components, not always easy to install, a deb package is available for debian/ubuntu distributions. in case of missing dependency, dependencies are available on debian mirrors. the package is installed with a sample bank for immediate testing.

complete installation is detailed on the project web site in the installation section.

abbreviations
ajax: asynchronous javascript and xml. techniques used in web programming for asynchronous requests and rendering; api: application programming interface ; das: distributed annotation system ; dbi: database interface, i.e. driver to database; gmod: generic model organism database project; rest: representational state transfer . all requests are available with http get requests and url parameters; webgl: web based graphics library, javascript library to use the computer display card's graphics processing unit .

competing interests
the authors declare that they have no competing interests.

authors’ contributions
os developed the software and packaged it. ar installed the infrastructure and made all the system installation on the genouest platform. ab helped on the design of the web interface. all authors read and approved the final manuscript.

