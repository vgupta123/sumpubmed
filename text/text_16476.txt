BACKGROUND
one vision of biodiversity informatics is that of a cloud of digital records representing objects and events such as images, specimens, macro-molecular sequences, phenotypes, observations, publications, and taxonomic names. each digital record would carry a globally unique identifier that would both identify that object and, given appropriate technology, be used to retrieve what we know about that object, including how it is linked to other objects. implementing this vision, which is essentially that of linked data  <cit> , requires services that can mint, resolve, and discover identifiers  <cit> .

a minting service creates identifiers, and ensures their uniqueness. given an identifier we need a resolution service that can retrieve the object identified . this service may return information in multiple formats, such as binary data , or metadata about the object. lastly, if we don't have an identifier for an object it should be straightforward to discover if one has already been minted.

to illustrate these services, consider digital object identifiers , widely used by the academic publishing industry to identify articles. a doi has two parts, a naming authority assigned centrally by crossref  <cit>  , and the local identifier, assigned by the publisher themselves . because the naming authority is assigned centrally, no two publishers will have the same naming authority, and hence no two publishers can assign the same doi to a different article. publishers are free to use whatever local identifier they find most useful to identify their articles, such as a primary key in a local database, or an identifier generated from metadata about the article  <cit> . a doi can be resolved in a number of ways. a user can append a doi to a http resolver such as  and they will be taken to a document . given a doi one can also retrieve metadata for the corresponding article  using a service provided by crossref,. third-party tools can make use of this service, examples include the social bookmarking services connotea  <cit>  and citeulike  <cit> . a user of the connotea web site need only enter a doi and connotea retrieves the metadata from crossref, sparing the user the need to manually enter bibliographic details.

crossref also provides an openurl resolver  that takes user-supplied metadata  and returns a doi, if it exists. publishers can use this service to find dois for articles in the "literature cited" section of an author's manuscript, and hence when the manuscript is published online it will contain electronic links to the literature cited in that manuscript.

minting identifiers
ensuring an identifier is unique needs some care. typically identifiers are unique within some scope, such as a local database, or a particular discipline. however, once one moves outside that scope, we can have unintended collisions between identifiers. as an example, the paper by mesibov  <cit>  contains strings that match existing genbank accession numbers, such as dq <dig> . however, in the context of this paper, dq <dig> is a utm grid reference for a locality in tasmania with the co-ordinates 41° 26' 31" s, 146° 17' 02" e. clearly, within the context of  <cit> dq <dig> is not intended to be interpreted as a genbank accession number.

namespaces
collisions between identifiers can be minimised using namespaces - agreed prefixes that specify the scope within which an identifier is to be interpreted. these namespaces may be centrally assigned, for example the naming authority in a doi, fig.  <dig>  or by community convention such as the life science record name  project  <cit>  and the "info" uri scheme  <cit> . the uniqueness of identifiers such as life science identifiers   <cit>  and http uris is in part guaranteed by the use of internet domain names, which are globally unique. for example, a lsid  includes an authority component that is a domain name that can be resolved by the internet dns . by using the existing dns infrastructure, lsids avoid the need to set up a new central naming authority.

generating identifiers from metadata
one approach to minting identifiers is to generate them based on metadata for the object, which has the advantage that, in theory, two different people acting independently of each other will generate the same identifier, obviating the need for a central agency to mint the identifier. this also greatly simplifies identifier discovery - the identifier can be generated from the object at hand.

a rather baroque example of this approach is the serial item contribution identifier   <cit> , which generates an identifier for a journal article from the journal's issn, date of publication, volume, issue, starting page, and article title, and includes various control characters and a checksum. cameron's  <cit>  journal article citation convention  is a rather simpler example of this approach. for a given journal article the jacc identifier is of the form <journal identifier>:<volume>@<starting page>, where typically the <journal identifier> is the issn of the journal. figure  <dig> shows an example of a journal article and the jacc generated from information on the first page. given that each issn is unique, most sici's and jacc's will be unique , although complications arise if two articles start on the same page, or if a journal's pagination doesn't span a volume .

uuids
another approach to generating identifiers is to use unique strings such as uuids  <cit>  which can be generated completely independently, with a very low probability that the same uuid will be generated more than once. such a system is attractive if identifiers need to be coined independently of any central agency . it may also be an issue for projects that aggregate information from a range of sources, each source of which may mint it's own guids. using uuids should ensure that the guids are, actually, unique. the catalogue of life  <cit>  adopted uuids for its lsids for this reason  <cit>  .

uuids are opaque identifiers, that is, they that contains no information about the object it identifies. in this sense it is the antithesis of an identifier such as a sici or jacc, which embed detailed bibliographic metadata.

resolving identifiers
identifiers by themselves have limited utility unless they can be resolved, that is, given an identifier we should be able to retrieve information about the object the identifier refers to. in practice, resolution means that we can retrieve information about the object from the web. for identifiers such as http uris, this is straightforward , but for other identifiers we need a resolution mechanism.

lsids
lsids are the identifier recommend by the biodiversity information standards  organisation  <cit> . for the biodiversity informatics community the attractions of lsids include the distributed nature of the identifier , the low cost, and the convention that resolving a lsid returns metadata in rdf. the later facilitates integrating information from multiple sources using tools being developed for the semantic web  <cit> .

despite being specifically developed to provide globally unique identifiers for objects in biological databases  <cit> , within mainstream bioinformatics relatively few "early adopters" have deployed lsids  <cit> . in part this may because of the complexity of the resolution mechanism. a lsid client resolves a lsid in four steps:

• find location of lsid resolution service by querying the dns service  records to find the hostname and tcp/ip service port for the lsid authority

• retrieve from the lsid authority the wsdl that defines the lsid resolution service

• retrieve a second wsdl file  that specifies how the metadata and/or data corresponding to the lsid can be retrieved

• retrieve the metadata , typically using http get

not only is resolving a lsid more complicated than resolving a http uri, setting up a lsid resolution service is non-trivial.

furthermore, although lsids are conceptually rooted in the semantic web in the sense that the default metadata format is rdf, current approaches to realising the semantic web  have settled on using http uris as the identifier. using http uris to identify both real world objects and web pages has the potential to cause ambiguity - if i use "" as an identifier, am i talking about the city in scotland, or the web page with that url? the linked data community has adopted the use of http  <dig> redirects and content negotiation to distinguish between a resource and a document that describes that resource  <cit> . a client resolving a uri  will receive a http  <dig>  redirect, which tells the client that  identifies a non-information resource , and it will also receive a location for a document that describes the resource . enabling lsids to comply with linked data approaches requires a resolver that supports this mechanism.

discovering identifiers
the activities of minting and resolving identifiers tend to receive more attention than discovering existing identifiers. however, if a major goal of biodiversity informatics is to integrate biodiversity resources then data providers need to re-use shared global identifiers wherever possible  <cit> , rather than simply mint new identifiers. having multiple identifiers for the same object is potentially a major obstacle to integrating data, hence we need services that can discover whether an identifier already exists for an object. perhaps the most obvious domain where this is relevant is literature databases, where publishers, digital repositories , institutional archives , indexing services , and domain-specific databases may all assigned one or more identifiers to a scientific publication. one method the digital library community has developed to retrieve information about a bibliographic item is openurl  <cit> .

openurl
as originally conceived , openurl is a http get syntax "for transporting metadata and identifiers from an information service to a linking server"  <cit> . figure  <dig> shows a simple openurl that has two components, the referenced resource  and the openurl linking server or resolver . the openurl may also contain a "service identifier"  that identifies the source of the openurl. in essence, an openurl is a set of key-value pairs that describe a resource. an openurl resolver takes this url and returns what information  it has about that resource. whereas openurl  <dig>  is fairly simple , the official standard is openurl  <dig>   <cit> , which has a frankly byzantine syntax. although the construction of an openurl is defined in excrutiating detail in the standards document, that standard says nothing about what the openurl resolver should return. most return a web page, that is, they assume that the request is being made by a human. crossref is a notable exception in optionally returning xml.

one reason for the standard's complexity is its attempt to be highly generic, and thus applicable outside the library community. for example, chute and van de sompel  <cit>  use openurl to request regions  from a jpeg  <dig> image. we could also use openurl to request metadata for a specimen, or indeed any other object of interest.

some of the complexity in the openurl standard reflects an emphasis in the digital library community on providing the "appropriate copy"  <cit> , for example, a copy that the user  has the right to access. most openurl resolvers are local in scope , and will return web pages telling the user if an item exists in the library . while such a service may be locally useful, if we assume that a user simply wants access to the resource , and doesn't care about where it resides , then the practical utility of many openurl resolvers is somewhat limited.

coins
the modular nature of an openurl means that we can swap one openurl resolver for another, but keep the key-value pairs intact. openurl  <dig>  makes this explicit with its notion of a "context object" . the context object in span  technique  <cit>  exploits this modularity by encoding the context object in a <span> tag in a html web page so that coins-aware tools can parse the html and add functionality to the web page. examples of such tools are the firefox browser  <cit>  add-ons openurl referrer and zotero . openurl referrer  <cit>  extracts coins from a web page and converts them into clickable links to a user-specified openurl resolver. zotero  <cit>  extracts bibliographic metadata from the coins to help populate the user's bibliographic database.

page-level identifiers
discovering identifiers from metadata can become challenging if the items of metadata associated with the identifier in a database differs from the metadata actually available. for example, many taxonomic citations are not to articles or books  but to an individual page. this mismatch in granularity can frustrate efforts to link identifiers for taxonomic names to identifiers for literature. hence, it would be desirable to have a service that can return the containing document for a given page.

to illustrate, the index fungorum database record for hyaloperonospora galligena  göker, riethm., voglmayr, weiss & oberw.  <dig>  gives the bibliographic source as "mycol. progr. 3:  <dig> ". there is no article in volume  <dig> of mycological progress that starts on page  <dig>  so a standard search for a doi using crossref's openurl resolver will fail. however, if we repeat the search, each time decreasing the start page by one, we will retrieve a document , which starts on page  <dig> and ends on page  <dig>  this article contains page  <dig>  and so we can now link the identifier for the name hyaloperonospora galligena  göker, riethm., voglmayr, weiss & oberw.  <dig>  to the identifier for the publication .

bioguid
this paper provides a brief description of bioguid , which implements a set of services for resolving, discovering, and minting identifiers. the initial design and development of this site is described on the bioguid blog  <cit> , however the version of bioguid described in this paper differs in several ways, including support for openurl  <dig> , and linked data-compliant resolution of lsids.

implementation
bioguid is written in the php programming language, and the source code is available from . the lsid resolution code comes from the lsid tester project  <cit> . other third-part libraries used include the adodb database abstraction library  <cit> , and the pear net_dns module  <cit> .

doi resolution uses crossref's openurl resolver. article metadata is cached locally in a mysql database to minimise requests to external services, and to facilitate locating articles based on an individual page. the mysql database also contains metadata for articles that don't have dois but which are available online, such as those in dspace repositories  <cit>  that support oai-pmh harvesting  <cit> .

RESULTS
bioguid  implements a range of services, the core ones being an openurl resolver, and a lsid resolver. additional services include journal issn look-up, author name matching, and monitoring the status of biodiversity data providers.

openurl resolver
the openurl resolver can be accessed directly by appending the openurl context object to . by default the resolver returns a html web page, but setting the parameter display to rdf or json instructs the resolver to return rdf or json, respectively, making it easy to use the resolver as a web service. if the user doesn't append the context object, then  displays web forms where the user can enter bibliographic metadata that describes the article .

minting identifiers
for journal articles the bioguid openurl resolver will generate a jacc for an article, provided that sufficient metadata  are available. this provides a globally unique identifier for an article, and in the absence of an existing doi, pubmed number, or url, it may be the only available guid for that article.

resolver
bioguid can act as a resolver for several different identifiers by appending the identifier  to the base url . for example, the jacc 1175-5326:1671@ <dig> becomes  <cit> , and the doi  <dig> /bib/bbn <dig> becomes . for these identifiers bioguid simply uses the apache web server's mod_rewrite to rewrite the urls to openurls.

lsids
to resolve lsids clients simply append the lsid to . the resolver supports http  <dig> redirects and content negotiation  <cit>  . clients requesting rdf will receive the raw metadata from the corresponding lsid authority, clients requesting html will receive a simple web page displaying a human-readable version of the metadata .

other services
in addition to the core services listed above, bioguid provides additional  services.

journal issn lookup
bioguid has a local database of journal names and issn numbers. a user can lookup a issn for a journal name by appending the journal name  to the url . this service returns a list of titles that match the request, together with their issns, in json format. the bioguid openurl resolver web page uses this service to find the issn of a journal the user has entered.

author name matching
bioguid's article cache includes author names, unfortunately for any given author there may be more than one way their name has been recorded in various bibliographic databases. for example, my name may be stored as "roderic d. m. page" or "r. d. m. page". as a first step towards normalising author names bioguid implements feitelson's  <cit>  weighted clique algorithm for finding equivalent names. this service takes a set of forenames  and returns a set of names that can regarded as equivalent. names can be entered in a web form at , or the service can be called directly by sending a http post request to the url  with a parameter names whose value is a list of author names , and an optional parameter format with the value html or json.

service availability
the vision of a linked web of biodiversity data assumes that all biodiversity data of interest is available online. much of it isn't, but even that which is online might not always be available. a constant frustration during the course of developing bioguid  has the limited reliability of some data providers. inspired by david vieglais's the big dig  <cit> ,  is a simple monitoring service that every hour polls a number of web sites, lsid authorities, and digir providers by sending a simple http get request and recording the http status code and the time taken to respond to the request. for example, fig.  <dig> shows the status of the index fungorum lsid authority and the doi http proxy in the month of march  <dig>  in the first few days of that month the index fungorum service frequently timed out, before becoming more stable as the month progressed. in contrast, the doi proxy was consistently online over the same period.

CONCLUSIONS
the lack of standard uris for biodiversity data objects reflects a broader lack of agreement on this issue  <cit> . it is likely that http uris will become broadly adopted, at least outside biodiversity informatics, and they are at the heart of linked data  <cit> . however, http uris have their own problems. we are currently faced by either a great dearth of uris or, ironically, an over abundance of them. if an entity is shared across multiple domains, then there may be multiple, competing uris for the that entity. for example, there are numerous web sites that make statements about individual books, often using uris that embody an isbn. in such cases there often is not an obvious reason to choose between any of the uris. in the same way, we have multiple identifiers for articles . in such cases, tools such as openurl may have a role, in that the openurl context object can contain an identifier as one of its kev-value pairs. hence, we could use the context object to encode this information, but delegate the choice of resolver to the client.

in cases where these is an obvious canonical source for information about an object, and that source issues http uris, it would make sense to use those uris. museum specimens would seem to be an obvious case . however, there are few such uris available. i regard the lack of uris for individual specimens is one of the greatest obstacles to progress in data integration in biodiversity informatics. again, in the absence of a recognised identifier one could adopt the openurl approach of encoding sufficient metadata to enable some services to retrieve a digital record about the specimen, if and when it becomes available.

bioguid is being developed to address some of these issues, in that it supports openurl for literature , and can resolve non http uri identifiers  following linked data guidelines. these services can be accessed with a web browser, or programmatically. for example, the basis of my entry  <cit>  in the elsevier grand challenge  <cit>  was a database populated by harvesting data on literature, specimens, and genbank using bioguid's openurl resolver. having tools such as bioguid may help mobilise biodiversity data that is currently digitised but not easily accessible, and thus bring the goal of a linked web of biodiversity data a little closer to being realised.

availability and requirements
• project name: bioguid

• project home page: . source code is available from .

• operating system: the bioguid web site is usable with any modern web browser. the source code can be easily installed on a mac os x, linux server. it has not been tested on a windows machine.

• programming language: php

• other requirements: web server

• license: gnu general public license version 2

• any restrictions to use by non-academics: none

list of abbreviations used
doi: digital object identifier; dns: domain name service; gbif: global biodiversity informatics facility; guid: globally unique identifier; isbn: international standard book number; issn: international standard serial number; json: javascript object notation; lsid: life science identifier; rdf: resource description format; sici: serial item and contribution identifier; tdwg: taxonomic databases working group; uri: uniform resource identifier; urn: uniform resource name; uuid: universally unique identifier; wsdl: web service definition language

competing interests
the author declares that he has no competing interests.

