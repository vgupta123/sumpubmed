BACKGROUND
multiple sequence alignment , the simultaneous alignment among three or more nucleotide or amino acid sequences, is one of the most essential tools in molecular biology. the goal of multiple sequence alignment is to align sequences according to their evolutionary relationships. for small lengths and small numbers of sequences, it is possible to create the alignment manually. however, efficient algorithms are essential for good alignments with more than eight sequences  <cit> . the existing algorithms can be classified into three main categories, exact, progressive and iterative. the iterative approaches can be of two types: iterative, and iterative plus stochastic.

the dynamic programming  approach  <cit>  is good at finding the optimal alignment for two sequences. however, the complexity of this method grows significantly for three or more sequences  <cit> . we must mention here that msa is a well-known combinatorial problem  where the computational effort becomes prohibitive with a large number of sequences  <cit> .

the progressive alignment algorithm , proposed by feng and doolittle  <cit>  iteratively utilizes the method of needleman and wunsch  <cit>  in order to obtain an msa and to construct an evolutionary tree to depict the relationship between sequences. a good number of global alignment algorithms, based on the progressive alignment method, have been proposed, such as multalign  <cit> , multal  <cit> , pileup  <cit>  and clustalx  <cit> . multal generates the final alignments by aligning closest sequences subsequently. multalign and pileup make the final alignments from the guide tree , which is constructed using upgma  <cit> . clustal w  <cit> , based on a progressive approach, is a global method. the clustal w builds up the final alignments from a guide tree, which is calculated by a neighbour-joining algorithm  <cit> . clustal w uses the weighted sum of pair score, which considers sequence weighting and position dependent gap penalties. although this approach is successful in a wide variety of cases, it suffers from its greediness  <cit> . the clustal series  were developed based on the same algorithm. the alignments produced by these programs are exactly the same; the only difference among them is the way the user interacts with the program. pima  <cit>  uses local dynamic programming to align only the most conserved motifs. it offers two alignments, mlpima and sbpima. t-coffee  <cit>  is a sensitive progressive alignment algorithm which combines information from both global and local alignments. this method is fast but there is a possibility to be trapped at a local minima. the difficulty with the progressive approach is that they usually converge to local optima  <cit> . to overcome such a limitation, it is recommended to use an iterative or stochastic procedure  <cit> .

the iterative approach starts with an initial solution and then the current solution is improved using iterative steps. muscle  <cit>  is based on a progressive and iterative algorithm. it has three stages: draft progressive, improve progressive, and refinement. in each stage, a multiple sequence alignment is generated. similarly mafft  <cit>  is also based on a progressive and iterative algorithm. it uses a fast fourier transform  to identify homologous regions. to evaluate the multiple sequence alignments, mafft uses the clustal w scoring system. probcons  <cit>  is a probabilistic and consistency based algorithm. it computes posterior-probability matrices and expected accuracies for each pairwise comparison. it also computes an expected accuracy guide tree to progressively generate a final alignment by applying a probabilistic consistency transformation. probcons achieves more accurate results than muscle and mafft, but is slower than those algorithms  <cit> . prrp  <cit>  is another global alignments program which is based on a progressive approach. this approach is robust, but it does not guarantee optimum solutions  <cit> . dialign  <cit>  uses a local alignment approach based on a segment to segment comparison, rather than on a residue to residue comparison. this method is successful in highly conserved flanking core blocks, but is unreliable outside the conserved motifs.

there are some iterative and stochastic approaches for msas . hmmt  <cit> , based on a simulated annealing method, maximizes the probability for sequence alignment where the solution could be trapped in local optima  <cit> . evolutionary algorithms  are population based stochastic global search algorithms. ea starts with an initial population of individual solutions. different eas use different representations  for the individuals and different reproduction operators  to generate offspring for the next generation. the main driving force in eas is the selection of individuals based on their fitness . individuals with higher fitness have a higher probability to be chosen as members of the population of the next generation . this corresponds to the principle of survival of the fittest in natural evolution. there has been a variety of different eas proposed over the years, such as evolution strategies , evolutionary programming , genetic algorithms  and their variants. gas, the most well known algorithm in the ea family, have been successfully used for both numerical and combinatorial optimization. when using eas for msa, an initial seed is generated by a progressive alignment method, and then the steps of an ea are applied to improve the similarities among the sequences. for example, msa-ea  <cit>  improves the solution of the clustal v  <cit>  algorithm by initially generating one seed with clustal v. this method works well when there are a large number of fully matched blocks, but performs poorly when there are only a few fully matched blocks  <cit> .

there are other genetic algorithm  based methods, such as saga  <cit> , msa-ec  <cit> , ga-aco  <cit> , msa-ga  <cit>  and rbt-ga  <cit> . in saga, the initial generation is generated randomly with gap  symbols inserted randomly inside the sequences to make them equal in length. in this algorithm,  <dig> different operators are used to gradually improve the fitness of the msa. these operators are dynamically scheduled during the evolution process. the time complexity of saga is large, mainly due to the time required by the repeated use of the fitness function  <cit> . shyu et al.,  <cit>  proposed two other approaches using gas. in the first approach, ga was used to evolve an optimal guide tree which was created with the neighbor-joining method. shyu's second approach facilitates the optimization of a consensus sequence with a ga by using a vertically scalable encoding scheme, in which the number of iterations needed to find an optimal solution is approximately the same regardless of the number of sequences being aligned. another algorithm, ga-aco  <cit> , combines ant colony optimization with ga to overcome the problem of becoming trapped in local optimum. to do this, first ga is run with a randomly generated initial population . its crossover operator then produces one or two offspring  <cit>  from two parents, and a mutation operator provides another possible variation of the alignments. finally, aco  was applied on the best alignment of the ga approach. msa-ga is a simple ga based method with a different scoring function. to test this algorithm, the authors performed two sets of five runs for each of  <dig> test cases from the balibase  <dig>   <cit>  dataset. rbt-ga combines ga with the rubber band technique  to find optimal protein sequence alignments  <cit> . rbt is an iterative algorithm that uses a dp table. the authors  <cit>  solved  <dig> problems from references  <dig> and  <dig> of the benchmark balibase  <dig>  dataset. the experimental results showed that the overall performance of rbt-ga was better than the other methods compared in that paper. we must mention here that local search methods are sometimes integrated with gas  <cit>  to enhance the performance of gas in solving msa problems.

the eas have an important advantage over progressive methods in that the alignment component can be made independent of the objective function. this means that different fitness functions can be tested without making any adjustment to the alignment procedure, which makes them particularly attractive for testing new objective functions. another useful advantage of these methods is that the computational duration can be compressed by parallelization. these advantages motivate us to apply eas to solve msa problems in this research. in this paper, we propose a new approach based on a genetic algorithm, namely vdga, where we introduce a decomposition method to divide the sequences into smaller subsequences. these subsequences are then processed separately before being combined back into whole sequences. we have used the guide tree method to generate an initial population, in each separate part of the decomposition, and also during the mutation. the proposed method starts with the dp distance table. in the dp distance table, the distance between two sequences is calculated from a pairwise alignment using dynamic programming . we have used this distance table to generate a guide tree. we have applied and analyzed two techniques on the guide tree so as to generate an initial population. in running the ga, we have considered the weighted sum of pair score as the fitness measure, with the pam <dig>  <cit>  score matrix and the clustal w default gap penalties.

the performance of the proposed algorithm has been compared with the state of the art ga and non-ga based methods, namely saga, msa-ga, rbt-ga, prrp, clustalx, clustal w, dialign, hmmt, sb_pima, ml_pima, multalign and pileup <dig>  to allow us to compare with other methods, we have calculated the corresponding baliscore of the best wspm score. for comparison, the results of the  <dig> datasets solved by msa-ga, and the alignment results of the  <dig> datasets solved by rbt-ga were taken from the published papers  <cit>  and  <cit>  respectively. however, the results of the other methods mentioned above, were obtained from balibase  <dig>   <cit> . based on the calculated baliscores, vdga outperforms the ga and non-ga based methods mentioned earlier.

this paper is organized as follows. after this background, the next section describes briefly the steps of a basic genetic algorithm and the details of the proposed vdga method. the analyses of vdga and comparisons with other well known methods have been discussed in the results and discussion section. finally, we have summarised our work and planed for future work in the conclusions section.

methods
genetic algorithms: the basic structure
the general steps of the common ga can be summarized by the following pseudo-code:

 <dig>  initialisation: generate initial population

 <dig>  evaluation: evaluate the individuals using a fitness function

 <dig>  select individuals  and then

 <dig>  apply the genetic operators to them so as to create children

 <dig>  new generation: create new generation from some combination of old generation and new child generations.

 <dig>  go to  <dig> until it meets the stopping criteria.

 <dig>  end.

vdga: the proposed algorithm
the proposed vertical decomposition using genetic algorithm  is a modification of the basic ga. its steps are: generation of initial population, generation of child population by applying genetic operators, forming a new population for the next generation, vertical division, and the stopping criteria. the vertical division is the key concept of the proposed algorithm. this technique is applied after generating the first/initial generation and after generating child generation. to do this, inside the vertical division step, the alignment is separated into different parts, the null symbols are removed from each part and then the tree-base method is applied on each part. after applying the tree-base method, each part produces an alignment. the alignments of each part are then combined to create a new alignment. as the solution of the progressive alignment method  usually converges to a local optimum, therefore in the initial generation stage, we use the guide tree method to find the local optima and its neighbouring points in two ways; randomly generated subtree and shuffling. with the genetic operator , we also use the guide tree method. to calculate the guide tree, we use both dp  <cit>  and the kimura distance  <cit>  tables as discussed below.

distance calculation
dynamic distance calculation
for this, the dp distance of each pair is calculated using equation  from a pairwise alignment  <cit> . to construct the dp distance matrix , which shows the distance between all sequence pairs are calculated,

  dynamic distance=mismatch∕align length 

kimura protein distance calculation
equation  is an alternate means to calculate the distance, and was developed based on the relationship between the observed amino acid substitutions and the actual  substitutions from pam or blosum  <cit> . the match score is calculated by summing the number of exact matches. in this method, the partial matches between ambiguous symbols also contributes to the match score as fractional scores. the value of s is computed by dividing the match score by the number of positions scored. gap positions are ignored, and only exact matches contribute to the match score  <cit> .

  s=exact_matches∕positions_scoredd=1-sdistance=-ln1-d- <dig> d <dig> 

the flowchart of the proposed vdga method is shown in figure  <dig>  in this figure, the block arrows represent the steps of the algorithm and the black color arrows represent the use of guide trees inside the proposed algorithm. the steps of this method are explained below.

initial generation
the aim of this step is to generate good initial solutions. the flowchart for generating the initial population is shown in figure  <dig> and the stages are described below.

stage 1
vdga starts with a dp distance table. the guide tree is constructed from this table, which is referred to as tr <dig>  and we generate a multiple sequence alignment  from this guide tree.

stage 2
in the second stage, the distance table is calculated from the multiple sequence alignment , which is called the kimura distance table. the kimura distances are calculated from the aligned sequences. the second tree, tr <dig>  is constructed from the kimura distance table, and we then produce msa <dig> as shown in figure  <dig> 

stage 3
in this stage, two mechanisms are implemented on the two trees  to generate  <dig> different trees. the first mechanism is to generate guide trees with randomly selected sequences and the second is shuffling the sequences inside those trees. the initial population produced by this method contains a set of multiple sequence alignments. therefore, after receiving the set of guide trees, it needs to make a set of multiple sequence alignments. the functions of these mechanisms are explained below.

mechanism 1
in this case, sequence numbers are selected randomly from one tree . the selected sequences then make a new sub-tree with the same branching orders as the original one, and the non-selected sequences make another new sub-tree. lastly, these two sub-trees are connected together to make a new tree. figure  <dig> shows the behaviour of mechanism  <dig> 

mechanism 2
in this case, two sequence numbers are selected randomly from one tree . then, these two sequences exchange their positions to make a new tree. figure  <dig> shows the function of mechanism  <dig> 

fitness
the weighted sum of pair method  is commonly used as a fitness measure for msas. for it, each column in an alignment is scored by summing the product of the scores of each pair of symbols and their pair weight. the score of the entire alignment is then summed over all column scores by using equations  and .

  s= ∑l=1lslwhere,sl= ∑i=1n-1∑j=i+1nwijcost 

here, s is the cost of the multiple alignments. l is the length  of the alignment; sl is the cost of the l-th column of l length. n is the number of sequences, and wij is the weight of sequence i and j. in clustal w, the weight is calculated for each sequence and the pair weight is the product of the two sequence weights. cost is the alignment score between the two aligned sequences ai and aj. when ai ≠ '-' and aj ≠ '-' then cost is determined either from the pam  or blosum  <cit>  matrix. also when ai = '-' and aj = '-' then cost =  <dig>  finally, the cost function cost includes the sum of the substitution costs of the insertion/deletions when ai ≠ '-' and aj = '-' or ai = '-' and aj ≠ '-', using a model with affine gap penalties as shown in .

  g=g+nx 

here, g is the gap penalty, g is the cost of opening a gap, x is the cost of extending the gap by one and n is the length of the gap.

in clustal w, the author used different weight matrices, which depend on the estimated divergence of the sequences to be aligned at each stage, and proposed dynamically changeable gap penalties to overcome the local minima issue. therefore, in this research, the clustal w weighted scheme, the clustal w default gap penalties , and the pam <dig> matrix, a mutation probability matrix, were considered for the wspm fitness measure. note that pam <dig> is considered to be a good general matrix for protein database searching. also, the pam matrices have been developed based on global alignments. to calculate the weight of each sequence, we used the clustal w weight function.

to optimize the vdga, we have used both the sum of pair and the weighted sum of pair methods for the fitness function in our research. however, we have only reported the weighted sum of pair scores as the algorithm with this method performed better than with the sum of pair fitness function. this is because of the features  and the parameters used in our algorithm.

child generation
for each individual in the initial population, the wspm score is calculated, and the individuals are then sorted according to the descending order of their scores. to generate a child population of  <dig> individuals in any generation, the following three genetic operators are used.

i. single point crossover.

ii. multiple point crossovers.

iii. mutation.

the following sub-section 'selection of parameters' considers the relative proportion of when these operators are used.

1) single point crossover
in this crossover, one individual is selected from the top 50% and another from the bottom 50% of the parent generation. the single point crossover  <cit>  is implemented as shown in figure  <dig>  its procedure is that first a column position is selected randomly as shown with a "*" in figure  <dig>  the parent having the better score is then divided vertically at that column. let us assume that parent a has the better score column, so that this parent is separated vertically into two pieces. the second parent b is also divided into two pieces in such a way that each row of the first piece  has the same number of elements as the first piece  of the first parent. these pieces of these two parents are then exchanged and merged together to generate two new individuals as shown in figure  <dig>  however, only the better new individual is chosen to be a child.

2) multiple point crossovers
the parents' selection process is the same as for the single point crossover. for multiple point crossovers, each parent is divided into three pieces. these pieces are then exchanged between the parents and are then merged together to generate two new individuals. however, only the best one will be taken as a child. the crossover is implemented in two steps as described below.

step 1
to cut the first piece effectively, we compare the scores of the first 25% of columns for both parents. the parent having the better score is then divided vertically at that column. the other parent is also divided using the mechanism that was introduced in the single point crossover, as can be seen in figure  <dig> 

step 2
we now have two pieces of each parent from step  <dig>  to create another piece, we follow the same procedure of step  <dig>  but considering the last 25% of columns . this gives us three pieces for each parent. to complete the crossover, the middle pieces are exchanged between the parents, and then all three pieces are merged together to generate the two new individuals as shown in figure  <dig> 

in figure  <dig>  the lengths  of the two parents  are  <dig> and  <dig> respectively. the first 25% of the columns of the 1st parent are two and a half  columns. in that case, we considered  <dig> columns. in the first step, parent a has the better score in the first 25% columns. therefore, parent a is divided first and parent b is tailored according to parent a. after the division, we have two pieces from each parent . in the 2nd step, parent b has the better score in the last 25% of columns. therefore, parent b is divided first and parent a is tailored accordingly. this division provides two new pieces for each parent . next, two new individuals are generated by connecting the pieces as  and . from these two individuals, the better one is selected as a child.

3) mutation
one individual  is randomly selected from the whole population. from this msa, the distance among sequences are calculated and stored in a distance table. the new guide tree is constructed from this calculated distance. in the new guide tree, the sequence numbers are shuffled to find a better guide tree and the msa of the new guide tree is considered as a mutated child. this process is repeated until  <dig> sequential unsuccessful attempts occur. if this operator finds a better guide tree then it is considered as a new child, otherwise there is no effect on this generation. the procedure of mutation is shown in figure  <dig> 

4) elitism
this common ga approach is used, whereby the best solution is passed on unchanged to the next generation.

vertical division
for vertical division, we first separated  each alignment vertically into two or more sub-alignments as shown in figure  <dig>  after that, the null symbols "-" were removed from each decomposed part. then the guide tree method was applied in each decomposed part. therefore, we received a new alignment from each decomposed part, which may or may not be the same as the previous part. now all of the new decomposed parts are connected together. in this way we obtain a new alignment. if the new alignment is better than the previous one, then the new alignment is kept, rather than the previous alignment. the working stages of this process are illustrated in figure  <dig> 

the vertical division step is applied in two places inside the proposed algorithm: once after the initial generation, and then after each child generation. we do this so we can apply the vertical division technique on aligned sequences. this is due to the fact that if the sequences are of different lengths, the vertical decomposition of unaligned sequences may introduce either too many or too few residues in each decomposed part. therefore, it could be difficult for the vertical decomposition of an unaligned sequence to produce a good solution.

motivation for vertical division
it is known that smaller sequences can be aligned quickly with a high level of accuracy  <cit> . this motivates us to divide the longer sequences into smaller parts, align them separately, and then combine them to generate aligned complete sequences. we have also observed in aligned sequences that sometimes the residue  is almost trivially incorrectly aligned. however, for larger sequence lengths and for a large number of sequences, it is computationally expensive to improve the multiple sequence alignments by simply shuffling the null symbols for the entire length of the msa. therefore, we have proposed an alternative, but efficient option to improve the msa.

new generation
to form the new population, the best 50% of the combined parents and children are selected while ensuring that there is no duplication of the individuals. we must mention here that we have also studied other splits, such as 40- <dig>  and 60- <dig>  the study results showed that the 50- <dig> split outperforms the 40- <dig> and 60- <dig> splits with an average improvement of  <dig> % and  <dig> % respectively. therefore, we have chosen the 50- <dig> mix with the proposed vdga, which ensures a better balance between exploration and exploitation. the population size of  <dig> is chosen as it was used in saga  <cit> . moreover, we have experimented with other population sizes which are discussed in a later section. the process of forming a new generation is demonstrated in figure  <dig>  the new population is then considered as the parent population in the next generation and so is used to continue the evolution process of vdga.

termination condition
the best solution in each generation is recorded. if the best solution remains the same in  <dig> consecutive generations, the algorithm will be terminated. we have set this termination condition based on our experimental observations. we have tested our vdga algorithm for up to  <dig> generations after getting the best solution, and we have observed that the best solution was hardly changed and that the variation of the average solution per generation was also insignificant.

RESULTS
test datasets
in order to evaluate our proposed approach, we have solved a good number of test datasets from the benchmark balibase alignment database. the original balibase version  <dig>   <cit>  consists of  <dig> reference alignments with over  <dig> sequences. balibase version  <dig>   <cit>  is an improved version, which was extended from version  <dig> to have  <dig> reference alignments and over  <dig> sequences. balibase version  <dig>  contains eight reference sets. each reference set has a variety of alignment problems. reference  <dig> contains small numbers of equidistant sequences. the orphan or unrelated sequences are considered in reference  <dig>  reference  <dig> contains a pair of divergent subfamilies where the two groups are less than 25% identical. reference  <dig> contains long terminal extensions, and reference  <dig> contains large internal insertions and deletions. lastly, references from  <dig> to  <dig> contain test case problems where the sequences are repeated and the domains are inverted. the details of the datasets used for the experiments are given in the 'quality of solution' subsection.

experimental study
in this section, we have first analyzed the performance of the vertical decompositions with both the guide tree and the genetic algorithm. from that, we have proposed the appropriate number of decompositions with the proposed ga method. finally, we have compared our algorithm  with other well-known methods. in this research, we have analyzed our results based on  <dig> independent runs. in comparison, msa-ga and rbt-ga used  <dig> and  <dig> runs respectively.

vertical division/decomposition with guide-tree
initially, multiple sequence alignments were done without a ga but solely by the application of the guide tree method followed by the results of that being decomposed vertically into two, three and four parts, namely decomp_ <dig>  decomp_ <dig> and decomp_ <dig> respectively with the null symbols  being removed from each part. then we again applied the guide tree methods in each part of the decomposition. these alignments were evaluated by wspm and their corresponding baliscore were also calculated. both of these scores are reported in table  <dig>  in this case, we carried out experiments for  <dig> datasets from references  <dig> and  <dig> of the balibase   <cit>  datasets. we also performed a non-parametric statistical test, namely the wilcoxon signed rank test  <cit> , with respect to the wspm score and also baliscore as shown in table  <dig> 

comparing with the wspm solutions of the guide tree, it is observed from table  <dig> and table  <dig> that decomp_ <dig> was better in  <dig> test cases and found the same best results in  <dig> test cases out of  <dig> test cases. decomp_ <dig> found the same best results in  <dig> test case and successfully found better solutions in  <dig> test cases. moreover, decomp_ <dig> was better in  <dig> test cases and in two test cases it found the same best results. the non-parametric test shows that these vertical decompositions are significantly better than the guide tree.

on the other hand, comparing with the corresponding baliscore solutions of the guide tree, decomp_ <dig> was better in  <dig> test cases and found the same best results in  <dig> test cases, decomp_ <dig> was better in  <dig> test cases and found the same bestn results in one test case, decomp_ <dig> was also better in  <dig> test cases and in three test cases it found the same best results out of  <dig> test cases. the non-parametric test also shows the significant advantage of these vertical decompositions with guide tree.

from the experimental observation it is clear that the multiple sequence alignments using vertical decompositions perform better than that of the guide tree method in most of the test cases. in a very few cases, these techniques perform badly, or the same as, the guide tree. although in general decomp_ <dig> performed best, sometimes the other decompositions performed better in some cases. after these experimental and statistical analyses with respect to the objective function and benchmark scores, we can safely conclude that the vertical decompositions can play an important role in improving existing solutions, and there is a possibility to find a better multiple sequence alignment if the vertical decompositions are used with an evolutionary approach .

analysis of vertical decomposition with ga
in this section, we have first analyzed the selection of parameters for vdga and then discussed the appropriate number of decompositions. in some of the following analysis, the decomposition was kept inactive to judge the parameter's effect individually.

• selection of parameters
in the proposed vdga algorithm, we have used two basic search operators: crossover and mutation. in order to determine the probabilities of crossover and mutation, we have excluded the decompositions from ga and have carried out five different experiments , using ten randomly selected balibase datasets   <cit> . our ga with the 50%-crossover & 50%-mutation option obtained the best solutions for seven out of ten datasets, the 60%-crossover & 40%-mutation for two and 40%-crossover & 60%-mutation for one . the options 100%-crossover achieved the best solution in one test case. however, the option 100%-mutation did not achieve any best solution. the solutions obtained by the 50%-crossover & 50%-mutation for the other three datasets were close to the best scores. the ga with 50% crossover & 50% mutation  achieved an average improvement of  <dig> % over the first mix,  <dig> % over the second mix,  <dig> % over the fourth mix and  <dig> % over the fifth mix. from the experimental performance, we have decided to use 50% probabilities of crossover and mutation with vdga. for the 50% crossovers, single point crossover is selected for half of them and double point crossover is used for the other half. these selections were also decided based on experimental analysis.

to form the child population from the parent population, we have divided the population into two groups based on their fitness values. then one individual is selected from the top 50% and another from the bottom 50% for crossover. however, the new population is formed by taking the best individuals from the combined previous parents and children.

we have chosen the population size of  <dig> as used in saga  <cit> . however, we have run experiments using the population sizes  <dig>   <dig> and  <dig> with 50% crossover & 50% mutation. the performance of vdga with a population size  <dig> is significantly better than that of with a population size  <dig> and shows no significant difference with a population size  <dig> 

• effect of operators and initial population
the proposed genetic algorithm, vdga, uses an improved initial population and new genetic operators that contribute to it performing better than other algorithms. to analyze the effect of these two components on the algorithm's performance, we have excluded the decomposition part from our algorithm and tested two sets of new experiments. in the first set, ga was run with a randomly generated initial population , and the second set used a hill climbing approach  starting from the improved initial population. wspm was used as the fitness measure. based on the corresponding baliscore of the best found wspm solution, the full algorithm achieved an average improvement of  <dig> % compared to the same with the randomly generated initial population and  <dig> % compared to the hill climbing approach. the first set of experiments thus proved the superiority of our proposed initial population, and the second set demonstrated the strength of our proposed genetic search operators.

• staging of decomposition with vdga
to determine the appropriate stage for using decomposition inside vdga, we have carried out experiments on eight datasets from balibase. we have tested our algorithm using decompositions in three cases. in the first case, the decompositions were used only after the initial generation, in the second case only after each child generation and finally after all generations. for decomp_ <dig>  the vdga with the third case improved the average solution by  <dig> % better than the first case and  <dig> % better than the second case. for decomp_ <dig>  the vdga with the third case improved the average solution of  <dig> % compared to the first case and  <dig> % compared to the second case. moreover, decomp_ <dig> with the third case improved the average solution by  <dig> % more than the first case but the second case performed better than the third case. although vdga with the third case for decomposition  <dig>  did not perform better than with the second case, with other decompositions  vdga achieved much better average performance when the decompositions were used after all  generations, in comparison to the first and second cases. from these experimental observations, we have decided to use the decomposition technique after all generations.

• performance of decompositions with ga
in order to judge the performance of the vertical division with ga, we have also performed experiments for  <dig> datasets from balibase  <dig> , where we have considered the ga and also the vdga  algorithms. the algorithms were each executed for  <dig> independent runs. for each dataset, the best and average wspm scores out of the  <dig> runs were recorded, and the corresponding baliscore of the best found wspm solution was also calculated. the best and average wspm scores and the corresponding baliscore are reported in table  <dig>  to compare with other results and algorithms, we have carried out non-parametric statistical testing with the wilcoxon signed rank test with respect to the best wspm score, the average score and also with the corresponding baliscore as shown in table  <dig> 

a. based on negative ranks.

b. based on positive ranks.

comparing with ga in terms of the best wspm score, vdga with decomp_ <dig> was better in  <dig>  and worse in  <dig> test cases as shown in table  <dig> and table  <dig>  the z value  and the significant test result  in table  <dig> show that vdga with decomp_ <dig> is significantly better than ga. vdga with decomp_ <dig> found better solution in  <dig> test cases, and the statistical test results  prove that the vdga with decomp_ <dig> is also significantly better than ga. moreover, vdga with decomp_ <dig> is also significantly better than ga. in doing so it found better solutions in  <dig> test cases and worse in  <dig> test cases. among these three decompositions, vdga with decomp_ <dig> successfully found better msas in a maximum of  <dig> test cases out of  <dig> 

when considering the average wspm score, the vdga with all three decompositions  are significantly better than ga. in doing so vdga with decomp_ <dig> was better in  <dig> test cases, decomp_ <dig> was better in  <dig> and decomp_ <dig> was in  <dig>  in this comparison, vdga with decomp_ <dig> successfully found better solutions in a maximum number of test cases in comparison to the other two decompositions. the statistical test results in table  <dig> show that vdga with decomp_ <dig> is not significantly different than vdga with decomp_ <dig> 

also comparing with the corresponding baliscore of the best finding wspm solution, vdga with decomp_ <dig> was better in  <dig> test cases, decomp_ <dig> was in  <dig> and decom_ <dig> was in  <dig>  the significance test in table  <dig> shows that vdga with decomp_ <dig> and also with decomp_ <dig> are not significantly different than ga. however, vdga with decomp_ <dig> is significantly better than ga. moreover, vdga with decomp_ <dig> and with decomp_ <dig> are not significantly different. vdga with decomp_ <dig> is significantly better than the two other decompositions.

from the above experimental results and their statistical analyses, it is clear that vdga with decomp_ <dig> found better solutions in more test cases than the other two decompositions and ga, and it is significantly better than ga and vdga with the other two decompositions with respect to the best and average wspm score and the corresponding baliscore. as we have considered test sequences with length of up to  <dig> in this research, we considered that the appropriate number of divisions for vdga is  <dig>  for sequences up to  <dig> length. further experiments would be required to determine the best setting for longer sequences.

• computational effort and convergence
the computational time required for finding good multiple sequence alignments is dependent on the sequence length, the number of sequences, and the similarities of the sequences. in addition, the choice of algorithmic parameters also plays an important role. we have tried to develop a relationship between the computational time required  and the sequence length and sequence numbers. however, it is hard to make any firm conclusion based on linear/nonlinear regression analysis.

to show the convergence behavior of our algorithm , we have plotted the best and the average wspm scores against the number of generations. as examples, three such plots  for three datasets from reference  <dig> are presented in figure  <dig>  these graphs show that our algorithm improved both the best and the average scores very rapidly at the initial stage of the search process and that the best score then converged to a solution. this is the type of pattern we expect from good search algorithms. as of the plots, although the average scores do not converge, the rate of improvement for the best score in the later generations of the algorithm is insignificant.

quality of solutions
to judge the quality of the solutions produced by our algorithm, we have considered only those benchmark datasets and algorithms that were considered in the papers reporting msa-ga and rbt-ga, and that used baliscore  to measure the accuracy of the solutions. the authors of msa-ga considered the best solution of five runs for each dataset and reported the baliscore. moreover, the authors of rbt-ga also reported the best solution of ten runs with baliscore. in our algorithm, we considered ten independent runs of each dataset and have used the corresponding baliscore of the best found wspm solution. baliscore scores a solution  between  <dig>  and  <dig> . if the solution is identical with the corresponding manually created reference alignment then the score is  <dig> . if nothing matches with the reference alignment then the score is  <dig> . however, if some parts match with the reference alignment, then the score is in between  <dig>  and  <dig> .

in msa-ga, the authors considered  <dig> test datasets from references  <dig> to  <dig> and reference  <dig>  among them,  <dig> datasets were from reference  <dig> and two were from each of the other reference datasets. however, currently baliscore does not work for reference set  <dig>  this is because of insufficient information supplied either by the reference alignment file or by the annotation file. therefore, we excluded the two datasets of reference  <dig>  thus leaving  <dig> for comparison. in rbt-ga, the author considered all  <dig> test datasets of reference  <dig>  and  <dig> out of  <dig> from reference  <dig>  in total, we considered  <dig> test datasets, including  <dig> from reference  <dig>   <dig>  from reference  <dig>   <dig> from reference  <dig>  and  <dig> from each of references  <dig> and  <dig>  all these datasets belong to the balibase  <dig>  benchmark datasets.

problem solving with vdga_decomp_3
for each of the  <dig> datasets, we have executed our algorithm for  <dig> independent runs and recorded the best, worst, and average wspm scores with standard deviation, and the corresponding baliscore of the best wspm score in table  <dig>  the wspm scores could be either positive or negative, as it depends on the level of similarity among the residues in the sequences. this is because, if the residues among the comparable sequences are similar, or partially similar, it needs a small number of null  symbols to make an alignment of the sequences. in this case, the wspm score of this alignment is positive. on the other hand, if the dissimilar parts among the sequences are high, a large number of null symbols are added to the alignment. in this case, the wspm score becomes negative because of gap penalties. note that high positive values and low negative values are considered as good scores. we must also mention here that the average scores in the  <dig> runs were not very different and hence the standard deviations were small.

for comparisons with other methods, we have taken from the published literature  <cit>  the benchmark baliscore results of those methods. the authors gondro and kinghorn  in msa-ga  <cit> , taheri and zomaya,  in rbt-ga  <cit> , and bahr et al.  in balibase  <cit>  reported only sps scores for comparisons. therefore, for them we have compared only the sps scores. however, we also recorded the cs score of the proposed vdga method from the baliscore program. the comparisons are discussed below.

comparing vdga with msa-ga and other methods
to compare with the other methods, we have considered all three decompositions  with vdga. the authors of msa-ga  <cit>  selected  <dig> test cases from references  <dig> to reference  <dig> and reference  <dig>  as discussed earlier, we have considered  <dig> out of these  <dig> test cases. the results are provided in table  <dig> and are plotted in figure  <dig> 

in table  <dig>  the bold face data represents the best performing scores among the methods. from table  <dig> and figure  <dig>  it is observed that vdga with three decompositions achieved more accurate solutions than the others, in  <dig> out of  <dig> test cases, while decomp_ <dig> was better in three, decomp_ <dig> was in eleven and decomp_ <dig> was in six test cases. msa-ga achieved better msas for only two test cases, msa-ga w/prealign for four, and clustal w for one test case. however, both msa-ga w/prealign and clustal w found the same solution in one test case. in seven test cases where vdga did not achieve the best solutions, the solutions are close to the best solutions reported in the table. the vdga with decomp_ <dig> achieved accurate results in the maximum number of test cases reported in this table.

based on the average scores reported in the bottom row in table  <dig> and plotted in figure  <dig>  the vdgas with decomposition  achieved higher scores and among these vdga_decomp_ <dig> was the best for the  <dig> datasets. from the experimental results, we can claim that vdga had better performance on these  <dig> test cases. the average cs score of vdga for the msa-ga selected datasets was  <dig> .

comparing vdga with rbt-ga
we have also compared our results with rbt-ga. we have considered all of the  <dig> datasets and their approximate results as reported in the rbt-ga paper  <cit> . the summary of the experimental results of references  <dig> and  <dig> are presented in table  <dig> and table  <dig> and are plotted in figure  <dig> and figure  <dig> respectively.

• performance of vdga in reference 2
the  <dig> datasets in this reference are significantly different in lengths and numbers of their sequences. they also contain what is called "orphan sequences". vdga performed differently with different datasets. to judge the performance of vdga with respect to baliscore, we have compared with saga, rbt-ga, prrp, clustalx, dialign, hmmt, sb_pima, ml_pima, multalign and pileup <dig>  table  <dig> and figure  <dig> show that for the  <dig> test cases, the vdgas  were successful in finding more accurate solutions than the others in  <dig> test cases, and rbt-ga was successful in finding better solutions in  <dig> out of  <dig> test cases. rbt-ga and vdga achieved the same best value in one test case. among the successful  <dig> test cases, vdga with decomp_ <dig> found the best baliscore in  <dig> test cases, vdga with decomp_ <dig> in four test cases and vdga with decomp_ <dig> successfully found better solutions in  <dig> test cases. in  <dig> test cases, where vdga could not achieve the best solution, it was close to the best solution.

the average scores are also shown in table  <dig> and are plotted in figure  <dig>  this figure shows that all vertical decompositions achieved higher average accuracy than the other methods considered in this section. of those, vdga_decomp_ <dig> achieved the highest average accuracy, as it performed better for almost all test cases in reference  <dig>  the average cs score of vdga for reference  <dig> was  <dig> .

• performance of vdga in reference 3
reference  <dig> contains sub-groups of sequences where the residue identities between groups are less than 25%. in this paper, we considered  <dig> test cases out of  <dig>  and the experimental results that are presented in table  <dig> and figure  <dig> show that vdga found more accurate msas in  <dig> test cases, where vdga_decomp_ <dig> was best in  <dig> test case, decomp_ <dig> in  <dig> and decomp_ <dig> in  <dig> test cases. vdga_decomp_ <dig> and  <dig> found the same highest score in one test case. whereas saga was successful in finding the best score in one, prrp in one and ml_pima in one test case. prrp and ml_pima found the same solution for one test case . figure  <dig> shows that for some test cases, most of the methods could not find any similarities in their solutions, in comparison to the reference alignments. therefore, these methods received zero score. prrp and ml_pima achieved the same best score in one test case, but both received a zero score for another test case. however, vdga did not obtain any zero score.

the overall performance of all the methods for this reference is presented in figure  <dig>  although the vdga method did not achieve high accuracy solutions in some test cases, the average performance of this method with three decompositions are clearly better than the others as shown in figure  <dig>  the average scores of vdga_decomp_ <dig> and decomp_ <dig> are higher than other methods, while vdga_decomp_ <dig> achieved the highest average score. therefore, we can conclude that the overall performance of vdga in reference  <dig> is also better than the other methods mentioned earlier. the average cs score of our vdga approach for the rbt-ga selected datasets for reference  <dig> was  <dig> .

statistical analysis
to study the difference between any two stochastic algorithms in a more meaningful way, we have performed statistical significant testing. we have chosen a non-parametric test, wilcoxon signed rank test  <cit> , as it allows us to judge the difference between paired scores when it cannot make the assumption required by the paired-samples t test, such as that the population should be normally distributed. the results based on the best found solutions of vdga are presented in table  <dig>  where w  is the sum of ranks based on the absolute value of the difference between two test variables. the sign of the difference between two independent samples is used to classify cases into one of two samples: differences below zero , or above zero . as a null hypothesis, it is assumed that there is no significant difference between two samples. the alternative hypothesis is that there is a significant difference in the fitness values of the two samples. hence, if the hypothesis test rejects the null hypothesis, then there is a significant difference, otherwise there is no significant difference. the number of test problems is n =  <dig> and  <dig> for msa-ga and rbt-ga respectively, and we have used the 5% significance level. based on the test results/rankings, we assigned two words  for the comparison of any two algorithms , where 'yes' means that the vdga algorithm  is significantly different and better than the second, 'no' means that this is worse and there is no significant difference between the two algorithms, and 'same' means there is no significant difference and vdga performed the same as the second. we tested for significant with the baliscore corresponding to the best found wspm scores produced by vdga, in comparison to the published baliscore results of the other methods.

in this comparison, we have considered only decomp_ <dig> with vdga, as we have found from the preceding experiments that decomp_ <dig> is the best for vdga. therefore, to test the significance we have considered only vdga_decomp_ <dig>  in table  <dig>  it shows that there is a significant difference when vdga_decomp_ <dig> is compared with msa-ga, msa-ga w/prealign and clustal w for the dataset used in msa-ga, and when comparing vdga_decomp_ <dig> with prrp, clustalx, saga, dialign, hmmt, sb_pima, ml_pima, multalign, pileup <dig> and rbt-ga for the dataset used in rbt-ga, as indicated by the hypothesis test decision and the significance values. vdga_decomp_ <dig> is also significantly different than vdga_decomp_ <dig> and vdga_decomp_ <dig> for all the test cases. from the experimental observations, it is clear that vdga is significantly better according to the wilcoxon signed ranks test.

CONCLUSIONS
in this paper, a new ga based algorithm with vertical decomposition  has been proposed to solve multiple sequence alignment problems. this approach works with the solution of a guide tree. to generate an initial population, two mechanisms are used. to assess the performance of the algorithm, a number of experiments were carried out for deciding the initial population, the genetic operator, an appropriate set of parameters for ga and of the suitable number of decompositions.

an initial experiment was run to determine the parameters, and from the experimental results, the probability of crossover and mutation was set to 50%-50%. a simple hill climbing method with the standard vdga initial population was performed to verify the performance of the genetic operators. moreover, the vdga method was also run with a randomly generated initial population to judge the performance of the initial generation. to test the performance of the decomposition, this technique was applied on the solution of the guide tree as well as inside the ga. the experimental results showed that the decomposition technique can successfully find better multiple sequence alignments, and also that the optimum number of decompositions for vdga is  <dig> .

to evaluate our proposed approach, we considered a good number of benchmark datasets from balibase  <dig> , so as to cover all the test sets of msa-ga and rbt-ga. the proposed method was optimized based on the weighted sum of pair score. therefore, the baliscore corresponds to the best wspm score. this was used to compare with other methods, as the baliscore is widely used as the measure of quality/accuracy of multiple sequence alignments. the experimental results showed that vdga performed better for most of the test cases. although the solution of vdga was not the best for some test cases, it was close to the best for those cases. the overall behavior of our proposed method outperformed all of the other methods considered in this paper. vdga performed better than the others mainly because of our proposed initial generation, the genetic operators, the operators setting, and most importantly its decomposition technique.

after statistical and experimental analysis, we can safely conclude that the proposed method, vdga with decomp_ <dig>  can be considered as an effective method for solving multiple sequence alignment problems.

future studies intend to extend the experiments and to find a more efficient way for decomposition of longer and many sequences. we also want to optimize vdga with different scoring schemes, such as the recent log expectation scoring function  <cit> , as well as the consistency-based objective functions of coffee  <cit>  and t-coffee  <cit> , in order to test its performance.

authors' contributions
fn was responsible for conception, design, implementation, testing and drafting of the vdga. rs and de were responsible for revising this proposed concept critically and hence for important intellectual content. all authors have read and approved the final manuscript. the first author contributed  <dig> percent of this work. the other authors contributed  <dig> percent each.
