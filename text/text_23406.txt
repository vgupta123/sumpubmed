BACKGROUND
the hyperpolymorphic human leukocyte antigen  system, spanning about 4 mb on the short arm of chromosome  <dig>  contains a number of genes that play key roles in the adaptive immune response  <cit> . especially the “classical” hla genes encoding the  <dig> major antigen-presenting proteins  play a crucial role in solid organ and haematopoietic stem-cell transplantation , where outcome is mostly determined by the genetic concordance of hla alleles between donors and recipients  <cit> . with more than  <dig>  allelic variants identified today , combinatorial diversity in this region explodes, and the search for a matching unrelated donor can resemble the search for the proverbial needle in a haystack.

despite  <dig> million potential unrelated donors for patients in need of an allogenic hsct being currently registered worldwide , finding suitably matched donors can be severely hampered by the heterogeneous quality of the available genotyping information  <cit> . until recently, unrelated stem-cell donor registries all over the world, which provide the bulk of this information, have utilised different serological and dna-based hla typing methods with variable resolution, such as sequence-specific oligonucleotide probes , sequence-specific primer  pcr, or sequence-based typing  using sanger sequencing.

however, these technologies are limited in throughput, precision and achievable coverage when compared to next generation sequencing -based hla typing methods  <cit> . benefits of ngs-based typing approaches include high throughput through massive parallelisation, clonal sequencing of single molecules, sample multiplexing, and reduced costs per sample  <cit> . whilst the extensive allelic diversity of hla class i and ii genes has made, and continues to make, high-resolution hla typing challenging, the advances in ngs technologies have made ultra-high-throughput, cost-effective and precise hla typing possible at an unprecedented scale .

to date, dkms hosts hla genotyping data for  <dig> million registered donors across germany, poland, the uk and the usa. currently all dkms samples from newly recruited donors are typed at the high-throughput genotyping facility dkms life science lab in dresden, germany. as of  <dig>  dkms life science lab successfully replaced a sanger sbt workflow with a ngs hla-typing workflow, initially based on illumina miseq and later illumina hiseq  <dig> amplicon sequencing  <cit> . rapid advances in laboratory automation and increasing sequencing capacity have led to a dramatic growth from  <dig>  donor samples processed every month to currently over  <dig>  samples per month over the last three and a half years . the total number of donor samples typed by ngs surpassed  <dig>  million samples in june  <dig> , whilst the costs for hla genotyping have dropped by more than 50% as compared to sanger-based sequencing.fig.  <dig> cumulative and monthly numbers of donor samples genotyped at the dkms life science lab since  <dig> as part of routine operations. the grey line shows the total cumulative number of genotyped samples, the coloured lines show gene-specific cumulative numbers; grey-shaded bars indicate monthly throughput. black horizontal bars show yearly mean throughput. the y-axis is square root scaled to enhance readability




ngs technologies also make it easier to adapt read coverage to the experimental demand at minimal increases in cost. this results in an opportunity to expand the donor genotyping profile with ease and cost effectiveness by adding genes of interest that either may impact clinical outcome after hsct , or that provide additional information to clinicians selecting the best possible donor . consequently, these markers were gradually added to the dkms typing profile, starting with ccr <dig>  abo and rhd as of  <dig> and followed by kir genes as of  <dig> 

using ngs technologies in a highly automated, high-volume production environment with high demands on data quality provides a number of key benefits over traditional sanger sequencing and enables routine typing operations at an unprecedented scale. at the same time ngs poses a number of novel challenges and introduces complexities of its own. here, we report on our experiences of using amplicon-based hla typing by ngs at a massive scale. we present not only the performance metrics of our ngs-based typing approach but also key lessons we learned over a time period of three and a half years typing  <dig>  million donors for six hla loci.

RESULTS
high throughput at high resolution
between january  <dig>   <dig> and june  <dig>   <dig> a total of  <dig> , <dig> samples were processed by amplicon-based ngs hla typing first on the illumina miseq platform until august  <dig>  and from then on predominantly on the hiseq  <dig> platform  <cit> . the move from miseq to hiseq  <dig> was driven by capacity demands and illumina providing the “rapid run mode” with 2× <dig> bp read lengths. the initially available read length of 2× <dig> bp on the hiseq had not allowed for full coverage of the exons for our direct amplicon sequencing approach.

since october  <dig>   <dig> , <dig> donors have additionally been typed for ccr <dig> and the blood groups abo and rhd  <cit> ; since october  <dig>  <dig> , <dig> donors have additionally been typed for the presence/absence of kir genes .

the monthly throughput during the first year  ranged from  <dig>  to  <dig>   donor samples; this throughput then increased ranging from  <dig>  to  <dig>   samples across  <dig> and  <dig>  and increased further in  <dig> ranging from  <dig>  to  <dig>   samples .

based on data from the hla core exons  <dig> and  <dig>  between  <dig> %  and  <dig> %  of the samples could be typed at high resolution or better as defined by efi standard v <dig>  , with the exception that null alleles caused by a mutation outside of exons  <dig> and  <dig> remain unidentified . for the remainder of the samples intermediate typing resolutions were obtained, with the exception of  <dig> low-resolution hla-b samples .table  <dig> ngs genotyping resolution for six hla loci in  <dig>  million dkms donors


anull alleles caused by a mutation outside of exons  <dig> and  <dig> remain unidentified




source material variability
for the vast majority  of samples dna is extracted from buccal cells. donors are provided with two nylon dna-sampling swabs  and instructions to scrape the inside of the cheek with the swabs firmly for 30 s. these self-administered swabs are subsequently mailed to dkms life science lab for dna extraction and genotyping.

dna concentrations achieved by extracting from buccal samples have varied over a wide range. ninety percent of all extractions yielded between  <dig>  ng/μl and  <dig>  ng/μl of dna . in addition, we observed marked fluctuations in median dna concentrations over the complete time period and a strong dependence on sample provenance . buccal samples derived from uk donors generally yielded lower dna concentrations  than samples derived from donors in poland or germany . the reason for this discrepancy is unclear but might reflect differences in compliance, sample envelope material or sample transit time. no significant seasonal effects were detected.fig.  <dig> quarterly average concentration of donor dna extracted from buccal cells. panels present differences between germany , poland  and the uk. overall trend lines are generated by loess smoothing




it is possible that the large variability in the obtained dna concentration is at least partially driven by differences in compliance with swabbing instructions by donors. for instance, a concerted effort in 2015q <dig> by dkms polska emphasising the importance of the sampling procedure appears to have caused a dramatic increase in dna yield from median  <dig>  ng/μl  before  <dig> to  <dig>  ng/μl  in  <dig> and  <dig> 

at least for fluidigm-based workflows, dna concentrations lower than 2 ng/μl have been found empirically to compromise genotyping results severely. overall,  <dig> % of all samples fell below this threshold, but the prevalence of such low-quality samples varied over time and with sample provenance . these cases warrant a second extraction attempted using the alternative swab provided by the donor. unfortunately, the first dna extraction is a very good predictor of dna concentrations obtained from the second swab . only  <dig> % of the samples with an initial dna concentration lower than 2 ng/μl achieved a dna concentration higher than 2 ng/μl using the second swab. since there was a 26-day period  between the first and second extraction attempt, one may argue that the prolonged storage of swabs may have adversely affected dna yield. however, samples with higher initial dna concentration also tend to yield high concentrations in a second extraction . this reinforces the notion that, to a large extent, individual patterns of compliance impact the yield from dna extractions.

read quality
high-quality and unbiased sequence read data obtained from the sequencing platform greatly facilitate accurate and reproducible high-throughput typing. for illumina instruments, the two common metrics for overall run performance are the number of reads passing filter , i.e., reads that pass an internal quality filtering procedure , and the total percentage of bases that are assigned a phred quality score of  <dig>  or better . the density of clonal clusters on illumina flow cells is expected to strongly influence overall performance. by increasing the cluster density the read yield is increased until too many clusters are so close that they cannot be separated algorithmically. at this point, read yield saturates and may even decline and, according to illumina documentation, sequencing quality may suffer. illumina suggests optimal ranges of cluster densities and the corresponding expected outputs for different sequencing chemistries and instruments . achieving a designated cluster density requires loading the correct amount of high-quality library dna onto the flow cells.fig.  <dig> reads passing filter vs. cluster density on illumina miseq and hiseq instruments. each data point represents a run . shaded areas denote supported ranges of cluster densities and expected output for different chemistries/kits as specified by illumina. the colour gradient indicates the total percentage of bases reaching a quality score of  <dig> or higher per run. trend lines are generated by generalised additive model fits using a cubic penalised regression spline. m = millions




special considerations, however, apply for sequencing from low-diversity libraries such as libraries generated from amplicons. diverse or balanced libraries  show an approximately equal distribution of all four nucleotides in every cycle. the illumina rta  software originally was optimised using balanced libraries to accurately locate cluster coordinates during the first sequencing cycles . amplicon-based libraries, in contrast, tend to show a biased nucleotide distribution which may lead to a failure to segregate adjacent clusters and can adversely affect yield as well as data quality. even though recent rta versions have been optimised to be more robust with regard to low-complexity libraries, illumina still recommends spiking in 5-10% phix for increasing diversity and targeting a more conservative cluster density.

for our ngs-based routine typing operations we have thus far performed a total of  <dig>  runs distributed across two illumina platforms  and  <dig> versions of sequencing chemistry . whilst we initially tried to attain the cluster densities supported for balanced libraries it quickly became clear that our low-diversity hla libraries required a reduction in template input for optimal yield and data quality . this effect was especially noticeable early on for the miseq v <dig> chemistry where a number of runs showed reduced pf read counts at supported cluster densities or higher . with miseq v <dig>  less data degradation was observed at the supported cluster densities but the strategy to undercluster was retained since optimal yield was readily achieved even at lower densities . hiseq flow cells with rapid run sbs v <dig> chemistry could safely be clustered at the supported upper limit of  <dig>  k/mm <dig> without sacrificing a linear increase in yield at all . interestingly, no obvious relationship between achieved cluster densities and %q <dig> became apparent in our runs. the most likely explanation for amplicon libraries performing markedly better on miseq v <dig> and especially the hiseq is that new versions of the rta software were rolled out by illumina with algorithmic improvements to better cope with low-diversity libraries.

a critical factor influencing data quality and yield are template-independent primer-primer interactions that take place during the pcr steps and give rise to artificial products, especially primer dimers   <cit> . although primer dimer formation can be reduced by careful primer design and the application of stringent pcr conditions, it becomes increasingly difficult to avoid all primer interactions when developing multiplexed reactions. for the fluidigm workflow, before the split of low-concentration and high-concentration samples into separate workflows, we experienced average monthly pd rates ranging from  <dig> % ±  <dig> % sd in january  <dig> to  <dig> % ±  <dig> % sd in november  <dig>  average monthly pd rates were independent of sequencing instrument and/or chemistry  but decreased significantly over time due to continuous process optimisations and tweaking of the primers used in routine operations .

a particularly troublesome source of increased pd formation was identified after a careful analysis of the sequences of primer dimer products. it showed that the primer sequences involved exhibited recurring patterns of degradation at their 3’-ends. we tracked the cause to the hot-start pcr system used . in contrast to the documentation’s claim of “inactivity at low temperatures”, only the polymerase activity is minimised at ambient temperatures by the hot-start modifications. the 3’-exonuclease providing the proofreading capabilities is not modified to require heat activation. as a consequence, the 3’-exonuclease degrades primers at the 3’-end in the reaction mix during cooled storage and reaction setup. a change in protocol was therefore applied during may/june  <dig> where standard primers were replaced by modified primers incorporating three phosphorothioated nucleotides at the 3’-end to inhibit exonuclease degradation . prior to this protocol change we observed an average pd rate of  <dig> % . after the protocol change was fully implemented the average pd rate dropped to  <dig> % .

the pd problem was further exacerbated by a peculiarity of our amplification protocol: in a standard pcr setup, pds derived by 3’-end degradation are expected to form an inefficient substrate for amplification in subsequent pcr cycles as the majority of primers continue to carry intact 3’-ends and fail to bind to the degraded pds. in contrast, with the 1-pcr 4-primer approach used in our fluidigm workflow, we use two inner primers with target-specific 3’-tails and a 5’-tail complementary to two outer indexing primers  <cit> . thus, pds formed by the inner primers constitute an appropriate substrate for further amplification by the outer primers in subsequent cycles.

the notion that the 1-pcr 4-primer  protocol increases the risk of pd formation was also supported by a significant reduction in pd rates after the alternative 2-pcr 2-primer  protocol was introduced for low-dna-concentration samples in november  <dig> . the move from the 1-pcr 4-primer setup to the 2-pcr 2-primer setup is confounded by a commensurate move from the fluidigm nanofluidics platform to standard 384-well plates with significantly larger reaction volumes and consequently different reaction kinetics. however, the kir genes were always amplified on plates. to tease apart the relative contributions of pcr protocol and reaction volume, we analysed different hla and kir amplicons separately . disregarding amplicons with negligible pd rates  and an extreme outlier , the remaining  <dig> hla and  <dig> kir amplicons had similar decreases in average pd rates when moving from a 1-pcr 4-primer approach to a 2-pcr 2-primer setup . this suggests that there is no correspondence between reaction volume and the degree of primer dimerisation.

high proportions of primer dimers not only reduce the number of reads on target per sample but also influence overall run quality by lowering %q <dig> . this is probably explained by the fact that we are sequencing more cycles than the average length of the primer dimers whereby the polymerase runs off the template and produces low-quality data for the remaining sequencing cycles.fig.  <dig> average percentage of bases reaching a quality score of  <dig> or higher per run  vs. proportion of primer dimers 




independent of primer design and pcr conditions, we also found template dna concentration to influence primer dimer rates within the nanofluidic fluidigm workflow. at dna concentrations below 30 ng/μl we observed a steep rise in the average pd rate, whereas the pd rate remained relatively constant at higher concentrations . whilst low target copy numbers have been reported before as favouring primer oligomerisation and mispriming  <cit> , this effect appears to be exacerbated on a nanofluidics platform. when the  <dig> pcr workflow was introduced for low-concentration samples in november  <dig>  we observed an overall marked decrease in pd rate in this workflow . independent of this effect, however, both the fluidigm workflow  and the  <dig> pcr workflow  showed significant rates of increase in pds with decreasing dna concentrations . the magnitude of the effect was slightly, albeit significantly, larger in the fluidigm workflow with an estimated  <dig> %  change in pd rate for each ng/μl dna less compared to the  <dig> pcr workflow with an estimated  <dig> %  change in pd rate for every ng/μl dna reduction.fig.  <dig> dependency of primer dimer rate from initial dna concentration and workflow. the two workflows differ both in their reaction volumes and amplification strategies . solid lines depict generalised additive model fits using a cubic penalised regression spline. the shaded bands around the regression lines indicate the pointwise 95% confidence intervals on the fitted values




we conclude that we were able to reduce the pd rate from 17% to less than 1% mainly by  using pto-modified primers to avoid degradation and  moving from a 1-pcr 4-primer setup to a more conventional 2-pcr setup for target-specific amplification and barcoding pcr. given the strong effect on overall run quality, monitoring and minimising pd rates seems highly advisable when performing amplicon-based ngs experiments.

read artefacts
in addition to primer dimer formation, amplicon-based sequencing is subject to another frequent methodological pitfall, namely the pcr-mediated formation of cross-over products between the two allelic variants, also called pcr chimerism  <cit> . these reads incorporate portions of both alleles present in the sample into a single sequence, forming recombinants between the starting templates. chimeras are more likely to be observed with increasing numbers of pcr cycles and if the initial concentration of templates is high  <cit> . such cross-over events can be filtered by our analysis software as long as the parent sequences are not significantly outnumbered by the recombinant sequences. in addition, chimeras combining target and off-target reads stemming from, for instance, hla-drb3/4/ <dig> or hla-h are especially challenging to detect. high rates of chimeric reads can therefore confound the ability to successfully call the correct alleles  <cit> .

the observed rate of chimeras depends heavily on the amplified locus and exon . hla-c exon  <dig> consistently shows the largest average proportion of chimeras amongst on-target reads , whilst hla-dpb <dig> exon  <dig> and hla-drb <dig> exon  <dig> rarely show any sign of chimeric reads. the low chimerism rates observed for exon  <dig> of these class ii hla genes is possibly explained by the fact that the currently known exon  <dig> allelic sequences exhibit little variability, which facilitates escaping chimerism detection.fig.  <dig> proportion of pcr-mediated recombinant reads  for different hla amplicons and different workflows 




as with the other aforementioned metrics of quality, we observed a dramatic reduction in cross-over rates for the  <dig> pcr workflow . this seems counterintuitive given that the fluidigm workflow has fewer amplification cycles compared to the  <dig> pcr workflow .

however, it has been observed that chimera formation largely occurs at late pcr cycles  <cit>  and is also promoted by higher initial template concentration  <cit> . in the fluidigm pcr reactions initial template dna concentrations are much higher  than in the  <dig> pcr reactions . this is to ensure that sufficient product for downstream processing is generated in the much smaller reaction volumes on access array chips. unfortunately, as these reactions now reach saturation earlier they also offer more opportunities for crossing-over at earlier cycles  <cit> .

quantification-free post-pcr normalisation
ideally, each pcr product should be quantified individually and subsequently pooled equimolarly. however, to keep the workflow simple and cost-effective we opted to pool pcr products without attempting to normalise dna concentrations.

as data from samples amplified on fluidigm access array chips accumulated, we found a weak but significant relationship between initial dna concentration and the final number of reads generated for a sample . initial dna concentration c
i and final number of reads per sample r show a saturation dependency that can be modelled by a michaelis-menten equation:fig.  <dig> correlation between initial dna concentration and read coverage per sample before  and after  august  <dig>  in august  <dig> a new post-pcr equilibration strategy was introduced based on a michaelis-menten saturation curve  estimated from the data. solid lines show generalised additive model fits using a cubic penalised regression spline. the shaded bands around the regression lines indicate the pointwise 95% confidence intervals on the fitted values


 r=ci/ci+kmrmax 


fitting a two-parameter michaelis-menten model to the available data allowed for estimating the michaelis-constant, k
m, and the maximum achievable read number, r
max, for our system. thus, we could use the overall k
m estimate to predict relative final read counts for samples processed on an access array chip and adjust the amount of pcr product entering the post-pcr pool to minimise the effect of differences in dna concentrations. implementing this quantification-free post-pcr normalisation strategy balanced read coverage further across a wide range of initial dna concentrations . only samples with dna concentrations of less than 5 ng/μl yield too little pcr product to enable complete compensation.

read coverage
for reliable genotyping it is advantageous not only to achieve a balanced depth of coverage across samples but also across amplicons. analysing  <dig>  donor samples distributed over  <dig> miseq/hiseq runs performed between january  <dig> and june  <dig>  on average  <dig>   reads were produced per sample. of these reads,  <dig> %  mapped to their respective target amplicons and could be used for subsequent analyses. these reads partition into those targeting hla loci , kir loci , and others .

the number of on-target reads for individual hla amplicons varied across loci and exons . across amplicons we obtained a median coverage ranging from 867-fold  to 2034-fold . however, the relative read counts across amplicons have also changed over time as primers were tweaked to try to equalise read counts as much as possible . uneven read depths across amplicons are due to different primer efficiencies and amplification biases where some sequences tend to amplify better than others  <cit> . different primer efficiencies can be alleviated by tweaking either the primer sequence itself or by adjusting the relative amount of primer added to the pcr reaction, with the former being undertaken after the initial primer design. unfortunately, it turns out that some degree of adjusting the relative amount of primer may become necessary when new batches of reagents are put into use . however, despite perfectly adjusted primer sets, sample-to-sample variations  resulted in diverging amplification kinetics and varying endpoints. therefore, only post-pcr quantification and normalisation would deliver fully balanced amplicons. costs for such a quantification step for each amplicon would increase current sequencing costs several fold. the low cost of sequencing allowed us to bypass such normalisation by way of massively oversampling for most samples and targets. thereby, despite omitting quantification and normalisation,  <dig> % of all hla amplicons achieved more than 100x on-target coverage, which is sufficient for genotyping at the highest possible quality.fig.  <dig> distribution of on-target paired-end reads across amplicons. different colours indicate different hla loci. solid lines indicate exon  <dig> amplicons; dashed lines indicate exon  <dig> amplicons




sample repetition
on occasion, the initial genotyping attempt fails to meet quality standards and the affected sample has to be reanalysed. this can affect either individual loci or the entire sample. a full sample repeat typing including a second dna extraction is triggered if the initial dna concentration is less than 2 ng/μl or more than  <dig> hla loci fail to produce credible results.

we can identify several common factors that may trigger repeat typing of an individual locus: technical concerns include too low total read numbers, too few reads on target, or heavy imbalances across alleles. other causes for concern are the presence of more alleles than expected, which might be a result of one or more of many factors such as pcr chimerism, sample contamination, co-amplification of closely related loci, misamplification or sequencing errors. an analysis must also be repeated if the genotyping software fails to construct a genotype. this may occur for benign reasons  or be a result of stochastic allelic dropouts during pcr  <cit> . more insidiously, allelic dropouts may remain “silent”, i.e., be technically valid but erroneous homozygous or even heterozygous genotypes may arise  <cit> . therefore, machine learning techniques are employed to help identify “implausible” genotypes that warrant repeated analyses to check their validity  <cit> .

whilst we strive to keep the rate of repetition low for economic reasons, it is important to employ conservative quality thresholds and repetition criteria to achieve the highest possible reliability of genotyping results. repeat genotyping runs are automatically suggested by the genotyping software based on conservative thresholds for quality metrics and triggered manually after inspection of suspicious typing results  <cit> . in addition, analysts may trigger repeat typings for putative novel alleles and in cases where genotypes suggested by the software are considered to be implausible .

between january  <dig> and november  <dig>  a total of  <dig> % of individual loci were repeated and a total of  <dig> % of full samples were rerun. both locus and workflow heavily affect repetition rates. subjecting all samples to the fluidigm workflow, hla-c, -dqb <dig>  and -dpb <dig> showed high average repetition rates around 4%, whilst hla-a, -b, and -drb <dig> showed average repetition rates around 2% . the introduction of the  <dig> pcr workflow for low-concentration samples in november  <dig> had a marked twofold effect on repetition rates. first, repetition rates decreased from  <dig>  to  <dig> % for individual loci and from  <dig>  to  <dig> % for full samples for the high-concentration samples still processed on fluidigm chips. such an improvement was expected since problematic samples with low dna concentrations were shifted to another workflow. second, and more unexpected, with  <dig> % for individual loci and  <dig> % for full samples repetition rates for the low-concentration samples now processed with the  <dig> pcr workflow were comparable to or lower than the repetition rates achieved for the high-concentration samples on the fluidigm chips . this effect is likely explained by the fact that low dna concentrations on fluidigm access array chips adversely affect allelic dropout rates  <cit> . accordingly, we find that the likelihood of a locus being repeated correlates inversely to the initial template dna concentration when using fluidigm chips whilst this relationship is markedly less pronounced in the  <dig> pcr workflow .fig.  <dig> monthly median repetition rate of hla loci for different workflows . error bars show median absolute deviation . if more than  <dig> loci of a sample require repetition, the full sample  is repeated


fig.  <dig> dependency between initial template dna concentration and the number of hla loci per donor that have to be verified in repeat typing. colours indicate different workflows . the point size is indicative of sample size




novel alleles
our in-house genotyping software classifies and reports all consensus sequences differing from the reference alleles found in the imgt/hla database as potential novel alleles. given the likelihood of such signatures being generated by artefacts such as pcr errors, co-amplification products, or noise, these samples need to be verified by repeat typing of an independent pcr product. prior to  <dig> this verification process was not performed automatically but was left to a technician’s discretion. as of january  <dig>  all potential novel sequences routinely undergo repeat typing. for this reason, we restrict the analysis described to samples typed between january  <dig> and may  <dig>  for which data are exhaustive.

during this period of 17 months, a total of  <dig> , <dig> hla alleles from  <dig> , <dig> donors were typed. in the case of  <dig>   alleles repeat typing was triggered because the signature of a potential novel allele was detected.  <dig>   sequences could be confirmed as bona fide novel alleles, representing  <dig>  unique novel sequences. in the remainder of the cases repeated sequencing suggested that the initial deviation from known alleles was caused by pcr or sequencing artefacts. it follows that we observe a maximum pcr error rate of  <dig> %. a pcr error might mask a true novel allele if the error actually restored the polymorphism to the reference sequence. if we assume an average length of 730 bp for an allelic sequence composed of exons  <dig> and  <dig>  we expect approximately one undetected novel allele in  <dig> million bona fide novel alleles. novel alleles were not equally distributed amongst hla loci. overall we found class ii loci three-fold over-represented amongst samples with novel alleles . interestingly, however, the rate of discovery of distinct novel sequences was far smaller amongst class ii loci compared to class i loci .fig.  <dig> the cumulative numbers of novel hla alleles discovered between january  <dig> and may  <dig> during routine genotyping of exons  <dig> and  <dig>  all allelic sequences were verified by replicate typing using an independent pcr reaction. grey shades denote distinct novel sequences; blue shades denote additional samples with previously observed novel sequences





using these data we attempted to forecast the expected number of distinct novel alleles for the different hla loci. first we resampled the original  <dig>  million samples into  <dig> bins of  <dig> samples each. this resampling procedure was repeated  <dig> times. treating these bins as successive points in time  we estimated the rate of discovery of distinct novel alleles at each of these successive sampling points. the expected decrease in discovery rate was modelled using a 3-parameter exponential decay model . the models for the different loci were used to project the expected number of novel distinct alleles at  <dig>   <dig>  and  <dig> million typed samples, respectively .

it should be noted that these projections are subject to two important limitations: first they presume no change in the current ethnic make-up of donors processed at our lab  and, second, they concern only novel alleles due to variation at exons  <dig> and  <dig> alone.

given these restrictions, our estimates indicate that hla-a is expected to be saturated for novel alleles  in the near future and that we can expect that most undiscovered variation is still harboured in class ii loci, especially hla-dqb <dig> 

discussion
in recent years it has been extensively demonstrated that ngs can be applied to allele-level hla typing with minimal ambiguity, providing benefits such as clonal sequencing to achieve in-phase reads and massive parallelism to enable the expansion of the regions sequenced . many tissue typing laboratories, however, have yet to fully embrace ngs technologies for routine high-throughput operations. dkms life science lab was one of the first labs to do so and has subsequently accumulated three and a half years of experience with routine hla typing using an amplicon-based approach on the illumina platforms. over that period, more target genes were added  and genotyping data for  <dig>  mio samples was retrieved from  <dig>  miseq and hiseq sequencing runs .

the amplicon-based approach was initially chosen for two major reasons. first, it is applicable to most genomic loci with modest modifications both in the laboratory workflow and the analysis software. this allowed rapidly supplementing the standard hla typing portfolio with abo, rhd, ccr <dig> and kir typing as straightforward extensions to the core hla typing workflow  <cit> . perhaps equally important for production-scale operations is the simplicity and robustness of a pcr-based approach. most steps in a pcr-based workflow can be automated using liquid handling robotic stations and high-throughput pcr solutions such as the fluidigm access array system or high-capacity thermocyclers. moreover, the amplicon-based approach proved highly resilient to the wide range of dna quantities derived from the primary sources material , tolerating as little as 2 ng/μl initial template dna concentration .

with our large-scale ngs hla typing strategy we obtained high-resolution genotype assignments for an average of  <dig> % of the hla-a, -b, -c, -drb <dig>  -dqb <dig>  and -dpb <dig> loci typed . the lowest high-resolution typing rate was achieved for hla-c . hla-c turned out to be a challenging locus in many regards: in the fluidigm workflow, before the separation of low- and high-dna-concentration samples, it showed the highest average primer dimer rate amongst all loci  and, correspondingly, the lowest average numbers of on-target reads . exon  <dig> of hla-c showed the highest proportion of detected chimeric reads amongst all amplicons  and hla-c is amongst the loci with the highest average repetition rates .

however, many of these issues resolved or improved not only for hla-c but also other loci when we started to treat low-dna-concentration samples  and high-dna- concentration samples differently. low initial dna concentrations in conjunction with the nanolitre-scale reaction volumes on  <dig>  fluidigm access array chips and the time- and cost-effective single-pcr 4-primer approach caused problems that largely dissipated once standard microlitre reaction volumes and 2-primer pcr setups were used. alternatively, nanofluidic chips could be used in combination with a conventional 2-primer chemistry, which would probably also resolve issues with primer dimers and pcr artefacts. it should also be noted that the sample repetition rates reported arise from two main cases: a) truly failed amplifications producing insufficient read depths for genotyping and b) results that are largely correct, but get flagged for repeat typing for verification purposes. this includes rare alleles as well as results that may have suffered from pcr drop out due to limited dilution. for low dna concentration samples applied to the  <dig> chips, that effect, although still rare, becomes measurable  <cit> . therefore, these repeats reflect our extremely high standards with regard to minimal error genotyping.

whilst the fluidigm workflow as originally devised  <cit>  has some operational advantages, such as being less complex to automate and having low reagent costs, in our lab it remains a viable option only for high-quality samples. as the dna concentration effects are relatively subtle, they often only become possible to track down after a large amount of data has accumulated. thus, we now see clear evidence that sequential 2-primer pcrs should be preferred over 4-primer amplicon tagging workflows especially if increased formation of chimeric reads and primer dimers pose a risk for the intended application. we also suggest that proofreading polymerases should best be used in conjunction with pto primers unless it is assured that exonuclease activity is effectively inhibited before heat activation. finally, when low template dna concentrations cannot be avoided for applications that are sensitive to stochastic dropout of alleles, a standard pcr setup might be a better fit than a nanofluidics platform.

large-scale ngs-based typing of hla in the context of donor registries with tens of thousands of samples processed each month requires substantial upfront investments in terms of laboratory automation, it infrastructure and data analysis capacity. at dkms life science lab we currently employ  <dig> custom automatic dna extraction platforms with a total capacity of more than  <dig>  samples/week,  <dig> robotic liquid handlers,  <dig> hydrocyclers,  <dig> fluidigm cyclers, and  <dig> hiseq  <dig> instruments for sequencing. on one hiseq  <dig> instrument, up to  <dig>  samples can be jointly sequenced within 3 days using “rapid run mode” and a dual flow cell run. data analysis is performed using custom software  <cit> , currently running on  <dig> xeon processor cores and requiring  <dig>  tb of storage capacity per month to retain fastq files and analysis data. however, for the more common use case of hla typing in a clinical setting or a diagnostic lab, the requirements are typically much less demanding. example workflows suitable for low- to medium-throughput hla genotyping using ngs have been discussed extensively in the literature  <cit> .

the next step in hla genotyping subsequent to a ngs-based amplicon approach is promised by long-read single molecule sequencing technologies such as offered by sequencing systems by pacific biosciences  or oxford nanopore technologies . combined with long-range pcr they hold the promise of efficient and accurate haplotype sequencing over multiple kilobases, thereby making available exon and intron sequences not covered by the current approach. at present the clinical impact of sequence information outside the peptide binding groove remains unknown except for variations encoding null alleles  <cit> . once scalable hla typing workflows and analysis software become available for these technologies, it would appear short-sighted to dismiss the clinical potential of fully characterised hla alleles. however, whilst hla typing is already feasible with pacbio instruments  <cit> , the limited benefit for donor search strategies seems not to justify the five-fold higher sequencing costs in the registry context. an alternative to pacbio-based systems may be provided by ont’s high-throughput promethion system which promises hiseq-like throughput rates  but at the time of writing has not yet been released to customers.

CONCLUSIONS
although long-read sequencing platforms are likely to deliver the next revolution in hla typing, they cannot at present truly compete in terms of throughput and costs per sample with the currently available widely applied and proven short-read-based approaches. we expect amplicon-based ngs typing workflows to remain, at least for the next few years, as the workhorse in high-volume targeted typing applications like hla genotyping for donor registries. our experience after genotyping more than  <dig>  million samples confirms that the short-amplicon-based genotyping approach exhibits the robustness required for everyday routine high-throughput operation. in particular, compared to sanger sequencing, the ngs based approach has proven to be by far more cost efficient, easier to handle and less error prone. taking advantage of these lessons learned will help to further increase robustness and efficiency and raise awareness for potential pitfalls, thereby minimising spurious genotyping results. ngs has revolutionised whole genome sequencing and is starting to have a tremendous impact on targeted sequencing applications, enabling projects of a whole new scale and breadth.

