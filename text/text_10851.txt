BACKGROUND
signals recorded from the surface of the cerebral cortex are composites of the electrical activity of a large number – probably millions to billions – of individual cells. therefore, one would expect that several different processes – each produced by a different neuronal structure with a characteristic activity pattern – would be occurring simultaneously. the critical question here is: can these superimposed signal patterns be separated and analyzed independently? in order to address that issue, we propose to utilize an experimental technique based on measuring neural activity in a controlled setting  as well as under exposure to some external stimulus – nicotine, in this case  <cit> . application of stimuli that affect the observed signals often has an effect only on a subset of the sources. the information about which sources are affected by the stimuli can provide an interesting insight into the problem of neural activity analysis, but cannot be measured directly. based on the assumption that each of the sources produces a signal that is statistically independent of the others, the observed signals can be decomposed into constituents that model the sources, also referred to as basis functions. each of the observed signals is a linear combination of those modeled sources. due to the fact that some sources influence some locations stronger than others, each source can be present in each observed signal with a different magnitude. the magnitudes are modeled as coefficients in the aforementioned linear combination. the change in the coefficients, as a result of applied stimuli, corresponds to the change in the contribution of a source in generation of a given signal.

independent component analysis  can be useful in this kind of analysis, as it allows for determination of an impact of the external stimuli on some specific neuronal structures, supposedly represented by the discovered components. the link between the stimulus and a given source can be verified by a classifier that is able to "predict" under which condition a given signal was registered, solely based on the discovered independent components.

the general idea behind all decomposition techniques is to represent the original signal x in terms of some basis functions m and a set of coefficients a, with an addition of some noise or, simply, error e:

x = ma + e.       

with this approach, the temporal properties of the system are preserved by the basis functions. the original sequences are replaced by a set of scalar coefficients that represent the original data in the space spanned by the basis functions. the process of reconstruction into the original input space is simply based upon a linear combination of the basis functions .

for example, the following artificially generated dataset consisting of three sequences y <dig>  y <dig>  y <dig> belonging to one of the two categories a and b each , can be replaced by two basis functions m <dig>  m <dig>  and a new dataset consisting of the coefficients a <dig>  a <dig>  , for the basis functions m <dig>  m <dig> respectively, that will represent the original vectors y <dig>  y <dig>  y <dig> in the new attribute space  <cit> .

such a transformation can be very useful for classification. the feature space has been tremendously reduced  and the task becomes straightforward – in this example, even a single decision rule will be sufficient to classify the signals in the database without error.

in the above example, the signals of class a are those that contain both the sinusoidal and the square pulse components, while the type b sequences have no square component at all. this could possibly be deduced from the analysis of the shapes of the signals, but can also be based upon the analysis of the coefficients and some simple classification rule generation . this is a very simple, synthetic example, so the classes are known a priori; however, one can imagine a "real-life" problem where such a solution would be very desirable. for example, assuming that signals in class b are registered right after an application of some external stimulus, one could conclude that the stimulus inhibits the source that generates the square pulse, but has no apparent influence on the source related to the sinusoidal component.

over the years, various decomposition techniques have been successfully applied to the domain of signal classification. unquestionably, one of the most commonly used methods for that task is independent component analysis . even though it proves to be a powerful and often successful tool, one of the main weaknesses of ica is its assumption about the statistical independence of the components – this will rarely be sufficient for a successful differentiation between signals that belong to different classes. another important flaw of ica is the fact that the cardinality of the resultant set of independent components is always the same as the number of the input signals. this poses a difficult question as to the importance of the discovered components for a given classification task: which of the components explain the classification in the best possible way? this can also become a major computational problem, especially with a large size of the analyzed database. thus the idea of combining the premises of a reliable and accurate decomposition of signals  with the determination of the components that really matter in terms of segregation of the input sequences into separate categories seems plausible.

classificatory decomposition  is a general term that describes our research study that attempts to improve the effectiveness of signal decomposition techniques by providing them with "classification-awareness". the description of previous stages of the study and some examples of applications can be found in  <cit> . in this article, we investigate hybridization of multi-objective evolutionary algorithms  and rough sets  to perform decomposition in the light of the classification problem itself. the idea is to look for basis functions whose coefficients allow for an accurate classification while preserving the reconstruction. we propose a simple extension of the well-known multi-objective evolutionary algorithm vega  <cit> , which we call end-vega . the extension, in its initial form introduced in  <cit> , supplies the algorithm with the considerations related to elitism and non-dominance, lack of which is known to be its main drawback  <cit> . we also investigate the idea of utilizing ica to initialize the population in the moea. the details of the modifications as well as a short theoretical background are given below.

methods
independent component analysis
independent component analysis  is a signal processing technique originally developed to deal with the cocktail-party problem  <cit> . ica is perhaps the most widely used method in blind source separation  in various implementations and practical applications  <cit> . the basic idea in ica is to represent a set of random variables using basis functions, which are as much statistically independent as possible. the central limit theorem states that the distribution of a sum of independent random variables, under certain conditions, tends toward a gaussian distribution. thus a sum of two independent random variables usually has a distribution that is closer to gaussian than any of the two original random variables. therefore, the key concept in ica is based on maximization of non-gaussianity of the sources. there are various quantitative measures of non-gaussianity, one of the most popular among which is kurtosis . one of the most popular ica algorithms based on finding the local maximum of the absolute value of kurtosis is fastica  <cit> .

the open-source matlab package fastica  <cit>  was used as the implementation of the ica algorithm in this project.

multi-objective evolutionary algorithms
many decision making or design problems involve optimization of multiple, rather than single, objectives simultaneously. in the case of a single objective, the goal is to obtain the best global minimum or maximum , while with multi-objective optimization, there usually does not exist a single solution that is optimal with respect to all objectives. therefore, the goal of multi-objective optimization is to find a set of solutions such that no other solution in the search space is superior to them when all objectives are considered. this set is known as pareto-optimal or non-dominated set  <cit> .

since evolutionary algorithms  work with a population of individuals, a number of pareto-optimal solutions can be found in a single run. therefore, an application of eas to multi-objective optimization seems natural. the first practical moea implementation was the vector evaluated genetic algorithm  proposed in  <cit> . although it opened a new avenue in multi-objective optimization research, the algorithm seemed to have some serious limitations, at least partially due to the lack of considerations of dominance and elitism  <cit> . to deal with the first of the above considerations, a non-dominated sorting procedure was suggested in  <cit>  and various implementations based on that idea of rewarding non-dominated solutions followed  <cit> . elitism, in other words the notion that "elite" individuals cannot be expelled from the active gene-pool by worse individuals, has recently been indicated as a very important factor in moeas that can significantly improve their performance  <cit> . both these aspects, while preserving the simplicity of implementation of the original vega, were taken into consideration in the design of the end-vega algorithm described here.

a c++ evolutionary algorithms library implemented by tgs was used for the meoa experiments.

rough sets
the theory of rough sets  deals with the classificatory analysis of data tables  <cit> . the main idea behind it is the so-called indiscernibility relation that describes objects indistinguishable from one another. the indiscernibility relation induces a split of the universe , by dividing it into disjoint equivalence classes, denoted as b . these classes can be used to build new partitions of the universe. partitions that are most often of interest are those that contain objects that belong to the same decision class. it may happen, however, that a concept cannot be defined in a crisp manner. the main goal of rough set analysis is to synthesize approximations of concepts from acquired data. although it may be impossible to precisely define some concept x, we can approximate it using the information contained in b by constructing the b-lower and b-upper approximations of x, denoted by bx and x respectively, where, bx = {x|b ⊆ x} and x = {x|b ∩ x ≠ ∅}. only the objects in bx can be with certainty classified as members of x, based on the knowledge in b.

a rough set can be characterized numerically by the so-called quality of classification:



where bx is the lower approximation of x, b¬x is the lower approximation of the set of objects that do not belong to x, and u is the universe.

another very important aspect of rough set analysis is data reduction by means of keeping only those attributes that preserve the indiscernibility relation and, consequently, the set approximation. the rejected attributes are redundant since their removal cannot worsen the classification. there are usually several such subsets of attributes and those that are minimal are called reducts. finding a global minimal reduct  is an np-hard problem. however, there are many heuristics  designed to deal with this problem.

rsl – the rough set library  <cit>  was used for the estimation of the rs-based fitness function measures in this project.

ica, rs, and moea-based classificatory decomposition
the main concept of classificatory decomposition  was motivated by the hybridization of eas with sparse coding with overcomplete bases  introduced in  <cit> . using this approach, the basis functions as well as the coefficients are being evolved by optimization of a fitness function that minimizes the reconstruction error and at the same time maximizes the sparseness of the basis function coding. this methodology produces a set of basis functions and a set of sparse  coefficients. this may significantly reduce dimensionality of a given problem but, just as ica, does not assure the classificatory usefulness of the resultant model.

in the approach proposed here, the sparseness term is replaced by a rough sets-derived data reduction-driven classification accuracy measure. this should assure that the result will be both "valid"  and useful for the classification task. furthermore, since the classification-related constituent also searches for a reduct, the classification is done with as few as possible basis functions. finally, the single-objective ea utilized in the aforementioned technique is replaced by a multi-objective approach, in which the ea deals with the reconstruction error and classification accuracy, both at the same time  <cit> .

since the approach proposed here is based upon finding a solution satisfying two potentially conflicting goals , an application of moeas seems natural. in the experiments described here, we investigate a simple extension of vega, which supplies it with elitism and non-dominance, lack of which is known to be its main drawback. we call this extended algorithm end-vega .

end-vega
the main idea in vega is to randomly divide the population, in each generation, into equal subpopulations. each subpopulation is assigned fitness based on a different objective function. then, the crossover between the subpopulations is performed as with traditional eas, with an introduction of random mutations.

as indicated earlier, vega has several quite significant limitations related to the lack of dominance and elitism. to address the former, we propose a simple approach based on multiplying the fitness of a given individual by the number of solutions that this individual is dominated by . since the fitness function is being minimized in this project, the dominated solutions will be adequately penalized. to deal with the latter, we utilize the idea of an external sequential archive  <cit>  to keep track of the best-so-far  solutions and to make sure that their genetic material is in the active gene-pool.

the general schema for the end-vega algorithm
the general schema for the application of end-vega for classificatory decomposition can be represented by the following pseudo-code:

t = 0;

p := initializepopulation();

a := initializearchive();

while  do

         := dividepopulation  );

        evaluatefitness ,  freconstruction );

        evaluatefitness ,  fclassification );

        t := t + 1;

        prec.  := select );

        pclass.  := select );

        p := crossover, pclass. , a );

        mutate  );

        a := getnondominated ∪ a );

end while;

chromosome coding
each chromosome forms a complete solution to a given classificatory decomposition task and provides a description of both the set of basis functions and the coefficients for all the signals in the training data set. for example, for n signals with n samples each, and the task of finding m basis functions, the chromosome will be coded in the way presented in fig.  <dig> 

each of the m basis functions has the length of the original input signal , and there are n vectors of coefficients  of dimensionality equal to the number of basis functions .

fitness evaluation: reconstruction error
the measure employed in this project to calculate the distance between the original and the reconstructed signals is the well known 2-norm  <cit> , referred to in signal processing as the signal energy-based measure, presented in .



where x represents the original signal, m is the matrix of basis functions, a is a set of coefficients, and t =  <dig> .n, where n is the number of samples in the signal.

in order to deal with raw signals which can have large numbers as values , a simple normalization of the energy-based measure by the energy of the original signal is proposed  <cit> :



subsequently, the reconstruction error fitness function frec for a chromosome p takes the following form:



where  is the normalized reconstruction error for the ith signal and n is the total number of the input signals.

fitness evaluation: classification accuracy and reduction in the number of coefficients and basis functions
the problem of maximizing the classificatory competence of the decomposition scheme, and at the same time reducing the number of computed basis functions, can be dealt with by the application of rough sets. in this project, the rough sets-based quality of classification, as presented in , is used for the purpose of estimating the classificatory aptitude.

the quality of classification is computed directly on the candidate reduct, which can be computed by any of the existing algorithms/heuristics. in this project, we are utilizing a variation of a simple greedy algorithm to compute a single candidate reduct only, as described by johnson in  <cit> .

note that the main objective that deals with the classificatory capability of decomposition can actually be considered a bi-objective optimization problem itself. on one hand, we are looking for the best possible classification accuracy, but on the other, we want to use as few basis functions as possible. however, based on previous applications of eas in the search for reducts, as described in  <cit> , we decided to deal with it by minimizing a single-objective fitness function that is simply a summation of the classification error and the relative length of the reduct, as shown in .



where p is a given representative , l is the length of the potential reduct r , normalized by the total number of conditional  attributes m, and γr is the quality of classification coefficient for the candidate reduct r.

an interesting question here is what to do with the coefficients  that are not a part of the reduct. since we are looking for the best possible classification accuracy, while using as few basis functions as possible, some mechanism capable of emphasizing the "important" coefficients/basis functions would be advisable. a solution to this problem is possible due to the application of the "hard" fitness computation idea, which allows the fitness function itself to introduce changes directly to the genetic material of the evaluated chromosome  <cit> . in this paper we propose to utilize a coefficients/basis functions annihilation approach, which simply zeroes-out the "not important" genetic material. the idea here is that if we remove the basis functions that are not vital in the classification process, the ea will improve the remaining basis functions in order to compensate for an increase in the reconstruction error.

RESULTS
experimental data
the dataset used in this study was derived from neurophysiological experiments performed at arkansas state university  <cit> . in the experiments, recordings in the form of evoked potentials  of a duration of  <dig> second triggered by an auditory stimulus were collected from the cerebral cortex of two rats. one of the animals had been exposed to the cigarette smoke in utero , while the other had not. the research problem here is to investigate how treatments  could alter responses to discrete stimuli.  <dig> signals were registered for the unexposed animal and  <dig> for the exposed one. the eps were sampled at the rate of  <dig> khz. the original signals for the unexposed and exposed rats are shown in fig.  <dig> 

analysis
in the first step of the analysis described in this paper, the fastica algorithm was utilized to compute the ics to be used in the initial population in the moea. the algorithm yielded  <dig> ics  along with the corresponding coefficients. as typical with ica, the reconstruction was nearly perfect , but the entire set of  <dig> components had to be used to achieve that level of precision. furthermore, the differentiation between the two ep classes , based on all the corresponding coefficients, was not completely accurate . finally, as the cardinality of the resultant set of ics was the same as the number of the input signals, there was no indication as to which of the discovered independent components were significant for the underlying classification task.

in order to investigate the feasibility of the proposed approach, a number of moeas was launched simultaneously. the best results were obtained with the number of maximum possible generations set to  <dig> and the size of the population set to  <dig>  mutation probability was initialized with a small random value and was being adapted along the evolution process . crossover probability was randomly determined in each generation . single-point crossover was utilized. all genetic operators were applied across both decision classes, as the ica-derived initial ics were computed for the entire dataset, without any additional information regarding the classification.

due to the very limited available number of signals, the entire dataset of  <dig> eps was used for both training and testing in our experiments. in the future, to assure a higher robustness of the response obtained from the classifier being created along the decomposition process, the data should be split into training and testing parts.

several variants of ica used to initialize the population in end-vega were tried. both initialization of the full as well as a part of the population were simulated. in the first case, the changes in the basis functions can only be introduced by mutation, while in the second, some randomness is present from the beginning. the maximum allowable number of basis functions was set to  <dig>   <dig>  or  <dig>  in the first two cases, a random subset of  <dig> or  <dig> ics  was chosen for each chromosome, and in the third, a permutation of all  <dig> ics was used.

as an example, fig.  <dig> presents a set of components, averaged for the unexposed and exposed animal separately, for  <dig> basis functions determined to be a 5-element reduct. the figure represents an average contribution of the basis functions in generation of the eps in the unexposed and the exposed animal respectively.

even a quick visual analysis of fig.  <dig> reveals significant differences in how the sources are represented in the unexposed and the exposed rat. the dissimilarities can be simply expressed by amplitude variations , or can be as major as the sign reversal . further analysis of such phenomena can provide an interesting insight into the mechanisms behind the influence of nicotine on cortical neural activity. it is interesting to note that the general shape of the independent components computed by ica has been preserved , but, as shown below, the reconstruction error is noticeably smaller thus indicating an improvement of the reduced set of resultant components.

the average reconstruction error was significantly improved as compared to the previous study  <cit> , especially in the case of the full set of the ics being used to initialize the moea. note, however, that this set was still reduced to about  <dig>  thus determining the ics important for classification and at the same time "improving" them to account for the increase in the reconstruction error caused by removing the other  <dig> components, which were not classification-relevant according to the reduction algorithm. this impact of the improvement of the selected basis functions is clearly visible in table  <dig>  where we compare the reconstruction error of  <dig> components generated and "improved" by moea to a set of corresponding "non-improved" components taken directly from ica. the collection of the corresponding original components taken directly from ica was manually created by a visual comparison of the components shown in fig.  <dig> to the ica's basis functions presented in fig.  <dig>  as indicated earlier, the general shape of the independent components computed by ica has been preserved, therefore it is easy to identify the prototypical ics.

as for the classification accuracy, it was in most cases reasonably high  and the problems appeared to be related to  <dig> unexposed eps being classified as exposed. the accuracy was statistically significantly improved as compared to ica. the determined number of the basis functions required to preserve that accuracy  oscillated around  <dig>   <dig>  and  <dig>  for the maximum allowable number of  <dig>   <dig>  and  <dig> of the basis functions respectively.

in order to assess and quantify the classification usefulness of the coefficients computed with the presented method and compare them with the ones obtained with ica, a two-layer linear artificial neural network   <cit>  was trained using the coefficients and tested for generalization. the ann was trained with the leave-one-out training scheme: the training was performed n times, in each training instance coefficients that represent n- <dig> input signals were used for training, and the classifier was tested with the remaining  <dig> vector of the coefficients. a different vector of coefficients was used for testing in each training run. the testing error was measured as the difference between the ann's output and the class label :

ei = oi - yi,       

where oi is the ann's output for the ith vector of coefficients and yi is the class label of the signal represented by the ith vector of the coefficients. the generalization is computed as the mean squared error , using all testing errors:



since ann training strongly depends on the initial weight values , the leave-one-out training was repeated  <dig> times, and generalization errors were averaged. the resulting mean generalization errors are listed in table  <dig>  where the columns present the utilized decomposition method and its parameters, obtained generalization error, and significance of the fact that the decomposition method produces component coefficients that yield lower emse than those obtained with ica, measured by the left-tailed t-test p-value.

the relatively large values of errors in table  <dig> are due to the utilization of a linear classifier. the generalization error emse measures the average difference between the classifier's response and the desired response. because a simple classifier was utilized, emse also measures how easy the classification of the vectors of coefficients is. hence lower generalization error indicates that a given decomposition method produces components that are more useful for classification of the studied signals.

CONCLUSIONS
this article presents a general framework for the methodology of classificatory decomposition of signals based on hybridization of independent component analysis, multi-objective evolutionary algorithms, and rough sets. in order to investigate the impact of the stimulus on the sources of neural activity, we designed a classification system that is capable of "predicting" if a given signal was registered under one or the other condition, solely based on the decomposition coefficients. thus the relation between the stimuli and the sources can be analyzed. the preliminary results described here are very promising and further investigation of other moeas and/or rs-based classification accuracy measures should be pursued.

the incorporation of ica-derived basis functions and coefficients as the starting point in the moea significantly improved the reconstruction error and more closely related the concept of classificatory decomposition to the traditional signal decomposition techniques. on the other hand, one of the main advantages of our approach is the fact that rather than solely relying on often unrealistic assumptions about statistical independence of sources, it generates a reduced set of components that are relevant in the light of a given classification problem itself.

the modifications in end-vega, although they improved the reconstruction slightly and sped up the overall convergence of the algorithm as compared to previous experiments, worked much better in tandem with ica. in future research, it would be interesting to apply the results obtained using other decomposition approaches, e.g., principal component analysis   <cit>  or sparse coding with overcomplete bases   <cit> , as the initial population in moea.

authors' contributions
rb performed the underlying neurophysiological experiments. tgs and gmb implemented the testing software and performed the simulations. tgs, rb, gmb, mm, and aap analyzed the results. all authors read and approved the final manuscript.

