BACKGROUND
a common problem arising in bioinformatics is to classify experimental results into two categories, according to the presence or absence of some property of interest. such classification problems are widespread and diverse. for example, in genome-wide association studies , genotype data is collected at snp or other marker loci across the entire genome for a large number of cases and controls , and the marker loci are classified according to whether or not they are associated with the disease under study. another example is the prediction of protein subcellular localisation, in which predictors such as protein sequence are used to identify to which internal structures, or organelles, a protein will be transported after synthesis. a third example is the use of morphological differences to classify cells and tissues, for example to classify whether a cell is cancerous or not, or to determine whether a cell has a parasitic infection.

despite the diversity of these applications, each can be reduced to binary classification, e.g. disease-associated or non-associated, trafficked or not, infected or parasite-free, etc. whatever the specific context, it is important to quantify the accuracy of the classifier, in order to assess the level of confidence one should place in the predictions, and so that alternative classifiers can be compared and ranked. classifiers can be assessed in terms of their sensitivity and specificity. the sensitivity of a binary classifier is the proportion of positive individuals that are correctly identified as such. similarly, the specificity is the proportion of negative individuals that are correctly identified.

ideally, there will be a gold standard data set available for evaluating classifiers, comprised of data for which the true classification of each individual is known with certainty, or at least for which there is an accepted best available classification. when a gold standard is available, it is straightforward to estimate the sensitivity and specificity as proportions of the gold standard positives and negatives respectively. however, it can happen that there is no gold standard. for example, if the classifier is a medical diagnostic test such as a swab for swine flu, there may not be any more accurate means of diagnosing the disease, or none that is affordable for a large study. because such diagnostics are experimental in nature, it is not even possible to simulate data against which to benchmark, and a gold standard for evaluating them may therefore be difficult or impossible to obtain. in bioinformatics the absence of a gold standard can occur because data that can be classified with perfect accuracy is either non-existent or too limited for reliable estimation. it may also be that any gold standard data that was available has already been used to train one or more of the classifiers, rendering the data unsuitable for comparing the classifiers. a concrete example is the analysis of genome-wide association studies, where the data set can include millions of individual snps, a negligible proportion of which are known to be associated with the disease. simulated data can sometimes be generated, but it may not be clear that it is sufficiently realistic. in such cases, it is still possible in principle to evaluate and compare competing classifiers, provided that multiple classifiers are available. in medical diagnostics, this is typically in the order of 2- <dig> classifiers.

the intuition here is that the extent to which competing classifiers agree or disagree provides information about the reliability of each classifier. in the absence of a gold standard, all that is known is the imperfect binary classifications, which can be organised into a matrix such as shown in table  <dig> for k classifiers and n individuals. note that the true classification is not known for any individual. however, in broad terms, where two classifiers tend to agree  our confidence in both of them increases, whereas where two classifiers tend to disagree , we cannot have high confidence in both. this intuition is given a mathematical expression in the bayesian models discussed below.

a variety of techniques have been proposed in the medical statistics literature for comparing diagnostic tests in the absence of a gold standard. a spectrum of approaches has been developed to suit specific variations of the problem; for example, some approaches assume log-linear models of errors, as compared to error models that assume normally distributed errors. approaches also differ in methodology, for example, maximum likelihood approaches compared to bayesian approaches. not only do the approaches vary, but the assumptions also vary, with some approaches requiring data from two or more populations with different prevalences of the disease , and others considering tests re-administered to the same individuals at two or more time-points . a review of the diverse approaches and methods is provided by  <cit> . we focus on the simplest and most common setting in practice, in which k binary classifiers are applied to n individuals randomly selected from a single population, each at a single time-point. in particular, we consider the bayesian model of  <cit> , and its potential for application to data sets in the bioinformatics domain. this model is described in detail in the first sub-section of the methods. to the best of our knowledge, the model has not previously been applied in this domain, despite the numerous potential applications. we provide a new, concise and highly efficient implementation in winbugs. our implementation is freely available, applicable to any number of classifiers, and as we demonstrate below, is able to handle genomic-scale data sets.

once classifiers have been compared, the question naturally arises how to combine them to form a new classifier that is better than any of the constituents. a simple method is to take a consensus, that is, to classify an individual as positive if most of the component classifiers ‘vote’ for a positive classification, and classify an individual as negative otherwise. a weighted consensus, in which the vote of some classifiers counts more than others, is also possible. but what is the optimal way to combine classifiers? this problem has been extensively studied . however, the problem of how to combine classifiers in the absence of gold standard data does not appear to have been studied, and in particular the potential for the model of joseph et al. <cit>  to solve this problem has not been explored. in the second sub-section of the methods, we present a new method that estimates the sensitivity and specificity of all logical combinations of classifiers in the absence of a gold standard. the method is implemented in r and again the code is freely available. we note that if no additional covariates are available to distinguish individuals, that is, if the only information we have for each individual is its set of classifications, then logical combinations of the classifiers encompass all possible ways of combining the classifiers, including all weighted voting schemes. the use of additional covariates to better combine classifiers may be a possible extension of the methods proposed here, but is beyond the scope of this paper.

RESULTS
the model was loaded into winbugs, and run with three test data sets: protein subcellular localisation, to test performance in the presence of a gold standard; swine flu diagnostics, to test performance with a small data set; and classification of snps in macular degeneration, to test performance with a large data set. test data are provided in additional file 1: section s <dig> and on our website.

evaluation with gold standard data: classifying protein subcellular localisation
we first evaluated our method relative to a gold standard. protein sequences were obtained from the arabidopsis proteome in fasta format from the website of  <cit> : http://bioinfo <dig> noble.org/atsubp/. the localisation of these sequences to each of the cytoplasm, chloroplast, golgi body, mitochondrion, nucleus, plasma membrane, and extracellular region is known. the protein sequences were run through three classifiers available from the atsubp website: the amino acid composition-based svm , dipeptide composition-based , and n-center-c terminal region-based  classifiers. these classifiers were converted into binary classifiers by taking one organelle  and treating localisation to that organelle as a positive result, with localisation to a different organelle as a negative result. results for all seven organelles are presented in additional file 1: section s <dig> , but in the main text we present only the results for chloroplasts, which were representative .

the output of each classifier was converted to a sequence of 0s and 1s, indicating which proteins were localised to the chloroplast region  and which were not . for each parameter of the model, summary statistics for its marginal posterior distribution were obtained . density plots  and time series plots  were also produced. sensitivities and specificities for the classifiers were evaluated using the gold standard classification, and these are shown as vertical lines in figure  <dig>  it should be noted that these gold standard estimates also have standard errors  because they are based on finite sample sizes.

our inferred mean posterior sensitivities were typically greater than  <dig> standard deviations above gold standard estimates, but the latter were nevertheless within the range of values obtained. a similar statement applies to specificities, and the prevalence of chloroplast localisation. importantly, the classifiers were ranked in the correct order of sensitivity and specificity. that is, classifiers  <dig>   <dig> and  <dig> had increasing sensitivity and decreasing specificity . these comments apply to all seven organelles, with the caveat that out of the  <dig> sensitivities and specificities,  <dig> cases resulted in a tie according to the gold standard. for example, classifiers aa and dp had equal sensitivities when applied to mitochondria. in these cases, the method has not identified a tie, but has otherwise ranked the classifiers correctly.

application to a small data set - swine flu
we then tested our method on data for the diagnosis of swine flu in patients, where the data set is very small and no gold standard is available. the data contains the diagnosis of n= <dig> individuals for presence or absence of swine flu using k= <dig> different diagnostic tests. the tests are referred to as the nasopharyngeal aspirate  and nasal swab . the parameters of the model were each initialised to  <dig>  and the model allowed to run for  <dig> iterations. summary statistics and time series plots were obtained for all parameters . there was no discernible burn-in, indicating rapid convergence.

density plots were produced using the last  <dig> iterations of the time-series, as shown in figure  <dig>  the inferred densities exhibited low standard deviations, with an average standard deviation of  <dig>  and a maximum of  <dig> , indicating surprisingly good confidence in determining the parameters with a small data set . notice in figure  <dig> that the sensitivity of the npa test  is substantially higher than the sensitivity of the ns test . however, the specificity of npa  is marginally lower than the specificity of ns . on balance, the npa appears to be the better test, and this conclusion is supported by the ranking criteria that we introduce below . note that, in additional file 1: section s <dig> . <dig>  the npa test  scores higher than ns  according to all four ranking criteria.

application to a large data set - snp classification
to test the method’s performance on a large data set, data from a genome-wide association study was analysed. this study identified snps associated with age-related macular degeneration, according to k= <dig> independent classifiers . the classifications were produced by running the classifiers on a filtered set of snps in the hapmap phase i+ii ceu data . the post-filtered set of snps contained n= <dig> snps. the first classifier was plink  <cit> , which took as a predictor variable the dosage of the minor allele, and as covariates, the age and gender of each individual. the second classifier was a gene-based method using vegas  and the third classifier was an evaluation of the proportion of significant pairwise interactions between snps involving each snp . these methods all assigned p-values to each snp. it was decided  to assign approximately the top  <dig> snps, as classified by each method, to the disease-associated class. to do this, thresholds needed to be set for each classifier. the maximum thresholds that accepted the smallest number of snps equal to or greater than  <dig> were  <dig>  for plink,  <dig>  for the snp by snp interaction method and  <dig>  for the gene-based method. these accepted numbers of snps were  <dig>   <dig> and  <dig>  respectively. for comparison, a second data set with approximately  <dig> snps was also generated, using thresholds  <dig> ,  <dig>  and  <dig>  to obtain  <dig>   <dig> and  <dig> positives respectively.

the parameters of the model were again initialised to  <dig> , and the model run for  <dig> iterations. summary statistics were produced for each parameter . the last  <dig> iterations were used to produce plots of the densities of the sensitivity, specificity and estimated prevalence of disease association .

the summary statistics and density plots show smaller standard deviations than the swine flu data, indicating greater confidence in predicting model parameters, which can be attributed to the larger data set. our method finds that the plink classifier had significantly higher sensitivity than the other two classifiers, and slightly higher specificity as well. however, all three had high specificities, as a consequence of classifying such a small proportion of the data as positive.

the difference between the results for the different thresholds is unexpected and informative. our expectation was that using higher thresholds would increase the sensitivities of all three classifiers and decrease the specificities. moreover, ideally the method should obtain roughly the same estimate of disease prevalence regardless of the thresholds, since the underlying population is the same. instead, using the higher thresholds resulted in lower estimates of both sensitivity and specificity, but only slightly . a more dramatic change is that the estimate of disease prevalence increased approximately six-fold. this unexpected behaviour may be indicative of conditional dependence between the classifiers.

notably however, the ranking of the three classifiers by sensitivity or by specificity remains the same - with plink significantly outperforming the other classifiers on both measures. this observation underscores our major point that the ranking of classifiers is in general robust to violation of the assumption of conditional independence.

inference of the best combination of classifiers
the r-script described in the methods was used to invoke the winbugs model from r , and the model was rerun for all three test cases for a burn-in of  <dig> iterations. then, for each case the model was run for a further  <dig> iterations, and at every iteration an estimate of the sensitivity and specificity was calculated for all possible logical combinations of the classifiers. only the last  <dig> iterations were used for the following analyses.

to determine which logical combination of the classifiers performed best, we applied the four ranking criteria based on the  product,  sum of squares,  sum of absolute values, and  minimum of the sensitivity and specificity. summary statistics for sensitivities and specificities of all logical combinations of the swine flu classifiers, and selected simple combinations of the chloroplast localisation and snp classifiers, are shown in tables  <dig>   <dig> and  <dig>  for all three test cases, ranking criteria 1- <dig>  identified the union of all classifiers as the best combination. for the snp data, ranking criterion  <dig>  also inferred that the union of all classifiers was best, while for the subcellular localisation and swine flu data, the best combination was a union of all but one of the classifiers. the better performance of a union of classifiers is due to higher sensitivity at the expense of lower specificity, as the union of all classifiers necessarily has higher sensitivity and lower specificity than any union of a subset of the classifiers.

sd: standard deviation.

‡optimal combination using ranking criteria  <dig>   <dig> and  <dig> 

†optimal combination using ranking criterion  <dig> 

sd: standard deviation.

‡optimal combination using ranking criteria  <dig>   <dig> and  <dig> 

†optimal combination using ranking criterion  <dig> 

sd: standard deviation.

‡optimal combination using ranking criteria  <dig>   <dig>   <dig> and  <dig> 

for the two cases with < <dig> data points , the optimal logical combination was the same in up to  <dig>  iterations of the gibbs sampler. however, for the snp data with more than  <dig> data points, the optimal classifier was the same at every iteration. this is expected, as more data should increase the confidence with which the optimal classifier can be identified.

posterior density plots for the sensitivity and specificity of all possible logical combinations of the swine flu classifiers are presented in additional file 1: section s <dig> , and for selected logical combinations of the subcellular localisation classifiers in additional file 1: section s <dig> . additional file 1: sections s <dig>  to s <dig>  also contain summary statistics for the four ranking criteria in each case, generated using the r code in additional file 1: section s <dig> . a curious anomaly is observed for the swine flu data: ranking method  <dig> identified the npa classifier  as best in a majority of mcmc iterations, yet the average of the method  <dig> score is slightly higher for the union of the npa and ns classifiers . we note that the standard deviations of the ranking scores are quite large relative to the differences between ranking scores, which may suggest that combinations of classifiers other than that identified as ‘best’ remain plausible candidates. nevertheless it is clear that the union of all classifiers ranks well if not best for all data sets and any ranking criterion examined here.

run times
run times for the various data sets are shown in table  <dig>  times for the  <dig> iteration runs for the chloroplast localisation, swine flu, and snp data are in the column headed ‘winbugs’. the times were approximately the same for the swine flu and chloroplast data sets, despite the greater number of data points and extra classifier in the latter. the snp data set was > <dig> times larger and as expected the run time was much greater.

the run times of the  <dig> iteration runs from the previous sub-section included an r component and a winbugs  component, shown in the last two columns. winbugs apparently runs faster when called from r. although the r combination algorithm  is o, the main time cost of the genomic scale snp runs  is in the run time of the winbugs comparison algorithm , which is only linear in k.

CONCLUSIONS
the method presented in this paper addresses two significant problems with ubiquitous applications in bioinformatics: comparing binary classifiers in the absence of a gold standard, and identifying the optimal logical combination of such classifiers. using bayesian models developed for evaluating medical diagnostic tests, we present the first applications of these models in the bioinformatics domain and demonstrate their feasibility and utility for comparing classifiers on genomic scale data sets. a new, concise and highly efficient implementation of these models was developed in winbugs, and is the first freely available implementation applicable to an arbitrary number of classifiers. to identify the optimal logical combination of classifiers, we developed an entirely new algorithm and again demonstrated its feasibility for genomic scale data sets. the algorithm is the first to employ the above-mentioned bayesian models to evaluate logical combinations of classifiers and indeed apparently the first to systematically evaluate all logical combinations. it is implemented in r and is freely available. the algorithm is o in the number of classifiers k, and thus further research is required for large k.

the methods were evaluated on a protein subcellular localisation data set for which a gold-standard data set was available for the purpose of comparison. some discrepancy in the estimates of sensitivity and specificity was expected because a key assumption of the model - conditional independence of the classifiers - is often violated in practice. however, we found that the discrepancy was in most cases small and more importantly, that the method was able to correctly rank the classifiers.

in all of our examples, a simple union of the classifiers was found to be optimal according to three out of four alternative ranking criteria . while this finding is unlikely to be general, we propose as a rule of thumb that the union of classifiers is likely to be close to the optimal logical combination.

