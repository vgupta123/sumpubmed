BACKGROUND
making an effective and efficient medical diagnosis is pivotal in clinical daily practice, clearly because of the impact of this singular decision making process in the eventual illness trajectory and disease management. for such a reason, the optimization of the diagnostic process in terms of number and duration of patient examinations, with corresponding accuracy, sensitivity, and specificity, is known to reduce morbidity and mortality rates, control costs and improve both doctor-patient and community-facility relationships  <cit> .

the task of medical diagnosis, like almost any other diagnostic process, is made more complex to obtain even for a medical expert because of a web of relevant uncertainties, in the form of information incompleteness, impreciseness, fragmentariness, not fully reliability, vagueness and contradictoriness  <cit> . specifically, patients may not be able to describe exactly the natural history of their disease in terms of what has happened to them or how they feel; doctors and health care practitioners may not understand or interpret exactly what they hear or observe; the accuracy of available laboratory reports, which may come with some degree of error; and the effects of treatment in an individual patient or in a group or population as a whole in terms of how diseases alter the normal functioning of the body  <cit> . the need to identify the most accurate medical diagnosis in a very timely manner increases dramatically in the case of mortal diseases, as both the rapid and accurate diagnosis and prompt initiation of treatment are recognized as necessary conditions to limit further complications, cut costs and reduce human suffering.

in order to improve the possibility of early and accurate diagnosis of illness, there is thus the need for the application of diagnostic decision support systems  in the process, because these are known to improve practitioner's performance, reduce costs and improve patient outcomes  <cit> .

the first ddsss proposed in literature relied on crisp models based on thresholding for solving medical classification problems. nevertheless, they neither take into account the fuzziness of input data nor reproduce the expert decision-making process applied in a vague-laden domain such as medicine. as a matter of fact, the decision-making model every trustworthy physician has in mind to perform heuristic diagnosis is often pervaded by uncertainty and vagueness.

recently, ddsss based on a multi-valued logic and, in particular, on fuzzy logic, have been applied to medical classification problems  demonstrating their capability both to overcome the problem of managing imprecise and uncertain knowledge and offer a support for the medical decision making process. moreover, fuzzy logic is quite close to natural language and allows presenting the results to clinicians in a more natural form. this makes fuzzy based ddsss more acceptable to human users than black box systems, because both the semantic expressiveness and reasoning  are comprehensible and may be validated by human inspection.

crisp and fuzzy modeling
the first ddsss were mainly based on crisp logic, which provides an inference morphology for drawing conclusions from existing neat and clear-cut information: specifically, new truths can be inferred from old ones. in more detail, it relies on rules, defined as conditional statements written in the following form: if crispantecedent then crispconsequent where crispantecedent is a crisp-logic expression made of one or more simple predicates linked by logical operators and depending on input crisp variables, and crispconsequent is an expression of output variables which are assigned crisp values. a crisp variable c  is characterized by a set of crisp values c it can assume, whereas a predicate p classifies the values belonging to a variable into two groups or categories, i.e. values that make it either true or false, respectively. in other words, the set defined by p is written as: {c|p}, and is just a collection of all the values for which p is true. for instance, {c|c is a positive integer less than 3} is the set { <dig> }.

crisp modeling is the task of determining the parameters characterizing a rule base, classified into the followings:

• structural parameters. related mainly with the size of the rule base, they include the number of variables involved in the rules and the number of rules.

• connective parameters. related with the topology of the rule base, these parameters include antecedents, consequents, and weights of the rules.

unlike crisp logic, fuzzy logic resembles human reasoning in its use of vague information to generate decisions  <cit> , where vague predicates are used and values belonging to a variable cannot be classified into two groups . in this sense, fuzzy logic incorporates an alternative way of reasoning, which allows modeling complex systems using a higher level of abstraction originating from knowledge and experience  <cit> .

in more detail, in fuzzy logic, a fuzzy variable f  represents a concept that is measurable in some way either objectively or subjectively and is defined by a set of fuzzy terms t , and by the membership functions μt associated to these terms; fuzzy terms set a membership value from  <dig> to  <dig> to elements u within a predetermined range u  as follows:

  t={|u∈uandμt:u→ <cit> } 

the central notion, thus, is that truth values or membership values are indicated by a value on the range  <cit> , with  <dig> representing absolute false and  <dig> representing absolute truth. for instance, figure  <dig> shows the linguistic variable heart rate made of three terms , and defined in u =  <cit>  bpm .

the fuzzy inference morphology relies on a fuzzy inference system  based on if fuzzyantecedent then fuzzyconsequent rules, where fuzzyantecedent is a fuzzy-logic expression made of one or more simple fuzzy expressions linked by fuzzy operators and depending on input fuzzy variables, and fuzzyconsequent is an expression of the output variables which are assigned fuzzy terms.

essentially, fuzzy reasoning is made of four steps, namely fuzzification of input variables, rule evaluation, aggregation of rule outputs and, finally, defuzzification. fuzzification of input variables converts crisp  inputs into fuzzy terms. for each fuzzy rule, rule evaluation applies such fuzzified inputs to its antecedents, making use of a fuzzy operator in case of multiple antecedents, with the final aim of generating a single value indicating its degree of activation. such a value is, then, used in the implication to infer the conclusion of the rule. aggregation combines the membership functions of all rule consequents previously evaluated in order to generate a single fuzzy set as output. finally, defuzzification determines the best representative crisp value of this aggregated output fuzzy set.

fuzzy modeling is the task of determining the parameters of a fis, classified into the followings  <cit> :

• logical parameters. they include the shape of the membership functions, the fuzzy logic operators applied for and, or, implication, and aggregation operations, and the defuzzification method.

• structural parameters. related mainly with the size of the fuzzy system, they include the number of variables involved in the rules, the number of membership functions defined for each linguistic variable, and the number of rules.

• connective parameters. related with the topology of the system, these parameters include antecedents, consequents, and weights of the rules.

• operational parameters. these parameters define the mapping between linguistic and numeric representations of the variables, so characterizing the membership functions of the linguistic variables.

the most usual, and cheapest, way for modeling medical knowledge in fuzzy-based ddsss is asking an expert to write if-then rules. moreover, after formalizing the expert's knowledge under the form of rules, the designer and the expert have to choose the shape and location of membership functions for all the linguistic values related to all the variables involved. this step, sure enough, requires both medical expertise and technical intervention along with great effort to identify which among the design choices are suited to the given problem. alternatively, an emerging solution is represented by data driven fuzzy modeling, that is being widely adopted in different application domains to automatically generate a rule base from data, even if the interpretability is not guaranteed in many situations and redundancy can occur in the rules produced.

according to  <cit>  a fuzzy model is interpretable when:  the fuzzy terms associated to a variable  are interpretable as linguistic labels,  the rule base is as small as possible, and  the if-part of each rule does not includes all the independent variables but only a subset of them.

state of the art
a number of fuzzy-based ddsss has faced up these challenges, and has attempted to address the subjects of knowledge acquisition, representation, and utilization in medical diagnosis.

in  <cit> , a rule-base self-extraction and simplification method is proposed, devised to establish interpretable fuzzy models from numerical data. a fuzzy clustering technique associated with a fuzzy partition validity index is used to extract the initial fuzzy rule-base and find out the optimal number of fuzzy rules. to reduce the complexity of fuzzy models while keeping good model accuracy, some approximate similarity measures are presented and a parameter fine-tuning mechanism is introduced to improve the accuracy of the simplified model. experimental results are reported with respect to different case studies, such as function approximation, dynamical system identification and mechanical property prediction for hot rolled steels. these test-cases are characterized by a relatively small number of input-space variables. no experimental test has been reported for problems characterized by a high number of input-space variables.

in  <cit> , an evolving hierarchical fuzzy system based on probabilistic incremental program evolution is presented. the use of hierarchical fuzzy systems allows to limit both the number of rules and the number of fuzzy operations with respect to single level systems. worthy results are described for case studies concerning non linear system identification, such as makey-glass chaotic time series prediction, and the iris and wine classification.

in  <cit>  a data-driven innovative approach is presented for generating a fuzzy rule based decision support system for the diagnosis of coronary artery disease. the implemented methodology relies on four stages.

in the first stage, a decision tree is induced from the dataset, while in the second stage, a set of rules is extracted from it. this set of rules is in disjunctive normal form  and involves crisp variables modeling neat and clear-cut quantities. it can be used as a whole to classify new incoming data coherently with the knowledge embedded into the initial dataset. in the third stage, the crisp model is fuzzified, i.e., the crisp rules are transformed into fuzzy ones, using a fuzzy membership function instead of the crisp one and definitions of s and t norms. finally, in the fourth stage, the parameters entering the fuzzy model are adapted using a global optimization technique.

in  <cit>  a generic methodology is presented for the automated generation of fuzzy models. the methodology is realized in three stages. initially, a crisp model is created whereas, in the second stage it is transformed into a fuzzy one. in the third stage, all parameters entering the fuzzy model are optimized. a specific realization of this methodology is implemented, using decision trees for the creation of the crisp model, the sigmoid function, the min-max operators and the maximum defuzzifier for the transformation of the crisp model into a fuzzy one, and four different optimization strategies, including global and local optimization techniques, as well as hybrid approaches.

in  <cit>  a generic approach to the design of interpretable data-driven fuzzy models, which can be used in the construction of ddsss, is proposed. the approach addresses several design steps, including fuzzy partitioning, rule learning, variable selection and rule base simplification. the fuzzy partitioning step consists in generating a collection of fuzzy partitions of various sizes from two to a user-defined maximum value, based upon indices or an objective function. the rule learning step includes two categories of methods, namely region based methods and prototype based ones. the rule base simplification merges some rules into a more generic incomplete rule, where some variables  appear in some rules. the variable selection determines the number of terms for a given variable necessary to get a good rule base, in terms of trade-off between its complexity and accuracy, measured by performance indexes.

the contribution of the work
recently, we informally introduced  <cit>  a methodology to design and develop a fuzzy-based ddss for medical classification problems by extracting fuzzy knowledge from data. in this work, we first propose a formalization of a refined and assessed version of such a methodology, which essentially specifies a flow of stages needed to develop a fuzzy-based ddss as well as the characteristics of the input and output produced and consumed in the different stages. as a result, it formalizes the role assumed by each stage in terms of its interface, but it does not provide any indication about how the single activities have to be done in the form of strategies to be adopted or algorithms to be applied, especially because this choice is strictly linked to the specific application domain. this issue is further motivated by the fact that the methodology, whose strength relies on its generality and modularity, has been thought as a basis for the development process of fuzzy-based ddsss by supporting the integration of alternative techniques in each of its stages. both the generality and flexibility make it applicable to almost any medical classification domain and, also, enable the possibility to test the efficiency of different methods in order to detect their best integration with respect to specific classes of problems.

in more detail, the assessed version of the methodology  includes six stages:  extraction of crisp rules,  selection of a significant partition from the extracted rule set,  reduction of the selected rule set,  creation of fuzzy rules,  generation of the whole fuzzy inference system and  its optimization. in the first three stages a set of crisp rules is initially created and, then, appropriately elaborated in order to be compliant with some characteristics, which are strongly necessary for this methodology to make the fuzzification feasible. in this work, such characteristics are formulated with the definition of fuzzifiability. successively, the last three stages are in charge of  transforming the  crisp rules into the corresponding fuzzy versions, i.e. in terms of connective and structural parameters,  defining the most appropriate logical parameters to be used in the fuzzy inference system and  optimizing all the operational parameters composing the fuzzy rules and, if required, also the relative relevance of each of them, specified in the form of a weight.

the methodology has been realized in the form of a modular and portable architecture according to a component-based software development , with the aim of defining a collection of components customizable or extensible by existing available solutions that are compatible to the original placeholders.

the architecture has been developed in java according to the object-oriented paradigm in order to create a truly portable ddss, solving the problem of having parts of it implemented for different platforms. the resulting architecture can be considered as well-suited for almost any medical domain where the real world is simulated in a broad sense and a diagnosis in terms of classification is required.

as a proof of concept, such an architecture has been used to instantiate a ddss example aimed at accurately diagnosing breast masses starting from the widely used wisconsin breast cancer dataset  to evaluate the feasibility of the methodology.

the manuscript is structured as follows: in methods, the proposed methodology is formally described and the choices and techniques which are identified for the specific realization are analyzed. furthermore, the design approach used for the development of the proposed architecture is introduced. in results, the architecture designed is diffusely described and a summary inspection of the main components is reported as well. moreover, the proof of concept ddss for validating the methodology is introduced and its results are discussed. finally, conclusions and on-going activities are outlined in the last section as closure to the paper.

methods
our methodology includes six stages, where the first three stages work with crisp models, whereas the last three ones are employed on fuzzy models .

rule extraction
the rule extraction stage is essentially devised to the extraction, from a specific input dataset, of a collection of if-then rules constituting the crisp rule base , specifically represented in a weighted disjunctive normal form . we work with this representation because of its high degree of compactness and knowledge synthesis. the crb, thus, is a disjunctive system of crisp rules, where at most one rule must be satisfied by an item of the initial dataset, i.e. the rules are linked by mutually exclusive or connectives. more formally, each crisp rule rijc  in the crb denotes the ith rule which predicts the jth class, with j =  <dig> .. m and i =  <dig> .. nj, where m is the total number of classes and nj is the total number of rules that predict the jth class, respectively. as a result, in general, there could be one or more than one rule for each class predicted, while each class is likely to be covered by at least one rule in the crb.

the structure of each rule rijc is composed of a conjunction of antecedent predicates , based on the set of features xc={xlc}, with l =  <dig> .. l , and one consequent term indicating the specific class predicted. more in detail, for the ith rule which predicts the jth class, given the sets of crisp predicates pijc={pijkc}, with k =  <dig> .. kij , and let yjc be the consequent crisp term representing the predicted jth class, its formulation is defined as:

  rijc:pij1c∧….∧pijkijc→yjc 

with each crisp predicate expressed as:

  pijkc≡ 

where xijkc is the specific feature selected from the set xc, opijkc is a comparison operator selected from the sets {=, ≠} and {<, >, ≤, ≥} in the cases of categorical and numerical features, respectively, and vijkc represents a categorical value or a crisp numerical threshold.

this dnf is labeled as weighted since each rule is associated with a degree of relevance, such as its coverage or accuracy, with respect to its predicted class depending on the domain of application and the specific dataset concerning this domain.

different solutions can be adopted to extract rules, ranging from purely logical approaches to statistically-based ones or relying on artificial neural networks, genetic algorithms and on non-connectionist machine learning   <cit> . independently from the specific method used, it is relevant to point out that it is possible to extract rules able to correctly classify an item in the dataset from its known features, i.e. every item in the input dataset is covered by exactly one rule in the crb, but without avoiding the possibility of overfitting the input dataset which can be characterized by a degree of uncertainty. this uncertainty may arise from two different sources. the first is mis-measurement, i.e., for a variety of reasons, the value of a feature or class may be incorrectly measured. the second source of uncertainty is the occurrence of extraneous factors not recorded, but affecting the results so that the class of an item in the dataset cannot be determined wholly from its recorded features. the resulting crb extracted in these situations tends to be very large and many rules reflect particular items in the training dataset which are very unlikely to occur in further examples, i.e. they cover a very small part of the input space, are matched only by a few examples, lack generality and can become counter-productive. this issue represents the motivation for the second stage, i.e. selection.

selection
the selection stage is in charge of determining the sufficient number of rules, as are necessary to get a good crb with respect to the specific dataset concerning the domain under observation, where a good crb represents a reasonable trade-off between complexity, determined by the number of rules, and accuracy, measured by appropriate performance indexes. the selection is done with the idea of granting two main factors emerged as primary determinants of interpretability. first, the number of rules should be small so involving that a full set of complete rules should be avoided since it can quickly lead to a combinatorial explosion when the number of features rises  <cit> . the second condition is strictly linked to the first one and is specific to complex systems with a large number of features: rules must not systematically include all the features, but only the important ones in the context of the rule, so generating the often called incomplete rules  <cit> .

different methods can be applied to perform the rule selection and, thus, implicitly also the variable selection, each of them exploiting ad-hoc heuristics guided by user-defined indicators  and suitable with respect to the different applications and their specific requirements. for example, the choice of the good crb can be piloted by a longest-match criterion, i.e. depending on the longest left-hand-side  of the rules that match an item of the dataset. its rationale is based on the conclusion that longer antecedent part will contain more accurate and richer information for the final classification than the shorter ones. differently, a most confident selection could be adopted, by identifying the rules with the highest confidence as the best one, where the longer rule is chosen in case of a tie. the rationale of this criterion is based on the assumption that the testing dataset will share the same characteristics as the training dataset used in the rule extraction. so if a rule has a high confidence in the training data, then this rule will also show a high confidence in the testing data, which means the class predicted by that rule will be most likely to occur next.

summarizing, this stage has a double relevance in terms of the functionalities offered: first, it performs the rule selection to achieve a smaller number of more general rules with the idea that it may have greater predictive accuracy on unseen data, at the expense of no longer correctly classifying some of the items in the original training dataset; then, it implicitly carries out also a feature selection leading to an incomplete rule base which takes into consideration only those features that are really required since included into the significant rules previously selected.

it is worth noting that the elimination of features in order to obtain incomplete rules could be undertaken at the extraction level since the rule extraction step can remove features from the whole rule, for example using statistical indexes. differently, this stage is intended to select only a subset of the features instead of the whole set of candidate ones for other reasons: i) it can be cheaper to measure only a reduced set of features; ii) prediction accuracy may be improved through exclusion of poorly significant features; iii) the final ddss to be built might be simpler and potentially faster when fewer input variables are used; iv) knowing which features are more relevant can give insight into the nature of the classification problem and allow a better understanding of the final ddss. at this point, in the context of the same rule rij given as output after the selection, with respect to numerical features, different predicates can contain the same feature selected from the set xc, i.e. xijlc=xijkc with l, k ∈  and l ≠ k. this consideration represents the motivation for the third stage, reduction,

reduction
the reduction stage is in charge of simplifying the structure of each crisp rule in order to make it compliant with some characteristics, which are strongly necessary for this methodology to make the fuzzification feasible. in this work, such characteristics are formulated with the definition of fuzzifiability. a crisp rule is defined as fuzzifiable, if and only if each of its numerical features appears in one or at most two predicates in its antecedent part, according to one of the following forms:

  pic≡ 

  piic≡ 

  piiic≡ 

this stage thus puts into effect a simplification procedure, that iteratively searches, in the context of each rule, each couple of predicates involving the same feature, using comparable operators and needed of being made compatible with the fuzzifiability. it is important to point out the meaning of comparable operators. two comparison operators are intended as comparable in this procedure if and only if they appear in one of the situations reported in  <dig>  independently of their order:

  comp≡∨ 

when a couple of candidate predicates is detected, since in the rule under evaluation they are connected by conjunctions , they can be reduced as follows:

  ∧→xc>maxif 

  ∧→xc<minif 

after applying this simplification procedure to every couple of candidate predicates, each resulting predicate will be formulated according to one of the forms defined in equations 4- <dig>  a clarifying note has to be reported about the categorical features. as they assume mutual exclusive values and, in each rule, the antecedent predicates are admitted to be connected only by means of conjunction operators, it is not possible  at all that two different predicates might contain the same feature assuming different values in the context of the same rule. thus, the reduction does not involve the categorical features in its simplification procedure.

fuzzification
after the first three stages, a crisp model made of rules based on clear-cut boundaries has been generated in accordance with the fuzzifiability property. successively, starting from the crisp model produced, the generation of the fuzzy model begins with the fourth stage, named fuzzification.

in more detail, in order to create fuzzy variables and terms, in this stage, crisp rules are translated into a corresponding fuzzy version, where every feature contained in the crisp rules is associated to a linguistic variable. it is relevant to mention that only features determined relevant to the classification by means of the previous stages are fuzzified. of course, also the predicted class is associated to a linguistic variable. successively, each linguistic variable is further characterized by a set of terms subjectively describing it. with respect to this issue, the degree of detail to be used in partitioning the universe of discourse of each variable, i.e. number of linguistic terms to be referred to it, has been chosen in accordance with the crisp model. summarizing, the fuzzy rule base  achieved in this stage continues to be a disjunctive system of rules. nevertheless, differently from the crisp case, where exactly one rule must be satisfied, the fuzzy rules are linked by simple or connectives, where at least one rule must be satisfied, i.e. one or more rules may be weakly or strongly activated simultaneously.

each fuzzy rule rijf  in the frb denotes the ith rule which predicts the jth class, with j =  <dig> .. m and i =  <dig> .. nj, where m is the total number of classes and nj is the total number of fuzzy rules that predict the jth class, respectively. the structure of each fuzzy rule rijf is composed of a conjunction of antecedent fuzzy predicates, based on the set of linguistic variables xf={xuf}, with u =  <dig>  ..., uc, , and one consequent fuzzy variable indicating the specific class predicted. more formally, for ith rule which predicts the jth class, given the sets of fuzzy predicates pijf={pijkf}, with k =  <dig> .. kij , and let yjf be the consequent linguistic variable representing the predicted jth class, its formulation is defined as follows:

  rijf:pij1f∧…∧pijkf→yjf 

with each fuzzy predicate expressed as:

  pijkf≡ 

where vijkf represents a  numerical interval in the case xijkf is a fuzzy variable associated to a numerical feature or a  categorical value in the case xijkf is fuzzy variable associated to a categorical feature. finally, also the frb is weighted since each fuzzy rule is associated with the same degree of relevance pertaining the crisp rule it has been generated from.

different methods can be applied to fuzzify crisp rules  <cit> ,  <cit> ,  <cit> ,  <cit> , ranging from solutions which, on the one hand, exploit the symbolic structure of the crisp rules to generate fuzzy variables and terms to be inserted into the predicates of the corresponding fuzzy rules and, on the other hand, soften the sharp crisp thresholds to minimise continuous terms close to the decision boundaries from misclassification, to approaches where the crisp rule structure is only used to define fuzzy variables and terms and the sharp thresholds are not taken into account at all into the fuzzification process.

fis configuration
at this point, after generating the fuzzy model in terms of rules, linguistic variables and terms, the overall fuzzy inference system underpinning the ddss has to be generated in the fifth stage, named fis configuration, depending on the domain of application and its specific requirements. as depicted in figure  <dig>  a fis is a system aimed at solving a typically complex and nonlinear problem by utilizing fuzzy logic methodologies. its basic structure includes four main components, namely a fuzzifier , an inference engine , a defuzzifier , and a fuzzy model .

connective and structural parameters of the fis  have been thus defined in the previous stage. differently, in this stage, the most appropriate logical parameters to be used in the fis have to be determined by the designer based on experience and depending on the domain characteristics. typical choices for the reasoning mechanism are mamdani-type, takagi-sugeno-kang -type, and singleton-type. common fuzzy operators are min, max, product, probabilistic sum, and bounded sum. the most common membership functions are triangular, trapezoidal, gaussian and bell-shaped. for defuzzification several methods have been proposed with the center of area  and the mean of maxima  methods being the most popular. moreover, depending on the typology of reasoning mechanism desired, different inference engines can be used, for instance, for supporting the rule chaining  <cit>  or operating in accordance with an if-then-else rule structure  <cit> .

fis adaptation
after determining the most appropriate logical parameters, in order to complete the generation of the fis underpinning the ddss, the operational parameters have to be identified in the last stage, named fis adaptation, in terms of parameters characterizing shape and location of membership functions for all the terms related to all the linguistic variables involved in the frb.

for what concerns linguistic variables linked to categorical features, the determination of shape and location of their membership functions is very simple. since a categorical feature is one that has two or more categories, but there is no intrinsic ordering to them, they can be modeled as independent singular singletons. referring to linguistic variables linked to numeric features, a tuning process considering the whole frb obtained has to be used a posteriori to adjust the membership function parameters. the classic way to refine the membership functions is to adjust through slight modifications their parameters in order to find the local or global minimum of a mono/multi objective function f opportunely defined, which takes into account specific indexes modeling at least three characteristics  <cit>  a ddss should possess.

first, the performance of a ddss in performing a diagnosis can be evaluated with reference to the correct classification rate , even if the system should jointly provide also a numerical value  indicating its confident in the outcome produced. furthermore, a ddss should provide the physicians with the possibility of deeply understanding how this outcome has been generated , in order to increase its trustworthiness and not to appear as a black box that produces unintelligible outputs. it is worth noting that these characteristics can often result conflicting.

this tuning process can be implemented using parameter adjustment algorithms, such as neural networks algorithms, and in most cases, the gradient of a cost function with respect to each adjustable parameter can be calculated and the parameters can be updated accordingly. there are also some derivative-free optimization such as genetic algorithms and random search methods. as a concluding remark, it is worth noting that in this stage not only the operational parameters can be adapted, but also the weights of each rule in the frb, which have been previously extracted in the first stage and associated to the fuzzy rules in the fourth stage, can be refined. such a way, it is possible to induce a better cooperation among rules and to more accurately modulate the firing strength of a rule in the process of determining the output class.

implementing the methodology
the proposed methodology has been realized in the form of a modular and portable architecture according to a cbsd approach, with the aim of defining a collection of replaceable  components characterized by a functional cohesion  and a low degree of coupling in terms of composition and interaction between them.

the choice of a cbsd approach is based on the idea that each single component can be not only implemented from scratch but, in particular, also customized or extended by existing available solutions that are compatible to the original placeholders. such a way, the cbsd can significantly reduce development effort and time-to-market, and improve maintainability, reliability and overall quality of final architecture designed. the architecture is developed in java according to the object-oriented paradigm in order to create a truly portable ddss, solving the problem of having parts of it implemented for different platforms.

in the following section, the architecture designed will be diffusely described and a summary inspection of the main components in terms of uml class diagrams will be reported as well.

RESULTS
the proposed architecture
the cbsd approach has generated an extensible and layered architecture. an extensible architecture has been necessary because the proposed methodology is intended to support the realization of ddsss according to both general-purpose and special-purpose application needs. special-purpose requirements need to be incorporated depending on specific medical scenarios, whereas, general-purpose mechanisms will be common across all applications. moreover, it has been conceived as layered since, such a way, it can support design based on increasing levels of abstraction, thereby partitioning the overall design problem into several sub-problems. plus, it supports enhancement and reuse since, assuming that the interfaces between the layers do not change, it allows for changes to occur within the layer in relative isolation without impacting the other layers. this improves the scalability of the architecture as well as quality and testability. thus, such an architecture can lead to standard interfaces for each layer and its components, so that layer implementations can be re-used across different ddsss. the system architecture adopted is shown in figure  <dig> 

the architecture provides for three different layers: a process layer, which coordinates the activity flow foreseen by the methodology in terms of two loosely coupled sub-processes, a method layer which handles highly cohesive and well-defined operations  to be done in each sub-process, and a model&data layer which manages the data structures used to read and store crisp and fuzzy knowledge bases as well as data repositories. moreover, in accordance with the cbsd approach, the architecture highlights a transversal layer, shared between the method layer and the model&data layer, which reflects the idea that each single component at these two layers can be implemented not only from scratch but also by re-using or customizing existing available solutions, such as data mining software packages  or fuzzy logic libraries .

the definition of the components for the process layer has been guided by a top-down approach, where the methodology, which can be seen as a high level business process, has been divided into two smaller, more manageable and loosely coupled sub-processes. the need for a process modularization has been pushed by different motivations. first, the proposed methodology is large and complex, and it becomes very difficult to navigate, understand, debug issues and track changes in its implementation. moreover, referring to the stages of the methodology, the clear and well-defined separation between the crisp and fuzzy domains suggests a natural strategy of modularization from a logical and functional perspective. as a result, the methodology has been modularized in two sub-processes, one for each domain, so as to contextually balance the need of performance and manageability versus the need of reuse as well. the first sub-process, handled by the crisp rule generator, is devised to generate a set of fuzzifiable crisp rules starting from an input dataset, whereas the second one, managed by the fis generator, is thought to produce a whole fis starting from the output of the first sub-process. both these components assume the role of coordinators with respect to the activity flow foreseen by each single sub-process. they coordinate all the components of the method layer by activating their functionalities, examining and validating their results, and continuing the handling of the respective sub-process accordingly.

the method layer contains the building block components for implementing each single task foreseen in both the crisp and fuzzy sub-processes. these components are implemented on top of the model&data layer in terms of a collection of modules which accesses the respective knowledge bases and data repositories, elaborates such an information and stores the results into the knowledge bases again. moreover, all these components can be opportunely specialized in order to support different criteria, also by wrapping or utilizing existing tools.

more in detail, the crisp rule extractor  is the component in charge of first extracting a collection of crisp rules from a specific data repository and then storing it into the crisp knowledge base. the extraction procedure can involve the strict collaboration with a crisp inference engine, which is a component responsible of evaluating the firing of a rule with respect to a specific input data item.

the crisp rule selector  is responsible of getting the extracted rules from the crisp knowledge base, determining the best n rules  to be successively used with respect to a specific criterion and finally storing such a new rule set into the crisp knowledge base again. since the criterion applied for the selection might require the identification of the set of rules with the highest confidence, such a component can interact with the crisp inference engine to evaluate how each rule works on the dataset gathered from the data repository.

last, the crisp reductor  is devised to get the selected rules from the crisp knowledge base, simplify their structure in order to grant the fuzzifiability and finally store the reduced rules into the crisp knowledge base again.

the components of the method layer described until this point cooperate among them to model the first sub-process, and, thus, they are all coordinated by the crisp rule generator. the remaining components belonging to the method layer act together for bringing in realization the second sub-process, and, thus, they are all coordinated by the fis generator.

more precisely, the fuzzifier is designed to get the reduced crisp rules as well as the description of each single feature involved in them  from the crisp knowledge base, fuzzify them into fuzzy rules, linguistic variables and terms and store the results into the fuzzy knowledge base, so as to actually realize the fourth stage of the methodology . in detail, such a component makes in practice the specific fuzzification criterion described at the end of the methods section.

referring to the fifth stage of the methodology, the component involved is the fuzzy configurer , which is responsible of setting the most appropriate logical parameters to be used for the construction of the final fis, opportunely determined by the designer based on experience and depending on the domain characteristics. moreover, depending on the typology of reasoning mechanism desired, the specific fuzzy inference engine, aimed at performing a fuzzy inference to obtain a fuzzy output, is also configured and instantiated.

the component engaged for the realization of the last stage of the methodology is the fuzzy adapter , which is mainly devised to first get the partially defined fis from the fuzzy knowledge base, successively tune its logical parameters, in terms of parameters characterizing shape and location of membership functions, and rule weights, and finally store the completely defined fis into the fuzzy knowledge base again. such a component strictly interacts with the fuzzy inference engine since every optimization strategy requires the evaluation about how each fuzzy rule works on the dataset gathered from the data repository.

finally, the model&data layer is responsible for the management of the structures for inserting and gathering information, respectively into and from both the crisp and fuzzy knowledge bases and, in addition, for accessing the disk-based data structures used by the data repositories. in particular, with respect to the data format in the repositories, comma-separated values  are used to store tabular data  in a plain-text form. in particular, the first row contains the attribute names  followed by each data row with attribute values listed in the same order . this choice is due to the fact that many data repositories or spreadsheet applications save or export data into flat files in this format.

the design of this architecture in terms of software classes has been depicted as uml class diagrams, where each class has been summarily outlined below in terms of only the most significant public operations defined, with respect to the three different modules. in order to better facilitate the reading of the diagrams, note that a solid line models a structural association between two classes, whereas a broken line indicates a functional dependency between them. the first diagram includes the main classes devised for implementing the architectural components, across the three layers, which operate in the crisp domain and are responsible of realizing the first three stage of the methodology . the second one includes the main classes realizing the components which operate in the fuzzy domain and are in charge of implementing the last three stage of the methodology . the last one includes the main classes realized for the management of the data repositories . these classes also offer facilities for automatically partitioning the dataset into learning and testing sets, in order to support the n-fold cross validation method.

proof of concept: a ddss for diagnosing breast masses
the architecture has been used to instantiate a ddss example aimed at accurately diagnosing breast masses starting from the widely used wisconsin breast cancer dataset  to evaluate the feasibility of the methodology.

this dataset was computed from fine needle aspiration  of a breast mass through image processing and was collected at the university of wisconsin; it can be obtained from uci  machine learning repository. the samples contain features describing characteristics of the cell nuclei present in the image. the version of wbcd used consists of  <dig> features obtained from fna, namely radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, fractal dimension. each feature is represented with  <dig> values, namely the mean, standard error and the worst or largest , but only the mean value was taken into account. the two outputs are benign and malignant. all the instances were properly recorded without any missing value. the diagnosis class is distributed with  <dig> benign samples and  <dig> malignant samples.

the architecture was instantiated for creating the ddss as described in the following. preliminary, it is worth noting that the tenfold cross validation method was used for the assessment of such a ddss and the classification rate was chosen as metric to evaluate the goodness of the final results achieved. such a way, the whole methodology was tested for its validation, since the goodness of the final results was considered as proof of feasible and efficient integration of different methods according to its activity flow in order to obtain fuzzy-based ddsss. as a result, the ddss is described below with respect to each stage of the methodology, by reporting the partial results calculated only for a fold for the sake of brevity.

crisp rules were extracted as equally weighted from a j <dig> decision tree, induced by wbcd. such a method was wrapped on the top of its weka implementation. figure  <dig> outlines the resulting decision tree and the corresponding set of rules, grouped according to the diagnosis class, i.e. malignant and benign, respectively and ordered according to their coverage, shown in brackets in figure  <dig>  indicating the number of correctly classified instances.

successively, a simplification method based on the most confident selection with respect to the rule coverage was adopted, where the number of rules to be generated was fixed to  <dig>  i.e. one for each output class. the selected rules , were opportunely reduced by involving, in particular, only the rule  <dig> .

with respect to the fuzzification stage, a method here proposed, based on a determinant of interpretability, was applied. in more detail, in the case of numeric features, a collection of at most three partitions is generated for the universe of discourse of each linguistic variable, by considering one crisp rule at a time and translating its predicates involving numeric features from their crisp forms to the corresponding fuzzy representations:

  pic≡→pif≡ 

  piic≡→piif≡ 

  piiic≡→piiif≡ 

where tif,tiif and tiiif correspond to linguistic terms  linearly ordered and with boundary regions between successive terms. it is relevant to highlight that the semantics associated to each linguistic term obtained in such a way is strictly application-dependent and is meaningful only in the context of the rule for which it has been defined. as a result, this method defines as many terms for each linguistic variable as the different crisp predicates which use the corresponding feature when examining rule by rule. moreover, in such a way, the crisp boundaries expressed in each predicate for the numeric features involved are discharged. thus, on the one hand, the same fuzzy predicate can be generated from two different instances of a specific form of crisp predicate appearing in two different rules. on the other hand, by working at rule level, even if, in more than one rule, many crisp predicates share a same numeric feature with different crisp boundaries, its universe of discourse is not further partitioned in order to consider the possible sub-partitions identified by the overlapping of different crisp intervals. so, any relationship that could exist between the rules in terms of shared features is not taken into account. both these methodological choices, which can appear as a simplification and a loss of generality, are guided by the consideration that defining a justifiable, i.e. limited, number of terms for each variable is another determinant of interpretability  <cit> . indeed, taking into account all the possible overlapping of different crisp predicates in terms of numeric intervals, by observing the whole crb, in order to deeply partitioning a shared feature, would generate an overfitting of its universe of discourse, compromising the overall interpretability.

furthermore, in the case of categorical features, the method generates a collection of as many terms for each linguistic variable as the crisp predicates occurring in the whole rule set which assign a different categorical value to the feature used.

for what concerns the predicted class, since it is also a categorical variable, the number of terms associated to the corresponding linguistic variable is assumed to be equal to the number of different values the predicted class can assume.

as a result, the fuzzification of the reduced rules generated a linguistic variable for every feature appearing in their antecedent parts, namely perimeter, texture, concave points and radius, each of them characterized by at most two partitions for the corresponding universe of discourse. moreover, the output class, i.e. diagnosis, was also modeled as a linguistic variable assuming two specific values, namely malignant and benign. the resulting fuzzified rules are outlined in figure  <dig>  bottom), where linguistic variables and their terms are indicated with their first letter capitalized.

concerning the generation of the other parameters pertaining the final fis underpinning the ddss, the terms low and high generated after the fuzzification for the input variables were modeled with piece-wise linear membership functions, whereas the terms malignant and benign for the output variable were represented as singletons. the singleton-type reasoning mechanism was used, where min and max operators were chosen as t-norm and s-norm functions, respectively. moreover, the min operator and the center of gravity singleton methods were applied for implementing the implication and defuzzification functions, respectively. a not-chained fuzzy inference engine was used by wrapping the implementation given within the jfuzzylogic tool.

finally, adaptation based on the deltajump algorithm was carried out for optimizing only the membership functions linked to the terms of the linguistic variables involved into the fuzzy rules obtained. such an algorithm was wrapped on the top of the implementation provided by the jfuzzylogic tool. only a metric based on straight mean square error is used for the evaluation with respect to the classification rate, while interpretability  <cit>  and confidence  <cit>  are not considered yet. the fuzzy partitions achieved for each linguistic variable are outlined in figure  <dig> 

these results were finally validated with respect to the classification rate measured for the ddss instantiated on the first fold for the wbcd dataset. in more detail, they were calculated on both the training and learning sets, depending on the rules achieved in the different stages of the methodology, i.e. ranging from the crisp rules obtained at the end of rule extraction, to their selected and reduced version, until the fuzzified rules before and after their adaptation. the validation results are outlined in figure  <dig> 

figures  <dig> and  <dig> sketch the gui implemented for facilitating the construction of a ddss on the top of the proposed architecture: the user is asked to specify the dataset to be used, the algorithms or techniques to be adopted in all the stages of the methodology with all the parameters required for their configurations, and, finally, the validation method for evaluating the results with respect to a specific metric or index indicated .

CONCLUSIONS
having in mind to extend the range of possible users of fuzzy-based ddsss with extensive and easy-to-use facilities which could considerably reduce the level of knowledge and experience required to their design and realization, this work has first presented a formalization of a refined and assessed version of a six-step methodology to design and implement fuzzy-based ddsss. its strength relies on its generality and modularity since it supports the integration of alternative techniques in each of its stages. stages are employed for:  the extraction of crisp rules,  the selection of a significant partition from the whole rule set extracted,  the reduction of the selected rule set,  the creation of fuzzy rules,  the generation of the whole fuzzy inference system and  its optimization.

differently from other existing approaches  <cit> ,  <cit> , the described methodology is extremely flexible and does not depend on the typology of fuzzy model to be defined, since it enables the design and realization of fuzzy-based ddsss by taking into account many different and often conflicting requirements, such as the accuracy maximization or the complexity minimization. in more detail, it can not only integrate state-of-the-art rule-induction and rule-optimization methods, but also freely choose the structural and operational elements of the fuzzy model to be used, such as shape of membership functions or the t-norm and s-norm connectors as well as the implication and defuzzification operators. moreover, methods that use shared fuzzy sets for the rule base  <cit>  are appropriate within a small size work space with a good coverage. otherwise, in case of a weak coverage the rule base completeness is not guaranteed and, when dealing with large systems, the number of combinations to manage is huge  <cit> . on the contrary, the proposed methodology is well adapted for large work spaces and generates more compact incomplete rules with only the most locally significant variables, defined successively with a partitioning strictly dependent on the rules where they are involved.

the presented methodology has been realized, according to a cbsd approach, in the form of a modular and portable architecture that has been carefully described from a software engineering perspective.

this architecture has been conceived to support the design of a fuzzy-based ddss on increasing levels of abstraction, thereby partitioning the overall design problem into several sub-problems, where each single component at every layer can be implemented from scratch or customized by existing available solutions. such a way, it can significantly reduce development effort and time-to-market, and improve maintainability, reliability and overall quality of final cdss designed.

the development of this architecture has been last carried out by using the java language since it contains several features that argue for it. it is widely distributed and has become one of the major programming languages. the development kit, including compiler and debugger, is freely available on a number of different computer platforms. the core libraries contain many functions which can be used directly and need to not be adopted from external libraries, which is not the case in c++ for instance. by exploiting java features and diffusion in the user-community, the proposed architecture has several unique advantages, e.g. it reduces programming work. thanks to the huge amount of available java software, in fact, it is really easy creating new methods to be added to the method layer of the architecture without the effort of starting from scratch. such a way, it is possible to exploit the richness of quickly incorporating new developments made by the active research community which is always working in emerging fields. moreover, due to the use of a strict object-oriented approach for the its components, the architecture can be used on any machine with java. indeed, the concept of modularity of code is highly essential to increase the level of portability. as a result, any user can apply the architecture to implement a ddss on his machine, independently of the operating system.

as a proof of concept, such an architecture has been used to instantiate a ddss example aimed at accurately diagnosing breast masses starting from the widely used wisconsin breast cancer dataset. the results obtained in terms of classification rate proved the feasibility of the whole methodology implemented in terms of the architecture proposed.

for what concerns the on-going work, knowledge representation techniques such as ontology modeling are investigated to be exploited in order to better define from a semantic point of view the fuzzy variables and terms involved into the rules and improve the readability and understandability of the whole fuzzy-based ddss. the choice of using java as programming language will be able to facilitate this integration since the most representative tools in the context of knowledge engineering are implemented in java and released as open source projects. moreover, since ddsss, however, typically have unequal classification error costs so that straight cr cannot be assumed as a careful measure of the goodness of a ddss, in the future, also the confidence χ will be evaluated to be used for selecting a ddss; in fact, a good ddss should be highly confident with correctly classified examples while it should be doubtful with misclassified data points. in such a direction, also more sophisticated adaptation techniques able to optimize multi-objective cost functions will be integrated, so taking into account simultaneously cr, the confidence χ and the interpretability. the last important point for future work is to integrate the multi-threading and distributed computing to speed computations up during the definition and the adaptation of the fuzzy-based ddss by using widely available multi-processors and multi-core hardware.

competing interests
the authors declare that they have no competing interests.

authors' contributions
all the authors contributed to the formalization of the methodology, the design of the architecture and the realization and testing of its implementation. all the authors cooperated to the writing of the manuscript, read and approved its final version.

declarations
the publication costs for this article were funded by the merit project.

this article has been published as part of bmc bioinformatics volume  <dig> supplement  <dig>  2013: computational intelligence in bioinformatics and biostatistics: new trends from the cibb conference series. the full contents of the supplement are available online at http://www.biomedcentral.com/bmcbioinformatics/supplements/14/s <dig> 

