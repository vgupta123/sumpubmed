BACKGROUND
next-generation sequencing technologies have revolutionized genomics, leading to a tremendous increase in the amount of available sequence data, while bringing down the cost per base. however, de novo assembling of mammalian genomes using these short reads has met with limited success  <cit> , despite recent strides in assembling smaller microbial genomes  <cit> . the market for these short-read technologies has largely been driven by resequencing efforts, where reads are mapped to a genome sequence that was typically assembled using some other sequencing technology such as sanger sequencing. the deduced genomic differences are then studied to characterize and understand the genetic diversity among individuals of the species. the last few years have seen tremendous progress in the development of algorithms and software to map short reads onto a reference sequence and to identify differences between the individual and the reference  <cit> .

much less has been published about identifying genomic differences in species where an assembled reference sequence is lacking. we describe our computational pipeline, called dial, for deducing genetic differences within a target species without the help of a reference genome. the method works even when the depth of coverage is insufficient for de novo assembly. it is also designed to work with sequences from multiple individuals, a situation that can pose difficulties for de novo assembly. we evaluate the performance of dial on several extensive datasets.

the directions taken in this study were shaped by our immediate needs, which were to identify a pre-determined number of single-base differences to design a custom genotyping array using only as much sequence data as necessary. with current array technology, at most a few thousand differences can be assayed at an affordable cost , so our main interest here lies in knowing how to identify that many differences with a low false-positive rate, and in understanding the main sources of error at low levels of sequence coverage.

implementation
dial is implemented as a pipeline where the output from one stage serves as the input to the next stage. snps  are called every time a new set of sequences is added. a set can be the yield from a lane of illumina sequence, a quadrant of  <dig> sequence, or any subset as decided by the user. once the number of called snps is adequate for the desired application, we can stop sequencing. the general flow of the pipeline can be divided into several steps, as summarized in figure  <dig> and described in greater detail below.

masking of repetitive sequences
duplicate reads can arise from pcr duplicates, sequencing artifacts such as poly-a and poly-n reads, noise in cluster detection, and from genomic dna shearing at the same location in different molecules. these clones can lead to incorrect coverage assessments and erroneous snps. we collapse all such clones of a read into a single sequence by using a simple hash function. we identify reads with the same md <dig> checksums  <cit>  and if they have identical bases, then we replace them with a single sequence. interestingly, we found that the watson roche/ <dig> sequence dataset had an average of  <dig> % duplicate reads in a single set, whereas the observed number for the illumina reads was higher at about  <dig> %.

we then feed the non-duplicate reads to another module, which is based on windowmasker  <cit> .  windowmasker uses linear-time scans of a genome to identify repetitive sequences, and has been proven effective in cases where much of the sequence is in draft form. we modified windowmasker's algorithm slightly so it could be used in conjunction with reads from sequencing runs. we soft-mask probable repeats  by using lower-case letters for nucleotides, so that alignment anchors are not chosen in those regions. we achieve this in three sequential passes, as described below.

 <dig>  we use the first pass to calculate appropriate parameters and cutoff values to be used by the module. we define k, the size of the k-mers to be used, as the largest integer such that  >  <dig>  where n is the total sequence yield from the current input set. we calculate and store the frequency of each k-mer, freq, in a splayhash for efficient access, then sort the k-mer frequencies and store the results in a temporary array f. if c is the total number of unique k-mers from the sequences in the set, then cutoffs thigh, tthreshold, textend and tlow are calculated as f  +  <dig>  f +  <dig>  f +  <dig> and f +  <dig>  respectively. we compute a score for each k-mer to mitigate the effects of a few k-mers with very high frequencies and to avoid keeping track of counts for many infrequently occurring k-mers. the score is calculated as:  

 <dig>  we use the second linear scan to divide each sequence into a set of overlapping windows. each window contains  <dig> k-mers, and we calculate the score of each window by averaging the scores of the k-mers in it. adjacent windows share  <dig> k-mers, and we mask the contents of a window if it has a score greater than tthreshold. we also mask the interval between two masked windows, if the score of each of the windows in it exceeds textend.

 <dig>  the last pass of the masker looks for local repeats. we calculate the frequency distribution of the k-mers within a read, and if any of the frequencies exceed textend, then we mask the whole read. this is particularly effective in reducing the time required to calculate overlaps between read sequences. this pass also unmasks any masked segment less than  <dig> bp long; the motive here is to unmask the high-frequency k-mers in non-repeat sequences.

calculation of overlaps
we align the masked sequences from the current input set against all of the masked reads from the sets processed earlier, using a "seed and extend" strategy so that most of the spurious comparisons are discarded at the outset. we use the lastz aligner  <cit> , which is a drop-in replacement for blastz  <cit> . the appropriate scoring matrix for lastz varies depending on the error model of the sequencing technology used. for sequences generated with the  <dig> technology  <cit> , we set the match score to  <dig> and the mismatch score to  <dig>  along with gap open and gap extend penalties of  <dig> and  <dig>  respectively. we require a perfect match of at least  <dig> bp between two reads for them to be considered for gapped extension. we find the endpoints of the alignment by terminating the gapped extension whenever its score falls  <dig> points below the maximum score observed so far for that alignment path. two reads are called "aligned" if they have an overlapping segment of at least  <dig> bp with an identity of 96% or more.

we modified this score set for use with the shorter  <dig> bp illumina reads; the gap open and gap extend penalties are both set to  <dig>  while the match score is  <dig> and the mismatch score is  <dig>  we find the endpoints of the alignment when the score falls  <dig> points below the maximum score observed on that path. two reads are called as "aligned" if they have an overlapping segment of  <dig> bp with an identity of 96% or greater.

for every read in the input, we store its header and sequence along with the names and observed differences of all the other reads that align to it, in a data structure we term as a "cluster". these clusters are cumulative over the sequencing runs. each new sequence set is initialized with its own set of clusters, in one-to-one correspondence with the reads. the set of masked sequences is then aligned to all of the masked reads from the previous sets, growing the clusters corresponding to each of two reads if the two reads align. we mark the clusters as "candidate repeats" if they exceed thresholds for the number of members  or the number of observed differences. this is done to prevent a combinatorial explosion that would occur if we stored all the details of all the clusters. we stop adding reads to the cluster once it is marked as a candidate repeat, even though its original corresponding read can still be added to other clusters if it aligns with their sequences. the only exception to this filter is in the case of transcriptome datasets, as transcripts have variable abundance depending on their expression in the sample.

one of the inputs to dial is the expected length of the target genome. this information is used to calculate the threshold for identifying a pileup of reads. we normally set it to the expected size of the closest available genome sequence if the expected size of the target genome is not known. if the average depth-coverage is c, the coverage k at each base of the genome approximately follows the poisson distribution  

dial calculates a value cthreshold that equals the  <dig> th percentile of the cumulative distribution function for the coverage depth. when a sequence set is added and the number of members in a cluster exceeds cthreshold, then the cluster is marked as a candidate repeat. a cluster is also marked as a candidate repeat if the number of differences in it exceeds a limit dthreshold. for the reported datasets, this threshold was set to  <dig> 

determination of snps
we can call snps after the overlaps from two input sets are calculated. the determination takes place in several steps:

 <dig>  first, we filter the clusters to consider only those which have substitution differences supported by at least two reads for each allele . such clusters with more than  <dig> substitution differences, or with two or more such differences that are within  <dig> bp of each other, are ignored. if the input to the pipeline consisted of sequences from more than one individual, then we also filter to keep only clusters that have at least two reads from each of at least two individuals. these filters are restrictive and increase the false negative rate of snp discovery. however we find that they also help in reducing the false positive rate, especially at low coverage. this step also ignores clusters which were marked as "candidate repeats"; they are not considered in the snp calling stage.

 <dig>  we then iterate through the filtered clusters and select those containing only member reads that have not been observed in any of the clusters selected before it. this ensures that a read appears in at most one snp call.

 <dig>  we also ignore clusters whose sequences align to any of the completely masked sequences in the earlier steps. this includes reads that have local repeats as determined earlier and reads that had less than  <dig> unmasked bp. for this we use the same lastz parameters and score sets as we did for calculating the overlaps between the reads. this step ameliorates any local frequency bias in a particular input set. we also remove reads whose clusters have been marked as candidate repeats from the member lists of all other clusters.

 <dig>  next we create "micro-assemblies", i.e., de novo assemblies for each selected cluster. for the  <dig> technology we extract the reads corresponding to the members of a cluster from the original sff file to create an input file for newbler, which is part of the software package distributed by roche/ <dig> with their sequencing machines. we use velvet  <cit>  for assembling the illumina short reads. a different assembler can be specified, as long as it works with the particular sequencing technology and generates output in the ace file format  <cit> .

 <dig>  we then read the ace file output and deduce positions in the assembly where two different alleles are observed with support from at least two reads for each allele.

 <dig>  a final, optional step filters the candidate locations to find snps which are suitable for genotyping. the filters involve a combination of factors, including the number of variants and reads found in the assembled contig, the distance of the candidate snp from the ends of the contig and the quality scores for the reads. another optional filter disallows snps in homopolymer regions. in the reported datasets  the following requirements were imposed:

 only one snp is found in a micro-assembled sequence.

 the snp has at least  <dig> bp on both sides in the assembled sequence for the  <dig> datasets, and at least  <dig> bp on both sides for the illumina reads.

 the minimum quality score for the reads at the site is  <dig>  reads supporting an allele with a lower quality score are not counted.

 the snp is not part of a homopolymer of length greater than  <dig> bp in a window of length  <dig> bp centered on the snp.

RESULTS
an overview of the computational problem
our strategy is to observe the fragments of dna sequence data produced by a sequencing machine and identify pairs of fragments that overlap, but differ in one or several of the overlapped positions. when the total number of identified variant positions reaches a desired level , sequencing can stop. if the fragments are all from the genome of one individual, then we are looking for heterozygous positions , whereas with sequence fragments from more than one individual, we can also find positions where every individual is homozygous , but at least two individuals differ. the differences we seek are typically single-nucleotide substitutions that occur within a species or population of interest. we call each position at which different nucleotides appear among the individuals being sequenced a "snp", short for single nucleotide polymorphism. each variant nucleotide that appears at a snp we call an "allele"; for this project we make no requirements concerning the frequency of an allele in the population, other than that it occurs in our samples, and we assume there are precisely two alleles at each snp. our approach can also detect insertions/deletions of one or several nucleotides, but these are of less interest to us because high-throughput genotyping methods  focus on nucleotide substitutions. here we describe two difficulties with our strategy, explain how we address them, and analyze their effects on our results.

the first difficulty is dealing with sequencing errors, which can masquerade as snps. the actual frequency of snps depends on the genetic diversity of the species being sequenced. for instance, the frequency of snps between two humans averages around one per  <dig>  positions, but in some species snps are much less frequent. in any case, the rate of errors in individual fragments produced by the sequencing instrument  is probably much higher than the rate of snps. this means that when we see a nucleotide difference between two overlapping reads, the odds are that it is a sequencing error rather than a true snp. a cornerstone of our approach is to require that each allele of a putative snp be observed at least twice, which we believe is significantly more likely to indicate an actual difference than a repeated sequencing error.  this requires the genomic position to appear in at least two fragments with one allele and two fragments with the other, and raises the following question: given λ-fold genome coverage in random fragments of an individual genome , what fraction of the genome positions will be contained in two or more fragments from each parent? or similarly when looking for homozygous differences between two individuals that are each sequenced to depth λ/2? in theory, the answer is z <dig>  where z =  <dig> - e-λ/ <dig>   for instance, given 1-fold coverage of an individual, the expected genome fraction where heterozygous positions can be identified is  <dig> . thus, if a genome has  <dig> million heterozygous positions, we expect around  <dig> of them to be detectable with 1× sequence coverage if no other issues are considered. under the same conditions, 2× coverage should put  <dig>  snps within reach of our method. for a genome with fewer heterozygous positions, our potential yield is proportionately less, e.g.,  <dig> snps from 1× coverage of a genome with  <dig>  heterozygous positions. typically, we will not know the snp density in the sequenced individuals when we start a project; we are more likely to know the number of snps that can be accommodated on a genotyping array that is within our budget.

the second difficulty is that overlapping reads differing at one position may not be from the same part of the genome; instead they may be from the two copies of a recent duplication that have accumulated a mutational difference. in that case, a detected difference is of no use for a population-genetic analysis because every individual genome of that species may contain both variants and thus falsely appear to be heterozygous at a single location in a genotyping assay. hence we consider a snp prediction to be in error if it turns out to be caused by our failure to distinguish copies of a repetitive region. our main defense against this possibility is to limit the number of sequenced fragments that contain a given snp. if a genomic interval has hundreds of nearly identical copies throughout the genome , then even with only  <dig> × coverage we will see a pile-up of overlapping reads, which can be easily rejected. the problem comes from regions that have only a few nearly identical copies in the genome. to give a sense of the frequency of these low-copy repeats in one particular species, consider ncbi build  <dig> of the human reference genome. there,  <dig> % of the bases are in a repeated region  with precisely  <dig> copies; for  <dig> or  <dig> copies the fractions are  <dig> % and  <dig> %, respectively. it is not obvious how to extrapolate that estimation to the parts of the human genome not represented in the reference assembly , or to another species. however, success of our method clearly requires that the fraction of the genome contained in such high-identity, low-copy-number repeated regions be very small, particularly since the fraction of nucleotides in a duplicated region that differ among the copies is likely to exceed the fraction of positions in single-copy regions that are snps.

these two difficulties lead to competing constraints on our strategy; we need to thread our way between not having enough overlapping reads and having too many. moreover, this threading must be done in the dark, because we typically will not know all of the critical parameters, including the number of actual snps among the individuals being sequenced and the frequency of recent genomic repeats in the target species. however, we can obtain guidance by using theoretical models that we fit to observed data. for instance, one decision that needs to be made is the upper limit, call it x, on the number of reads that appear to contain a snp. for identifying heterozygous snps in a single genome, the fraction that can in theory be identified when requiring two observations of each allele and at most x overlapping reads, given λ-fold coverage of the genome, is:  

for intervals that appear precisely twice in the genome , the fraction of differences that will be mis-identified as snps is approximately p .  if we assume there are  <dig>  true snps in single-copy regions,  <dig>  differences in duplicated regions, and  <dig>  copies of a duplicated region on average, then table  <dig> shows the numbers of correct positions and the relative number of copy-induced false positives; these numbers roughly reflect data observed for humans.

the values are given as a function of fold coverage  and the upper bound on the number of overlapping reads . for instance, at λ =  <dig>  and x =  <dig>  there are  <dig>  correct snp calls and  <dig> % as many duplication-induced erroneous ones. this is a theoretical analysis based on informally fitting a model  to data from the genome of dr. james watson.

this model shows the following general trends. the main trend in the percentages is a difference among the rows. namely, at  <dig> × coverage, the number of incorrect calls caused by low-copy repeats is roughly 60% of the number of correct calls, but as the coverage increases, that percentage drops significantly, reaching around 10% at  <dig> × coverage. a less dramatic but still noteworthy trend is seen within rows: at  <dig> × coverage, allowing more than x =  <dig> overlapping reads makes little difference, while at  <dig> × coverage, setting x =  <dig> produces substantially more correct snp calls, but with a substantially higher false-positive rate.  the specific values in the table depend heavily on the assumed numbers of snps and of differences within low-copy repeats, but similar trends are observed over a wide range of conditions and provide insight into the problem at hand.

our goal is to perform only as much sequencing as is needed to produce a desired number of snp predictions with an acceptable false-positive rate. in practice, we impose additional restrictions on our snp predictions, as described above in the implementation section, but the two main conditions are that we see each putative allele of a snp at least twice, and that there not be too many reads that appear to overlap the snp. the additional conditions, such as high quality scores and lack of other snp predictions in the assembled region around the snp, are intended to decrease the fraction of erroneous snp calls and to provide predictions that are appropriate as input for certain experimental protocols . however, the extra restrictions unavoidably reduce the number of correct snp calls.

snp detection probabilities
assume that each position in the genome has equal probability to be covered by a read, and that the reads are independently generated. with small read lengths and sparse snp distributions over the genome, we ignore the correlation of read coverage for adjacent positions. as a result, the distribution of the number of reads covering each position in the genome can be well approximated by a poisson distribution, whose mean is the average read coverage per base pair. denote the genome size by l, the read length by l , and the total number of reads by n; the read coverage per base pair can then be calculated as λ = . the probability of a position in the genome being sequenced at least x times is therefore . as a special case, we denote the probability of a position being sequenced at least twice as f =  <dig> - e-λ.

our first rule for calling a snp is that each allele must be observed at least twice. assume that we sequenced one or more individuals from the same species or population, with equal read coverage for each individual and the total coverage λ. for simplicity, we further assume that each putative snp is biallelic, and we denote the proportion of each allele over all individuals by p and q, where p + q =  <dig>  for example, if only one individual is sequenced, a snp must be heterozygous with p = q =  <dig> . we can calculate snp detection probabilities according to the first rule as f f . as a special case, if we sequence two individuals, we have the following results:

 <dig>  if both individuals are heterozygotes at the snp, or they are homozygotes but with different alleles, the probability of both alleles being sequenced at least twice is .

 <dig>  if one individual is a heterozygote and the other is a homozygote at the snp, the probability of both alleles being sequenced at least twice is .

snp calling in the presence of segmental duplications
let lt denote the total length of all distinct segments that have precisely t copies in the genome, e.g., l <dig> denotes the total length of segments unique in the genome, and l <dig> denotes the total length of  all segments that appear twice. the genome size is then given by , and the average read coverage per distinct base pair in the genome is . here we treat base pairs occurring at the same position in multiple duplicates as a collapsed single base pair, i.e., a column in the multiple alignment of duplicated regions. as a result, the expected read coverage at each collapsed base pair is λt. the larger t is, the higher the chance that there has been a nucleotide substitution in one copy, which could be mistaken by dial for a putative "snp". the term "snp" is quoted because it may correspond to mutations occurring in different copies of a duplicated region. our goal is to call snps within unique regions in the genome and discard all "snps" in duplicated regions . without a reference genome, however, we do not know whether a segment is duplicated or not. we therefore select an upper bound for the observed read coverage per base pair, beyond which the corresponding "snp" will be treated as a duplication difference and discarded. the choice of the upper bound for read coverage can be used to strike a balance between sensitivity and specificity. suppose that we only call a snp if  both alleles are observed at least twice; and  the number of reads covering the "snp" is at most x . within unique regions , the calling probability of each snp can be calculated as:  

the calling probability of a "snp" in duplicated regions can be similarly calculated with λ replaced by λt, i.e., pr = p .

to assign the values of allele proportions p and q in unique regions, we may let p = q =  <dig> , which is exact if only one individual is sequenced. for regions with t copies, the frequency of duplication differences is typically much higher than that of snps, and thus when t =  <dig>  we will have p = q =  <dig>  most of the time. when t >  <dig>  without additional information, we may assume a star tree topology for the evolution of duplicated regions, and thus we can let p =  <dig> -  and q = .

given the above equation, we can define and calculate the sensitivity of our snp calling  as:  

assume that the average density of "snps" in a t-copy region is pt, and thus the expected number called in all t-copy regions is n = ptltp. the specificity of our snp calling can therefore be defined and calculated as:  

among all of the calls, the proportion of correct snps  is approximately:  

watson genome dataset
the genome of dr. james watson was the first full genome to be sequenced using a next-generation sequencing technology  <cit> . several groups subsequently computed single-nucleotide differences from the human reference sequence, including the inferred heterozygous positions that we used to evaluate the accuracy of our snp-calling method. fortunately, we have an extremely accurate assembly of a human reference genome sequence, making it possible to reliably determine which of the positions lie in near-identical, low-copy-number repeats, which are the bane of our strategy for calling snps without a reference .

we proceeded as follows. of the  <dig>  million high-quality reads that were generated for the watson project,  <dig>  million are available in the short read archive at ncbi. the available reads total  <dig> , <dig>  bases and provide approximately  <dig> -fold coverage of the genome. we downloaded the corresponding sff files . dial processed this data in about  <dig> cpu hours on a workstation. starting at  <dig> × coverage, and then at regular increments, dial predicted heterozygous positions. along with each snp prediction, dial reports an interval of assembled sequence that contains the putative variant position. our subsequent analysis focused on high-confidence snp calls where no other snps were predicted nearby; this was done because our main intended application for dial is to make predictions used for subsequent experimental studies, which frequently require variant-free sequences on both sides, e.g., for pcr primers. however, it also helps to eliminate low-copy-number duplicated regions that split more than a few million years ago, because older duplicates will probably have accumulated multiple nucleotide substitutions. 

figures  <dig> and  <dig> show the number of heterozygous positions that dial predicted for the watson genome at various levels of sequence coverage. for instance, at 1× coverage dial identified  <dig>  potential snps, while at 2× and  <dig> × it identified  <dig>  and  <dig> , respectively.

we judged that a heterozygous snp predicted by our method is correct if  the accompanying assembly of flanking sequence aligned to precisely one position in the human reference genome  at appropriately high thresholds  and  it agreed with any of the watson heterozygosity calls that we downloaded from three sources . figures  <dig> and  <dig> plot the total number of snp calls, the number of snp regions that appeared to be unique , and the number of these that were verified against the heterozygous positions found by other groups using a reference-mapping approach . figure  <dig> indicates that for high genome coverage, the rate of erroneous snp predictions is around 10%, over half of which appears to be over-prediction in single-copy genomic regions . however, of the  <dig>  snp predictions that dial made in single-copy genomic regions at  <dig> × coverage but which were not in the other sets of watson snp calls, 43% were found in other databases of snps, which suggests to us that many and perhaps most of our putative false positives are incomplete heterozygosity calls in the other watson snp collections. figure  <dig> shows the various sources of error in the false-discovery rate. at  <dig> × coverage of the watson genome, over 50% of dial's heterozygosity calls were judged incorrect; the main source is low-copy-number genomic repeats, as discussed above. the coverage and difference filters applied to mark clusters of reads as "candidate repeats" work much better as the coverage increases. a small fraction of the assembled neighborhoods of putative snps fail to align to the human reference genome at our requirement of at least 98% identity, though they do align under relaxed conditions.

we also considered the number of false negatives, i.e., heterozygous positions in the watson genome that seem to meet the conditions we impose but are not identified by dial. we estimate that there are about  <dig> million heterozygous positions that do not lie in a repetitive region or near another heterozygous position, but dial finds only  <dig>  from  <dig> × of sequence data. the reasons for this discrepancy include the difficulty of predicting heterozygous positions with only low-coverage data and the variety of additional conditions  that we require before calling a snp.

orangutan dataset
illumina sequencing technology generates short reads only  <dig> bp long, but the amount of data generated in a single run far exceeds that from the roche/ <dig> technology. we used a single run of  <dig> bp reads from a bornean orangutan pongo pygmaeus which contained  <dig> , <dig> single reads to call heterozygous sites and evaluate our approach on illumina reads. we adjusted the alignment parameters in the pipeline to make them more amenable to the short reads and were able to identify  <dig>  high-confidence snps for this dataset. a consequence of the short read lengths is that the assembled contigs for the orangutan reads were about  <dig> bp, whereas the flanking regions for the watson genome dataset were around  <dig> bp. as for the watson dataset, we plot the number of snps called by dial and the snp locations which could be verified in figure  <dig> 

we had snp calls from various outside sources for the watson genome dataset; however no such calls exist for this one. hence, we verified our calls against a mapping of reads on the orangutan draft assembly , which we made using maq  <cit> . an allele was deemed to be correct if it was supported by at least one read on the assembly with a mapping quality greater than  <dig>  even though we make similar number of snp calls for the orangutan and watson datasets, the false-discovery rate for these shorter reads is much higher, as deduced by the comparison to the results from maq. in order to investigate the reasons for this difference, we mapped a single illumina run of  <dig> bp single reads from the genome of a korean individual  <cit>  to the human assembly. maq was able to align  <dig> % of the reads, mapping 86% of them uniquely on the human assembly. in contrast, for the orangutan, maq was able to align 89% of the short reads, but map only  <dig> % of them uniquely on the orangutan draft assembly, much lower than for human assembly. the biggest source of false-positives for this dataset is the number of assembled contigs that could not be uniquely mapped. many of the contigs map to the unassigned scaffolds  along with a mapping on some other chromosome. we believe that our results for this dataset will show a much lower false-positive rate once an improved assembly is available and more data from the scaffolds is assigned to chromosomes.

mule deer transcriptome dataset
compared to generating sequence data from an entire genome, sequencing transcribed dna  has several potential advantages. for a typical mammalian genome, the total length of all gene transcripts is approximately 1% of the genome's length; this means roughly  <dig> million bases instead of  <dig> billion. thus, much less sequence data is required to reach depth- <dig> coverage for a substantial fraction of the target. and for some applications, the much higher fraction of functional sites is useful. however, there are disadvantages as well. while blood is a typical source of dna for whole-genome sequencing, the number of genes transcribed in blood cells is rather low. if possible, it may be preferable to sequence dna from another tissue or combination of tissues. another complication is that the relative amounts of transcript dna vary widely among genes, which lowers the efficiency of transcript sequencing , and forces us to remove our constraint on the maximum number of fragments that overlap a snp.

to experiment with this approach, we applied dial to  <dig> mbp  of roche/ <dig> transcript data obtained from a pooled  lymph-node cdna library from mule deer . despite the reduced number of genes transcribed in lymph nodes compared to certain other tissues , we identified  <dig> high-confidence snps, which is sufficient to populate a small genotyping array with snps having a good chance of relating to differences in immune-system genes with phenotypic effects. note that a similar amount of whole-genome dna would provide only about  <dig> -fold coverage, which according to our results discussed above is expected to reveal less than  <dig> snp, assuming a total of about  <dig> million nucleotide differences in the sampled mule deer.

CONCLUSIONS
the application that motivated this project is to use high-throughput sequencing and genotyping methods to help maintain endangered species, for which there is rarely an available reference genome assembly. the use of genome technologies in species conservation efforts is appropriate, given studies that suggest a positive relationship between heterozygosity and breeding success in endangered populations  <cit> . in outline, our strategy is to sequence only as much dna as is required to design an affordable genotyping array, to then genotype a large number of animals from the species of interest, and finally to use other computational tools  <cit>  to analyze the resulting genotyping data and help to guide reintroduction or captive-breeding programs.

the computational pipeline described above fulfills our needs for calling snps without a reference genome. snps are called by a completely automatic process that requires only modest amounts of data to make an appropriate number of snp calls. this paper provides a theoretical model of the numbers of correct and false positive calls, as well as experimental data that suggest parameter values for that model. these will help researchers to estimate both the amount of dna sequence data required to populate a custom-built snp array and the rate of inevitable incorrect calls caused by low-copy segmental duplications. this information will be useful in practice to balance sequencing costs with snp-call reliability as the relative costs of sequencing and genotyping fluctuate.

the number of snps that one can afford to genotype is typically far less than the total number of common snps in the species of interest. hence, it may be cost-effective to limit sequencing to dna from specific parts of the genome, so that a given amount of sequence data will produce far deeper coverage. one approach, mentioned above, is to sequence gene transcripts, and thereby target protein-coding regions. another strategy is to sequence "reduced representation libraries"  <cit> , created by size-selecting fragments produced by complete restriction endonuclease digestion, which can target regions distributed throughout the genome. a third approach could be to capture particular intervals by hybridization to a collection of genomic fragments, using either a microarray  <cit>  or a solution-based  <cit>  technique. the methods described in this paper will work for sequence data generated by any of these approaches.

we are applying this strategy to the tasmanian devil , the largest living carnivorous marsupial. the species is currently found in the wild only in the state of tasmania, australia, and is under threat of extinction by an aggressive transmittable cancer  <cit> . the phylogenetically closest available genome sequence is from the laboratory opossum . the two lineages diverged about  <dig> million years ago, making it impossible to use the opossum sequence as a reference genome for mapping the devil reads. using roche/ <dig> machines, we generated low-coverage sequence for two tasmanian devils from geographically dispersed locations. dial then called snps, which were used to design a genotyping array. the results of that study will be presented in a separate report.

availability and requirements
• project name: dial

• availability: http://www.bx.psu.edu/miller_lab/

• operating system: linux

• programming language: c/python

• other requirements: newbler, velvet for assembling the flanking regions of the called snps

• license: mit license

• any restrictions to use by non-academics: none

authors' contributions
ar and wm designed the project. yz performed the probabilistic analysis. ar designed and implemented the dial software. vmh and scs founded the tasmanian devil genome project, which motivated calling snps without a reference. ar and wm wrote the paper with input from the co-authors. all authors read and approved the final manuscript.

