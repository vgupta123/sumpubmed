BACKGROUND
as the number of electronic resources grows it is crucial to profit from powerful tools to index and retrieve documents efficiently. this is particularly true in life sciences where new technologies, such as dna chips a decade ago and next generation sequencing today, sustain the exponential growth of available resources. moreover, exploiting published documents and comparing them with related biological data is essential for scientific discovery. information retrieval , the key functionality of the emerging "semantic web", is one of the main challenges for the coming years. ontologies now appear to be a de facto standard of semantic ir systems. by defining key concepts of a domain, they introduce a common vocabulary that facilitates interaction between users and softwares. meanwhile, by specifying relationships between concepts, they allow semantic inference and enrich the semantic expressiveness for both indexing and querying document corpus.

though most ir systems rely on ontologies, they often use one of the two following extreme approaches: either they use most of the semantic expressiveness of the ontology and hence require complex query languages that are not really appropriate for non specialists; or they provide very simple query language that almost reduces the ontology to a dictionary of synonyms used in boolean retrieval models  <cit> . another drawback of most ir systems is the lack of expressiveness of their results. in most cases, results are simply proposed as a set of resources with no further explanations concerning the match between the resources and the query. even when an ir system proposes a list of ranked resources, no explanation is provided with regard to  this ranking, which means the results are not made explicit. in the absence of any justification concerning the results of ir systems, users may be confused and may not know how to modify their query satisfactorily in an iterative search process.

this paper describes an original alternative. our ontology based information retrieval system  relies on a domain ontology and on resources that are indexed using its concepts . to fully benefit of this system, queries have to be expressed using concepts of the same ontology. obirs' interface thus provides query formulation assistance through auto-completion and ontology browsing. it estimates the overall relevance of each resource w.r.t. a given query. such an overall relevance is obtained by aggregating the partial similarity measurements between each concept  of the query and those indexing the resource. aggregation operators we use are preference models that capture end user expectations. the retrieved resources are ordered according to their overall scores, so that the most relevant resources  are ranked higher than the least relevant ones . more interestingly, defining an overall adequacy based on partial similarities enables a precise score to be assigned to each resource w.r.t. every concept of the query. we summarize this detailed information in a small explanatory pictogram and use an interactive semantic map to display top ranked resources. thanks to this approach, the end user can easily tune the aggregation process, identify, at a simple glance, the most relevant resources, recognize entities adequacy w.r.t. each query concept, and identify the most discriminating ones.

the main contribution of this work is to favor interactivity between end users and the information retrieval system . this interactivity is based on the explanation of how a resource is ranked by the ir system itself: explaining how the relevance of a resource is computed provides additional knowledge that is useful to end users to more appropriately reformulate their query. this is achieved by evaluating how well each resource matches the query based on both query/resource index semantic similarities and end user preferences and by providing a visual representation of retrieved entities and their relatedness relation to each query concept. note that this visual representation does not aim to represent the large number of documents contained in the database to visually identify related ones - as proposed for instance by  <cit>  for genes indexed by concepts of the gene ontology - but to represent a small subset of the most relevant ones with visual indications of their relatedness to the query.

the state of the art below starts by presenting general aspects of ir systems. it details particularly operators that are used to aggregate different query concepts, query expansion and the different approaches of similarity measurement used in this context. then, the methods section describes a new resource-query matching model based on multi-level aggregation of relevance scores. the results section starts by comparing obirs engine with some other methods on a benchmark. then the interactive query rendering interface of obirs is detailed. a case study is carried out that aims at identifying transcription factors involved in hemopoiesis pathway. synthesis and perspectives of this work are then given in the conclusion section.

information retrieval systems overview
the contribution of this paper is related to the use of semantics for information representation and visualization in information retrieval systems.

information retrieval is generally considered as a subfield of computer science that deals with the representation, storage, and access of information. the field has matured considerably in recent decades because of the increase in computer storage and calculus capacity and the growth of the world wide web. some domains, such as life sciences, have particularly benefited from this technological advance. nowadays, people no longer labor to gather general information, but rather to locate the exact pieces of information that meet their needs  <cit> . the main goal of an information retrieval system  can thus be defined as "finding material  that satisfies an information need from within large collections "  <cit> . the main use of an irs can thus be summarized as follows: needing information within an application context, a user submits a query in the hope of retrieving a set of relevant resources. to achieve this goal, irss usually implement three processes  <cit> :

• the indexation process aims at representing resources  and queries with sets of  terms  that best summarize their information content.

• the search is the core process of an irs. it contains the system strategy for retrieving documents that match a query. an irs selects and ranks relevant documents according to a score strategy that is highly dependent on their indexation.

• the query expansion is an intermediate process that reformulates the user query, based on internal system information, to improve the quality of the result.

in most irss, the indexation process boils down to representing both documents and queries as a bag of weighted terms   <cit> . irss that use such document representation are keyword-based. a serious weakness of such systems is that they can be misled by the ambiguity of terms  and ignore relationships among terms   <cit> . to overcome this difficulty, recent irss map keywords to the concepts they represent  <cit> . these concept-based ir systems thus need general or domain conceptual structures on which to map the terms. conceptual structures include dictionaries, thesauri  or ontologies . it is now widely acknowledged that their use significantly improves the performance of irss  <cit> , and there is still room for improvement since most ontologies are not optimized to achieve this goal  <cit> . a survey of concept-based ir tools can be found in  <cit> . many concept-based irss were developed based on theoretical frameworks for the indexing process as well as for relevance measurement  <cit> . the latter assigns a score to each document  depending on how well it matches the query.

the work presented here is in line with the concept-based approach and takes as a starting point the existence of domain ontology. both resources and queries are represented by a set of concepts from this ontology. let us see on an example based on the gene ontology  how ontologies can help reduce the number of relevant documents missed by boolean irss . here resources are genes from the uniprot database, that have been indexed by go concepts  <cit> . such gene indexing were originally done manually using experimental evidence or through sequences' similarities. note that recent works propose to mine the scientific literature in order to enrich conceptual indexation of genes  <cit>  or to retrieve scientific articles  <cit> . having the following concepts set as query: {"erythrocyte development", "dna binding"}, our system retrieves, among the  <dig> best results, the gene hoxb <dig> that is relevant though indexed by none of the query concepts. indeed in its annotation one may find: "sequence-specific dna binding" and "erythrocyte homeostasis", the first concept being a specialisation of "dna binding" and the second one being a generalisation of "erythrocyte development" . hence a boolean search engine will not retrieve such gene  but by extending query concepts to hyponyms and hypernyms ontology based information retrieval systems do. such automatic query expansion process have been implemented in pubmed long ago and have been shown to significantly improve document retrieval  <cit> .

boolean requests and their generalizations
boolean requests are certainly the most simple and widespread requests. however, studies indicate that even simple boolean operators  are rarely used in web queries  <cit> , and are even sometimes misused  <cit> . indeed, even when users know that all the terms must be included in the indexation  or, on the contrary, that only one is needed , they do not mention it to the system. in the following, we thus focus on common requests where the user query is only a set of a few concepts.

minkowski-hölder's lp norms are aggregation operators that provide a theoretical framework to express whether a query is conjunctive or disjunctive using only one parameter  <cit> . they are particularly well suited to cases where the terms of the request are weighted. these weights may be related to term frequencies within the corpus, e.g. tf-idf  <cit> , or come from a fuzzy set indexation model. in this latter, a weight is associated with each concept indexing a document to represent to what extent a concept is a reliable indexation of a document  <cit> .

unfortunately, by summarizing the relevance of the document in a single score, aggregation operators tend to favor information loss and to fuzz out query results  <cit> . indeed, unlike end users, they do not differentiate between documents whose scores result from cumulative minor contributions of all concepts within the query and those whose scores are due to the major contribution of a single concept. in addition, as they do not take advantage of semantic resources , they are unable to find relevant documents that are indexed by concepts that are different but semantically related to those of the query. indeed, these operators only aggregate weights of a sub-set of terms: the ones that appear in the query. this statement is the basis of query expansion.

query expansion
query expansion is an intermediary step between the indexing and the matching process. as stated in  <cit> , end users can rarely perfectly formulate their needs using query languages because they only have partial knowledge of irs strategy, of the underlying semantic resources, and of the content of the database. based on this statement, automatic query refinement and expansion strategies have been developed. these reformulations may modify a query by adding concepts to it, by removing "poor" concepts from it or by refining its concepts' weights. many query expansion techniques have been proposed, among which the widespread relevance feedback  <cit> . this query expansion technique uses the documents that are judged to be relevant by the user after an initial query to produce a new one using reformulation, re-weighting and expansion  <cit> . when done automatically, this process is called relevance back-propagation  <cit> .

query expansion may also be based on external vocabulary taken from ontologies or thesauri  <cit> . a common expansion strategy aims at supplementing the query through adding its concepts' hyponyms. this method is an interesting complement to the boolean search system detailed above. indeed, it is then possible to select documents that are not indexed using exactly the same terms as the query and thus avoid silences. this strategy is used for instance by the irss of pubmed  <cit>  and gofish  <cit> . however, since no distinction is made between the initial terms and those added, users may be puzzled by the set of documents retrieved. indeed, since they are not aware their query has been altered, they may not be able to understand the selection of a document indexed with none of their query terms. moreover, query expansion can lead to disseminate the most relevant documents within a very long list of results. xploremed  <cit> , clustermed  <cit>  or gopubmed  <cit>  try to overcome these problems by structuring and filtering search results in a semantic manner while textpresso relies on predefined semantic categories to refine document search in a given field . we propose an alternative solution where the score of a document vary depending on whether it is indexed by an exact query concept, a semantically close concept or a distant one. this allows our system to identify and retrieve a subset of the most relevant documents and to graphically represent these scoring subtleties to explicit document relevance with respect to the query.

semantic similarity measurements
it is possible to improve query expansion by using similarity measures. these measures not only enable selection of documents indexed with terms related to those of the query, but also retrieved documents to be ranked according to their semantic similarity to the query.

since our approach extensively relies on semantic similarity measurements that significantly impact rsv calculus , we detail some of them below. as some of these measures satisfy distance axioms, we use semantic proximity, closeness or similarity randomly in the following.

the similarity measurements that have been proposed can be grouped in two main categories depending whether they are defined by intention or by extension. the first use the semantic network of concepts as metric space, and the second use a statistical analysis of term appearance in a corpus of documents  <cit> .

while the semantic network may include various kinds of concept relationships, most intentional similarity measures only rely on the subsumption relationship, denoted as is-a  <cit> . indeed this relationship is the only one shared by all ontologies and it constitutes their backbone. the key role of the is-a relationship is clearly made explicit in the formal definition of an ontology proposed by  <cit> . the set of is-a relationships among concepts can be conveniently represented by a directed graph whose vertices are concepts and whose edges indicate their subsumption relationship . many concept similarities are based on this is-a graph. one of the most straightforward uses of this graph structure is to consider the length of the shortest path between two concepts c <dig> and c <dig> as their semantic distance  <cit> . if all the edges of the path have the same orientation, one concept is subsuming the other, but the more changes in direction the path contains, the harder it is to interpret. therefore,  <cit>  proposes to adapt this classical graph distance to produce a more sensitive proximity measurement, πh <dig>  which takes into account the length of the path p between c <dig> and c <dig>  lg and the changes in direction within the path, nbc:

  πho=minp = lg+k*nbc 

the k factor modulates the influence of changes in direction on the overall measurement. when k =  <dig>  πh <dig> is equivalent to the distance proposed in  <cit> . on the other hand, a high value of k implies a minimum number of changes and thus a path that meets either one of the least common ancestors of c <dig> and c <dig>  denoted by lca or one of their greater common descendants, denoted by gcd. since  <dig>  when  <cit>  first proposed to use lca in this context, it has played a key role in several similarity measurements. however, while focusing on the lca, this measurement neglects the symmetric notion of gcd and completely ignores whether concepts share common descendants, or not.

one main limitation of all these graph-based measurements is that they assume edge homogeneity, whereas each edge of the is-a graph represents a specific degree of generalization or specialization. the semantic measurement proposed in  <cit>  tries to capture this information based on the number of descendants of each concept. as this measurement is based on the is-a graph, it is denoted disa and the authors demonstrate that it satisfies distance axioms. more formally, denoting by sc a set of concepts from an ontology, by hypo the set of concepts that are hyponyms of at least one concept of sc and by ancex the set of concepts that are ancestors of either c <dig> or c <dig> , disa is defined as:

  disa=hypo)∪hypo∪hypo-hypo∩hypo 

in this approach, the information content of a concept is evaluated by intention using only the ontology but not the corpus. alternatively, extensional measurements are mostly based on the corpus and often rely on the concept information content  defined in  <cit> . the ic of a concept c <dig> is derived from the probability p that a document of the corpus is indexed by c <dig> or one of its descendants:

  ic=-log) 

combining the ideas of lca and ic,  <cit>  introduces the notion of the most informative common ancestor  of a pair of concepts and defines a semantic proximity based on it as: πresnik = ic). it should however be noted that mica is not necessarily a lca of c <dig> and c <dig>  this proximity measurement is tightly correlated with the individual ic of the two concepts.  <cit>  proposes a variant to correct this bias:

  πlin=2*ic)ic+ic 

 <cit>  proposes another evaluation of ic of a concept. the main idea behind such a formulation of ic lies in the assumption that a concept with many hyponyms has a greater probability of being present in a given corpus . indeed, a concept is considered present in a corpus when at least one of its hyponyms is present. the expressiveness of a concept is thus inversely proportional to the number of its hyponyms. it should be noted that the ic value is  <dig> for the root and  <dig> for leaves.

  ic=1-log+1)log 

where maxcon is the number of concepts in the ontology. from now on, we assume that this ic estimation is used to define lin and resnik proximities. proximities can be used in different contexts and their choice strongly depends on final objectives. adequacy with real concepts' relatedness  must also be taken into account within the measurement choice  <cit> . the following section describes our aggregation model, based on a semantic similarity that leads towards relevance scoring of document with respect to a query.

methods
an original multi-level score aggregation to assess documents' relevance based on semantic proximity
our work refers to concept-based irss. our retrieval status values  are calculated from a similarity measurement between the concepts of an ontology. we propose to break down the rsv computation into a three stage aggregation process. first, we start with a simple and intuitive similarity measure between two concepts of the ontology ; then, a proximity measure is computed between each concept of the query and a document indexing ; finally, these measures are combined in the global rsv of the document through an aggregation model . the last stage  captures and synthesizes the user's preferences and ranks the collection of retrieved documents according to their rsv. the aggregation model enables restitution of the contribution of each query term to the overall relevance of a document. hence it provides our system with explanatory functionalities that facilitate man-machine interaction and support end users in iterating their query. furthermore in order to favor user interactions concept proximities must be intuitive  and rapid to compute .

we estimate the similarity of two concepts based on the jaccard index between their descendant sets. two main objectives are followed here: i) avoid silence when no document is indexed with the exact query concepts but with related concepts  to increase the recall of the system; ii) make the query results more explicit concerning the way a match is computed, in particular documents indexed by query concepts and documents indexed by hyponyms or hypernyms need to be distinguished.

semantic similarity between concepts and sets of concepts
the choice of the semantic similarity measurement used by our irs has a major impact on: i) the relevance of the retrieved documents, ii) the system's recall and iii) user comprehension of the document selection strategy. hence, we propose a variant of the similarity measurement proposed by  <cit> , with a valuation of the informational content of a concept based on the number of its hyponyms  <cit> .

because it has been emphasized that query concepts should only be replaced by hyponyms or hypernyms, we estimate the semantic proximity of two concepts based on how much their hyponyms overlap  as long as one is a hyponym of the other and otherwise we fix it at 0:

  πjd=hypo∩hypohypo∪hypoifc1∈hypoorc2∈hypo0otherwise 

it should be noted that πjd is comprised between  <dig> and  <dig>  πjd =  <dig> if, and only if, c <dig> and c <dig> have no hyponym relationship while πjd =  <dig> if, and only if, c <dig> = c <dig> .

several solutions have been proposed to extend similarity measurement between two concepts to measurement of similarity between two sets of concepts. this problem is of particular interest in life sciences because similarity between two gene indexations through the gene ontology  may provide hints on how to predict gene functions or protein interactions  <cit> . whereas comparing gene indexations  requires similarity measurements to be symmetric, this is not the case in ir. indeed, when matching documents to queries, it seems normal to penalize a document because one concept of the query is absent from its indexing; on the other hand, penalizing a document because it is indexed by one concept absent from the query would be rather odd. this latter remark leads to define the proximity between an elementary query  and a document as the maximum value of the similarities calculated between the query concept and each concept of the document indexing. by extension, this leads to a simple and intuitive proximity measurement between each query concept and a document based on the maximum operator. more formally, if π denotes the similarity between two concepts from an ontology o, and di denotes the ith concept of document d index, i =  <dig> .|d|, then we define the similarity between a concept qt of the query and d as π max0≤i≤|d| π.

proximity measurement between a document and a query
after determining similarities between each concept of the query and  a document, the next step consists in combining them in a single score that reflects the global relevance of the document w.r.t. the query. user's preferences have to be taken into account during this process in order to determine the overall relevance of a document w.r.t. a query, i.e. its rsv.

as mentioned above, computing documents' rsv enables them to be ranked according to their relevance. furthermore having the score details of a document for each query concept allows us to justify and compare the source of the match of each document with the query. this is clearly related to the preference representation problem that has been extensively studied in decision theory  <cit> . a classical solution is to define a utility function u in such a way that, for each alternative d, d' in a list d of alternatives, d≽ d'  if u ≥ u. the decomposable model of krantz  <cit>  has been widely used when alternatives are n dimensional. following this model the utility function u is defined as: u = h,..,un) where ut, t =  <dig> . n, are real-valued functions in  <cit>  and h:  <cit> n →  <cit>  is an aggregation operator that satisfies the following conditions:

• h is continuous;

• h =  <dig> and h = 1;

• h is monotonous: ∀j in  <dig> .n if aj ≥ bj then h ≥ h

in our context, the n dimensional space corresponds to n query concepts. the n coordinates of a document correspond to its proximities with each concept of the query, i.e., π, t =  <dig> . n, defined in the previous section correspond to the ut functions. the aggregation model combines the degrees of relevance  of a document indexing w.r.t. each query concept w.r.t. the user's preferences. the aggregation function h captures the preferences of the user: the way the elementary degrees of relevance are aggregated depends on the role of each query term w.r.t. the user's requirements. three kind of aggregation can be distinguished:

• conjunctions , h,..,π)≤mint= <dig> .|q|π;

• disjunctions , h,..,π)≥maxt= <dig> .|q|π;

• compromises, mint= <dig> .|q|π≤h,..,π)≤maxt= <dig> .|q|π.

with the goal of improving man/machine interaction, we hope to give users a friendly and intuitive way of expressing their preferences concerning the overall relevance scoring strategy between a document and a query. we thus focus on compromise operators because they fit the widespread decision strategy that constrains the overall score to be between the minimum and the maximum value of elementary scores . our approach is consequently based on yager's operators  <cit> . these define a parameterized family of functions that represents compromise operators:

  ym,..,π)=∑t=1|q|πq∕|q|1∕q,q∈ℝ 

to get a better idea of the wide range of aggregation functions that are possible with this operators' family, let us exert some remarkable values:

• q =  <dig>  arithmetic mean,

• q = - <dig>  harmonic mean,

• q →  <dig>  geometrical mean,

• q → + &# <dig>  max

• q → - &# <dig>  min 

a compromise operator can thus be selected by the user who may simply provide the value of parameter q. the choice of an aggregation operator is simply reduced to the choice of parameter q which still corresponds to our intuitive man/machine requirements. indeed, our irs interface includes a cursor to control the value of parameter q and to indicate whether the aggregation should tend toward a generalized "or", a generalized "and", or should tolerate more or less compensatory effects.

when criteria do not play a symmetric role in the aggregation process, the relative importance of criteria can also be introduced in aggregation operators. in our case, it is possible to check that the yager family can be extended to the weighted operators' family:

  Ȳwm,...,π)=∑t=1|q|pt.πq1q,∑t=1|q|pt= <dig> 

when the above weighted operators's family is used, the user has to fit both q parameter and the weights distribution upon the query terms. in order to keep the query terms weights selection simple and intuitive, our irs interface allows the user to move cursors  and to see inline effects of that change in results.

this rsv 3-step computation  has been integrated in an efficient and interactive querying system as detailed in the following section. note that this 3-step strategy can be used with any concept/concept similarity measure. our querying system, obirs, let the user chose between the lin  and the jaccard proximities.

RESULTS
querying systems endowed with query expansion that add hyponym concepts to the query can be seen as the first step towards a semantic querying system. our approach refines basic solutions to avoid silences by selecting documents that are indexed by the semantically closest hyponyms or hypernyms of the query concepts. furthermore, we are convinced that users should easily be able to understand the rsv at a glance to favor interaction with the irs during query reformulation. our 3-stage relevance model  integrates both the semantic expressiveness of the ontology based data structure and the end-user's preferences. the more user friendly the man-machine interface, the more efficient the interaction between the irs and the end-user.

the 3-step relevance model presented in this paper has been implemented and a web-based client is available through  <cit> . the model is experimentally validated as follows. first we perform experiments to determine the impact of similarity measurement using the muchmore collection  <cit>  and secondly we use obirs in a use case dedicated to gene identification.

obirs results on an experimental campaign
to study the impact of ic based semantic similarity measures on obirs' performances, we need to fix system parameters such as q value , number of retrieved documents  and rsv threshold . three measures have been implemented and used for this experiment: lin, resnik and jaccard. our search strategy is also compared with boolean search using and/or operators.

the muchmore collection consists of  <dig>  medical paper abstracts and  <dig> queries with their relevance judgments. documents and queries in that collection are indexed using mesh concepts. the evaluation methodology used for this campaign follows the trec protocol  <cit> . note that during experiment, some query terms and document terms haven't been mapped to mesh concepts leading to smaller precision values than expected. this issue is known as semantic coverage problem and is still under analysis.

results are summarized in figure  <dig> by the variation curve of the system precision for ten recall points . obirs performances with lin, jaccard or resnik proximities are comparable and far better than those obtained by a basic boolean search using and or or operators. the online version thus let the user choose between the jaccard proximity and the lin one that is semantically richer but also harder to interpret.

before detailing case studies it is necessary to describe obirs user interface and main functionalities.

overview of obirs user interface
the screenshot presented in figure  <dig> shows an overview of obirs querying website interface. the loaded corpus contains the whole genome of  <dig> species . the querying field of this website  allows users to retrieve genes, of a given species, that are related to some go concepts  <cit> . auto-completion assistance is provided to help users to set query go concepts. using the advance search link, users may see for each selected concept of the query its position within the go hierarchy . they may also adjust each concept weight to give more influence to certain concepts. figure 3-c shows the parameters' setting panel, where users can easily tune the aggregation function according to their preferences by moving a cursor from rough  to tolerant , limit the number of retrieved documents  and fix a threshold for the rsv .

once the  query is completed, results appear on another screen . the irs selects relevant genes and displays them on a semantic map  in such a way that their physical distance to the query symbol  is proportional to the rsv values. each gene may be displayed either by a pictogram or by its official symbol . users can thus identify at a glance the most relevant genes. the pictogram details adequacy between gene annotations and the query: the contribution of each query concept to the rsv assessment is synthesized in a histogram where a bar is associated with each concept qt of the query. this bar is coloured depending on whether the closest  concept of the gene annotation is exactly qt , a hyponym  or a hypernym  of qt. the bar is purple in other cases. the size of the bar associated with qt is proportional to the elementary relevance of the document w.r.t. qt ). a visual lens synthesizes information of a gene when the mouse hover its pictogram . further details may be obtained by clicking on it : "show description" gives its official symbol, link towards uniprot database and its description according to uniprot. "match explanation" details each query concept's elementary relevance.

to refine their queries, users can change relative importance of query concepts by adjusting their weight . modifying a weight refreshes the visualisation screen and histogram positions change in order to take into account new weight values. results may be exported as csv or xml.

it should be noted that expanding query with hyponyms and hypernyms de facto increases recall and decreases the precision of an irs. however in obirs, since users may distinguish at a glance the most relevant genes, they benefit of query expansion without its downside.

cases studies: application to gene identification
this section describes two case studies illustrating the relevance of obirs for gene retrieval.

during the generation of red blood cells which is called the erythropoiesis, the expression of several transcription factors is required in progenitor cells to induce their differentiation. amongst these genes, some such as gata <dig>  tal <dig> and sp <dig> are known to be essential. here obirs is used in order to obtain the list of known transcription factors involved in human hemopoiesis pathway. our query was made of three concepts: {"erythrocyte development", "regulation of transcription, dna-dependent", "dna binding"} limiting result to the best  <dig> genes . the first  <dig> genes were known genes amongst which  <dig> were linked to erythropoiesis and the remaining ones were involved either in leukemia derived from red blood cell precursors or in more embryonic steps of blood formation. moreover, the top  <dig> genes were of strong interest . despite the large number of human genes in uniprot database  and go concepts  the result of this query is obtained in a few seconds. the second case study is focused on zebrafish, a model organism used in agronomy to study fish immune responses to viruses . during viral infections, many genes are involved in the anti-viral response, amongst which, those responsible for the inflammation. however, the inflammation can also be induced by other conditions such as autoimmune diseases or cancers. here obirs is used in order to obtain the list of known genes involved in this anti-viral response. our first query was made of two  concepts: {"defense response to virus" and "inflammatory response"} limiting result to the best  <dig> genes. most of the retrieved genes were of strong interest however some, such as the gene pxk, were not directly related to anti-viral response but to lupus, an autoimmune condition which induces also inflammation  <cit> . we thus refined our query by giving more weight to "defense response to virus" as compare to "inflammatory response" . the new result contained  <dig> viral-reponse related genes plus a locus  having no gene name. as expected, pxk is no longer in the top result list.

CONCLUSIONS
the approach described in this paper is an important step towards an irs that benefits from the semantic expressiveness of ontologies while remaining easy to use. an original three stage aggregation model has been described to compute rsv scoring. this model has the particularity to embed end user preferences. the resulting obirs prototype is one of the first irs able to elucidate its document selection to the user thanks to the decomposition of the rsv score that can be transcribed through intuitive pictograms. by locating these pictograms on a semantic map, obirs provides an informative overview of the result of the query and new possible interactions. we are currently working on an obirs extension that will let users reformulate their query by graphically selecting the documents they value and those in which they have no interest. this reformulation can be done by adding/removing concepts from the query, specifying/generalizing initial concepts of the query or adjusting the aggregation function. reformulation leads to several optimization and mathematical questions but also raises important issues concerning feedback to users to enable them to continue to understand the irs process and fruitfully interact with it. we believe that there are many advantages to coupling the ir engine and rendering the result of the query, and that they should be considered simultaneously to provide a new efficient, interactive query environment. the rsv decomposition described in this paper is a good example of the benefit of simultaneously considering two related problems: i) how to rate documents w.r.t. a query ii) how to provide users feedback concerning rating of the documents. the latter is crucial to favor user/irs intuitive interaction in iterative improvement of the query.

list of abbreviations
csv: comma separated values; dna: deoxyribonucleic acid; gcd: greater common descendant; go: gene ontology; ic: information content; ir: information retrieval ; lca: least common ancestor; mesh: medical subject headings; mica: most informative common ancestor; obirs: ontology based information retrieval system; rsv: retrieval status value; tf-idf: term frequency-inverse document frequency; trec: text retrieval conference; umls: unified medical language system; xml: extensible markup language.

competing interests
the authors declare that they have no competing interests.

authors' contributions
vr and sr initiated and coordinated this project. jm proposed an aggregation model based on decision theory. mc initiated, designed and developed the visualisation api used in obirs. supervised by vr, sr, jm and mc, mfs conceived and developed obirs and carried out the muchmore benchmark evaluation. ar carried out the case study. vr, sr and mfs wrote most of the manuscript, all authors read and approved the final version.

