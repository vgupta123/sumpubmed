BACKGROUND
a goal of proteomics is to distinguish between states of a biological system by identifying protein expression differences  <cit> . shotgun proteomics is a large-scale strategy for protein identification in complex mixtures that involves pre-digestion of intact proteins followed by peptide separation, fragmentation in a mass spectrometer, and database search. its name is derived from dna shotgun sequencing, which in turn follows the analogy of a shotgun's quasi-random firing pattern and dispersion to ensure the target is hit.

multi-dimensional protein identification technology  is a shotgun proteomics technique capable of identifying thousands of proteins in proteolytically digested complex mixtures  <cit> . mudpit separates peptides according to two independent physicochemical properties using two-dimensional liquid chromatography  online with the ion source of a mass spectrometer. this separation relies on columns of strong cation exchange  and reversed phase  material, back to back, inside fused silica capillaries. the chromatography proceeds in cycles, each of which consists of increasing salt concentration to "bump" peptides off the scx followed by a hydrophobic gradient to progressively elute peptides from the rp into the ion source. this process identifies mixture components by tandem mass spectrometry . for didactic purposes, a simplified and interactive mudpit simulator is available at the project's web site; its interface is described in figure  <dig> 

computational approaches for lc/ms-based differential proteomics usually involve in silico chromatogram alignment followed by pattern recognition strategies  <cit> . however, because of the more complex nature of mudpit's lc/lc method and the alternating acquisition of mass spectra and tandem mass spectra, chromatographic alignment is more complicated than for lc/ms data. a milestone that eventually allowed differential mudpit analysis was set with the development of alternative protein quantitation methods that use features from the tandem mass spectra  as surrogate measures of protein abundance  <cit> . an important step was taken when liu et al. demonstrated that the number of tandem mass spectra obtained for each protein, or "spectral count", correlates linearly with protein abundance in a mixture for two orders of magnitude  <cit> . these advances allowed lc/lc/ms/ms to produce semi-quantitative data on mixtures; however, two issues have remained open: how to normalize spectral count data for profile comparisons and how to statistically identify bona fide differences between samples . heretofore, differential proteomics by mudpit spectral counting has relied on repeating assays to increase the number of identified proteins, improve protein coverage, and enable traditional statistical methods to pinpoint differences between biological states. studies have shown student's t-test, fisher's exact test, and the g-test to be trustworthy for composing putative differential marker tallies when three or more replicates are available  <cit> .

recently, chen et al. increased peptide and protein identifications in complex protein mixtures by re-analyzing samples digested in the presence of different ms-compatible detergents  <cit> . moreover, the improved proteolytic digestion protocols potentially increased identification of less abundant proteins. however, the experimental design described by chen et al. introduced additional data analysis challenges, since replicate readings are not acquired.

the contributions by liu et al. and chen et al. serve as foundations for this work. here, we introduce a simple to use, yet efficient and panoptic, software for differential shotgun proteomics that addresses the data analysis issues of the experimental designs mentioned above. our software, patternlab for proteomics, or just patternlab as referred to throughout, achieves its goal by featuring two new data analysis methods in addition to other widely adopted statistical approaches. the first method, acfold, addresses experiments with less than three replicates from each state  or having data acquired by different protocols, as described by chen et al. acfold uses a combined criterion based on expression fold changes and the ac test  <cit> ; its importance is demonstrated here with experimental data. the other method addresses experimental designs that comprise multiple replicates from each state and is referred to as nsvm  because of its roots in evolutionary computing and statistical learning theory  <cit> . we benchmarked nsvm against the widely adopted student's t-test over a spiked marker dataset and identified its niche. a detailed description of acfold, nsvm, and patternlab's overall architecture is given, and critical issues of each method and how they were addressed are provided in the implementation section.

implementation
patternlab's current version is optimized for lc/lc/ms/ms data using spectral counts. its architecture comprises four core modules . these modules can be operated programmatically or through the graphical user interface  that also provides specialized graphing tools to aid interpretation. details of each module and a walkthrough of patternlab, including the two new feature selection procedures acfold and nsvm, are described below.

parser module
let "project" refer to one's experimental data from all mudpit assays of all biological samples from both control and case states. patternlab relies on the parser module to translate a project's ms data into an index file and a sparse matrix file. the index file lists all identified proteins within the project and assigns each one a unique protein identification  integer. as for the sparse matrix, each row follows the schema: ⟨class label⟩⟨pid⟩:⟨value⟩...⟨pid⟩:⟨value⟩. in the latter, ⟨class label⟩ ∈ {- <dig>  +1} is used to identify a biological state ; ⟨pid⟩ and ⟨value⟩ correspond, respectively, to a protein identification index in the project's index file and to the spectral count verified for that protein during the corresponding mudpit analysis. so, for example, the row "+ <dig> 1: <dig> 2: <dig> 3:6" specifies an analysis from the positive class having spectral count values of  <dig>   <dig>  and  <dig> for pids  <dig>   <dig>  and  <dig>  respectively, all other pids having value  <dig> 

there are various softwares that identify proteins by matching tandem mass spectra according to a database of peptide sequences, such as sequest  <cit>  and mascot  <cit> . the current parser can address both sequest followed by dtaselect  <cit>  and mascot having results exported to the dtaselect format. to use the parser, one should place the dtaselect results from the control and case analyses in different folders and then simply indicate their paths in the gui.

normalization module
one or more normalization methods can be applied to the sparse matrix. patternlab currently implements: ln , z  <cit> , total signal, maximum signal, and row sigma. the ln normalization is obtained by taking the natural logarithm of every value and aims at increasing the signal of the pids with low spectral counts with respect to the more abundant pids. the z normalization is achieved by subtracting from each original value the mean of all values of the corresponding pid and dividing the result by the standard deviation of all values from the same pid; the mean then becomes  <dig> and the standard deviation  <dig>  the total signal normalization is achieved by dividing each value by the sum of all values in the respective row. the maximum signal normalization is obtained by dividing each value by the largest value in its row; an underlying assumption is that, in each mudpit analysis, peptide identifications were obtained at or near the capacity of the tandem ms instrument. the row sigma normalization is achieved by calculating the mean and standard deviation of all values in a row and then dividing each value by the mean plus three standard deviations. the latter is introduced in this work as a variation of the maximum signal normalization that better handles assays that obtained an exceedingly high maximum value for a protein; further advantages are addressed in the results and discussion section.

acfold feature selection
the acfold analysis introduced in this paper is intended to evaluate data from projects having less than three replicate assays per class or assays obtained using different mass spectrometry protocols as described by chen et al.  <cit> . the acfold analysis takes advantage of two accepted criteria in proteomics to pinpoint differences between samples: the generalized ac test  <cit>  and expression fold changes  <cit> . the algorithm first parses the project's data as described in the parser module section. the sparse matrix is then compressed into two rows, one representing each class. the new rows' values for each pid are obtained by averaging the original values of the corresponding pids within their classes. but given the nature of the experiment at hand, it is likely that low-probability events  will not always be observed; this would result in a calculated average of  <dig> and imply a probability of  <dig> that is not justifiable by evidence according to cromwell's rule. to avoid the zero-frequency problem  <cit>  and make fold-change calculations possible, a pseudo spectral count of  <dig> is then added to each pid value of the two resulting rows, including the unobserved pids, following the process known as laplace's rule of succession. patternlab then calculates the ac test probabilities and the expression fold changes according to one of the user-specified normalization methods: total signal, row sigma, or none.

finally, a false-discovery rate  is estimated by the benjamini-hochberg procedure  <cit>  for a given fold-change cutoff. let m be the number of identified proteins minus the number of proteins that failed to pass the fold-change cutoff test. for  <dig> ≤ i ≤ m, let hi <dig> be the  hypothesis that the ith protein is not differentially expressed, and pi its p-value. assuming p <dig> ≤ p <dig> ≤ ... ≤ pm , and α the minimum fdr at which a test can be called significant, let

 k=max⁡{i:pi≤imα}. 

the null hypotheses hk <dig> are then rejected . if no such i exists, no hypothesis is rejected.

the user can define stringency levels aided by a distribution plot and supplementary information offered by the gui indicators. the stringency is performed by specifying a minimum fold-change cutoff, an ac test p-value cutoff, and the fdr α. we refer to figure  <dig> to demonstrate how the results are presented. lastly, the final report can be exported to text.

nsvm feature selection
nsvm is a feature selection algorithm introduced in this work and used here to pinpoint differences in protein expression profiles when multiple replicates of each class are available. the algorithm begins by parsing the project's data as described in the parser module section. nsvm then uses the structural risk minimization  principle from statistical learning theory  <cit>  to drive the convergence of a genetic algorithm . briefly, a ga is a stochastic optimization technique inspired in evolutionary biology which imitates inheritance, selection, crossover, and mutation to evolve a population of abstract genomes   <cit> . each individual represents a candidate solution  and is coded as an array of bits ; the nth bit value hypothesizes that either the protein whose pid value is n is differentially expressed  or not . the general aim of a ga is to evolve an initial population of randomly generated individuals so that, after a number of generations, the solution will be encoded in the genome of the historically fittest individual.

the ga works by generating successive populations on the premise that the average individual fitness  will increase for each new population. each new solution from nsvm is produced by first selecting parents according to their quadratically normalized fitnesses. formally, let s denote the set {i <dig>  i <dig>  ..., in-1} of individuals, ordered by nondecreasing fitness. let j and k be two randomly chosen numbers in the range from  <dig> to n2-  <dig>  the two individuals chosen to mate will be the ones in s indexed by the greatest integers no greater than the square roots of j and k ; if the same individual is chosen twice, this process is repeated. clearly, fitter individuals have significantly higher chances of being selected. during the mating process, a uniformly random crossover operator is used, so the single offspring receives each gene  from either one of its parents with equal chances. the ga then performs mutations on the newly produced offspring according to a user-specified mutation index. for example, a mutation index of  <dig> allows the ga to perform up to two mutations in the offspring's genome. the mutation is performed by switching the values of randomly chosen bits with a bias towards mutating them to  <dig> . we recall that  <dig> and  <dig> represent excluding or including a feature, respectively. this bias accelerates the ga in finding solutions with fewer features. in addition, a fine-tuning parameter termed mutind1after can be set through the gui. this parameter stands for "mutation index  <dig> after", so after the algorithm has reduced the initial set of candidate proteins to a number below the one the parameter specifies, the mutation index is reduced to  <dig>  this allows the ga to search within the remaining combinations with a lower probability of making great shifts away from the local optimum it is approaching. the processes of mating, crossover, and mutation are repeated until a population of the same size as the initial one is formed for use in the next iteration of the algorithm. the user can also configure the ga to allow "elitism", permitting a specified fraction of the fittest individuals to continue on to the new population. the algorithm terminates when a user-specified number of generations has elapsed without the appearance of an individual that is fitter than the fittest found so far.

fitness evaluation is certainly one of the most important aspects of a ga. as far as we know, this is the first time a ga takes advantage of the srm principle  <cit>  to drive its convergence. briefly, the srm principle allows the evaluation of how well data points are separated in a feature space by a classification function, according to an empirical error measure on known examples and an upper bound on the function's error when generalizing for unknown examples  <cit> . the srm principle is the basis of the svm pattern recognition method, which searches for a classification function with the "best" trade-off between empirical error and worst-case generalization error. the upper bound on the generalization error grows monotonically with the machine's so-called vc dimension, so lower vc dimensions are preferred. additionally, another upper bound on the generalization error depends on the machine's number of support vectors in a way that a small number of such vectors is also preferred  <cit> . we use this other bound as well. in the remainder of this section we refer to each row of the sparse matrix as an input vector.

each individual's fitness is evaluated by how well the input vectors are "separated" in the feature space defined by the individual. first, the input vectors are mapped onto the feature space taking into consideration only the proteins whose pids have value  <dig> in the individual. secondly, an svm model is generated and the empirical error is evaluated by the leave-one-out approach  <cit> . the vc dimension, the number of support vectors, and the number of bits having value  <dig> in the individual are also recorded. finally, the fitness score for an individual is calculated as

 fitnessscore=c1loo+c2+c3+c4ng 

where loo is the svm leave-one-out error, h is the vc dimension, nsv is the number of support vectors, ng is the number of bits with value equal to  <dig>  and c <dig> through c <dig> are user-specified constants having default values set to  <dig>   <dig>   <dig>  and  <dig> , respectively. clearly, the lower the score, the fitter the individual. we note that the first three parameters are calculated using svm light  <cit> . figure  <dig> summarizes the nsvm process up to this point.

nsvm relies on the island model to keep population diversity and to better address the issue of a large search space. this approach works with a user-specified number of populations that evolve independently. individuals will migrate, from time to time, according to a user-specified time parameter. the migration proceeds as follows. first the ga randomly chooses two populations from its pool and pauses their computations after the fitness evaluation step. secondly, a random number is picked  to indicate the number of individuals to be exchanged between the two populations. thirdly, individuals are selected  and are exchanged between the populations. finally, the ga continues to evolve both populations from where they were stopped. patternlab takes advantage of the recent multi-core processors by having each population "live" in a different computing thread. thus, a computer with a certain number of cores can manage as many populations concurrently without sacrificing performance.

the features for the final classification model are selected by executing nsvm multiple times . for every nsvm execution, each time the fittest individual is replaced its genomic information is saved in a text file . after multiple nsvm executions, several history files are available and a ranking of the features can be established according to the frequency of occurrence of each pid in the history files. furthermore, a number of minimal discriminative features can be estimated by generating a two-column list having pids ordered by their ranks in the first column and their achieved frequencies in the second. the set of discriminative features is then estimated by locating, in this list, the two consecutive rows that present the greatest difference in frequency values. the number of features is then computed by counting how many features have scores above or equal to this gap's upper limit.

other available statistical inference methods and the result analyzer module
patternlab offers several additional feature selection methods that are widely adopted by the proteomics community. these methods include: svm recursive feature elimination   <cit> , forward svm , golub's index  <cit> , and student's t-test  <cit> . it is beyond the scope of this manuscript to detail these methods since they are well documented in the literature. figure  <dig> exemplifies patternlab's gui to access the feature selection methods and a result analyzer. in a future version of patternlab, we intend to add new components to the result analysis module. figure  <dig> exemplifies nsvm's interface.

RESULTS
two main issues characterize feature selection challenges in bioinformatics: the large input dimensionality and limitations in the dataset size. to deal with these problems, various feature selection techniques have been designed by experts from the machine learning and data mining fields. the philosophy behind patternlab is that there is no single, universally optimal feature selection technique  <cit> ; additionally, the existence of more than one subset of features that discriminates the data equally well  <cit>  should be considered. we believe that each feature selection strategy has its own niche, so it is important to know its idiosyncrasies, when to effectively apply it, and also to be aware of its limitations. for example, while the output provided by univariate feature rankings can be more intuitively grasped because they analyze each feature independently, protein subgroups that could possibly interact can only be detected through multivariate techniques .

row sigma normalization
methods with ease of interpretation tend to be more readily accepted, which is in line with the possibility of intuitive interpretation that is one of the goals of the row sigma normalization strategy introduced in this work. this strategy joins the robustness of the total signal normalization  with the ease of interpretation of the maximum signal normalization . this is achieved by dividing the expression value of each protein by the mean plus three times the standard deviation taken inside its row in the sparse matrix. when compared to the maximum signal normalization, which relates all proteins solely to the most abundant one, row sigma normalization is seen to help avoid misleading conclusions that might be reached should the most abundant protein have a large variance associated with it.

suggestion of when to apply acfold
acfold combines the ac test with fold changes to pinpoint differentially expressed proteins between classes; this is important because conclusions drawn only from fold changes could be equivocated. for example, suppose spectral counts of  <dig>  and  <dig>  were observed for protein x  during the control  assay. both x and y have a twofold up-regulation from the control to the case assay, but it is much likelier that the fold change for protein x happened by chance. the conditional probability of finding a spectral count of x <dig> in biological state  <dig> given that a spectral count of x <dig> was found in biological state  <dig> can be estimated by the ac test.

the ac test outputs a p-value related to testing a single hypothesis. in large-scale proteomic strategies, such as mudpit, thousands of hypotheses are tested simultaneously, requiring an appropriate error rate control instead of relying solely on p-values. the most well-known strategy to deal with multiple hypotheses is the bonferroni, but it is in some ways too conservative and this has led to the proposal of new ones  <cit> . the solution we propose to this massive multiple-hypothesis test problem is to analyze the data from an fdr perspective instead of that of p-values. the fdr is defined as the expected proportion of false positives among the results declared significant  <cit> . for example, by specifying an fdr of  <dig> , it is expected that no more than 10% of the results declared significant  be false positives. this control, however, cannot be obtained with the p-value alone.

label-free shotgun proteomics currently uses a random sampling process to estimate the relative quantitation of thousands of proteins. for this reason, determining the true number of differentially expressed proteins, which would require precise quantitation instead, has remained an open challenge. due to the lack of a training set with known answers, our approach relies on a theoretical fdr estimator to cope with imprecise quantitation. one strategy to evaluate the effectiveness of fdr approaches is to spike protein markers with known concentrations into complex protein mixtures  to perform real, but controlled, experiments, which are therefore verifiable. for example, zhang et al.  <cit>  compared replicate lc-ms/ms assays of the s. cerevisiae lysate plus six protein markers  and observed that the true fdr of the ac test, when comparing two assays, varied from ~1% to ~13%, depending on the marker. furthermore, the authors concluded that fisher's exact test, the g-test, and the ac test all "give reasonable false positive rates even with limited sampling numbers from a single replicate." in view of these results, our choice to rely on a theoretical estimator instead of physical measurements seems justified.

suggestion of when to apply nsvm
our observations suggest that nsvm's niche comprises projects targeting the selection of a minimum set of proteins  that nevertheless allows the highest rate of correctness to be achieved on unseen samples in classification problems. this selection entails the solution of the difficult bioinformatics combinatorial problem of choosing one out of the 2n sets into which n identified proteins can be combined. two widely adopted classes of methodologies to solve this problem are the filter and the wrapper approaches. briefly, the filter approach relies on a probabilistic method to eliminate or rank features, similarly for example to our use of the t-test. however, according to cover and van campenhout  <cit> , no ordering of error probabilities is guaranteed to produce the optimal feature subset or subsets. moreover, feature sets can be algorithm-dependent to achieve good results. on the other hand, wrapper methods handle the problem by relying on the classification algorithm during the feature selection process, but the algorithm becomes more prone to overfitting.

nsvm is a wrapper feature selection approach that couples a nature-inspired optimization technique  with the state-of-the-art classifier  to address the overfitting problem. this hybrid approach is justifiable because, even though svms efficiently generalize on noisy datasets, they have no internal feature relevance evaluation; therefore, noisy features can degrade their performance. consequently, feature selection plays a key role prior to svm classification, especially for complex datasets as in shotgun proteomics. our ga is a stochastic heuristic that deals with massive resampling to handle the idiosyncrasies of a dataset as related to a classifier to avoid overfitting. additionally, the feature sets selected by our ga are optimized for classification by an svm because our ga's fitness function considers the same principles that drive the svm classifier . accordingly, we showed that nsvm efficiently dealt with the overfitting problem on a high-dimensional and noisy dataset by correctly pinpointing the relevant features  and outperforming the t-test filter approach, as described below.

we demonstrate nsvm's niche using data from a real , yet controlled, experiment , which is therefore verifiable. the dataset was obtained from four aliquots of  <dig> μg of a soluble yeast total cell lysate that were mixed with bio-rad sds-page low range weight standards containing phosphorylase b , serum albumin , carbonic anhydrase , and trypsin inhibitor  at relative levels of 25%,  <dig> %,  <dig> %, and  <dig> % of the final mixtures' total weight. four mudpit assays were acquired for each aliquot as described by liu et al.  <cit> . finally, we generated three sparse matrices to simulate three benchmarking scenarios; in the first scenario, the input vectors originating from the 25% protein spiking were labeled as belonging to the positive class and all the rest as to the negative class. on the second scenario, the 25% and the  <dig> % input vectors were labeled as from the positive class and the rest as from the negative class. finally, the third scenario labels the 25%,  <dig> % and  <dig> % input vectors as belonging to the positive class and the remaining  <dig> % as belonging to the negative class.

each sparse matrix was normalized according to the z method and nsvm was applied to predict which and how many markers were spiked in the first matrix  using the linear svm kernel and varying some parameters of the ga . almost all parameter combinations correctly top-ranked the spiked markers, and pinpointed how many markers were spiked in the lysate. the dataset used for testing originated from a  <dig> salt step mudpit of a whole cell yeast lysate having more than  <dig> identified proteins; this is far more complex than the average proteomic experiment. therefore, more combinatorial possibilities were available, increasing computation time. given these facts, nsvm is expected to perform faster in less complex studies . the island mode and mutation index play a key role in the ga; while apparently there are no great changes in execution time, runs using a mutation index of  <dig> with the island mode turned on yielded better results in our dataset. we then opted to use the island model and a mutation index of  <dig> to evaluate nsvm over the other two scenarios; the method correctly predicted which markers were spiked in both cases .

nsvm was executed  <dig> times for each of the  <dig> specified conditions. phs <dig>  alb, cah, and itra stand for the spiked protein markers, and the numbers in the respective columns indicate the ranking. the elitism column stands for how many individuals of the population were allowed to remain untouched for the following generation. the islands column indicates how many seconds were required before a migration could even occur; a zero indicates that the island model was not applied. the no. feat. column indicates what nsvm suggested as the minimum set of optimal features. the avg. no. subst. column indicates how many times the fittest individual was substituted. the time column indicates the average time and standard deviation of  <dig> nsvm run. these results were obtained for scenario  <dig> as described in the suggestion of when to apply nsvm section. due to the stochastic nature of the method, results may vary.

phs <dig>  alb, cah, and itra stand for the spiked protein markers, and the numbers in the respective columns indicate the ranking according to nsvm.

the z normalization was chosen because one of the steps for estimating the vc dimension, according to the svm light algorithm  <cit> , is based on approximating the radius of the smallest hyphersphere that encompasses all input vectors by the norm of the largest support vector. after applying the z normalization, the new mean for each pid becomes  <dig> and the data points become "evenly" distributed around the origin; this makes the vc dimension estimate more accurate. however, if the changes between samples are expected to be minimum, applying nsvm on "unnormalized" data can also be considered.

the widely adopted student's t-test was then applied to check if we could rank the spiked markers as the topmost in the three scenarios after z and total signal normalization. these results are listed in table  <dig>  by comparing the results from nsvm  against the t-test results , it can be noted that the latter was unable to correctly rank the markers for some scenarios  and therefore did not reveal the optimal set. on the other hand, nsvm correctly ranked the spiked markers from all sparse matrices, which justifies the extra computation time it required. recall that nsvm encompasses multiple executions , and therefore more time to terminate . the limitations of the t-test seem to be that it relies on estimates of the mean and variance that do not necessarily reflect the true values when only a few samples are available  <cit> . nsvm's stochastic nature, combined with the various executions, makes it less prone to overfitting, but we note that it was unable to obtain the global optimum in any of the three matrices with only a single execution.

phs <dig>  alb, cah, and itra stand for the spiked protein markers, and the numbers in the respective columns indicate ranking according to student's t-test applied to the sparse matrix normalized by z or total signal normalization.

we recommend the t-test over nsvm for experiments where many changes are expected. table  <dig> shows that even though the optimal result was not always achieved, very satisfactory results were obtained. therefore, the t-test can offer a quick "bird's eye" view over changes throughout the entire experiment. on the other hand, nsvm works its way down to a minimum set of features, optimized for classification purposes, and therefore is probably not advisable for a holistic view.

differently than traditional gas, nsvm offers a new strategy to estimate which proteins are differentially expressed. our approach is a variation of the one used by li et al.  <cit>  to select differentially intensified mass spectral peaks from surface enhanced laser desorption ionization – time of flight proteomic profiles. briefly, the authors repeatedly executed their ga rooted in k-nearest neighbor, a non-parametric pattern recognition method, to obtain relatively small subsets of discriminative mass spectral peaks between classes of specimens. then peak appearance frequencies in the solutions were calculated and the authors showed that the most frequently selected peaks were the most discriminative. the efficiency of the algorithm was then proven on a validation set. the heuristics behind nsvm are far more computationally expensive than the one described by li et al., so multiple executions  would invalidate its applicability. however, nsvm adopts a strategy that allows it to converge to very satisfactory solutions within only a few runs .

prior attempts at performing feature selection based solely on some function of the vc dimension  <cit>  have been reported. however, our ga is based on the srm principle that combines such a function with empirical error measures. furthermore, we take advantage of a second theoretical error bound related to the number of support vectors to make nsvm converge faster . we compared nsvm's performance with and without computing the empirical error measures; the former achieved better results on our dataset .

CONCLUSIONS
the identification of trustworthy protein markers is not an easy task, since mass spectrometry based proteomics is still in development and spectral counting effectiveness can vary on the experimental setup, including mass spectrometry type and data-dependent analysis configuration. patternlab implements several existing strategies and adds two new tools to the proteomic data analysis arsenal, each one having its own niche. our results showed that even in simple scenarios, where the spiked concentrations can be considered relatively high, the data can still play tricks on well-founded feature selection methods. this is due to the dataset's high dimensionality, sparseness, and lack of a known a priori probability distribution. in even more realistic and complex scenarios, markers might be present in extremely low concentrations. modification in the experimental designs to isolate sub-proteomes is a solution; however, these separations are many times not straightforward if protein content is to be disturbed only minimally. therefore, even with all the advances in pattern recognition techniques, a set of bona fide markers requires experimental and computational validation in unseen samples to ensure the model is not a result of overfitting.

availability and requirements
• project name: patternlab for proteomics

• project home page: 

• operating system: windows xp or vista. patternlab is expected soon to run under linux and macintosh, thanks to the mono project  <cit> .

• programming language: c#

• other requirements: .net  <dig> 

• license: gnu

• any restrictions to use by non-academics: license needed

authors' contributions
pcc coded the software and wrote the first draft of the manuscript under the guidance of vcb and jry. eic and jsgf generated the mudpit experimental, helped test the software, and suggested the inclusion of important features in it. all authors read and approved the final manuscript and contributed to the development of the project's website.

