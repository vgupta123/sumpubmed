BACKGROUND
during evolution, genomes are subject to genome rearrangements, which are large scale mutations that can alter the ordering and orientation  of the genes on the chromosomes or even change the genome content by inserting, deleting, or duplicating genes. because these events are rare compared to point mutations, they can give us valuable information about ancient events in the evolutionary history of organisms. for this reason, one is interested in the most "plausible" genome rearrangement scenario between two genomes. more precisely, given two genomes, one wants to find an optimal sequence of rearrangements that transforms this genome into the other. in the classical approach, each gene has exactly one copy in each genome, and only operations that do not change the genome content are considered. a breakthrough in the research of these "classical operations" was hannenhalli and pevzner's algorithm for sorting by reversals  <cit> , and due to further research, we are now able to sort multichromosomal genomes by reversals, translocations, fusions, and fissions in polynomial time  <cit> . if one also considers transpositions, the problem gets more complicated, and there are only approximation algorithms known  <cit> . to simplify the existing algorithms, yancopoulos et al. invented the double cut and join operator , which can simulate reversals, translocations, fusions, fissions, and block interchanges , resulting in a simple and efficient algorithm  <cit> .

however, restricting the genes to be unique in each genome does not reflect the biological reality very well, as in most genomes that have been studied, there are some genes that are present in two or more copies. this holds especially for the genomes of plants, and one of the most prominent genomes is the one of the flowering plant arabidopsis thaliana, where large segments of the genome have been duplicated . there are various evolutionary events that can change the content of the genome, like duplications of single genes, horizontal gene transfer, or tandem duplications. for a nice overview in the context of comparative genomics, see  <cit> . in a pioneering work, sankoff tackled the challenge of genomes with duplicated genes with his "exemplar model"  <cit> , where the following problem was examined. given two genomes with duplicated genes, identify in both genomes the "true exemplars" of each gene and remove all other genes, such that the rearrangement distance between these modified genomes is minimized. this approach was later extended to the "matching model", where one searches for a maximum matching between the copies of each gene such that the genome rearrangement distance according to this matching is minimized  <cit> . however, both approaches have been proven to be np-hard for the breakpoint distance and the reversal distance  <cit> . note that these approaches do not construct the evolutionary events that changed the genome contents, i.e. the set of operations is still restricted to the classical set of operations. the first approach that explicitly constructed duplication events was done by el-mabrouk  <cit> , where one searches for a hypothetical ancestor with unique gene content, such that the reversal and duplication distance from this ancestor to a given descendant  is minimized. this work has been further extended during the last years . still, the duplications were technically limited to have the length of one element, and therefore the algorithms can only be applied if no large segmental duplication happened during evolution. one idea to overcome this problem was to simulate duplications by insertions, as it has been done in  <cit> . recently, yancopoulos and friedberg provided a mathematical model  of a genome rearrangement distance for genomes with unequal gene content  <cit> , combining the dcj operator with arbitrary but length-weighted insertions and deletions. to the best of our knowledge, the first work that allowed duplications of arbitrary segments was done by ozery-flato and shamir  <cit> , who demonstrated that a simple greedy algorithm can find biologically realistic sorting scenarios for most karyotypes in the mitelman database of chromosome abberations in cancer  <cit> . further simplifications of the model led to an algorithm with provable approximation ratio of  <dig>  <cit>  . recently, we proposed a heuristic algorithm for sorting a unichromosomal genome by dcjs, tandem duplications, and deletions of arbitrary segments  <cit> . in this work, we extend this approach to multichromosomal genomes. we are now able to sort an ancestral multichromosomal genome by a large set of operations, including duplications and deletions of arbitrary size. as a constraint, two chromosomes in the ancestral genome must be either disjoint or identical. although this restriction seems to be very limiting, this is often fulfilled in practice. a possible application is to examine the evolution of a cancer karyotype out of a diploid set of healthy chromosomes.

methods
preliminaries
a chromosome  is a string over the alphabet Σ = { <dig>  ..., n}, where each element may have a positive or negative orientation . we get the inverse of an element   by inverting its orientation. the reflection of a chromosome  is the chromosome . it is considered to be equivalent to πi. a genome π= {πi, ..., πm} is a multiset of chromosomes. the multiplicity mult of an element x is the number of its occurrences  in π. a segment is a consecutive sequence of elements of a chromosome. each element x can also be represented by the ordered set of its extremities xt and xh, where the ordering of the extremities is xt xh if x has positive orientation, and xh xt otherwise. for example, the chromosome  can also be written as . the two extremities belonging to the same element are called co-elements. we say the tail/head xt/h of an element x is a telomere if there is a chromosome in π beginning or ending with xt /h. the value t determines how often xt is a telomere in π, the value t is defined analogously. two consecutive extremities in a chromosome πi which are no co-elements form an inner adjacency w.r.t. another genome ρ if they are also consecutive in a chromosome of ρ, otherwise they form an inner breakpoint. an extremity which is a telomere in π forms a telomere adjacency w.r.t. another genome ρ if it is also a telomere in ρ, otherwise it forms a telomere breakpoint. for example, if ρ = {} and π = {, }, then 1t and 3h form telomere adjacencies, while 2h forms a telomere breakpoint. applying an operation op to a genome π yields the genome op. a genome rearrangement problem is defined as follows. given two genomes ρ and π and a set of possible operations, where each operation is assigned a weight, find a sequence of minimum weight  that transforms ρ into π. this minimum weight will be denoted by d. in our algorithm, we will consider all operations listed in subsection "operations". for simplification, we will assume that two chromosomes in ρ are either disjoint  or identical. furthermore, each element in Σ must appear in at least one chromosome of ρ. note that these restrictions only hold for the genome ρ, not for π.

operations
the following set of operations will be considered by our algorithm. a reversal inverts the order of the elements of a segment. additionally, the orientation of each element in the segment is inverted. a transposition cuts a segment out of a chromosome, and reinserts it at another position in the same chromosome. if we additionally apply a reversal on this segment, we speak of an inverted transposition. a fusion concatenates two chromosomes. both chromosomes πi and πj can be inverted before the operation, i.e. we can replace πi or πj by its reflection. a fission splits a chromosome into two chromosomes. a translocation splits two chromosomes πi and πj into πi, t, πi, h and πj, t, πj, h, and then concatenates πi, t with πj, h and πj, t with πi, h. again, both chromosomes can be inverted before the operation. a tandem duplication inserts an identical copy of a segment immediately after this segment in a chromosome. a transposition duplication inserts an identical copy of a segment at an arbitrary position in the genome. a deletion cuts a segment out of a chromosome. a chromosome duplication creates an identical copy of a chromosome. a chromosome deletion deletes a chromosome.

all operations have weight  <dig>  except for  transpositions and transposition duplications, which have weight  <dig>  these weights are chosen rather by mathematical than biological reasons, but still result in biologically realistic scenarios . to simplify the analysis of the effects of the operations in section "a lower bound", we will there use the double cut and join operator , which has been introduced in  <cit> . a dcj cuts a genome at two positions, and rejoins the cut ends in two new pairs. it is a well-known fact that reversals, translocations, fusions, and fission can be described by one dcj operation, while transpositions can be described by two dcj operations. duplications and deletions cannot be described by dcj operations and therefore must be examined separately.

the breakpoint graph
our main tool for developing the algorithm is the breakpoint graph, which is an edge-colored multigraph that visualizes the current adjacencies and breakpoints. it has been introduced by bafna and pevzner to solve rearrangement problems on genomes without duplicated genes  <cit> . we extend this graph such that it can also be used for genomes with duplicated genes. the breakpoint graph of two genomes ρ and π can be constructed as follows. first, we write the set of vertices {1t, 1h, 2t, 2h, ..., nt, nh} from left to right on a straight line. second, for each pair  of extremities that are no co-elements but consecutive in a chromosome of ρ, we connect the corresponding vertices by a gray edge. third, we analogously add a black edge for each pair of extremities that are consecutive in π. however, if one of the endpoints is not an endpoint of a gray edge , we do not add the black edge . for an example, see fig.  <dig>  in contrast to the original breakpoint graph, each vertex can be the endpoint of several gray and black edges. the multiplicity of an edge  is the number of black edges between v and v'. a black edge  is called a loop. let l denote the number of vertices with a loop. two vertices v, v' are in the same component of the graph if and only if there is a path  from v to v'. let c denote the number of components in the breakpoint graph of π. a black edge is called a 1-bridge if the removal of this edge increases c. a pair of black edges is called a 2-bridge if none of the edges is a 1-bridge and the removal of both edges increases c.

a lower bound
instead of searching for a sequence of operations op <dig>  ..., opkthat sorts ρ into π, one can also search for the inverse sequence  that sorts π into ρ . this is more convenient, because then the breakpoint graph has some nice properties due to the limitations in ρ. note that simply restricting π instead of ρ would not work, because the operations are directed from the restricted ancestral genome to the unrestricted descendant, i.e. we would nevertheless have to invert the operations. in the following, we will examine what effects the inverse operations have on the breakpoint graph. in the unichromosomal case, we were mainly interested in the effects on components and loops . as the breakpoint graph does not contain edges for telomeres, we additionally have to consider the amount of incorrect telomeres ), which is defined as follows.  

in other words, for each telomere xt in ρ, we count how often we must create this telomere in π such that t ≤ t  ≤ t). additionally, we add the amount of telomeres xt and xh in π that have to be removed, i.e. they are not telomeres in ρ.

lemma  <dig>  if π = ρ, then c is maximized, and l and t are minimized.

proof. if π = ρ, the set of gray edges and the set of black edges in the breakpoint graph are equal. thus, removing black edges does not increase the number of components, and adding black edges can only decrease the number of components. therefore, changing π cannot increase c. l and t are both positive numbers, and if π = ρ, they are equal to  <dig> and therefore minimized.   □

in  <cit> , we have shown that all operations can either change the number of components by  <dig>  or change the number of loops by  <dig>  these observations still hold for our slightly modified breakpoint graph. we will now examine how an operation can change t.

inverse reversal, translocation, transposition
these operations can be simulated by one or two inverse dcjs , thus it is sufficient to examine the effects of a dcj . a dcj can only change t if one of its edges is the end of a chromosome. then, a telomere xt /h is removed and a new telomere yt /h is created. this decreases t  by  <dig> or  <dig> if xt /h is not a telomere in π and yt /h is a telomere in π, otherwise the operation does not decrease t. however, in the first case, the dcj did not cut any black edge, as we neither draw black edges for telomeres in ρ nor for telomeres in π.

inverse fusion 
splitting a chromosome will only decrease t if both new telomeres are also telomeres in ρ. in this case, no black edge in the breakpoint graph is removed, i.e. c and l remain unchanged. t is decreased by at most  <dig> 

inverse fission 
concatenating two chromosomes can decrease t by at most  <dig>  this operation never removes a black edge, thus c cannot be increased and l cannot be decreased.

inverse tandem duplication
this operation does never change the set of telomeres in π, and therefore cannot change t.

inverse transposition duplication
this operation can decrease t only if the duplicated segment is at a chromosome end, and the new chromosome end  is a telomere in ρ. in this case, no black edge with multiplicity  <dig> is removed, therefore c and l remain unchanged. the decrement of t is ≤  <dig> 

inverse deletion 
this operation can only change t if we insert a segment at a chromosome end. in this case, no black edge is removed, i.e. c cannot be increased and l cannot be decreased. t is decreased by at most  <dig> 

inverse chromosome duplication
this operation can decrease t by at most  <dig> . only black edges with multiplicity ≥  <dig> are removed, thus c and l remain unchanged.

inverse chromosome deletion 
this operation can decrease t by at most  <dig> . in the breakpoint graph, no black edges are removed, i.e. c cannot be increased and l cannot be decreased.

theorem  <dig>  a lower bound lb of the distance d is  

where c = n + , and li is the number of vertices with a loop in component ci in the breakpoint graph of ρ and π.

proof. an operation that decreases t will neither increase c nor decrease l, therefore we can separate every sequence into operations that decrease t and operations that decrease . each operation decreases t by at most  <dig>  so we need at least  operations of the first kind. furthermore, if ρ = π, then c = c and therefore cl =  <dig>  as each operation decreases cl by at most one , we need at least cl operations of the second kind. therefore, any sorting sequence must have at least lb operations.   □

corollary  <dig>  lb =  <dig> 

unfortunately, there are genomes π ≠ ρ with lb =  <dig>  i.e. it is not sufficient to sort until the lower bound reaches  <dig>  we therefore have to introduce another distance measure τ. we will use the following definitions.  

for example, if ρ =  and π = , then lb =  <dig> and τ =  <dig> 

lemma  <dig>  if ρ = π, then τ =  <dig>  otherwise, τ >  <dig> 

proof. it is clear that τ =  <dig> if ρ = π. in order to minimize τ, it is necessary to minimize m and to maximize ia and ta. ia and ta are independent of π and therefore fixed. each inner adjacency is weighted by  <dig>  we can interpret this as if each of the adjacent extremities is weighted by  <dig>  therefore, we can say that each element in π can account at most  <dig> to ia + ta, and this value is reached if there is an adjacency on both sides of the element. thus, the contribution to τ of all occurrences of an element x in π is minimized if mult = mult and no extremity of x is part of any breakpoint. every additional occurrence of x may increase ia + ta by  <dig>  but also increases m by  <dig> and therefore increases τ by at least  <dig>  this means that τ is minimized if each element has the same multiplicity in ρ and π, and the breakpoint graph contains no breakpoints. this is the case if and only if ρ and π are identical.   □

the algorithm
the algorithm uses a greedy strategy to sort π into ρ by inverse operations. for better readability, we will simply write "operation" instead of "inverse operation" in this section. first, we define Δlb = lb - lb) and Δτ = τ - τ). the score σ of an operation op is defined as the tuple σ = , Δτ). the comparison operator between two scores is defined by σ >σ if Δlb > Δlb ∨ Δ = Δlb ∧ Δτ  > Δτ ). in each step, we search for operations that decrease the lower bound, and apply the one with the greatest score. if no such operation exists, we use additional heuristics to find operations that do not change the lower bound but have a positive score  >). there is still the possibility that we even do not find such an operation. in this case, we use a fallback algorithm that is guaranteed to terminate.

operations that decrease the lower bound
finding operations that increase c can be done by finding 1-bridges and 2-bridges in the breakpoint graph and verifying additional preconditions, as shown in  <cit> . the only difference is that now a dcj can cut only one black edge. this is the case when the other cutting point contains a telomere in ρ or π. thus, we also have to consider dcjs that act on a 1-bridge and a telomere. such a dcj can be interpreted as inverse reversal, translocation, fission, or transposition. in the last case, we have to find a third cutting point in the same chromosome such that the resulting inverse transposition still increases c. also finding operations that decrease l is straightforward and can be done as in  <cit> . the remaining task is to find operations that decrease t. for this, we create a list of telomeres in π that are not telomeres in ρ, and another list of inner breakpoints in π where at least one of the adjacent elements is a telomere in ρ. operations that decrease t must act on one or two points of these lists, depending on the operation type. creating the lists can be done by a linear scan over π, therefore all operations that decrease t can be found in quadratic time. the only exceptions are inverse deletions and inverse chromosome deletions, which may add segments of arbitrary content. practical tests have shown that it is better to only allow the insertion of segments without any breakpoints. this does not only lead to better sorting sequences, but also keeps the time complexity of finding the operations in o.

additional operations
if there is no operation that decreases lb, we may still find operations that do not change the lower bound but decrease τ. searching for all these operations would exceed our computing capacity, so we just search for the following subset of these operations that can be found easily.

• there are inverse tandem duplications and transposition duplications that do not change σ, but decrement τ. we therefore search for identical consecutive segments that are maximal, i.e. they cannot be extended in any direction, and check the effect on σ and τ if we remove one of them. this operation corresponds either to an inverse tandem duplication, or to an inverse transposition duplication.

• depending on the telomeres of a chromosome, the lower bound can remain unchanged during an inverse chromosome duplication, but τ can decrease. we therefore search for identical chromosomes and check the score of removing one of them.

• inserting a segment of consecutive elements x with mult >mult decreases τ. we search for such segments of maximal length and try to insert them by an inverse deletion. note that this is not always possible as this operation can increase the lower bound by merging two components.

• creating inner or telomere adjacencies never increases the lower bound, but decreases τ. we therefore search for dcjs and inverse fissions that create new adjacencies without splitting old ones.

the fallback algorithm
it is possible that there is neither an operation that decreases lb, nor an operation that decreases τ, so the main algorithm gets stuck. however, this case cannot occur if all elements have the same multiplicity in ρ and in π.

lemma  <dig>  if ρ ≠ π and mult = mult holds for all elements x, then there is an operation with positive score.

proof. when the preconditions are fulfilled, there must be at least one breakpoint in π. we have to distinguish three cases.  this is a telomere breakpoint. w.l.o.g. a chromosome in π ends with xh, but xh is not a telomere in ρ. then, mult = mult , and therefore there must be another breakpoint including t. an operation that creates an adjacency between xh and t will not decrease the lower bound, but decrease τ  by at least  <dig>   the breakpoint is an inner breakpoint between two extremities that are telomeres in ρ. in this case, the score of cutting the chromosome at this breakpoint is , because both extremities become telomeres  increases by 2), and we create two telomere adjacencies.  the breakpoint is an inner breakpoint, and at least one of the adjacent extremities is not a telomere in ρ. w.l.o.g., the breakpoint is of the form , and xh is not a telomere in ρ. then, mult = mult, thus there must be another breakpoint including t. an operation that creates an adjacency between xh and t will not increase the lower bound, but decrease τ by at least  <dig>    □

the fallback algorithm will first ensure that the precondition of the lemma holds. for each chromosome ρi in ρ, we determine the element x with the most occurrences in π. we then create maximal segments of consecutive elements  ... such that each element z in the segment belongs to ρi and mult <mult, and add this segment by an inverse deletion to π. note that this can be done without creating new breakpoints. this step is repeated until all elements belonging to the same component in ρ have the same multiplicity in π. we then transform ρ into ρ' by applying chromosome duplications and chromosome deletions on ρ such that for each element x, mult = mult. now, we apply our normal algorithm to sort π into ρ'. to ensure that the precondition of lemma  <dig> is always fulfilled, we forbid operations that create or delete elements, i.e. any kind of duplication or deletion. due to lemma  <dig>  the algorithm will find a sorting sequence that transforms π into ρ'. as last step, we have to undo the operations that transformed ρ into ρ'.

RESULTS
we tested our algorithm on artificial data and on cancer karyotypes from the "mitelman database of chromosome aberrations in cancer"  <cit> .

artificial data
we used simulated data to assess the performance of our algorithm. first, we created genomes ρ with n different elements and c different chromosomes. each chromosome has the same size, the ploidy  of the chromosomes is  <dig> or  <dig>  then, we generated the genome π by applying random sequences of operations of weight w = αn on ρ . the operations of the sequences are independently distributed, with all operations having the same probability. although these probabilities do not match the biological reality, this is still convenient to assess the performance of the algorithm. once the type of an operation was determined, the operation was drawn from a uniform distribution of all operations of this type. the genomes ρ and π were now used as input to our algorithm. the parameters n and c were chosen such that they reflect the properties of biologically meaningful datasets. to understand what "biologically meaningful" means, let us have a brief look on biological datasets. in most of them, elements do not represent single genes but synteny blocks, i.e. regions of a chromosome that are highly conserved and do not contain breakpoints. these synteny blocks normally contain several genes. the amount n of different synteny blocks depends on the allowed dissimilarity between the synteny blocks as well as on the evolutionary distance between the genomes. for example, el-mabrouk et al.  <cit>  tested their algorithm on yeast genomes with  <dig> synteny blocks, zheng et al.  <cit>  identified  <dig> synteny blocks between rice, sorghum, and maize. salse et al.  <cit>  used  <dig> synteny blocks to compare arabidopsis thaliana and rice. a recent comprehensive study of drosophila genomes  <cit>  identified between  <dig> and  <dig> synteny blocks, depending on the evolutionary distance of the species. our datasets reflect those parameters. dataset  <dig> contains  <dig> chromosomes of ploidy  <dig> with a total of  <dig> elements, this approximately matches the yeast genome. dataset  <dig> contains  <dig> chromosomes of ploidy  <dig> with a total of  <dig> elements, dataset  <dig> contains  <dig> chromosomes of ploidy  <dig> with a total of  <dig> elements. these are realistic values for plant genomes. dataset  <dig> contains  <dig> different chromosomes with a total of  <dig> elements, two of them with ploidy  <dig> and three of them with ploidy  <dig> . this reflects the values for closely related drosophila species. each dataset contains  <dig> different test cases for each generated distance w. together with the use of different distances w, this allows us to get a much more robust result than just testing on a few biological datasets. the results of our experiments are depicted in fig.  <dig>  the diagrams show that, on average, the calculated distance and the true evolutionary distance w lie close together. in many cases, the calculated scenarios were even slightly shorter than the true distance. in the fourth diagram, an additional saturation effect can be observed, i.e. we can find a sorting sequence with weight ≲ <dig> for most genomes π, independent of the true distance w.

cancer karyotypes
the "mitelman database of chromosome aberrations in cancer"  <cit>  contains the descriptions of cancer karyotypes which have been manually collected from publications over the last twenty years. for our experiments, we used the version of may  <dig>   <dig>  which contains  <dig> datasets. the data is represented in the iscn format, which can be parsed by the software tool cydas <cit> . from all datasets which could be parsed by cydas without error , we removed all unknown elements and compressed all segments without breakpoint, i.e. if a set of consecutive elements contains no breakpoint in any chromosome, it can be represented as one element. the resulting datasets were used as input to our algorithm. most of the scenarios are rather easy to reconstruct, the average lower bound is  <dig>  and the average calculated weight is  <dig> . however, there are some more complicated karyotypes, with rearrangement scenarios of over  <dig> operations. exemplarily, the reconstructed scenario for one karyotype is shown in fig.  <dig>  this karyotype was reported in  <cit> , and can be described by the iscn formula .  

in principle, our algorithm correctly identified all operations. the triple translocation t and the new chromosome +i are not allowed operations in our model. our algorithm replaced the triple translocation by two translocations, and the new chromosome by a tandem duplication with a subsequent fission, which are the best possible explanations within our model.

discussion
in the last sections, we have shown that our algorithm will terminate in any case, and finds rearrangement scenarios of reasonable quality. however, the weights of the operations are chosen due to a mathematical model and do not reflect the biological reality. this leaves room for further investigations. for example, the algorithm could be improved by giving unwanted operations a larger weight or completely omit them. while adapting the theoretical model to other weights seems to be the obvious way to improve the algorithm, it might also be worth to examine how robust the results are w.r.t. the chosen weights. in other words, does the optimal rearrangement scenario change when we use other weights? some observations in the genome can be explained at best by a specific operation , no matter how this operation is weighted. such observations are predominant in closely related genomes, and the corresponding operations can be reconstructed even with a wrong weighting scheme. in more diverged genomes, there are often different possible rearrangement scenarios, and the weighting scheme matters. thus, further investigations should examine what the "critical distance" between two genomes is, i.e. up to which distance the optimal rearrangement scenario is mostly robust w.r.t. the weighting scheme.

CONCLUSIONS
we presented an algorithm to sort multichromosomal genomes by a wide range of different operations. although our results are promising, this algorithm should be seen as a single step towards an algorithm that produces biologically reliable results. while one direction of further research should investigate the chosen weighting scheme , other possible improvements are closer lower bounds or better heuristics.

competing interests
the authors declare that they have no competing interests.

authors' contributions
mb designed the algorithm, implemented it, performed the tests, and drafted this manuscript.

