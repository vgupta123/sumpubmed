BACKGROUND
machine classifiers and feature selection algorithms have been proposed for clinical diagnosis and prediction based on favorable comparisons to competing methods  <cit> . for high-throughput biomedical data, feature selection is a necessary preprocessing or embedded step that can bias the comparison of classifiers. in an effort to compare classifiers fairly, we introduce the idea of classifier "suitability" to a particular application. every classifier is more or less suited to model particular feature relationships. for example, linear classifiers anticipate modeling features exhibiting a mean shift between classes, whereas nonlinear classifiers can model more complex corner shapes or quadratic curves  <cit> . classifier suitability depends on two key aspects:  how frequently the feature relationships it models discriminate between classes in the data and  how thoroughly we explore the feature space to find those relationships. we propose "win percentage" as the probability that a classifier will perform better than its peers on a finite random sample of feature sets.

as an analytical tool to aid in our estimation of win percentage, we design a monte carlo wrapper  algorithm for feature selection that gives each classifier equal opportunity to find informative feature sets. mcw succeeds when its best-performing feature set is among a top-performing fraction of all possible feature sets. this fraction, combined with a tolerated failure rate, defines the number of random samples that mcw must explore. we show that the most suitable classifier for an application depends on how thoroughly we explore the feature space, and apply win percentage in the analysis of eight biomedical gene expression classification problemsa  <cit> .

determining the most suited classifier to a particular problem has applications in many domains but we are most interested in the translation of machine learning algorithms for clinical diagnosis and prediction. ideally, an exhaustive search of all feature sets could identify the optimal feature set for each classifier. however, for high-throughput biomedical data many thousands of features make this infeasible and necessitate the use of feature selection methods. a multitude of computationally efficient, yet suboptimal, feature selection methods have been proposed  <cit>  but these have made comparing the resulting learning machines more difficult and potentially exclude otherwise suitable feature sets. often, the same feature selection method precedes the comparison of all classifiers using cross-validation. however, the performance of a classifier depends on the feature selection method that precedes it. one way to deal with this inherent dependency is to consider a combinatorial approach of feature selection methods and classifiers, selecting combinations of both that perform well on cross-validation  <cit> . another way is to attempt to find a feature selection method that performs well for a variety of typical datasets  <cit> . we simplify both approaches by considering a single unbiased feature selection method that gives every classifier equal chance to perform well. instead of finding a classifier that performs well for a given feature selection method, we attempt to identify classifiers that fit the problem.

feature selection methods can be categorized into filter- and wrapper-based approaches. filter-based methods rank genes based on some measure of utility such as the difference between class means . this emphasis on class means favors linear classifiers that consider the mean as the single distinguishing characteristic among classes . however, nonlinear classifiers have been shown to perform well for a variety of problems  <cit>  and deserve equal treatment when it comes to feature selection. wrapper-based feature selection attempts to find feature sets that perform well for a particular classifier using that classifier as a black box  <cit> . several heuristic wrapper-based feature selection methods are commonly used for nonlinear classifiers, such as sequential forward selection or backward elimination  <cit> . however, these suffer from a nesting structure that causes all explored feature sets to contain highly overlapping feature membership. one way to give each classifier an equal chance of finding a suitable feature set is to conduct a randomized search of the feature space using a wrapper-based approach. the classification performance of each candidate classifier determines the quality of a feature set.

randomized algorithms come in two basic varieties: those that provide the correct answer for a given input every time , and those that may give different answers to the same problem on multiple runs   <cit> . las vegas algorithms have been proposed for feature selection  <cit>  but have fallen out of favor perhaps due to the relative success of faster heuristic methods. monte carlo feature selection has been used to select features that commonly appear in different cross-validation runs  <cit> . stochastic algorithms such as simulated annealing and genetic algorithms offer a compromise in that previous results guide the search but maintain randomness to avoid local optima  <cit> .

regardless of feature selection method, the utility of a particular classifier depends not only on its performance on a carefully selected feature set but also on the difficulty in discovering that feature set. that is, depending on computational resources and time, the most suitable classifier may change. by randomly sampling feature sets, we remove classifier bias and separate the comparison of classifiers from feature selection. although this approach requires significantly more computational resources than heuristic methods, it provides a foundation for a fair comparison between classifiers.

RESULTS
first, we motivate our study by illustrating that each classifier appears to perform better than its peers for each dataset given the right feature set. therefore, the difficulty in finding the right feature set must play a role in determining the suitability of a classifier. second, we show that win percentage accomplishes this goal in a simple example, and demonstrate the correspondence between the continuous version of our win percentage and the discrete version. after demonstrating the utility of our approach using synthetic datasets from known distributions, we apply it to analyze datasets from the fda maqc-ii project  <cit> .

demonstrating the utility of each classifier for each dataset
* top performing classifier and any classifiers that do not have a significantly lower mean using a paired t-test with α =  <dig> .

an illustrative example of win percentage
to illustrate our approach, we simulate experimental data starting with gaussian conditional distributions p for three candidate classifiers with the following parameters:

  p=n,p=n,p=n,p=1/3p=1/ <dig> p=1/ <dig> 

this example shows the fundamental difference between classifiers that we are modeling; some classifiers perform well on a wide variety of feature sets, whereas other classifiers perform well on the right set of features. a fair way to compare them is to consider how thoroughly we can explore the feature space given practical computing limitations.

synthetic datasets
to explore a wide variety of probability densities, we repeat the previous example  <dig> times and compare the theoretical and discrete estimate of the win percentage in equation  <dig> and  <dig>  respectively. we varied p by drawing means from n, standard deviations from |n|, and p uniformly. the root mean square error  across  <dig> random distributions,  <dig> repeated trials of drawing m =  <dig>  samples, and  <dig> ≤ n ≤  <dig> was  <dig> %. increasing m to  <dig>  reduces the error to  <dig> %. these results show a clear correspondence between the ideal case with known continuous distributions and the more practical discrete distributions. we next consider cases where the underlying distributions are not parametric.

gene expression datasets
we apply the win percentage analysis on clinical gene expression microarray data. we constrain the feature set space to contain all gaussian feature pairs, corresponding to our gaussian candidate classifiers. evaluating all pairs for all datasets required approximately  <dig> days of computation using matlab on  <dig>  ghz servers with  <dig> gb of ram. we use a discrete distribution of p to compute win percentage.

the win percentage at n =  <dig> reveals the top classifier considering all feature sets. however, these results are not statistically significant because they are based on the performance estimate from only one "best" feature set. under the null hypothesis that all classifiers have equal chance to perform better, a repeat performance estimate would likely identify a different classifier. focusing on win percentages outside the shaded statistically insignificant region, we find significant win percentages for smaller n. if we are content with a feature set performing among the top  <dig> % of all feature sets 99% of the time, we may focus our attention on n =  <dig>  exploring feature sets at this level of thoroughness, uda performs near the top on five of the six non-control datasets.

for panels a, b, c, d, g, and h linear classifiers appear to perform better when exploring a small number of feature sets. for panels a, b, c, e, f, g, and h nonlinear classifiers perform significantly well for larger n. these data suggest nonlinear classifiers perform better when we explore the feature space more thoroughly, and linear classifiers perform better when we do not. on the other hand, the neuroblastoma data in panels e and f show that nonlinear classifiers also have significantly high win percentage for smaller values of n. the positive control in panel g shows the most striking result. lda has significantly high win percentage for n ≤  <dig>  surprisingly, the negative control has statistically significant win percentages for small n. this suggests that the null hypothesis that every classifier has equal chance to perform better than its peers on a given feature set is not true. most striking is n =  <dig>  in this case, millions of feature sets are analyzed and the null distribution would expect that every classifier perform better than its peers almost exactly 1/ <dig> of the time. this is clearly not the case, suggesting that knowing the top performing classifier for one feature set may influence our expectation for other feature sets. we revisit this discrepancy in the discussion.

insignificantly high win percentage helps eliminate some classifiers from consideration. for example for n >  <dig>  uda for dataset b; sda for dataset c; qda for dataset d; nc and sda for dataset e; nc, dlda, and sda for dataset f; dlda, sda, and for dataset g; and dlda and sda for dataset h do not show a significantly high win percentage. some classifiers clearly fail based on our significance test and may be considered unsuitable for some combinations of dataset and n.

multiple sampling of microarray data
in the previous example, we used all feature pairs to compute the exact win percentage. however, it will typically be impractical to evaluate all feature sets under consideration. now, we repeat the previous example using a finite random sample of size m from the total number of feature pairs. we repeat  <dig> trials, each time selecting m random samples from all gaussian feature pairs, and computing the win percentage based on the sample. for a given m, variance increases as n increases. figure  <dig> shows the variation in win percentage for dataset f using m =  <dig> million. the dashed lines are the same as panel f of figure  <dig> and the solid lines indicate the 95% confidence interval for the mean performance of the  <dig> trials. intuitively, when n is much larger than m, win percentage depends on a classifier's relative performance on only one  feature set. any variance in that selection transfers to win percentage. for example, some of the trials did not contain the top overall feature set resulting in confusion about the top performing classifier. the confidence interval for uda and lda reflects this by spanning the entire range. on the other hand, for smaller n, win percentage averages over many feature sets reducing the variance.

  rmse≈ <dig> ×nm <dig> . 

the last row in table  <dig> shows the predicted rmse based on equation  <dig>  in order to accomplish a rmse of less than 1%, these data suggest selecting m > 750n.

the fraction of top feature sets, p, gets smaller as n increases. for p near zero, p  ≈ -/n corresponding to the last column of the table.

discussion
the suitability of a classifier for a dataset cannot be determined after feature selection. we show that given the right feature set, any of the six classifiers examined here could be judged as suitable. however, if we consider the difficulty of finding a good feature set for a classifier we may evaluate a classifier for a dataset rather than for a particular feature set. these ideas motivate our proposed win percentage measure for comparing the relative suitability of a classifier to a dataset. however, as an initial investigation there are several points that bear consideration for future study.

we provide examples to illustrate the potential usefulness of win percentage for analyzing and comparing classifier performance. eventually, we would like to use win percentage to inform the model building process. one approach that seems promising is to use win percentage to assist practitioners in selecting or eliminating classifiers from consideration. after determining a suitable classifier, we could choose a tailored feature selection method within cross-validation to estimate its performance.

one key aspect of our approach is that we do not attempt to model the absolute performance of each classifier across the feature space. win percentage only compares classifiers and does not comment on their absolute performance. in general, we would expect these classifiers to perform near chance on the random labels. however, we observe that the mean of x appears to exceed  <dig>  on every dataset including the negative control. this bias can be explained by the selection of one best performance among the six candidate classifiers. the expected value of the largest sample among six random samples from a gaussian distribution is μ +  <dig> σ  <cit> . therefore, it is reasonable to expect the observed mean shifts. future work might incorporate whether the top classifier performs better than chance on the dataset.

in estimating the performance of each classifier for a feature set, we use two iterations of three-fold cross-validation. such a method is itself a randomized algorithm and multiple trials produce different results. in particular, our notion of "best" may be extended to include those classifiers that perform insignificantly differently from the best or "among the better" classifiers during cross-validation. this improvement would likely move all win percentage curves closer to 1/ <dig> in figure  <dig> and reduce the apparent significance of all results. in particular, it would partly address the apparent significance of the negative control  dataset.

intuitively, we would expect the negative control to exhibit win percentages that are likely to be drawn from the null distribution. for n =  <dig> in panel h of figure  <dig>  this is obviously not the case. another contributing factor could be that the feature sets are not independent of each other. if a classifier performs better than its peers on a single feature, it would stand to reason that it is more likely to perform better than its peers on all feature sets containing that feature. if this is the case, it reduces the number of independent observations used to compute the null distribution. in the extreme, the win percentage computed from individual features is also exhibited by all feature pairs. in this case, the number of independent observations is reduced from c combinations to merely f. we can easily adjust our critical win percentages by reducing m to f in equation  <dig> and using equations 12- <dig>  by doing this, none of the win percentages for the random endpoint are significant. however, estimating the actual redundancy among feature sets for an arbitrary dataset proves difficult as does adjusting m. future work could estimate the null distribution empirically by computing win percentage using multiple permutations of the class labels for each dataset. this computationally expensive approach could lend insight into the true null distribution and the effective number of independent feature sets implied by m in our theoretical null distribution.

although we focused on a pair-wise analysis of the feature space, our proposed approach easily extends to higher dimensions. whereas it is often impractical to estimate the performance of all feature triplets or quadruplets, these data suggest that sampling only 750n of these higher dimensional feature sets may be useful in comparing classifiers that explore n random feature sets. as the feature sets become larger, it may also be useful to define the probability of selecting each feature set. for example, one can favor features based on a preferred ranking criterion. whereas heuristic methods quickly find local minima, the randomness in this approach makes a more thorough exploration of the feature space possible.

CONCLUSIONS
we propose a novel way to compare classifiers based on the probability that they will outperform their peers  on a random sample of the feature sets. unlike cross-validation that estimates classifier performance using random subsets of all samples, win percentage estimates classifier suitability using random subsets of all feature sets. first, we illustrate the utility of this approach using all gaussian feature pairs. then, we show that precise estimates  can be achieved using a smaller random sample of all feature pairs.

we show that win percentage performs as expected on synthetic datasets and then apply it to real microarray data. we observe that the selection of the most suitable classifier does not only depend on the dataset but also on the thoroughness of feature selection. in addition, the results suggest that nonlinear classifiers perform better when the feature space is explored more thoroughly and linear classifiers perform better when it is not. using a theoretical null distribution, we can exclude some classifiers from consideration because their win percentage falls within a statistically insignificant region.

