BACKGROUND
proteome research requires the analysis of large-volume protein data in a high-throughput manner. mass spectrometry  is a common analytical tool in proteome research. it can be used as a technique to measure masses of proteins/peptides in complex mixtures obtained from biological samples. this provides tremendous potential to study disease proteome and to identify drug targets directly at the protein/peptide level  <cit> .

in a typical proteomic experiment, a huge volume  of ms data is often generated. each of ms spectra consists of two large vectors corresponding to mass to charge ratio  and intensity value, respectively. the first step in proteomic data analysis is to extract peptide induced signals  from raw ms spectra. peak detection is not only a feature extraction step, but also an indispensable step for subsequent protein identification, quantification and discovery of disease-related biomarkers  <cit> . however, peak detection is a challenging task since mass spectra are often corrupted by noise. as a result, various algorithms have been proposed to facilitate the identification of informative peaks that correspond to true peptide signals. these algorithms differ from each other in their principles, implementations and performance. in order to provide a comprehensive comparison of existing peak detection algorithms and extract reasonable criteria for developing new peak detection methods, we need to answer the following questions:

 <dig>  what's the working mechanism of an algorithm?

 <dig>  what are the differences and common points among different algorithms?

 <dig>  what is their performance in ms data analysis?

to address the above questions, we study the peak detection process using a common framework: smoothing, baseline correction and peak finding. such a decomposition enables us to better elucidate the fundamental principles underlying different peak detection algorithms. more importantly, it helps us to clearly identify the differences and similarities among existing peak detection algorithms.

we describe each part in the peak detection process with particular emphasis on their technical details, hoping that this can help readers implement their own peak detection algorithms.

during evaluation, we choose five typical peak detection algorithms to conduct a comparative experimental study. in the experiments, we use both simulation data and real maldi ms data for performance comparison. the results show that the continuous wavelet-based algorithm provides the best average performance.

the remainder of this paper is organized as follows: section  <dig> provides details on existing peak detection algorithms and highlights their differences and similarities; section  <dig> conducts a performance comparison on some typical peak detection algorithms using simulation data and real maldi ms data; section  <dig> concludes the paper.

methods
peak detection process
usually, peptide signals appear as local maxima  in ms spectra. however, detecting these signals still remains challenging due to the following reasons:

 some peptides with low abundance may be buried by noise, causing high false positive rate of peak detection.

 the chemical, ionization, and electronic noise often result in a decreasing curve in the background of maldi/seldi ms data, which is referred to as baseline  <cit> . the existence of baseline produces strong bias in peak detection. it is desirable to remove baseline before peak detection.

to facilitate peak detection, we often use the framework shown in figure  <dig>  it should be noted that smoothing and baseline correction may switch their locations in the pipeline. figure  <dig> gives a concrete example of peak detection by showing the result after each step of the pipeline.

categorization
existing peak detection algorithms can be categorized according to the methods used in each step of peak detection process. table  <dig> lists some popular ms data analysis methods and their peak detection functions. in this paper, we use cwt to denote massspecwavelet and lms to denote local maximum search. similarities and differences among these methods can be addressed from the table. here we would like to highlight the following:

here "s" denotes smoothing filter, "b" denotes baseline correction method, "p" denotes peak finding criterion and "-" means smoothing or baseline correction method is not used. cromwell, limpic, lms, cwt, and process are designed for single spectrum peak detection. lcms-2d, mapquant, msinspect, mzmine, openms and xcms are designed for lc-ms  data analysis. prems is a gui  package based on cromwell.

 the algorithms in table  <dig> are chosen according to three criteria:

• the software is mainly designed for ms data pre-processing.

• the software is open source.

• the software is described in a publication.

 in table  <dig>  s1-s <dig>  b1-b <dig> and p1-p <dig> denote different smoothing methods, baseline correction methods and peak finding criteria, respectively. we shall provide their details in subsequent sub-sections.

• smoothing

s1: moving average filter

s2: savitzky-golay filter

s3: gaussian filter

s4: kaiser window

s5: continuous wavelet transform

s6: discrete wavelet transform

s7: undecimated discrete wavelet transform

• baseline correction

b1: monotone minimum

b2: linear interpolation

b3: loess

b4: continuous wavelet transform

b5: moving average of minima

• peak finding criterion

p1: snr

p2: detection/intensity threshold

p3: slopes of peaks

p4: local maximum

p5: shape ratio

p6: ridge lines

p7: model-based criterion

p8: peak width

smoothing filters
these methods usually apply traditional signal processing techniques such as moving average filter, savitzky-golay filter and gaussian filter. for an input spectrum, we represent it as  with the first element as m/z vector and the second element as intensity vector . to facilitate descriptions in signal processing, we further use x to denote the continuous form of intensity vector and use x to denote the discrete form of intensity vector. here t and n serve as indexing variables. the input spectrum is always discrete. we use the continuous form to be consistent with the original description. in real applications, we usually sample the continuous filter to obtain its discrete form. we can obtain m/z values from m/z vector easily by using the corresponding indexing variable as well. a spectrum after smoothing can be expressed as y = x * w for discrete case and y = x * w for continuous case, where * denotes convolution operation. in above equations, w and w are a weight vector and a weight function, respectively. the use of different w and w will lead to different filters.

s1: moving average filter  <cit> :

the output of the moving average filter y reads:

  y=x∗w=12k+1∑i=−kkx, 

where w=12k+ <dig>  -k ≤ n ≤ k. the odd number 2k +  <dig> denotes filter width. the greater the filter width, the more intense the smoothing effect.

s2: savitzky-golay filter:

the saviztky-golay filtering can be considered as a generalized moving average filter. it performs a least squares fit of a small set of consecutive data points to a polynomial and takes the central point of the fitted polynomial curve as output.

the smoothed data point y after savitzky-golay filtering is given by the following equation:

  y=x∗w=∑i=−kkaix∑i=−kkai, 

where w=an∑i=−kkai, -k ≤ n ≤ k. here, ai controls the polynomial order. figure  <dig> shows savitzky-golay filters with different polynomial orders. for more information about ai, please refer to  <cit> .

s3: gaussian filter

after a signal x passing gaussian filter, the output reads:

  y=x∗w=∫−∞+∞xwdτ, 

where w=12πσe−t22σ <dig>  the degree of smoothing is determined by the standard deviation σ. in fact, we can view gaussian filter as a weighted moving average filter. this filter sets larger weight factors for points in the center and smaller weight factors for points away from the center. figure  <dig> shows gaussian filters with different σ.

some researchers use the second-derivative of gaussian to perform smoothing. their argument is that the second-derivative of gaussian can implicitly remove background when smoothing signals  <cit> .

s4: kaiser window

after a signal passing a kaiser window:

  y=x∗w=∑i=−∞+∞xw, 

where w=i02)i <dig>   <dig> ≤ n ≤ n. α determines the shape of the kaiser window. a large α indicates a sharp kaiser window. n denotes the width of window. i <dig> is zeroth-order modified bessel function of the first kind  <cit> . figure  <dig> shows two kaiser windows with different α values.

s <dig>  s <dig>  s7: wavelet based filters

wavelet can be grouped as continuous wavelet transform and discrete wavelet transform. the continuous wavelet transform can be written as

  y=x∗w=1|a|∫−∞+∞xψdτ, 

where w=1|a|ψ. a denotes scale and ψ denotes mother wavelet function. in continuous wavelet analysis, du et al  <cit> choose mexican hat wavelet. mexican hat wavelet reads as:

  ψ=23π1/4e−t2/ <dig>  

then w forms a scaled mexican hat wavelet. figure  <dig> shows w with different a. here, a determines the width of the wavelet. with different a, we can use w to model peaks with different width. this is especially important for low-resolution data in which peak width varies a lot. peaks with higher m/z values tend to have larger width. using fixed-window filters will not perform well in this case. discrete wavelet transform computes on scales and translations based on the power of two. figure  <dig> shows a typical method for computing discrete wavelet transform, where h is a high-pass filter and g is a low-pass filter. the procedure to compute discrete wavelet transform is as follows:

 signal is decomposed simultaneously by a low-pass filter g and a high-pass filter h.

 the output of h is then down sampled by two to generate detail coefficients and the output of g is down sampled by two to generate approximation coefficients. the coefficients obtained from the output of h are named level one coefficients.

 the output of g goes through another group of high-pass filter and low-pass filter. steps  and  go on until we obtain the last level of coefficients.

the advantage of discrete wavelet transform over continuous wavelet transform is its efficiency because it only computes on the scales and positions based on the power of two, while the redundancy of continuous wavelet transform makes the interpretation of ms peak detection easier  <cit> .

discrete wavelet transform is shift-variant. to achieve shift invariance, undecimated discrete wavelet transform has been proposed  <cit> .

baseline correction
baseline correction is typically a two-step process:  estimating the baseline and  subtracting the baseline from the signal. in the following, we list details of some commonly used baseline correction methods. since baseline substraction is straightforward, we mainly focus on the baseline estimation procedure in different methods.

b1: monotone minimum

this method includes two steps to estimate baseline. the first step is to compute the difference, which can be used to determine the slope of each point. then, this method starts from the leftmost point a in the spectrum and continues the following procedure until the rightmost point is reached:

• if the slope of a local point a is smaller than zero, a nearest point b to the right of a whose slope is larger than zero is located. all points between a and b serve as baseline between a and b.

• if the slope of a local point a is larger than zero, a nearest points b to the right of a whose intensity is smaller than a is located. the intensity of every point on the result baseline between a and b equals to the intensity of a.

• let a = b.

b2: linear interpolation

linear interpolation takes two steps to estimate baseline:

• divide the raw spectrum into small segments and use the mean, the minimum or the median of the points in each segment as the baseline point.

• generate a baseline for the raw spectrum by linearly interpolating baseline points across all small segments.

b3: loess

first, it divides the raw spectrum into small segments. then, in each small segment, it computes the quantile. after that, it estimates a predictor in every small segment for baseline estimation. the predictor in each small segment is obtained using the following rules:

• if the intensity of a point a is smaller than the quantile in the segment, then the intensity of corresponding point on predictor equals the intensity of a.

• if the intensity of a point is larger than or equal to the quantile in the segment, then the intensity of corresponding point on predictor equals the quantile.

baseline is obtained by applying local polynomial regression fitting to the predictor.

b4: continuous wavelet transform

in local regions, baselines are monotonic. baseline can be modeled as the following function:

  base = b + c, 

where c is a constant and b is an odd function  <cit> . the continuous wavelet transform of the equation reads:

  base = ∫ bψa,bdt + ∫ cψa,bdt, 

where ψa,b=1|a|ψ. because wavelet function has zero mean, the second term of equation  is

zero. if we use a symmetric wavelet function , the first item in equation  is also zero. thus, continuous wavelet transform removes baseline automatically.

b5: moving average of minima

this method uses two steps to estimate baseline:

• estimate a rough baseline by finding local minimum within a two da window for each point.

• use a moving window to smooth the rough baseline obtained in the first step.

peak finding criteria
there are many peak detection methods. most methods detect peaks after smoothing and baseline correction. however, it should be noted that there is a special case, cwt does not have explicit smoothing and baseline correction steps. du et al.  <cit>  claim that baseline can be removed if continuous wavelet transform is carried out on a raw spectrum. we have shown this fact in section baseline correction. in the following, we illustrate the criteria used by different algorithms to find similarities among different algorithms.

p1: snr

snr stands for signal to noise ratio. different methods define noise differently. below are two examples:

• noise is estimated as 95-percentage quantile of absolute continuous wavelet transform  coefficients of scale one within a local window  <cit> .

• noise is estimated as the median of the absolute deviation  of points within a window  <cit> .

p2: detection/intensity threshold

this threshold is used to filter out small peaks in flat regions. in these regions, the median of the absolute deviation  is quite small, which may result in big snr. using snr alone may identify many noisy points as peaks.

p3: slopes of peaks

this criterion uses the shape of peaks to remove false peak candidates. in order to compute the left slope and the right slope of a peak, both the left end point and the right end point of the peak need to be identified. peak candidate is discarded if both left slope and right slope are less than a threshold. the threshold is defined as half of the local noise level  <cit> .

p4: local maximum

a peak is a local maximum of n neighboring points.

p5: shape ratio

peak area is computed as the area under the curve within a small distance of a peak candidate. shape ratio is computed as the peak area divided by the maximum of all peak areas. the shape ratio of a peak must be larger than a threshold.

p6: ridge lines

ridge lines are obtained in the following steps:

• carry out continuous wavelet transform on raw spectrum. this step produces 2-d coefficient matrix with size of m × n, where m is the number of scales and n is the length of spectrum.

• connect nearest local maximal coefficients of adjacent scales to obtain ridge lines. the distance between two adjacent points on a ridge line should be smaller than a window size.

• use a variable gap to count how many successive times that a local maximal coefficient can not find its nearest counterpart in the next scale. if the gap is larger than a given threshold, the ridge line is dropped.

ridge lines are used in the following ways:

• false peaks are removed if the length of their ridge lines are smaller than a given threshold supplied by users.

• the width of a peak is proportional to the scale corresponding to the maximum amplitude on the ridge line  <cit> . a peak candidate is dropped if its width is not in a given range.

p7: model-based criterion

the application of this criterion consists of three steps:

• locate the endpoints of both sides for each peak. the left endpoint and right endpoint of a peak define its peak area.

• estimate the centroid for each peak. for m/z axis, the centroid of a peak is computed as intensity-weighted average of points within the peak area  <cit> .

• use a model function to fit peaks.

different methods choose different model functions to fit peaks. openms  <cit>  chooses asymmetric lorentzian or sech <dig> function while mapquant  <cit>  uses gaussian function to fit peaks.

p8: peak width

the two end points of a peak define its peak area. the intensities of all points within the peak area should be larger than a given noise level. a simple way to locate a peak area is to start from a point with intensity above a given noise level and move to the right until we run into a point with intensity below the noise level.

after peak end points have been identified, peak width is computed as the mass difference of right end point and left end point. the peak width should be within a given range.

RESULTS
data description and algorithm selection
in comparison, we use one group of simulation data and one group of real maldi ms data. the low resolution simulation data is downloaded from the website of m. d. anderson cancer center  <cit> . the high resolution real data is obtained from aurum data set  <cit> , which contains known purified and tryptic-digested proteins. for simulation data, the number of true peaks in a spectrum is around  <dig> on average. the m/z range is between  <dig> da and  <dig> da. the mass variation is: Δm ∈ . the median of snr is around  <dig> . for real data, the number of true peaks in a spectrum varies from  <dig> and  <dig>  the m/z range is between  <dig> da and  <dig> da. the mass variation is: Δm ∈ . the median of snr is around  <dig> . the reader is referred to additional file  <dig> for more details on the data.

software programs for lc-ms data analysis consider additional information along the lc-axis during peak detection. in order to obtain a fair comparison, here we only focus on single spectrum based peak detection algorithms. according to this criterion, only five algorithms in table  <dig> will remain: cromwell, cwt, lms, limpic and process. these algorithms are designed to analyze maldi ms data. they can also be used to analyze ms/ms data in a spectrum by spectrum manner. it should be noted that these methods are very representative as lc-ms oriented programs also use similar ideas for peak detection along the m/z axis.

evaluation criteria
in simulation data, the list of ground-truth peaks is the input before data generation. in real data, the trypsin-digested theoretical peaks  are used as the ground-truth peaks. in both cases, a detected peak is labeled as a false peak if its mass is not within the ± 1% error range of the expected m/z value. multiple peaks within the error range will be considered as one peak. we use false discovery rate  and sensitivity to measure the performance of algorithms. false discovery rate is defined as the number of falsely identified peaks divided by the total number of peaks found by algorithms. sensitivity is defined as the number of correctly identified peaks divided by the total number of true peaks. for two algorithms with the same false discovery rate, the larger the sensitivity, the better the algorithm performance.

it is difficult for two algorithms to produce the same false discovery rate. here we divide false discovery rate into small segments. such segments have clear interpretations. for example, the fdr  range reveals the algorithm's ability to recognize the most abundant  peaks in the spectrum. every time when we obtain peak lists, both false discovery rates and sensitivity are computed. we group the sensitivity together if the corresponding false discovery rates fall into the same small segment. then average values of sensitivity in the same group are computed. the average value of sensitivity is used to evaluate the performance of one algorithm in that area.

as ground truth is known for both simulation data and real data in this paper, the roc curve is probably the most informative measure for evaluation of different peak detection methods. however, the false discovery rates of wavelet-based methods are limited to a relatively small range across all possible parameter settings. on one hand, this reflects the robustness of wavelet-based methods. on the other hand, the plot of roc curve becomes difficult in wavelet-based methods. here we use the following alternative method to conduct performance comparison: we select four regions of false discovery rate:[ <dig>   <dig> ), [ <dig> ,  <dig> ), [ <dig> ,  <dig> ), [ <dig> ,  <dig> ) and compare sensitivity of different algorithms in these regions using boxplot. such strategy is capable of providing an overall performance evaluation since it is roughly a "discrete" roc curve in four regions. moreover, the boxplots illustrate the performance variances of different algorithms.

different programs have different parameters to adjust when performing peak detection. since it is very time consuming to optimize each algorithm using all potential combinations of different parameters, we mainly test combinations of parameters that are related to peak finding and use default values for other parameters. please refer to additional file  <dig> for more details.

comparison of algorithms using simulation data
the simulation data is generated using a model that incorporates some characteristics of real maldi-tof mass spectrometers: the simulation engine takes a peak list with both m/z values and intensity values as input and generates an artificial spectrum as output. the user-specified peaks are labeled as the ground-truth during data generation, while other peaks are labeled as false peaks in the simulation spectrum. in addition, the simulation engine assumes that the isotopic distribution follows the bernoulli distribution. it also includes exponential baseline curve and gaussian additive noise.

this data set has  <dig> groups of data and each group has  <dig> spectra. each spectrum has a true peak list provided by data set. we directly use these peak lists as ground truth in our experiment. we use different parameter settings to perform peak detection repeatedly on  <dig> spectra in the same group, and then compute the average value of sensitivity with corresponding false discovery rates locating in the same small region. for each algorithm, we obtain  <dig> average values of sensitivity in each small region.

comparison of algorithms using aurum data
aurum dataset is a high resolution data set, which contains spectra from  <dig> known, individually purified and trypsin-digested protein samples with an abi  <dig> maldi tof/tof mass spectrometer. in the experiments, we do not use ms/ms data and limit our analysis only to ms spectra. for each ms spectrum, we generate the ground truth peaks in silico using the following parameters: trypsin digestion with a maximum of one missed cleavage, monoisotopic peaks and single charge state. we also consider some typical ptms : carboxyamidomethyl cysteine as the fixed modification and oxidation of methionine as the variable modification. note that peptides having missed cleavages and ptms are also used to generate ground-truth peaks. after obtaining the theoretic peak list, we merge identical peaks into one peak and delete peaks whose m/z values are not in the range between  <dig> da and  <dig> da. we select  <dig> spectra, and divide the spectra into eight groups. we perform the same performance test as we did for the simulation data.

report of running time
for high-throughput data analysis, high efficiency is always desirable. in table  <dig>  we list the average running time of different algorithms on both simulation data and aurum data, respectively. we obtain the running time of different algorithms using their original software packages on the same pc. it should be noted that these programs are implemented in different languages. even without considering the implementation efficiency, complexity comparison is reasonable only for programs implemented in the same language. in this sense, table  <dig> only serves as a reference for those readers who are interested in computational cost.

parameter tuning
when the false discovery rate is 5%, half of true peaks are not detected; when 90% of true peaks are detected, many other identified peaks are noise peaks. we use the f <dig> measure to measure the performance of an algorithm by compromising between false discovery rate and sensitivity. the f <dig> measure is defined as:

  f1=2××sensitivity1−fdr+sensitivity. 

the larger f <dig> is, the better a parameter combination will be. we exhaustively try combination of parameters and count the numbers that a parameter combination provides the maximal f <dig>  the parameter combination that produces the largest number of maximal f <dig> is considered as the best combination. we also test the peak detection precision for each algorithm with its best parameter combination. for readers who are interested in parameter settings, please refer to the additional file  <dig> for more information.

CONCLUSIONS
in this paper, we provide a comprehensive survey of existing peak detection methods. in addition, we compare performance of five single spectrum based peak detection algorithms. results show that cwt provides the best performance.

the reasons that cwt provides the best performance are two-fold:

 cwt optimally characterizes the shape of peaks in mass spectra. in a real spectrum, peak width varies a lot  <cit> . hence smoothing the spectrum using fixed-window filters may fail. cwt avoids the problem by performing multi-scale smoothing.

 true peptide-related peaks are more consistent at multiple scales than false positive peaks that are mainly caused by high frequency noise. the concept of forming ridge lines in cwt effiectively removes false positive peaks.

algorithms studied in this paper mainly focus on how to identify peak positions correctly. they ignore how to compute peak abundance, which is very important in some applications . in our future work, we plan to study the issue of peak detection in lc-ms data. it will be interesting to see if additional information along the lc-axis may help to improve peak detection results.

authors' contributions
cy performed the implementations and drafted the manuscript. zh participated in the categorization of related work. wy conceived the study and finalized the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
data and results. this file lists the data used in this paper and the results for the experiments.

click here for file

 additional file 2
parameter setting. this file gives parameters settings in experiments for each program compared in this work.

click here for file

 acknowledgements
we are grateful to the anonymous reviewers for their valuable comments and suggestions, which greatly helped us improve the manuscript. this work was supported with the grf grant  <dig> from the hong kong research grant council, a research proposal competition award rpc07/ <dig> eg <dig> and a postdoctoral fellowship award from the hong kong university of science and technology.
