BACKGROUND
illumina currently represents the dominant technology in the sequencing market. therefore, a better knowledge of systematic errors in illumina sequencing data is urgently required to derive accurate and meaningful results. here, we tested and compared the most established illumina platforms: the hiseq  <dig>  the miseq and the genome analyzer ii. the genome analyzer  was the first solexa/illumina sequencing platform and launched in  <dig>  although popular for many years, it has now been outperformed by newer sequencers that offer longer reads and higher throughput. we included the platform to determine to what extent biases in the ga persist in the newer illumina technologies. the hiseq is currently one of the most popular platforms. the newest version  can produce up to  <dig> billion paired-end reads of  <dig> × <dig> bp within ≤ <dig> days  in high output mode. in rapid run mode the platform can achieve up to  <dig> million paired-end reads of  <dig> × <dig> bp in ≤ <dig> h . illumina’s benchtop sequencer, the miseq, can produce the longest reads with up to  <dig> × <dig> bp. cluster generation, sequencing and base calling takes approximately  <dig> h and results in up to  <dig> million paired-end reads.

here, we extend our work on error profiles in amplicon data sets in connection with the miseq platform  <cit>  to metagenomic data sets and included additional illumina platforms as well as more low-input library preparation methods. amplicon sequencing is an important tool to study microbial diversity and to identify the bacteria present in samples, however, 16s rrna sequencing cannot reveal the functional capacities of the organisms. metagenomics reveals information about the complete genomes of the organisms and offers insight into their functional abilities resulting in a much broader picture of the community. further, any amplification step prior to library construction is optional and not necessarily required. in contrast, amplicon sequencing always requires several cycles of polymerase chain reaction  prior to the library preparation process representing an additional source of errors and potential biases. our previous study identified library preparation and forward and reverse primer combination as the driving factors leading to distinct error patterns. for metagenomics, the dna is extracted and directly prepared for sequencing omitting the initial pcr amplification step. note however, that the library preparation method involves a limited cycle pcr amplification step for the tagmentation of the fragments.

several library preparation methods are available nowadays and the different experimental techniques potentially introduce different biases into the sequencing data. the standard illumina method for preparing sequencing libraries starts with the fragmentation of the template dna by either sonication, nebulisation or shearing. this is followed by dna repair and end polishing, plus ligation of platform specific adaptors. these adapters comprise flow cell adapters, that allow the fragments to bind to the flow cell surface, sequencing primers, required for the synthesis of the template during the sequencing, and optional indices for multiplexing. illumina’s standard truseq sample preparation kit supports this workflow and is available with  <dig> indices that can be used for single or dual indexing. for the truseq method  <dig> μg of input dna is recommended  <cit> . however, most of the input material is lost during the library preparation and the method is time-consuming and labor intensive. recently, a new technology was developed that combines these steps into one reaction. the nextera transposome technology allows simultaneous fragmentation and tagmentation by using an adapted in vitro transposition. this method requires less input dna and offers shorter preparation times  <cit> . the transposome consist of the transposase and a transposon complex with engineered transposon ends. the transposase catalyses the insertion of excised transposons into the template dna resulting in random double stranded breaks. during this process the 3’ end of the transposon strands, including a unique adapter sequence, gets attached to the 5’ end of the target dna. after the template dna is labeled at the 5’ end, a complementary tag is added to the 3’ end using a polymerase extension. platform specific sequencing adapters can additionally be added, and the sample can be enriched and bar-coded with the standard illumina indices using limited-cycle pcr. libraries can be prepared in  <dig> min and are optimised for  <dig> ng of input dna. further, the low input nexteraxt kit enables libraries prepared with only  <dig> ng of input dna.

parkinson et al. introduced another low input library preparation method promising libraries from picogram quantities  <cit>  by using a modified transposome-mediated fragmentation technique. their results indicate, that a similar coverage can be achieved with  <dig> pg compared to the coverage obtained from a standard library prepared with  <dig> μg of dna.

previous studies have shown that illumina errors are not random and that biases are likely to be related to sequence context. a general increase of errors towards the end of the reads has been observed as well as a strand bias . however, no coherent patterns were identified across these studies. dohm et al. and nakamura et al. only analysed gaii data sets. the former was an early study investigating reads of 27– <dig> bp. they found that errors are frequently preceded by g  <cit> . nakamura et al. reported sequence-specific errors related to inverted repeats and ggc sequences  <cit> . furthermore, minoche et al. observed positions with elevated error rates investigating five data sets sequenced on the ga and hiseq and an overrepresentation of g’s in close vicinity to the errors. however, they report that there was no universal short motif that co-occurred with elevated error rates in their study. they also observed elevated rates of insertions and deletions in homopolymeric regions  <cit> . the most frequent errors observed by meacham et al. was ggg to ggt  <cit> . a more recent study by allhoff et al. was based on the analysis of four data sets and the biases they identified were platform dependent  <cit> .

we conducted a comprehensive and systematic study of errors in illumina sequencing data. multiple illumina platforms were investigated, including the gaii, hiseq and miseq benchtop sequencer. we used a diverse mock community and various parameters for the library preparation including new transposome-based methods requiring only nanograms of dna. low input library preparation methods present a great advance for dna sequencing as large quantities of input material are not always available. these methods make sequencing accessible to a broader range of research areas, including clinical and environmental studies as well as forensics. we analysed the errors and biases associated with these methods to test their capabilities and compare them to the standard library preparation method. we show that there are coherent patterns for all investigated illumina sequencing platforms and that these systematic errors persist in newer sequencing platforms. previously these errors have been ascribed to secondary structure and folding effects due to inverted repeats. instead, we hypothesise that these errors are due to the engineered ddntps and polymerase which are the two key elements for any sequencing-by-synthesis method.

RESULTS
this section starts with a detailed discussion of the nucleotide- and position-specific error profiles for one hiseq data set  and a comparison of the quality scores associated with the different types of errors. the same detailed analysis was conducted for all  <dig> miseq,  <dig> hiseq and  <dig> gaii data sets . for the overall comparison of all  <dig> data sets, we also investigate the error rates with regards to the original nucleotide and substituting nucleotide. this is followed by a comparison of the motifs identified for each of the data sets. furthermore, we examine the accuracy of the quality scores and their ability to predict different types of errors. this section concludes with an outline of the capacities of different error removal approaches across platforms and library preparation methods.

detailed error and quality profiles for data set ds70
here, we present the detailed error profiles for ds <dig>  one of the hiseq data sets, where the library for the balanced mock community was prepared with the nextera kit using  <dig> ng of input dna . the substitution profiles of the metagenomic data set can be found in the additional file 1: . the graphs highlight the tendency of substitutions to cluster together.

error profiles
the overall error rates of this data set were very low: a rate of  <dig>   was recorded for the r <dig> reads and  <dig>   for the r <dig> reads. however, the errors are not evenly distributed across positions and nucleotides, creating a notable bias. the nucleotides a and c showed the lowest error rates with  <dig>   in the r <dig> reads and  <dig>   in the r <dig> reads for both nucleotides. g showed a slightly higher average error rate of  <dig>   in the r <dig> reads and  <dig>   in the r <dig> reads. t exhibited the highest average error rate with  <dig>  and  <dig>  , respectively for r <dig> and r <dig>  further, much higher error rates were observed at individual positions. for example, at read position  <dig> in the r <dig> reads substitutions of t were observed at a rate of  <dig>  . overall, error rates increased towards the end of the read and errors are twice as likely to occur in r <dig> reads. in addition, we observed a clear bias in terms of the substituting nucleotide . g seems to be preferentially incorporated if an a, c or t is sequenced and if g is sequenced a t is falsely incorporated for the majority of substitutions.

indel errors occurred at a much lower rate compared to substitutions: rates of  <dig> ·10− <dig>  for r <dig> insertions and  <dig> ·10− <dig>  for r <dig> deletions were observed. for r <dig> we observed rates of  <dig> ·10− <dig> and  <dig> ·10− <dig> , respectively for insertions and deletions. indel errors were more evenly distributed across the length of the read, with a minor increase for the last 10bp, and seemed mostly independent of the read cycle. deletions of all four nucleotides were observed at comparable rates and, similarly, insertion rates were comparable across all nucleotides with the exception of g insertions, where marginally higher rates were recorded. the insertion and deletion profiles as well as the distribution of unknown nucleotides can be found in additional file 1: figure s <dig> 

next, we analysed the quality scores for the different error types . overall, the data sets displayed very high quality scores with an average of  <dig> and  <dig> for r <dig> and r <dig>  respectively. a large part of the substitution errors were well characterised:  <dig> % of the r <dig> substitutions and  <dig> % of the r <dig> substitutions showed quality scores below  <dig>  for insertions and deletions, on the other hand, the quality scores were meaningless as the majority of indel errors were assigned a very high quality score. only  <dig> % of the r <dig> and  <dig> % of the r <dig> indel errors showed quality scores below  <dig> 

overall comparison of error and quality profiles
in the following we compare the error rates as well as biases with regards to the substituting nucleotide and nucleotide distribution across all data sets. furthermore, we examine the motifs  associated with substitution and indel errors and examine the overall reliability of quality scores.

substitution rates
the overall error rates for all data sets are displayed in fig.  <dig>  the upper two graphs compare the substitution rates between platforms and library preparation method and show the differences between the r <dig> and r <dig> reads. the genome analyzer ii  displayed the highest error rates with average substitution rates of  <dig>   for r <dig> reads and  <dig>   for r <dig> reads. the hiseq data sets showed the lowest substitution rates of all three platforms with average rates of  <dig>   for r <dig> and  <dig>   for r <dig>  the average error rates for the miseq, illumina’s benchtop sequencer, were lower than the gaii but higher than the hiseq error rates. note however, that the miseq provides substantially longer reads than both hiseq and gaii. we recorded average substitution rates in the miseq data sets of  <dig>  and  <dig>   for r <dig> and r <dig>  respectively. the bar plots show the proportion of errors associated with the four different types of original nucleotides. for both the gaii and hiseq, the highest substitution rates were observed for t and rates roughly doubled for the r <dig> reads. additionally, the rates confirm that improvements for the hiseq not only resulted in lower rates, but also in more similar substitution rates for a, c and g, however, the bias for t remains. for the miseq, r <dig> error rates were comparable for all for nucleotides. for r <dig> substitutions, higher rates were observed for a and t. . overall, the largest fluctuation in substitution error rates were recorded for the miseq.
fig.  <dig> comparison of error rates. the upper graphs indicate the proportion of substitutions of a, c, g and t for each data set, respectively. the two graphs in the middle show the proportion of inserted a, c, g and t nucleotides and the lower graphs show the proportion of deletions associated with each of the four nucleotides. data sets are grouped by sequencing platform  and library preparation method . error rates are measured as errors per base



indel rates
insertion and deletion rates  were generally very low. a sharp increase in insertions was observed for hiseq data set ds <dig>  as well as the miseq data sets that were prepared with the standard library preparation method . for this hiseq data set, two tight peaks in the position-specific insertion rates were recorded and for the two miseq data sets insertions accumulated over ≈ <dig> bp in the centre of the reads.

substituting nucleotide
in addition to recording the substituted nucleotides, we also analysed the substituting nucleotides that were falsely incorporated. additional file 1: figure s <dig> and s <dig> show the results for the r <dig> and r <dig> reads, respectively. for the gaii and hiseq, c was rarely the substituting nucleotide in the r <dig> reads. a bias towards g was recorded for both r <dig> and r <dig> for all hiseq data sets. one gaii data set  showed a high rate of g in the r <dig> reads. the nucleotides mainly affected by substitutions in this data set were t and a. for the miseq data sets a bias towards preferential incorporation of g and t was identified.

motifs
the motifs  for all data sets were recorded. the results for the r <dig> reads are displayed in fig.  <dig>  for the r <dig> reads the analogous figure can be found in the additional file 1: . we start by examining the motif-based nature of substitution errors for all three platforms. a coherent pattern for the substitution motifs was detected for all three platforms. the two most common motifs for both r <dig> and r <dig> reads for the gaii were “cgg” and “ggg”. on average, the first motif accounted for  <dig>  % and  <dig>  % of all substitutions in r <dig> and r <dig> reads, respectively, and the second motif accounted for  <dig>  % in r <dig> and  <dig>  % in r <dig> reads. for the hiseq data sets the same two motifs were identified. here, “ggg” was the first motif and “cgg” the second most common motif for every data set. the bias is more pronounced with on average  <dig>  % and  <dig>  % of all r <dig> and r <dig> substitutions, associated with the first motif. for some data sets more than  <dig> % of all r <dig> substitutions were associated with “ggg”. for the second motif on average  <dig>  % of the r <dig> and  <dig>  % of the r <dig> substitutions were preceded by this motif. the top three motifs accounted on average for  <dig>  % and  <dig>  % in the r <dig> and r <dig> reads, respectively. for the miseq data sets a strong bias associated with “ggg” and “cgg” was recorded, though more variation among the top motifs was observed. the top three motifs accounted on average for a total of  <dig>  % of the r <dig> and  <dig>  % of the r <dig> substitutions. we summarised the most common motifs for all three platforms in table  <dig>  it is notable, that all first and second motifs for all three platforms end in “gg”.
fig.  <dig> motifs and occurrence rates: the top three motifs  for r <dig>  substitutions,  insertions and  deletions are displayed on the left. the rates associated with each motif are displayed on the right. data sets are grouped by sequencing platform and library preparation method



the motifs associated with insertions showed a higher degree of variability, however  <dig> % of the top three motifs in the r <dig> reads were homopolymers and  <dig> % in the r <dig> reads. overall, “aaa” was among the top three motifs in  <dig> data sets, “ggg” was observed  <dig> times, “ttt”  <dig> times and “ccc”  <dig> times for the r <dig> reads. we observed a similar bias for the r <dig> reads with “aaa” among the top three motifs in  <dig> data sets, “ggg” in  <dig>  “ttt” in  <dig> and “ccc” in  <dig> data sets. the top three motifs accounted on average for  <dig>  %/ <dig>  %/ <dig>  % of the r <dig> insertions and  <dig>  %/ <dig>  %/ <dig>  % of the r <dig> insertions for gaii/hiseq/miseq. however, the top three motifs accounted for as much as  <dig>  % of r <dig> insertions and  <dig>  % of r <dig> insertions in some data sets.

the two most common motifs in connection with deletion errors were “aaa” and “ttt”. in  <dig> data sets “aaa” was either the first or second most common motif and in  <dig> data sets “ttt” was among the two most common motifs in the r <dig> reads. for the r <dig> reads, “ttt” and “aaa” were recorded as the two most common motifs in  <dig> data sets, respectively. the third motif showed more variation across data sets. the top three motifs accounted on average for  <dig>  %/ <dig>  %/ <dig>  % of the r <dig> insertions and  <dig>  %/ <dig>  %/ <dig>  % of the r <dig> insertions for gaii/hiseq/miseq. the maximum rate for all three motifs was  <dig>  % and  <dig>  % for r <dig> and r <dig> deletions, respectively.

the motif bias was observed regardless of the sample that was sequenced. these included two diverse mock communities as well as single organisms. however, to ensure that the observed biases are not a result of an over-representation of particular 3mers, we also computed the top three motifs while taking their frequencies into account. additional file 1: figure s <dig> shows that indeed a nonuniform distribution of the  <dig> possible 3mers is observed in the reads, with some 3mers occurring more frequently than others. additional file 1: figure s <dig> summarises the three most common motifs associated with errors taking the motif frequency based on the reads into account . for the majority of the data sets the top three motifs associated with substitutions were “ggg”, “cgg” and “agg”. for insertions and deletions a higher degree of variation was observed.

quality scoresfig.  <dig> quality scores: overview of the 75th quartile of quality scores associated with errors across all data sets. the results for the r <dig> reads are displayed on the left and the results for the r <dig> reads are on the right. data sets were grouped by library preparation method  and substitution, insertion and deletion errors are displayed separately



nucleotide distribution
for all nextera and nexteraxt libraries we observed uneven nucleotide distributions at the start of the reads. these library preparation methods rely on the transposome technology, where the transposase, is used to simultaneously fragment and tagment the template dna. for most nextera and nexteraxt data sets these fluctuations affected approximately the first  <dig> bp of the r <dig> and r <dig> reads. an example can be found in fig.  <dig> and additional file 1: figure s <dig> displaying the results for data set ds <dig>  the resemblance of the patterns for the r <dig> and r <dig> reads was particularly noticeable. for the parkinson libraries, which use an adapted version of the nextera technology, similar fluctuations were observed . here, these fluctuations affected a larger part of the start of the read . the observed fluctuations were also more pronounced. for further reference, an example for the standard truseq library preparation method is included in the additional file 1: .
fig.  <dig> nucleotide rates: comparison of occurrence rates of the four nucleotides across the reads for data set ds <dig>  the library for this data set was prepared with the nextera kit and sequenced on the hiseq



comparison of error rate removal techniques
we tested two error removal strategies for the reads: quality trimming and error correction. for quality trimming the program sickle  <cit>   with a minimum quality score of  <dig> and a minimum read length of  <dig> was used. for error correction we used the program bayeshammer which is part of the spades assembler  <cit>  . for the combination of both approaches the reads were first quality trimmed and then error corrected. figure  <dig> displays the results for the r <dig> and r <dig> reads. similar rates in error reduction were observed for r <dig> and r <dig>  averaged over all data sets, quality trimming reduced the r <dig> error rates by  <dig> %  and r <dig> error rates by  <dig> % . error correction with bayeshammer reduced the r <dig> rates by  <dig> %  and r <dig> rates by  <dig> % . the best results, on average, across all platforms were achieved by combining the two approaches: r <dig> error rates decreased by  <dig> %  and r <dig> rates decreased by  <dig> % .
fig.  <dig> comparison of error removal strategies for r <dig> and r <dig> reads: quality trimming with sickle , error correction with bayeshammer  and a combination of the two approaches  was tested on all data sets. 



further, in fig.  <dig> we compared the substitution error rates for the different library preparation methods for all platforms. the grey error bars represent the initial errors based on the raw reads. the highest error rates were encountered for the gaii, followed by slightly lower rates for the miseq and the lowest rates were observed for the hiseq. for each platform, the data sets prepared with the nextera library preparation method yielded the lowest error rates on average. the low input libraries, nexteraxt and parkinson, resulted in slightly higher error rates, however, the highest error rates were observed for the standard library preparation for the gaii and hiseq. for the miseq the highest raw error rates were associated with nexteraxt data sets. the results based on quality trimming are presented by the red bars. the greatest error reduction was observed for the standard and nexteraxt library miseq data sets. on average quality trimming worked better than error correction  except for the parkinson and standard library gaii data sets. generally, quality trimming followed by error correction  yielded the best results and the error rate showed less variability. only for the standard library miseq data sets did quality trimming without error correction result in slightly lower error rates. overall, the hiseq data sets exhibited the lowest error rates after trimming and error correction and the best results were achieved in connection with the nextera and nexteraxt library preparation method. the miseq data sets showed comparable error rates after processing the reads and the best results were also achieved with the nextera and nexteraxt library preparation method. notably, the miseq read lengths are still considerably longer after error removal . the processed gaii data sets exhibited the highest error rates where the best results were also achieved in connection with the nextera library preparation method.
fig.  <dig> comparison of error rates for different correction methods: overall substitution error rates split by sequencing platform and library preparation method. the grey bars display the error rates of the raw reads. the red bars represent the error rates after quality trimming with sickle  and the yellow bars represent the results after error correction with bayeshammer. the results of the combination of both methods are displayed by the blue bars. 



aligned reads
all error rates and calculation are based on aligned reads. figure  <dig> shows the percentage of aligned reads for all data sets and table  <dig> shows the average rates for the raw reads across all sequencing platforms and library preparation methods. overall, very good alignment rates were attained for all methods. the highest rates for each platform were obtained for the nexteraxt libraries sequenced on the hiseq and miseq and the nextera libraries sequenced on the gaii. the percentage of aligned reads slightly decreased after quality trimming and error correction, as reads may be shortened or discarded by the programs. after trimming and subsequent error correction  <dig> – <dig>  % less gaii reads aligned,  <dig> – <dig>  % less hiseq reads and  <dig> – <dig>  % less miseq reads.
fig.  <dig> comparison of aligned r1+r <dig> reads: percentage of aligned reads for the raw data sets, after quality trimming, error correction and the combination of the two approaches 



discussion
for sequencing-by-synthesis methods, such as illumina, the dna polymerase is a key element. the e. coli dna polymerase i  proteolytic  fragment was the first polymerase used for sanger sequencing and the only dna polymerase available at the time. fortunately, this polymerase permits the incorporation of chain termination dideoxynucleotides  which inhibit the dna synthesis and form another key element for this sequencing method. unlike natural dntps, the ddntps lack the 3’-hydroxyl  group that is required for the phosphodiester bond formation between the incorporating nucleotide and primer terminus. therefore, the dna polymerase terminates after the incorporation of a ddntp. different fluorescent labels are covalently attached to each of the four ddntps enabling automated dna sequencing and single tube reactions. further advances included the replacement of the 3’-hydroxyl group with a larger, cleavable chemical group facilitating the reversible termination of the dna synthesis and facilitating the current ngs sequencing-by-synthesis methods . an overview of the illumina sequencing process can be found in fig.  <dig> 
fig.  <dig> chemical formula of dntps and ddntps: a deoxynucleotides : natural nucleoside triphosphates that get incorporated during dna polymerase. b reversible dye-terminators: engineered nucleotides used for illumina sequencing-by-synthesis

fig.  <dig> overview of illumina sequencing using reversible dye-terminating ddntps: the template dna sequences including the primers are first immobilised on a solid surface. during each cycle the polymerase incorporates one reversible dye-terminator base . the synthesis is temporarily paused and the dye is excited with a laser to identify the incorporated nucleotide. all remaining ddntps are washed off and the fluorescent tag and 3’-o blocking group is removed. this is followed by another washing step before the dna polymerase recommences



changing the 3’-oh group results in a modified moiety and makes it harder for the dna polymerase to accept the engineered nucleotides. the original klenow enzyme was not capable of efficiently incorporating these modified nucleotides, creating a need for a new enzyme. sequencing information facilitated the discovery of multiple dna polymerases from mesophilic/thermophilic viruses, bacteria and archaea and greatly advanced the search for a new enzyme suitable for sequencing-by-synthesis methods.

biases associated with ddgtps
the thermus aquaticus  dna polymerase has been a commonly used enzyme for dna sequencing as the taq pol is readily purified, thermostable and can be further modified. the original enzyme incorporates ddntps much more slowly than dntps. a mutation  greatly increased the efficiency of ddntp incorporations. however, the taq pol enzyme favours the incorporation of ddgtp over the other ddntps, due to interactions between the guanidinium side chain of the arginine residue  <dig>  and o6/n <dig> atoms of the guanine base. a substitution in the arg <dig> residue with a negatively charged aspartic acid, aims at remediating this bias  <cit> . however, this is no longer achieved if the larger reversible dye-terminators are used. development of different 3’-o-blocking groups has been an active field of research. illumina/solexa developed the 3’-o-azidomehtyl 2’-deoxynucleoside triphosphates and a mutant of the archaeal 9°n dna polymerase of the hyperthermophilic thermococcus sp. 9°n- <dig> is used during sequencing. limited information is available on the exact mutations due to commercial considerations . however, for all data sets we still observed a dominant bias towards the incorporation of ddgtps.

not only preferential incorporation but also ten times faster incorporation rates for ddgtps have been reported for the taq dna polymerase  <cit> . li et al. subsequently studied the crystal structure for the different ddntps. the ddgtp ternary structure differs from the other ddntps as it possesses an additional hydrogen bond between the side chain of the arg- <dig> residue and the base of the ddgtp complex. a mutation of the arg- <dig> can reduce the incorporation rate of ddgtp and resolved the problem for sanger sequencing methods . we hypothesise that a similar bias occurs in connection with the archaeal 9°n dna polymerase resulting in higher error rates after multiple ddgtps have been incorporated which results in a bias towards motifs ending in “gg”. on average,  <dig> % of all substitution errors can be associated with only three motifs.

a non-uniform distribution of the  <dig> possible motifs was observed which is likely due to the biological significance of 3mers. codons consist of three nucleotides and correspond to a particular amino acid which form the building blocks of proteins. to confirm that the observed motif bias is not simply a result of an over-representation of particular motifs in the data, we also took the motif frequency into account. this provided further support for the motifs identified in connection with substitution errors which are the main type of errors for illumina sequencing. for insertions and deletions the top three motifs associated with these errors showed a larger degree of variation after normalising for motif frequency. this suggest that although more insertions and deletions are connected to certain motifs, that this might be due to an over-representation of these motifs in the data rather than a systematic error occurring during the sequencing process.

while some of the observed errors might be due to clonal mutations, the error profiles were independent of the sequenced input material suggesting that clonal mutations do not account for the detected motif bias. we sequenced several single organisms as well as a diverse mock community with different abundance distributions. if the error profiles were driven by clonal mutations, this would be reflected in the error distributions as these would depend on the sequenced sample. however, the motif bias was observed independent of the sequenced input material.

increased error rates towards the end of the reads and accumulation of errors
the individual error profiles confirmed an increase of the error rates towards the end of the reads, which has been previously reported and is attributed to an accumulation of phasing and pre-phasing problems during the run. the chemical and structural properties associated with the ddntps seem to contribute to this effect. after cleavage of the linker group carrying the fluoresphore, extra chemical molecules on the normal purine and pyrimidine bases remain and result in a vestige. these vestiges can perturb the dna polymerase and limited the possible read length as they impair the stability of the dna and hinders the substrate recognition and primer extension. chen et al.  <cit>  described an accumulation of these vestiges in illumina sequencing. illumina has been able to achieve longer reads by adding reversible terminator nucleotides without the fluorophore to reduce the effect of vestiges, but their impact is still apparent as increased error rates towards the end of the reads. we hypothesise that these vestiges encourage the accumulation of errors.

transposome bias
furthermore, a bias associated with the transposome-based library technologies  was recorded, confirming previous findings in the context of transposons  <cit> . the transposase used for this technology is based on a mutated tn <dig> transposome  <cit> . transposons are capable of inserting themselves into a target dna sequence. the wildtype tn <dig> enzyme has been described as inactive  <cit> , however, the mutations resulted in an increased insertion rate making the enzyme suitable for library preparations. for the wildtype tn <dig>  hot spots for insertions have been reported. the enzyme contains 19bp target recognition sites that are present at the ends of the transposase , a protein that is part of the transposon complex and responsible for the catalytic steps. the target recognition sites are required in order for the transposon to bind to the template dna for the subsequent insertion and it has been hypothesised that specific contacts must be formed between tnp and the target dna  <cit> . ason et al.  <cit>  observed high frequencies of insertions into a/t rich regions  flanked by gc pairs. as the recognition sequence of tnp contains the same subsequence , they suggest that tnp favours insertions into regions containing a portion of the recognition site. our data suggests that the mutated tn <dig> enzyme used in the nextera technology, shows a similar bias accounting for the uneven distribution at the start of the reads. the length of the fluctuations concurs with the length of the recognition site and higher rates of a/t were observed in the first part of this region followed by elevated g/c rates. however, this bias was not associated with errors and therefore these fluctuation do not need to be removed by trimming the start of the reads. it needs to be determined though if this tendency results in a coverage bias of the sequenced genomes and/or the coverage of the genomes in the community.

the nextera method has many apparent advantages: it requires less dna input material and the template dna is simultaneously fragmented and tagmented facilitating shorter preparation times. a limited-cycle pcr step is involved in the tagmentation step, therefore, higher error rates were expected for the nextera data sets. however, for all three platforms the data sets prepared with the nextera kit showed the lowest error rates .

CONCLUSIONS
metagenomics has become widely available facilitating studies of fine-scale variation. single-nucleotide polymorphisms in the human genome can cause life-threatening diseases, pathogenic and non-pathogenic bacterial strains often differ by only a few nucleotides and a small number of mutations can have disastrous effects on the virulence of an infection. in order to develop effective diagnostic and therapeutic approaches we need to be able to accurately characterise and identify systematic sequencing errors and distinguish these errors from true genetic diversity.

we showed that quality scores can characterise the majority of substitution and deletion errors for nextera, nexteraxt and the truseq library preparation method. however, quality scores are meaningless for insertions. insertion and deletion rates are  <dig> times lower than substitution rates and therefore less significant. for applications where low frequency variants are important, the motifs identified in connection with indel errors can be used as further indication for the reliability of observed snps. quality trimming  combined with subsequent error correction  provided the best results in terms of error removal. the number of aligned reads decreased on average by  <dig> % which is likely due to shortening and discarding of some reads during the different error removal strategies, however, error rates could be reduced by as much as  <dig> %. on average substitution error rates were reduced by  <dig> %.

any experimental procedure has the potential to introduce biases and errors. comparing the different library preparation methods, the best accuracy was observed for the nextera and nexteraxt methods. this technology facilitates simultaneous fragmentation and tagmentation of the dna sample, resulting in shorter preparation times. in addition less input dna is required for these methods. in connection with the proposed error removal strategy, we were able to reduce the error rates of the longer miseq reads to a level comparable to the hiseq reads. this accentuates the miseq benchtop sequencer and the nextera library preparation method as an excellent option for sequencing applications.

current methods are not designed to target systematic errors in illumina sequencing. additional file 1: figures s11–s <dig> and tables s <dig> and s <dig> show that a strong bias remains after error removal based on quality scores. in order to achieve any further error reduction novel methods that specifically target these idiosyncrasies are required and will facilitate more accurate and detailed analysis of fine-scale variation in illumina sequencing data.

