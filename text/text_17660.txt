BACKGROUND
scientific strategies and approaches based on next-generation sequencing  have been revolutionizing genetics over the last few years. many aspects of basic, applied and clinical research now rely on the generation of enormous amounts of sequence data from various sample sources, to assess polymorphism , or expression data  at the genome level  <cit> . this shift in the scale of sequence acquisition has been achieved by simultaneous progress in bioinformatics, the availability of genome assemblies and key technical findings in the domains of biochemistry and sequencing device physics  <cit> . in this context, the  <dig> gs-flx , illuminaÂ© technology  and solidtm systems  offer a number of complementary solutions for specific requirements .  <dig> gs-flx titanium technology provides around  <dig> , <dig> sequences in a single 10-hour run. these sequences, with an average read length equal to  <dig> bp, may be up to  <dig> bp in shotgun libraries conditions, much longer than can be obtained with the other available approaches. this makes mapping easier, particularly for repetitive regions, and facilitates de novo genome sequencing, exome capture, metagenomics and amplicon sequencing  <cit> .

one of the basic questions arising from this spectacular increase in sequence volume concerns the possible detrimental effects of this shift in quantity on the quality of the obtained data. in other words, is there a tradeoff between the quantity and quality of information? it is widely accepted that next-generation sequencing approaches generate such large amounts of sequence data that even if overall accuracy  or quality  is suboptimal it is still possible to reconstruct polymorphism rigorously by comparing redundant sequences that cover the same genomic region multiple times   <cit> . this is the typical "quick and dirty" view of ngs. this approach may sound reasonable, but it is based on assumptions such as low error rate and error randomness for the unambiguous detection of polymorphism. if this last assumption is challenged, even a low error rate has a huge impact on sequence analysis, as in cases of related allele detection, paralogous sequences or pseudogene identification. in these cases, the "quick and dirty" approach is inadequate, because consensus sequence calculation is accurate only if these three sources of sequence diversity are distinguishable from the error due to background noise  <cit> .

in  <dig>  s. huse and collaborators raised the question of the accuracy and quality of massively parallel pyrosequencing gs <dig> systems, performing an empirical analysis of the per-base error rate  <cit> . this was needed as "the quality score of a position is not a measure of a confidence that the correct base a called at that position, as with a traditional phred score. instead, the gs <dig> quality score is a measure of confidence that the homopolymer length at a position is correct"  <cit> . they used v <dig> hypervariable region sequences from cloned microbial ribosomal dna for this purpose. they concluded that the accuracy rate was  <dig> %, on average, and that 82% of the sequences contained no error. they also demonstrated that 39% of the errors corresponded to homopolymer effects  <cit> . finally, they detected no significant correlation between error and distance from the 5' end of the sequences for  <dig> positions. surprisingly, despite changes in this technology over the last four years, the accuracy and quality of 454-based sequences has not been reevaluated and this previous study remains the basic reference used by the scientific community to account for error rate in  <dig> gs-flx systems . over the same period, chemistry, acquisition devices  and quality filtering algorithms have evolved. a new analysis is therefore required, and this was the main goal of this work.

furthermore, in addition to estimating the per-base error rate, we aimed to identify the potential causes of sequencing errors and possible solutions for improving both the accuracy and quality of pyrosequences. we selected several variables likely to affect sequencing errors directly or indirectly:  the position of the nucleotide base within the sequence ,  the primary structure of the sequence, including, in particular, the presence of homopolymers,  the length of the sequence generated , and  the position of the bead carrying the sequence both within and between the regions on a pt plate  , and between multiple pt plates. our analyses are based on roche test fragments. these are sequences used for gs-flx titanium diagnostics that are included in all runs, but not subjected to pcr amplification before sequencing. thus with these fragments we estimate the sequencing error due to pyrosequencing. huse et al.  <cit>  found that the experimental sequences they used display error rate five times higher than the gs <dig> roche test fragments . since almost all of their results are based on experimental sequences, we cannot directly compare our results to theirs. however, we do not intend to focus on a general error rate, but rather assess the effect of several variables on error generation.

RESULTS
accuracy and quality of sequences
we assessed the quality of the sequences obtained by  <dig> gs-flx titanium sequencing, using the control dna fragment type i sequences  as reference templates . as these internal controls are added to the pyrosequencing process during the sequencing step, they are modified only by sequencing errors and are not related to any previous step. the quality of these control sequences is not influenced by the samples themselves, particularly with titanium technology, in which loading beads are isolated from each other and there should therefore be no interference from adjacent beads. we analyze here the  <dig>  sequences that passed the quality filters, representing the six control dna fragments from three  <dig> gs-flx runs. these results revealed several general trends in the sequencing error generated by  <dig> gs-flx titanium technology . it also provided detailed information about the different types of error: insertion, deletion, mismatches and ambiguous base calls. we first analyzed the error on the first  <dig> sequenced positions from the 5' end  of the control dna fragments. we compared the sequences obtained with those for the gs <dig> system and then extended the error analysis to full-length sequences .

the different types of error are detailed for each reference sequence for  <dig> sequencing. errors are classified according to the nomenclature used by huse et al. : insertions, deletions, mismatches and ambiguous base calls . error rates are given for two length categories .

the error rate for the first  <dig> sequenced positions  displayed a mean =  <dig> %   for  <dig> gs-flx titanium data. this global error rate is five times higher than the error rate obtained by the analyses of gs <dig> test fragments and is similar to that obtained from for gs <dig> experimental sequences. indeed,  <dig> % of the positions were erroneous for a comparable dataset relating to  <dig> positions . if we break down the global error rate for all reference sequences according to the type of error, insertions are found to be the most common errors , followed by deletions , mismatches , and ambiguous base calls . this pattern is entirely consistent with that described by huse et al.  <cit> . this pattern is in agreement with the study of  <dig> gs-flx  <cit>  but markedly different from illuminatm sequencing, in which insertions and deletions of single bases occur less frequently than mismatches  <cit> . in total,  <dig>  sequences  of this length were found to be free from error. this trend is similar to that reported for gs <dig> experimental sequences, for which 82% of sequences matched the corresponding reference sequence perfectly. unfortunately the data are not available for gs <dig> test fragments.

if we restricted the analysis to full-length sequences , we found for the  <dig>  sequences that passed the  <dig> quality filters  that  <dig>  bases were erroneous . the pattern observed for the first  <dig> positions was confirmed for the full-length sequence data, with insertions  and deletions  being the most common types of error and mismatches  and ambiguous base calls  making a smaller contribution to global error rate. only  <dig>  of the  <dig>  full-length sequences  had no error with respect to the corresponding reference sequence. this result strongly contrasts with the higher proportion of error-free sequences for the first  <dig> bases.

the comparison of error rates between sequences of different lengths  highlighted two key developments in addition to the doubling of the global error rate for full-length sequences as found elsewhere  <cit> . this length-associated overall increase in error rates did not reflect a common mechanism for all types of error, as insertion and deletion rates increased only slightly , whereas mismatch and ambiguous base call rates increased to a much greater extent . this decoupling of the changes in rate for different types of errors modified the contribution to global error rate of the various types of errors. thus, mismatch and ambiguous base call errors made a greater contribution to global error rate for longer sequences, although their effects remained moderate. thus, overall error rates and the rates of different types of error are not uniform for the sequences obtained by  <dig> gs-flx titanium sequencing. consequently, the conclusions drawn for short sequences should not be directly extrapolated to longer sequences, as sequence length affects error rates. another key result in this in-depth analysis of error was the finding that error rate  should be seen in the light of the large number of erroneous sequences  in the dataset. this combination of a low error rate and a large number of erroneous sequences results from the occurrence of only very small numbers of errors in individual sequences, on average. these findings conflict with those reported for gs <dig> sequencing and suggest that the removal of erroneous sequences may not be useful, to increase the overall quality.

however, the consequences of this may be relatively minor even if most sequences display errors , as the overall error rate is low, with only  <dig> % of bases being problematic. it is widely believed that deep sequencing coverage  should make it possible to correct for errors in this context  <cit> . like other types of high-throughput sequencing,  <dig> pyrosequencing is thought to be suitable for use in this context. indeed, for some applications, such as snp discovery in whole-genome sequences  <cit>  or amplicon sequencing  <cit> , an almost unlimited number of sequences may be obtained. we need to consider the number of sequences required to correct an erroneous position appropriate, at a given probability, for an error rate of  <dig> %. as detailed in additional file  <dig>  at least five sequences would be required to correct for random error at low error rates . however, an analysis of error along the length of the sequence  indicated that error rate was heterogeneous along the length of the sequence. longer sequences therefore would be subject to higher error rates at their 3' ends. the distribution of error, as illustrated in figure  <dig>  does not fit a stochastic model, for any error type. most of the positions are correct, but a few have high error rates, even exceeding 50% in some cases. there is no clear way to resolve the issue, particularly when this pattern  is repeatable between runs  <cit> .

this pattern is particularly problematic for  <dig> data, as the number of sequences significantly decreases after  <dig> bases  whatever the reference sequence considered . in summary, for the longest sequences , the combination of higher error rates along the length of the sequence, combined with the decrease in the number of sequences available, may make it difficult to correct errors. this difficulty results from a deficit in the number of sequences required to decrease the probability of erroneous assignation for a given sequence position, under a reasonable coverage threshold .

this issue is further complicated by the heterogeneous distribution of the error types among the six different control dna reference sequences, within and between gasket regions for a pt gs-flx titanium plate and also between pt plates, as initially estimated from the large standard errors  in the error estimate. this overall variability of error distribution makes it difficult to draw any clear conclusions ruling out particular parameters that might potentially influence error rates or to identify a single mechanism accounting for the observed errors in the dataset. this pattern requires an in-depth analysis of the interaction and explanatory power of various factors before we can assess the degree of sequencing error and identify solutions for preventing artifacts.

interactions between variables and error characterization
the evolution of  <dig> technology combines progress in chemistry, acquisition devices, such as ccd cameras and pt plates handling equipment, and improvements in quality filters and base-calling algorithms. all these modifications are potential sources of variation in the amount, length and quality of sequences. in this work, we analyzed the interaction of seven variables identified as potential sources of sequencing error. we characterized sequencing error as a function of information about position in the sequence , the presence of homopolymers  and reference sequence type , all considered being sequence-specific information. location on the pt plate was also taken into account through the region of origin , the distance of beads to the region center  or the plate center  as both the flow of chemicals through the plate and the central position of the ccd camera may play a role in the error generation. before this analysis, we tested the hypothesis of homogeneous error rates on the three pt plates. this hypothesis was rejected . the significant result obtained in this test is mostly due to the high power of detection associated with the large number of samples available, but this heterogeneity requires the specification of individual parameter values for the logistic model describing each pt plate. the three runs were therefore analyzed separately. this approach did not prevent us from extracting the common trends influencing error rate and distribution. the models  explained between  <dig> % and  <dig> % of the error distribution and were highly significant .

the nullity of r  between pairs of the seven variables was tested independently for each run. as the usual assumptions required to infer the distribution of the test statistics were not met, we used permutations to approximate the distribution of the test statistic under h <dig>  we used a type i error rate of  <dig>  and benjamini-hochberg correction  <cit>  to take multiple testing into account. most of the pairs of variables  were significantly correlated, using a threshold of Î± =  <dig>  in a permutation test for multiple testing. however  <dig> % of the pairs of variables correlated with  <dig>  < r <  <dig> . the pair of variables displaying the strongest correlation was the position of the error in the reference sequence  and sequence length , with  <dig>  < r <  <dig> , depending on the pt plate considered. the second strongest correlation was that between distance to the region center  and distance to the pt plate center , with  <dig>  < r <  <dig> .

the nature and significance of a correlation between two variables does not provide any information about the ability of this combination of variables to explain a third variable  <cit> . for each plate and each kind of error, we considered a logistic model  <cit>   accounting for the binary  variable in terms of the seven variables considered. for the separation of the effect of a given explicative variable from the combined effect of the other variables, we propose  breaking down each explanatory variable into three additive terms: the effect of the variable itself, the combined effect of the other variables and the rest. the combined effect of the variables ranged from 20% to 80% of the total variation in error rate . more specifically, for individual error types, the combined effect accounted for  <dig> % Â±  <dig>  of the total information for mismatch errors,  <dig> % Â±  <dig>  for ambiguous base call errors,  <dig> % Â±  <dig>  for insertion errors and  <dig> % Â±  <dig>  for deletion errors. the remaining information results from the specific effects of each variable. these high percentages of shared information highlight the high degree to which the error can be explained by combinations of variables. this may be due to partial redundancy of the information contained in each variable or the combined contribution to the total amount of error explained  <cit> . in the first case, a variable may substitute for the effect of others, whereas, in the second, only the combined information provided by each variable can account for the observed pattern. the results of correlation analysis, indicating that most regression coefficients were low, ruled out redundancy as the primary cause of the observed pattern, as most variables were independent. there is therefore no single variable consistently accounting for the distribution of sequencing error, as detailed in figure  <dig>  we investigated the main trends highlighted by the logistic model, by focusing on the distribution of sequencing error at sequence level. we then characterized the variables most strongly influencing error in terms of the location of the bead carrying the sequence, in a given region of a pt plate.

at dna sequence level, we detailed the variables individually accounting for the highest proportion of the error rate for each error type. it was essential to bear in mind, during this analysis, the fact that most of the explanatory power of these variables was obtained with combinations of variables. we analyzed each type of error independently.

for insertion errors , the variable homopolymer accounted for  <dig> % Â±  <dig>  of the variation in error on its own, and was concurrent to the error rate. this finding is consistent with available published empirical observations linking errors to homopolymers  <cit> . the variable position accounted for  <dig> % Â±  <dig>  of the variation and was also concurrent to error. in other words, the error rate due to insertions increased along the sequence. finally, the variable seq.length accounted for  <dig> % Â±  <dig>  of the variation. insertion rates were lower for longer sequences and higher for shorter sequences. these last two results may appear paradoxical, but the combined information for these variables indicates that the distribution of insertion errors along sequences is not random, with more insertions in 3' end, whatever the length of the sequence considered. this is fully explained if we considered that i) the number of sequences decreases with length , hence changing the number of sequences for which error rates are computed with respect to the reference and ii) the quality filtering process  implemented in the gs-flx system involves the trimming of reads with many off-peak signal intensities by the software. in particular, for insertions, the trimback valley filter trims sequences from the 3' end until the number of valley flows  is <  <dig> %  <cit> . this implies that short sequences are not short because the strand synthesis stops prematurely, but due to a rapid decrease in the quality of the flowgram  resulting from early out-of-phase synthesis. trimming eliminates the 3' end with above-threshold ambiguous base calls, but the remaining sequence still contains errors.

for deletion errors, seq.type accounted for  <dig> % Â±  <dig>  of the variation, reflecting substantial heterogeneity between the reference sequences. the variables homopolymer  and position  were both concurrent to the deletion rate. deletion errors tend to occur more frequently in homopolymers and their rates are higher towards the 3' end of sequences.

finally, mismatch and ambiguous base call error rates were both found to be linked to position  and seq.length , with higher error rates found in 3' positions within sequences and longer sequences tending to have lower error rates.

given this pattern, the next step in the integration of information is characterizing the effect of bead localization on error rate. in particular, it is useful to consider whether position in a particular region or on the pt plate is linked to error rate. heterogeneity in error rate as a function of bead location was found for insertions and deletions, whatever the pt plate analyzed. heterogeneity was observed at both the region and plate scales. more precisely, error rate variation was mostly accounted for by the combination of several variables but, when the distribution of insertion errors fitted a gradient following the y-axis in each region , it was not accounted for by the variable dist.region alone. however, the proportion of the model accounted for by the remaining variables is small . adding the dist.region to the model increases explanatory power to  <dig> % Â±  <dig> . the situation was similar for extraction of the signal at plate level, with dist.plate increasing the explanatory power to  <dig> % Â±  <dig> . in summary, all regions had heterogeneous insertion and deletion error rates, but there were conserved gradients along both the x and y axes. inverse physical gradients were observed for insertions and deletions. the covariation of these error types and sequence length indicates that they are influenced by a single latent variable .

CONCLUSIONS
from statistical inference to technical causes and perspectives
as detailed in the results and discussion section, error rate variability is mostly accounted for by the combination of the seven variables analyzed. however, the heterogeneous physical pattern may be partially driven by the combined influence of the central ccd camera  with chemical flow direction . this explanation is, however, insufficient in itself to account for the observed pattern, and other variables clearly influence error rate. the negative relationship between insertion and deletion errors is probably related to physical acquisition issues, but chemistry-related artifacts probably also have an effect , including the cafie effect  in particular. carry forward occurs when a trace amount of nucleotide remains in a well after the apyrase wash, perpetuating premature nucleotide incorporations for specific sequence combinations during the next base flow and contributing to signal 'noise'. incomplete extension occurs when some dna strands on a bead fail to incorporate during the appropriate base flow. the strands that fail to incorporate must await another flow cycle for sequencing to continue and are thus incorporated out-of-phase with the rest of the strands  <cit> .

this study clearly demonstrates that sequencing error rate, as deciphered here, is a heterogeneous feature in  <dig> gs-flx titanium pyrosequencing. we cannot extrapolate the results obtained for other technologies, such as the gs <dig> system, to this system, nor is the use of a single global error rate inappropriate. our results provide information about the number of sequences required to correct for a specific erroneous position, when detected, but this procedure requires the error rate to be computed from within the  <dig> pt plate regions in which the physical distribution of error rate is heterogeneous. internal dna controls should therefore be used when appropriate  <cit>  , together with an error-corrected base caller  <cit> , and routine procedures taking error data into account should be defined. when error rate is not estimated, a large number of potential false-positive polymorphisms would be expected and only post-sequencing validation can account for these artifacts  <cit> . for the resolution of this issue, the use of both sequencing primers and deep coverage, combined with the use of random sequencing priming sites, should partially compensate for error -- even for high error rates -- although it may be more difficult to distinguish between low-frequency alleles and errors than previously anticipated.

