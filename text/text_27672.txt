BACKGROUND
with the rapid advances in proteomics, mass spectroscopic serum proteomic pattern diagnostics has been appearing as a revolutionary cancer diagnostic paradigm. however, this technology still remains as an important field in clinical research study rather than a clinical routine testing  <cit> . there are many issues to be resolved to realize the routine clinical testing. asides from the issues like data reproducibility and quality control  <cit> , one essential issue prevents it going beyond clinical research study sets is that there is no robust supervised learning algorithm to classify proteomic patterns with high sensitivities and specificities. although there is an urgent need to predict cancer molecular patterns with high accuracies to support clinical decisions, it is still a challenge for oncologists and computational biologists to achieve a high-performance classification due to the special characteristics of mass spectral data.

the mass spectral data has large or even huge dimensionalities. it can be represented by a n × m matrix, each row of which represents the ion intensity values of all biological samples in investigation at a mass charge ratio ; each column of which represents the ion intensity values of a single biological sample at different m/z values. each raw data can be called a pseudo-gene since it is similar to a gene in a gene expression dataset. generally, the total number of m/z ratios is in the order of 104~ <dig> and the total number of biological samples is on the magnitude of hundreds, i.e., the number of variables is much greater than the number of biological samples. although there are a large number of m/z ratios in a mass spectral profile, only a small number of them have meaningful contributions to the data variations.

many feature selection algorithms are employed to reduce the protein expression data dimensions, remove noise, and extract meaningful features before further classification or clustering. these algorithms include two-sample t-tests, principal component analysis , independent component analysis , nonnegative matrix factorization  and their different variants  <cit> . pca may be the most employed among them for its simplicity. it projects data in an orthogonal subspace generated by the eigenvectors of the data covariance matrix. the maximum variance direction-based subspace spanning guarantees the least information loss in the feature selection. however, as a holistic feature selection algorithm, pca can only capture global features instead of local features  <cit> . the global and local features contribute to the global and local characteristics of data that are responsible for interpreting the global and local behavior of data respectively. the standard pca by nature can not extract local features. this not only leads to difficulty in interpreting each principal component  intuitively, but also causes some difficulty in achieving high-performance proteomic pattern discovery, because only the features interpreting global behavior of data are used to train a learning machine   <cit> ). since redundant global features may be involved in training, it will decrease the generalization of the learning machine and increase the risk of misclassifications or over-fitting. moreover, the global data characteristics of a cancer or normal pattern are generally similar because they follow the same protein profiling mechanism. this can be easily verified by the direct visualization of mass spectral profiles. in other words, the local data characteristics play a key role in distinguishing cancer and normal proteomic patterns.

one reason for the holistic mechanism of pca is that its data representation is not 'purely-additive'. the linear combination to calculate each pc contains both positive and negative weights. the positive and negative weights are likely to partially cancel each other in the linear combination. in fact, weights representing contributions from local features are more likely to be cancelled out because of their frequencies. this partial cancellation may directly lead to missing captures of local features for each loading vector. another reason for the global nature of pca is that it lacks some level sparse representation. each loading vector receives contributions from all input variables in the linear combination. changes in one variable will inevitably affect all loading vectors globally.

imposing nonnegativity constraints on pca can remove the partial cancellations in the linear combinations and make data representation consist of only additive components, i.e., restrict all entries of the input data and each pc as nonnegative items. adding nonnegativity on pca is also motivated by proteomic pattern discovery itself. the mass spectral profiling data is generally represented as a positive matrix naturally. it is reasonable to require its corresponding dimension-reduction data to be positive or at least nonnegative to maintain data locality in the feature selection for the sake of pattern discovery. furthermore, imposing nonnegativity constraints on pca also leads to the sparse representation of loading vectors.

in this study, we present a nonnegative principal component analysis  algorithm and propose a nonnegative principal component analysis based support vector machine algorithm  for high-performance proteomic pattern discovery. we demonstrate its algorithm superiority by comparing it with six peer classification algorithms on four benchmark mass spectral serum datasets. in addition, we present an effective biomarker discovery approach based on nonnegative principal component analysis.

this work is evolved from our previous naïve work on protein expression classification  <cit> . however, our current work has the following major advances/differences compared to the previous work.  <dig>  a robust gradient learning scheme is developed for nonnegative principal component analysis and a complete nonnegative principal component analysis based support vector machine algorithm is proposed rigorously.  <dig>  the optimal orthogonal parameter selection method is discussed and an empirical parameter choice approach is given. in addition, we also give a method to set the sparseness control parameter.  <dig>  in addition to including previous three datasets and regenerating all simulation results, we include a new dataset: colorectal data in the experiment. moreover, a new comparison algorithm: ica-svm is included in the simulation.  <dig>  a nonnegative component principal analysis based filter-wrapper biomarker discovery by employing bayesian t-test based filtering is proposed and its biomarker discovery results for the ovarian and colorectal data are analyzed and visualized.  <dig>  the major global feature selection methods are presented and the two key concepts: global and local features are defined and their impacts in classifications are discussed.  <dig>  we dropped all figures, and tables, and redundant results  from the previous work.

methods
nonnegative principal component analysis is an extension of the classic pca algorithm by imposing it with nonnegativity constraints to capture data locality in the feature selection. let x = , xi ∈ ℝd be a zero mean dataset, the nonnegative pca can be formulated as a constrained optimization problem to find maximum variance directions under nonnegative constraints as follows,   

where u = , k ≤ d is a set of nonnegative pcs. the square frobenius norm for a matrix a is defined as . the penalty parameter α ≥  <dig> controls the orthonormal degree of each loading vector. the principal component matrix u is a near-orthonormal nonnegative matrix, i.e., utu~i calculating the gradient of the objective function with respect to u, we have the learning scheme: u = u - η∇uj/||∇uj||, u ≥  <dig> where ∇uj  = xt + 4αut and η is the t time level iteration step size. we select η =  <dig> in the implementation to avoid an expensive trust region search. this is equivalent to finding the local maximum of function  under the constraints usl ≥  <dig> on the scalar level , where coefficients c <dig>  c <dig> and c <dig> are parameters to be determined in the local optimum finding. the final principal component matrix u is a collection of nonnegative roots of function f . calculating the stationary points and collecting the coefficients of usl and , we obtain the following coefficients c <dig>  c <dig> .      

the nonnegative principal component analysis complexity is o, where n is the total iterations needed to meet the algorithm termination threshold ||∇uj|| ≤ 10- <dig> in the implementation. other authors also proposed a similar approach to solve a nonlinear optimization problem induced by a nonnegative sparse pca  <cit> , where two penalty parameters were employed to control the orthonormality and sparseness of the pc matrix. however, an additional sparseness control parameter will increase the risk of algorithmic convergence difficulty with the increasing of the parameter values  <cit> .

we propose a nonnegative principal component analysis based classification algorithm to achieve the high-performance proteomic pattern prediction. the algorithm employs nonnegative principal component analysis to obtain the nonnegative representation of each sample in a low-dimensional, purely-additive subspace spanned by meta-variables. a meta-variable is a linear combination of the intensity values of the pseudo-genes in a mass spectral profile. the nonnegative representation for each sample is denoted as a meta-sample, which is the locality-preserved prototype of the original biological sample with low dimensionalities. then, a classification algorithm, which is chosen as a support vector machine algorithm   <cit>  in this study, is applied to the meta-samples to gain classification information. given a protein expression training dataset consisting of d biological samples across n pseudo-genes and their label information: , where x = t xi ∈ ℝn and c = t, ci ∈ {- <dig>  1}, the npca-svm algorithm finds the meta-samples u = t, u ∈ ℝd×k, k ≤ d ≪ n, by the described steepest descent method. then, an optimal separating hyperplane oh: wtu + b =  <dig> in ℝd is computed to attain the maximum margin between the '-1' and '1' types of the meta-samples. this is equivalent to solving the following quadratic programming problem in ℝd,   

given an unknown type sample x' ∈ ℝn, the npca-svm learning machine employs the following decision rule to determine its class type: , where ui, u' ∈ ℝd are the meta-samples of samples xi, x' computed from nonnegative principal component analysis respectively. the vector α =  ≥  <dig> is the solution of the dual problem of the qp in eq.  and k is a kernel function for the support vector machine, which maps these meta-samples into a same-dimensional or high-dimensional feature space. we only focus on the linear and 'rbf' kernels for their popularity  <cit> .

we employ a sparse-coding approach to improve the sparseness for each meta-sample. the sparseness of a nonnegative vector v = t, vi ≥  <dig>  i =  <dig>  2⋯n, is defined as a ratio between  <dig> and 1:  according to the relationship of two norms  <cit> . a large sparseness δv indicates less number of positive entries in the vector v extreme cases δv =  <dig> or δv =  <dig> indicate that there is only one entry or all entries are equal in v respectively. the sparse coding of a meta-sample , i =  <dig>  2⋯d, k ≤ d ≪ n seeks to find a nonnegative vector v ∈ ℝ1×k such that , and δv achieving a specified sparseness value. in other words, for each loading vector  in the nonnegative pc matrix, the nearest nonnegative vector v on behalf of l <dig> and l <dig> distances is found to achieve a specified sparseness δv. it is equivalent to calculating the nonnegative intersection point between a hyperplane  and a hypersphere  such that the sparseness degree . since the traditional approach to find the optimal α is computationally expensive  <cit> , we select α ∝ d in practice because of ||utu|| = d <dig> in the extreme case where u is the identity matrix, if there is no further sparse coding applied to loading vectors. otherwise, we select . also because data sparseness is a by-product of the nonnegativity constraints in eq. , we usually select the sparseness degree for each nonnegative principal component as δv ≤  <dig> .

we implement the npca-svm algorithm under the  <dig> trials of 50% holdout cross validations , i.e.,  <dig> sets of training and testing data are generated randomly for each dataset. the final classification rate, sensitivity and specificity are the average values of these measures among the  <dig> trials of classifications. to improve computing efficiency, the pc matrix u in the nonnegative principal component analysis is cached from the previous trial and used as the initial point to compute the next principal component matrix in the computation.

RESULTS
four serum proteomic datasets: ovarian, ovarian-qaqc , liver and colorectal are included in this study  <cit> , which are generated from three different profiling technologies. table  <dig> provides the detailed information about the datasets.

we conducted the following preprocessing for each dataset: baseline correction, smoothing, normalization, peak identification and peak calibration by using matlab bioinformatics toolbox  <dig> . in addition, we applied the standard two-sample t-test to select  <dig>   <dig>   <dig> and  <dig> most significant pseudo-genes for the ovarian, ovarian-qaqc, liver, and colorectal data respectively before further classifications. the goal of this basic feature selection is to select approximately  <dig> × d most significant features for each input dataset x ∈ ℝd×n before classification. we compared the nonnegative principal component analysis based support vector machine algorithm with the six peers: k-nn, svm, pca-svm, nmf-svm, ica-svm and pca-lda algorithms in terms of average classification rates, sensitivities, and specificities under  <dig> trials of 50% hocv. detailed information about the algorithms: lda, nmf and ica algorithms can be found in  <cit> . in the npca-svm algorithm, we set the orthonormal control α =  <dig>  the sparseness for each loading vector δv =  <dig> , and k = d -  <dig> in the npca feature selection due to n ≫ d

we showed the average performance of the seven algorithms in terms of average classification rates, sensitivities, specificities, and their corresponding standard deviations in table  <dig>  we did not include performance of the svm, pca-svm, ica-svm and nmf-svm algorithms under the 'rbf' kernel, because the first three encountered over-fitting and the last had lower performance under the 'rbf' kernel than the linear kernel. we had the following observations from these results. 1) the npca-svm algorithm achieved obviously leading advantages over the others. its average specificities for the two ovarian cancer datasets reached 99%+ that was the population screening requirement ratio in the clinical diagnostics. it also achieved  <dig> % average specificity for the colorectal data and  <dig> % average sensitivity for the liver data. it was the only algorithm among the seven algorithms that achieved consistently leading performances for all datasets. 2) there was no over-fitting associated with the npca-svm algorithm under the 'rbf' kernel. alternatively, it achieved exceptional sensitivities and specificities under this kernel. 3) the conventional feature selection algorithms pca, nmf and ica generally did not contribute to the improvements of svm classifications.

classifying rate
sensitivity
specificity
biomarker discovery by nonnegative principal component analysis
in this section, we presented a nonnegative principal component analysis based filter-wrapper biomarker capturing algorithm. the bayesian two-sample t-test  <cit>  and nonnegative principal component analysis functioned as filters and a svm classifier worked as a wrapper in this algorithm. unlike other peak-selection based biomarker capturing methods  <cit> , our algorithm could identify which pseudo-genes were more effective in predicting cancer patterns. the npca-based biomarker discovery algorithm can be described as follows. for an input mass spectral data x ∈ ℝn×m with m pseudo-genes and n biological samples, we first filter a potential biomarker set sb by conducting the two-sample bayesian t-test, which is a novel approach to evaluate each pseudo-gene according to their differentially expressed levels. the potential biomarker set sb consists of significantly differentially-expressed pseudo-genes. for each dataset, we select at least the top 1% pseudo-genes with the smallest bayesian factors, i.e., |sb| = ⌈m ×  <dig> ⌉ to construct sb. then, nonnegative principal component analysis  is employed to decompose the input data: xt~put for each pseudo-gene, a coefficient τ is used to rank its contribution to all pcs. for example, the coefficient for the ith pseudo-gene is calculated as the weighted sum of the ith row in the nonnegative p matrix: , where  is the ratio of variance explained in the jth pc among the total data variance. a large coefficient value of a pseudo-gene indicates it has significant contributions to the pcs.

each pseudo-gene in sb is used to train a svm classifier under the leave-one-out cross validation . the first biomarker g <dig> is selected as the pseudo-gene with the highest accuracy. if there is more than one candidate, the pseudo-gene with the largest coefficient in npca-ranking will be selected. the potential biomarker set is updated by removing the selected biomarker, i.e, sb = sb - {g1}. the second biomarker g <dig> is selected from the current sb such that the svm classifier reaches its maximum classification rate for the combination of g <dig> and g <dig>  if there is more than one candidate, the pseudo-gene with the largest coefficient in the npca-ranking will be chosen as g <dig>  similarly, sb is updated as sb = sb - {g2}. such a proceeding continues until the svm classifier achieves the maximum classification accuracy with the fewest biomarkers.

we applied the nonnegative principal component analysis based biomarker capturing algorithm to the colorectal dataset. the potential biomarker set sb was initialized by  <dig> pseudo-genes with the smallest bayes factors. the alpha value in npca was set as α =  <dig> to maintain consistency with the previous classification setting. table  <dig> shows the information about three biomarkers discovered for the colorectal data. the total svm accuracy under the three biomarkers was  <dig> % and the corresponding sensitivity and specificity were  <dig> % and 100% respectively, which was better than the biomarker discovery results obtained in  <cit> . it was interesting that these biomarkers were not peaks with very large intensity values. the similar results can also be obtained by running the biomarker capture algorithms under the 'rbf' kernel. the final svm accuracy also reached  <dig> % with three biomarkers at  <dig> ,  <dig>  and  <dig>  da. interestingly, the biomarkers from different kernels not only shared a same pseudo-gene at  <dig>  da, but also demonstrated a spatial coherence, i.e., they were neighbors close or very close to each other among  <dig> m/z ratios in the data. it indicated that m/z ratios in the downstream interval 960- <dig> da may be more sensitive in discovering cancer patterns than others. figure  <dig> visualizes all samples of the colorectal data by using the three biomarkers found under the linear kernel. it is clear that the  <dig> samples are partitioned into two groups:  <dig> cancers and  <dig> controls, and the two types of samples showed significantly different mean and variance values.

similarly, we applied this algorithm to the ovarian data and obtained 100% predication accuracy  from four biomarkers at m/z ratios:  under the linear kernel. moreover, the svm classifier also achieved  <dig> % accuracy, 100% sensitivity, and  <dig> % specificity under the 'rbf' kernel from three biomarkers at m/z ratios: . also similar to the previous case, the biomarkers discovered under different kernels illustrated spatial proximity and shared same pseudo-genes. figure  <dig> visualizes all  <dig> samples by using the three biomarkers obtained from the 'rbf' kernel. it was also obviously that cancer and control samples were separated clearly by the three biomarkers.

discussion
although nonnegative principal component analysis has overcome the global nature of the standard pca algorithm, and contributed to the high-performance proteomic pattern prediction and effective biomarker capture, it is an expensive algorithm with a high complexity o compared to the classic pca algorithm o for an input data x ∈ ℝd×n, d ≪ n. it may require some basic feature selection preprocessing such as the two-sample t-test to avoid a large computing burden for a high-dimensional dataset. on the other hand, since the final pc matrix in nonnegative principal component analysis is computed through a fixed instead of an optimal step size in the iteration, it may miss some local optimal solutions and lead to potential convergence problems. in the following work, we plan to improve nonnegative principal component analysis  in the following aspects.  we plan to employ the wavelet based multi-resolution approach to overcome the high algorithm complexity in npca. a wavelet transform is first employed to decompose an input data into a multi-resolution form. the nonnegative principal component analysis  is then employed to extract the local data features from the fine level wavelet transform coefficients, which are relatively low dimensional data compared with the input protein expression data.  we will employ a projected-gradient algorithm  <cit>  with a dynamic step size to improve the nonnegative principal component analysis algorithm convergence. as a local feature selection algorithm, nonnegative principal component analysis can be integrated with other state-of-the-art classification and clustering algorithms to develop a family of statistical learning algorithms. for instance, we are interested in combining it with the linear programming svm algorithm  <cit>  to further explore its potentials in proteomic data pattern prediction. moreover, we will continue to investigate the applications of the npca-svm algorithms in snp, cgh array data analysis, and other related topics in future work, in addition to integrating the sparse-coding in our previous npca-svm algorithm developed for gene expression profiles  <cit> .

CONCLUSIONS
in this work, we developed a novel feature selection algorithm, nonnegative principal component analysis, and proposed the nonnegative principal component analysis based support vector machine algorithm with sparse coding for high performance proteomic pattern discovery. we demonstrated the superiority of this algorithm by comparing it with other six peer algorithms on four proteomic datasets. in addition, we have designed a npca-based filter-wrapper biomarker capturing algorithm and applied it to effectively capture meaningful biomarkers for the colorectal and ovarian data. our analysis suggests that nonnegative principal component analysis has advantages over the conventional feature selection algorithm such as pca, ica, and nmf in local feature selections. although its algorithmic complexity is higher than that of widely used pca algorithm, its nature of local feature selection contributes to the high-performance serum proteomic pattern classification and meaningful biomarker discovery.

competing interests
the author declares that they have no competing interests.

authors' contributions
hey did all the work for this paper.

