BACKGROUND
the majority of neuroscience experiments include some type of inferential statistical analysis, where conclusions are reached based on the distance between the observed results from some hypothetical expected value. discovering how the brain and nervous system work requires the proper application of statistical methods, and inappropriate analyses can lead to incorrect inferences, which in turn leads to wasted resources, biases in the literature, fruitless explorations of non-existent phenomena, distraction from more important questions, and perhaps worst of all, ineffectual therapies that are advanced to clinical trials  <cit> . pseudoreplication is a particularly serious error of analysis that has not received much attention in the neuroscience literature, and which hurlbert defined over twenty years ago as the "... use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated  or replicates are not statistically independent"  <cit> . put simply, it is a confusion between the number of data points with the number of independent samples, and can be illustrated with the following example. suppose the following information was provided in the methods section of a manuscript: "ten rats were randomly assigned to either the treatment or the control group, and performance on the rotarod  was tested on all the rats on three consecutive days. differences between groups were assessed with a two-tailed independent samples t-test, with p <  <dig>  considered statistically significant." then in the results section the authors report that "the treatment group did significantly better than the control group ." have the authors analysed the data correctly? no. with a sample size of ten rats, there should only be eight degrees of freedom  associated with this statistical test. the concept of degrees of freedom is perhaps not the most intuitive statistical idea, but it can be thought of as the number of independent data points that can be used to estimate population parameters , and whenever something is estimated from the data, a degree of freedom is lost. therefore the total df is equal to the sample size only if all the samples are independent: measuring the height of ten unrelated individuals provides ten independent pieces of information about the average height of the population from which they were drawn; measuring the height of one person ten times provides only one independent piece of information about the population. in the above rat example, the three observations from each rat  were treated as independent samples, and hence the  <dig> degrees of freedom arose from thinking that the fifteen data points in each group contain independent information about the effect of the treatment . incidentally, the correct analysis with a t-statistic of  <dig>  on  <dig> df has a corresponding p-value of  <dig> . in addition to the incorrect degrees of freedom, there is also the problem of false precision, which is discussed at greater length below . however, it should be noted here that the df problem has greater relevance when sample sizes are small, but false precision is arguably of greater concern in general.

the assumption of independence means that observations within each group or treatment combination are independent of each other. an alternative way of expressing this concept is to say that the errors  are independent, once the effects of all the other explanatory variables have been taken into account. in addition, other variables that are not included in the analysis  must not influence the outcome or be correlated with the residuals. the remainder of the introduction will define some commonly used terms, illustrate why pseudoreplication is problematic, and finally, discuss the four situations in which it can arise.

the terms sample, replicate, observation, experimental unit, n, and experiment have overlapping meanings, are often used interchangeably, and can have different meanings based on the context. an experimental unit is defined as the smallest entity that can be randomly assigned to a different treatment condition  <cit> . a person or a rat are typical experimental units, because they can be allocated to different treatments. the sample size is usually reported as the "n" and is defined as the number of experimental units, but the term is slightly ambiguous because one could take two blood samples from a rat  and therefore there are twice as many samples as rats, but the "sample size" still refers to the number of rats. an observation occurs whenever a value of an outcome variable is recorded, and it is equivalent to the number of data points; if there are twenty rats and only one observation is taken on each rat, then the number of observations equals the sample size . if multiple observations are taken from each rat, then observations within each rat are not independent and therefore all of the observations cannot be summed to give a total sample size. in cell culture experiments, the whole procedure is often repeated three or more times and reported as three "independent replicate experiments". in this case n is the number of experiments. the term experiment is ambiguous in this context because all of the independent trials or runs taken together can be thought of as "the experiment". replicates also typically refers to independent observations, and hence the term pseudoreplication when this is not the case. however, cumming et al. use replicates to refer to "repetition of measurements on one individual in a single condition, or multiple measurements of the same or identical samples", and thus they use the term replicate to refer to observations that are not independent  <cit> . the difference here is between biological replicates which are independent  and technical replicates which are not independent . in this paper, replicates refers to biological replicates unless otherwise indicated; references to samples or sample size  refer to the number of independent values, and observations are used to refer to individual data points, which most likely are not independent . the term pseudoreplication is used synonymously with lack of independence of observations, correlated observations, and correlated errors.

pseudoreplication leads to the wrong hypothesis being tested and false precision
ignoring lack of independence leads to two major problems. the first is that the statistical analysis is not testing the research hypothesis that the scientist intends, in other words, the incorrect hypothesis is being tested. this is illustrated in figure  <dig>  where two rats are sampled from a population, and the interest is in determining whether the rats come from a population with a mean of  <dig> on some arbitrary outcome variable , or is their value far enough away from  <dig> that we conclude that they come from a different population. this can be stated as h0: μ =  <dig> , and h0: μ ≠  <dig> . ten measurements are made on each rat and a one-sample t-test can be used to compare the mean of this single sample of rats to a hypothesised population value. the incorrect analysis would give t <dig> = - <dig> , a p-value of  <dig>  × 10- <dig>  and a 95% confidence interval  from  <dig>  to  <dig> . the correct analysis would give t <dig> = - <dig> , p =  <dig> , and 95% ci = . the change in p-value between the two analyses is six orders of magnitude, which demonstrates the importance of dealing with pseudoreplication appropriately. when calculating standard errors and confidence intervals, and making inferences between different groups with statistical tests, the assumption is that all the values are independently drawn from the parent population, but clearly the rat that the observation came from partly determines what that value is. statistical analyses performed on such data without regard for this structure are often meaningless . the incorrect 95% confidence interval does not include the true population mean, while the correct 95% ci spans the whole distribution . multiple observations on each rat provides increased precision for estimating the true mean for that rat, but does not directly provide increased precision for estimating the population mean in the way that increasing the number of rats does. as the number of samples within each rat increases, the incorrect error bar in figure  <dig> will get increasingly narrower, while the correct error bar will remain the same—as it should, because no new information about the population of rats will be obtained by further sampling of these two rats. this idea is also extended to cases where there is more than one experimental group or condition; it is necessary to distinguish between those measurements that are independent samples from the population and which increase precision and decrease uncertainty about the population parameters , and those measurements that only increase the precision of the value for a particular subject.

the second problem that arises is that correlations between observations can lead to calculated p-values that are either higher or lower than the true p-value. for the above example, "correlation between observations" refers to the degree of similarity of the observations within each rat, relative to the observations between rats. this is called the intraclass correlation  and is expressed as a ratio of variances. we can model the data in figure  <dig> as  

where yij are the values of the response, i is an index indicating the rat that the observation comes from , and j is an index for the observation within each rat . the grand mean  is denoted by μ, αi is the amount by which the mean of each rat is above or below the grand mean, and εij are the residuals, which is the distance of each of the  <dig> values from the mean of their respective rat. the intraclass correlation can then be calculated as  

where  is the variance of the means of the rats about the grand mean, and  is the variance of the residuals . the variability in the data is therefore partitioned into the variability between rats () and the variability within rats (). as can be seen from the above equation, as  gets large, ic approaches zero, and when all the observations within each rat are identical , ic approaches one. the ic can thus be interpreted in a similar manner to the pearson correlation, but restricted to positive values. for the above rat example,  =  <dig>  and  =  <dig>  giving ic =  <dig> , which indicates that the observations within each rat are highly correlated.

a detailed analysis by scariano and davenport showed that both the type i  and type ii  error probabilities can be affected by within group correlations  <cit> . when there is a positive within group correlation , the type i error probability  will be greater than  <dig> , and the greater the correlation the greater the number of false positives. for example, a two independent group comparison with n =  <dig> in each group and with a modest within group correlation of ic =  <dig>  would give an α probability of  <dig> ; in other words, 37% of the time  the null hypotheses would be  rejected. thus when there is a positive correlation, null hypotheses will be rejected too often, and this is the reason that violating the independence assumption can be more serious than violating the normality or equal variances assumption  <cit> . the four situations in which pseudoreplication can arise are discussed next and summarised in table  <dig> 

repeated measurements on the same experimental unit
a common situation is when observations are taken at different times or under different experimental conditions on the same subjects, and this is usually a planned part of the experimental design. data of this type are typically analysed with a paired-samples t-test if there are only two conditions or time points, or a repeated measures  analysis of variance  if there are more than two time points. there are a number of advantages of such designs, including a reduction in the number of animals or participants used, and increased statistical power because subjects act as their own control. the important distinction is that observations from different subjects are independent of each other, but not the observations within each subject. these data are often analysed correctly , possibly because undergraduate statistics courses for biologists usually cover the difference between "within subjects" and "between subjects" designs.

data with a hierarchical structure
a second common design where pseudoreplication can occur is when data are hierarchically organised. biological data are often sampled at different spatial scales or levels of biological organisation. for example, several brains may be sliced into sections, and a number of regions on a section may be examined histologically , and perhaps only a certain number of cells within each region would be examined. thus there is a hierarchy, with the whole brain  at the top, sections within a brain, regions within a section, and cells within a region . if cells are the unit of interest, then typically many cells are examined per brain. consider an experiment with two experimental conditions , with one rat in each condition. the outcome variable is the number of synapses on cells in the ca <dig> region of the hippocampus, and  <dig> cells are examined in each rat. this would give  <dig> rats ×  <dig> cells per rat =  <dig> data points. the incorrect way to analyse this data is with a t-test with an n of  <dig> . this is incorrect because differences due to the treatment are completely confounded with natural animal-to-animal differences between the two rats. the standard deviation , in the denominator of the t-statistic is meant to represent the variability between rats, not within rats. furthermore, the standard error  is a measure of the uncertainty associated with the means of the population of rats, not the populations of cells within rats. the n in equation  <dig> must therefore represent the number of independent observations, which in this case is the number of rats, not the number of cells   

where  is the group mean. cells within rats will tend to be more similar than cells between rats and therefore are not independent of each other. including all of the  <dig> data points in the analysis as if they were independent gives a false estimate of the precision  because t gets big as  gets big. two rats will never be exactly the same and therefore it is simply a matter of taking enough measurements on two rats to show that they are statistically different. this point generalises to experiments with more than two groups and more than one factor. if the experiment had used two rats in each experimental condition and  <dig> cells were observed in each rat, there would still be  <dig> data points  in total, but the same problem remains, although the treatment effect is not completely confounded with the inter-rat variability.

another common case of hierarchically structured data is when multiple animals are born in a litter. animals within a litter are not independent because they share the same parents and the same prenatal and early postnatal environment, and animals are therefore nested within litters  <cit> . laboratory animals are often highly inbred and genetically identical , but epigenetic and developmental factors may play a role, and two rats from the same litter are likely to be more similar than two rats from two different litters, and litter effects have been found on a variety of outcome variables, including life span  <cit> , body weight  <cit> , total brain volume , behavioural tests , and plasma concentrations of various substances . it is likely that litter effects are present in many response variables, but few papers mention how these were dealt with in the experimental design stage, or whether the data were examined for the presence of litter effects. if all animals in the control condition are from one litter while all the animals in the treatment condition are from another litter, then the treatment effects will be completely confounded with litter effects, making it difficult to attribute differences between conditions to the effect of the treatment.

other examples include applying treatments to cages of rats rather than individual rats , or applying treatments to pregnant females but examining the effect in the offspring. here, cage and pregnant females are the experimental units, and not the individual animals since the treatments can only be applied to whole cages and pregnant females and not to the individuals animals. this type of experimental design is often referred to as a split-plot design and is characterised by the restrictions on randomisation; it needs to be distinguished from a design where individual rats can be randomised to different conditions. in addition, cells in the same flask or well of a cell-culture experiment are not independent; they will tend to be more similar than cells in different flasks or wells and will be subject to the same uncontrolled effects.

observations correlated in space
observations may be correlated in space because multiple measurements taken at one location will all be affected by the idiosyncratic aspects of that location. for example, 96-well plates often contain small amounts of fluid, and wells near the edges of the plate may evaporate faster than wells in the centre, and thus alter the concentration of substances such as metabolites, secreted hormones, etc. placing the control samples in the first column of the plate and the treated samples in the second column would therefore not be a good idea. this is also the reason why microarrays have replicate probes for the same gene scattered throughout the array and not placed beside each other, as this accounts for any spatial effects in the quality of the array that may have arisen during manufacturing or handling. spatial dependence may also arise in incubators for culturing cells. a large cell culture experiment may use two incubators, but differences particular to each incubator may affect the outcome variable. for example, the temperature and humidity levels may be different, or these variables may fluctuate more in one incubator than another, perhaps because one may be used more and thus the door is opened more often as people access their samples. good experimental design would dictate that the treated samples are not placed in one incubator while the control samples are in the other, as it would be impossible to separate the effect of the treatment from the effect of the incubator.

observations correlated in time
unlike repeated measurements on the same samples, observations that are correlated in time are often not a planned feature of the experimental design, but arise from the sampling protocol, the phenomenon under investigation, or the way in which the experiment is conducted. in addition, observations need not be on the same subject. for example, rats have a circadian rhythm in the stress hormone corticosterone, which peaks at the beginning of the dark  phase, and gradually decreases throughout the night  <cit> . suppose that plasma corticosterone concentration is the main outcome variable and blood samples from twenty rats need to be taken. if the sampling starts at the beginning of the dark phase  and takes  <dig> hours to complete, there might be an overall decrease in corticosterone concentration in rats that were sampled at later time points compared to earlier ones. this could confound the results if the first ten rats were the control rats and the next ten were in the treatment group, as it would be difficult to distinguish treatment effects from circadian effects. it would therefore be better to alternate rats from each group when sampling the blood. a circadian effect would not be eliminated, but it could now be taken into account by including time or sample number in the model, which would not be possible if treatment is confounded with time. one example of such a time-dependence between sample number and the main outcome variable is discussed in reference  <cit> .

methods
the proportion of papers that had pseudoreplication in a large number of journals was not quantified because the majority of papers do not provide sufficient information for this to be assessed  <cit> . in addition, the purpose of this paper is not to determine the prevalence of pseudoreplication in the neuroscientific literature but to  bring the problem to the attention of the neuroscience community,  demonstrate the variety of forms it can take,  show how to detect instances of it in publications, and  provide alternative analytical methods for dealing with it—and these objectives can be better accomplished with a detailed examination of a few specific papers. it is also in the spirit of hurlbert's original paper on the topic: "the citing of particular studies is critical to the hoped-for effectiveness of this essay. to forego mention of specific negative examples would be to forego a powerful pedagogic technique"  <cit> .

a single recent issue of nature neuroscience  was therefore examined as a "case study". this journal was chosen because it has detailed instructions for reporting the results of statistical analyses , and as a consequence, a greater proportion of manuscripts have sufficient information to assess the analyses. in addition, this is a leading neuroscience journal, and the implication is that if errors of this sort can be found in studies generally considered to be of high quality, then they are also likely to be found elsewhere. this particular issue was chosen simply because of its suitability in illustrating the points being made. it is therefore not necessarily representative of other issues.

the simulated data in figure  <dig> was produced with r   <cit> .

RESULTS
of the nineteen papers published in the august  <dig> issue of nature neuroscience, seventeen papers  used inferential statistics; of these, only three  had sufficient information to assess whether there was pseudoreplication. of these three, two appeared to have pseudoreplication. of the fourteen papers that used inferential statistics but did not provide sufficient information, five  were suspected of having pseudoreplication, but it was not possible to determine for certain. a table summarising this information can be found in additional file  <dig> 

manuscripts with pseudoreplication
fiorillo et al. performed electrophysiological recordings from the brains of two macaque monkeys  <cit> . we can sympathise with the desire to use as few animals as possible , but neurons from the same brain are not independent: they are identical genetically, they have the same developmental history, and they share the same environment, and thus two neurons from the same brain will respond to an experimental stimulus in a similar manner . additional technical considerations include neurons receiving inputs from the same structures , and they can be interconnected either directly via gap junctions or synapses, or indirectly via interneurons. the paper presents data from  <dig> and  <dig> neurons  and inferential statistical tests are performed between two experimental conditions. unfortunately, the inter-neuron variability is conflated with the inter-animal variability and the analysis must reflect this distinction. it should be noted that this type of analysis is standard in the neurophysiology field, not because it is the optimal approach to address research questions, but because of ethical considerations limiting the number of primates used, and because it is technically easier to record from more neurons in one monkey rather than to record from more monkeys. however, such an experiment with two animals is limited mainly to descriptive statistics such as means and standard deviations.

in another paper, sato et al. classified rod terminals in the retina as either bipolar or not, and examined whether the proportion of these two terminal types differed between control and pikachurin knockout mice  using a chi-square test  <cit> . the figure caption indicates a total sample size of n = 651; however, this is the number of rods and not the number of mice, which is six . one of the assumptions of the chi-square test is that observations must be independent; however the factors affecting whether a rod is bipolar or not will tend to affect all rods in the same retina in a similar manner. a more appropriate analysis would be to determine the proportion  of each rod type for each individual mouse, resulting in six data points that would have values between zero and one. an independent-samples t-test could then be used to test whether the mean proportion differed between the knockout and control mice. with a sample size of only six, a t-test would only detect large differences between groups and would likely be underpowered. a potential trade-off would be to examine fewer rods in each mouse and examine more mice.

both manuscripts used hierarchical sampling but did not distinguish between the number of data points and the number of independent observations. these types of errors are not limited to this issue  or this journal, but can be found in other top general science journals such as cell  <cit> , pnas  <cit> , and science  <cit> .

manuscripts with suspected errors
the following manuscripts possibly had pseudoreplication, but insufficient information was provided to determine for certain. they are nevertheless discussed because they contain other types of analyses where pseudoreplication can arise.

toni et al. examined how new dentate gyrus neurons integrate and form functional synapses with cells in the hilus and ca <dig> region of the hippocampus  <cit> . they examined the size of mossy fibre boutons and reported in the methods section that 20- <dig> boutons were analysed per time point in the ca <dig> region, and between 20- <dig> were analysed at each time point in the hilus. the results were presented for the ca <dig> and hilus as t <dig> =  <dig> , p <  <dig>  and t <dig> =  <dig> , p =  <dig> . there appears to be multiple observations on each mouse, unless the number of mice was greater than  <dig> . in addition, the corresponding figure caption  states that the graph displays the means and standard error of the means. however the error bars are so narrow that they are obscured by the data points of the mean values. in both the graph and the analysis there appears to be a misspecification of the structure of the data, and the number of observations  has been confused with the number of independent observations . other studies by the first author used the same  analysis  <cit> .

groc et al. examined the effect of corticosterone on ampa receptor trafficking and synaptic potentiation in vitro  <cit> . the supplementary methods state the data were analysed with t-tests and anovas  and that 2- <dig> different sets of hippocampal cultures were used. the figure captions however show very large n's--greater than  <dig> in one case. since degrees of freedom were not provided, it is not known whether values were averaged before analysis, but the large n's suggest that they may not have been.

pocock and hobert examined the effects of oxygen levels on axon guidance and neuronal migration in c. elegans  <cit> . the figure captions indicate that n's were typically over  <dig>  and it is also stated that "data were combined from three independent experiments". the data are displayed  as the percentage of animals with a defect, which suggests that some data reduction has occurred and the values represent the mean percentage over the three independent experiments. however, all of the analyses were conducted with two-sample z-tests, which are typically used if the population standard deviations are known, or if the sample size is sufficiently large . since population standard deviations are not known and need to be estimated from the data, a two-sample t-test should have been used if the sample-size was three . using a z-test in such a case leads to an inflated type i error rate . for example, a test statistic of  <dig>  with a z-test would give a two-tailed p-value just under  <dig>  , whereas a test statistic of  <dig>  with a t-test with n =  <dig>  would give a p-value of  <dig> , leading to a different conclusion. therefore either a z-test was used where a t-test should have been, or a z distribution was somewhat approximated by a large n, but then the n does not reflect the number of independent experimental units. it should be noted that there were very large differences between the groups, and therefore the conclusions are unlikely to change if the data were reanalysed.

using electrophysiological recordings, chen et al. examined how the difficulty of a task affected the activity of neurons in the primary visual cortex of two monkeys  <cit> . the data were analysed with two-sample nonparametric tests  and figure captions stated n's for the number of neurons, but not the degrees of freedom, and therefore it is not clear whether each neuron was treated as an independent observation or whether the data were averaged before analysis. the latter is unlikely since there are only two animals.

serguera et al. examined how dopamine in the olfactory bulb of female mice impairs the perception of social odours contained in male urine  <cit> . figure four a in their paper presents data for the amount of time five female mice spent sniffing five different concentrations of male urine . each female mouse was exposed to each of the five concentrations of male urine, and the outcome variable was the ratio of time spent sniffing urine versus water. the methods section only states that the data in the paper were analysed with a two-way anova or t-test. clearly neither of these are appropriate for a one factor experiment with five levels, such as this. since this is a within-subjects design, where multiple observations are made on each mouse, it is not clear whether the authors used the correct rm anova, or whether they treated the  <dig> observations as being independent. they report the overall anova analysis as f <dig> =  <dig> , p =  <dig> . given the available information, we can calculate the p-values for both a one-way anova and a rm anova . a one-way anova with  <dig> and  <dig> df would give p =  <dig> , whereas a rm anova on  <dig> and  <dig> df would give p =  <dig> . neither analysis corresponds to their reported p-value of  <dig> , but the one-way anova is closer , suggesting that they used the incorrect analysis . note that discrepancies between p-values and their associated test statistics are common, even in top journals such as nature and the bmj  <cit> . the f statistic would also be different if the incorrect analysis was used, and so it is not just a matter of different degrees of freedom.

in ambiguous situations such as this, readers have to form some sort of judgement regarding the statistical competence of the authors. based on other aspects of their data analysis, one may be reluctant to give them the benefit of the doubt. first, an f-test has two degrees of freedom associated with it; however, throughout the paper the authors only reported the numerator df and not the error df . this suggests that the authors  may not be aware that there are two dfs associated with f-tests. second, in figure four c in their paper, the authors tested whether the means of two groups were significantly different with an anova, and then followed this analysis up with a posthoc test between the same two groups! not surprisingly, performing the same test twice produced the same result. this is an excellent example of unthinking, rote statistical analysis. the means of two groups can be compared with either a t-test or an anova f-test, and they will produce identical results; the square of the t-statistic is equal to the f-statistic, and both have an identical corresponding p-value , where n is the number of independent samples and g is the number of groups). finally, in figure seven in their paper, the authors present the number of pregnancies carried to term in the female mice under different experimental conditions. they analyse the data with a chi-square test, but placed asterisks over group number four in the graph. a chi-square test is an omnibus test, and it provides no information on whether a specific group or condition is different from any other group or condition, yet the authors decorated the figure with asterisks over the group with the lowest number. so did the authors use the correct repeated-measures anova analysis for the data in figure four a? there is not enough information to know for sure, and this is left for the reader to decide.

discussion
in a well-publicised study, ioannidis concluded that most published research findings in the medical literature are false  <cit> . one thing he assumed was that at least the statistical analyses were carried out correctly, and inappropriate analyses such as those discussed in the present paper will only increase the number of false conclusions. it is likely that the standard of statistical analysis is much lower in preclinical animal studies, which is due to a variety of reasons, including  these studies are less likely to have a statistician associated with them,  effects are often large, and the correct conclusions can still be reached no matter how bad the analysis is,  they are much less likely to be a part of a meta-analysis  <cit>  and therefore deficiencies in the design, analysis or reporting are not highlighted,  the cost of being wrong is minimal since no single animal study will influence the treatment of patients or alter public policy, and  experiments are not registered before being conducted, giving greater scope for data dredging and selective reporting. registering animal experiments has therefore been suggested as a way to improve the quality of animal studies  <cit>  or to have "animal subject committees" to "...scrutinise drug trials in animals. the task of such committees would be to assess sample size, randomisation of treatments, blinding of observers, selection of animal subjects, statistical methods... "  <cit> . these suggestions are not realistic because the goals and therefore methods of preclinical research differ from clinical trials  <cit> . for example, a typical preclinical grant will contain many small experiments examining many disparate outcomes , not one big study with a single primary outcome. furthermore, later experiments often depend on the results of earlier experiments, and major details may be modified as other studies are published or new methods or techniques become available. registration and further scrutiny by committees will add a great deal of bureaucracy with only a small improvement in the quality of experiments. other ways to improve the quality of preclinical studies should be tried first. 

the term pseudoreplication was coined by hurlbert in  <dig>  <cit> , where he argued that current practices in the ecological literature were inadequate, much like altman did in the medical literature  <cit> . since hurlbert's original publication, pseudoreplication has become less frequent in ecological studies, but has not been eliminated completely  <cit> . a natural question to ask is "how common is pseudoreplication in the neuroscience literature?" hurlbert found that almost 50% of studies that used inferential statistics had some type of pseudoreplication  <cit> . subsequent studies found that the prevalence had decreased to 32%  <cit>  and 12%  <cit>  in the ecological literature. there is no a priori reason to think that the neuroscience literature is better than other fields. indeed ecologists  tend to have greater statistical knowledge than many other biologists , possibly because they cannot perform highly controlled experiments as laboratory-based scientists can, and so the alternative is to measure potential confounding variables and then take them into account statistically. given that there has not been much discussion about pseudoreplication in the neuroscience literature, one might speculate that the prevalence is towards the higher end of the scale. there have been some recent papers critical of common statistical practices in the neuroimaging field, where lack of independence is also a central issue  <cit> .

reporting guidelines
medical studies involving human patients have detailed reporting guidelines such as the consort statement  <cit> , and there are also guidelines for biological studies such as the uniform requirements for manuscripts submitted to biomedical journals by the international committee of medical journal editors  <cit> . in  <dig> curran-everett and benos provided a set of guidelines for reporting statistics in journals published by the american physiological society  <cit> , and in a follow-up paper published three years later they reported that these guidelines had little impact on subsequent practice  <cit> . cumming et al. have also made suggestions for describing what error bars in figures should represent  <cit> .

in addition to the above guidelines, further specific information should be provided in order to check whether analyses were carried out correctly. these include:

 <dig>  report the sample size and number observations for each experiment. the sample size  should refer to the number of independent samples and not the number of observations. if there is only one observation per subject then the sample size and the number of observations will be the same. however, if multiple observations are made on each subject, then it is necessary to distinguish the sample size from the observations, as it will allow readers to better understand the design of the study. in addition, the most important reason for the inclusion of these values is that they are necessary to check the results when combined with the information in the next guideline.

 <dig>  report the value of the test statistic, degrees of freedom, and exact p-value. these provide the necessary information to check whether the analyses were carried out correctly. they can also allow readers to understand the analysis better if the verbal description was ambiguous. if p-values are very small, then p <  <dig>  would suffice, but not p >  <dig> , p <  <dig> , or p <  <dig> .

 <dig>  error bars should correspond to the analysis. graphical measures of uncertainty such as confidence intervals and standard errors of the mean should be based on the number of independent samples, that is, the graphical representation of the data should correspond to the statistical analysis that was performed on them. if there are two groups of five animals, with multiple observations, the t-test should have  <dig> -  <dig> =  <dig> df and the error bars should be based on . this is similar to cumming et al.'s third rule: "error bars and statistics should only be shown for independently repeated experiments, and never for replicates"  <cit> . here they are referring to the fact that one run of the experiment is the experimental unit, and not the number of observations in each experiment .

without this information it is not possible for peer-reviewers to adequately assess whether the statistical tests were carried out appropriately, and they must merely assume that the authors have performed the analyses correctly. this is not something that can be safely assumed, and a recent systematic survey identified a number of problems with the reporting, experimental design, and statistical analysis of studies using laboratory animals  <cit> . these guidelines need to be made requirements for publication and therefore the initial responsibility lies with journal editors. the nature series of journals have already improved their reporting requirements after a study showed numerous errors in one of their journals  <cit> , but there is still room for further improvement. the european journal of neuroscience has also recently issued guidelines which are similar to those suggested above  <cit> , and these recommendations could be easily adopted by other journals. until journal requirements change, reviewers must insist on this information being provided. including all of the above information might make the text difficult to read, especially if a number of results are presented in succession. all of this information therefore need not be presented in the main text, and the full results can be reported in the online supplementary material , and the main text could include only p-values and the sample size for example.

remedies for pseudoreplication
pseudoreplication does not necessarily imply that the studies are flawed, and a reanalysis of the data may be all that is required. it may however become apparent that the sample size is too small to make any meaningful inferences about the parameters of interest. pseudoreplication can be dealt with prior to analysis, for example by using only one mouse per litter for a particular experiment, thus eliminating any litter effects. statistical methods for dealing with pseudoreplication are available and four such methods are discussed below and summarised in table  <dig>  other options for ecological studies are discussed by millar and anderson  <cit> .

averaging dependent observations
in the opening example with ten rats undergoing rotarod testing on three consecutive days, the results from the three days can be averaged so that each rat contributes only one value to the analysis. this is particularly useful when there is no expected trend over the days of testing, or if there is, it is not relevant to the research question; for example, if three trials were simply used to get a better estimate of the rats' motor functioning. similarly, in a hierarchical sampling design, one could average values from multiple neurons in a rat to obtain one value per rat that will then be carried forward for statistical analysis. averaging has the advantage of simplicity, and common statistical tests can be applied . a drawback is that information is lost when averaging; for example, there may be a different number of observations for each rat, and the observations for some rats might be more variable than for others. therefore some estimates of the mean response for each rat are more precise than others, but this information is not used in the analysis and each mean value is treated equally, rather than being weighted according to its precision. it should also be noted that averaging can lead to bias when the number of observations is correlated with the outcome variable, however this is more of a concern for observational studies and longitudinal clinical studies. this is not a major concern for laboratory-based experimental studies, where the number of observations is under control of the experimenter, and any missing values typically occur at random.

summary-measure analysis
another alternative to using the mean of a number of dependent observations is to use some other relevant value which captures a feature of interest, such as the slope, intercept, or area under the curve  <cit> . this is referred to as summary-measure analysis or derived-variable analysis. using the same rat example, suppose the researcher was interested in whether there was a change in rotarod performance over the three days, such as a practice effect. for each rat, a regression analysis could be carried out using day as the explanatory variable  and time spent on the rotarod as the response variable . the slope of the regression line would then be used for further analysis, perhaps to compare whether one group improved faster than another, and in the absence of a practice effect, the values of the slopes should be centred around zero. as above, it has the advantage of reducing many correlated observations to fewer independent observations, it is conceptually straightforward, and it allows for the use of standard tests. the drawbacks are also the same, namely, information is lost. in addition, since there were only three time points, the estimates of the slopes would have low precision . this analysis is not the recommended one and is only given for completeness. an alternative analysis that could be used is a repeated-measures anova, where day would be the within-subjects factor and condition  would be the between subjects factor.

there is also an important point to be made when deciding whether to average over observations or to use slopes as a summary measure , and it is based on whether the research question is  do subjects with high values of x also have high values of y, or  within each subject, are high values of x associated with high values of y. these are different questions and are discussed in a series of papers by bland and colleagues  <cit> . in the first case, average all the x and y observations on each subject, and then use these for analyses. in the second case, analysis of the slopes or mixed models can be used.

separate analyses
another option is to conduct separate analyses on each of the three days, using an independent samples t-test at each day. this means that more than one statistical test is performed, raising the issue of whether corrections for multiple tests should be made. furthermore, there is no integrated final result, only a collection of disjointed p-values that may be difficult to interpret. what would the interpretation be if there was no significant difference between groups on the first or third day of testing, but there was a significant difference on the second day? should we conclude that there is a treatment effect because there was at least one significant p-value, or should we take a majority vote: two non-significant versus one significant p-value means that the treatment really did not have an effect ? it is complicated by the significant result being the middle day and not the first or last day . separate analyses are therefore not very useful, but they are often performed to test whether there are significant effects at each time point after a test for an overall main effect. this goes by the name of posthoc testing, which is routinely performed and mostly unnecessary  <cit> . it is not uncommon to have a significant main effect of treatment and then have none of the posthoc tests significant. this is due to the reduced power of the posthoc tests.

mixed models
as noted above, a repeated measures anova is a common analyses for the rotarod example if there was interest in testing for differences across the three days; however, this method has been superseded by more recent methods with superior properties that are called random  effects models, hierarchical models, multilevel models, or nested models   <cit> . these are similar to the averaging and summary-measure methods discussed above, but instead of performing the analysis in two steps  the analysis is performed in one step. one important advantage is that information on the precision of the estimates is retained and used in the analysis

a key feature of these models is the distinction between fixed and random effects. fixed effects are the familiar explanatory variables such as treatment, sex, condition, and dose, and are usually something that the experimenter is interested in testing directly. fixed effects affect the mean of the outcome variable; for example, the effect of a treatment is to increase the value of the outcome variable compared to a control condition by a certain number of units. random effects are less familiar and are usually something that the experimenter is not interested in directly  but must be taken into account. a variable can be treated as being either fixed or random, but usually one is more appropriate, and the interpretation of the results is different. for example, if a researcher was interested in testing whether there are differences between cages on some outcome variable, twenty rats could be randomly assigned to four cages , labelled a-d. in this example there is no other experimental variable, only the cage that the rat is in. treating cage as a fixed effect would lead to a one-way anova with four levels. if significant differences are found between cages, then conclusions can only be made about these four cages, and not about other unobserved cages. if rats in cage c had particularly high values, there is no reason why in a subsequent experiment rats in a cage also labelled c would also have high values, rather than rats in cage b for example. there is nothing about the letter c on the front of the cage that affects the mean value of rats in that cage, or that can be used to predict the value of rats in other cages also labelled c; thus the cage labels are said to be uninformative. contrast this with a true fixed effect such as dose of a drug; if the  <dig> mg/kg group had higher values than the  <dig> mg/kg control group, then one would also expect that the  <dig> mg/kg group would have the higher values in a subsequent experiment . if instead cage is treated as a random effect , then these four cages are treated as random samples from a population of cages, and inferences can be made about the effect of cages in general. a good discussion of the difference between fixed and random effects can be found in references  <cit>  and  <cit> .

one important drawback of the repeated measures anova is that the assumptions of compound symmetry and sphericity are rarely met. these terms refer to the correlation structure of the data. returning to the rat rotarod example, the correlation of the outcome variable at each combination of time points can be calculated . if these three correlations are all similar, and in addition the variances at each day are similar , then the data are said to be compound symmetrical. if differences rather than correlations between each combination of time points are calculated, then the data are said to be spherical if these difference scores all have the same variance . these assumptions are usually not met because observations closer in time tend to be more highly correlated than observations further apart. the traditional solution has been to adjust the degrees of freedom of the f-statistic so that the actual α-level is closer to the nominal α-level . modern statistical methods can model different types of variance-covariance relationships directly, making ad hoc adjustments to degrees of freedom unnecessary. kristensen and hansen provide an excellent introduction and a comparison of different analyses on rats  <cit>   and gueorguieva and krystal discuss the advantages of mixed models over repeated measures anova  <cit> ; introductory books include zuur et al. , crawley  <cit>  and faraway  <cit> , and a more comprehensive treatment can be found in pinheiro and bates  <cit> .

mixed models and their extensions  are the preferred methods for analysing the type of data discussed in this paper and are already being used to model litter effects  <cit>  and hierarchically structured data  <cit> . the above methods do not exhaust the possibilities for dealing with data of this type, but highlight some of the more common methods and their advantages and disadvantages.

how to check reported values
when the necessary information is provided, it is easy to check whether pseudoreplication has been handled correctly, for this one needs to know the degrees of freedom associated with common statistical tests, and these are provided in table  <dig> for reference. the hypothetical rotarod example in the introduction can now be easily checked. the authors stated that there were ten rats and that they used an independent samples t-test; table  <dig> indicates that the correct number of df =  <dig> +  <dig> -  <dig> =  <dig>  and not the  <dig> that the authors reported. even if degrees of freedom are not reported, it is still possible to determine whether pseudoreplication was handled correctly using standard software such as openoffice calc or microsoft excel. both of these programmes have a tdist function which can calculate a p-value given a t-statistic, degrees of freedom, and whether the test is one or two-sided. if the dfs were not reported for the rotarod example, we could check the results with =tdist, where  <dig>  is the reported t-statistic ,  <dig> is the correct number of degrees of freedom, and  <dig> indicates that it is a two-tailed test. the result is p =  <dig> , which does not correspond to the reported value of p =  <dig> . substituting  <dig> df for  <dig> in the above command does give p =  <dig> , and allows us to conclude that the reported analysis is incorrect. the same procedure can be carried out in r using the command pt* <dig> .

n <dig> = sample size of group one; n <dig> = sample size of group two; n = total sample size; n = total number of observations ; g = number of groups; ga = number of groups for factor a; gb = number of groups for factor b; obs = number of repeated observations on the same subject; r = number of rows; c = number or columns. † "mixed" refers to the presence of both between and within subjects effects, and should not be confused with mixed effects models described in the text.

a similar procedure can be carried out for anova f-tests using the =fdist function in openoffice/excel and pf in r/s-plus. this analysis may not be of much use if the authors only report whether the p-value was greater or less than  <dig> , but it does provide a method for checking partially reported results.

CONCLUSIONS
the problem of pseudoreplication has been recognised for many years in ecology and related areas  <cit> , as well as in the medical literature, where altman writes "in some conditions it is possible to take several measurements on the same patient, but the focus of interest usually remains the patient. failure to recognise this fact results in multiple counting of individual patients and can lead to seriously distorted results. analysis ignoring the multiplicity violates the widespread assumption of statistical analyses that the separate data values should be independent. also, the sample size is inflated, sometimes dramatically so, which may lead to spurious statistical significance"  <cit> . the neuroscience community has not recognised the importance of dealing with this type of data appropriately, although a recent paper has highlighted the negative effect this can have on translational research  <cit> .

statistical competence will not happen overnight, but stricter reporting requirements will make it easier to detect pseudoreplication, and this requires the sample size , degrees of freedom, the test statistic, and precise p-values to be reported, allowing many of these errors to be detected at a glance. this information also allows for a more detailed analysis if required, either by editors, reviewers, or other readers.

continuing with the present situation suggests that statistical analysis is not really important, it's just something scientists go through to obtain p-values that can be tacked on to the end of sentences, or to calculate the number of asterisks that can be used to decorate a graph. many medical advances are based on preclinical animal research, it is therefore important that preclinical studies are conducted, analysed, and reported correctly.

author information
sel's academic  and research background are primarily in experimental neuroscience. sel also holds a masters in computational biology and currently works as a senior research statistician in the pharmaceutical industry.

response to lazic
by christopher d. fiorillo

email: fiorillo@kaist.ac.kr

address: department of bio and brain engineering, korea advanced institute of science and technology, daejeon, korea

lazic makes many useful points about the misuse of statistics. however, his article on pseudoreplication does not demonstrate a clear understanding of the goals and issues at stake in primate neurophysiology, nor does it clarify the importance of independence in statistical tests.

lazic's criticisms of my paper  <cit>  generalize to virtually all papers within the field of primate neurophysiology, in which the standard is to record from many neurons, but in only two monkeys. that the field has been able to make substantial progress is largely dependent on there being relatively little variation from one brain to another in most of the phenomena that have been examined. . the interest in virtually all and papers is inacross responses within and across neurons, not in comparisons across animals. of course there could always be differences between individual animals, and it is important not to conflate variation across neurons with variation across animals. my manuscript may indeed have given the appearance of conflating the two. the data was in fact presented separately for the two animals in the original manuscript, but during the review process it was combined for the sake of simplicity, as is common within the field. the standard is that statistical significance should be demonstrated separately within each monkey. i did this for each monkey, although i only stated this in the main text with respect to figure four c, but not with respect to figure three c.

the statistic comparison that the author has questioned  was designed to test whether there is a difference in neuronal firing rates  between responses to juice reward depending on when juice was delivered following a conditioned stimulus. if one knows the subject matter and reads my paper for more that just statistical methods, then one has a strong prior expectation that the statistical comparisons made in figures three c and four c will be highly significant, and thus the statistical significance is not of much interest. the interesting point of the figures, as described in the main text, is that although significant, the differences are surprisingly small relative to the very large difference seen in comparing either of these responses to unpredicted reward. the statistical significance that was quantified in these figures was thus superfluous and could have been omitted entirely. there were no statistical tests performed on the important effect because it was so large as to be obvious . to paraphrase a colleague who is an excellent scientist but a reluctant statistician, it passed the "bloody ... obvious test."

lazic also makes the point that "neurons of the same brain are not independent." if this statement does not signify confusion on the part of its author, it may nonetheless confuse readers. there is an important difference between physical or causal independence on the one hand, and logical or statistical independence on the other. neurons in the same brain may be physically or causally related. however, that fact in itself does not necessarily require that the neurons are or are not statistically or logically independent. statistical independence is all that matters with respect to statistical tests, and statistical dependencies could be present regardless of whether the neurons are in the same brain or different brains.

a second critical error that is often made is to confuse the functioning of a physical system with our knowledge of that system. statistics and hypotheses are derived from the latter. "statistical independence" means that we, the people performing the statistical test, do not have knowledge of how two pieces of data  are related. we believe that neurons within a defined population, clustered together in the same region of the brain, are likely to have direct or indirect physical interactions with one another, and likewise, to display some sort of correlations in their firing rates. but at the outset of a typical study, we do not know what these correlations are, and thus it is rational for us to treat the data from each neuron as independent. by contrast, if we already knew neurons within clusters of known dimensions to be tightly coupled to one another through gap junctions, then we would probably try to avoid recording from neurons within the same cluster, and when we did obtain data from neurons in the same cluster, we might average it together before doing an analysis of responses across clusters. the data from my neurons was "statistically independent" because, at the time that i did the statistical test, i was ignorant of any relevant relationship between the firing rates of discrete neurons. given a different state of knowledge, statistical independence may not apply and another type of statistical test may be more appropriate.

i strongly recommend probability theory, the logic of science, in which e.t. jaynes discusses these issues and other "pathologies of orthodox statistics other "path "  <cit> . it will be clear to anyone reading his text that there are fundamental disputes within the field of probability and statistics that have yet to be fully resolved. given that information and probabilities are inherently linked, this topic is of particular importance in studying information processing within the nervous system  <cit> .

