BACKGROUND
sequence alignment algorithms
one of the most used alignment algorithms for sequence homology search is the smith-waterman algorithm
 <cit> . it computes the optimal local alignment and the respective similarity score between the most conserved regions of two sequences, with a complexity proportional to
o. the algorithm is based on a dynamic programming  approach that considers three possible mismatches: insertions, deletions, and substitutions. to ensure that a local alignment is found, the computed scores are constrained to a minimum value of  <dig>  corresponding to a restart in the alignment. to circumvent the computational complexity of the smith-waterman and similar alignment algorithms, alternative heuristic methods  were developed. however, their lower complexity is obtained at the cost of sacrificing the resulting sensibility and accuracy.

an effective way that has been adopted to speed up these dp alignment algorithms is the exploitation of data-level parallelism. one of the most successful parallelization methods was proposed by farrar
 <cit> , who exploited vector processing techniques using the intel sse <dig> instruction set extension to implement an innovative striped data decomposition scheme . in his approach, each vector contains several cells from the same column of the scoring matrix. however, contrasting to other implementations, these cells are not contiguous. instead, they are exactly k cells apart, in order to minimize the inter-row dependencies. essentially, this processing pattern assumes that there is no dependencies across the vertical ‘segment sections’ . whenever this assumption is not verified, the existing data dependencies have to be solved by a second inner loop . since these vertical dependencies among cells are unlikely , the resulting algorithm proves to be very effective in the average case.

meanwhile, rognes proposed a different method in his swipe tool
 <cit> , which achieved even better performances than farrar’s. contrasting to farrar’s, which was based on the exploitation of intra-task parallelism, rognes’ method also makes use of sse <dig> vector processing but exploits an inter-task parallelism scheme , by using a lock-step processing model . each vector is loaded with n different sequences, one in each vector element , and the algorithm concurrently aligns them against a target sequence, by using the n vector channels to hold the independent computed values. the drawbacks of this strategy are concerned with its restrictive application domain, resulting from the fact that the n alignments proceed coalesced, from the beginning to the end. any divergence on the program flow carries a high performance penalty, either as stoppage time or as wasted computing potential . even so, the complete elimination of data dependencies between the values inside the same sse register allows this technique to achieve quasi-optimal speed-ups. therefore, this software implementation is often regarded as the fastest choice.

other authors have even focused on the use of more specialized hardware architectures, such as gpus
 <cit> , asics
 <cit> , or on parallelizing the algorithm onto a multi-node grid, usually by dividing the sequence database in blocks and independently searching on each block.

markov models and viterbi decoding
instead of searching with a single query sequence, several applications have adopted a previously built consensus, conveniently defined from a family of similar sequences. this consensus structure is usually known as a consensus profile and it provides a more flexible way to identify homologs of a certain family, by highlighting the family’s common features and by downplaying the divergences between the family’s sequences.

a common method to perform a profile homology search rests on a well-known machine learning technique: hidden markov modelss . as an example, an hmm may be constructed to model the probabilistic structure of a group of sequences, such as a family of proteins. such resulting hmm is then used to search within a sequence database, by computing the probability of that sequence being generated by the model. hmms may also be used to find distant homologs, by iteratively building and refining a model that describes them .

in  <dig>  krogh et al.
 <cit>  developed a straightforward and generalized profile hmm for homology searches that emulates the results of an optimal alignment algorithm. the model is mainly composed by three different types of states, corresponding to matches/mismatches , insertions  and deletions , with explicit transitions between the three types of states. figure
 <dig> depicts an example of such model, where the match states  are represented by squares, the insertions  by rhombus and the deletions  by circles. the model also contains an initial and a final state, represented by hexagons.

the most important algorithms to process hmms are the forward algorithm, which gives the full probability for all possible model state paths; and the viterbi’s algorithm, used to compute the most likely sequence of model states for the generation of the considered sequence. the complete path of states that is extracted by the application of viterbi’s procedure thus corresponds to an optimal alignment of the considered sequence against the profiled model.

hence, for a general markov model, viterbi’s algorithm computes the most likely sequence of hidden states. by denoting as p) the probability that the most likely path at time i ends at vj, viterbi’s algorithm defines the following relation to compute this probability: 

  p)=pmaxj′{vj′tj′j} 

in this equation, p represents the probability of observing xi in state vj. the
tj′j term represents the transition probability from state
vj′ to state vj.

these equations are very similar to the corresponding recurrences of the forward algorithm, with viterbi’s using a maximum operation while forward uses a sum. to avoid possible underflows resulting from the repeated products, the involved computations usually use logarithmic scores . this conversion also replaces the multiplication operations by sums, which further simplifies the calculations. to simplify this log-odds notation, the term vj will herein represent log)). the recurrence equations of viterbi’s algorithm, for the profile hmms, in log-odds, are presented in equation  <dig>  

  vjm=logemjqxi+maxvj-1m+logvj-1i+logvj-1d+logvji=logeijqxi+maxvjm+logvji+logvjd+logvjd=maxvj-1m+logvj-1i+logvj-1d+log 

the terms eij/qxi, relating the emission probabilities ) and the background probability distribution  of belonging to the standard random model, represent a normalized probability of observing the event xi at state ij. the remaining variables in these equations represent the following:
vjm represents the logarithm of the probability of the most likely path ending at state mj in column j, after processing i letters from a given sequence. likewise,
vji and
vjd represent the logarithm of the probability of an insertion and deletion, respectively. txy represents the probability of transitioning from one state to another .

hmmer
hmmer
 <cit>  is a commonly used software tool that uses hmms to perform homology search. the original version of hmmer relied on a model architecture entirely similar to krogh-haussler’s model. the current version  employs the ‘plan 7’ model architecture, presented in figure
 <dig>  although the core of this architecture is still very similar to krogh-haussler’s, plan  <dig> has no d→i or i→d transitions, which simplifies the algorithm. furthermore, some special-states are added at the beginning and at the end, in order to allow for arbitrary restarts  and multiple repeats . these special states can be parameterized to control the desired form of alignment, such as unihit or multihit, global or local.

this latest hmmer version also introduced a processing pipeline, comprehending a combination of several incremental filters. each incremental filter is more accurate, restrictive and expensive than the previous one. all of these filters have already been parallelized by single-instruction multiple-data  vectorization using farrar’s striped processing pattern
 <cit> . the viterbifilter, in particular, has been parallelized with 16-bit integer scores. accordingly, the present work proposes a new parallelization approach of this filter based on rognes’ processing pattern
 <cit> , with a novel strategy to improve the cache efficiency.

cache-oblivious simd viterbi with inter-sequence parallelism
the proposed cache-oblivious parallel simd viterbi  algorithm represents an optimization of the viterbi filter implementation in local unihit mode . global alignment is not currently supported by the latest version of hmmer.

the presented implementation was developed on top of the hmmer suite, as a standalone tool. a full integration into the hmmer pipeline was deemed unsuitable, since the pipeline was designed to execute a single sequence search at a time, while the proposed approach exploits inter-sequence parallelism, i.e., it concurrently processes several sequences at a time in the simd sse <dig> vector elements.

a coarse grained structure of the implemented algorithm, when compared with the original hmmer implementation, is presented in listing  <dig>  the following subsections will describe the several code transformations that were required to implement the proposed processing approach.   

rognes’ strategy applied to viterbi decoding
although hmmer extensively adopts the farrar’s intra-sequence vectorization approach, the presented research demonstrates that the inter-sequence parallel alignment strategy that was proposed by rognes
 <cit>  can be equally applied to implement the viterbi decoding algorithm. the proposed vectorization comprehends the computation of the recursive viterbi relations, by using three auxiliary arrays to hold the previous values of the match , insert  and delete  states . after each loop over the normal states, the special states  are updated. since the proposed implementation does not support multhit alignments, the j transitions were removed from the original model.

just like farrar’s and rognes’ vectorizations, the implementation that is now proposed uses 128-bit simd registers, composed by eight 16-bit integer scores, to simultaneously process eight different sequences. furthermore, similarly to the hmmer implementation, the scores are discretized by using a simple scaling operation, with an additional bias and saturated arithmetic. hence, just like the ‘- <dig>  nat approximation’ that is used by hmmer, the n → n and c → c transitions were set to zero, and a - <dig>  score offset was added at the end. this value approximates the cumulative contribution of n → n and c → c transitions which, for a large l, is given by
logll+ <dig>  as a result, the b contributions become constant, since they only depend on the n values  and on the j values .

a required and important step in this inter-sequence simd implementation of the viterbi decoding is the pre-loading and arrangement of the per-residue emission scores. however, these emission scores depend on the searched sequences and they cannot be predicted, pre-computed and memorized before knowing those sequences. furthermore, each new batch of  <dig> sequences to search requires the loading of new emission scores. rognes’ solution to circumvent this problem can also be adapted to viterbi decoding and consists on loading the emission scores for the  <dig> different residues from the  <dig> sequences under processing  before starting the main loop of the model . to accomplish this, the scores must be transposed from the original continuous pattern into a convenient striped pattern, by using the unpack and shuffle sse operations. the implemented processing pattern is illustrated in figure
 <dig>  while the corresponding pseudo-code implementation is presented in listing  <dig>    

inline pre-processing of the scores
rognes’ method to pre-load and pre-process the emission scores before each inner loop iteration  suffers from a considerable handicap: it needs an additional re-write of the scores to memory, before the actual viterbi decoding can start. to circumvent this problem, an alternative approach is herein proposed. instead of transposing all the emission scores for each tuple of residues in the outer loop of the algorithm , over the sequence residues), the transposition was moved inwards to the inner loop  and subsequently unrolled for  <dig> iterations. hence, each iteration starts by pre-loading  <dig> emission values: one from each of the  <dig> continuous arrays. these emission values are then transposed and striped into  <dig> temporary sse <dig> vectors and used in the computation of the next model state for each of the  <dig> sequences under processing. hence, the inner loop is unrolled into the  <dig> state-triplets that are processed by each loop iteration. with this approach, the emission scores can be kept in close memory, thus improving the memory and cache efficiency. furthermore, the re-writing in memory during this pre-loading phase is also avoided.

to take full advantage of this vectorization approach, the number of considered model states should be always a multiple of  <dig> . nevertheless, this restriction is easily fulfilled, by padding the model with dummy states up to the next multiple-of- <dig> state barrier. these dummy states should carry dummy scores , so that they have a null influence on the final results, representing a negligible effect on the overall performance. according to the conducted evaluations , this optimization of the inlined scores loading procedure leads to an execution time roughly 30% faster than the pre-loading method used by rognes’ tool.

model partitioning
one common problem that is often observed in these algorithms is concerned with the degradation of the cache efficiency when the score arrays exceed the capacity of the innermost-level data caches, leading to an abrupt increase of the number of cache misses and causing a substantial reduction of the overall performance. this type of performance penalties is also present in hmmer farrar-based viterbifilter implementation, whenever larger models are considered.

to circumvent this cache efficiency problem, a loop-tiling  strategy based on a partitioning of the model states was devised in the proposed implementation, in order to limit the amount of memory required by the core loop. the required code transformations are illustrated in listing  <dig>  accordingly, the m, i and d model states are split in blocks , whose optimal dimension  length) is parameterized according to the size and organization of the l <dig> data  cache. with this approach, all the defined partitions are now iterated in a new outermost-loop ). as a result, the inner loop  is substantially shorter and it is now possible to obtain an optimal cache use in loops a and b — the middle loop  iterates over the  <dig> database sequences, while the inner loop  iterates over a single partition of model states.the middle loop , over the database sequences, mostly re-uses the same memory locations  that are accessed in the inner core loop . consequently, these locations tend to be kept in close cache. by limiting this model states loop to a pre-defined number of state-triplets defined by the mp length, it can be assured that the whole sequence loop  is kept in cache. hence, with this optimization, the memory required by the inner loop  is always cached in close memory and repeatedly accessed over the whole sequence loop, thereby drastically reducing the occurrence of cache misses. to attain the maximum performance, the mp length should be adjusted in order to achieve an optimal cache occupation, i.e., one that fills the available capacity of the innermost data cache . the processing pattern resulting from the proposed partitioned model is represented in figure
 <dig> 

listing  <dig> presents the pseudo-code of the whole algorithm implementation. the pseudo-code corresponding to the procedures emission_scores_preprocess and compute_state_triplet, used in the inner loop , are depicted in listings  <dig> and  <dig>  respectively. the notation adopted in this pseudo-code is closer to the provided software implementation than equations  <dig> and  <dig>  defining the algorithm. accordingly, the variable names re properly adapted. in particular, the j indexes were omitted and use cv . likewise, pv  was used to represent the index j- <dig>  hence, variable mpv represents
vj-1m. similarly, dpv represents
vj-1d and ipv represents
vj-1i. it is also worth noting that these variables are not arrays. instead, once the values are computed they are copied to the arrays mmx, dmx and imx, respectively. the transition probabilities t, are stored in  <dig> arrays . the computation of each match value is split between iterations. hence, an additional variable mnext is required to carry the partial computed value onto the next iteration.     

table
 <dig> represents the memory footprint required by the proposed cops implementation, when compared with the original hmmer viterbyfilter. m represents the model length. at this respect, it is important to note that although the presented approach exceeds the innermost cache capacity sooner, since  <dig> times more transition scores and 8-fold larger dynamic programming arrays are required in the inner loop, the cumulative amount of cache misses along the time is substantially lower, as a result of the proposed partitioning.

m represents the model length and all the computed scores are represented with 16-bit integers.

overall, the partitioned cops implementation has an expected memory footprint of around 240×m+ <dig> bytes . it can thereby be estimated an optimal mp value as the maximum model length  that limits the memory footprint within the size of the l1d cache. hence the mp length can be determined by: 

  mp=size- <dig> 

nevertheless, a conservative tolerance should be considered when approaching this maximum estimate, justified by the sharing of the l1d cache with other variables not correlated with this processing loop, process or thread. in fact, the conducted experimental procedures demonstrated that the actual mp values are very close to the best values that were theoretically estimated: 

•  <dig> to  <dig> states, for  <dig> kb l1d cpus ;

• around  <dig> states, for  <dig> kb l1d cpus ;

• <dig> to  <dig> states, for  <dig> kb l1d cpus .

there are, however, two memory blocks that cannot be strip-mined: 

• emission scores, which must be refreshed  for each new round of sequence tokens. these values are accessed only once, so it is counter-productive to consider their cacheability.

• dependencies that must be exchanged between adjacent partitions. the last match , insert  and delete  contributions from each partition have to be carried on in the next partition, and so they have to be saved at the end of each partition. hence, each partition receives as input one line of previous states, with one state-triplet for each 8-fold round of sequences, and produces as output another line of values to be used by the next partition. these dependencies can be minimized to  <dig> values per sequence round  after re-factoring the core code and moving the computation of mnext with the  <dig> state dependencies to the end. the re-factored inner loop code can be seen in listing  <dig> 

methods
to conduct a comparative and comprehensive evaluation of the proposed approach, the cops algorithm was ran against the viterbifilter implementation of hmmer  <dig> b <dig>  based on farrar’s striped vectorization. for such purpose, a benchmark dataset comprehending both dna and protein data was adopted, covering a wide spectrum of model lengths, ranging from  <dig> to  <dig> model states, with a step of about  <dig> 

in particular, the dna data consisted on hmms sampled from dfam  <dig>  database of human dna hmms
 <cit> , and the human genome retrieved from the ncbi archive.

as of march  <dig>  dfam uses hmmer <dig> b <dig> to create the models. the complete list of hmms is the following : m0063-u <dig>  m0804-ltr1e, m1597-tigger6b, m2500-l1m4c_5end, m0101-hy <dig>  m0900-mer4d <dig>  m1683-fordprefect, m2596-l1p4a_5end, m0200-mer <dig>  m1000-l1meg2_5end, m1795-l1mb4_5end, m2706-charlie <dig>  m0301-eulor9a, m1106-l1md2_3end, m1961-charlie <dig>  m2789-l1mc4_3end, m0401-mer <dig>  m1204-charlie17b, m2101-l1meg_5end, m2904-l1m2_5end, m0500-ltr72b, m1302-hsmar <dig>  m2204-cr1_mam, m2991-hal1m <dig>  m0600-mer4a <dig>  m1409-mlt1h-int, m2275-l1p2_5end, m0700-mer77b, m1509-ltr104_mam, m2406-tigger <dig> 

the protein data consisted on a mix of  <dig> small and medium-sized hmms from pfam  <dig> 
 <cit>  and  <dig> large hmms created with hmmerbuild tool from protein isoforms sampled from uniprot, and the nrdb90
 <cit>  non-redundant protein database. the short protein models, from pfam, were the following: m0063-act_ <dig>  m0400-alginate_exp, m0800-patched, m1201-duf <dig>  m0101-bactofilin, m0500-lant_dehyd_c, m0900-polc_dp <dig>  m1301-orbi_vp <dig>  m0201-adeno_52k, m0600-mpp <dig>  m1002-srfb, m0300-aldose_epim, m0700-pox_vert_large, m1098-cobn-mg_che.

the longer models used were generated from the following uniprot isoforms: m1400-q8cgb <dig>  m1800-q9byp <dig>  m2203-p <dig>  m2602-o <dig>  m1500-q9v4c <dig>  m1901-q <dig>  m2295-q3uhq <dig>  m2703-q8bti <dig>  m1600-q6nzj <dig>  m2000-q9ny <dig>  m2403-q9ugm <dig>  m2802-q9der <dig>  m1700-q3uh <dig>  m2099-q8nf <dig>  m2505-o <dig>  m2898-q868z <dig>  m3003-a2awl <dig> 

the benchmarks were run on two different machines: 

• intel core i <dig>  <dig> k, with an ivy bridge architecture, running at  <dig>  ghz with a  <dig> kb l1d cache;

• amd opteron  <dig>  with a bulldozer architecture, running at  <dig>  ghz with a  <dig> kb l1d cache.

all the timings were measured as total walltime, by using the linux ftime function.

RESULTS
cache misses
to evaluate the cache usage efficiency of the considered algorithms, the number of l1d cache misses for the cops tool and for the hmmer viterbifilter implementations were measured with papi performance instrumentation framework
 <cit> . to ensure a broader and more comprehensive coverage of measures, a wider and random dataset of dna models was considered in this specific evaluation.

when intel processors  are considered, the theoretical estimates suggested a critical point for optimal l1d cache utilization corresponding to models of size m≈ <dig> for the hmmer viterbifilter and m≈ <dig> for cops. to confirm the formulated estimation, the experimental procedure started by considering a non-partitioned implementation, which was evaluated in conjunction with the corresponding hmmer implementation. the obtained values, illustrated in figure
 <dig>  demonstrate that the theoretically estimated critical points coincide very closely with the observed abrupt increases of the l1d cache misses, as well as with the corresponding performance drops, which are strongly correlated in the observed results.
non-partitioned
 cops  implementation on the intel core i <dig> with  <dig> kb of l1d cache.

after partitioning, the overall performance of the proposed cops algorithm behaved remarkably close to what had been predicted, maintaining the same level of caches misses and computation performance for any model length . as it can be seen in this figure, cops even managed to be slightly faster than hmmer viterbifilter for models up to m≈ <dig> in  <dig> kb l1d cache machines. for longer models, cops gains are close to  <dig> -fold speedup over hmmer viterbifilter, due to the cache degradation observed in hmmer. when compared with the non-partitioned cops implementation , the partitioned version was about 50% faster for long models , demonstrating the remarkable benefits of the proposed partitioned processing approach.

performance
figures
 <dig> and
 <dig> represent the performance ) of the two implementations and the observed speedup of the presented cops approach, when using the intel core i <dig> processor. figures
 <dig> and
 <dig> represent similar results, observed in the amd processor.

for short models , the penalizing overhead of farrar’s lazy-f loop is clearly evident. as a result, the hmmer viterbifilter has a very poor performance on these models. in contrast, the proposed cops solution does not suffer from this problem and presents a much smaller performance penalty in these small models . as a result, with these short models, cops achieved a considerable  <dig> -fold speedup, when compared with hmmer.

for medium-length models , the proposed cops implementation is about as good as hmmer, reducing the observed speedup to about  <dig> -fold. these performance values correspond to model lengths where the striped version does not exceed the size of the innermost data cache.

for longer models, from  <dig> bps or  <dig> bps , it can be observed that the performance of hmmer quickly deteriorates as the length of the model increases and the memory requirements of hmmer farrar-based approach reach the maximum that the innermost l1d caches can provide . in contrast, the proposed inter-sequence cops is able to consistently maintain the same performance level with increasingly long models, thus achieving a 2-fold speedup on amd and a  <dig> -fold on intel, against the hmmer version for longer models.

CONCLUSIONS
the main insight of the presented approach is based on the observation that current parallel hmm implementations may suffer severe cache penalties when processing long models. to circumvent this limitation, a new vectorization of the viterbi decoding algorithm is proposed to process arbitrarily sized hmm models. the presented algorithm is based on a sse <dig> inter-sequence parallelization approach, similar to the dna alignment algorithm proposed by rognes
 <cit> . besides the adopted alternative vectorization approach, the proposed algorithm introduces a new partitioning of the markov model that allows a significantly more efficient exploitation of the cache locality. such optimization, together with an improved loading of the emission scores, allows the achievement of a constant processing throughput, regardless of the innermost-cache size and of the dimension of the considered model.

in what concerns the partitioning, the proposed implementation was based on the observation that the poor cache performance of hmmer is related to the size of the model and to the fact that it is necessary to update all the states in the model for every letter of a query sequence. as a result, large models will force recently computed values out of cache. when this phenomena occurs for every letter in a query, it naturally results in a significant bottleneck.

we speculate that a similar phenomena occurs for the striped pattern of farrar, in which case our partitioning technique could prove useful. still, farrar’s algorithm processes one single query at a time, instead of  <dig>  therefore, the slowdown should only occurs for models  <dig> times larger, i.e., models of size larger than  <dig> 

according to the extensive set of assessments and evaluations that were conducted, the proposed vectorized optimization of the viterbi decoding algorithm proved to be a rather competitive alternative implementation, when compared with the state of the art hmmer <dig> decoder. being always faster than the already highly optimized hmmer viterbifilter implementation, the proposed implementation provides a constant throughput and proved to offer a processing speedup as high as  <dig>  depending on the considered hmm model size and l1d cache size.

future work may also extend this approach to intel’s recent instruction-set extension avx <dig>  allowing the processing of twice more vector elements at a time.

availability and requirements
project name: cops project home page:https://kdbio.inesc-id.pt/~lsr/copsoperating system: linuxplatform independent programming language: crequirements: gcc, makelicense: a variation of the internet systems consortium  license.restrictions to use by non-academics: referencing this work.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
mf analyzed the problem and implemented the prototype, which was subsequently used for profiling and evaluation. nr and lr introduced the problem, along with an initial analysis, and recommended experimental approaches. all authors read and approved the final manuscript.

