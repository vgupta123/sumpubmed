BACKGROUND
the possibilities offered by next-generation sequencing  platforms are revolutionizing biotechnological laboratories, but to fulfill their true potential one hurdle has yet to be overcome: data analysis  <cit> . presently, getting a de novo, 454-based  sequence for a non-model species transcriptome or an illumina-based  genomic or transcriptomic resequencing of several samples is very affordable.

however, these new sequencing technologies cannot be analyzed with older software designed for sanger sequencing. both the quantity and quality of the data are very different  <cit> . a slew of new software and data formats are continually being created: assemblers , mappers  and file formats . these fast-paced developments have made the field of bioinformatics very dynamic and therefore difficult to follow despite the guidance provided by resources like the seqanswers internet forum  <cit> , which is dedicated to presenting and documenting the tools used to analyze ngs data. in addition, once the optimal tools are selected and the analyses are finished, huge files are obtained. these files are usually processed by creating small pieces of software called scripts. in our opinion, both the selection of the various programs and parameters as well as the creation of these small scripts render the analysis process cumbersome and non-reproducible, especially if the laboratory lacks a dedicated bioinformatics staff.

these problems can be relieved by using a standardized method and easy-to-use pipelines capable of integrally analyzing all the steps required to go from raw ngs sequences to the set of final annotations. some notable prior efforts in this area have produced several pipelines. three prominent examples are galaxy  <cit> , clovr  <cit>  and est2assembly  <cit> . est2assembly is a good assembly pipeline, but unfortunately it is unable to process illumina and solid  reads  <cit> , does not work in parallel and does not do read mapping. the applications of the ngs sequencing platforms are endless  <cit> . for example, up until recently, the use of snps in non-model species has been uncommon. however, the combination of ngs sequencing and affordable, high-throughput genotyping technologies , veracode or beadarray  <cit>  is facilitating the rapid discovery and use of these molecular markers. millions of sequences can now be generated at a low cost, and, given an easy way to analyze them, a huge number of snps can be rapidly obtained. this abundance of snps creates a need for new software, as researchers are usually interested in selecting an snp subset targeted at a specific experiment. the snps in these subsets should have a low error rate, should be adapted to the genotyping technique to be used and should be variable in the individuals to be genotyped.

we aimed to create a powerful yet easy-to-use application capable of performing ngs sequence analysis and polymorphism prediction. in order to build a truly useful tool, the software development was paired with a laboratory experiment: the search for snvs in tomato. in this species, due to the narrow genetic base present  <cit> , it has proven difficult to find highly polymorphic snps. several previous studies have mined the public tomato est databases  <cit> . in these studies, thousands of tomato snps were predicted between certain accessions. unfortunately, the polymorphism for these snps in other tomato accessions has not been assessed nor reported, rendering these snps cumbersome to use in other tomato materials. other approaches have been able to find tomato snps and report their polymorphism, but only a few hundred snps have been obtained by means of manual resequencing  <cit>  and oligonucleotide array hybridization  <cit> . the search for novel and highly informative snvs  by resequencing the tomato transcriptome will in and of itself be exceedingly useful for scientists and breeders working on this species.

the software developed along with its documentation is freely available under the agpl license and can be downloaded from the comav's bioinformatics service web site  <cit> , as well as in additional file  <dig> 

implementation
when the architecture of ngs_backbone was created, several characteristics were regarded as important: the use of standard file formats and third-party free software tools, modularity and extensibility, analysis reproducibility and ease of use.

to facilitate interoperability with other tools, most input and output files have a standard format, such as fasta, fastq, bam, vcf and gff, which can be produced and used by other tools. for instance, it is very easy to view the mapping and annotation obtained by loading the bam and gff files into a viewer, such as igv  <cit> .

ngs_backbone uses third-party tools of recognized quality, such as samtools or gatk whenever possible, in order to maintain the quality of the analyses. this approach takes its toll on the installation process, but in order to make it less complicated, we have packaged and precompiled most of these third party tools and have written a detailed step-by-step installation manual that is distributed with the tool  <cit> .

modularity was also an important design aim of the ngs_backbone architecture. users demand an ever-changing set of analyses to be carried out, and these analyses have to be adjusted for every project. to meet this requirement, a collection of mapper functions focused on different tasks, such as cleaning or annotating, were created. these functions have a common interface, they all take a sequence and generate a new, modified sequence and constitute the steps of our pipelines, which are generated at runtime for every analysis.

finally, even though we are presenting ngs_backbone as a command-line tool, this is not the only way to use it. the underlying library that powers this tool is called franklin and is written in python. this library has other capabilities that at this time are not exposed through the present command line interface, but its api is documented and easy to use, and python programmers willing to develop their own scripts and tools on top of it are welcome to do so. its development can be followed at the github website  <cit>  and its license is also open .

RESULTS
ngs_backbone pipeline algorithms
this section describes the methods used internally by ngs_backbone. the third-party software cited is not supposed to be run by the user, as it will only be used internally by ngs_backbone. only a couple of commands  will suffice to complete any analysis.

a typical analysis carried out by ngs_backbone starts with a set of sanger,  <dig>  illumina or solid read files. the first step is the read cleaning. in this process adaptor, vector and low-quality regions are removed. the exact algorithm used for every cleaning step depends on the type of read. for instance, quality trimming in the long reads is done by lucy  <cit> , but for the shorter reads, an internally implemented algorithm is used instead. for more details about the host of read-cleaning modules available, refer to the documentation distributed with the tool  <cit> . once the cleaning is finished, quality and length distributions can be created for the raw and clean reads as a quality assessment of the cleaning process.

if a reference transcriptome is unavailable, one can be assembled with the clean reads by using the mira assembler  <cit> . mira allows hybrid assemblies with sanger,  <dig> and illumina reads. ngs_backbone automates the preparation of a mira project. after running mira, the obtained set of contigs may be annotated with all available annotations: microsatellite, orf, functional description, go terms, intron location and orthologs.

once a reference transcriptome or genome is available, the reads may be mapped onto it. for the mapping, the algorithm employed also depends on the read length. short reads are mapped by the standard bwa  <cit>  algorithm, while the longer reads use that of bwt-sw. ngs_backbone generates a bam file with all the mapped reads in it. the generated bam files are processed using samtools  <cit>  and picard  <cit> , and are merged and adapted for gatk  <cit>  running a custom code.

one frequent objective of the projects that use ngs sequences is to look for sequence variation . to improve this process, a bam file realignment may be done by using gatk  <cit>  prior to the snv calling. for the snv calling, the reads with a mapping quality lower than  <dig> are not considered. the allele qualities are calculated by using the quality of the three most reliable reads  using the formula pq <dig> +  <dig>  * . this method is a slight variation of the one used by mira  <cit>  to calculate the quality for a consensus position. the snv annotation takes into account the accumulated sequence quality for every allele as well as the mapping quality for each read. a threshold is set for both parameters, and only positions with two or more high-quality alleles are considered as snps or indels. thousands of snvs are typically generated from these bam files, so in order to be able to select the most useful ones, a set of snv filters has been developed . the code used to run the snv filters was all custom code written for ngs_backbone. the snvs finally obtained along with the filter information are stored in a vcf file  <cit> .

although the analysis explained is a typical one, each of the steps is in fact optional. the pipeline has several entry points and results. one could start with raw sequences and do just the cleaning, or alternatively start with the bam file and use the tool to call the snvs. every analysis is independent of the others; it just takes a set of standard files as input and generates another set of standard files as output.

using the software, tomato ngs_backbone analysis
to test the tool, a complete analysis of the tomato transcriptome was carried out, from the read cleaning to the experimental snv validation. all these analyses were done using ngs_backbone. all public tomato sanger est reads available at the sgn and genbank databases  <cit>  with known tomato accession origins were included in this study. in addition to these sanger sequences,  <dig>  million illumina reads obtained from a normalized cdna library, built with an equimolar mix of floral rna extracted from the tomato lines uc- <dig> and rp75/ <dig>  were added .

after removing the low-quality regions and vector and adaptor contaminants,  <dig>  million illumina and  <dig>  sanger sequences remained. the most-represented tomato lines were micro tom , ta <dig>  and the rp75/59-uc <dig> mix . ngs_backbone calculated statistics about sequence features and the cleaning process .

the cleaned reads were mapped to the sgn tomato transcriptome  <cit> .  <dig>  million illumina as well as all sanger reads were mapped, obtaining an average coverage of  <dig>  for the sanger and  <dig>  for the illumina sequences . to improve this alignment, the realignment functionality provided by gatk  <cit>  was applied prior to the snv calling.

the snv annotation took into account the accumulated sequence quality for every allele as well as the mapping quality for each read. a threshold was set for both parameters, and only positions with two or more high-quality alleles were considered as snps or indels. all  <dig>  snvs found are reported in the vcf file .

despite satisfying the quality criteria, not all snvs seemed equally reliable. several filters were applied to tag those most likely to be real . for example, a most frequent allele frequency  filter was applied to the illumina set because a ratio between the alleles close to  <dig>  is expected in most cases when two equimolar cdna samples are mixed. in our case, the mix corresponded to the tomato lines uc- <dig> and rp75/ <dig>  and the alleles present in both of them were expected to appear in the ests an equal number of times for most unigenes. also, a filter that labeled the snvs in highly variable regions  was applied to avoid unigenes with too much variation. the  <dig>  snvs that passed both filters were considered to have a higher likelihood of being real and constituted the hl set . the snv counts presented from this point on will not include the snvs that did not pass these filters unless explicitly stated.

sa: sanger collection: snvs detected with sanger sequences.

il: illumina collection: snvs detected with illumina sequences.

hl: higher likelihood collection: snvs detected with illumina and sanger sequences.

when using snvs in an experimental setting, not all are equally useful and easy to use. depending on the technique used to detect them, several snv characteristics can ease or hinder an experiment, like closeness to an intron boundary, to another snv or to the end of the unigene. also, the snvs located in unigenes that are very similar to other unigenes were tagged to avoid gene families that could make following the pcr and primer design processes difficult. this was done by applying the unique and continuous region filters, i <dig> and cl <dig>  available in the ngs_backbone filter collection . all filters applied in order to label the snvs as well as the results obtained are shown in table  <dig>  the  <dig>  snvs that passed these filters made up the easily usable set .

sa: sanger collection: snvs detected with sanger sequences.

il: illumina collection: snvs detected with illumina sequences.

hl: higher likelihood collection: snvs detected with illumina and sanger sequences.

co: common collection: snvs detected in illumina and sanger collections.

po: polymorphic collection: snvs with an estimated frequency of most common allele under  <dig> .

eu: easily usable snvs set: snvs selected using ucr i <dig> and cl <dig> filters.

it is also desirable to tag the snvs with high polymorphism.

the main advantage of these highly polymorphic markers consists in their ease of use across different individuals. snvs with a low pic  have a low likelihood of having different alleles between two randomly chosen individuals. by enriching the selection with highly polymorphic snvs, the proportion of discriminating snvs in any experiment dealing with a random collection of individuals is increased, thereby reducing laboratory costs.

the polymorphism in a population can only be correctly inferred by having an extensive and well-genotyped sample of individuals. since ests convey genotyping information from different individuals, ngs_backbone does a crude estimate of the polymorphism for each snv by counting the number of tomato accessions in which each allele appears. the reliability of this inferred polymorphism depends on, among other parameters, the number of individuals sequenced. taking into account only the snvs sequenced in at least six different tomato accessions, the  <dig> snvs with a frequency for the most common allele under  <dig>  were included in the polymorphic  set. this set was small in spite of the good sequence coverage for four of the tomato accessions, as not many sequences were available from other tomato materials. the intersection of this po set with the easily usable one  produced  <dig> snvs.

to augment the number of putative highly polymorphic snvs, less stringent criteria were also applied, creating a new set with the variable snvs in both the illumina and in the sanger sequences, regardless of their estimated polymorphism.  <dig>  snvs were selected, of which  <dig> were also present in the eu selection . these snvs were denominated common , as they were polymorphic in the public est collection as well as in the illumina sequences. the snvs found only to be polymorphic in the sanger or in the illumina collections were named sa and il, respectively.

experimental validation of software predictions
the quality of the in silico snv calling was tested in a collection of  <dig> tomato accessions that included  <dig> commercial cultivars and  <dig> tomato landraces . the technique used to genotype these materials was hrm pcr   <cit> . to assign the melting curves to the snv alleles, the accessions rp75/ <dig> and uc- <dig>  which comprise the illumina est set, were used as controls when possible. when no polymorphism was expected between these accessions, restriction enzyme polymorphism  was used to differentiate the alleles.

a total of  <dig> in silico snvs were experimentally tested . the hrm technique was able to confirm 85% of these . this high success rate makes the use of the in silico-predicted snvs possible even without any previous extensive experimental validation. moreover, the success rate was with all probability underestimated due to the experimental technique used. hrm pcr is not able to distinguish all allele pairs, and it is quite likely that in some cases the failure to detect some of the in silico-predicted snvs was due to a flaw in the pcr.

a percentage of polymorphic markers: number of polymorphic markers with respect to detected hrm markers or total markers for each set.

b average frequency of most frequent allele of all detected markers or total markers for each set.

c pic  of all detected markers or total markers for each set.

this high success rate was achieved despite the low coverage employed , although it was probably obtained at the expense of a low specificity that was not assessed in the experimental design presented. the parameters used to do the selection were even adjusted so as to tag as unreliable some snvs that, even though they were supported by enough coverage, were in regions with high variability or that presented an allele frequency that was off balance in the equimolar rp75/ <dig> - uc- <dig> illumina sample.

one of the aims of this study was to devise and test a strategy for selecting the most polymorphic snv subset by using both the publicly available as well as the new illumina ests. although it is not possible to do an accurate pic estimate just by using a collection of public sequences gathered from different heterogeneous projects, a rough index related to polymorphism might be calculated by counting the number of individuals in which each allele appears. despite several confounding factors, a low pic snv will tend to produce very off-balance individual counts for the different alleles. the expected mean polymorphism of the svn sets with different pics was estimated by genotyping  <dig> tomato accessions. two snv sets were used to define the polymorphism baseline to expect. the only polymorphism-related filter applied to these sets was the requirement of having at least two different alleles in the illumina or sanger sequences. once the tomato collection was genotyped, using snvs randomly selected from these sets, we found that  <dig> out of  <dig> snvs tested in the sanger set and  <dig> out of  <dig> in the illumina set were polymorphic, which is to say that the most frequent allele frequency was lower than 95%.

other snv sets that were expected to be somewhat more polymorphic were those built by sieving the snvs that were polymorphic in both the sanger and the illumina sequences  as well as those from the po set . in both sets, 70% of the markers tested were polymorphic, which was clearly higher than the 21% and 42% found in the polymorphism baseline.

in these sets, the polymorphic information content  was also expected to be higher than the one found in the sanger and illumina sets, where pic was  <dig>  and  <dig> , respectively. in the co set, the pic was in fact higher,  <dig> . lastly, the snvs that were expected to be most polymorphic were the ones from the po set. in these, the sequences from at least  <dig> plants were available and the allele count was quite balanced. the pic found, in this case, was  <dig> , so when looking for highly polymorphic snvs, this final strategy pays off.

unfortunately, a selection like this cannot be done directly in all non-model species with public ests, as in many cases almost all sequences come from just a handful of different individuals. in fact, not even in public tomato sequences is there much diversity. 81% of these public sequences came from just  <dig> individuals. given the results shown, we would recommend that, when looking for snvs, the number of individuals sequenced be taken into account.

CONCLUSIONS
to analyze ngs transcriptome data, we have developed a highly configurable, modular and parallel pipeline written in python named ngs_backbone. this software presents a new strategy for using ngs technologies that will speed up research in non-model species and facilitate the use of these technologies by laboratories with or without a specialized bioinformatics staff.

in the tomato example presented, the analysis started with  <dig>  million reads, which, after being cleaned and mapped, yielded  <dig>  putative snps and indels .

according to experimental validation, 85% of the in silico-predicted snvs were real. this high success rate makes the use of the in silico-predicted snvs possible even without any previous extensive experimental validation. in addition, the collection of  <dig>  highly polymorphic snvs created will be a useful resource as tomato landraces and vintages have a narrow genetic base, making it quite difficult to detect polymorphic markers in these materials.

the ngs_backbone software provides an ideal way to carry out a complete analysis on ngs sequences, including read cleaning, mapping, transcriptome assembly, annotation and snv calling. this is an open-source tool released under the agpl, written in python and available at the comav's bioinformatics service web site  <cit>  in addition, the underlying library that powers ngs-backbone is called franklin. its development can be followed at the github website  <cit> . its api is documented and easy to use, and python programmers willing to develop their own scripts and tools on top of it are welcome to do so.

availability and requirements
project name: ngs_backbone

project home page: http://bioinf.comav.upv.es/ngs_backbone/ and http://github.com/joseblanca/franklin

operating system: linux

programing language: python 

other requirements: please refer to website for full listing.

license: open source, agpl

restrictions of use by non-academics: none

authors' contributions
lp obtained the experimental data and participated in the analysis. jc designed the study and experiments and participated in the analysis. jmb and pz developed the ngs-backbone software and participated in the analysis. fn selected and handled the plant material. jmb, lp, fn and jc wrote the manuscript. all authors read and approved the final manuscript.

supplementary material
additional file 1
ngs_backbone  <dig> . <dig> software. ngs_backbone  <dig> . <dig>  last version, released on 31-08- <dig> 

click here for file

 additional file 2
materials and methods. materials and methods, for the experimental software validation.

click here for file

 additional file 3
vcf data file containing all snvs identified. data file containing all snvs identified. the snvs have not been selected with any additional filter, but the file includes the data for all ngs_backbone filters used in this work.

click here for file

 additional file 4
accessions employed in the snv validation. landraces were provided by the comav genebank, comercial cultivars were obtained from semilleros cucala agricola .

click here for file

 additional file 5
primer sequences. file with primer sequences used in this work.

click here for file

 acknowledgements
the authors thank the comav for kindly providing the tomato accessions. the help of joshua bergen in improving the english of this manuscript is also gratefully acknowledged.
