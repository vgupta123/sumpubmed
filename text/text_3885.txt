BACKGROUND
randomization methods are techniques for significance testing that are based on generating data that shares some of the same properties with the real data, but lacks the structure of interest. for example, if we are interested in predicting a target variable on the basis of some explanatory variables, then we can randomize the target variable to remove any real connection between the explanatory and target variables. the prediction method is run on randomized data, and the accuracy of the resulting classifier is noted. this is repeated for, say,  <dig> randomizations, and the accuracy of the classifier obtained on real data is compared with the results on randomized data to obtain an empirical p-value. see  <cit>  for an overview on using randomization methods for significance testing.

a randomization method is based  on a null model, i.e., a description of what the data would look like in the absence of the pattern of interest. in the example above, the null model states that the data looks like the original data, except that the target variable is random . a well-studied example of a null model is in the context of 0- <dig> matrices, where one can consider the class of matrices having the same row and column sums as the original data  <cit> . in the realm of gene expression data, 0- <dig> matrices can be produced by discretizing data into differentially and non-differentially expressed values. using the null model to maintain the number of 1s in the columns and rows in significance testing tells whether the data analysis result is caused just by the row and column sums, i.e., the count of differential expression values for genes and samples.

permutation testing has been widely used in biological studies, as it is a natural fit with comparative clinical trials . straightforward permutation methods have, however, a limited scope, but a larger variety of problems can be tackled by using computationally more advanced methods. advanced methods, e.g. markov-chain monte carlo based algorithms, have had success in fields such as ecology  <cit> . ecological data cannot in most situations be produced using statistically controlled procedures such as replicates and comparing experimental samples to control samples. in molecular biology similar challenges are faced especially when using high-throughput measurement instruments. as an example, null models have been used in determining the significance of co-occurrence patterns in studying potential transcription factor binding sites  <cit>  or generic time-dependent variation of gene expression  <cit> .

nowadays vast amounts of data are produced by using microarrays, with the intent of detecting complex patterns such as periodic gene expression. periodic expression is central in many fields, e.g., in cancer research and in circadian rhythm research. in analyzing possible periodicity of gene expression, testing the significance of the results is quite difficult: as there are thousands of genes, it is likely that at least some of them will show periodic behavior by chance. in general high-throughput exploratory methods such as dna microarrays are very prone to misinterpretation  <cit> .

gene periodicity studies measure expression for a large set of genes in a series of time steps, covering typically one or more assumed gene cycles. due to practical reasons sampling interval is often long and a control experiment repeating the same measurement on non-induced cells has not been performed. microarray measurements are laborious and expensive, limiting the amount of samples that can be made. emerging rna-seq methods are poised to replace expression microarrays, but for the foreseeable future the amount of samples is equally limited with them.

there has been discussion on the true interpretation of gene periodicity results since the first two experimental yeast studies  <cit> . the first studies suggested that  <dig> to  <dig> genes are periodically expressed. however most notably cooper et al. have suggested that data produced in the yeast studies does not warrant such strong conclusions about gene periodicity  <cit> . they stress the importance of careful data analysis and interpretation, calling for new methods of significance testing.

methodological research in periodicity studies has concentrated on producing alternative approaches for identifying periodicity in gene expression time series. proposed methods vary from simple fourier analysis to more elaborate mathematical and statistical analyzes  <cit> . as it is often in rapidly developing fields, computational methods for verification of results are developed more slowly. there has been no critical studies on methods of periodicity significance testing, until the interesting study by futschik and herzel  <cit> . they proposed a background model of gene expression based on autocorrelative random processes. using the background model they were able to improve the quality of significance detection, compared to random permutations or gaussian models. an important aspect of the futschik and herzel study was that they discussed the assumptions and justifications of their background model, which had not been done in previous work. obviously a single study can cover the area only partially and the important significance testing question should be studied further. in biomedical sciences, periodicity studies have continued since, with the focus moving from gene cycle to circadian cycle . however on the method development arena new developments have been scarce.

in this study we present null models for significance testing. the null models are formulated as randomization methods, i.e., algorithms for generating datasets. generated randomized datasets share certain characteristics with the original dataset, so that they are realistic samples for calculating how extreme values of periodicity we should observe by chance. first we define the randomization methods and describe how they can be applied to periodicity analysis. next we review the differences in the results produced by different methods, and last we discuss the implications of our results for microarray studies and other fields of high-throughput biosciences.

methods
we present the significance testing approach and the randomization algorithms to be used within the framework. first we describe how periodicity can be measured from gene expression and how measured periodicity is interpreted statistically.

measuring periodicity
as a measure of gene periodicity we use a simplified version of the fourier score approach used by  <cit> , based on a fourier analysis of the gene expression time series. while several different approaches to periodicity detection have been discussed in the literature, the original fourier score approach is the most commonly used and also was found to perform better than more intricate methods  <cit> .

we consider time series matrices produced by measurements of gene expression at consecutive time points. let the gene expression matrix e have n rows and m columns, corresponding to n genes and m sequential time points. the matrix has been produced by measuring expression of each gene at uniform intervals during c gene cycles.

periodicity score f for gene g at frequency w is defined in equation . here e is the normalized expression value of gene g ∈ { <dig>  ..., n}, at time t ∈ { <dig>  ..., m}.   

for normalization we use the same procedure as futschik and herzel  <cit> . first missing values are imputed with knn-imputation. then each row is shifted to have mean  <dig> and scaled to have standard deviation  <dig>  these normalization steps are done to original and randomized data before calculating periodicity scores  <cit> .

detecting significant periodicity: empirical p-values
periodicity scores allow us to compare periodicity levels between genes. to identify genes that are to be considered periodic, we need a way to decide which periodicity scores are significantly large. in the original studies periodicity was decided by using a threshold for the score  <cit> . threshold is selected in an ad hoc manner by looking at some characteristics of the dataset, and selecting a value that gives results that are considered to be reasonable.

using a fixed periodicity score threshold does not give us any approximation on how likely it is to detect the given level of periodicity by pure chance. for estimating how unlikely a periodicity score is, we compare it against the distribution of scores that we would probably see when only non-induced genes are measured, i.e., against the null distribution. p-value is derived by comparing score in the original dataset to the null distribution. to define and estimate the null distribution, we use randomization based null models that are described later.

let f be the periodicity score of row x, let g be the original row and let  be the r randomized rows. empirical one-tailed p-value for the periodicity of row g is defined by the equation .   

if the empirical p-value is small, then we can conclude that under the null model it is unlikely that the observed periodicity of g is a product of luck. however when multiple observations are made, the probability of detecting periodicity by chance increases. therefore we need to employ multiple testing correction. the traditional way to handle multiple testing correction is to control family-wise error rate , using for example holm-bonferroni method  <cit> . fwer is typically considered too strict for dna microarray studies  <cit> . it controls the probability of making even a single error, resulting in very narrow rejection regions. microarray studies increasingly use less strict approach of controlling false discovery rate , i.e., the proportion of mistakes, with the benjamini-hochberg method  <cit> . it will also be used here as one of the two multiple testing methods.

an important caveat of the benjamini-hochberg method is that it controls the expected rate of false discoveries and can have suboptimal performance in permutation testing  <cit> . as a randomization based null model can be readily used to generate either cyclicity scores from the null distribution or empirical p-values, majority of available multiple testing approaches can be used. unfortunately the more advanced solutions, such as those from  <cit>  or  <cit> , are more complicated. as multiple testing control methods are not the focus of this study, we use the more simplistic benjamini-hochberg method.

detecting significant periodicity: futschik and herzel threshold
futschik and herzel use an alternative approach to significance testing  <cit> . they do not calculate empirical p-values, but instead use the raw periodicity scores produced by the randomization methods. equation  below defines the futschik-herzel empirical false discovery rate fh for threshold f. the equation is based on periodicity scores in randomized data f and on periodicity scores in the original data f. count of scores greater than or equal to f are calculated over all genes g ∈ { <dig>  ..., n} and randomized matrices r ∈ { <dig>  ..., r}, where r is the number of randomized matrices produced.   

here  is one when the condition p is true, and zero otherwise. by using equation  it is trivial to select f so that fh = q, where q is the desired false discovery rate. this approach is similar to threshold selection in  <cit> , but it is important to note that futschik and herzel do not manually tune the score threshold f, but instead select q and derive the value of f by using the equation. they do not discuss the statistical justifications of the method, or specify the underlying assumptions or the exact nature of the provided control for "empirical false discovery rate".

randomization methods
empirical p-values provide a way of using a null distribution of gene expression matrices to assess the significance of periodic signals in the original measured data. to produce null distributions of expression matrices, we describe four techniques for generating randomized n × m matrices e'; that share certain characteristics with the gene expression matrix e. the methods either use the original dataset as the data to be randomized or calculate parameters of a data generating model from the original dataset  <cit> . the methods are illustrated in figure  <dig> 

p: permuting all entries
randomized matrix  is generated by permuting the values of all entries in matrix e. the result  maintains the original value distribution of e, but does not maintain, e.g., the row and column distributions.

it is obvious that permutations are a drastic randomization method and do not preserve much of the structure in the dataset. they are used in periodicity studies, as in the seminal paper by spellman et al., where they argue that an upper bound for false positive count can be achieved with permutation randomization  <cit> .

r: permuting within rows
randomized matrix  is generated by permuting the values within each row of e. the result  maintains the original value distribution and the row value distributions. a row in  is dependent only on the corresponding row in e.

spellman et al. argue that an upper bound for false positive count can be achieved with randomization by permuting within rows  <cit> , so the actual false positive count would be between the amount of false positives produced by methods p and r. our results indicate that both of the methods should be considered optimistic: they produce only loose lower bounds for the number of false positives.

a: autocorrelation
cell cycle time series data often exhibits autocorrelation due to the procedure employed to bring cells to a synchronized state in the beginning of the time series. in a method proposed by futschik and herzel gene expression is modeled as an autoregressive ar process for which the value of expression at time t depends linearly on expression at time t -  <dig> up to a normally distributed random variable  <cit> . specifically, xt = a <dig> · xt- <dig> + zt, where xt is the time dependent random variable, i.e., gene expression, while a <dig> is the first order autocorrelation of xt and zt is the independent normally distributed random variable.

as zt is a random variable, we can easily calculate a <dig> and the variance of zt from the original data matrix e and use them to generate a random matrix  with the same autocorrelation and variance of zt. yang and su have suggested using multiple methods simultaneously for estimating autocorrelation coefficients of circadian gene expression data  <cit> . in our experiments we did not see non-marginal differences when using other coefficient estimators and for that reason used the yule-walker method that was also used by futschik and herzel  <cit> .

one aspect of the futschik and herzel approach is that variance of the whole row is normalized, but the data generating model preserves only variance of the normally distributed random variable zt. with datasets of this study, we typically see variances less than the original in randomized data. when variance is normalized after randomization, the data is effectively scaled up on average. as fourier score is used for periodicity detection, also the periodicity scores are scaled up. unlike the other methods,  does not maintain the original value distribution of the matrix elements. for a detailed description of the method we refer to supplementary materials of the original article and especially to the related r-package  <cit> .

s: splitting rows into parts
we propose a new randomization method that takes a gene cycle and splits it into two equal sized parts referred to as the prefix and the suffix. then we simply randomize the connection between prefixes and suffixes, i.e., each suffix is randomly assigned to follow a prefix.

more formally, assuming there is only one gene cycle, a row g in the randomized matrix  with m time points is generated as  

here index h is chosen at random without replacement from { <dig>  ..., m}. in the formula, we combine rows g and h in the original data to produce a randomized row. when there are multiple gene cycles, we split each cycle into prefixes and suffixes. then again, we combine the prefixes of row g with suffixes of row h to create a new row. each row in the randomized data contains values from two rows in the original data, except for the rare case that g = h, when the randomized row is actually equal to a one row in the original data.

interpreting the null models
we motivate our randomization method by interpreting the methods as null models that produce random samples from a null distribution. what we would like to get are samples from a situation where the same genes are measured under the same circumstances, but without a treatment that is assumed to induce cyclic expression.

the randomization method p can be interpreted as sampling each expression value from a global null distribution. the strength of the method is that there are a large amount of individual values in a microarray experiment and thus we have a robust sampling distribution. however by sampling each value independently we completely eliminate dependencies between values of a single gene. in time series data consecutive values are naturally strongly dependent and a realistic null model cannot be produced without taking these dependencies into account.

in expression data there are differences between the average level of expression of different genes, but complete permutation makes data uniform. the method r improves on p in this aspect: now we sample each gene from its own distribution, preserving differences between average levels of expression of the genes. unfortunately, there are very few values per gene and building a sampling distribution from, say,  <dig> values is not robust. besides that, the distribution of values for induced and non-induced genes can be very different, and permuting expression values of an induced gene does not necessarily produce an expression profile that a typical non-induced gene might have. dependencies between values of consecutive time points for a single gene are not taken into consideration with r, either.

it is possible to sample genes by taking their average level of expression and adding normally distributed noise, with the same variance than in original data. this approach assumes that non-induced genes produce constant level of expression, excluding the noise. however processes not related to cyclicity can produce varying expression patterns that are not captured in this model. the randomization method a takes into account some of the patterns by adding ar autocorrelation to the model. ar autocorrelation can capture certain stress response patterns that the treatment induces and are argued to be a major feature of gene cycle data  <cit> . formulation of a does not, however, preserve the average level of expression for each gene, but the level is mostly decided by the rather arbitrary first measurement value. also it is not known how well stress response is modelled by ar autocorrelation: the model can only capture a linear pattern that covers the whole timeseries, but the actual stress response pattern might be non-linear and only cover the first timepoints of the dataset. an open question is also what other patterns of non-induced expression should be taken into account.

as can be seen, producing a general null model of gene expression is very difficult. and what is more important, development of null models has been progressing from simple to more advanced. each new feature that is taken into account can make the model more realistic, but we do not know when the model is realistic enough. the idea of significance testing with p-values is to assume the absence of the effect or pattern of interest , to gather evidence to invalidate null hypothesis in favor of an alternative hypothesis and to assess the probability that the evidence was a product of luck. as null hypothesis is the starting assumption and we are producing evidence against it, data should be analyzed in a conservative way. if there is a bias in our methods, it should be towards conservative interpretation. a null model that exaggerates the randomness of the null distribution clearly contradicts with the statistical framework.

method s maintains the original value distribution, value distributions of each column, and also the correlations between columns inside prefixes and inside suffixes. the method is based on the assumption that many of the genes are non-induced and hence the distribution of each column is close to the distribution of a non-treated sample. we take into consideration that data can contain global patterns such as stress response. values are not mixed across timepoints and hence global stress response patterns are not removed, but experiment wide cyclic structures are. the method can produce a small amount of cyclic genes to the randomized data by combining two cyclic genes, with matching phases, in the original data , which produces a slight conservative bias.

we point out that method s does not take into account the known fact that there are correlations between genes. however our model is a conservative null model for gene periodicity detection: correlations do not play a role when the periodicity of each gene is measured independently. if we would be assessing the significance of, say, clustering, then correlations should be taken into account.

analysis of periodicity score distributions
this section analyzes the theoretical properties of the randomization methods. reading this section is not necessary for understanding the rest of the article.

to better understand results produced by different randomization methods, we investigate the theoretical properties of the periodicity scores produced by them. this analysis shows clearly how the commonly used randomization methods cannot take the time component or the sampling frequency of periodicity data into account even though the methods are used for analyzing exactly the time-based structure of the data.

a perfect cyclical gene has values  for t =  <dig> , <dig>  ..., m - 1; let ck = ). denoting by e = n × m a dataset, and by eg the gth row of e, let eg · ck be the dot product of the vectors eg and ck. consider the slightly modified kth periodicity score for the gth row eg of e defined by  

we use this modified version of the periodicity score to make the results more easily understandable while still retaining all the important characteristics of the metric. the periodicity score of ck is . therefore periodic genes receive the same score regardless of the data size and sampling frequency.

by straightforward calculations based on these definitions it is possible to analyze the periodicity scores given by each of the randomization methods. in the following we express the expectations and variances of periodicity score distributions. details for the results can be found in the additional file  <dig> 

periodicity scores when randomizing by elements
for the p method that permutes all the data elements at random the expected value and variance of the periodicity score for any gene in the randomized data are  

in the equations  and  denote the average values of matrix e's elements raised to power  <dig> and  <dig> respectively. the statistics of the values generated by the p method do not have any connection to the time component of the data, but only to the value distribution of data. this can be restated so that the method simply calculates the whole signal energy , with noise included, contained in the data set. however only the fluctuation corresponding to some cyclical structure should be accounted.

it can also be seen that as the number of samples in data m grows, both the statistics become smaller and smaller. this is true, however, only if the data contains no cyclical components. in crude terms, any actual cyclical structure in data supports the periodicity score and prevents it from sinking significantly lower than some given level as m grows. therefore, as seen in figure  <dig>  the p-values for data with cyclical structure move consistently toward zero as m grows.

for the r method that randomizes each row separately we reach similar results. the statistics of the periodicity scores of the randomized data are  

 here again, the statistics do not depend on anything except the value distribution of the data without any regard to the order or placement of the elements. these results on the theoretical properties of the p and r methods make them questionable if any consideration should be given to the actual dataset or the purpose of the experiments.

periodicity scores produced by a: autocorrelation
in the randomization method of  <cit>  the values of a data row  <dig> × m are generated as values of a martingale: x <dig> = z <dig>  x <dig> = αx <dig> + z <dig>  x <dig> = αx <dig> + z <dig>  ..., xm- <dig> = αxm- <dig> + zm- <dig>  we require that |α| <  <dig> and the zt are i.i.d. samples from some distribution  with . let us present the data signal as its fourier decomposition, i.e., as a sum of different cyclical sine signals . we may then calculate the autocorrelation of the signal to equal  

here the values αk give the amount of cyclicity the signal has for each different frequency. the expectation and variance for periodicity scores of data samples generated with this autocorrelation method of  <cit>  can be shown to be   

in the equations the notation  means that the error term has absolute value at most some constant times f.

the periodicity scores of the generated samples depend on  since the more the data fluctuates the higher is the variance  chosen in the method and the more probable it is for any generated sample to receive high periodicity score. furthermore the time component and the periodicity of the original gene data has influence on the samples' periodicity through the autocorrelation measure α. to further simplify the equations one can note that for a strongly periodic gene its α is practically equal to for  some single k.

the statistics depend on m the same way as for methods p and r, decreasing as the number of samples grows, because for higher sampling frequencies the data fluctuates rather slowly  and the high frequency noise is more easily distinguished. this also exemplifies how a measures the significance of gene cyclicity against a background of pure noise. figure  <dig> illustrates the behavior of the p-values with increasing sample counts.

periodicity scores produced by s: splitting rows into parts
we now make an assumption that cyclical signals are cyclical in all parts of the signal, without turning into pure noise at any point. if we denote by αgk the kth fourier coefficient of the gth gene, i.e., the gth gene has fourier decomposition , then we can express the periodicity score statistics as  

these values indicate that the cyclicity of the genes on each of the frequencies gets randomized separately, since the statistics depend only on the current value of k. on the contrary to the case of methods p, r and a, the statistics do not diminish with longer sample sizes or higher sampling frequencies. this happens, because the s method focuses on detecting genes that are exceptionally cyclical in the context of all the genes in the given dataset, not against an absolute background of pure noise as the other methods do.

summary of analysis
we illustrated how s is the only method using the background distribution of the analyzed genes instead of a pure noise background. we also showed how the behavior of p, r and a changes with an increase in the sample count. methods p, r and a suffer from high noise levels in data whereas s dissects the data and considers each frequency separately, which is also how the actual test score is calculated. finally the p and r methods cannot even be seen as actual periodicity tests since they ignore any periodicity in the data and redistribute only the signal energy of the data, no matter whether it arises from periodicity, other meaningful patterns or from random noise.

RESULTS
in the previous section we analyzed the four different randomization methods discussed in this manuscript. the analysis showed that the method s has sound theoretical foundation for assessing the significance of periodicity. in this section we will back analytical results with experiments. we will also compare two different multiple hypothesis testing approaches and discuss their differences.

all methods were implemented with matlab and experiments were run with a standard workstation pc. matlab code is available from a companion web site at http://www.cs.helsinki.fi/u/akallio/periodicity/.

datasets
results in rest of this section are based on gene expression data from well-known yeast gene cycle studies  <cit> . the goal of those studies was to identify the subset of genes that are cyclically expressed. for comparison we include a different type of dataset where majority of the genes show strong cyclic pattern, from a malaria protozoan study  <cit> . a summary of the datasets and their characteristics is presented in table  <dig> 

the dataset alpha2c has been produced by bringing the whole yeast cell culture to a hypothetically synchronized state using alpha-factor arrest. the dataset alpha1c is a version of alpha2c with only the first cycle included , mimicking a very low quality dataset. datasets cdc <dig> and cdc <dig> are based on temperature arrest of cdc <dig> and cdc <dig> temperature sensitive mutants. contrary to the other yeast datasets, the dataset elu is based on elutriation where instead of external treatment a subset of hypothetically synchronized cells was collected from the population. the dataset malaria is based on a synchronized in vitro culture.

we could use the hypothetical numbers of gene cycles reported for each of the datasets, but the accuracy of periodicity calculation can be improved by detecting the number of cycles from the dataset  <cit> . for that we find cadj, where  <dig> <cadj ≤ 2c and c is the hypothetical number of cycles. we iterate with  <dig>  increments and choose cadj so that it maximizes the sum of periodicity scores for all genes. chosen values of cadj are reported in column detected cycles of table  <dig> 

comparison of randomization methods
as our initial test of the methods, we studied the behavior of the periodicity score on randomized data. we computed the scores for each row in each dataset, and selected for each dataset three thresholds δs so that a fraction of s =  <dig> , s =  <dig> , or s =  <dig>  of the genes have scores higher than δs.

for each dataset and each method we then generated  <dig> randomized datasets, and computed the fraction of randomized data rows that have periodicity scores higher than the threshold. if there is no periodic structure in the data, the distribution of the periodicity scores in the randomized data should be about the same as in the original data, and hence the fraction of randomized rows with scores above δs should be about s. on the other hand, if the data has strong periodic structure, the distribution of the scores of the randomized rows should be clearly different from the distribution of the scores in the real data, and the fraction of genes with scores above δs should be less than s. the ratio of randomized genes labeled as periodic is reported in table  <dig> for each combination of dataset and randomization method.

periodicity is determined by thresholds δs determined so that a fraction of s of the genes in the original dataset have scores above δs.

on the other hand, results for method s are different for different datasets: for datasets alpha1c elu, and malaria the fraction of genes with high scores is the same in the randomized data and in the real data, while for alpha2c, cdc <dig>  and cdc <dig> the randomized datasets have fewer genes with high scores. thus method s indicates that only in the datasets alpha2c, cdc <dig>  and cdc <dig> there is clear evidence of periodicity. these assessments produced by method s are consistent with the previous results. elu was found to be low quality already by the original authors  <cit> . alpha1c was artificially constructed to be an example of a dataset with too few timepoints for reliable periodicity detection. the dataset malaria can be seen to be a special case, and it is discussed next.

limitations for generating non-induced expression
the malaria dataset is an example of an atypical microarray dataset and it is not a good fit for randomization methods presented in this study. this is because we are measuring here the count of genes that have significantly higher periodicity scores than given by the null model. malaria dataset contains mostly periodic genes, so it does not contain much information about gene expression of other types than of the synchronized periodic pattern. simple permutations p and r do not preserve much of the structure of the original dataset and compare strongly periodic original dataset to noise mostly, resulting in no periodic genes to be detected in the randomized data. method a is based on the assumption of gaussian expression with ar autocorrelation, and does not preserve much of the structure in the original data either. as was previously shown in the analysis, method s is different in this regard. it uses the dataset to build a model of all expression patterns without other major assumptions about the data, and as the dataset is very biased, so is the model. s reports that there is no periodicity in the data that cannot be explained by chance alone.

the advantage of microarray experiments is that if we are studying the behavior of maybe some dozens of genes, the dataset will contain hundredfold of non-induced genes to compare to  <cit> . as was shown in our analysis, the method s can be used to detect genes that are exceptionally periodic. however, the assumption about large numbers of non-induced genes is not true for malaria. for that reason we do not consider any of the results for malaria realistic, but hold the view that the recommended solution would have been to conduct more measurements to get data on typical expression patterns in the case of no treatment. if such data is not available, then additional assumptions have to be done to roughly approximate the null distribution of the periodicity score. assumptions behind the method a are not realistic in this case. level of autocorrelation and variance of gaussian distribution are calculated from data, which now contains mostly induced genes. therefore calculated parameters are not necessarily realistic for non-induced genes.

the number of periodic genes
to estimate the number of periodic genes in the datasets, we use the above randomization methods to produce randomized samples and apply both the benjamini-hochberg procedure on empirical p-values and the method of futschik and herzel on periodicity scores. for validation we use a benchmark gene list from  <cit>  that presents three benchmark sets of genes for the yeast periodicity studies, corresponding to datasets from spellman et al. and cho et al.. the first benchmark set comprises of  <dig> genes that were confirmed to be periodic in small scale laboratory experiments. the other two sets are based on less decisive methods, so we restrict our comparison to the first set only.

for comparing the methods we use the positive predictive value , which was also used in  <cit> . for a given result list the value is defined as   

here tp is the count of true positives, i.e., the number of genes that the method considers periodic and that are found from the small scale experiment benchmark set. fp is the count of false positives, i.e., the number genes that the method reports as periodic, but which do not occur in the benchmark set. it is important to bear in mind that the benchmark set does not contain all periodic genes, so the absolute value of ppv has little meaning, but the relative difference between values indicates a difference between the quality of the two candidate gene lists. also for this reason only one variable should be changed at a time when doing the comparison. the ppv and total count of positives is reported in table  <dig>  as ppv is not robust when the number of positives is small, and undefined when there are no positives, we omit the value when there are less than  <dig> positives.

values are calculated using fdr levels  <dig> ,  <dig>  and  <dig>  with different control procedures.

when comparing results with those of futschik and herzel, we need to remember that there are differences in wavelengths that are used for fourier calculations. we detected the number of gene cycles  from the data, whereas futschik and herzel used the wavelength from the original study by spellman et al.. for the cdc <dig> dataset they are the same and hence our results are also practically identical to the futschik and herzel study. the small differences are due to different knn-imputation implementations and slight difference in wavelenght that is not visible due to rounding. for cdc <dig> the detected wavelength differs and that yields differences especially for the stricter thresholds; at fdr  <dig>  they are already quite close .

when looking at fdr control parameters, the typical fdr threshold  <dig>  produces the best ppv values, while the more relaxed fdr  <dig>  is already too forgiving, at least when compared to the validation data. the fdr threshold  <dig>  is very strict for this kind of study and that can be seen from the results also. there are no differences between the fdr control methods: the futschik and herzel method produces practically identical results to the benjamini-hochberg method.

the poor quality of the artificially constructed bad dataset alpha1c is identified by both a and s methods, but not by the simpler permutation methods. that same deficiency is even more exaggerated for the elu dataset that was found bad in the previous section and also by spellman et al.  <cit> : the simple methods p and r report over hundred significant genes already at fdr  <dig>  and scale up to thousands at higher fdr levels. judging by this, these two methods cannot be recommended even for simple sanity checking of the data.

for the good quality datasets alpha2c and cdc <dig>  both a and s give more conservative results that have better ppv values. the method s is systematically more conservative than a, but also gives a better ppv value in every case.

we can examine these differences closer by looking at null distribution of periodicity scores produced by the two methods, when compared to original distribution from data. figure  <dig> shows randomization generated null distribution together with distribution from data in the case of cdc <dig>  the vertical line shows threshold of significance as decided by the benjamini-hochberg method at fdr  <dig> . the method s produces a null distribution that very closely follows the distribution in original data, except naturally for the bump at the high end, i.e., where alternative distribution of periodic genes mixes with the non-periodic ones. method a produces a null distribution that is biased towards the low periodicity scores, as the peak of the distribution is too far in the low end. in the score range  <dig> to  <dig> the null distribution drops faster than the original. this probably cannot be explained by the periodic distribution mixing with the non-periodic distribution, because it seems highly unlikely that non-trivial portions of the periodic distribution extends to that range. as the null distribution is systematically off the mark for the low and mid ranges, it does not seem plausible that a could be used to reliably separate the periodic and non-periodic distributions in the high range. a threshold decision based on the method s is the one that is supported by figure  <dig> 

the methods a and s disagree on the cdc <dig> dataset. method s asserts only two genes to be significant even on the high fdr levels, while the method a gives more optimistic estimates. the case is analogous to dataset alpha2c, where both give zero significants at fdr  <dig> . the only difference is that cdc <dig> is given more conservative assessment, to the point that s does not accept more than two positives at fdr  <dig> . however at fdr  <dig>  also method s would have already given  <dig> positives and we could more easily observe similar trend than in alpha2c. to examine this further, we again plot the distributions produced by the two methods in figure  <dig>  when compared to the dataset cdc <dig>  the bump of periodic genes is not pronounced at all and it seems that the area where significant scores are located is populated mostly by genes from the non-periodic distribution. in this dataset method s still produces a distribution that more closely follows the original one, though the difference to method a is not as large. the peak of the distribution is somewhat too far in the low end for a, and perhaps slightly for s also. when looking at the null distribution produced by s, it is easy to understand why only two genes were declared significant. the two distributions of induced and non-induced genes are so mixed that reliably separating them with a threshold does not seem possible. method a paints a more optimistic picture and admittedly it is not far off the mark in the distribution graphs, but still an optimistic bias is visible. it casts doubts on the much larger numbers of positives reported by a.

as a summary, we demonstrated firstly how previously used permutation methods p and r give overly optimistic results, and secondly, how also the more recent and more advanced autocorrelation method a has optimistic tendencies and better results can be obtained by the more conservative method s. the two multiple testing control methods had identical performance. hence we can recommend the benjamini-hochberg method, as it is established and has a well documented theoretical basis, unlike the method used by futschik and herzel.

discussion
microarray data is commonly noisy and can be plagued by effects not related to the biological question under study. it is therefore crucial to focus on null models of gene expression, i.e., what type of patterns we expect to see by chance. typically most of the attention is given to careful formulation of the pattern we are looking for, such as formulation of a periodicity score. however, equal amount of attention should be given to formulation of the null model, i.e., defining what we expect to see when there are no patterns that we are interested in. construction of traditional parametric statistical models for periodicity studies or any other complex and novel area of bioinformatics is very difficult. randomization methods are an viable alternative for analytical significance testing.

however, simple randomization tests, as simple permutations applied to gene cycle data, do not produce meaningful information. comparing any real data with noise will assess patterns in the data significant. so it is more important for a randomization test to retain the realistic general structure of the data than to remove all structure under study. otherwise the result will be too optimistic.

originally it was estimated that for  <dig> to  <dig> genes the periodic signal is strong enough to be considered statistically significant  <cit> . in our analysis we found out that data contains evidence of periodicity only for a smaller number of genes, when using the same datasets as an evidence. this does not mean that there would be only a small number of cell cycle regulated genes, but that their robust identification requires more effort. a recent meta-analysis integrated large number of different datasets and reported more than  <dig> percent of genes in fission yeast to be periodically expressed  <cit> . that demonstrates how improving the data in quantity and quality allows to identify larger number of periodic genes. besides increasing the number of timepoints, an obvious improvement in periodicity studies is to perform control experiments to get a good empirical null distribution and to use it in assessing the significance of results.

to stress the point, the seminal results of gene periodicity studies were reported in  <dig>  and now more than  <dig> years later studies are still made that change the interpretation of those results. biosciences are a rapidly developing area of research with new data and instruments coming out at a fast pace, and therefore more attention should be placed on significance testing.

CONCLUSIONS
when gene expression data is mined for complex patterns, little if no effort is made to test results for statistical significance. with randomization, significance testing can be extended from simple classical statistical tests to complex pattern mining. existing methods for testing significance of periodic gene expression patterns were found too simplistic and optimistic. we introduced significance testing framework that accounts for multiple testing and allows simple yet more realistic null models to be crafted with randomization algorithms. as a result, a much smaller number of genes showed significant periodicity.

as dna microarrays have now become mainstream and new high-throughput methods are rapidly being adopted, we argue that not only there will be need for data mining methods capable of coping with immense datasets, but there will also be need for a new approach to significance testing. we need methods that do not require demanding mathematical analytics and that can be easily modified and adopted to different situations. randomization algorithms are intuitive and can be readily constructed by the growing numbers of computer literate bioresearchers. future of biosciences seems to be characterized by large developments in data production capability and that development must be met with appropriate tools to guard against false interpretations.

authors' contributions
ak implemented the method and carried out experiments. nv carried out the mathematical analysis. ak and nv drafted the manuscript. the design and development of the methods was done by all authors. all authors have read and approved the final manuscript.

supplementary material
additional file 1
appendix: periodicity score calculations. additional file  <dig> contains more detailed mathematical derivations of the results in section analysis of periodicity score distributions.

click here for file

 acknowledgements and funding
we would like to thank gemma garriga, jarno tuimala, esa junttila, kai puolamäki and the anonymous reviewers for their insightful comments that helped to improve the presentation and structure of the manuscript.

this work was supported in part by academy of finland grant  <dig> . nv was funded by finnish graduate school in computational sciences and mo was funded by helsinki graduate school in computer science and engineering.
