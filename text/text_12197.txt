BACKGROUND
conventionally, genetic and transcriptional studies of non-model organisms have been restricted due to the lack of reference genomes that impede their analyses. nevertheless, non-model organisms are of great ecological and economic significance; consequently the understanding of their unique metabolic pathways by investigating their gene expression profiles is crucial. the advent of next generation sequencing  and its continuing improvement, as well as the development of corresponding bioinformatics analysis tools have boosted the number of sequenced transcriptomes in non-model organisms and their automated assemblies have become common over time  <cit> .

numerous software and pipelines have been used to automatically build transcriptomes and several methods that integrate de novo assembly together with genome based assembly have been proposed for non-model organisms  <cit> . two major alternatives can be employed: 1) aligning reads to the existing reference genome and then assembling the remaining unmapped reads or 2) performing a de novo assembly first and then using the genome to improve the transcript assembly  <cit> . however, many open challenges in defining genes remain, particularly where genomes are not available or are incomplete. in spite of the large numbers of transcriptome assemblies that have been performed, quality control of the transcript building process is rarely performed.

manually defined and curated transcripts or good quality ests could be used to assess the quality of automated transcriptome assembly, but to the best of our knowledge, they have not been used. in non-model organisms in particular, it is critical to have genes built from the species being studied, as closely related well-annotated species might not be available. in spite of the great potential importance, the processes used to manually define and curate genes have not been documented until now .

in this study, the transcriptome of the bloom-forming alga emiliania huxleyi was built. e. huxleyi is a cosmopolitan unicellular photoautotroph that plays a prominent role in the marine carbon cycle  <cit> . its intricate calcite coccoliths account for a third of the total marine caco <dig> production, making it highly susceptible to future ocean acidification  <cit> . in addition to their role in the biogeochemistry of carbon and related climatic impacts, coccolithophores produce the sulfur containing compound dimethylsulphoniopropionate, precursor of the dimethylsulfide gas which is a major source of sulfur to the atmosphere where it can influence aerosol formation and consequently cloud condensation nuclei  <cit> .

the recently published genome assembly of e. huxleyi is a draft that was constructed from sanger reads  <cit> . a large number of available unassembled genomic reads, numerous repeats and duplications, as well as holes in the genome, indicated that the genome alone would not provide a good basis for building transcripts. therefore we opted for an integrative pipeline to build the transcriptome. to test and improve the quality of the automated transcriptome reconstruction, we used  <dig> manually defined and curated e. huxleyi genes, several of them experimentally validated. after each step in the automated definition pipeline, the presence of the manually defined genes was checked, allowing troubleshooting of missing genes and improving our pipeline. this is the first time that an automated transcript definition is subjected to quality control using manually defined and curated genes and thereafter the process is improved.

RESULTS
description of the experimental system
the experimental system was e. huxleyi cells subjected to viral infection over a time course, with two different viruses, lytic  and non-lytic . rnaseq was performed for six samples using the ilumina hiseq <dig> as follows: control  and infected with ehv <dig> or ehv <dig>  at two time points:  <dig> and  <dig> hours post infection. the in-depth description of the experiment and its biological significance was submitted elsewhere . table  <dig> summarizes the number of reads obtained for each sample after removing adaptors and trimming to  <dig> bases. the sequences were trimmed because of a decrease in quality scores in late sequencing cycles due to the high gc content . the sample containing ehv <dig> for  <dig> hours had the highest number of reads with adaptor sequences  and a relatively low number of reads, since many of the e. huxleyi cells were dead due to the viral infection  <cit>  and therefore there was a higher percentage of low quality or degraded rna .

*after removing adaptors and trimming to  <dig> bp  **each sequence was counted once even if several reads have the same sequence, each sequence aligned to the genome ~2- <dig>  times in average . percentage of cleaned reads in parentheses.

transcriptome assembly
the available genome assembly  is a draft, and was constructed from sanger reads. there are unassembled reads available, and this together with our experience in hand-curation, where we observed many repeats, duplications, and holes in the genome , led us to believe that additional genomic information is essential to achieve a high quality transcript assembly. in addition to the genome, there were publicly available ests, which can provide additional information. in light of this, three different approaches were applied to define e. huxleyi transcripts , two of them utilizing our rnaseq data. the first was de novo assembly; the second was a genome-based alignment to an improved version of the genome assembly. the third approach utilized a collection of publicly available e. huxleyi ests. all approaches were integrated at the end.

de novo assembly of transcripts
the adaptor-cleaned and trimmed fastq files were assembled using clc assembly cell. reads from all samples were pooled for assembly. this resulted in  <dig> fragments. in order to reduce redundancy cap <dig>  <cit>  was used to cluster the fragments. the outcome was  <dig> contigs and  <dig> singlets.

genome based definition of transcripts
in addition to the emihu <dig> current assembly of the genome, the jgi website database includes sanger sequencing reads that were not included in the current assembly. they are called unplaced genomic reads and consist of  <dig> bases  in  <dig> sequences. the unplaced reads were assembled using newbler, resulting in  <dig> contigs. to evaluate the possible contribution of these contigs to the transcriptome definition, the reads of sample 10be which had the lowest number of e. huxleyi reads , were aligned to these newly assembled contigs.  <dig> reads were mapped to the new contigs, as compared to  <dig>  million reads previously aligned to emihu <dig>  in view of the high number of reads aligned to the new contigs, they were assembled to the available genome using minimus <dig>  <cit>  to create an improved genome version, emihu1plus .

the reads of each sample were aligned separately to the emihu1plus genome using tophat  <cit> . the total number of reads aligned to the genome per sample spanned from  <dig> to almost  <dig> million. the exception was sample 10be, as mentioned above, which has only  <dig>  million aligned reads due to advanced cell lysis . after alignment, cufflinks and cuffcompare  <cit>  were applied to all the tophat outputs to define transcripts. in this process,  <dig> potential transcripts were defined.

est collection
ests were downloaded from ncbi, using the taxid  <dig>  for a total of  <dig> ests. the ests were clustered using tgicl.  <dig> of them were assembled into  <dig> clusters containing at least two ests. single ests that were not clustered  were not utilized for further transcriptome reconstruction.

manual definition of genes
in parallel to the automated approach for transcript assembly, we manually defined a set of e. huxleyi genes. these genes were used in order to assess the quality of the automated pipeline. the manual definition started from the choice of a target gene . protein sequences of the target gene from human, arabidopsis thaliana, and yeast , and if necessary additional species, were compared to the e. huxleyi genome on the jgi genome website using tblastn  <cit> . hits were inspected to see if there was any transcript evidence . if there were matching ests, they were assembled into transcripts, and compared to the predictions, if there were any. if there was incomplete est coverage, but a jgi predicted gene model, the blast results were used to fix the prediction accordingly. when the rnaseq reads became available, if possible, the putative transcripts were corrected on the basis of the reads. if there was more than one genomic hit in the blast results, each successive hit was checked to see if it was a truly independent hit, representing a family member, or a duplication, which was then classified as real or artificial . if no ests were available to use as an anchor for a predicted transcript, then a combination of reads , prediction based on blast hits and the jgi predictions were used to construct a transcript.

if there was no genomic hit, or if there was a genomic hit with a hole in the middle of a locus , and in some cases where there were no ests in jgi, searches were performed against e. huxleyi ests in ncbi, in order to identify sequences that might not have been mapped to the genome, generally due to missing sequence in the genomic build. transcripts were then constructed and extended as far as possible by running blast.

the sequence was then translated, and a blastp search was run at ncbi to determine if the putative protein was closest to the target gene in other species, and if it had the proper domains. we found that in many proteins repetitive sequences interrupted the canonical domain composition, and in some cases the domains themselves.

after the protein sequence was finalized, multiple alignments and phylogenetic trees were constructed with protein sequences from representative species to see that the sequence indeed belonged, who its closest relatives were, and in the case of multiple family members, to attempt to assign orthology.

we compiled a set of  <dig> “gold standard” genes  that were used afterwards for quality control of the automated transcript assembly. validation of  <dig> of the sequences was performed by real-time pcr with primers designed to the manually constructed sequences or western blot . all of the sequences but two had reads in the rnaseq data of the current dataset, and the remaining two have reads in an additional dataset .

the “gold standard” genes all have known functions, come from at least five different biological pathways, and are distributed in various cellular compartments. they include both globular and transmembrane proteins. they are a mix of short and long transcripts ranging from  <dig> to  <dig> base pairs, with varying numbers of exons, ranging from single exon to  <dig> exon genes, and are expressed in varying levels according to the rnaseq data . the “gold standard” genes therefore have a wide representation of the various types of genes extant in e. huxleyi.

first round of quality control using manually defined genes
to assess the quality of the transcriptome reconstruction, presence of the  <dig> “gold standard” genes in the three transcript definition approaches was examined . the transcripts were compared to the standard genes using blat  <cit> , with a minimum hit score of  <dig>  in order to ensure significant hits. four of the genes had less than  <dig> reads in the rnaseq, and therefore could not be found in the read-based arms of the assembly. in the genome based transcript collection, of the  <dig> possible genes, eleven were missed. in the de novo assembly, twelve genes were missed. in the est branch,  <dig> genes were found out of  <dig> with ests. the four that were missed either had only one est, or non-overlapping ests that were therefore not clustered. two of the four transcripts which did not have enough reads in the rnaseq were found in the est branch, and two were not found at all.

improving the transcript repertoire
in order to define the genes missed by the tuxedo suite  <cit>  , the partek® genomics suite™ software was applied. partek was used to find regions where reads aligned, excluding regions with cufflinks transcripts . these regions were required to have a minimum length of  <dig> base pairs and a minimum coverage of  <dig> reads, as the default partek settings are permissive. of the  <dig> “unexplained” regions defined by partek,  <dig> met the requirements .

in comparison to our “gold standard” collection, partek found only twenty-one genes, but that was due to the exclusion of previously defined transcripts. of the twenty-one, it found four that were missed by cufflinks, two of which had also been missed by clc assembly cell. the other  <dig> genes that were found by partek added new fragments to transcripts defined with cufflinks. all told, using all four approaches,  <dig> out of the  <dig> “gold standard” genes were found .

for each of the approaches, we characterized the transcripts and their translations. the median transcript length for the de novo fragments was  <dig> bp, with a median orf length of  <dig> amino acids, and a median orf coverage of 99%. the other methods resulted in longer sequences , but a lower orf percent coverage, 89% for all arms .

the transcripts defined by the three primary approaches and by partek,  <dig> sequences, were clustered using tgicl . this process resulted in  <dig> transcripts longer than  <dig> base pairs, including contigs and singletons as defined by tgicl. to further remove redundancy, cd-hit-est  <cit>  was applied and the number of transcripts was slightly reduced to  <dig> 

the next round of quality control was performed on the set of  <dig> transcripts. only  <dig> of the “gold standard” set were found, indicating that in our clustering  <dig> genes were lost. inspection of the output of both programs showed that the genes were lost by tgicl.

quality control on the protein level
the next step was to check for the presence of the “gold standard” genes on the protein level. the transcripts were translated taking only the longest open reading frame , with the orf defined from stop to stop. we did not require an initial methionine, as many of the transcripts are incomplete. only  <dig>  of the standard set were found on the protein level. missed proteins were inspected on a case-by-case basis, to understand how they were lost and improve the pipeline.

three causes were identified: 1) frameshifts due to either sequence or assembly errors; 2) correct reading frames were shorter than the longest incorrect frame ; 3) fusion of transcripts on opposite strands due to overlaps in 3’ utrs . in order to improve the fusion transcripts, we decided to split them. for each transcript the three longest orfs were examined, and in cases where translations derived from different orfs did not overlap and the resulting peptides were at least  <dig> amino acids length, the transcripts were split.

the process ended up with  <dig> sequences split into two and  <dig> into three potential transcripts.  <dig> of the split transcripts were found to be transcribed from opposite strands. redundancy of the whole set after splitting was removed at the protein level using cd-hit  <cit> , resulting in  <dig> transcripts. when this set of translations was compared to the  <dig> transcripts of the standard set that were detected,  <dig> proteins out of the  <dig> were found  an improvement of 11%.

additional improvement of the transcriptome
at this point, it was clear that tgicl lost transcripts during the clustering process. to overcome the problem of missing genes, the initial transcripts from the four approaches were clustered using cap <dig> . further redundancy was removed by cd-hit-est. the number of potential transcripts longer than  <dig> base pairs was  <dig>  in order to split artificially fused transcripts, the algorithm developed to split the sequences previously was used.  <dig> sequences were split in two, and  <dig> were split in three. once again, approximately half of the split sequences  were from opposite strands. redundancy at the protein level was removed with cd-hit, and subsequently the redundant transcripts were removed, resulting in  <dig> defined transcripts. these transcripts were compared to the standard set, and  <dig> out of  <dig> transcripts were found.

two different clustering algorithms were applied to the collection of potential transcripts. tgicl strongly removed redundancy, and makes longer transcripts  but loses genes. cap <dig> does not lose genes, but makes shorter transcripts  and leaves redundancy in the collection . to take advantage of both algorithms, the two collections were compared. transcripts defined by cap <dig> that were identical  or were contained in  tgicl defined transcripts were replaced by their matching counterparts in tgicl . this reduced the number of transcripts by  <dig>  additional transcripts defined by cap <dig> that did not have a good match to tgicl defined ones  were introduced into the final collection without changes. the final collection included  <dig> transcripts.

the final collection was compared to the “gold standard” set both on the dna and protein level.  <dig> transcripts and  <dig> proteins were found, an improvement versus the tgicl alone. the transcripts that were not detected on the protein level were analyzed in depth to discover the possible flaws in our pipeline. there were two major causes, one on the transcript level, and one in the translation process. on the transcript level, we discovered that many of the genes had both spliced forms and intronic read-throughs  . the retained introns disrupted the coding sequence. the second major cause of missing protein sequences was due to our definition of an open reading frame from stop-to-stop .

we examined whether  <dig> genes were enough for quality control by increasing the number of hand constructed genes to  <dig>  and running comparisons on the dna and protein levels at all stages of transcriptome reconstruction. the results show no change in the percentage of genes or proteins found in any of the steps . the additional  <dig> transcripts and  <dig> proteins that were not found at the final stage were due to reasons seen previously, lack of reads or longer irrelevant open reading frames, respectively.

the percentage of sequences found by each step of the transcriptome reconstruction is given.

*partek percentages are lower as the program was used to define transcripts in very limited regions.

characterization of the e. huxleyi transcriptome
the  <dig> transcripts range in length from  <dig> to  <dig> base pairs . close to half of the transcripts  are more than  <dig> bp long, with an additional 32%  longer than  <dig> bp . 70% of the transcripts had orfs over more than 80% of their length, and 44% had orfs covering the entire length of the transcript .

in order to assess how well the reads fit the final transcripts, the reads were mapped to the transcript collection, and 80% of the total reads mapped fully , in all samples except for sample 10be that included a high percentage of viral reads.

the final transcript collection was compared to the jgi ‘best transcript’ models   <cit>  by applying blat. our collection was used to query the target models. out of  <dig> transcripts, 63%  were found to have at least a partial match to the predictions, and only  <dig> %  of the transcripts matched in at least 90% of their sequences . of the  <dig> models,  <dig> %  had at least partial coverage of our transcripts. we compared the interpro annotations of the jgi matching and non-matching transcripts, and while only 29% of the jgi matching transcripts had annotation, 45% of the non-jgi transcripts did.

during the manual definition of the genes,  <dig> splice junctions were mapped to the genome sequence, and unlike published genomes, a prevalence  of non-canonical gc-ag junctions was observed . the gc junction was used only when it was obligatory, and there was no other way to map the sequence . thirteen of these junctions  were verified with reverse transcriptase pcr and sanger sequencing .

further investigation of this finding was performed with additional junction collections, based on ests or rnaseq reads, with both automated and manual analysis. the full collection of e. huxleyi ests was mapped to the genome using blat  <cit> . the mapping results were very noisy, so for automated analysis the donor junctions were filtered for ag acceptors . the gc to gt ratio was approximately 3: <dig>  further corroborating our initial finding . in addition,  <dig> junctions from  <dig> est-based transcripts in  <dig> randomly chosen genomic loci were manually aligned and examined, and the percentage of gc junctions was 64% .

splice junctions from automatically generated cufflinks transcripts were also analyzed, and donor  and acceptor  sites were extracted from emihu1plus. in order not to count the same junction twice in cases of alternate splicing, non-redundancy was performed on the genomic coordinates. the most prevalent splice donor is gc  while the canonical sequence gt was only used 37% of the time. the splice acceptor is almost entirely the canonical ag . the even less canonical sequences , or at least part of them, represent misalignments of tophat . validation was performed by manual inspection of spliced rnaseq reads mapped to the genome, representing  <dig> randomly chosen transcripts. a total of  <dig> junctions were inspected, and gc was found in 65% of the donors, while gt was found in 35% . in contrast, the jgi ‘best transcript’ models have the opposite distribution, with 82% gt and only 15% gc .

intron retention was observed as a major cause of redundancy in the transcript collection and loss of proteins in the translations. therefore the phenomenon was assessed in an attempt to quantify the level of intronic reads and additional types of alternative splicing. currently available programs to determine types of splicing could not be used, for two reasons. firstly, many have a transcript mapping step which relies on gt-ag junctions, which is inappropriate for e. huxleyi. others require a database of defined splice junctions, which is problematic to create due to the pervasive nature of the intronic reads . a manual inspection was performed on  <dig> introns taken from  <dig> loci, and 89%  showed reads throughout the length of the intron. the other instances of alternative splicing observed were:  <dig> exon skipping,  <dig> alternate acceptors, and  <dig> alternate donors.

transcriptome annotations
the final transcripts were annotated using gene ontology   <cit>  terms by applying blast2go  <cit>  at the dna level. 61%  of the transcripts had at least one blast hit. go terms were transferred to 84%  of the transcripts with blast hits, and finally, go terms were successfully assigned to 40% of the total transcriptome , after setting the annotation cut-off to  <dig> . the total number of annotations was 143794; the mean number of go annotations per gene was  <dig> and the mean level in go was  <dig>  . the most frequent organism in the blast top hits was afipia broomeae with  <dig> hits and the second one was aureococcus anophagefferens with  <dig> . a. broomeae is a very well annotated species, and that is probably the reason for the large number of hits, as it is phylogenetically distant from e. huxleyi. the other species are all relatively closer. the top go terms to which transcripts were assigned in the categories: biological process, molecular function and cellular component were oxidation-reduction process, atp binding and integral to membrane respectively . a list of terms and the number of genes annotated to them can be found in additional file  <dig> 

kog and kegg annotations were performed at the protein level;  <dig> proteins were annotated in the kog system and  <dig> in kegg .

discussion
definition of transcripts can and needs to be improved from the current standards. fully automated transcript building, while the only practical method on a large scale, can be improved, particularly for the individual genome. we recommend a manually curated reference set for quality control per genome, which will allow detection of genome specific issues, and allow improvement of existing tools and pipelines. once done, the new pipeline can then be utilized for all future transcriptomes of the same genome. in our particular case, e. huxleyi, the genome has non-canonical splice junctions, overlapping genes, duplications and holes in the genome. these issues were addressed, to the best level possible, utilizing the “gold standard” set of genes for quality control. we have since successfully analyzed additional transcriptomes from the same organism, taking these issues into account.

assembly strategy
one of the goals of this study was to achieve a complete picture of the transcriptome: utilizing all the available data, build accurate transcripts without either missing genes, or including spurious sequences. the data at hand was the genome assembly from jgi, ests from genbank, and our rnaseq reads. the genome was incomplete, and the est coverage was very partial, with only  <dig>  sequences for a genome with an estimated  <dig>  genes, and many more transcripts. the rnaseq data had good read depth, however, reads were relatively short  and transcript coverage was incomplete due to the low complexity and highly repetitive nature of the underlying sequence.

we used three different approaches to assemble the sequence: two utilized the reads, one genome based and one de novo, and an est only branch. for the read-based arms, the first decision that had to be made was whether to combine the reads from the different biological samples or to analyze them independently. for the genome-based approach, we chose to analyze each lane individually , and then join them , while for the de novo assembly , without the basis of the genome to build on, we used all the lanes together in order to maximize the chances of building a transcript, particularly those with low expression levels. the next decision was whether or not to include the ests in the de novo construction, as ests allow elongation of putative transcripts due to their length and different sequencing method . we decided to cluster and analyze the ests on their own first, in order to improve the ests’ reliability, and to combine the various tracks afterwards.

when the analysis was performed, the programs chosen were considered state-of-the-art. new tools have been developed in the interim, such as trinity  <cit>  and soapdenovo-trans  <cit>  among others. programs developed for genome assembly and later adapted to transcript assembly  seem to be less accurate than purpose-built transcriptome tools . while some of the newly developed tools may perform better building transcripts, a manually curated gene set is still highly recommended for troubleshooting, particularly for non-model organisms.

manually constructed and curated transcripts
the process of manual construction of genes is highly complex. complications arise in cases when the target sequence is not related closely enough to the genome of interest to rely on as a basis for comparison, and when there are no cdna or genomic sequences to compare to. we have both obstacles in e. huxleyi, but the first is the greater issue, as there are no close relatives, and each gene has a unique history, some closer to plants, some to mammals, some even to bacteria .

various techniques are used to bridge the gaps, many of which can be implemented manually, but are difficult to automate, particularly integration of information from multiple sources. this is particularly true of the human ability to process data based on visual presentation. theoretically, any one of these techniques may potentially be programmed, but in practice, it is not trivial, and the combination of methods for the decision-making process is not possible in the computer capabilities currently available.

quality control
one of the major goals of this project was to test the quality at each step of the transcriptome construction. in the very first step, examination of the reads, we found that the reads had to be trimmed due to a noticeable drop in quality towards the end  due to the high gc content.

the manually curated genes gave us the opportunity to detect faults in the process of automated transcript building. when we compared the “gold standard” to our initial gene build, many transcripts were missing. the loss of genes was caused by: 1) genes that were only predicted, with no transcript evidence at all; 2) genes that had coverage, but by only one est and had no reads; 3) genes with no reads and more than one non-overlapping est; 4) genes without enough sequence coverage for an automated program to build a long enough transcript; 5) genes that had read coverage, but cufflinks and clc assembly cell did not build. we improved our initial build by using the partek genomics suite to define transcripts missed by the earlier methods, and indeed, found all the missing genes that had coverage on the nucleotide level.

lost in translation
after we found all the “gold standard” transcripts possible in the dataset, we compared the hand curated and automated datasets on the protein level. many genes were lost on the protein level, even though they could be detected on the dna level. the first class of missing proteins was due to artificial joining of transcripts. this phenomenon has been observed in model as well as non-model plants  <cit> . transcripts were joined in several ways including on opposite strands overlapping in the 3’ utr, and on the same strand with some overlapping reads . it is interesting to note that we did not observe any overlapping transcripts in the coding regions. we corrected many cases by using a script to split transcripts, based on non-overlapping open reading frames.

the second class of missed proteins was due to the choice of the longest open reading frame, from stop to stop. in some cases, full orfs from methionine to stop were in the correct frame and longer than stop to stop , and in other cases, the functional protein was not the longest frame. we decided to use stop to stop because many of the transcripts were partial, and did not expect to see a methionine in these cases. another reason was that frame-shifts may cause the methionine to be lost. it may be worthwhile to build a program that will look at both stop to stop and met to stop and choose the longer of the two. for the cases where the longest frame was not the functional one, an additional check can be performed, for example utilizing blastx results to choose the proper frame. these issues are compounded in our particular case due to high percentages of intron retention and genomic repeats.

additional observations
an additional observation that resulted from the hand curation is the high percentage of non-canonical splice junctions. in the hand curated transcripts, ests, and cufflinks defined transcripts, there is a clear majority of gc splice donors . in contrast, the jgi “best transcript” collection has the opposite ratio . the probable cause of the shift from gt to gc is the high gc content of the e. huxleyi genome, 65%  <cit> , among the highest of all eukaryotes sequenced to date. this can create a burden on the splicing machinery, as the canonical junctions are 50% at . the splice donor gc is also seen in many other species, though at very low percentages, but the ‘universal’ splice machinery knows how to deal with gc, which may be why the splice acceptor remains completely canonical. the “new” canon for e. huxleyi gc-ag, is 25% at, and is much closer to the dna at hand. the jgi “best transcript” collection is composed of 68% ab initio predicted genes  <cit> . most of the ab initio algorithms are programed to look for canonical gt-ag junctions, and will only use non-canonical if there is nothing else. this leads to the many mispredicted junctions in that dataset.

the non-canonical splicing may contribute to the high frequency of intron retention. this phenomenon was observed both in the manually curated as well as automatically processed transcripts. from the hundreds of alternate events inspected manually, mainly intron retention was observed. it is not clear if the transcripts with intron retention are functional, as in virtually all cases checked, it led to premature stop codons in the putative protein and disturbed the domain structure of the potential proteins . this raises the following questions: 1) is the nonsense-mediated decay mechanism functional in e. huxleyi? 2) to what extent is the splicing machinery conserved? 3) is the efficiency of splicing affected by the change in the junction sequence? another possible explanation for the observed intronic read-throughs is the presence of unspliced pre-mrna in the rna used for sequencing. we view this as unlikely, as the rna was polya selected, though it cannot be ruled out, as this may also be different in e. huxleyi. these issues will have to be resolved by further research into the various mechanisms mentioned.

comparison of programs
cufflinks missed putative exons and transcripts, where partek genomics suite was able to define them. partek describes reads mapped to the genome, while cufflinks attempts to build gene structure. there was no obvious reason for the transcripts to be missed by cufflinks, as these regions had a minimum of  <dig> reads and length of  <dig> bp. it may be that the high rate of intron retention did not allow cufflinks to predict transcripts in those regions.

we clustered the transcripts using both tgicl and cap <dig>  while tgicl utilizes cap <dig>  due to its initial blast stage, it gives very different results. while tgicl reduced redundancy very efficiently, it lost many transcripts. on the other hand, cap <dig> alone clustered well, but left redundancy in the dataset. in both cases, it is not clear why the programs did not perform optimally. as there is no explanation of how or why sequences are removed by tgicl, we were surprised by this finding. cap <dig> left the redundancy in the dataset due to the many alternative isoforms.

the final dataset has redundancy, mainly due to putative splice variants. due to the high level of intron retention, many transcripts are predicted both with and without introns. the programs cannot differentiate between real splicing and spurious splicing, though one possible way to discern correct transcripts would be to take intact reading frames into account.

parameters to measure quality of the transcriptome
using the manually curated transcripts, we found all of the transcripts possible given the constraints of minimum coverage per transcript. on the protein level, we found most of the genes, but have room to improve the pipeline. in independent measures of quality, the length of the transcripts was reasonable , and 70% of the transcripts had orfs over more than 80% of their length. most reads and ests mapped to the final transcripts. taken together, it indicates a good quality build.

we compared our set of transcripts to the jgi ‘best transcript’ models, which are mostly based on gene prediction. while most of the jgi set was found in our transcript collection, we had many transcripts not covered at all by the jgi set . this raises the question of fairness of comparison: firstly of a predicted versus a sequence based set, and secondly our full transcriptome with isoforms included to the ‘best transcript’ collection. while the jgi models were partially sequence based, and many of the predictions had some evidence of existence in their rna tag sequencing, the incompleteness of the genome caused issues for proper gene prediction  <cit> . the predictions suffered additionally due to the fact that they relied on canonical splice junctions, while in e. huxleyi, the majority of junctions are non-canonical. while the ‘best transcript’ models generally have one isoform per locus, the full transcript set has additional transcripts of previously covered loci, while we have coverage of many loci not covered by jgi at all.

while this manuscript was in preparation, a study on the sulfate deficiency response in e. huxleyi was published  <cit> , where rnaseq was also performed. they found, as we did, that the jgi models were severely lacking, and do not give good coverage of the transcriptome. we cannot compare our results to theirs, as their sequences are not yet available in a public repository.

CONCLUSIONS
as has been suggested  <cit>  a reference set of transcripts should be generated for quality control. we have shown that the reference set should be genome specific and manually defined, and that it is critical for troubleshooting and accurate quality control. this is the first time, to the best of our knowledge, that a manually built set of genes has been used to improve a transcriptome pipeline. the advantages are clear; it gives an organism-specific picture of the pitfalls and problems that accompany any de novo transcriptome sequencing, particularly for non-model organisms.

