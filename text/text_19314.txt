BACKGROUND
networks, a construct emphasizing the interrelations between objects, have application in studying human behaviour, mathematics, physics, econometrics, etc. with the introduction of microarrays and other high-throughput systems, networks increasingly provide a means to organize and study the interdependencies of genes, proteins, metabolites, etc. gene transcription/regulatory networks, metabolic pathways, protein-protein interaction systems , signal transduction pathways, and phylogenetic trees are established tools in biology. prone-to-noise experiments are routinely coupled with computational algorithms to infer relationships. given the empirical reliance on uncertain data it seems natural to ask, “do these networks differ from one another?” this paper outlines and demonstrates an inferential process for performing one- and two-sample hypothesis tests when the sample data are biological networks.

numerous books are devoted to networks. bornholdt et al. <cit>  and junker et al.  <cit>  provide a broad introduction to networks with application to biology, e.g., correlation profiles and motifs, network behaviour in nematode development, etc. emmert-streib et al.  <cit>  is a collection devoted to inferring microarray-based networks. kolaczyk  <cit>  appears to be the first statistics text devoted solely to networks. brandes et al.  <cit>  provide an overview of analysis methods from a computer science perspective.

testing graphs is not trivial; comparing two static graphs is conceptually different from comparing a sample of stochastic graphs under one or more treatments. defining a suitable null probability model for a network is a difficult consideration  <cit> . erdős-rényi random graphs, where the probability of an edge between any two vertices is a fixed constant p, play an important conceptual role in our understanding of graphs  <cit> . the use of these graphs in forming statistical tests has received some criticism  <cit> . chung et al.  <cit>  explore a complex hybrid-graph model to mimic observed small-world networks. simple models, e.g., random, scale-free, or small-world graphs, etc., although useful for comparing features across a class of networks, may have less utility for weighted biological networks. schwöbbermeyer  <cit> , in discussing network motifs, makes a troubling comment regarding the formation of actual biological networks, “a single network generation mechanism may not be sufficient to resemble the structure of these networks.” this difficulty in defining a suitably rich or ‘realistic’ parametric model poses a challenge in forming a hypothesis testing procedure for network parameters. exponential random graph models , referred to as p* models in social network analysis, offer a theoretical model for stochastic networks and have been used in biological applications  <cit> . ergm parameterizations can contain attractive motif-like structures. but, these models do not typically assume that the nodes of a graph are aligned and suffer from model degeneracy concerns  <cit> . motif frequencies have been used to compare biological networks via a statistical test  <cit> . wiuf et al.  <cit>  outline a full-likelihood probability model approach to estimating the parameters of a c. elegans protein interaction growth model. cardelli  <cit>  suggests that qualitative models provide more insight than quantitative models due to parameter estimation and criticality concerns. in contrast, steinhauser et al.  <cit>  argue and demonstrate that correlation networks, a form of weighted network, provide more understanding of cellular systems relative to qualitative edge/no-edge models or node-based cell inventory quantitative models. unlike traditional parameter-centric testing procedures, this dichotomy suggests that a testing procedure might benefit from distinguishing structural  distinctions from weight  distinctions. the methods presented here will accommodate this need for a flexible model.

approaches for both two- and one-sample comparisons occur in the literature. faust et al.  <cit>  use a combination of p* models and correspondence analysis to compare networks. banks et al.  <cit> , using a symmetric difference based on a hamming distance, outline an estimation/hypothesis testing approach for labeled unweighted loop-free graphs. sanil et al.  <cit>  extend  <cit>  to networks whose edge set evolves in time. kahlem et al.  <cit> , in a reverse engineering application, suggest three approaches  for comparing two networks that conceptually differs from the approach pursued here. stolovitzky et al.  <cit>  propose roc and precision-recall curves as the method-of-choice for validating inferred models, a motivating example for a one-sample comparison. chen et al.  <cit>  use an additive element-wise score to compare a gene regulatory network estimate to a known network, a parallel to the one-sample procedure developed here.

while our focus here is on forming statistical comparisons of network model parameters, computer scientists can divide the graph comparison problem into two areas – exact graph matching and graph similarity  <cit> . graph similarity, tailored to deal with errors in the network data, has been addressed with three broad strategies: compare the difference of path lengths using all pairs of vertices, locate the maximal common subgraph between the two graphs, or use an edit distance. edit distance, which motivated and resembles the dissimilarity index proposed here, has been used in string matching applications and uses graph operations, e.g., node/edge insertions/deletions, to transform one graph into a second graph  <cit> . insertions/deletions can be implemented using set operators. xulvi-brunet et al.  <cit>  propose a bootstrapped degree of similarity via union/intersection operations. gill et al.  <cit>  combine a partial least squares-based connectivity score with an intersection/union measure to test for differential modular structures via permutation. expanding beyond strings to motif- or neighbourhood-like objects has demonstrated biological benefit. li et al.  <cit>  extend a node-based topological overlap dissimilarity to assist in defining relevant gene neighbourhoods. chen et al.  <cit>  capture a study where the statistical accuracy of protein function prediction was improved by incorporating information beyond the protein’s immediate neighbours in the network. such precedent helped motivate the definition of our dissimilarity measure. graph matching is typically limited to unlabeled graphs. we assume here that the graphs are labeled, an assumption critical for computational and interpretation reasons.

the use and analysis of weighted  networks is increasingly relevant in biological applications. zhang et al.  <cit> , in a close parallel to pure correlation-based networks, form weighted gene co-expression networks. langfelder et al.  <cit> , as part of an r package for analyzing weighted correlation networks, provide several measures for comparing network topologies. here, we utilize network models based on two forms of correlation matrices. anderson  <cit>  contains large-sample statistical tests for  correlation coefficients, canonical correlations, and various tests for covariance matrices. anderson  <cit>  uses a distance-based dissimilarity measure combined with a permutation procedure to compare dispersion matrices.

resampling methods are common in network analysis, e.g., see  <cit> , and will be used extensively here. their use in validating network models is less common. perkins  <cit>  uses cross-validation and resampling methods for validating a gap gene development model. toh et al.  <cit>  use bootstrap samples to assess edge reliability in a partial correlation network. emmert-streib  <cit>  combines a permutation-based procedure with a graph-edit distance to compare disease pathways. xiong et al. <cit>  use a permutation procedure to test the largest element-wise difference in a matrix of genetic network parameter estimates.

hypothesis tests for networks have a variety of obvious applications. a one-sample comparison can occur when comparing a network estimate to a known ‘gold standard’ model. the standard could reside in an online ontology, be defined via a data processing algorithm , or reflect the starting t <dig> state of a signal transduction network. algorithms that derive networks using in silico or in vitro/vivo data could be tested for the comparability of their network model estimates in either a one- or two-sample context. for obvious reasons, two-sample comparisons have a broad application range. two-sample tests are conducted by drug developers to compare pathways between an investigative compound at various doses or to a competitor’s compound. we conduct one- and two-sample tests using both simulated and microarray-based network data. to evaluate both type i error control and the power of our procedure, we examine both null and non-null cases under a small set of network models. we assume that each sampling unit is an independent realization of a network. our testing approach follows the traditional hypothesis testing route: define the null and alternate hypotheses  and the risk associated with a decision; define a ‘nearby neighbour’ dissimilarity-based test statistic for testing the hypothesis; compute the test statistic; compute the distribution of the test statistic under the null hypothesis; and, make a decision using the sampling distribution and the calculated test statistic. our approach applies to edge/no-edge graphs, weighted graphs, and can be extended to directional networks. in the one-sample hypothesis testing context a null network model is assumed or based on resamples from an a priori null sample to generate the sampling distribution of our test statistic under the null hypothesis. to implement a two-sample comparison we rely on the permutation testing principle as the formal basis of our inferential approach  <cit> .

methods
differential testing via dissimilarity
we consider an observed network as a realization of a stochastic process. a sample {xi, i =  <dig>  …, n} of independent observations is therefore a set of networks. in contrast to social networks, where the sampling unit is often the node, we assume that the biological network is inherent to each sample observation. we propose a dissimilarity measure as a test statistic to capture the separation between two networks. we assume that we can align the nodes across the networks to be compared. since each node in the network represents a gene or other molecular entity, the ability to align nodes across a set of networks implies that we know a unique identifier for each node to allow for a molecule-by-molecule comparison across a set of networks. we determine the sampling distribution of the test statistic under the null hypothesis via resampling techniques.

first, we begin with some definitions. our definitions are consistent with bollobás  <cit> . a graph g is an ordered pair of disjoint sets  where both v and e are finite sets. v = v is the set of vertices and e = e is the set of edges. an edge {x,y} is said to join, or tie, the vertices x and y and is denoted xy. if xy ∈ e then x and y are adjacent, or neighbouring, vertices of g and the vertices x and y are incident with the edge xy. two edges are adjacent if they have exactly one common endvertex. g’ =  is a subgraph of g =  if v’ ⊂ v and e’ ⊂ e. if x is a vertex of g we write x ∈ g in place of x ∈ v. the order of g is the number of vertices in g; the size of g is the number of edges of g. g denotes an arbitrary graph of order n and size m. a graph of order n and size  n <dig>  is a complete n-graph. a covariance matrix of nonzero elements with dimension n, Σn, is a complete n-graph. the set of vertices adjacent to x ∈ g, the neighbourhood of x, is Γ. for adjacent vertices x and y we have x ∈ Г and y ∈ Г. the degree of x is |Γ|. a vertex of degree  <dig> is an isolated vertex . a path is a graph p where v = {x <dig>  x <dig>  …, xl} and e = {x0x <dig>  x1x <dig>  …, xl-1xl}. the length of p is the size of p. a graph without any cycles, a path with length greater than or equal to three and comprised of distinct vertices, is an acylic graph. unlike trees, we allow for cyclic graphs with isolates. paths have an obvious tie to motifs and other regulatory functions. by definition, a loop ) is not allowed; multiple edges joining the same two vertices are not allowed. a graph g can contain a subgraph g’ that is not connected to the remainder of g. isolated nodes and subgraphs occur in biological networks. accommodating isolates is necessary to align nodes between two graphs.

it is common to represent a graph g in matrix form. the adjacency matrix a = a =  of a graph g is the nxn matrix given by  aij= <dig> vivj∈eg <dig> otherwise.  to extend the definition to a weighted graph replace  <dig> with wij, where wij is the strength, covariance, etc., between vertices vi and vj. given nxn network matrices g =  and h =  we define g-h in the standard algebraic sense, i.e., gij-hij. we do not require a matrix be square; some directed network forms are nxm matrices. but, our use of element-wise subtraction is key; we are not suggesting a definition based on subspaces/subgraphs or set complements. we need to map an nxn network onto the real line in order to define a measure of separation. under this definition of matrix subtraction, g-h =  <dig>  where  <dig> is an appropriately dimensioned matrix of zeros, implies no separation between two networks.

the concept of dissimilarity  is standard fare, especially in cluster analysis and pattern recognition. the dissimilarity measure drs between r and s satisfies the following: drs≥  <dig> for every r, s, drr=  <dig> for every r, and drs= dsr for every r, s. refer to gan et al.  <cit>  for an excellent catalogue of measures. dissimilarity measures for categorical data x and y are generally based on a simple matching distance,  δx,y= <dig> x=y <dig> x≠y . the well-known hamming distance  <cit>  is a symmetrical form of a simple matching distance for binary strings and is used in communication theory. to craft our dissimilarity measure we propose a modified element-wise version of a matrix norm. element-wise measures do not account for the interrelations present between nodes. similar to linkage measures in genetics , where markers are often correlated, we desire a measure that incorporates these interrelationships. since networks are defined using interrelationships the inclusion of information for both a node and its neighbours is an intuitive concept.

let wo=  be a  adjacency  matrix for the observed network estimate and wt=  the target network. in a one-sample comparison wt represents the true network model. for a two-sample comparison the distinction between the two labels is arbitrary. both wo and wt represent graphs of order n; the nodes are labeled, common to, and aligned between both graphs. for node i define the dissimilarity at that node to be a combination of that node’s dissimilarity,  diot=∑j≠iniwijo≠0−iwijt≠0+wijo−wijt , and the dissimilarity for node i’s neighbours,  dijot*=∑k≠i,jniwjko≠0−iwjkt≠0+wjko−wjkt , for nodes j ≠ i, j ∈ Г. for the overall network, the dissimilarity d is defined as

 d=∑indiot+∑j≠indijot*cij , where  cij=wijoiwijt≠ <dig>  for weighted networks and specified by the researcher for unweighted networks. i is defined as the standard mathematical indicator function. for a graph of order n a set/neighbourhood is formed at each node wii, i =  <dig> …,n. we measure the dissimilarity between a node and its adjacent neighbours between the two graphs. to account for the intrinsic network structure the neighbourhood is then extended to those neighbouring nodes that are incident to nodes in Γ for both the target and observed networks. i.e., we now measure the dissimilarity for the two subgraphs induced by Γ, where i ≠ j and wjj is an element of Γ. this ‘extended’ neighbourhood dissimilarity is added to the dissimilarity measured at wii. the contribution of the second nearest neighbours is weighted by cij in the definition of d. in a weighted network, e.g., a correlation network, this weight is easily motivated. in an unweighted network cij is set by the researcher. assuming a weight of cij=  <dig> for an unweighted 0– <dig> graph reduces d to an unscaled version of the familiar hamming distance and is comparable to the edit distance approach listed in  <cit> .

we form d using separate edge and weight l1-norms. we elaborate on this choice later. the need to align the two graphs is critical to calculating d. our approach does result in additional computational overhead since edge xy will be counted for nodes x and y. but, the counting is consistent and avoids the need for complex single-count network partitioning schemes. only those nodes with a path length of  <dig> or less from wii are included; d can easily be extended to include path lengths greater than  <dig> 

one- and two-sample differential network comparisons
defining an appropriate hypothesis in the context of networks can be nontrivial. for an erdős-rényi graph of order n, g, the obvious parameter to test is p. apart from ergms and  correlation networks, explicit network parametric models may not be readily apparent. the basic form of a one-sample network hypothesis test here is h0: η = η <dig> versus h1: η ≠ η <dig>  where η is anticipated to be a vector-valued parameter for most realistic  biological networks. for a g graph we have η = p and one could test h0:g = g versus h1:g ≠ g. we intentionally provide no explicit guidance for how to determine p. and, we make explicit the probability model for the graph rather than state the hypothesis in a more compact manner, i.e., h0:p = p <dig>  for a network defined using a correlation matrix Ω we construct hypotheses of the form h0:Ω = Ω <dig> versus h1:Ω ≠ Ω <dig>  one needs to make explicit the procedure used to establish the edges in the network and the probability model for the observation data. in an example we test h0:g = g versus h1:g > g. here, one needs to recognize that the amount of randomness/entropy for a g graph is less when p is close to  <dig> or  <dig> relative to p close to  <dig>  to successfully perform a one-sided test. primarily, we are interested in the question, “does this differ from the target?”

we employ a resampling approach to perform one- and two-sample network hypothesis tests. following the five-step procedure outlined in good  <cit>  we first analyze the problem. we identify the null and alternate hypotheses under an assumed probability model and choose a suitable type i error rate. second, we select a test statistic to test the hypothesis. here, d may be suitable in its stock form or require customization for the problem at hand. third, compute the test statistic. fourth, determine the distribution of the test statistic under the null hypothesis via a suitable resampling procedure. finally, make a decision using the sampling distribution of the test statistics as a guide.

to generate a null distribution for d in the one-sample case we assume a parametric model or explicit generative algorithm is available in order to draw samples from the null network model. in most customary testing situations a test statistic is an estimate for a parameter of interest. however, in some cases the sample itself is the statistic – concise reductions of the data may be limited or not obvious/possible. biological networks, where each edge or weight may be associated with transcription activity or a regulatory cascade, are inherently high-dimensional objects and may therefore lack parsimonious model parameterizations. in many applications a network algorithm f is required to produce an observed network. in these instances we may need to resample from f instead of resampling from the observed data {xi}. in other cases, e.g., the erdős-rényi g graph, the role of the xi may be suppressed or not apparent since we observe f. for networks based on explicit probability models, e.g.,  correlation networks, parametric bootstraps or monte carlo procedures may be possible under suitable assumptions  <cit> .

although a one-sample comparison has application for biomedical researchers, relative comparisons are of broad practical relevance. research clinicians and pharmacologists are interested in exploring standard-of-care and new treatment comparisons for therapeutics. transitioning to the h0: η1= η <dig> versus h1: η1≠ η <dig> two-sample problem allows one to draw upon established parametric and nonparametric comparisons. a null hypothesis of network equality versus an alternate hypothesis of network inequality is expected to be commonplace. alternate hypotheses such as h1: η1> η <dig> are possible but not explored here. we make the standard assumption of two independent and identically distributed samples {x <dig>  …, xn} and {y <dig>  …, ym}, where xi and yj are network-valued. following the notation of  <cit> , let p be a family of distributions for {x <dig>  …, xn} that are symmetric in the sense that for a permutation π of the subscripts { <dig>  …, n} we have p{ ∈ b} = p{, …, xπ) ∈ b} for all borel sets b. the random variables xi are said to be exchangeable – a condition established under the assumption of independent and identically distributed samples or via the principle of randomization/random allocation in experimental design. as noted in good, permutation tests rely on the assumption of exchangeability under the null hypothesis.

permutation testing, a procedure which relies on samples drawn from the combined pool of experimental units and the random assignment of a treatment label to each unit, is common in the bioinformatics literature due to the prevalence of ‘n<p’ wide data and the lack of closed-form sampling distributions for various test statistics proposed. pesarin  <cit>  lists a variety of settings where these conditional inference procedures are useful. some of the items listed that may apply to biological networks are: the distributional models for the responses are nonparametric, distributional models are not well-specified or may rely on too many nuisance parameters, the asymptotic null sampling distribution is unknown or depends on unknown quantities, or the sample size is less than the number of responses. to continue, these procedures might prove useful for multivariate problems where some variables are categorical  and others quantitative , in select multivariate inference problems where the component variables have different degrees of importance , and when treatment effects impact more than one aspect of the network. applying the permutation testing principle, as stated in  <cit> , to the two-sample network comparison problem via the customary mechanics serves as the inferential foundation for our two-sample testing strategy.

computer simulation
we first demonstrate d using an erdős-rényi g graph. we test h0:g = g versus h1:g > g. the order of g is  <dig> and p <dig> is  <dig> . we simulate four cases with  <dig> hypothesis tests  in each case. for the null case we assume that the observed network is a p = p <dig> =  <dig>  model. for the remaining two cases we assume that p =  <dig> . we set cij =  <dig> for both a null and an alternate case and cij = exp for the remaining two cases. these four cases illustrate the type i and ii error rates of our procedure both with and without the inclusion of the neighbouring information in calculating d.  <dig>  resamples were used to estimate the null distribution for d. the execution time for the set of  <dig> experiments using  <dig>  resamples was approximately  <dig> hour on a standard personal computer. the r package statnet  <cit>  was used to generate the g graphs. all of our computations were conducted using r . for testing large networks or large numbers of networks a more computationally efficient language such as fortran or c++ is recommended. the representation diversity and size of networks, combined with a need or interest to tailor d for a particular application, poses a challenge for software developers.

to evaluate d for a correlation-based network model we assume that the p-dimensional observations follow a multivariate normal distribution, np, with mean vector μ and positive definite covariance matrix Σ. transforming Σ into correlation form Ω allows us to form a partial correlation matrix. applying a threshold to each ρ estimate or a testing procedure to the entries of Ω can be used to define a correlation network. given Ω-1=  we compute the partial correlation matrix Π =  via  = −ωij/√. under multivariate normality two variables are conditionally independent given the remaining variables if and only if the partial correlation vanishes. the zeros in Ω- <dig> determine the conditional independence graph. as before, a threshold or testing procedure is used to define the partial correlation network. we evaluate both the type i error rate and the power of d under an alternate hypothesis.

apart from the erdős-rényi example, the one- and two-sample simulation comparisons assume that the observations are in their correlation form, i.e., xi~ n. to mimic a sparse biological network Ω consists of  <dig> nonzero 5x <dig> blocks along the diagonal. a sample rejection scheme guaranteed that the magnitude of each block entry exceeded a predefined threshold. the number of nodes  and the threshold  was common to all simulations. the same threshold ρ was used to define the data generation model and to estimate the observed correlation network in the one-sample case. for both comparisons we evaluated d using sample sizes of n1= n <dig> =  <dig>  for the one-sample comparison the target correlation network Ω <dig> is estimated from the sample data and the resamples are drawn from the observed samples to determine the null distribution for d. admittedly, this approach violates the true spirit of a one-sample test under an assumed null model. but, if historical sample observations are available for analysis and reflect the null hypothesis then these samples might provide the most scientifically defensible null distribution for Ω <dig>  the same approach is used for the derived microarray-based correlation network comparison; a case where the true null model is unknown. to simulate the alternate hypothesis of network inequality at least 10% of the 5 ×  <dig> blocks for a 30 ×  <dig> correlation matrix, with a minimum of one block per experiment, were varied using a random number generator. a total of  <dig> experiments were performed and  <dig>  resamples used in calculating each p-value. for both the correlation and partial correlation networks we calculate d using only the weight portion of the index since the existence of an edge was defined via the  correlation estimate. the execution time for the  <dig> experiments using  <dig>  resamples was approximately 1– <dig> hours on a standard personal computer for the one-sample comparison. the two-sample setting execution time was on the order of 4– <dig> hours. for select simulation data the algorithm used to estimate the partial correlation network, see below, would abruptly terminate. all of the resample p-values shown here were obtained upon a successful completion of the estimation process. the computation time for the actual biological data was negligible. this is likely due to the small networks involved; network comparisons involving a large number of nodes and/or edges present a non-trivial computational burden. refer to additional file  <dig> for the r code used.

we selected partial correlation networks as presented in  <cit>   for the two-sample comparisons due to their use in the literature, e.g., de la fuente et al.  <cit>  use partial correlations up to order  <dig> to model genomic data, and partial correlations are formed using a plurality of variables – an intuitive appeal for defining a network. markowetz et al.  <cit>  suggest that partial correlations may better reflect interdependencies in a network relative to a standard correlation coefficient. we fit these networks using the genenet algorithm presented in opgen-rhein et al.  <cit> , an extension of the algorithm from schäfer et al.  <cit> . the genenet r package is available from the cran r archive . the algorithm broadly consists of  <dig> steps. first, the correlation  matrix Ω is converted to a partial correlation matrix Π; pseudoinverses may be involved. the second step tests for the presence of ‘significant’ edges. the final step extracts the significant edges based on a user-defined criterion. we used the magnitude of the estimated  partial coefficient to establish this cut-off – the cutoff.ggm parameter for πij was set to  <dig> . default settings were used for the remainder of the genenet settings. we do not claim that ggms are superior for modeling gene/protein networks.  <cit>  demonstrate via simulation that the quality of various partial correlation estimators can vary according to the sample size and dimensionality of the observed data.  <cit>  document the need for larger samples in the practical use of ggms.

type ii diabetes mellitus data: one-sample comparison
type ii diabetes mellitus  is a serious metabolic disorder affecting a large number of people worldwide. mootha et al.  <cit> , using microarray transcriptional profiles obtained from  <dig> normal and  <dig> dm <dig> muscle biopsy samples, presented a study in conjunction with the gene set enrichment analysis  tool to detect differential expression patterns among functionally-related gene sets. they identified a single gene set, oxphos – an oxidative phosphorylation pathway, which exhibited differential gene expression levels between the two phenotypes and linked this gene set to clinically important variation in human metabolism. mootha et al. analyzed  <dig> gene sets –  <dig> were selected based on their involvement in metabolic pathways and the remainder based on co-regulated gene clusters from a mouse expression atlas. the transcript expression data and the gene set definitions from the original gsea study were obtained from the authors’ website  <cit> .

the original study examined average expression levels across the two groups. here, we explore the differences in covariance or correlation structures between the two phenotypes as expressed in a correlation network formed via a threshold for ρ. to facilitate a one-sample comparison the normal tissue samples are used to define a target network. the resampling procedure was outlined in the previous section. rather than analyze severely ill-conditioned correlation matrices , we formed correlation networks only for those gene sets with fewer than  <dig> probes in the pathway. a significant one-sample finding for one or more gene sets may suggest differences in co-regulation network behaviour of a person with dm <dig> relative to a normal subject.

ovarian cancer: two-sample comparison
ovarian cancer is the foremost lethal neoplasm of the female genital tract. we examine here the gene expression signatures of ovarian serous carcinomas  relative to serous borderline tumors  based on three recent studies. sieben et al.  <cit>  confirmed the activated role of a mitogenic pathway in sbts and uncovered downstream genes that helped differentiate sbts and scas. de meyer et al.  <cit>  investigated the role of the e2f/rb pathway in sbts and scas. chien et al.  <cit>  demonstrated the significance of the p <dig> and e2f pathways in serous carcinomas and reinforced the role of e2fs listed in  <cit> . these three studies demonstrate that differential expression patterns exist between sbts and scas. we examine a subset of these data to test whether or not covariation patterns differ between sbts and sca1s  and between sca1s and sca3s . changes in covariation patterns may assist in the design of follow-up studies, suggest a novel biomarker test or a better categorization of scas, or provide insight into a patient’s responsiveness to chemotherapy agents.

the microarray data analyzed here was obtained from the ncbi geo database  <cit>  via accession number gse <dig>  these data were originally presented in  <cit>  and contain the mrna expression profiles of  <dig> sbt,  <dig> sca <dig>  and  <dig> sca <dig> samples.  <dig> micropapillary pattern sbt samples were omitted from our analysis. de meyer et al.  <cit>  screened the original expression profiles to reduce the number of genes examined and cross-referenced their e2f target genes with the studies of bracken et al.  <cit>  and bieda et al.  <cit> .  <dig> of these genes were classified by  <dig> biological processes in  <cit> :  <dig> for the g1/s phase of the cell cycle,  <dig> from the s/g <dig> phase of the cell cycle,  <dig> checkpoint genes ,  <dig> development gene,  <dig> dna damage and repair genes, and  <dig> dna synthesis/replication genes. a table of the specific genes included is in additional file  <dig>  excluding the singleton subset, we estimated partial correlation networks for each of the  <dig> process-defined gene subsets across the three carcinoma subtypes.

RESULTS
simulation study
for the erdős-rényi g random graph comparison refer to figure  <dig>  apart from the appearance of a weak bias for both the cij =  <dig> and cij = exp cases the p-values are approximately uniformly distributed under the null hypothesis. in panels  and  we see that the nominal type i α level is approximately  <dig> . comparing panel  to  we see the improvement in the power of d when p =  <dig>  and cij = exp. when the neighbouring information was not used in calculating d 34% of the resample p-values were below a nominal α level of  <dig> . when the neighbouring information was included in d and scaled by cij = exp 55% of the p-values were below the nominal  <dig>  level. the likelihood of d detecting the true alternative hypothesis increased from 34% to 55% just by including the neighbouring information here.

for the one-sample correlation network comparison we examined both the type i error rate and the power of d to reject h0: Ω = Ω <dig> versus h1: Ω ≠ Ω <dig>  the tests were performed using cij =  <dig>  i.e., excluding the neighbouring information, and cij= rij, the thresholded correlation estimate. under the assumed null model the distribution of p-values, for both cij =  <dig> and cij= rij, had less mass at the extremes of the p-value range. additional simulation work, see  <cit> , suggested that when n1= n <dig> =  <dig> the type i error rate was conservative, produced an inflated error rate when n1= n <dig> =  <dig> , and the test achieved a proper size when the a priori sample size was a factor of  <dig> larger than the observed network based on a sample of n =  <dig>  these results suggest caution when trying to determine the null distribution for d using a finite set of a priori samples. figure  <dig> graphs pairs of p-values obtained from  <dig> experiments under the alternate hypothesis. this figure demonstrates that incorporating the neighbouring information in the calculation of d via cij= rij penalized our ability to reject the null hypothesis when compared to the use of cij =  <dig> 

additional file  <dig> graphs the type i error performance for the two-sample comparison under the null hypothesis of partial correlation network equality, h0: Π1= Π <dig>  in figure  <dig> we plot the pairs of p-values obtained under an alternate hypothesis. in both figures we illustrate the inclusion/exclusion of the neighbouring information in calculating d . under the null hypothesis we see that the p-values are approximately uniformly distributed with reasonable type i error control. including the neighbouring information does suggest more lack of fit; this is not surprising given the data’s correlated block structure and the correlated components used in d. under the alternate hypothesis simulated here  <dig> of the p-values determined with the neighbouring information were less than the corresponding p-value calculated excluding the neighbouring information, a result comparable to the flip of a fair coin. the more dramatic result is comparing the number of times we reject h <dig> at an alpha level of  <dig> . under h <dig>   <dig> of the p-values were less than  <dig>  when d included the neighbouring information;  <dig> of the p-values were less than  <dig>  when d excluded the neighbouring information. this gain in power by including the neighbouring information differs from the one-sample correlation network simulations. but, as the p-values shift away from  <dig>  and move in favour of h <dig>  excluding the neighbouring information consistently produced smaller p-values. this suggests the behaviour of d may vary according to the network inference method employed.

comparison of derived biological networks
we first compare the correlation networks for the dm <dig> phenotype to the normal phenotype. we test h0: Ωdm <dig> = Ωnormal versus h1: Ωdm <dig> ≠ Ωnormal where we assume Ωnormal is known. we form pearson product-moment-based correlation networks only for those gene sets with fewer than  <dig> probes in the pathway. the  <dig> normal samples were used to form Ωnormal. we emphasize here the potential power of our test since the true state of the null or alternate hypothesis is unknown. to generate the null distribution for d we resample, with replacement, from the original  <dig> normal samples.  <dig>  resamples were used throughout. as in the simulation study, the level of the test may be unreliable due to the resampling procedure employed. estimating an overparameterized correlation matrix based on small samples is a general challenge for the practical estimation of network models.

comparable to the earlier simulation study figure  <dig> illustrates the performance of d including/excluding the edge portion of d and including/excluding the neighbouring information at a correlation threshold of ρ =  <dig> . panel  suggests that the edge portion of d is redundant to the weight component and panels  suggest that including the neighbouring information detracts from the power of d. table  <dig> lists the resample p-values for the  <dig> network comparisons performed at correlation thresholds ρ =  <dig>  and ρ =  <dig>  . the edge portion was excluded from d and, despite the potential loss in power, the neighbouring information was included. even at less conservative type i error levels we fail to declare a network difference in the majority of the cases. the p-values have not been adjusted for the number of comparisons. to illustrate the estimated correlation networks for the two phenotypes near the p-value extremes we provide the normal and dm <dig> correlation network estimates for the map <dig> and map <dig> gene sets in table  <dig>  yates  <cit>  provides a post hoc analysis of the map <dig> correlation network under the assumption of a declared significant network difference.

resample p-values for the  <dig> gene sets analyzed based on correlation networks using thresholds ρ =  <dig>  and  <dig> .

the numerical values are the thresholded correlation estimates. a ‘.’ denotes the lack of an edge between two probes, ‘ <dig> ’ is a visual placeholder. the actual probes differ between the two gene sets.

for the two-sample comparisons we examined a limited ordered comparison of the sbt, sca <dig>  and sca <dig> phenotypes for the  <dig> biological processes. our subjective rationale was that a comparison of the sbt and sca <dig> phenotypes might suggest a biomarker candidate; comparing the sca <dig> and sca <dig> phenotypes might better characterize disease progression or provide insight regarding resistance to chemotherapy agents. table  <dig> lists the number of edges in the estimated partial correlation networks obtained using genenet for the subsets categorized by bracken et al.  <cit> . apart from the dna synthesis and replication process the estimated ggms are either empty or sparse. as a side note – some of our simulation work suggests that genenet tends to underfit a network. table  <dig> lists the resample p-values for a test of h0: Π1= Π <dig> versus h1: Π1≠ Π <dig> across the eight comparisons. all of these comparisons used the neighbouring information in calculating d; apart from the smallest p-value listed the differences between the neighbour/no-neighbour p-values were negligible. at a standard alpha level of  <dig> , a conservative value for a comparison of covariance-based matrices, none of the hypotheses would be rejected. rather than adopt such a conservative view, and ignoring the topic of multiple comparisons, we chose to provide the partial correlation networks for the  <dig> genes in the sca <dig> and sca <dig> phenotypes  in table  <dig>  this table suggests an observable difference between the two networks.

number of edges in the gaussian graphical model estimate for each of the three phenotypes across the five processes categorized by bracken et al.  <cit> .

resample p-values for phenotypic comparisons of the form h0: Π1= Π <dig> versus h1: Π1≠ Π <dig> 

the estimated sca <dig> and sca <dig> gaussian graphical model networks for the dna synthesis and replication genes. off-diagonal non-zero weights indicate the presence of an edge between two genes, ‘.’ denotes the lack of an edge, and ‘ <dig> ’ is a visual placeholder.

discussion
the definition of d was inspired by the question, “how long is the coast of britain?” this question motivates the definition of fractal dimension, a concept suited for complex objects. cutler’s  <cit>  definitions of packing and pointwise dimension suggest measures based on an additive decomposition of sets/neighbourhoods. the neighbourhood size and scaling behaviour can vary across the points in the set. combining these ideas with a riemann-like sum is our basis for a topological comparison of networks. nacu et al.  <cit>  suggest the use of neighbourhoods, of potentially varying radius, for identifying differentially expressed pathways. choosing a node as the centre of the local neighbourhood, and not an edge, allows for several advantages: a reduction in combinatorial complexity, facilitates molecular node-based post hoc tests, avoids the need for an ‘optimal’ tiling or network partition, and it limits the number of relational features/motif-like structures to compare. we limited our set/neighbourhood-based definition of d to internodal path lengths strictly less than three since this is the minimum distance necessary to capture a feedforward/feedback loop, it limits the range of topological comparisons, and d is prevented from revisiting the ‘centre’ of a neighbourhood in a cyclic graph. d can be extended to include a larger neighbourhood. apart from obvious computational implications, defining a second set of cij’s is necessary and the efficiency of d may deteriorate as d integrates a larger set of imprecise or variable estimates. in contrast to fractals, viewing a network as an inhomogeneous mixture of subgraphs suggests that a local distance may be preferable to large distances that span a large set of nodes.

we do not claim that d is or will be optimal under a broad range of network models. apart from the literature related to ergms, weighted biological network models with well-developed inferential theory analogous to maximum likelihood, uniformly minimum-variance unbiased, or minimax estimators is largely unavailable. the ability to apply d to a broad range of edge/no-edge and weighted/unweighted networks is sure to involve tradeoffs. banks et al.  <cit>  illustrate a case where a metric for a clustering application is ill-suited for a phylogenetic inference problem.  <cit>  state the need for fixed/absent edges in their informative prior approach to network inference. a decomposable additive measure can be tailored to reflect meaningful comparisons, e.g.,  <cit>  modify an l1-based edit distance for unweighted binary networks using protein signaling logic. while not constituting a definitive argument, we speculate that the value of including the neighbouring information in the g simulation example is due to the uniformity of p across the set of nodes. the loss in performance using the neighbouring information for a pairwise correlation network model is likely due to d unnecessarily integrating across a larger set of imprecise estimates. for the two-sample partial correlation network comparison, the gain in performance via the use of neighbours may be influenced by the fact that each edge is formed using a plurality of variables. for researchers interested in alternate network models, e.g., preferential attachment or small-world models, evaluating d  under various scenarios is recommended.

basing d on a more general form of a hamming distance parallels efforts in the area of global network alignment scoring. others have proposed alignment-based operations for use with biological networks, e.g.,  <cit>  for comparing phylogenetic trees. the use of set algebraic operations, e.g., union, intersection, strict and symmetric differences, is commonplace  <cit> . having d incorporate neighbouring information might be obvious to a systems biologist; see  <cit>  for an example in classifying breast cancer metastasis and  <cit>  for a pathway differential expression application.  <cit> , using partial correlations, found it useful to restrict the number of genes/nodes to condition on.  <cit>  use level- <dig> and level- <dig> neighbours to predict protein function.  <cit>  outline a weighted topological overlap measure to cluster gene modules.  <cit>  present a topological overlap measure that generalizes pairwise similarity to one based on shared neighbours. the precedent of incorporating neighbouring information for network objects is clear; the question of, “but how far do we go?” is less clear. as mentioned earlier, the matter of neighbourhood size is liable to impact the behaviour of d and may interact with network estimation procedures, data collection requirements, etc.

despite d’s simplicity and potential for use across a broad set of network models, the importance  of d’s components should receive serious consideration by the investigator. gower  <cit>  concedes that weighting components of a measure is challenging. a robustness study is recommended to address this topic, see yates  <cit> . for both one- and two-sample comparisons, tuning d to gage or improve its performance for a specific network model is advised. the selection of the weight constant cij for weighted graphs was motivated by the idea of conditional probabilities. if a and b represent two adjacent edges and information flows through their common vertex then it is reasonable to assume that some form regarding the state of b is meaningful to a. it is plausible to assume that the force two objects exert on one another is proportional to their proximity. preferential attachment networks assume that new edges are formed at a node conditional on the number of existing edges at that node. wei et al.  <cit>  employ gene-specific prior probabilities in a spatially correlated mixture model application. but, the quality of the weights may be suspect. ashyraliyev et al.  <cit>  found quantitative parameter estimates to be unreliable in modeling a gap gene circuit but that inferring a reliable qualitative network topology was possible. the relative weighting of edge and weight differences is a topic to discuss here. the idea of scaling or normalizing portions of d is complicated. gao et al.  <cit>  allow hubs to exert an unequal influence; yip et al.  <cit>  normalize their generalized topological overlap measure to the unit interval. given a rough similarity between d and the total sums of squares in regression modeling we allow for nodes with a high degree  to contribute more to the calculation of d. if one chooses to normalize portions of d at each node by some topological property then one has to justify the scaling factor. does one scale by the node’s  degree, a clustering coefficient, in- or out-degree for a directed network, etc.? if a network model has an efficient estimator whose sampling distribution is well characterized, the use of d may be contraindicated. at present, we do not expect this to be the case for experiment-based weighted biological networks. as a conservative measure, d could be reduced to its hamming-like form and only the inferential procedures outlined here applied. evaluating d using an in silico model under plausible scientific scenarios of interest to the investigator could also reduce concerns about aspects of the definition of d.

our choice of network architectures to evaluate with d was limited. computational expediency, an ability to explore and contrast common parametric models and their associated weighted network forms, and the use of a canonical structure  graphs) motivated our network model selection. both marbach et al.  <cit>  and altay et al.  <cit>  discuss difficulties associated with currently available network inference algorithms. müller-linow et al.  <cit>  found that the proximity of metabolites in a correlation network did not align with metabolite proximity observed in genome databases. hubert et al.  <cit>  suggest that final structural representations are unlikely to be global optima since the selection procedure, while reasonable, does not make use of a verifiable optimal search strategy. even the limited simulation work presented here suggests the nuance that network algorithms or models can inject into the process. relying on a priori samples or an assumed null model to generate a sampling distribution for d may be restrictive in the one-sample case; but, the complexities surrounding network probability models and the intractability of network-based test statistics does not suggest easy alternatives. large  networks are likely to be costly in terms of the data needed to estimate a family of network parameters. the literature on the analysis of large networks, with its emphasis on select topological comparisons, typically imposes a vast reduction in network complexity. thorne et al.  <cit> , in proposing a method to generate confidence intervals for network-related correlations and motif-abundances reinforced the complexities in defining a suitable null model for a biological process.

general criticisms leveled against the use of resampling methods are applicable here; see berger  <cit> . the practical matter of using d for observation studies raises the topic of partial exchangeability. it is unreasonable to assume that either of the biological datasets presented here met the assumption of exchangeability. however, unconditional procedures also struggle when confronted with observational data and/or missing or hidden covariates. our one- and two-sample comparison approaches require or assume a hypothesized network model for the purpose of sampling; we do not employ fixed-network random rewiring schemes or variations that seek to align a set of topological measures across a set of stochastic networks.

a significant one- or two-sample finding naturally suggests the question, “where do the networks differ?” similar to individual effect tests in regression, the answer to this question will involve one or more nodes or subgraphs. answering targeted questions routinely appear in the literature, see  <cit> . dong and horvath  <cit>  suggest approximately factorizable networks;  <cit>  locate regulatory hot-spots by integrating network topology and transcriptome data. since d is a sum of node-based dissimilarities, a test for dissimilarity between two aligned nodes or subgraphs can be formed using the portion of d attributable to that node or subgraph. such an approach may help isolate molecular targets or subnetworks for closer study. determining the sampling distribution for the modified test statistic is identical to the procedure developed here. see yates  <cit>  for more details and examples.

CONCLUSIONS
in this paper we have demonstrated an inferential framework for performing one- and two-sample hypothesis tests for biological networks. we proposed a suitable test statistic and evaluated it with both simulated and microarray data under a variety of situations. the dissimilarity test statistic proposed is a logical extension of existing measures used in sequence alignment/nominal data comparisons. in generating the null distribution of the test statistic we used an assumed parametric model, resamples from a collection of null samples, and made use of permutation testing principles. resampling methods suggest that counterintuitive behaviours may occur when dealing with a  network model. while not explicitly demonstrated here, our procedure easily facilitates node-level or subgraph post hoc tests.

competing interests
the authors declare that they have no competing interests.

authors’ contributions
this work is based on the phd dissertation of pdy. ndm suggested the problem and supervised the research. all authors read and approved the final manuscript.

supplementary material
additional file 1:
r code.

click here for file

 additional file 2:
ovarian cancer genes analyzed. subset of analyzed genes as categorized by bracken et al.  <cit> .

click here for file

 additional file 3:
two-sample comparison for partial correlation networks under h <dig>  a uniform qq-plot of the  <dig> resample p-values for a test of h <dig> Π1= Π <dig> versus h <dig> Π1≠ Π <dig> under the null hypothesis. the y-axis is the observed p-value; the x-axis the expected p-value.

click here for file

 acknowledgements
the authors thank drs. el boone, rk elswick, jr., l. dumenci, and v. ramakrishnan for serving on the dissertation committee. the authors also thank the referees for their helpful comments and suggestions.
