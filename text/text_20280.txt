BACKGROUND
one of the largest challenges in biology is relating the genotype  to the phenotype . in the past years, there have been major breakthroughs to efficiently determine the genotype using next generation sequencing technologies. moreover, in several fields, such as plant biology, major progress has been also made to efficiently and accurately quantify the phenotype  <cit> .

in particular, the root is frequently the subject of systematic and large-scale phenotyping, as its growth properties are of major importance for plant productivity, but has not been the target for extensive breeding efforts. as a simple model, the root of the model plant arabidopsis thaliana plays an important role in the root phenotyping community. this is not only because arabidopsis is a small plant that can easily grow in large numbers in vitro to allow for efficient image acquisition of the root, but is also a great object for transgenic approaches  <cit> , it has large mutant collections  <cit> , and many sequenced genotypic variations are available  <cit> .

the aim of arabidopsis root phenotyping experiments is to observe the growth of as many roots as possible with various genotypes. a large-scale phenotyping pipeline has been recently developed  <cit>  allowing for the acquisition of thousands of root images per day in very high resolution and over subsequent days. seeds are placed on the surface of agar plates that are filled with a nutrition gel. each compound of that gel is very accurately controlled, so that each plant has the same conditions to develop. the placement of the seeds are guided by a predefined grid layout, so the genotypes of each particular plant can be traced back during analysis. apart from image acquisition events, the plates are kept in a growing chamber, in controlled conditions.

the resulting images are subjected to image-processing based analysis methods for measuring traits . while the computational analysis of these and other root phenotypic data is developed well , there is no tool allowing to visually explore or compare the complex phenotypes of multiple individuals or their statistical properties. moreover, with hundreds of thousands of images and different genotypes distributed to many images, it is a challenge to conduct visual checks or to produce figures for presentations of specific genotypes. finally, after publication of such data, the current method to share visual  root data is to mail complete hard disks of images upon request making data sharing bothersome. these problems are amplified when working with time series of such complex data.

in this work we propose a system that supports root growth phenotyping of young seedlings of arabidopsis thaliana, being designed for analyzing its earlier stages when the root individuals have not developed lateral branches. the domain experts usually use that early-staged analysis when they are interested in comparing geometric properties of the root individuals . a simplified explanation of root phenotyping is the following. the researchers group individuals according to their genotype. in our test dataset a typical group has 10- <dig> individuals, but sometimes less, and in extreme cases many more is possible. for each individual in group traits are measured. then, statistics  are used resulting in one trait representing a genotype. finally genetic wide associaton mapping helps to reveal statistical relevance between the measured trait and the genetic sequences. this statistical approach already led to significant results, however it does not convey the characteristics of processed data.

in this paper we focus on a visualization system, which helps characterize the phenotypicical expressions, grouped by a common property .

in root phenotyping it is common to measure 15- <dig> individuals per group. however, according to the knowledge of the authors, the question was never approached if  <dig> individuals per group is enough for a valid statistics in root phenotyping. our system allows the domain researchers to visually explore many individuals which are grouped by their genotypes. this allows for the generation of highly informative figures for presentation and publications that go beyond root trait averages and make it possible to intuitively asses heterogeneity between different individuals.

related works
the shape variability between roots of one genotype can be analysed with respect to different properties. one possible way is to consider each root as a curve in a two-dimensional space. since each root  represents a possible different growth pattern, which defines the source of uncertainty in this context, one can make use of descriptive statistics to analyse the uncertainty associated to the process.

graphical representations of one dimensional descriptive statistics  have been widely used for statistical analysis in many different fields. its main purpose is to visually convey statistical quantities estimated from the data with few assumptions on the underlying distribution. several generalizations for multivariate statistics have been proposed. bagplot  <cit> , one of the earliest point-wise generalizations, uses convex hull peeling to convey the notion of centrality, spread, correlation, skewness and tails of the estimated distribution. distance based depth functions  are also a classic estimation strategy with the additional property of being simple to implement, while achieving good centrality estimates  <cit> . it has been shown that carefully aggregating many different plots into a single one can be an effective way to summarize such statistics in a meaningful way, while avoiding cluttering  <cit> .

from the data perspective, the linear root growth is highly related to trajectory data, like simulated ensemble of hurricane trajectories  <cit> . the shape variability of the trajectories ensemble exposes the need for a better understanding of the underlying uncertainty of the simulation process.

in the scientific visualization literature there have been many successful attempts to provide algorithms for understanding uncertainty through visualization. they are able to deal with different kinds of complexity such as: high dimensionality of the data, time series and ensemble simulations. in this sense, the visualization contribution of our work can be categorized as uncertainty visualization of ensemble of functions  <cit> .

cox et al.  <cit>  propose a visual representation of path ensembles in the context of hurricane forecasts. however, while it can lead to a cluttered and sometimes overcrowded visualization, its aggregation strategy can nicely convey the notion of shape variability among one ensemble of curves. we use a similar strategy as one part of our system by aligning the roots to a common starting point, which allows for the domain experts to explore the variability of shapes within each genotype.

in a similar context of weather forecast models, sanyal et al.  <cit>  propose a visual metaphor combining 1d colormaps with variable sized circular glyphs in order to encode variability among ensemble members, which does not allow for easily comparing different ensembles and also does not take into account time dependency.

the recent usage of radial basis functions  with adaptive bandwidth selection, in the context of hurricane prediction, improves the smoothness of such visualizations by interpolating a simplicial depth sampled from its path ensemble  <cit> . apart from leaving some parameters to be set by the user, their strategy is costly since, for each time step, they need to solve linear systems with the order equal to the number of samples, then to estimate minimum enclosing ellipses, and to perform a nonlinear filtering for smoothing its time step trajectories.

in descriptive statistics the data depth conveys the notion of centrality of the samples within the data distribution. in order to perform such estimates, one needs, among other things, to define the notion of center-outwards ordering, called data depth, where the "deepest" sample is the closest to the center. in the 1d case it is straightforward to do so by using a simple ordering of the samples, and using the distance to the median as the centrality measure. there are many different ways on generalizing this concept to higher dimensional samples  <cit> , to curves or functions .

sun et al. generalized data depth to 1d functions defined on a common interval. the authors defined bands with j samples, j at least two. a band consists of functions, chosen from the ensemble. a function lies in a band, if for the entire common domain interval its value can be bounded by other values coming from functions from the band. a data depth of a function is defined as the estimated probability of the function-in-question lying inside a band. it was also proved that computing band depth with iteratively increased j, than summing up the result gives a more robust estimator.

mirzargar et al. took the aforementioned method, and generalized it to multidimensional curves  <cit> . in their approach a curve lies in a band, if for the entire domain interval its points are inside the convex hull of the points, which are evaluated from the curves of the band.

in the aforementioned methods the most representative sample  is chosen to be the one, which has the highest depth value, consequently it is the most “central” one. as we show it with examples, this approach works well if there are enough samples for the analysis.

methods
motivations
biology researchers use numerical traits to describe root ensembles. these traits are crucial to understand the underlying phenomenon. however, these biological traits describe only one aspect of a root ensemble, therefore it is difficult to give answers for questions which relate to the behavior of the root ensemble as a whole. before thorough analysis it could be very useful to explore: 
what are the growing trends in the ensemble?

what is the variability of the ensemble?

what could be a typical root in ensemble?

what is the statistical quality of the ensemble?

is the acquisition feasible at all?




we were looking for a visual descriptive statistics, which addresses some of these requirements.

our system is tailored for early-staged phenotyping , and the traits used in these experiments aim to describe the shape of the root individuals. moreover, our domain experts claim that if a root branch  already developed a shape, that part will not change in future in these in-vitro experiments. in other words, to model the shape of the main root, it is enough to represent the time dependent position of the root-tip. putting the aforementioned detail under consideration, we decided that an early-staged growing process of a root individual can be modelled as a time-dependent parametric planar curve.

an overview of our system pipeline can be seen at fig.  <dig>  our system is a visual analysis frontend of the busch-lab root analysis toolchain system , which was developed by busch et al.  <cit> . for our methods we turned the representation of root individuals from pixel-based to a curve-based one. our visualization components  show various statistical aspects of a curve ensemble. for the spatial-variability visualization a centrality estimation is needed. this is achieved by our robust l
 <dig> data depth estimation component.
fig.  <dig> on overview of our system pipeline. green indicates imported technologies or methods




the brat toolchain outputs a subset of segmented pixels, where every pixel belongs to a root individual. for visual analysis we turn the pixel-wise representation into b-spline curve-based representation, where each curve represents one root individual. the knot vector  and control points are stored in an sql database. during visualization stage, according to the users needs our client application reads these parameters for the requested root individuals, and forms a curve ensemble for further analysis. every proposed method in this paper takes these b-spline curve ensemble as its input.

for our visual statistics of curve ensemble we have chosen the curve boxplots , which is a recent work of mirzargar et al.  <cit> . on one hand our system heavily relies on it, and uses similar visual design for spatial quartile zones. on the other hand due to the special manner of our application we had to introduce changes and extensions: 
a more robust curve data depth estimator

a robust ensemble representation

time-lines with confidence zones

ensemble validity indicator


fig.  <dig> concept of curve box plot. dark gray zone represents the 50% quartile, light gray zone represents the 100% quartile. green curve is the most central curve, red curve is the proposed representative curve





the key element of a visual statistical method is the choice of data depth. in case of functional data  it is most commonly based on band data depth, as it was the case also in mirzargar et al.  <cit> . however, in our application it occurs many times that the size of a root ensemble is rather low. in these cases the band data depth is not sensitive enough for the relative distances of the curves. furthermore, occasionally in these cases the usual way of picking a representative curve  does not seem to work properly. so we propose a new representative curve estimation, which could fit better to the small sized ensemble intuitively.

in our application we followed the same method as in the aforementioned works for determining inter-quartile, and inlier zones. with data depths we have an ordering. the median of the depth values is computed, and samples, whose depth values fall into the range of the highest depth value and the median depth value are considered to lie in the inter-quartile zone. a threshold for inlier/outliers is also computed, as the length of the inter-quartile depth range multiplied by a chosen scalar. for this scalar parameter we chose  <dig> in our application .

curve box-plots only tell us whether a spatial position has a probability to be occupied by a root, but it does not say anything about when it could be reached. however, growth time is a crucial information for our domain experts, so we designed a representation to convey that information for the user. in root phenotyping time and growing speed is an essential property, which is analyzed thoroughly. therefore it was vital for us to develop a visual representation, which could informatively express the overall growing speed, and its deviation in the ensemble.

reconstruction of roots as curves
our system was built on the top of the busch-lab root analysis toolchain system , which was developed by busch et al.  <cit> . the brat system allows for acquiring the images and creating binary segmentation. brat root segmentation framework first detects plants by finding their shoots . then it is assumed the edges of roots can be detected in the proximity of shoots by finding highly saturated edges . then the main root pixels are estimated with skeletonization.

representing roots with their raw segmented pixels has some drawbacks. due to the high resolution of scans these pixels can be numerous, even if the represented root is relatively smooth. in addition, the segmentation results can be noisy. therefore we chose to use a parametric representation, namely non-uniform b-spline curves  <cit> . b-splines are widely used in many fields, including cagd and statistics. they are flexible piece-wise polynomial representations, which can describe complex free-form shapes with relatively few parameters. in addition, with the right number of control points, a b-spline representation can smooth out the noise while keeping the complexity of shape in question. moreover, since it is a parametric representation, it allows a visually appealing rendering.

each root representation estimation starts with the list of segmented pixels , which belong to the same underlying individual root. one of the pixels is selected as the starting point . first parameter values have to be assigned to every discrete pixel. for that the segmented points have to be ordered, which is not necessarily straightforward due to many segmentation errors. we used a pixel-based dijkstra-algorithm, starting from the chosen start point, and the assigned parameter values are the resulting dijkstra-distances, normalized to  <cit> .

choosing the correct number of control points is important: using too few will over-smooth, using too many will over-fit the noisy data. we used a conservative measure proposed by yuan et al., which is based on fourier analysis of the b-splines  <cit> . their argument was, that the lower bound of the number of needed control points is #cp≥#samples4π, where the b-spline’s domain is . we fit our curves to domain of  <cit> , and we empirically selected #cp= <dig> ×#samples4π. with this choice the number of control points is above the lower bound, but close enough to avoid over-fitting. the knot selection comes from deboor’s classic knot selection algorithm  <cit> . it ensures, that the knots are chosen in a way, that the number of samples falling into curve segments are relatively the same.

then a least-squares optimization is performed, determining the control point coefficients. we used 4th order  curves. as a result we represented a root object as a continuous parametric curve in the image domain. however, our intention is to represent the time-dependent growth of root individuals with curves, not only their shape. therefore we re-parametrized our b-spline curves from  <cit>  to , where d stands for the day when the image containing the segmented root individual was acquired. despite it is a rough time-parameter approximation, and only accurate in daily unit, our domain experts agreed that it is enough for their purposes. however, technically more accurate estimation is possible. a raw acquired image of a plant, segmented pixels and reconstructed root-curve can be seen on fig.  <dig> 
fig.  <dig> on the left there is an example of acquired image of a root. the results of pixel level segmentation are shown in the middle. while on the right, reconstructed curve is shown




visual representations of root ensembles
our visual representations take root ensembles as inputs, where a root is modelled as a b-spline curve. the visualizations represent various statistics of curve ensemble, therefor they can be seen as parallel functionalities in our pipeline.

l <dig> data depth
for curve box-plot visualization, which represents the spatial variation of the ensemble data, a proper data depth estimation step is responsible for measuring the centrality of each curve. however, as it was mentioned before, the high variability in size of our ensembles makes it necessary to apply a robust data depth estimation.

vardi et al.  <cit>  introduced a robust data depth for multidimensional euclidean point-set, that they called l
 <dig> data depth. their formulation stems from the solution of a completely different problem, namely the fermat-weber location problem.

the geometric median  is the theoretical solution of the fermat-weber location problem.

in general case the geometric median is estimated by a simple iterative method, the weiszfeld algorithm. vardi  <cit>  pointed out a “leak” in the original classic method, and proposed the modified weiszfeld algorithm.

more importantly, the authors introduced a depth function, which they called l
1-depth. it can be seen that the a normalized weighted sum of the distances to the input samples is assigned to each position of the input space.

it is important to note, the point that maximizes the l
 <dig> data depth for a given point-set , is the fixed point of  weiszfeld algorithm, the geometric median. a great advantage of l
 <dig> data depth is its efficiency to calculate. however, a greater advantage for our application is that it is very stable, even for small number of samples. for further details and definitions please refer to  <cit> , or appendix a for a summarized version.

applying l <dig> data depth to b-spline curve ensemble
one of our main statements in this paper is that l
 <dig> data depth, which is so far defined on euclidean space can be exported to our b-spline representations.

so far, all curves in the ensemble are defined by different basis functions. in order to make them comparable, we have to convert them into a common basis. first, we check what is the maximal number of control points our curves in the ensemble have, that will be the control point number of the common basis. then, we concatenate all knots of every b-spline system in ensemble into a temporary buffer. after sorting and removing repeated knots, we define a new knot sequence with the already mentioned deboor’s method. each original curve is evaluated in several discreet parameter values, falling into the interval of the common b-spline basis. then with least-squares fitting we get the new curve in the common basis.

in this paragraph we claim that a b-spline function is well defined by its coefficients. also, it is easy to see that these b-spline functions hilbert-space distances can be approximated by their coefficients euclidean distance. for simplicity lets consider 1d b-spline functions at this moment, which are spanned by the same bases b
1…b
n . the functions are also members of the hilbert-space with l
 <dig> scalar product. lets define an operator





which maps the coefficient vector to its b spline function. it is known, that t is a linear bounded operator. also, we can define its adjoint operator





which is also known to be a linear bounded operator. the proof for generic cases  can be found in chapter  <dig> in  <cit> , as lemma  <dig> . assuming that f is a result of eq.  <dig> for some coefficients c
1…c
n, these original coefficients can be traced back by solving the system g[c
1…c
n]t=t
⋆, where g is the gramm-matrix of b
i-s. the boundedness of the coefficients comes from the fact that g is a positive definite d-band matrix, where d is the order of the b-spline basis.

our data depth estimation for non-uniform b-spline curve ensemble is simple. we re-define our b-spline curves into the same basis. then for each curve we concatenate its control points coefficients, so we represent each curve as a 2×n dimensional euclidean vector. we compute the l
 <dig> data depth for these vectors, and we can assign these depths as the data depths of the curves.

geometric median curve
the state-of-the-art methods choose the most central curve as a representative. in this way the representative curve is part of the ensemble.

in this section we propose another possibility. after we computed the l
 <dig> data depths each curve is represented as a 2×n dimensional vector. we compute the geometric median of these representative vectors. however this geometric median vector also represents a b-spline curve. as a consequence of the linear boundedness property of eqs.  <dig> and  <dig>  the geometric median based curve approximately inherits the “most central” property from the euclidean vector representation.

time information estimation
since our curves represent a time-dependent root growth, visualizing time is also an important issue. for time visualization we chose to use a design, where we show a continuous “front-lines”, or time-lines for different time values , also with their confidence zones. these confidence zones of time-lines deliver a visual clue about the variance of the statistical estimation of the particular time-line. it can deliver information about how diverse the growing speed of the rootsin the ensemble is.

a naive approach would be to estimate a smoothing spline for every time-line separately from the evaluated points of root-curves. however, in that way we would face technical difficulties, like parameter estimation, but more importantly we would not be able to ensure that the splines would not “tangle” to each other.

we followed a different approach. lets assume that t
 <dig> …,t
m are discreet values in the  domain of ensemble. the curves in the ensemble are evaluated in all t
i,i=1…m, resulting in pi∈ℝ <dig> i=1…m. we are looking for an interpolating smooth surface f:ℝ2→ℝ, f=t
i,i=1…m, as shown in fig.  <dig>  for this task literature offers many possibilities for statistical surface interpolation .
fig.  <dig> 
top left: input 2d sample , with control points as centers of tps . top right: input samples the time value as 3rd coordinate. bottom: fitted tps surface with its isocurves, representing front-lines




thin plate spline  surfaces are widely used in a variety of statistical applications, like machine learning, or non-linear support vector machines  <cit> . it is an interpolating surface with the property, that among all other smooth surfaces it minimizes the functional ∬ℝ2∂2f∂x22+2∂2f∂x∂y2+∂2f∂y22dx. this minimal property ensures us that the time-lines are not crossing each other.

a thin plate spline is defined in the form 
  <dig> f=β0+βtx+∑j=1nαjϕj, 


where β are coefficients for linear part, α
j,j=1…n are coefficients for the non-linear part, which are determined by kernel functions ϕ
j=‖x−c
j‖ <dig> log. here, the cj∈ℝ <dig> are pre-established centers. the linear and non-linear coefficients are found with least-squares fitting.

the reader can notice that estimation of tps surface does not need any other parameters  beyond centers.

in literature there are two approaches how to determine centers. the “classic” way is that samples p
i serve as centers. however, it produces a huge linear system to solve. also, ϕ
i= <dig>  therefore the matrix of the linear system would have a zero diagonal, which would lead to a extremely badly conditioned system. a second approach is to define centers as points on grid, considering only those which are inside the convex hull of input samples. however, that needs predetermining the grid size as an initial parameter, and also involves inclusion test for each grid points.

we decided to take advantage of our non-uniform b-spline representations. we used the control points of the curves as centers. this approach is resolution-less , however the control points are still “close” to the input samples . in addition, the formulation of the resulting tps will not depend on m, the discretization parameter of the curves. we got very similar results to the one with using sample points as centers.

the time-lines are rendered as isocurves of the tps surface in the planar domain .

we also wanted to convey a confidence zone for time-line. for that, a variance had to be computed for each timeline isocurve. every curve in ensemble was evaluated at the time value in question. then, the spline surface was also evaluated, getting the estimated time value for each position of the curve. the variance is computed from the differences of the expected time value and the evaluated time values.

time-lines are rendered as isocurves of the fitted tps spline. for each timestamp  we strike the isocurve of to the timestamp value. the corresponding confidence zone is rendered by alpha blending, linearly decaying toward the border of the zone. the time-lines are connecting to a timestamp indicator ruler continuously. in addition, each time-line is rendered with a unique color coming from a colormap. we chose a colormap based on color blue, where brighter blue colors indicate earlier timestamps. as examples in fig.  <dig> show, this technique immediately conveys diversities in growing speed among the ensemble.
fig.  <dig> examples for time-line visualization for different genotyped ensembles, with error zones. the technique immediately conveys diversities in growing speed among the ensemble. top: ensemble with  <dig> curves. bottom: ensemble with  <dig> curves




validity indicator of median estimation
in this section we propose a visual representation, which conveys information about how feasible our representative curve estimation is for the given ensemble. basically we compute a spatial confidence zone for the representative curve estimation, with respect to a “little” change in the consistency of the given ensemble. since representative curve estimation is a robust deterministic method, we had to borrow a statistical method, called bootstrapping.

lets define e={x
 <dig> …x
m} as the curve ensemble represented by euclidean vectors . then, we can define e
i={x
r <dig> …x
rm}, where i is a number between  <dig> and b, where b is the bootstrap samples, a predefined parameter. also, rj is a random integer between  <dig> and m. these randomly picked indexes do not necessary have to be distinct, and also it is allowed that they do not cover all numbers between  <dig> and m. then, since e
i is also an ensemble, we can compute its geometric median representation curve. lets denote that as x^i. if we repeat this computation b times, we can collect all x^i into a new ensemble Ê, consisting b vectors . if we compute the l
 <dig> data depth for Ê, the corresponding curves inter-quartile zone represents the spatial zone, which we call validity indicator of median estimation.

although we present this visual representation for a particular application, since bootstrapping is a general method, similar approach could be used to visualize the feasibility of other median/representation estimator.

RESULTS
implementation
we implemented our methods in c++, with the utilization of eigen and boost libraries. visualization was implemented in opengl.

for visualization of the ensemble, roots are represented by non-uniform b-splines, whose control points are in image space, scaled that one unit means one pixel. we found it informative if we showed a ruler with units in mm. the metric length could be computed from the dpi resolution of the scanned image, whom the root individual was acquired from.

for comparison we also implemented band data depth for curves, as it was presented in work of mirzargar et al.  <cit> . since our curves  are extremely irregular, for band depth we were forced to use the technique, which the authors called modified band depth. in this case the a curve’s inclusion in a band is not a binary function, but relative length of parameters where the curve lies inside the band.

initially we compared the computational speed of the two algorithms. in order to create a fair algorithmic comparison, we did not use multi-core optimization for either methods. we evaluated both methods for systematically increasing ensembles, where the individuals were randomly selected. for band data depth we used  <dig> discrete samples for curves. figure  <dig> shows, that for smaller sizes, the computational speed is comparable. however as size increases, the needed computational requirements became higher by orders. however it is also fair to mention, that band data depth is easily parallelizable, while for l
 <dig> data depth it is not that straightforward to parallelize.
fig.  <dig> computational speed for data depth estimations




geometric median curve
a big difference to the state-of-the-art is that the proposed representative curve is a virtual curve, it is not part of the original data ensemble . despite the proposed representative curve seems centralized, we had to verify its representation capability with domain experts. in addition we wanted to find out if our method compares better or worse to the existing state-of-the-art method. for that purpose we conducted a survey among domain experts with a comparative questionnaire.
fig.  <dig> comparing boxplots for the same curve ensemble , but estimated by different methods. green curves represent the deepest ones, red curves show the geometric median-based ensemble representer. left: mirzargar’s method. right: proposed method


fig.  <dig> comparing boxplots for the same curve ensemble , but estimated by different methods. green curves represent the deepest ones, red curves show the geometric median-based ensemble representer. left: mirzargar’s method. right: proposed method




in our survey we created a pdf form, which was spread by emails to domain experts internationally. the form started with an introduction, then it contained seven cases . each case the same root ensemble were shown with two different versions of curve box plots next to each other. one version was estimated with mirzargar’s box plots, using band data depth, and one visible curve indicated the deepest one as representative. the other version was also with curve box plots, using the proposed l
 <dig> data depths, and the representative estimated with our geometric median based method. the order of methods were randomly chosen. the experts had to rate from  <dig> to  <dig>  which version they found more representative. a rating of  <dig> meant the expert found the left plot more representative, while  <dig> indicated the same with right one, and  <dig> meant no significant difference.

we scored the answers of the questionnaire. an answer got score in  <dig> scales according to preference of the expert, ranging from “absolutely the mirzargar’s one”  over “no difference  to "absolutely our one” . as it can be seen on table  <dig>  the effect is rather small. on the other hand an important observation is, that if the ensemble contained less than  <dig> samples, the experts preferred the geometric median curve over the band data depth median.

ensemble size

mean
the methodological preference for an ensemble was computed by averaging the domain experts scores for the ensemble. if the ensemble contained less than  <dig> samples, the experts preferred the geometric median curve




despite that several domain experts found the new representation interesting , a smoothing effect over the representer curve has been noticed. in root phenotyping this is not desired, since some traits  are not represented well. this effect can be related with the proposed feature space and it requires a further investigation of its cause.

validity indicator
the validity indicator is defined as the inter-quartile of Ê, as it was previously detailed. to avoid confusion with the spatial box plot, we decided to render the indicator as an outline curve.

a very important question may arise, namely what parameter should be chosen as the number of bootstrap iterations. we answered this question empirically. for some ensembles with relatively high diversity, we computed the validity indicator with increasing number of bootstrap iterations. since bootstrapping is a stochastic method, too few iterations will cause changes in the shapes of indicators. our decision was that for an ensemble with 15- <dig> individuals using more than  <dig> iterations does not change the indicator shape significantly. an example can be seen in fig.  <dig>  in that plot some changes in the shape of the indicator can be seen when few iterations were applied.
fig.  <dig> showing geometric median curve validity indicator computed by different number of bootstrap iterations. using more than  <dig> iterations does not change the indicator shape significantly. top left:  <dig> iterations. top middle:  <dig> iterations. top right:  <dig> iterations. bottom left  <dig> iterations. bottom middle:  <dig> iterations. bottom right:  <dig> iterations




a straightforward hypothesis is that the area of validity indicator increases, as the size of the ensemble decreases. this hypothesis was confirmed by our experiment. we have a genotype, to whom around  <dig> individuals were segmented. for the full ensemble, and subsequent subsamples of them we estimated the validity indicator. results can be seen in fig.  <dig> 
fig.  <dig> plots with different ensemble size, with the same genotype. the area of validity indicator increases, as the size of ensemble decreases. left: ensemble with more than  <dig> curves. right: random subsample of left, consisting  <dig> samples




however, an interesting result is that the shape of the validity indicator can also be different for two ensembles, even if the ensembles contain around the same number of root individuals . this difference can only stem from the difference of the ensembles.
fig.  <dig> plots with similar ensemble size, but with different genotype. the shape of the validity indicator can also be different for two ensembles with similar sizes. left: ensemble with  <dig> curves. right: ensemble with  <dig> curves





feedback from domain experts
during development, we consulted with domain experts in root phenotyping. as stated by them, this method allows efficiently to inspect and compare variation of root growth patterns. according to their knowledge, there is no other tool to efficiently do this. this enabled for the first time to efficiently visualize complex statistical root growth properties for the roots in multiple large datasets that already exist. it allows for a quick visualization that would be otherwise take hours to assemble using the current workflows.

the visualizations of a large set of accessions led the domain experts to the immediate insight that there is a genetically determined control of variance for root growth that seems to be time dependent. some genotypes display control of root growth variation strictly throughout the five days of the time-course while other only do it during specific time-points; other genotypes don’t control for root growth variation very much. these observations indicate that the parameters for the visualization seem to be suitable for genetic mapping to identify genetic components that control this root growth variance.

CONCLUSIONS
presented workflow has taken as input images with segmented pixels of root centerline and resulted into visual representation of statistics of root shapes for comparison within and in between given ensembles. while this approach has been tailored for the particular problem in plant phenotyping, the method can be in principle applied to any application domain where images capture an evolving structure forming an ensemble. these could be movement trajectories of obtained from cameras of trails in public spaces for urban planning purposes, movement pattern studies of animals, such as flies, bees, ants or mice to name a few.

we have extended a recently introduced curve box plot representation with the notion of time. in addition we introduced a new visual clue to represent the statistical feasibility of the curve ensemble. we have introduced the proposed representations to the root phenotyping community and they seem to have the potential to form a standard in communicating summary of the emerging structure of the root ensemble. last, but not least we have contributed to the phenotyping community with a new representation for roots using parameterized curves that has drastically reduced the storage requirements. while this is not a strong contribution to visualization research, it can be a game-changer in how data for root phenotyping is stored and shared within the respective research community. a similar approach can be adopted in research areas dealing with evolving linear structures where method of sharing data are still large image collections.

appendix a modified weiszfeld algorithm and l <dig> data depth
the geometric median  is the theoretical solution of the fermat-weber location problem. given the sample points x={x
1…x
n} in an euclidean space, the problem is to find 
  <dig> y=argminx∑i=1nx−xi 


the modified weiszfeld algorithm starts with initial value y
0=mean, and until convergence the following is iterated: 
  <dig> yi+1=1−wyiryi+∑xi≠xxi∥xi−x∥∑xi≠x1∥xi−x∥−1+min <dig> wyiryiyi 


where 
  <dig> r=∑xi≠zxi−z∥xi−z∥ 


and 
  <dig> w=1ifz=xk,k=1…n0otherwise. 


using w and r, the l
 <dig> data depth is defined as follows 
  <dig> l1d=1−maxr−w,0n 


appendix b linear system for tps fitting
the linear and non-linear coefficients of eq.  <dig> are found with least-squares fitting, solving the system of 
  <dig> ay=b, 


where





since the kernel functions are not compact, the to-be-solved linear system is badly conditioned. therefore an iterative method is advised to use by the literature. in our experiments we used conjugate gradients to solve the symmetric system a
t
a
y=a
t
b.

the authors would like to thank manuela waldner for the help in designing the survey.

declarations
this article has been published as part of bmc bioinformatics vol  <dig> suppl  <dig>  2017: proceedings of the symposium on biological data visualization at vis  <dig>  the full contents of the supplement are available online at http://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-18-supplement- <dig> 

funding
this project has been funded by the vienna science and technology fund  through project vrg11- <dig> and also supported by ec marie curie career integration grant through project pcig13-ga-2013- <dig>  work in the busch lab is supported by funds from the austrian academy of science through the gregor mendel institute of plant molecular biology . publication costs were funded by the vienna science and technology fund  and the gregor mendel institute of plant molecular biology. these two funding sources have contributed equally.

availability of data and materials
the datasets used during the current study available from busch lab in gregor mendel institute  on reasonable request.

authors’ contributions
the general idea of using curve box-plots came from vv. he is also responsible for the implementation of all the proposed methods. the application of weiszfeld method and l <dig> data depth is a contribution of vv and dc. vv and dc were the major contributors in writing. pf’s contributions include suggesting the bootstrap method, and verifying the mathematical correctness. wb acted as our biologist expert and supervisor, and he helped us in conducting an international survey. iv was the supervisor in visualization, as such he was responsible for the novelty in the visualization community.

competing interests
the authors declare that they have no competing interests.

consent for publication
not applicable.

ethics approval and consent to participate
not applicable.
