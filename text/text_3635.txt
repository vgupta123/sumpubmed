BACKGROUND
one key function of the cerebral cortex involves the integration of elements into a percept that separates them from the background. in this process, changes in cortical networks are formed and modified by experience through the simultaneous excitation of groups of neurons  <cit> . these "long-range connections formed by excitatory cortical neurons"  are considered the anatomical substrate of this integrative capability. this integration has been modeled in detail for the visual system  and similar principles should also describe other sensory functions such as auditory speech perception and comprehension. this assumption was tested in the present study by probing patterns of co-activation within and across hemispheres during the processing of verbal and nonverbal acoustic material. intra-hemispheric co-activation was taken as a large-scale measure of functional network activation, and coherence of oscillatory electromagnetic activity served as measure of co-activation in time. coherence is defined as the correlated activity between two locations within a distinct frequency range.

event-related brain responses, traditionally used in the study of cognitive processes, have been found to result from regional perturbations in ongoing brain activities in a self-organizing system rather than constituting a response set from an otherwise silent system. for example, makeig and coworkers  <cit>  showed that event-related potentials  must be viewed as perturbations in the oscillatory dynamics of the ongoing eeg. the response of successively activated groups of neurons is governed by an attractor, which means that different neuron groups, one after the other, contribute to large-scale changes in the magnetic field that move across brain areas, indicating spatio-temporal changes on a macroscopic level. the basin of attraction guarantees robustness of the propagating synchrony. therefore, the activation of functional cortical networks may best be determined by examining the pattern of dynamic co-activation of groups of neurons  <cit> . as such, whenever neuronal cell assemblies fire 'in phase' the amplitude of oscillatory activity will increase.

on a macroscopic level, oscillatory coupling between large neuronal populations can be examined by externally driving the nervous system using oscillatory stimulation and then measuring the regional coherence of the electromagnetic activity  <cit> . amplitude modulation of the stimuli induces the oscillatory pattern of the steady-state-response . for auditory stimuli the ssr is most prominent at modulation frequencies around  <dig> hz  <cit> . patel & balaban  <cit>  assessed the synchronization of the magnetoencephalographic ssr at this frequency over time  in order to investigate neural correlates of musical comprehension. when the stimulus sequences formed a percept , coherence increased between left posterior and right frontal nodes. similarly, srinivasan et al  <cit>  found increased inter- and intra-hemispheric coherence in the visual ssr when subjects consciously recognized visual stimuli in their field of view. coherence measures have also been employed in the investigation of complex networks involved in the processing of nouns  <cit> , music  <cit> , the perception of necker cube reversals  <cit> , and in the acquisition of contingencies in a conditioning paradigm  <cit> .

the present study investigated coherence patterns of the auditory evoked magnetic steady-state-field , specifically coherence among ssf-generators within and across hemispheres, as a measure of neural networks involved in speech comprehension. if, as we hypothesized, the comprehension of speech was related to the activation of neuronal assemblies in the left hemisphere, then we should see increased coherence in this region with the recognition of a meaningful sentence as compared to an incomprehensible string of sounds. we further hypothesized that meaningful verbal stimuli should be processed differently from musical melodies. that is to say, verbal material should affect the coherence of electromagnetic signals more in the left than in the right hemisphere whereas listening to a nonverbal complement of a meaningful sentence like a melody will activate more right- than left-hemispheric neuronal networks and influence coherence patterns involving the right hemisphere. given that language and music share components, we assumed only a relative dominance in the interconnection of networks toward left- or right-hemispheric activity.

RESULTS
the present study studied co-activated cortical networks involved in speech comprehension by using auditory steady-state  stimuli and measuring the coherence of generator activity of the magnetic steady state response. steady-state stimulus modulation were used for a sentence, which – following a german play-of-words – was first presented as an incomprehensible string of sounds, but became a comprehensible sentence after the sentence's meaning was explained to the subjects and was properly articulated. in addition to verbal stimuli, non-verbal stimuli were also studied which included a 600-hz tone, a scale, and a melody-like combination of the scales' tones. the present analysis of ssf coherence in the source space  extended previous approaches  <cit> , which employed ssr in the signal space to disclose networks involved in auditory perception.

as expected for acoustic stimulation, overall mne amplitudes were most pronounced in auditory areas of both hemispheres, with a varying degree of laterality. for the laterality index  an interaction of condition × hemisphere  =  <dig> , p <  <dig> , ε =  <dig> ) verified that nonverbal conditions as compared to the verbal ones induced a more pronounced asymmetry with more activity in the right compared to the left hemispheres  =  <dig> , p <  <dig> , and for the main effect of condition, f =  <dig> , p <  <dig> , ε =  <dig> ). planned comparisons confirmed significant effects of hemisphere only for the nonverbal conditions  =  <dig> , p <  <dig> , scale, t =  <dig> , p <  <dig> , and melody-like tone sequence, t =  <dig> , p <  <dig> ).

intra-hemispheric coherence was specifically affected by conditions  =  <dig> , p <  <dig> , ε =  <dig> ): as illustrated in fig. 3a for the laterality index, higher intra-hemispheric coherence in the left than in the right hemisphere was induced when the string of words became a comprehensible sentence  =  <dig> , p <  <dig> ), whereas the tone induced higher intra-hemispheric coherences in the right as compared to the left hemisphere  =  <dig> , p <  <dig> ). the main effect of condition was significant for intra-hemispheric coherence  =  <dig> , p <  <dig> , ε =  <dig> ) and inter-hemispheric coherence  =  <dig> , p < . <dig>  ε =  <dig> ) indicating higher coherence was induced by nonverbal than by verbal conditions. since inter-hemispheric coherence may depend on the different generator strength, which was higher in the right than in the left hemisphere, the coherence measures were normalized in order to compensate for an effect of the signal to noise ratio. for normalization, the inter-hemispheric coherence measures were divided by the intra-hemispheric coherence measure of each condition. still, a main effect condition  =  <dig> , p <  <dig> , epsilon =  <dig> ) indicates that coherence was larger for nonverbal than for verbal conditions.

given that the major goal was to depict network signatures specifically involved in sentence comprehension, we applied an anova to compare the coherence measure of the two verbal conditions. these were identical with respect to the physical stimulation, but differed in meaningful comprehension. for intra-hemispheric coherence a significant interaction involving condition × hemisphere × gradient =  <dig> , p <  <dig> ) reflected a relatively higher coherence in the left-posterior area after the string of words had been made comprehensible by explaining the sentence's meaning as opposed to the higher coherence in the right-posterior area for the incomprehensible word string. profiles of intra- and inter-hemispheric coherence were similar, thereby resulting in similar statistical power for the condition effect. this cannot be explained simply by a reduced signal-to-noise ratio in the verbal conditions, because normalized values show the same effect. we rather assume that increased laterality varies with decreased inter-hemispheric communication .

inter-hemispheric coherence between dipoles located in the left  and right  auditory cortex and the remaining dipole sites are characterized  by larger coherence of activity across areas including the left auditory, occipital and right-posterior regions in response to the comprehensible sentence relative to the incomprehensible word string.

considering coherent activity, i.e., synchronized oscillations between spatially distributed maps, as the representation of a percept, we followed makeig et al.  <cit>  who discuss evoked activity in terms of oscillatory perturbations, i.e., alteration of synchrony in ongoing activity. the comparison of two conditions with identical physical stimulation but different degrees of integration into a percept revealed that the synchronicity of auditory ssf increased among areas in the posterior left-temporal and right-occipital cortex when a sentence was comprehensible compared to the same material being incomprehensible. this suggests that a network was activated when an intelligible sentence was being processed. this assumption is in line with previous research in which a left-posterior activity focus was found during semantic processing  <cit> , a left lateralized auditory-conceptual interface was localized at the temporal-parietal-occipital junction  <cit> , and an occipital focus of oscillatory activity found for the processing of  content words relative to verbs  <cit> .

whereas scott et al.  <cit>  reported an increase in regional cerebral blood-flow in the anterior part of the left superior temporal sulcus for intelligible sentences compared to acoustically equivalent non-intelligible sentences, the present results indicated such a pattern – enhanced left-anterior coherence – to be induced by the incomprehensible string of words . at this point, hypotheses to resolve this discrepancy must remain provisional. however, it seems possible, that the speech-like – though incomprehensible – stimuli activated syntactical processing which has been associated with frontal activity  <cit> . in addition, the attempt to determine a syntactical structure has been found to activate the right temporal area  <cit>  which would be in line with the right temporal coherence found for the present condition of incomprehensible word string processing . patel and balaban  <cit>  discussed increased coherence between the left posterior and right frontal areas for melody-like stimuli as a correlate of integrative processing of local and global pitch information. thus, it seems possible that in our study the condition of incomprehensible word string similarly activated pitch processing.

finally, there is the possibility that the order of stimulus presentations may have affected the results. while counterbalancing was not possible for the specific verbal stimulus condition , we would not have expected order effects to be large since similar temporal dynamics were not observed for the nonverbal conditions. however, an effect of time cannot be ruled out as steady state responses and their generator activity were largest for a simple 600-hz tone which was presented first.

ssf were larger for the nonverbal conditions  than for the verbal material, particularly in the right hemisphere. while right-hemispheric processing of tonal perception has frequently been reported  <cit> , the general dominance of right-hemispheric ssf remains to be explained. as mentioned before, it seems possible that it reflects a carry-over effect from the sequence of conditions which invariably started with the tone. it may also reflect bilateral processing of verbal material which has been indicated by various imaging approaches  <cit> . the combination of verbal and nonverbal conditions within one experimental session may have blurred rather than elucidated the co-activation of material-specific networks.

still, greater right- over left-hemispheric generator activity asymmetry was found in the nonverbal conditions and less asymmetry found in the verbal conditions. moreover, intra-hemispheric coherence patterns showed distinct, hemisphere-specific patterns for verbal  and nonverbal  processing. when lateralized coherence patterns were examined by a laterality index, the clearest left-hemispheric coherence focus emerged for the comprehensible sentence and the clearest right-hemispheric coherence focus emerged for the tone. while we had expected a melody induced dominant right-hemispheric activation, a more bilateral activation was found for the melody-like tone sequence. for the scale, there was a shift towards left-hemispheric asymmetry of coherence. an explanation for this finding might be that the 'melody' was constructed to include the tones of the scale which may have resulted in a melody-like tone sequence even though it did not resemble common melodies or songs. this processing of an unfamiliar 'melody' might have activated temporal  and spectral  processing, as suggested by  <cit> , resulting in a more bilateral activation. while a simple tone contains only spectral information, a melody also contains temporal information.

CONCLUSIONS
in sum, the present study demonstrates that the analysis of the synchronization of evoked magnetic steady-state fields in the source space can map neuronal networks activated during speech comprehension. our techniques add spatial information to evidence on left-hemispheric areas involved in language processing, and support co-activation or synchronization within complex neuronal networks as a cortical substrate of integration in perception – like speech comprehension.

