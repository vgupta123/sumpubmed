BACKGROUND
over the last  <dig> years, dna sequencing technologies have advanced rapidly, allowing sequencing of over one thousand microbial genomes  <cit> . still, this accounts for only a sliver of the fantastic diversity of microbes on the planet  <cit> . sequencing of environmental dna  has shown tremendous potential to drive the discovery and understanding of the "unculturable majority" of species -- the vast number of unknown microbes that cannot be cultured in the laboratory  <cit> . successful metagenomics projects have sequenced dna from ocean water sampled from around the world  <cit> , microbial communities in and on humans  <cit> , and acid drainage from an abandoned mine  <cit> . these and many other projects  promise to uncover the true extent of microbial diversity and give us a better understanding of how these unknown microbes live.

however, progress has been slowed by the difficulty of analysis of metagenomic data. the output from an environmental shotgun sequencing project is a large set of dna sequence "reads" of unknown origin. because these reads come from a diverse population of microbial strains, assembly produces a large collection of small contigs   <cit> . two important goals of metagenomics are to determine what species are in the mixture in what proportions and to assemble substantial portions of individual genomes. a fragmented assembly of short sequences makes attaining these goals difficult. advances in computational analysis techniques are essential to move the field forward.

to uncover what microbes are in a metagenomic sample, we must determine  which sequencing reads came from the same microbial strain, and  where those strains fit into the phylogenetic tree of life  <cit> . methods to solve these two problems are related. clustering methods solve the former problem by binning sequences into clusters that represent a single taxonomic class. classification methods aim to solve the latter problem by assigning a specific taxonomic class to every sequence.

in some cases, the presence of marker genes like  <dig> s rrna, which is very highly conserved across species but has variable regions, can be used to assign a taxonomic class to sequence fragments  <cit> , but this typically pertains to only a very small percentage of the reads. for example, ~ <dig> % of reads in a typical metagenomics project carry rrna genes  <cit> . more general sequence similarity-based methods align reads with blast  <cit>  to known genomes deposited in public databases like genbank  <cit>  and use those alignments to assign a taxonomic classification  <cit> . however, sequence alignment can only classify reads from organisms with a close evolutionary relative that has already been sequenced  <cit> . in most environments, this will not be the case for many of the reads; e.g. 70% of sargasso sea reads had a blast hit using "extremely lenient" search parameters, and only 30% aligned for nearly their whole length  <cit> .

composition-based methods for clustering and classification use properties of the dna sequence such as oligonucleotide frequencies. these "genome signatures" are influenced by a variety of factors including codon usage, dna structure, replication and repair processes, and evolutionary pressures  <cit> . they are fairly constant within a genome  <cit>  and can be useful for inferring phylogenies  <cit> . crucially for the use of genome signatures for clustering and classification, they persist even in conserved  <cit>  or horizontally transferred regions   <cit>  and remain diverse between species despite shared environmental pressures and interactions  <cit> . composition-based classification methods typically train on the oligonucleotide frequencies of all known genomes, and then classify sequences using supervised machine learning such as kernelized nearest neighbor  <cit> , support vector machines  <cit> , self-organizing maps  <cit> , and naive bayesian classifiers  <cit> . phymm, a recently developed composition-based approach developed in our group  <cit> , trains interpolated markov models  on known genomes in order to classify sequences.

while supervised learning has proven useful in practice, shortcomings exist. methods trained on the genomes in genbank make an implicit assumption that those genomes are representative of microbes waiting to be found by metagenomics projects. this assumption is clearly violated by many if not most metagenomic samples. supervised learning methods that tread carefully with respect to the potential biases caused by this assumption can still be useful analytical tools for many environments. alternatively, genome signatures can be used for unsupervised clustering by learning the signatures from the set of sequences without the use of known genomes  <cit> . such approaches may be required when publicly available genomes are a poor fit to the data.

as an alternative to oligonucleotide frequencies, markov chain models have shown great promise for characterizing genomic content  <cit> , and have been implemented for both supervised classification  <cit>  and unsupervised clustering  <cit>  methods. in this paper, we cluster sequences using interpolated markov models , a type of markov chain model that adapts the model complexity to take advantage of variable amounts of training data. this strategy is well suited to metagenomics clustering problems, where the amount of sequencing performed and the relative abundances of the species in the mix can vary widely. our clustering framework proceeds similarly to one used to cluster sequences using hidden markov models where optimization is performed iteratively by a relative of the k-means clustering algorithm  <cit> . we refer to our method as scimm .

we test scimm on simulated metagenomic datasets of fragments from mixtures of randomly selected known genomes and demonstrate improvement on the performance of the metagenomic sequence clustering programs compostbin  <cit>  and likelybin  <cit> . we also assess the limitations of unsupervised learning on complex datasets, and describe how a combination of scimm and phymm, which we call physcimm, clusters more accurately when useful training data is available.

methods
markov models have proven to be an invaluable tool for sequence analysis  <cit> , including capturing genome signatures  <cit> . here we present a clustering algorithm called scimm in which we use interpolated markov models  to model clusters of sequences. clustering of sequences is performed using a variant of the k-means algorithm.

interpolated markov models
a fixed-order markov chain is a model for generating a sequence of outputs  in which the ith element in the sequence has a distribution that is conditional on the previous w elements. thus, given a sequence s and a model m, we can compute the probability that s was generated by m by walking along the sequence and multiplying the conditional probabilities.

  p=∏i=w+1|s|pm 

alternatively, imms are variable-order markov chains, a strict generalization of fixed-order markov chains, and interpolate between multiple models of fixed size via weights . past work has found that increasing the order of the markov model  leads to more accurate predictions as long as there is sufficient training data. imms dynamically adjust the order of the models based on the data, which allows them to make the most of whatever information is available. this is particularly useful for clustering of metagenomic sequences where the amount of sequence from each species may differ widely due to differential abundance of organisms and the amount of sequencing performed on the sample. the variant of imms used in our system, introduced in the glimmer  <dig>  gene prediction software  <cit> , is even more general as it allows the nucleotide distributions to be conditional on a subset of indexes in the preceding size w window .

to train an imm on a set of sequences, consider each w+ <dig> sized window in the sequences and let the distribution of nucleotides at position i in the windows define random variable xi. training creates a probabilistic decision tree using information gain as the splitting criteria where each node specifies certain nucleotides at a subset of the window positions and defines a probability distribution for the final nucleotide in the windows. to construct this tree, first, compute the mutual information i between the final position in the window and positions i ∈  <dig> .w. define the initial split in the tree at the position with the greatest mutual information. create branches to new nodes for all four nucleotides at this position. next, perform a similar procedure for each branched node considering only windows containing the specific nucleotide at the position chosen. for these windows, compute the conditional mutual information of the remaining positions and choose the most informative position for the next split. repeat this procedure to fill in the full decision tree, stopping early on paths where data becomes too sparse. at some point walking down each path, additional nucleotide positions may fail to be informative. we recognize this by computing a chi-square test between each node's distribution and its parent node's distribution. if the distributions are sufficiently similar, we stop branching and interpolate between the node and its parent's distributions, weighting each one based on the chi-square test result and the number of training windows mapped to the node.

to compute the likelihood that a novel sequence was generated by this imm, consider each window of size w +  <dig> in the sequence as in equation  <dig>  for each window, follow a path through the decision tree to a leaf node according to the nucleotides at the positions defined by the nodes and branches. score the next nucleotide in the novel sequence using the leaf node's interpolated probability distribution. more details of the training of imms and sequence likelihood computations can be found in the glimmer descriptions  <cit> .

k-means clustering framework
the k-means algorithm is a widely used, simple and effective method for clustering data points. we review that algorithm before introducing our own approach to clustering sequences. points are modeled as having come from k sources, each represented by a cluster mean. the algorithm begins by initializing these cluster means, e.g. by randomly choosing k data points. next, one repeats the following steps. first, compute the distance between all points and the k cluster means. second, assign each point to its nearest cluster.

finally, recompute the cluster means using the current assignment of points to clusters. after a number of iterations, one arrives at a stable partitioning of data points that approximates the minimum sum of squared distances between data points and their assigned cluster means.

an alternative formulation of the algorithm leads more directly to our approach. the k-means algorithm has also been referred to as classification expectation-maximization  to optimize the classification maximum likelihood  criterion for data points generated from k gaussian distributions with equal variance and zero covariance mixed in equal proportions  <cit> . for data points x <dig> ..xn sampled from clusters c <dig> ..ck and gaussian density f parameterized by mean vectors u <dig> ..uk , cml is defined as

  cm l=∑k=1k∑xi∈cklog) 

that is, cml approximates the log likelihood that the cluster models generated the data points, but with each data point assigned a hard classification to a single cluster. cml can be further generalized to the case where data points are sampled from the clusters according to a multinomial distribution parameterized by p <dig> ..pk . here cem assigns each data point xi to the cluster ck that provides the greatest posterior probability log), and cml is defined as

  cm l=∑k=1k∑xi∈cklog) 

using cem, the cml criterion converges to a local maximum  <cit> .

scimm
scimm uses the same general algorithm as cem, where the data points are dna sequences and the cluster models are imms. here the goal is to find the imms and multinomial probabilities that maximize the cml criterion, which approximates the log likelihood that the mixture of cluster models generated the sequences. the algorithm begins by initializing k imms . then the following steps are repeated until convergence. first, for all sequences s and all imms m, compute the log likelihood that s was generated by m. second, assign each sequence to the cluster corresponding to the imm m that maximizes the posterior probability log + log). finally, re-train the imms on the sequences currently assigned to their corresponding clusters. this loop is depicted in figure  <dig>  over the course of the iterations, the imms converge to a set that should represent the phylogenetic sources.

because the maximization step is not straightforward maximum likelihood estimation , we lose the theoretical guarantee of cml convergence  <cit> . in practice, we did not find this to be a problem as the algorithm converged in all experiments. however, scimm halts when fewer than  <dig> % of the sequences change clusters in order to reduce computation time because the last few stages of this procedure tend to shuffle a small number of sequences with a negligible effect on clustering accuracy.

initial partitioning
scimm inherits the simplicity and effectiveness of the k-means algorithm, but also its sensitivity to initial conditions. we found that the likelihood landscape is riddled with local maxima from which the optimization cannot escape. initially partitioning the sequences by very simple clustering algorithms yielded insufficient results.

to improve performance, we tried using previous methods for unsupervised clustering of metagenomic sequences to initialize the imms. we focused on two particularly successful approaches, likelybin and compostbin. likelybin models sequences using k fixed 2nd-4th order markov models learned by counting oligonucleotides  <cit> . because likelybin uses simpler models with far fewer parameters than imms, a markov chain monte carlo algorithm is used to search the parameter space for the parameters that maximize the likelihood of generating the sequences. likelybin is publicly available at http://ecotheory.biology.gatech.edu/downloads/likelybin. the second approach, compostbin  <cit> , works as follows. for each sequence, count oligonucleotide frequencies and project the frequency vectors into three dimensions using principal component analysis. next, create a graph where each sequence is represented by a vertex and edges are placed between a sequence and its six nearest neighbors. finally, split the sequences into two partitions by finding a minimum normalized cut in this graph across which few edges exist  <cit> . this process is repeated until the desired number of clusters is reached. though compostbin is publicly available from http://bobcat.genomecenter.ucdavis.edu/souravc/compostbin, we re-implemented the main unsupervised ideas of the algorithm to better fit in our pipeline and refer to our version as cbcbcompostbin. one notable adjustment to the method was to make the number of nearest neighbors with which to build the graph a function of the number of sequences, because fewer sequences required a less connected graph for good performance. choosing the number of nearest neighbors is a difficult subproblem of the normalized cut clustering method upon which compostbin is based  <cit> . we found that the function f=2+12⌊ln⌋, where f returns the number of nearest neighbors and n is the number of input sequences, worked well in practice, but did not address this problem in depth because experimental results demonstrated that scimm's accuracy did not depend significantly on the parameter choice.

to initialize the imms for scimm, we can run either likelybin or cbcbcompostbin on a random subset of the sequences with a user-specified number of clusters k and train an imm on every cluster returned. we used a random subset because both algorithms can be slow for large data sets, and 2- <dig> mb of sequence was sufficient to train the imms to begin the iterative clustering procedure. because the two programs approach sequence clustering differently, they tend to succeed on different datasets -- e.g. for mixtures of  <dig> genomes, the standard deviation of the difference between likelybin's and cbcbcompostbin's precision  is  <dig> % and recall is  <dig> %. therefore, we initially partition the sequences with both likelybin and cbcbcompostbin and perform one iteration of scimm on each. for each partitioning, we compute the cml criterion and continue iterating on only the partitioning with the greater value.

supervised initial partitioning
as we will show, unsupervised clustering methods are very effective on low complexity datasets, but less accurate on metagenomic samples with many  microbial strains. with more strains, the genome signatures may blend together and become difficult to properly discern. alternatively, classification methods like phymm are immune to the complexity of the dataset because each sequence is classified independently of the others  <cit> . sequence classifications can be interpreted as implying a clustering, for instance by forming clusters from all sequences classified to the same genus. therefore, a classification method can also be used to obtain an initial partitioning for scimm.

we considered a hybrid of supervised and unsupervised learning referred to as physcimm where we obtained an initial partitioning of the sequences with phymm. first, we randomly chose a subset of sequences , classified the sequences, and clustered at a certain taxonomic level . due to misclassification noise, phymm will usually return too many clusters. to filter out clusters of misclassified sequences, we found only keeping clusters containing >20k% of the sequenced bases where k is the number of genomes in the mixture  to be a useful heuristic. note that phymm is not limited to returning k clusters, and the number of clusters returned depends on the strictness of filtering, which the user would need to specify in a novel environment. after filtering clusters, we moved all unclustered sequences to an additional cluster, otherwise scimm tended to incorrectly force these sequences into the generally high quality clusters from phymm classifications. finally, we iterated imm clustering as in the standard scimm algorithm. scimm and physcimm are available open source from http://www.cbcb.umd.edu/software/scimm under the perl artistic license http://www.perl.com/pub/a/language/misc/artistic.html.

RESULTS
simulated reads
to assess the performance of scimm and physcimm, we simulated sequencing reads from mixtures of  <dig> sequenced genomes in genbank  <cit>  as of  <dig> and clustered the reads with each method. the degree to which the diversity of a random mixture of these genomes is representative of a real metagenomic environment has not been explored in depth. we make two points in support of this experimental setup. first, because certain model and disease-related organisms are of particular interest to researchers, genbank contains many clusters of extremely closely-related genomes that make clustering difficult and may be representative of a heterogeneous species population from a real metagenomic environment; for example,  <dig> escherichia coli,  <dig> salmonella enterica, and  <dig> staphylococcus aureus genomes were included. second, while the expected clustering accuracy of any single method on a novel metagenomic environment may not exactly match the statistics reported in our tests, the simulations still serve to rank scimm and previous unsupervised approaches based on clustering accuracy.

for each test, we randomly chose k genomes and k corresponding uniformly distributed random numbers in the interval . we simulated  <dig> reads of length  <dig> base pairs  so that the percentage of reads from each genome in the sample was proportional to that genome's random number. we clustered the reads with scimm, likelybin, and cbcbcompostbin. likelybin runs used  <dig> mcmc start points and a 3rd order markov model. cbcbcompostbin runs used 5-mers.

clustering accuracy can be quantified using a variety of measures  <cit> . sequences from the same genome should be placed in the same cluster, which is measured by recall. let cij be the number of sequenced nucleotides from genome j placed in cluster i. then the recall for genome j is computed as

  recall =maxicij∑icij 

sequences placed in a cluster should belong to the same genome. this is measured as precision and computed for cluster i as

  precision=maxjcij∑jcij 

in order to obtain global performance statistics, precision and recall were combined across clusters and genomes by weighting each term by the number of sequenced nucleotides from the cluster or genome. we also measured accuracy using the adjusted rand index. the rand index is the proportion of pairs of data points that are correctly placed together or apart, and the adjusted rand index modifies this statistic based on the sizes of the clusters  <cit> .

we tested the unsupervised methods with mixtures of  <dig>   <dig>   <dig>  and  <dig> genomes, performing  <dig> trials of each, which resulted in standard deviations of ~ <dig> % for precision and recall and ~ <dig> % for adjusted rand index. scimm achieved superior performance over the other methods by all measures, as shown in figure  <dig>  in addition to having a greater average adjusted rand index, scimm had the highest adjusted rand index for 93% of the trials with ten genomes and 90% of the trials with twenty genomes. all methods were able to effectively partition sequences from two genomes. as we increased the number of genomes, performance degraded, but recall and precision >80% on average can be expected for mixtures of up to ten genomes.

we also examined the effect of sequence length on scimm's performance  by sampling mixtures of five and ten genomes and varying the length of the simulated reads while holding the total number of sequenced bp constant; e.g. doubling the number of reads while halving the sequence length. second generation sequencing technology from roche/ <dig> produces  <dig> bp reads, which should be sufficient for clustering sequences from low complexity environments with five or fewer strains as precision and recall are >85%. accuracy continues to improve with  <dig> bp fragments in both the five and ten genome tests, suggesting that longer read lengths or assembly of reads into contigs should be beneficial to metagenomic analysis.

in tests with mixtures of five and ten random genomes in random proportions, increasing read length leads to greater accuracy. even with  <dig> bp reads, the clusters are accurate enough to be useful for some applications.

all computational methods working with dna sequencing reads must account for sequencing errors. we expect imms to be robust to such errors. a mis-sequenced nucleotide may affect the probabilities of up to w+ <dig> nucleotides for window size w. however, the imm will learn which positions in the window are informative for the distribution of the next nucleotide, and errors at uninformative nucleotides will have a negligible effect. furthermore, even at what are considered high error rates, sequencing errors are rare enough to not overwhelm the genome signatures found in the sequences. to measure the effect of sequencing errors, we sampled mixtures of ten genomes and mis-called nucleotides in the reads at rates of  <dig> %,  <dig> %, and  <dig> %. table  <dig> summarizes  <dig> iterations of this test, such that the standard deviations of precision and recall are ~ <dig> % and adjusted rand index is ~ <dig> %. though clustering accuracy decreases slightly with errors, increasing the error rate further has a negligible effect, and altogether scimm appears to be fairly robust to sequencing errors.

in tests with mixtures of ten random genomes in random proportions, sequencing errors lead to decreased accuracy. however, the rate at which accuracy decreases as errors increases is slow so that scimm is fairly robust to error rates of  <dig> %.

unsupervised clustering performance degrades as the number of genomes reaches twenty or more, but classification methods like phymm are largely unaffected by the number of genomes. we re-ran the experiment above using physcimm for mixtures of  <dig>   <dig>  and  <dig> genomes. in order to thoroughly evaluate the performance of this supervised initial partitioning of the sequences, we performed separate tests of physcimm where phymm's trained imms were held out if they were based on the same strain, species, and genus classification as the genomes from which the reads were simulated. for example, if we held out imms at the genus level, no imms were used from microbial strains matching the genus of any of the genomes from which the reads were simulated. when imms from the same genus as those in the sample can be expected, physcimm produces accurate clusters . but performance suffers when imms are unavailable from the same genus, and unsupervised clustering appears to be more useful in this case at ten and fewer genomes. with few genomes, accuracy is comparable to unsupervised scimm, but the value of physcimm is readily apparent on the twenty genome mixture representing a more diverse metagenomic sample where performance is better than scimm even when imms are held out at the genus level.

fames
experiments clustering single datasets can teach us about specific strengths and weaknesses of the methods and how they can be applied most effectively. to use more realistic data, we clustered the arachne-assembled contigs from the fames simulated metagenomic datasets of low  and medium  complexity  <cit> . these were created by mixing real reads from the original sequencing projects of  <dig> organisms. the contigs are dominated by a few species, but have a long tail of very low abundance species. we clustered with scimm using k = 2- <dig> clusters and with physcimm initializing clusters from genus level classifications assigned to >1% of the total bp. because phymm has trained imms for these publicly available genomes, we held out imms similar to organisms in the mixture at the strain and species levels. in a noisy dataset with many organisms like this one, sequences from different strains of the same species are effectively indistinguishable. thus, we computed accuracy at the species level for the tests that follow.

the simlc dataset contains  <dig> contigs of mean size  <dig> bp from  <dig> different microbial strains, but is dominated by  <dig> contigs from rhodopseudomonas palustris haa <dig> that make up  <dig> % of the nucleotides and  <dig> contigs from bradyrhizobium sp. btai <dig> that make up  <dig> %. the clustering accuracy statistics depend significantly on the arrangement of contigs from these two strains. because rhodopseudomonas palustris haa <dig> and bradyrhizobium sp. btai <dig> are both from the family bradyrhizobiaceae and have similar high gc content , separating each strain into its own cluster is difficult. figure  <dig> displays the results for both methods. when clustering with scimm at k =  <dig> and  <dig>  nearly all contigs from the same strain were kept together leading to 99% recall, but each cluster contained a mix of species giving 80% precision. at larger values of k, some reads from the bradyrhizobiaceae strains break off into other clusters, reducing the recall, though at the benefit of increased precision. when holding out imms from the same strains, physcimm achieved very high accuracy , as rhodopseudomonas palustris haa <dig> and bradyrhizobium sp. btai <dig> were mostly separated from each other because phymm had a trained imm for each species. when imms from genomes matching species in simlc were removed, physcimm's precision dropped to 83%. if the initial clusters are formed from phymm classifications at the family level, the bradyrhizobiaceae strains cannot be separated and precision also drops to 83%.

simmc has  <dig> contigs of mean size  <dig> bp from  <dig> microbial strains. these contigs are distributed among the strains slightly more uniformly, but still only six species account for  <dig> % of the nucleotides. these six include two strains each from the species rhodopseudomonas palustris and xylella fastidiosa. bradyrhizobium sp. btai <dig> also appears and presents a challenge similar to that described above for simlc. for k =  <dig> and  <dig>  scimm formed strong clusters for the the xylella fastidiosa strains and the bradyrhizobiaceae strains, leading to very high recall. as we increased k, these strains were split among the clusters, significantly decreasing the recall . from this experiment and the last one, we see that clustering performance is best when k is set to the number of dominant phylogenetic sources. increased values of k risk splitting a dominant species into multiple clusters rather than effectively clustering a far less abundant species. precision did not increase with more clusters because when the bradyrhizobiaceae strains split into multiple clusters, each one contained a mixture of both species. when imms from the same species were available, physcimm produced much better clusters with 90% precision and 85% recall. but, accuracy dropped precipitously when imms were held out at the species level. thus, we see again that physcimm clusters more accurately if very related genomes are available for training, but pure unsupervised clustering is preferable for a metagenome containing organisms whose taxa are unsequenced.

instead of computing accuracy at the species level, we could consider a higher level in the hierarchy of taxonomic classification, such as the family level. by doing so, we reward the clustering algorithm for clustering together two sequences that originated from different strains in the same family, such as bradyrhizobium sp. btai <dig> and the rhodopseudomonas palustris strains. family level precision is >97% in all tests, meaning that generally when scimm is merging two separate species into a cluster, they are phylogenetically related.

in vitro-simulated metagenome
to further explore the effectiveness of scimm and physcimm on more realistic data, we clustered sequencing reads from an in vitro-simulated microbial community  <cit> . here, ten microbes were mixed into a simulated metagenome and sequenced using a number of different protocols and sequencing techniques. these ten were chosen to cover a wide range of microbial diversity, but also to include closely related species, specifically two lactobacillus strains and two lactococcus strains. the resulting reads were then assigned to their source genome via blast  <cit>  alignments to a database of the ten microbes' genomes. after combining classified reads from all non- <dig> datasets, we obtained  <dig> mated reads and  <dig> singleton reads.

we clustered the reads with scimm into  <dig> clusters for the  <dig> species in the data and computed 87% recall and 88% precision at the species level. as expected, the lactobacillus and lactococcus strains each clustered together well. high quality clusters formed around the medium abundance strains shewanella amazonensis sb2b and myxococcus xanthus dk  <dig>  but scimm split reads from the most abundant strain acidothermus cellulolyticus 11b into mainly two clusters. meanwhile, low abundance strains pediococcus pentosaceus atcc  <dig> and halobacterium sp. nrc- <dig> lacked the data to form their own pure clusters and co-clustered with the lactococcus strains and acidothermus cellulolyticus 11b respectively. knowing that scimm can struggle with low abundance species and seeing that  <dig> of the  <dig> clusters were effectively unused and contained far fewer reads than the rest, we reduced the number of clusters to  <dig>  doing so brought the two acidothermus cellulolyticus 11b clusters together and increased the recall to 94%.

clustering the reads with physcimm led to further insight. the level at which imms were held out did not have a significant impact on clustering accuracy for this dataset, so we discuss the results from holding out imms from the same genus as the strains in the simulated metagenome. we initially clustered sequences using family classifications that were assigned to >3% of the sequences. performance was considerably worse than unsupervised scimm with a 79% recall and 83% precision on the  <dig> clusters.

physcimm struggled with acidothermus cellulolyticus 11b because there are no other trained imms in its family acidothermaceae. instead, phymm assigned its reads to the families mycobacteriaceae, microbacteriaceae, and propionibacteriaceae. each of these families belong to the order actinomycetales, and so physcimm performed far better when initialized using order classifications . interestingly, phymm misclassifies many reads from the lactococcus strains, the lactobacillus strains, and shewanella amazonensis sb2b to the order enterobacteriales, but iterative imm clustering is able to effectively separate these species despite a poor initialization.

CONCLUSIONS
determining the relationships between sequences is a crucial step in metagenomics analysis. in this paper, we introduce scimm, an unsupervised sequence clustering method based on interpolated markov models . our experiments show that scimm clusters sequences more accurately than previous unsupervised algorithms.

by demonstrating the ability of imms to successfully cluster sequences, we add to the growing evidence of the effectiveness of imms for modeling dna sequences  <cit> . markov chain models have proven to be useful sequence modeling tools for many bioinformatics applications  <cit> . the increased modeling sophistication and ability to handle varying amounts of training data make imms preferable for many of these applications.

we compared two variations of clustering with imms. scimm is purely unsupervised and makes use of the previously published methods likelybin and compostbin to initially partition the sequences. physcimm partitions the sequences using supervised phymm classifications before the unsupervised iterative imm clustering stage. supervised learning proved to be a valuable addition to the pipeline when genomes were available to train on from the same genus as the microbes in the mixture. pure unsupervised learning is preferable when the available genomes to train on are not representative of those from which the sequencing reads originated. because the classification accuracy of phymm is independent of the complexity of the mixture, supervised learning also improves clustering of complex mixtures of twenty or more microbes. developing more sophisticated combinations of classification and clustering methods may prove to be a fruitful line of research.

we believe scimm and physcimm will be valuable tools for researchers seeking to determine the relationships between sequencing reads from a metagenomics project. for environments with ten or fewer species, unsupervised clustering with scimm finds accurate clusters. however, the number of clusters k must be chosen carefully by the user. specific knowledge about the environment, especially regarding the number of dominant microbes and their relationships to each other, can inform the choice of k and impact the utility of a clustering of the environment's sequences. nevertheless, tests on the fames dataset showed that various values of k can produce useful clusters. lesser values of k tend to provide greater recall with lower precision. greater values of k may decrease recall by dividing a particularly dominant species into more than one cluster but will usually improve precision.

when the microbes in an environment are thoroughly represented in public databases, physcimm finds even more accurate clusters. physcimm is also more effective for mixtures of twenty or more strains. the user does not need to choose the number of clusters with physcimm, but must choose the classification level and minimum support to initialize a cluster. the simulated read experiments offer guidelines for how to set these parameters effectively. tests with the fames and in vitro-simulated metagenome datasets demonstrated that incorporating knowledge about the dominant organisms in the environment can have a significant positive impact on the clusters.

metagenomics projects are increasingly turning to less expensive and higher throughput second generation sequencing technologies such as those from roche/ <dig> and illumina. clustering of  <dig> bp read lengths is still reasonably effective for mixtures of five or fewer strains. shorter reads, such as the 100- <dig> bp lengths currently available from illumina, cannot be accurately clustered by our methods for environments with realistic complexity. however, if these reads can be assembled into larger contigs, then effective clustering of the contigs is possible.

a number of avenues appear worthwhile for further research. a principled method for setting parameters that affect the number of clusters would certainly aid researchers using the method. preliminary sequencing of  <dig> s rrna or other marker genes followed by clustering may effectively achieve this goal  <cit> . the k-means iterative clustering framework used by scimm works well with a good initial partitioning of the sequences, but other optimization methods might prove more robust to the initial conditions and less prone to getting stuck in local maxima. because scimm nearly always improves on the clustering results of likelybin and cbcbcompostbin, there is reason to believe that it would also improve on initial clusters from more accurate future methods. we excluded an interesting feature from the original compostbin in our experiments whereby reads containing informative marker genes were identified and classified using amphora  <cit>  and the classifications were used to add supplemental edges to the nearest neighbor graph. a similar semi-supervised scheme could be implemented in scimm as well. finally, assembly and clustering are both important steps in metagenomics pipelines, and further exploration of the relationship between the two has the potential to improve both tasks.

authors' contributions
drk conceived, implemented, and tested the method. drk and sls wrote the manuscript.

