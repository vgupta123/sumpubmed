BACKGROUND
one of the main contributions of bioinformatics in molecular biology has been the introduction of ontologies for genome annotation. these circumvent the shortcomings of natural language descriptions  and consequently enable automated annotation and automated reasoning over annotations  <cit> . prominent among these is the gene ontology , which is dedicated to the functional annotation of gene products in a cellular context and a species independent manner  <cit> . it comprises three orthogonal ontologies  organised as directed acyclic graphs  which account for distinct aspects of gene products: molecular function, biological process and cellular location. the relationships between go terms can be either is-a  or part-of  relationships.

among other applications, the use of ontologies such as go enables the comparison of gene products based on their annotations, so that functional relationships and common characteristics can be inferred beyond the traditional sequence-based approaches. this requires the use of a semantic similarity measure to compare the terms to which gene products are annotated. there are two main approaches to measure semantic similarity  <cit> : edge-based measures, which assume a term's specificity can be directly inferred from its depth in the graph; and information content -based measures, which estimate a term's specificity from its usage frequency within a given corpus. in the case of go  the latter are more adequate because specificity is poorly related with depth in the graph, for instance: the terms binding and translation regulator activity are at the same depth but the latter is both semantically more complex and biologically more specific.

lord et al.  <cit>  were the first to apply go-based semantic similarity to compare gene products, testing three ic-based measures: resnik's  <cit> , lin's  <cit> , and jiang and conrath's  <cit> . these three measures, originally developed for wordnet, compare terms by finding their lowest common ancestor . however, the definition of lca is not straightforward in go, since go terms can have several disjoint common ancestors. lord et al.  <cit>  addressed this issue by using only the most informative common ancestor , while later, couto et al.  <cit>  considered that all disjoint common ancestors should be taken into account.

a more critical issue when applying these measures to gene products is that they are measures for comparing single terms whereas gene products have usually several terms . therefore, obtaining a single similarity score requires combining the semantic similarities of the gene products' terms . three distinct approaches have been proposed for this combination: lord et al.  <cit>  used an arithmetic average of the term similarities, pairing all terms of the first gene product with all terms of the second one; sevilla et al.  <cit>  used only the maximum similarity between all term pairs; and couto et al.  <cit> , schlicker et al.  <cit>  and azuaje et al.  <cit>  developed composite  averages where each term of the first gene product is paired only with the most similar term of the second one and vice-versa.

from a biological point of view, there are limitations to both the average and maximum approaches. the average approach is inaccurate for gene products with several shared or similar terms, for instance: two functionally identical gene products having both the terms antioxidant activity and binding have a similarity of 50% rather than the expected 100%, because similarities are calculated between all possible term pairs of the two gene products. by contrast, the maximum approach is indifferent to the number of unrelated terms between gene products, for instance: a gene product with the terms antioxidant activity and binding and a second gene product with only one of those terms would have a similarity of 100%, when functionally they are clearly not equal. the best-match average approach does not suffer from the above limitations, and accounts for both similar and dissimilar terms as would be expected biologically.

a different approach to the issue of gene products having more than one term  is to use a semantic similarity measure that compares sets of terms rather than single terms, thus avoiding the need to combine similarities. since the set of go terms of a given type to which a gene product is annotated can be seen as a sub-graph of that go type, a graph comparison measure can be used for this purpose. gentleman  <cit>  was the first to explore this possibility by developing the simui measure, which given the annotation graphs for two gene products, defines semantic similarity as the fraction between the number of go terms in the intersection of those graphs and the number of go terms in their union. despite accounting for both similar and dissimilar terms in a simpler way than finding matching term pairs, this measure weights all terms equally, and therefore does not account for term specificity. to overcome this limitation, we developed the simgic measure, which is similar to simui, but where each term is weighted by its information content  <cit> .

applications of go-based semantic similarity have been innumerous, and include such diverse subjects as: protein interaction prediction  <cit> , validation of function prediction  <cit> , network prediction  <cit> , prediction of cellular localisation  <cit> , automatic annotation validation  <cit> , integration of semantic search  <cit> , pathway modelling  <cit> , and improving microarray data quality  <cit> . however, two crucial questions still stand: which type of annotations should be trusted for semantic similarity calculations; and which semantic similarity measure performs better with go?

the first question is central to current molecular biology. on one hand it has become clear with the advent of automated sequencing that experimental work cannot be the sole source for gene product annotation, if the gap between sequence data and functional information is to be bridged. on the other hand, the increasingly important role for bioinformatics in annotation  <cit>  has lead to a growing number of annotations extrapolated from sequence similarity, which are prone to errors  <cit> . indeed, it has been suggested that as much as 30% of the annotations corresponding to detailed characteristics can be erroneous as a result of inferring annotations from sequence similarity, particularly from gene products whose annotations had already been extrapolated  <cit> . despite this, the precision of automated annotations methods has been increasing steadily , and as they account for a growing portion of the annotation space , the cost of ignoring them becomes heavier.

as for which semantic similarity measure is more suitable to go, it raises another question: how do you evaluate the performance of semantic similarity measures? authors have used correlations with sequence similarity  <cit> , with pfam similarity  <cit> , with gene co-expression  <cit> , and with protein interactions  <cit>  to evaluate their measures; some discarding electronic  annotations  <cit>  and others using all annotations  <cit> . this profusion of evaluation strategies, with new results not being directly comparable to previous ones, hinders the extraction of any global conclusion about the measures' performances.

in this work, we perform a systematic evaluation of several semantic similarity measures. the mould of this evaluation was to assess, given a set of gene products and a corpus of go annotations, how well each semantic similarity measure captures the similarity in annotations between gene products. as there is no internal means of making this assessment, an external source of data, correlated with the annotations, must be used. we opted for using sequence similarity, since it is well established to be related to function and there is some insight on that relation, namely: general functional characteristics are conserved at relatively low levels of sequence similarity , while specific functional characteristics are poorly conserved even at high levels   <cit> . since we have this type of insight only between function and sequence, and because the other go types have been shown to have a looser correlation with sequence similarity  <cit> , only the molecular function go type was used. to summarise our strategy, we are evaluating the measures by assessing how well they capture the expected relationship between functional and sequence similarity.

RESULTS
to evaluate the semantic similarity measures, we used two distinct sequence similarity measures: log reciprocal blast score  and relative reciprocal blast score . the former is similar to the sequence similarity measure used previously by lord  <cit> , but compensates for the fact that blast scores are not symmetric, while the latter is analogous to the sequence identity percentage  but takes amino acid substitutions into account. as rrbs is not directly affected by sequence length  we can assess whether the dependency on sequence length affects the outcome of the evaluation.

a total of fourteen semantic similarity measures were tested: resnik's, lin's, and jiang and conrath's term similarity measures, each with the average, maximum, best-match average , and bma plus grasm approaches; plus the graph-based simui  <cit>  and simgic measures  <cit> . we evaluated the influence of using electronic annotations by testing the measures on two distinct datasets: one with all annotations  and one without electronic annotations .

modelling the behaviour of semantic similarity
the raw semantic similarity vs. sequence similarity results were averaged over intervals of fixed number of points , so that the global behaviour of the results could be perceived.

upon observing the averaged results, it was clear that their behaviour was not linear , regardless of dataset, sequence similarity measure or semantic similarity measure used. what is more, within each dataset and sequence similarity measure, the majority of the semantic similarity measures were similar in behaviour, showing the same patterns in regard to sequence similarity. therefore, it was necessary to find a type of function that followed the overall behaviour of the results closely, and that could be fitted to all semantic similarity measures  so that we could quantify the differences between them. we chose to use rescaled normal cumulative distribution functions , which correspond to error functions, because in addition to fulfilling this requirement, the influence of their parameters in the shape of resulting curve is intuitive, and they promote a simple probabilistic interpretation of our results. the results from the full dataset, where a bimodal-like behaviour was evident , were modelled by two additive ncdfs, whereas those from the non-electronic dataset required only a single ncdf ; in both cases, scale  and translation  parameters were applied to fit the range of the results .

confirming their visible similarity, the majority of the semantic similarity measures  have modelling functions with identical shape parameters , and differ mostly in range . this means that the majority of the measures capture the same pattern  along the sequence, but tend to translate that pattern into different ranges of the semantic similarity scale. it should be noted that this difference in range occurs only in the averaged semantic similarity results, and not between the actual semantic similarity measures, which are all ranged in a 0- <dig> scale . therefore, the range of the results should be interpreted as a tendency of the measure, rather than a scale limit. this tendency is composed by two distinct properties: bias, i.e. the tendency to yield higher semantic similarity values, which is measured by the translation parameter of the modelling function; and resolution, i.e the relative intensity with which  variations in the sequence similarity scale are translated into the semantic similarity scale, which is measured by the scale parameter of the modelling function . a measure with a higher bias than another will likely yield a higher value of semantic similarity for a given value of sequence similarity, whereas a measure with a higher resolution than another will likely yield a greater variation in semantic similarity for a given interval of sequence similarity.

for each semantic similarity measure in the full dataset, and with each of the sequence similarity metrics , the mean and standard deviation parameters for the two additive normal cumulative distribution functions  used to model it are shown. also shown is the global resolution of the measure, which corresponds to the sum of the scale factors applied to each of the normal functions. although there is some variability on the normal parameters , most of that variability is due to the sensitivity of the modelling method, as the similarity in behaviour between the measures is evident  with the exception of the average approach. as the main criterion to distinguishing between the measures is their resolution, the highest resolutions  are highlighted in bold .

for each semantic similarity measure in the non-electronic dataset, and with each of the sequence similarity metrics , the mean and standard deviation parameters for the normal cumulative distribution function used to model it are shown, as well as the global resolution of the measure. the variability on the normal parameters with rrbs is evident because the fit is somewhat artificial, and does not reflect the fact that the behaviour of measures is visibly isomorphic. the highest resolutions, corresponding to simgic and resnik's measure with the maximum and bma approaches, are highlighted in bold .

the goal of our evaluation was to assess how well each semantic similarity measure captures the similarity in annotation between protein pairs. while previous studies have made this type of assessment by measuring linear correlation  <cit> , or analysing a roc  curve  <cit> , neither approach is suitable for our results because they are neither linear in behaviour nor binary in nature. since the majority of the measures show identical behaviours, we focused on the differences between them with implications on their performance, and choose resolution as an evaluation criterion. a measure with a higher resolution performs better because it is more likely to distinguish between protein pairs with different levels of sequence similarity than a measure with a lower resolution, which suggests it is more sensitive to differences in the annotations.

full vs. non-electronic dataset
overall there are two main differences between the results from the full dataset and those from the non-electronic dataset : semantic similarity values are globally lower in the latter than in the former; and the bimodal-like behaviour evident in the former is absent in the latter.

the lower semantic similarity values can be explained by the fact that the number of annotations per protein is smaller in the non-electronic than in the full dataset . because the proteins have less terms, they are less likely to have shared or similar terms, and therefore have lower semantic similarity.

as for the bimodal-like behaviour in the full dataset, we hypothesise that it is a direct result of data circularity, due to the presence of functional annotations inferred from sequence similarity within the electronic annotations. because functional inference is predominantly made at high levels of sequence similarity, if there was a visible influence of data circularity in our results, we would expect it to be in the form of an abnormal increase in semantic similarity from a given point of the sequence similarity scale. therefore, the hypothesis of data circularity is consistent with the observed second increase in semantic similarity at high sequence similarity values in the full dataset and with the absence of that behaviour in the non-electronic dataset .

we also considered the possibility that this behaviour was tied to the distribution of the number of annotations per protein, as there is a peak of annotations per protein consistent with the range of the transition between “modes” for the lrbs results . however, the absence of a corresponding pattern for the rrbs results is an argument against this possibility.

it should be noted that whether or not the bimodal-like behaviour is a consequence of data circularity doesn't affect the validity of our evaluation. while the issue of data circularity can be of dire consequences when applying semantic similarity for specific purposes, our evaluation of the semantic similarity measures is in no way based on the assumption that all annotations are correct. we are only assuming that there is a relationship between the annotations and sequence similarity  and testing how well each semantic similarity measure captures that relationship.

lrbs vs. rrbs
the differences in the results using the two sequence similarity measures  can be divided into two categories: shape differences, as reflected by the mean and standard deviation parameters of the modelling ncdfs; and range differences, as reflected by the translation and scale parameters . the shape differences correspond to differences in the sequence similarity scale, and are likely due to the fact that lrbs is a logarithmic measure whereas rrbs is a linear measure. indeed, we verified that upon rescaling either sequence similarity measure to the scale of the other one, the results from both measures are described by ncdfs with identical shape parameters . as for the range differences, they are likely tied to the other key difference between the measures: the fact that lrbs is biased by sequence length and rrbs is not. because of this difference, an increase in rrbs corresponds only to an increase in “actual” sequence similarity, whereas an increase in lrbs can be partially due to an increase in sequence length. therefore, we would expect semantic similarity to be more strongly related with rrbs than with lrbs, assuming there is no direct correlation between semantic similarity and sequence length. consistent with this hypothesis, we find that for all measures tested, the resolution is higher with rrbs than with lrbs.

the influence of the bias for sequence length is also visible in the distribuition of the average number of annotations per protein : there is a clear increase in annotations per protein at high lrbs values, whereas there is a sharp decrease in annotations per protein at low rrbs values. these differences can be due to the presence of large bifunctional proteins, which are expected to have a greater number of terms . the fraction of these large proteins in each averaged data point is expected to increase with the lrbs scale, which would account for the increasing number of annotations per protein for high lrbs values. furthermore, alignments between large proteins of low sequence identity, will yield relatively high lrbs values, but low rrbs values. the presence of these alignments is likely more predominant at lower rrbs values, which accounts for the higher number of annotations per protein for those values.

it should be noted that in the case of the rrbs results with the non-electronic dataset the parameters of the modelling functions are not identical between semantic similarity measures . this happens because these results do not match the typical ncdf shape , and therefore the mean and standard deviation are not restrained to the range of the results. because of this, the resolution of the measures could not be obtained from the modelling function and was instead calculated directly from the results . however, after re-scaling them to the lrbs scale, all results followed a ncdf curve with identical mean and standard deviation , leading to the conclusion that the differences in shape in the rrbs scale were only apparent.

combining term similarity measures
in the full dataset, the average combination approach differs from all other measures and approaches tested in that it shows a decreasing behaviour for high sequence similarity values . in order to describe this behaviour, the modelling function for these results required the addition of a negative linear component to be suitably modelled . as we clearly do not expect functional similarity and sequence similarity to be negatively correlated, and this behaviour is exclusive to the average approach, we can only infer that this approach is unable to capture the actual similarity in annotations for proteins with high sequence similarity. the reason behind this behaviour is likely tied with the limitations of the average approach, namely to the fact that it considers proteins as random collections of features. for instance, if two proteins  have the exact same two terms , the average approach compares not only the matching term pairs  but also all the unrelated ones . the consequence of this is that the more terms two functionally identical  proteins have, the less similar they will be considered by the average approach. consistent with this notion, we find that in the range of values where the average approach shows a decreasing behaviour, there is an inversely proportional increase in the average number of annotations per protein as function of sequence similarity . indeed, for high sequence similarity values, the behaviour of the average results is deeply tied with the inverse of the number of annotations per protein. curiously, we have found that if the results with the average approach are compensated for number of annotations per protein, their behaviour becomes identical to that of the other measures .

while in the non-electronic dataset the average approach is similar in behaviour to the other approaches , this is likely because overall the number of annotations per protein is smaller in this dataset and also because it is more uniform over the sequence similarity scale . despite this, the average approach is also the worst combination approach in this dataset, as it shows the lowest resolution .

as for the maximum approach in the full dataset, its low resolution  is a consequence of its simplicity. because this approach only looks for the most similar terms between two proteins, it is impervious to the number and similarity of other terms those proteins might have; therefore it is naturally limited in its ability to distinguish protein pairs. in addition, the maximum approach also shows singular behaviours at low sequence similarity values: with the lrbs measure it shows high dispersion, whereas with the rrbs measure it shows a decreasing behaviour . interestingly, both behaviours are directly related to the distribution of the average number of annotations per protein . this is not unexpected, since the more terms two unrelated  proteins have, the more probable it is that they have a common  term, and therefore the higher their semantic similarity will be with the maximum approach. in the non-electronic dataset, the limitations of the maximum approach are not visible because the number of annotations per protein is lower in this dataset, with the majority of the proteins having only one annotation . therefore the loss of information from using only one term to compare proteins is negligible, which is why this approach is similar in resolution to the bma approach.

the bma approach is clearly the best combination approach in the full dataset, since it not only yields the highest resolutions, but also also does not show the undesired behaviours of the other two approaches. this is because this approach considers all terms of the proteins , but compares only each term with its most similar . its performance is similar to the maximum approach in the non-electronic dataset, because the number of annotations per protein is small, and therefore there is not much term similarity combination involved.

in conclusion, the average approach is contradictory with the purpose of combining term similarities, due to its dependency on the number of annotations per protein; the maximum approach is limited in its ability to compare proteins as it looks for only one shared functional aspect; whereas the bma approach is able to account for all functional aspects independently of the number of annotations per protein.

the influence of grasm
compared to the most informative common ancestor  approach  <cit> , the grasm approach  <cit>  produced systematically lower semantic similarity values , regardless of dataset or sequence similarity measure . this is a natural consequence of this approach: since it considers the average information content  of all disjoint common ancestors instead of only the ic of the mica, it will necessarily yield smaller or equal semantic similarity values . however, the main question is whether considering more of the go graph's information  increases the performance of the semantic similarity measures. in the full dataset, the answer to this question is positive, as grasm leads to an increase in resolution  for all measures tested ; but in the non-electronic dataset the results are not conclusive, as grasm increases the resolution of jiang and conrath's measure, but decreases that of lin's and resnik's measures .

resnik's, lin's and jiang & conrath's measures
independently of approach, dataset or sequence similarity metric, the relationship between resnik's, lin's, and jiang and conrath's measures is always the same : resnik's measure has the lowest bias and the highest resolution; jiang and conrath's measure has the highest bias and the smallest resolution; and lin's measure falls in between the two . this relationship is obviously tied to the measures' definitions of semantic similarity: resnik's measure is directly given by the ic of the mica of two terms; lin's measure is given by a ratio of ics; and jiang and conrath's measure is given by a subtraction of ics. therefore, while all three measures produce results in the same range , they behave differently within that range, which leads to their different resolutions. we can only conclude that resnik's measure is the term similarity measure most adequate for go, since it consistently shows the highest resolution.

simgic and simui
the graph-based simui and simgic measures showed an identical behaviour to that of the term similarity measures combined with the bma approach, suggesting that qualitatively both graph-based and term-based approaches are suitable for protein semantic similarity . however, the fact that simgic showed the overall highest resolutions suggests that quantitatively there is an advantage in considering the information conveyed by the structure of the go graph, rather than just individual annotations. furthermore, simui and simgic have the clear advantage of being computed in a single step, without the need to find matching terms, and independently of the number of annotations per protein.

from the relationship between simui and simgic, we can conclude that, while go-based semantic similarity can be accurately measured without ic, using it considerably improves the resolution of the measure .

from all measures and approaches tested, we conclude that the simgic is the best suited to measure protein semantic similarity, as it yields the highest overall resolutions, which reflects a greater sensitivity to differences in annotation.

CONCLUSIONS
due to the number of go-based semantic similarity measures proposed over recent years, and to the diversity of strategies used to evaluate them, the questions of which measure performs better and what are the advantages and limitations of each measure were still open. to tackle these questions, we compared the majority of the existing go-based semantic similarity measures, and evaluated their performance by assessing how well they capture the expected relationship between functional similarity  and sequence similarity. the influence of electronic annotations was assessed by using two separate datasets, while the effect of protein sequence length was investigated by using two distinct sequence similarity metrics.

for all measures tested, we found that the relationship captured between functional and sequence similarity is not linear. the majority of the measures were similar in behaviour, and could be suitably modelled by rescaled normal cumulative distribution functions with the same shape parameters . one of the key differences between the measures was their resolution, i.e the relative intensity with which variations in the sequence similarity scale are translated into the semantic similarity scale. this was the main criterion used to evaluate the measures since it reflects their sensitivity in capturing the relationship between semantic similarity and sequence similarity.

of the three term similarity measures tested, resnik's measure was the best, having consistently a higher resolution than lin's and jiang and conrath's measures. as for the approaches to combine term similarities, the best-match average approach was clearly the best, not only because it had the overall highest resolutions, but also because it is independent of the number of terms being combined, unlike the average and maximum approaches. the grasm approach significantly increased the resolution of the measures in the full dataset , but produced inconclusive results in the non-electronic dataset. the simgic measure was overall the best performing measure, showing consistently a high resolution. by comparing the simgic and simui measures, we conclude that while the use of the information content in a measure is not essential to accurately convey semantic similarity, it significantly increases its resolution .

we suspect that there may be an influence of data circularity in the results for the full dataset, as the bimodal-like behaviour in this dataset is consistent with the inference of functional annotations between proteins of relatively high sequence similarity. the absence of bimodality in the non-electronic dataset suggests that the effect of data circularity is mainly due to the presence of electronic annotations.

the other major differences between the two datasets are the number of proteins and the number of annotations per protein, which are considerably smaller in the non-electronic dataset as a result of discarding electronic annotations. this loss of information is perhaps the best support for the use of all annotations in large-scale studies, whereas in specific applications where annotation quality is crucial, the use of electronic annotations should be carefully considered. however, as electronic annotations grow in quantity and quality  <cit> , the cost of ignoring them will eventually outweigh the gain.

recently, a number of novel go-based semantic similarity measures for proteins have been proposed  <cit> , employing various strategies. future work will include the evaluation of these novel measures as well as investigating the relationship between gene products semantic similarity and other protein aspects, such as pfam and enzyme commission classification.

