BACKGROUND
single-molecule techniques allow biophysicists to probe the dynamics of proteins, nucleic acids, and other biological macromolecules with unprecedented resolution  <cit> . it is now possible to observe viruses pack dna into capsids  <cit> , helicases unzip nucleic acids  <cit> , motor proteins walk on biopolymers  <cit> , and ribosome domains undergo structural rearrangements during translation  <cit> . these data are acquired by recording the fluorescent output or forces generated from, for example, biomolecules tethered onto microscope slides  <cit> ; walking on biopolymers  <cit> ; diffusing in hydrodynamic flow cells  <cit> ; or pulled by optical  <cit>  or magnetic  <cit>  tweezers. often the molecules studied move through a series of locally stable molecular conformations or positions  and give rise to data of the type shown in fig.  <dig>  from these data, the experimentalist wishes to learn a model describing the number of states occupied by the molecule and the transition rates between states. although the myriad experimental techniques available have much in common, the data they generate often differ enough to require unique models.

for example, some of these models will involve conversion of chemical to mechanical energy, or motion associated with diffusion, or motion associated with transitions between distinct configurational states. modeling the data, then, typically involves introducing several variables — some of which are observed, others of which are latent or “hidden”; some of which are real-valued coordinates, others of which are discrete states — and specifying algebraically how they are related. such algebraic relations among a few variables are typical in physical modeling ; models involving multiple conditionally-dependent observations or hidden variables with more structured noise behavior are less common. implicitly, each equation of motion or of constraint specifies which variables are conditionally dependent and which are conditionally independent. graphical modeling, which begins with charting these dependencies among a set of nodes, with edges corresponding to the conditional probabilities which must be algebraically specified  organizes this process and facilitates basing inference on such models  <cit> .

here we explore the application of a specific subset of gms to biophysical time series data using a specific algorithmic approach for inference: the directed gm and the variational bayesian expectation maximization algorithm . after discussing the theoretical basis and practical advantages of this general approach to modeling biophysical time series data, we apply the method to the problem of inference given single molecule fluorescence resonance energy transfer  time series data. we emphasize the process and caveats of modeling smfret data with a gm and demonstrate the most helpful features of vbem for this type of time series inference.

graphical models
gms are a flexible inference framework based on factorizing a  multivariate joint distribution into  conditionals and marginals  <cit> . in a gm, the nodes of the graph represent either observable variables , latent variables , or fixed parameters . directed edges between nodes represent conditional probabilities. for example, the three-node graphical model x → y → z implies that the joint distribution p ≡ ppp can be further factorized as ppp. data with a temporal component are modeled by connecting arrows from variables at earlier time steps to variables at later time steps. in many graphical models, the number of observed and latent variables grows with the size of the data set under consideration. to avoid clutter, these variables are written once and placed in a box, often called a “plate”, labeled with the number of times the variables are repeated  <cit> . this manuscript will denote hidden variables by z and observed data by d. parameters which are vectors will be denoted as such by overhead arrows.

as an example of a simple gm, imagine trying to learn the number of boys and girls in an elementary school class of n students from a table of their heights and weights. here the hidden variable is gender and the observed variable, , is a random variable conditionally dependent on the hidden variable. the resulting gm is shown in fig.  <dig>  with the parameters of p denoted by  and the parameters p denoted by  and . the expression for the probability of the observed data  and latent genders  is uniquely specified by the graph and the factorization it implies:


in such a simple case it is straighforward to arrive at the expression in eq.  <dig> without the use of a gm, but such a chart makes this factorization far more obvious and interpretable.

inference of gms
in some contexts, one wishes to learn the probability of the hidden states given the observed data,  where  denotes the parameters of the model and k denotes the number of allowed values of the latent variables . if  is known then efficient inference of  can be performed on any loop-free graph with discrete latent states using the sum-product algorithm  <cit> , or, if only the most probable values of z are needed, using the closely related max-sum algorithm  <cit> . a loop in a graph occurs when multiple pathways connect two variables, which is unlikely in a graph modeling time series data. inference for models with continuous latent variables is discussed in  <cit> . for most time series inference problems in biophysics, both z and  are unknown. in these cases, a criterion for choosing a best estimate of  and an optimization algorithm to find this estimate are needed.

inference via maximum likelihood
estimating  is most commonly accomplished using the maximum likelihood  method, which estimates  as

   

the probability  is known as the likelihood. the expectation maximization  algorithm can be used to estimate  <cit> . in em, an initial guess for  is used to calculate . the  learned is then used to calculate a new guess for . the algorithm iterates until convergence, and is guaranteed to converge to a local optimum. the em algorithm should be run with multiple initializations of , often called “random restarts”, to increase the probability of finding the globally optimal .

ml solved via em is a generally effective method to perform inference however, it has two prominent shortcomings  <cit> :

model selection: the first limitation of ml is that it has no form of model selection: the likelihood monotonically increases with the addition of more model parameters. this problem of fitting too many states to the data  is highly undesirable for biophysical time series data, where learning the correct k for the data is often an experimental objective.

ill-posedness the second problem with ml occurs only in the case of a model with multiple hidden states and a continuous observable . if the mean of one hidden state approaches the position of a data point and the variance of that state approaches zero, the contribution of that datum to the likelihood will diverge. when this happens, the likelihood will be infinite regardless of how poorly the rest of the data are modeled, provided the other states in the model have non-zero probabilities for the rest of the data. for such models, the ml method is ill-posed; poor parameters can still result in infinite likelihood.

in practical contexts, the second problem  can be avoided either by performing map estimation  or by ignoring solutions for which likelihood is diverging. that is, one does not actually maximize the likelihood. model selection can then be argued for based on cross-validation or by penalizing likelihood with a term which monotonically increases with model complexity  <cit> . we consider, instead, an alternative optimization criterion which naturally avoids these problems.

inference via maximum evidence
a bayesian alternative to ml is to perform inference using the maximum evidence  method. me can be thought of as an extension of ml to the problem of model selection. where ml asks which parameters maximize the probability of the data for a given model, me asks which model, including nested models which differ only in k, makes the data most probable. according to me, the model of correct complexity  is

   

the quantity  is called the evidence. sometimes it is also referred to as the marginal likelihood, since unknown parameters are assigned probability distributions and marginalized  over all possible settings. the evidence penalizes both models which underfit and models which overfit. the second expression in eq.  <dig> follows readily from the sum rule of probability provided we are willing to model the parameters themselves as random variables. that is, we are willing to specify a distribution over parameters, . this distribution is called the “prior”, since it can be thought of as the probability of the parameters prior to seeing any data. the parameters for the distributions of the prior  are called hyperparameters. in addition to providing a method for model selection, by integrating over parameters to calculate the evidence rather than using a “best” point estimate of the parameters, me avoids the ill-posedness problem associated with ml.

although me provides an approach to model selection, calculation of the evidence does not, on its own, provide an estimate for  the vbem approach to estimating evidence does, however, provide a mechanism to estimate . vbem can be thought of as an extension of em for me. both the vbem algorithm and considerations for choosing priors are discussed in methods.

smfret
before building a gm describing smfret data, it is helpful to review briefly the experimental method. the experimental technique is based on the spectroscopic phenomenon that, if the emission spectrum of a polar chromophore  overlaps with the absorption spectrum of another polar chromophore , electromagnetic excitation of the donor can induce a transfer of energy to the acceptor via a non-radiative, dipole-dipole coupling process termed florescence resonance energy transfer   <cit> . the transfer efficiency between donor and acceptor scales with the distance between molecules  as 1/r <dig>  with fret efficiencies most sensitive to r in the range of  <dig> − 10nm. because of this extraordinary sensitivity to distance, fret efficiency can serve as a molecular ruler, allowing an experimentalist to measure the separation between donor and acceptor by stimulating the donor with light and measuring emission intensities of both the donor  and acceptor   <cit> . usually a summary statistic called the “fret ratio” is used to report on molecular distance rather than the “raw”, 2-channel {ia,id} data, although inference of the raw 2-channel data is possible as well  <cit> . the fret ratio is given by

    

when the donor and acceptor are attached to an individual protein, nucleic acid, or other molecular complex, the fret signal can be used to report on the dynamics of the molecule to which the donor and acceptor are attached . when the experiment is crafted to monitor individual molecules rather than ensembles of molecules, the process is termed single molecule fret . for many biological studies, such as the identification and characterization of the structural dynamics of a biomolecule, smfret must be used rather than fret; the majority of molecular dynamics cannot be observed from ensemble averages. often the molecule of interest adopts a series of locally stable conformations during a smfret time series. from these data, the experimentalist would like to learn  the number of locally stable conformations in the data  and  the transition rates between states. although it is theoretically possible use the fret signal to quantify the distance between parts of a molecule during a time series, there are usually too many variables affecting fret efficiency for this to be practical  <cit> . consequently, smfret is usually used to extract quantitative information about kinetics  but only qualitative information about distances.

the photophysics of fret have been studied for over half a century, but the first smfret experiments were only carried out about fifteen years ago  <cit> . the field has been growing exponentially since, and hundreds of smfret papers are published annually  <cit> . diverse topics such as protein folding  <cit> , rna structural dynamics  <cit> , and dna-protein interactions  <cit>  have been investigated via smfret. the size and complexity of smfret experiments has grown substantially since the original smfret publication. a modern smfret experiment can generate thousands of time series to be analyzed  <cit> .

RESULTS
smfret as a graphical model
a model of the smfret time series for a molecule transitioning between a series of locally stable conformations should capture several important aspects of the process  <cit> . the observable smfret signal is a function of the hidden conformation of the molecule. the noise of each smfret state can be assumed to be gaussian, and the hidden conformations are assumed to be discrete and finite in number. the probability of transitioning to a new molecular conformation should be a function of the current conformation of the molecule . the ccd cameras commonly used in smfret experiments naturally bin the data temporally, so it is convenient to work with a model where time is discrete. the gm expressing these features is called a hidden markov model  and is shown in fig. 4a. from the graph, it can be seen that the probability of the observed and latent variables factorizes as

    

here,  must include parameters for the probability that the time series begins in each state  ≡ πk); parameters for transition probabilities between states  ≡ ajk); and parameters for the noise of the emissions of each state  = n, where µk and λk are the mean and precision of the gaussian). it is necessary to model p separately from all other transition probabilities since it is the only hidden state probability which does not depend on zt− <dig>  the ajk are commonly represented as a matrix, a, called a transition matrix. the probability the time series begins in the kth state and transition probabilities between states are drawn from multinomial distributions defined by  and the rows of a, respectively. the gm for this hmm is shown in fig. 4b. from the gm it can be seen that

    

for a time series of length t where each latent variable can take on k states, a brute summation over all possible states requires o calculations. by exploiting efficiencies in the gm and using the sum-product algorithm, this summation can be performed using o calculations , where each of the t latent states has k <dig> possible combinations of states). the sum-product algorithm applied to the hmm is called the forward-backward algorithm or the baum-welch algorithm  <cit> , and the most probable trajectory is called the viterbi path  <cit> .

there are several assumptions of this model which should be considered. first, although it is common to assume the noise of smfret states is gaussian, the assumption does not have a theoretical justification , and the gaussian distribution has suport , the data cannot be truly gaussian). despite this caveat, several groups have successfully modeled smfret the data as having gaussian states  <cit> . we note that other distributions have been considered as well  <cit> .

second, the hmm assumes that the molecule instantly switches between hidden states. if the time it takes the molecule to transition between conformations is on the same  order of magnitude as the time it spends within a conformation, the hmm is not an appropriate model for the process and a different gm will be needed. for many molecular processes, such as protein domain rearrangements, the molecule transitions between conformations orders of magnitude faster than it remains in a conformation and the hmm can model the process well  <cit> .

third, the hmm is “memoryless” in the sense that, given its current state, the transition probabilities are independent of the past. it is still possible to model a molecule which sometimes transitions between states quickly and sometimes transitions between states slowly . this situation can be modeled using two latent states for each smfret state. the two latent states will have the same emissions model parameters, but different transition rates.

illustration of the inference
a software package, vbfret, implementing the vbem algorithm for this hmm was written and described in  <cit> , along with an assessment of the algorithm’s performance on real and synthetic data. an illustration of the method is shown here, demonstrating three of its most important abilities: the ability to perform model selection; the ability to learn posterior parameter distributions; and the ability to idealize a time series. these abilities are demonstrated on three synthetic k =  <dig> state time series, shown in fig. 5c. the traces all have µ = { <dig> , <dig> ,  <dig> } and identical hidden state trajectories. the noise of each hidden state is σ =  <dig>  for trace  <dig> , σ =  <dig>  for trace  <dig> , and σ =  <dig>  for trace  <dig> .

model selection: for each trace, l, the lower bound of the log, was calculated for  <dig> ≥ k ≥  <dig>  the results are shown in fig. 5a, with the largest value of l for each trace shown in red. for traces  <dig> and  <dig>  l peaks for k* =  <dig>  correctly inferring the complexity of the model. for trace  <dig>  the noise of the system is too large, given the length of the trace, to infer three clearly resolved states. for this trace l peaks at k* =  <dig>  this result illustrates an important consideration of evidence based model selection: states which are distinct in a generative model  may not give rise to statistically significant states in the data generated. for example, two states which have identical means, variances, and transition rates would be statistically indistinguishable from a single state with those parameters. when states are resolvable, however, me-based model selection is generally effective, as demonstrated in traces  <dig> and  <dig> 

posterior distributions: the ability to learn a complete posterior distribution for  provides more information than simply learning an estimate for , and is a feature unique to bayesian statistics. the maximum of the distribution, denoted , can be used as an estimate of  . the subscript here differentiates it from the estimate in the absence of the prior, . the curvature of the distribution describes the certainty of the  estimate. as a demonstration, the posterior for the mean of the lowest smfret state of each trace is shown in fig. 5b. the x and y axes are the same in all three plots, so the distributions can be compared. as expected, the lower the noise in the trace, the narrower the posterior distribution and the higher the confidence of the estimate for the estimate of µ for trace  <dig> is larger than in the other traces because k* = 2; some the middle smfret state data are misclassified as belonging to the low smfret state.

idealized trajectories: idealized smfret trajectories can be a useful visual aid to report on inference. they are also a necessity for some forms of post-processing commonly used at present, such as dwell-time analysis  <cit> . idealized trajectories can be generated from the posterior learned from vbem by using  to calculate the most probable hidden state trajectory   <cit> . the idealized trajectories for each trace are shown in fig. 5c. for traces  <dig> and  <dig>  where k* is correctly identified, the idealized trajectory captures the true hidden state trajectory perfectly. because of the model selection and well-posedness of me/vbem the idealized trajectories learned with this method can be substantially more accurate than those learned by ml for some data sets  <cit> .

CONCLUSIONS
this manuscript demonstrates how graphical modeling, in conjunction with a detailed description of a biophysical process, can be used to model biophysical time series data effectively. the gm designed here is able to model smfret data and learn both the number of states in the data and the posterior parameter values for those states. the me/vbem methodology used here offers several advantages over the more commonly used ml/em inference approach, including intrinsic model selection and a well-posed optimization. all modeling assumptions are readily apparent from the gm. the gm framework with inference using me/vbem is highly flexible modeling approach which we anticipate will be applicable to a wide array of problems in biophysics.

