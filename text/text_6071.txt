BACKGROUND
microarray technology and next generation sequencing have played an important role in discovering important associations between gene expression patterns and phenotype  <cit> . such gene expression technologies have been instrumental in discoveries ranging from the retarding of aging in mice brought about by caloric restrictions in diet  <cit>  to the identification of various types of diffuse large b-cell lymphoma in humans  <cit> ; from characterizing the transcriptomes of in vitro maturated porcine embryos  <cit>  to uncovering the underlying genes and pathways involved in alzheimer’s disease  <cit> . while both microarray and next generation sequencing technologies allow researchers to study the differential expression of genes across conditions or treatments, each has their advantages and disadvantages  <cit> . however, in either case, the resulting increase in genetic knowledge has allowed researchers to group genes with common function into gene sets and test these gene sets for differential expression  <cit> .

one rich source of gene set knowledge is found in the gene ontology database  <cit> . the gene ontology  provides a controlled vocabulary that is not specific to any particular species. this vocabulary is divided into three general ontologies, molecular function , cellular component , and biological process . individual go terms form the basis of these vocabularies and are structured through parent child relationships with more general terms as parents and more specific terms as children. each go term typically contains a definition of its biological process  and other annotation as well as a mapping of all known gene products involved in its specified process . for convenience in presentation, the remainder of this work will focus on the biological process ontology.

gene set testing allows for the quantification of the significance of activity level differences between treatment groups for specific biological processes of interest. for example, a recent study on human longevity compared the gene expression profiles corresponding to  <dig>  different biological processes for nonagenarians and a control group to identify  <dig> biological processes associated with longevity  <cit> . when there are relatively few gene sets  of a priori interest , the impact of the multiplicity correction for the tests of differential expression  of the gene sets can be greatly lessened as compared to individually testing all member genes , improving the power of the test. even when no a priori gene set of interest can be specified, it can still be highly beneficial to test all known gene sets from a biological process database for differential expression, as the number of gene sets is still typically magnitudes smaller than the corresponding number of individual genes  <cit> .

many methods of gene set testing have been proposed in the literature as reviewed in  <cit> . these can essentially be divided into two classes of gene set testing, often referred to as competitive tests and self contained tests. the competitive tests compare the expression profiles of the genes in the set to those not in the set. the self contained tests focus only on those genes within the set and compares them to some fixed standard. while the first are more popular  <cit> , the second have been shown to be more powerful  <cit> . further, the null hypothesis associated with the self contained tests, \documentclass{minimal}



has been shown to be the more logical generalization of single gene testing  as compared to the competitive test null hypothesis \documentclass{minimal}



while gene set testing methods are varied in their approach, they are alike in that they test each go term, i.e. gene set, individually. thus, when more than one go term is tested simultaneously  some sort of multiplicity adjustment is necessary to preserve control over either the familywise error rate  or the false discovery rate  or a derivative of these error rates. the fdr is typically the error rate of choice in exploratory studies where follow up confirmatory studies are then conducted  <cit> . on the other hand, the fwer is typically the suggested error rate for confirmatory studies  <cit> . we also suggest that the fwer is highly appropriate for exploratory gene set studies as, in our experience, it is seldom more results that are desired, but the most promising real significances that are sought. the fwer offers the best error rate control for such conclusions  <cit> .

the focus level method is a powerful method of multiplicity adjustment for self contained gene set testing, which takes into account the structure of the go graph while controlling  the fwer  <cit> . this approach is more powerful than standard fwer controlling methods such as the bonferroni and uniformly more powerful bonferroni-holm  <cit>  procedures for multiple testing with go graphs  <cit> . however, it is important to note that this increase in power comes at the cost of requiring the logical structure of the go graph to become part of the multiplicity adjustment.

the focus level method allows the researcher to select the level of the go graph in which they are most interested. this is called the focus level. the procedure then applies a top-down and bottom-up approach from the specified focus level. first, the terms in the focus level are tested using the bonferroni-holm adjustment  <cit> . then, in the bottom-up approach, any term above the focus level is declared significant when any of its offspring in the focus level have been declared significant. this inheritance of p-values is accomplished through the assumption that a parent term must be differentially expressed if any of its children terms are differentially expressed, a logical assumption for the go graph structure  <cit> . in the top-down procedure, significance of the children of the focus level terms is decided through an application of the closed testing procedure of  <cit> .

while the focus level method is a powerful approach to adjusting for multiplicity, it quickly becomes computationally infeasible when the selected focus level contains a large number of offspring in the go graph  <cit> . this computational limitation makes it essentially impossible to perform the full top-down approach, a rather significant disadvantage  <cit> . using the full top-down approach provides researchers the default focus level of the root node  whenever they have no a priori interest in a given focus level, a common scenario, see for example  <cit> . this also allows adjusted p-values to be considered apart from their context in the go graph which is advantageous to reporting on single significant gene sets of interest. discussions of the significant findings of the focus level method are currently restricted to their context within the go graph  <cit> .

this work proposes an improvement to the top-down portion of the focus level method  <cit>  which we call the short focus level as it performs a shortcut of the full focus level method. this is accomplished using a novel application of the general graphical bonferroni adjustment for multiple testing as proposed by  <cit> , which is a generalization of closed testing based on weighted bonferroni tests  <cit> . the short focus level procedure shows a significant improvement in computational speed  while maintaining similar power to that of the original focus level procedure and even showing a gain in power over the original focus level procedure for certain scenarios while experiencing a loss in power for others. most importantly, the computational improvements are such that the full top-down method can now be performed on a standard operating system within just a few minutes. the r code  <cit>  for the short focus level procedure is included in the mvgst package  <cit> ; see also additional files  <dig> and  <dig> 

methods
the focus level procedure  <cit>  adjusts for multiple gene set tests using the structure of the directed acyclic graphs of the gene ontology . two basic assumptions underly the method. a non-differentially expressed parent gene set implies the children gene sets are also non-differentially expressed.

if the children gene sets form a partition of the parent gene set and are all non-differentially expressed, then the parent gene set is also non-differentially expressed.



these assumptions ensure coherence of the resulting significant subgraph and facilitate interpretations  <cit> . note that these assumptions are necessary if the objective is to control the fwer within the structure of the go graph. if preserving the graph structure in the multiplicity correction is not of interest to the researcher, then the fwer  could be controlled by the standard holm’s correction  <cit>  , however such an approach will result in a loss of coherence for the significant subgraph.

assumptions a <dig> and a <dig> require that the null hypothesis for each gene set is that no genes in the gene set are differentially expressed. the alternative in each case being that at least one gene in the set is differentially expressed. thus, only self contained gene set testing methods  can be used to test the gene sets of the go graph if the focus level method of multiplicity adjustment is used. this excludes gene set enrichment methods such as those proposed in  <cit>  but supports very well the global test of  <cit> , p-value combination methods such as fisher’s and stouffer’s methods  <cit> , as well as global ancova  <cit> , plage  <cit> , and sam-gs  <cit> .

as prescribed by  <cit>  there are two requirements in the selection of the focus level. these requirements are labeled “fl1” and “fl2” here for later reference in subsection “the short focus level procedure”. no offspring of a focus level term be contained in the focus level.

all remaining terms are either ancestors or offspring of the focus level terms.


three possible focus levels  for a simplified example toy go graph.
 full bottom-up approach.  intermediate focus level.  full top-down approach.



the top-down portion of the focus level procedure of  <cit> , which applies the closed testing approach of  <cit> , requires closing the go graph under all unions from the focus level down. this is done by treating each focus level term, along with all of its offspring terms, as separate graphs which are each closed under all possible unions. as these separate closed graphs will share common elements, the full closed graph \documentclass{minimal}

to demonstrate, consider the closures of each of the example go graphs from figure  <dig> as shown in figure  <dig>  in each case, the nodes above the focus level remain unchanged, while the creation of several sets not present in the original example go graph  are required in order to close the graph under all possible unions from the focus level down. since the closing of the graph is only required from the selected focus level down, it is clear from figure  <dig> that the more offspring terms the focus level contains, the greater the number of sets that must be created to close the graph. closing the graph can quickly become computationally infeasible in practice. importantly, performing the full top-down approach as in panel  of figure  <dig> is rarely possible in real applications due to the computational burden.figure  <dig> 
closures of the go graphs from figure 
1
 where the filled nodes represent the different choices of the focus level.
 full bottom-up approach.  intermediate focus level.  full top-down approach.



to partially amend the computational difficulties of the focus level method,  <cit>  implement a more efficient method of computing the closed graph using what they term “atom sets”. these atom sets form a core collection of gene sets which form a basis for all gene sets in the graph. all other gene sets in the graph  can be created through unions of the atom sets. this ensures the size of the closed graph is 2k− <dig>  where k is the number of atom sets, which is often smaller  than the size of the original closed graph. further,  <cit>  recommend selecting the focus level so that no more than 9- <dig> atom sets are required to recreate the offspring of any single focus level term. they also suggest computing only the smallest few adjusted p-values to save computation time in place of computing all adjusted p-values.

this work offers an alternative solution to improve on the computational speed of the top-down portion of the focus level method through an application of the general graphical bonferroni adjustment of  <cit> . this allows for a short-cut of length m in place of the currently applied full closed testing approach of  <cit> . in the following section, we summarize the general graphical bonferroni adjustment approach and show how we tailor the method for a powerful application to the top-down portion of the focus level method.

the graphical bonferroni adjustment
a powerful and versatile graphical generalization of weighted bonferroni based closed testing  <cit>  which provides strong control of the familywise error rate  at a specified level α was proposed in  <cit> . their approach represents all m hypotheses of interest, h <dig> …,hm as nodes in a directed graph. each node can be thought of here as a gene set, with a corresponding hypothesis hi testing for differential expression. node i, representing hypothesis hi, is allocated a local threshold αi for all i= <dig> …,m. nodes are joined by edges with weights gij dictating the proportion of the local threshold αi that is allocated to all connected hypotheses  hj in the case that hypothesis hi is rejected. the structure of the graph as well as the size of the local thresholds αi and edge weights gij is dependent on the objectives of the study. the versatility of the method is in the generality of the regularity conditions and updating algorithm for the directed graph. the regularity conditions require the following  <cit> : the local thresholds α <dig> …,αm satisfy \documentclass{minimal}

the edge weights satisfy 0≤gij≤ <dig>  gii= <dig>  and \documentclass{minimal}



the updating algorithm defines a sequentially rejective test procedure and is given as follows  <cit> . note that pi represents the observed p-value for the test of hypothesis hi.



the proof that algorithm  <dig> defines a sequentially rejective closed testing procedure which strongly controls the fwer at level α is found in the appendix of  <cit> , and depends directly on theorem  <dig> from  <cit> . both  <cit>  and  <cit>  claim that theorem  <dig> from  <cit>  cannot be directly applied to the hypotheses of the go graph as the hypotheses are nested, creating logical restrictions. in their own words,  <cit>  claim that “the shortcut procedure of  <cit>  cannot be applied to restricted hypotheses”. similarly,  <cit>  state, “these methods  <cit>  cannot make use of logical relationships between hypotheses and, as such, do not incorporate graph-based methods which exploit such relationships, such as  of  <cit> ”. however, in the following we present a restricted hypotheses example where the methods of  <cit>  can be applied. the following section sets forward some important notation and vocabulary and then demonstrates that while these claims are technically true, the methods of  <cit>  can be applied to the focus level method if one of the assumptions underlying theorem  <dig> of  <cit>  is slightly relaxed. we prove this with theorem  <dig> 

restricted hypotheses example
let h <dig> …,hm denote m hypotheses of interest and call these the elementary hypotheses. let i denote a non-empty index set such that i⊆{ <dig> …,m} and denote an intersection hypothesis by hi where hi=∩i∈ihi. the closed test procedure  <cit>  utilizes the intersection closed set of hypotheses \documentclass{minimal}

as the hypotheses corresponding to any go graph are always restricted, the methods of  <cit>  cannot be applied to the go graph under the current framework. however, the following closed test example from  <cit>  can be extended to demonstrate how algorithm  <dig> can be applied to the case of restricted hypotheses. this example sets the stage for theorem  <dig>  where we relax the assumptions of  <cit>  to formally establish how the methods of  <cit>  can indeed be applied to restricted hypotheses, and hence, the go graph.

consider the partially nested elementary hypotheses h <dig>  h <dig>  h <dig>  and h <dig> defined as follows for the parameters θ <dig> and θ <dig> where δ <dig> δ2> <dig>    \documentclass{minimal}

the full closure family of hypotheses  of these four elementary hypotheses would contain 24−1= <dig> distinct intersection hypotheses if they were unrestricted. however, the restrictions stemming from the partial nesting of h <dig> with h <dig>  and h <dig> with h <dig>  reduce the final closure to just eight distinct intersection hypotheses. for example, h12=h1∩h2=h <dig> and h34=h3∩h4=h <dig>  computing all intersections and retaining only the disctinct intersection hypotheses shows   \documentclass{minimal}

each of the null parameter spaces corresponding to the hypotheses in  are graphically depicted in panel  of figure  <dig> figure  <dig> 
visualization of elementary hypotheses and their closure.
 graphical demonstration of the elementary hypotheses h
1
,…,h
 <dig> and distinct intersection hypotheses. the null parameter space is shaded in gray for each hypothesis. redundant intersection hypotheses are written in parentheses.  the closed test approach given the structure of the hypotheses. testing begins with h
 <dig>  the full intersection hypothesis, and terminates at or before testing h
 <dig> and h
 <dig> 



a closed test approach to  is given in  <cit>  which begins with the raw p-values p <dig> p <dig> p <dig>  and p <dig> obtained from testing the original elementary hypotheses h <dig> h <dig> h <dig>  and h <dig>  each with α-level tests, respectively. to define the closed test approach, they compute the closed test p-values \documentclass{minimal}

the closed test procedure only tests a hypothesis \documentclass{minimal}
graphical bonferroni adjustment approach for the partially nested elementary hypotheses
h
1
,…,h
4
 which performs the closed test described in [
28
] when algorithm  <dig> is applied to the graph.




consider the sequential rejection procedure resulting from the application of algorithm  <dig>  <cit>  to the directed graph shown in figure  <dig>  initial local thresholds of α/ <dig> are assigned to h <dig> and h <dig> and local thresholds of zero assigned to h <dig> and h <dig> as depicted in figure  <dig>  the weighted edges provide for the reallocation of the local thresholds in the case of rejection of either h <dig> or h <dig>  if neither h <dig> nor h <dig> can be rejected at the α/2-level, then the testing is stopped with no rejections. this corresponds to the first step of the closed test procedure described previously, as proposed in  <cit> . as can be seen in panel  of figure  <dig>  the closed test requires the rejection of the intersection hypothesis h <dig> before any other rejection can occur. this requires that the previously defined closed test p-value \documentclass{minimal}
flow chart demonstration of the equivalence of the graphical shortcut tailored from the methods of [
19
] to that of the full closed test procedure proposed in [
28
] within the context of the previously established restricted hypotheses example. at each step in the chart, the left graph represents the full closed test approach, while the right graph depicts the graphical shortcut.



definitions and preliminaries to theorem 1
a deeper inspection of figure  <dig> will reveal the reason why the shortcut from  <cit>  can be applied to the example of restricted hypotheses of the previous section. to explain how, we must first define two terms, consonance and natural consonance.

the traditional definition of consonance  <cit>  relies on the idea of maximal hypotheses. it states that consonance is the property of certain closed tests where rejection of an intersection hypothesis \documentclass{minimal}

natural consonance is a similar, but slightly more relaxed property than consonance, and differs in that it implies the rejection of only an elementary hypothesis  whenever any other hypothesis \documentclass{minimal}

examining the flow chart of figure  <dig> will reveal that the closed test procedure proposed by  <cit>  has this property of consonance with respect to the elementary hypotheses h <dig>  h <dig>  h <dig>  and h <dig>  i.e., the closed test for this example is naturally consonant. this follows from the fact that rejection of the intersection hypothesis h <dig> implies rejection of either of the hypotheses h <dig> or h <dig> which are two of the original four elementary hypotheses. note as before that rejection of h <dig> requires that either 2p1<α or 2p3<α by the definition of \documentclass{minimal}

we now extend theorem  <dig> of  <cit>  to restricted hypotheses, and thereby verify the appropriateness of the graphical shortcut of  <cit>  for restricted hypotheses. to this end, let m elementary hypotheses h <dig> …,hm of interest be given and denote by  their closure under intersection. for the purposes of theorem  <dig>   can be either restricted or unrestricted. let αi denote the local significance levels for an intersection hypothesis \documentclass{minimal}

theorem <dig> 
 if for ∅≠i,j⊆{ <dig> …,m} with ∅≠hi⊂hj it holds that αi≤αi, then the closed test for  based on local bonferroni tests is naturally consonant and a shortcut equivalent to the following procedure is possible .  <dig>  set m={ <dig> …,m}.

 <dig>  set i equal to the smallest subset of m such that hi=hm.

 <dig>  reject hj if there exists j∈i such that pj≤αj. if no such j exists, then stop.

 <dig>  set m→m∖j.

 <dig>  if |m|≥ <dig> return to step  <dig>  otherwise, stop.



proof.
first, note that in the case of unrestricted hypotheses, natural consonance and consonance are identical  <cit>  so that the proof is already demonstrated in theorem  <dig> of  <cit> . consider then the case of restricted hypotheses in the sense that for ∅≠i,j⊆{ <dig> …,m} with i≠j it is true that ∅≠hi=hj so that \documentclass{minimal}

discussion of theorem 1
some comments are in order regarding theorem  <dig>  first, while an intersection hypothesis hi may not be unique in , it must not be empty for the nested shortcut of length m to exist. second, the only difference between the proof here and the proof for unrestricted hypotheses  <cit>  is in the definition of consonance. here we follow the suggestion in  <cit>  and allow natural consonance, which can be seen as a loosening of the requirements of consonance to include all elementary hypotheses instead of just all maximal hypotheses. the important distinction is that for unrestricted hypotheses, all elementary hypotheses are maximal. the same is not necessarily true for restricted hypotheses. third, as in the previous restricted hypotheses example, restricted hypotheses are often the result of nested elementary hypotheses. this is certainly the case for the hypotheses attached to the gene sets of the go graphs. fourth, the main importance of the extended theorem  <dig> rests with its assurance that a naturally consonant closed test based on weighted bonferroni tests exists so long as the monotonicity condition αi≤αi is satisfied for all ∅≠hi⊂hj in . fifth, theorem  <dig> does not specify that any graph with local thresholds of α= and edge weights g={g}ij, denoted by , can combine with algorithm  <dig> and lead to a consonant closed test. it simply specifies the conditions under which a consonant closed test based on local bonferroni tests can be formed.

one important rule on the graph  when the hypotheses are restricted is that the local threshold αi for an elementary hypothesis hi must remain zero until all elementary hypotheses hj with hj⊂hi are first rejected. this property can be seen to hold for the graph of figure  <dig>  however, if the graph in figure  <dig> allowed for any of h1’s threshold to be passed to h <dig> or similarly, if h <dig> allowed for anything to be passed to h <dig>  this property would no longer hold. so, while theorem  <dig> assures that a consonant closed test exists when local bonferroni tests are used for the testing of each \documentclass{minimal}

that algorithm  <dig>  when applied to a graph , preserves the monotonic property that αj≤αj for i,j such that hi⊂hj can be seen by noting that algorithm  <dig> only provides for the local thresholds αi to remain the same size or increase. never does it allow for them to become smaller. further, at any point in the iterative process, the local thresholds αi define the weighted bonferroni test thresholds αj for the intersection hypothesis i corresponding to the intersection of the elementary hypotheses with non-zero thresholds . hence, as hj will be tested only after hi is first rejected whenever hi⊂hj, it follows that algorithm  <dig> will provide αj≤αj.

the short focus level procedure
we obtain the short focus level procedure by modifying the top-down portion of the focus level method. this is done by tailoring the general graphical shortcut  <cit>  to a go graph as follows. label the m hypotheses corresponding to the test of significance for each go term  as h <dig> …,hm starting with the root node and proceeding in an organized manner through each level of the go graph, ending with the terminal nodes.  let f⊂m={ <dig> …,m} denote the index set of the nodes corresponding to the pre-selected focus level of the go graph. for all mf nodes in the focus level, assign local significance levels of αi=α/mf to each hypothesis hi with i∈f. assign initial local significance levels of  <dig> to all children nodes of the focus level. note that nodes above the focus level will still be tested using the bottom-up approach of the focus level method and are not considered when applying the top-down portion of the method.

using the structure of the go graph, assign to each edge from parent node i to child node j a weight of gij=1/mi, where mi denotes the number of children nodes of node i. after all edge weights have been assigned for the edges defined by the go graph, all terminal nodes are individually joined with mf new edges to each of the mf focus level nodes. these new edges are given weights of 1/mf. ).

at this point, a modified form of algorithm  <dig> of  <cit>  is applied to the resulting directed graph to obtain the final set of significant hypotheses. the modifications ensure that no child node is tested before all parent nodes are first found significant, maintaining the strong control of the fwer under the restricted hypotheses of the go graph as well as maintaining property fl <dig> of the basic assumptions  underlying the focus level method . figure  <dig> demonstrates the application of the described graphical bonferroni adjustment to the top-down portion of the example go graphs of figure  <dig>  comparing figure  <dig> to figure  <dig> provides a heuristic understanding of how the new top-down approach is computationally faster than the original closure approach because no new nodes need to be created.figure  <dig> 
the suggested shortcut to the top-down portion of the focus level method exploits the natural consonance of the weighted bonferroni tests applied to the go graph to avoid closing the graph under all unions as in the original top-down approach.




an algorithm which implements the short focus level procedure is detailed in algorithm  <dig>  here, h denotes the index set of testable hypotheses  and w={wi}i∈h the corresponding set of weights such that α/wi provides the local thresholds αi for each hypothesis hi indexed by i∈h. as described previously, f⊂{ <dig> …,m} denotes the index set of all pre-selected focus level nodes. the notation ci denotes the index set of children nodes of the parent hypothesis hi. similarly, the notations pi and ai denote the parents and all ancestors, respectively, of the node corresponding to the hypothesis hi. finally, we use r and s to denote the index sets of the current and cumulative rejected hypotheses, respectively.



RESULTS
a natural question at this point concerns the advantages and disadvantages of changing the top-down portion of the focus level procedure from the original closed test approach as in  <cit>  to the graphical shortcut of  <cit>  as proposed for the short focus level. if the local tests for each intersection hypothesis were originally performed with weighted bonferroni tests, then the difference between the methods would be that the first performed the full closure test requiring the testing of somewhere on the order of 2m− <dig> intersection hypotheses, while the second, which applies a shortcut, would test no more than m hypotheses with no reduction in the power of the tests. when using the global test for each intersection hypothesis as suggested by  <cit> , the answer to the differences in computation time and power is not as clear. the following simulations demonstrate that neither method is uniformly more powerful than the other, with each having the advantage for certain scenarios. however, as these simulations demonstrate, the newly proposed short focus level procedure is uniformly  computationally faster than the focus level method which will hopefully better enable its use by practitioners.

simulation 1

the following simulation based on the toy go graph depicted in figure  <dig> panel  demonstrates the advantages and disadvantages of moving to the newly proposed graphical shortcut of  <cit>  in the top-down portion of the focus level procedure. the simulation was performed with the phenotype y as a dichotomous class variable  and the data x representing an rna-seq counts matrix with rows as genes  and columns as samples . the number of samples belonging to the treatment group was simulated according to a binomial distribution, where n is the total number of samples, with the added rule that at least two samples were in each group. this allowed for unbalanced data, with the tendency towards fairly balanced designs. separate simulations for sample sizes of n= <dig>   <dig>  and  <dig> were performed.figure  <dig> 
a toy go graph example illustrating the difference between the current focus level method and the proposed short focus level method.
 the full closure of the example toy go graph depicted in panel  that is currently utilized by the focus level method.  the graph  corresponding to the example toy go graph depicted in panel  that is utilized by the proposed short focus level procedure.



the structure of gene assignments to the sets a, b, c, d, e, and f of figure  <dig>  as well as the total number of genes assigned, was allowed to vary in each simulation according to certain parameters. genes were first assigned to the leaf node gene sets c, d, and e. this was accomplished by randomly selecting both the number of distinct sets in each of these sets  as well as the number of genes shared by all possible combinations of the leaf node gene sets. common genes between all or many gene sets was discouraged with small probabilities of occurrence, while common genes between a few gene sets was allowed to occur more frequently. following the assignments of genes to leaf nodes, parent nodes were randomly assigned new genes  as well as all genes contained by their children nodes. the result was a nested graph with at least some overlap common to many gene sets, as is the case within go graphs.

the data counts matrix x was simulated using an actual rna-seq data set as a sampling distribution for the per-gene means in the control group. specifically, the counts kij for all samples j assigned to the control group were generated from a nb distribution, where the means μi were randomly sampled from the per-gene means of the control group from the actual rna-seq data set. the scaling parameter d was set at  <dig> for all simulations. leaf node gene sets  were then selected at random to be significant. each gene mapping to the selected significant leaf nodes was assigned a treatment mean of \documentclass{minimal}

the averaged results of simulation  <dig> are presented in table  <dig>  this example shows greater power for the current implementation of the focus level procedure where the globaltest  <cit>  is used to test all intersection hypotheses and all elementary hypotheses. the greatest power differences of the two methods appear for small sample sizes, n= <dig> in this simulation, and for nodes with relatively few child nodes. the power of the two methods is comparable otherwise. importantly, the computation time for the short focus level procedure is significantly faster, even for this extremely small toy go graph whose closure contains just  <dig> nodes. interestingly, the focus level procedure as it is currently implemented seems to operate best, computationally speaking, when the sample size is moderate, n= <dig> in this simulation.table  <dig> 
summary of results for simulation 1


mean
node
computation

n
method
a
b
f
c
d
e
time 
power calculations were averaged over all levels of the effect size λ and both sizes of m, the maximum leaf node gene set size, for each level of the sample size n.

fl: focus level.

sfl: short focus level.



;simulation 2;

a second simulation study using the toy go graph of figure  <dig> was also used to compare power and computation time of the original focus level method to the short focus level. the closure of the toy go graph in figure  <dig> is more complex than that of the previous simulation, containing  <dig> nodes as compared to the  <dig> of figure  <dig>  panel . this simulation considered the continuous phenotype y∼n and its correlation with simulated gene expression values x. for this simulation m= <dig> genes were partitioned to the  <dig> go ids of figure  <dig> as specified in table  <dig>  expression values xij for each sample i= <dig> …,n and gene j= <dig> …,m were generated as n variates. go ids  <dig>   <dig>  and  <dig> were designated as significant by adding ry, r∈ <cit> , to the expression values of the corresponding genes . thus, by inherentance, go ids  <dig>   <dig>   <dig>   <dig>   <dig>  and  <dig> were also significantly associated with the phenotype y. values of r close to  <dig> provided a strong signal and greater power for detection while r near zero resulted in a very weak signal and correspondingly very low power for detection. goeman’s globaltest  <cit>  was used to test each go id for association with the phenotypic variable y. given that simulation  <dig> suggested that the current focus level procedure performs best at a moderate sample size, n= <dig> was used for this simulation.figure  <dig> 
structure of the toy go graph used in 
simulation 2
. shaded nodes correspond to those go ids which were simulated to be significantly associated with the phenotype y.
allocation of simulated genes to the go ids of the go graph in figure 
8




power and computation time were averaged over  <dig>  simulations. results are presented in table  <dig> for the most interesting case of r= <dig> . they show the short focus level method having greater power at every go id. the computational speed advantage of the improvement is also manifest, showing nearly a  <dig>  fold increase in speed over the current focus level procedure. this second simulation emphasizes the fact that neither approach to the focus level procedure is uniformly more powerful than the other. while it is clear that each has the advantage in certain scenarios, at least theoretically, more work needs to be completed to determine exactly where each is most appropriate. practically speaking however, the computational advantage and similar statistical power  of the short focus level should solicit its use except perhaps for choices of the focus level near the leaf nodes of the graph where the current focus level method is computationally tractable.table  <dig> 
results of the power simulation for the go graph in figure 
8


go:01
go:02
go:03
go:04
go:06
go:07
go:10
go:11
go:13
time
fl: focus level.

sfl: short focus level.



key difference between focus level and short focus level methods
the focus level  and short focus level  methods are the same in the bottom-up approach. they differ in the top-down approach. both are similar in the top-down approach in that they apply the closed testing strategy to the go graph  by closing the graph under all unions of gene ids corresponding to the go ids . the fl method uses the global test to test each intersection hypothesis of the closed go graph <cit> . the sfl method would be a direct shortcut of the fl method if the fl method instead used a weighted bonferroni test to test each intersection hypothesis.

however, it is important to recognize that the fl method could only perform the full closure test, not the short-cut that the sfl method performs, even if the fl method was modified to use the weighted bonferroni tests. the fl method is more consistent in applying the same global test to both original go id hypotheses as well as to all intersection hypotheses. however, it is computationally expensive because it performs the full closure test. the sfl method makes a slight shift in allowing any test  for the elementary hypotheses  and then performing weighted bonferroni tests for all intersection hypotheses. this simplification or generalization allows for the resulting short-cut. hence, the power comparison between the fl and sfl methods is not obvious, and the simulations above show that neither method is uniformly more powerful than the other.

real data application 1
a drawback to the otherwise powerful focus level method is the computational burden which prohibits the full top-down approach from being applied to real data sets  <cit> . when no a priori focus level exists, as is often the case  <cit> , the root node of the go graph is a logical default choice, but requires the full top-down approach. under the newly proposed short focus level method, this is now a computational possibility. the following application to rna-seq counts data from porcine oocytes demonstrates the performance of the full top-down approach of the short focus level procedure to real data. the biological process  root node was selected as the focus level for this study due to there being no other focus level of greater a priori interest.

it is well known that in vivo  maturated oocytes show far greater developmental competence than do those matured in vitro  <cit> . yet, the underlying genetics are still not well understood. to uncover the genetic differences of in vitro matured oocytes as compared to those matured naturally , transcript counts for  <dig> in vivo and  <dig> in vitro maturated porcine oocytes were obtained using the illumina rna-seq platform  <cit> . lanes were populated as shown in table  <dig>  these data from the lab of dr. clay isom of the utah state university department of animal, dairy, and veterinary sciences are reported on here with permission. in this oocyte study, all animal procedures were performed with the strictest adherence to animal welfare guidelines and with regulatory oversight by the institutional animal care and use committee at utah state university.table  <dig> 
experimental design for the
in vivo
  and
in vitro
  oocyte maturation rna-seq data



oocyte no.
embryo type
pig 

*lane  <dig> contained quality problems and was removed from the analysis.



individual p-values testing the differential expression of  <dig>  genes were calculated using the deseq package of bioconductor  <cit>  with pig mother, as identified in table  <dig>  included as a covariate. specifically, these p-values were obtained under the null hypotheses that the per-gene expression strength of the in vivo maturated oocytes  is equal to that of the in vitro maturated oocytes  when accounting for any pig mother effect. this was done through the deseq package  <cit>  which compares a full model  to a reduced model  to determine significance for a given gene.

a gene set analysis using the go bp ontology was then performed to characterize differentially expressed gene products between the two types of oocytes . p-values for each of  <dig>  bp go terms containing at least  <dig> of the  <dig>  entrez ids from the single gene  analysis were calculated using stouffer’s method  <cit> . the r code  <cit>  for stouffer’s method is included in the mvgst package  <cit> ; see also additional file  <dig>  briefly, stouffer’s method transforms each of the p-values  corresponding to an individual gene in the gene set to a standard normal z-score. a single p-value for the gene set is then obtained from the mean of the z-scores by computing the appropriate tail probability  beyond the mean z-score. stouffer’s p-value combination method was applied here as it is more powerful for the consensus alternative than say fisher’s p-value combination test  <cit>  or goeman’s globaltest  <cit> , see discussions in  <cit> . finally, multiplicity adjusted gene set p-values for each bp term were calculated using the short focus level procedure, with the root bp go term  as the focus level. this adjustment  took just  <dig> minutes and  <dig> seconds of processing time on an intel pentium m  <dig>  ghz processor with  <dig> gb of ram. the current focus level method is computationally intractable for thesedata.
significant results from the gene set testing of porcine oocytes obtained from the short focus level procedure using the full top down approach.




real data application 2
as a second real data demonstration of the short focus level method, and to further discuss differences between the short focus level and focus level methods, we used a subset of the famous golub data set  <cit> , specifically the  <dig> training samples publicly available in the r package golubesets  <cit> . briefly, affymetrix hgu <dig> chips were used to profile gene expression in leukemia patients, and we test for differential biological process activity between  <dig> patients with acute lymphoblastic leukemia  and  <dig> with acute myeloid leukemia . using the same focus level method demonstration as in the vignette of the globaltest package  <cit> , we looked only at biological process “cell cycle”  and its descendants in the go graph, with  <dig> total nodes.

we also applied the short focus level method to the same example , and note that while the focus level method  takes nearly  <dig> minutes, the short focus level method  takes  <dig> second. finally, we applied the short focus level method using the root node as the focus level, which also took  <dig> second. in stark contrast, the focus level method using the root node as the focus level was deemed computationally intractable, as even a run time of two weeks was not sufficient to complete it.
adjusted p-values for each of the  <dig> biological processes considered in the golub example. the focus level  method with its default focus level is compared to the short focus level  method using  the same focus level as the fl method and  the root node focus level . red dashed lines correspond to the familywise error rate of  <dig> , and the solid black line represents the line of equality. all axes are on the log scale.



as discussed in the original focus level paper  <cit> , different focus levels provide for different power at differing areas of the go graph. for this reason, it is difficult to make definitive comparisons using results from different focus levels. while the stronger agreement seen between the fl method  and sfl method  in figure  <dig> may seem interesting, the important point is that the fl method is effectively computationally intractable using the root node focus level. the decision to use the fl or sfl method should not be based on power considerations, but rather on computational considerations, especially when no real reason exists to choose the focus level anywhere other than the root node , in which case the fl method is computationally intractable. however, the sfl method is computationally efficient and strongly controls the familywise error rate within the structure of the go graph.

CONCLUSIONS
as pointed out in  <cit> , the go graphs are structured and “it is wasteful not to make use of that structure” in correcting for multiplicity. further, they stress the importance of not making any assumptions on the joint distribution of the test statistics corresponding to each of the gene sets in the go graph while correcting for multiplicity. the focus level procedure both avoids any such assumptions and capitalizes on the inherent structure of the go graph to adjust for the multiple tests performed, resulting in a powerful approach. another advantage of the focus level method is the possibility of incorporating biological knowledge into the adjustment approach through the selection of the focus level, where the method has the greatest power.

this work improves upon the focus level procedure of  <cit>  by altering the top-down portion of the method to utilize the graphical shortcut of  <cit>  in place of the full closed testing approach of  <cit>  as originally suggested by  <cit> . this was made possible by extending the result from  <cit>  to restricted hypotheses  as the hypotheses corresponding to the go graph are always restricted.

the main advantage of the short focus level procedure proposed in this work is the exponential decrease in computational burden. this provides for the most logical default choice of the root node of the go graph as the focus level when no other a priori choice can be specified. another advantage of the improvement is in the ability to consider the adjusted p-values apart from their context within the significant subgraph of the full go graph under the full top-down approach. when the focus level is selected to be anything other than the root node, individual hypotheses must be considered in context of their position within the significant subgraph. however, this is not altogether a disadvantage as “the interpretation of an individual adjusted p-value should depend on the location in the graph where it occurs”  <cit> .

it is our hope that this shortcut for the focus level procedure, the short focus level, will result in more wide-spread use of the method. still, future work remains to be done. the simulations performed within this work demonstrate that each approach appears to be more powerful under different circumstances. hence, further theoretical work is needed to determine the conditions under which each method is most powerful.

additional files
additional file  <dig> 
short focus level and stouffer’s p-value combination r code.




additional file  <dig> 
short focus level r code help file.




additional file  <dig> 
full legend to figure 
 <dig> 



competing interests

the authors declare that they have no competing interests.

authors’ contributions

gs developed the short focus level procedure and performed the simulation studies and real data analysis as well as the original draft of the manuscript. jrs consulted with gs in the simulation design, real data analysis, and development of the method and helped draft the final manuscript. sci designed the porcine embryo experiment, collected all corresponding data and provided the biological interpretations as well as helped draft the final manuscript. all authors read and approved the final manuscript.

